[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Many people experience a traumatic event during their lifetime. In some extraordinary situations, such as natural disasters, war, massacres, terrorism or mass migration, the traumatic event is shared by a community and the effects go beyond those directly affected. Today, thanks to recorded interviews and testimonials, many archives and collections exist that are open to researchers of trauma studies, holocaust studies, historians among others. These archives act as vital testimonials for oral history, politics and human rights. As such, they are usually either transcribed, or meticulously indexed. In this project, we look at the nonverbal signals emitted by victims of various traumatic events and seek to render these for novel representations that are capable of representing the trauma without the explicit (and often highly politicized) content. In particular, we propose to detect breathing and silence patterns during the speeches of trauma patients for visualization and sonification. We are hoping to glean into cultural and contextual differences of bodily expression of trauma through automatic processing of thousands of hours of testimonials from all over the world.   Trauma Diagnosis DSM-IV defines a traumatic event as an event that is “generally outside the range of usual human experience” and would “evoke significant symptoms of distress in almost everyone.” In the diagnosis of Posttraumatic Stress Disorder (PTSD) and acute stress disorder (ASD), DSM symptoms are essential. If the duration of symptoms is less than 3 months, it is defined as Acute Stress Disorder, otherwise, it is defined as Post Traumatic Stress Disorder. There are scarcely any studies that detect PTSD and acute stress disorder without clinical data. Some of these studies include information about heart rate, pulse and breathing patterns of individuals (Davis et al., 1996; Shalev et al., 1998; Ogden and Minton, 2000). Although vocal parameters are used in order to detect clinical depression, no studies investigating the relationship between voice and trauma were found. Some recent work investigates PTSD symptom severity in a multimodal fashion, using questionnaires and skin conductance physiology (Mallol-Ragolta et al., 2018). In this work, we focus on the relationship between trauma, speech and breathing. It is important to note that we do not work with PTSD, as not all trauma survivors have PTSD.   General Approach Our general approach is to use speech features to automatically segment trauma survivor testimonials into speaking, breathing, and silence classes. The duration of the silences, the frequency of breathing and its quality, the whole dynamics of the testimonial are investigated through unsupervised learning and visualization. Statistical testing will be used to measure the effect of cultural and contextual factors.   Speech and Breath Features The most commonly used speech processing techniques in the recognition of emotions and clinical depression in the literature are related to prosody (i.e. pitch, jitter, energy, pause time and speaking rate), as well as the spectral features (i.e. formants) and cepstral features (i.e. Mel frequency cepstral coefficients). Prosodic, source, and acoustic features, as well as vocal tract dynamics are speech-related features affected by depression. Researchers have found that depressed subjects are prone to possess a low dynamic range of the fundamental frequency, a slow speaking rate, a slightly shorter speaking duration, and a relatively monotone delivery (Low et al., 2010; Cummins et al., 2011; Williamson et al., 2013; Mundt et al., 2007; Alghowinem, 2013).  Breathing consists of two phases called inspiration and expiration. During inspiration, the diaphragm is used to increase the volume of the chest cavity, causing the air to enter by mouth or nose to fill the low-pressure lungs. During a normal expiration, the diaphragm and external muscles are relaxed, the chest volume is lowered, the pressure increases, and breath is dispelled. During speech activity, some of the air stored in the lungs are spent to produce sounds. Subsequently, long bouts of speaking also require taking a breath.  Breathing is intimately connected to oxygen flow, and thus is involved in all internal and external systems, including circulation, hormone systems, and the nervous system. Deep breathing in general seems to allow emotional release and processing of the energy stuck in the body from the trauma. Additionally, Ogden and Minton (2000) observed that subjects can have breathing difficulty when they are talking and extemporizing about their trauma.  Many features can be extracted from the speech signal for the automatic detection of breathing, including MFCC parameters, short-time energy, zero crossing rate, spectral slope, and duration. However, this is a difficult problem to tackle \"in-the-wild\". The testimonials we are planning to use in this work have many sources of noise to confound the automatic algorithms, including background music, other persons talking (and breathing), and external noise sources. Sometimes the level of the sound is far from ideal, and the performance of the breath detection will not come close to the ideal situation where the speaker is isolated in a lab environment with high quality recording equipment.   Dataset and Annotations  In order to test the quality of various collections, as well as if the language, culture, and the traumatic events’ nature and date have a measurable effect on breathing patterns, we have collected short clips from survivors' testimonials. We sampled different languages (English, Chinese, Japanese, Spanish, Kinyarwanda), and different mass-traumas (Holocaust, Nanjing Massacre, Tsunami, Guatemalan Genocide, Tutsi Massacre). In the first phase of the project, 20 clips from 10 survivors were manually annotated for \"speaking\", \"silence\", \"breathing\", \"lip noise\", \"other people speaking\" classes. For each survivor, we selected a ‘normal’ speech and a more ‘traumatic’ speech segment. For some survivors, the date of the events were decades ago (i.e. Nanjing Massacre and Holocaust), for some, the memories were fresh (i.e. Tsunami). Each speech segment is annotated to extract the time span of speech, silence, and breathing periods.     Analysis In Figure 1, each line represents one survivor’s speech segments, where light blue circles are speech, red circles are breathing, and green circles are silences. The horizontal dimension shows the time (the longest segment is one and a half minutes), and the circle radii are proportional to duration. On the left side, the normal speech segments are aligned, whereas the ones on the right side are emotionally charged. Typical features in the latter include long silences, pierced by deep breath -especially before telling about the most traumatic event-, sometimes frequent and sharp breathing. As expected, even in such a small sample size, certain characteristics prevail: for instance, personal speech patterns are different, and set the tempo of the speech as well as breathing, but within the personal tempo, deep breathing emerges. Some of these patterns are not heard while listening to the video/audio recordings themselves, but become visible only after the annotated portions of breathing patterns are visualized. Cultural approaches to trauma and disasters might dictate the way events are described, but this small sample suggests the possibility that the breathing and silence patterns that occur while telling a traumatic event are shared across cultures. A much larger scale empirical investigation will follow.",
        "article_title": "Hidden in a Breath: Tracing the Breathing Patterns of Survivors of Traumatic Events",
        "authors": [
            {
                "given": "Alkim Almila",
                "family": "Akdag Salah",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, Netherlands, The",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Meral",
                "family": "Ocak",
                "affiliation": [
                    {
                        "original_name": "Bogazici University, Turkey",
                        "normalized_name": "Boğaziçi University",
                        "country": "Turkey",
                        "identifiers": {
                            "ror": "https://ror.org/03z9tma90",
                            "GRID": "grid.11220.30"
                        }
                    }
                ]
            },
            {
                "given": "Heysem",
                "family": "Kaya",
                "affiliation": [
                    {
                        "original_name": "Namik Kemal University, Turkey",
                        "normalized_name": "Namık Kemal University",
                        "country": "Turkey",
                        "identifiers": {
                            "ror": "https://ror.org/01a0mk874",
                            "GRID": "grid.412006.1"
                        }
                    }
                ]
            },
            {
                "given": "Evrim",
                "family": "Kavcar",
                "affiliation": [
                    {
                        "original_name": "Independent Artist, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Albert Ali",
                "family": "Salah",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, Netherlands, The",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "multilingual / multicultural approaches",
            "multimedia",
            "cognitive sciences and psychology",
            "English",
            "public and oral history",
            "cultural studies",
            "audio",
            "video",
            "speech processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Historians of science usually face challenges in accessing the literature for their research. In the past, they had difficulties with acquiring the data they needed due to missing digital formats as the manuscripts were lying in archives not accessible to all researchers. Although in the present time manuscripts are mostly digitized and transcribed, the digital editions are not connected. Today there are many online digital editions available, but each presented in an individual platform without any connection to other editions. For example, a historian of science interested in the mathematics of the 17th-18th century must search for the data in various platforms; works of Leonhard Euler and the mathematicians of the Bernoulli dynasty are available in the  Bernoulli-Euler Online platform  (BEOL),    Bernoulli-Euler Online,  BEOL ,   https://beol.dasch.swiss/     works of Isaac Newton in  The Newton Project ,   The Newton Project,  http://www.newtonproject.ox.ac.uk/     works of Gottfried Leibniz in the  Leibniz Archive ,   Leibniz Archive ,  https://www.gwlb.de/Leibniz/Leibnizarchiv/english/introduction/     etc.   Although most of these mathematicians were corresponding and had cited each other's works in their books and articles, this linkage of the data is not evident because the works of these mathematicians are currently presented in individual digital platforms which are not semantically linked. For a historian of mathematics who tries to track these mathematicians’ lines of thought, having access to the information about how the data is linked plays a crucial role. From this information, the researcher can find the origin of ideas, estimate the effects of one mathematician's contributions on works of others, observe the relation between ideas, etc. Therefore, having access to the literature of all contemporary mathematicians along with a graph representing the semantic connections of the data through one single platform will facilitate historians' research dramatically. In our project, we aim to provide such a platform without locally storing all digital editions.    Project Description   As the base platform which will provide access and semantic links to other edition projects, we are going to use the BEOL platform. Because it is based on Knora, a web-based virtual research environment for the humanities, providing various research tools to historians as an HTML based Restful API (Geer, 2016). As for editions,  BEOL  contains the correspondence of Leonhard Euler,   Leonhard Euler exchanged more than 3100 letters with 300 correspondents. Currently  BEOL  contains just Euler's correspondence with Christian Goldbach but eventually, it will integrate all of Euler’s and the Bernoullis’ correspondence as well as their articles and books (Alassi, 2018).    and of various members of the Bernoulli dynasty as XML-based online editions (Schweizer, 2017). On the other hand,  The Newton Project  is a TEI XML based digital edition project of Newton's correspondence and works. Since data in both projects are in a structured XML format, they prove to be the best candidates as a prototype. As proof of concept, we first attempt to connect the letter networks of the two projects.   Although Knora provides a generic storage system, an RDF triplestore, our aim is not to store all the data of  The Newton Project  in  BEOL ,   Unlike the similar project  ePistolarium  which stores all the editions of works of Descartes and Huygens locally, we will attempt to avoid the redundancy of data by retrieving the data from  The Newton Project  on demand. In this way, there won't be any need to update the database of  BEOL  by every editorial change in third-party repositories    instead we just store the metadata of letters,   The original facsimiles of Newton's correspondence are served externally by the Cambridge university digital libraries.    such as the date of creation, authors, recipients, URIs pointing to the facsimiles of Newton letters, subjects discussed in the letters, etc. These metadata will be extracted from letters of  The Newton Project  and will be imported into  BEOL . The correspondents and mentioned persons of the letters will then be linked to the existing persons of  BEOL , and the letters containing specific subjects will be connected to  BEOL  letters about the same topic, see Figure. 1.      Figure.1 Brief depiction of project framework and data model as RDF triples   Furthermore, with the search features of Knora, a  BEOL  user can perform specific queries on the data, for example, to acquire all letters written in a specific date interval by a specific person. Knora will then check the entire  BEOL  database to retrieve all the letters complying with this search criteria. The search results which are originally part of  BEOL  will be directly presented to the user in  SALSAH ,   SALSAH,  System for Annotation and Linkage of Sources in Arts and Humanities    https://dhlab-basel.github.io/Salsah/     the graphical user interface of Knora, and the results which are originally part of  The Newton Project  will be retrieved from the server of  The Newton Project  and presented to the user in  SALSAH . Moreover, there will be a search component integrated into  The Newton Project  with which the users of  The Newton Project  can benefit from the search functionality of Knora to perform advanced queries on the database of  The Newton Project .   Lastly, since  BEOL  users have their own work-space in which they can access the data and annotate it, it is important that when users are working on resources retrieved from  The Newton Project , they have permanent access to the state of the letters which they were initially presented. Therefore, in case, a user starts to annotate a resource, a stable version of the retrieved Newton letter will be cached and stored in Knora in order to both preserve user's annotations and keep track of the user's works on a resource through versioning functionality of Knora. Caching the data can be avoided if the third-party repository- in this case  The Newton Project  -provides versioning of data. Then the required version of the data will be fetched from that repository.  Conclusion  Connecting the repositories of digital editions will preserve the information which would be difficult to access without semantically linking the data. In general, establishing a link between repositories will be useful for all disciplines of humanities. In  BEOL  focus is on the early modern mathematics but all the features developed for this project will be generic and can be used for other projects in humanities.  ",
        "article_title": "Newton virtually meets Euler and Bernoulli",
        "authors": [
            {
                "given": "Sepideh",
                "family": "Alassi",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab, University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Tobias",
                "family": "Schweizer",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab, University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Hawkins",
                "affiliation": [
                    {
                        "original_name": "Faculty of History, University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Iliffe",
                "affiliation": [
                    {
                        "original_name": "Faculty of History, University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Rosenthaler",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab, University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Mattmüller",
                "affiliation": [
                    {
                        "original_name": "Bernoulli Euler Center, University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Helmut",
                "family": "Harbrecht",
                "affiliation": [
                    {
                        "original_name": "Bernoulli Euler Center, University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "linking and annotation",
            "theory and methodology)",
            "English",
            "history of science",
            "semantic web and linked data",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This short paper raised as a part of my ongoing PhD thesis that aims to introduce and study the concept of a Digital Waqf library. The paper argues that the current rules and guidelines of the concept of Waqf needs to be reviewed and updated in order to adapt with the digital age and the new digital innovations.  Waqf is an Islamic concept. It has existed since the days of the prophet Mohammed (peace be upon him). Waqf can be defined as a kind of pious endowment with special requirements and conditions. It aims to continuously benefit the community as well as seek a reward and forgiveness from God for the donor of the Waqf. The word Waqf in the Arabic language means stop. Therefore, the concept of Waqf aims to stop an asset from being sold, given away, or even inherited, and extracts the benefit from it to certain individuals, groups, or organizations (Nasution, 2002).   Waqf can come in different forms, such as real estate, libraries, schools, wells, etc. However, there is a set of mandatory requirements which must be applied for any donation to be registered as a Waqf. If these requirements cannot be met, then the donation cannot be registered as a Waqf; rather, it could be a normal donation and will not fall under the rules and protections of the Waqf. These requirements are mentioned in many different Islamic studies which discuss Waqf, such as Aldawood (1980), Kbha (1999), Haggar (2002), and Alshangity (2015).   Waqf libraries are a type of Waqf. They are ancient libraries, and many old Waqf libraries still exist today and contain valuable historical collections that include manuscripts, books, and artefacts. An example of an existing Waqf library is the Arif Hikmat Waqf library in the city of Medina. Hence, a Waqf library is a kind of public service body where the owner maintains the collection for the benefit of certain people or organizations.   However, Waqf libraries have not received as much attention as other libraries, even in the Islamic world. The literature lack studies concerning traditional Waqf libraries, let alone digital Waqf libraries. The majority of users in the Islamic world do not have enough knowledge about Waqf in general, let alone Waqf libraries (Kbha, 1999).  Waqf libraries have been important contributors to shaping Islamic civilization due to their valuable collections that include books, manuscripts, and artefacts. Hennigan (1999) explains that and states, “it is not an exaggeration to claim that the Waqf, or a pious endowment created in perpetuity, has provided the foundation for much of what is considered ‘Islamic civilization’’ (p. 1).  Waqf libraries spread through the Islamic world into cities such as Damascus, Istanbul, Mecca, Medina, and Cairo; some still exist, while others have been destroyed due to wars, such as the conflict in the western Kosovo town of Gjakova/Djakovica. In this example, the Waqf library of Hadum Suleiman Aga, which was founded in 1595, was burned by the Serb military around 1999, which resulted in the loss of its complete ancient collection (Riedlmayer, 2007).   Waqf libraries, like other types of libraries, have the ability to benefit from any type of technology to develop their services, which will include creating a digital version of their existing library. Reading through the literature about Waqf libraries, it mainly discusses the traditional version of these libraries, not a digital version. Therefore, one of the main and important questions of my thesis is to determine whether we can introduce the concept of a digital Waqf libraries, which leads us to the question: is a digital Waqf library still a Waqf library?  The challenge in this is that the traditional Waqf libraries are already following strict requirements in order to be recognized and registered as Waqf libraries, but how can we apply these to a digital version? Therefore, can we simply create a digital library and decide that we want it to be recognized as a digital Waqf library? The initial findings of my thesis argue that based on the current requirements and rules of Waqf, not any type of digital libraries would qualify to be registered and recognized as a Waqf digital library. I concluded that based on the current requirements of Waqf, only a hybrid digital library would qualify as a Waqf library. And by a hybrid digital library I mean a digital library that is generated and based on an approved physical Waqf library. Therefore, the current rules and requirements of Waqf would prevent the registration and recognition of any born digital library (a library that exist only as a website or an application and does not have a location on the ground) as a Waqf.  Hence, that raised the need to advocate to an immediate review and an attempt to update the current requirements and rules of Waqf in order to regulate how electronic innovations such as born digital libraries, virtual schools, digital books, can qualify to be registered and donated as a Waqf.  At any rate, though Waqf is an ancient concept, it has not yet been reviewed by an official Islamic committee to adjust its rules and regulations to fit the current digital age. Increasingly, however, this kind of adjustment is vital and necessary, because of the great many new things that now exist, though a high portion cannot be accepted and registered as a Waqf; the current rules and requirements of Waqf would prevent such registration, and no one can simply issue a statement to adjust Waqf’s rules and requirements. An official committee of recognised Islamic scholars would have to study this issue and come up with new regulations on how to deal with Waqf in the digital age, thereby fulfilling the need for a detailed Islamic study to be conducted to study this important matter. Such review and update will allow different types of digital innovations including born digital libraries to be qualified to be registered as a Waqf .  ",
        "article_title": "Waqf Libraries And The Digital Age",
        "authors": [
            {
                "given": "Ahmed Mohammed",
                "family": "Alshanqiti",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow, United Kingdom",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "electronic literature",
            "library & information science",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " PROJECT SUMMARY The Dutch Republic established its importance on the world stage through its early successes in global trade, becoming for a time the preeminent circulator of luxury and wholesale goods for the European market. This success was due in part to the new country’s nimbleness in adapting to local circumstances to undermine their Iberian competitors, and their financial creativity and business acumen in creating the first multinational publicly-traded stock company, the Dutch East India Company (VOC), and its counterpart, the Dutch West India Company (WIC). Our project is a collaboration between specialists in the VOC and the WIC, who have both worked to establish the centrality of trade and the circulation of goods to Dutch Golden Age art history, and now join forces to bring the previously siloed considerations of these companies, East and West, together, through the examination of different modes of textile circulation.   The types of textiles that circulated the globe on VOC and WIC ships were varied—ranging from Indian cotton to Haarlem wool to Silesian linen—and their values rose and fell according to demand.  Data on textile circulation are likewise varied and dynamic, often moving (un)easily between quantitative and qualitative categories. For example, whereas textiles frequently circulated as wholesale commodities, they were also presented as diplomatic gifts, whose value was determined by social—rather than monetary—terms. Moreover, beginning as early as the second half of the seventeenth century, textiles were the favored bartering currency in the inhumane brokering of human life known as the Atlantic slave trade, giving them an uneasy status in early modern social and economic history.  Textiles—re-presented as garments in paintings and prints—also became potent signifiers in an increasingly global world, where clothing played a critical role in shaping identity in colonial and European circles. Our project,  Batavia and the Gold Coast: Mapping Textile Circulation in the Dutch Global Market, seeks to make connections between these economic, social, and visual data—which so often exist as discrete epistemological categories—through the development of an open-access database and an interactive map.  While our project was inspired by foundational research questions that engage both art historical and economic methodologies, they are united by factors that are fundamentally spatial: What types of textiles circulated in Batavia and the Gold Coast and where was their point of origin? How were representations of textiles linked to their circulation (either as gifts or commodities)? What were the internal and external factors in each geographic location that impacted the textile trade? What impact did the slave trade have on the trade/circulation of textiles by both the VOC and the WIC?  What role did indigenous communities play in facilitating or hindering the local and global circulation of textiles? Despite the disparate geographies served by the VOC and WIC, how were the Companies linked? The intellectual and spatial breadth of our questions necessitated a project that was both collaborative and digital, one that we hope will provide a model for future work in Digital Art History.  THE CONTENT The Interactive Map The richness of our data—which includes visual images, archival data, and geospatial coordinates—requires a robust web-based database upon which other scholars of Dutch trade can eventually build.  We are working closely with a developer to create an interactive map using Leaflet, an open-source JavaScript library, which will visualize our textile data in geographical space.  This platform will enable users to geolocate our VOC/WIC data, which is drawn from published and archival cargo lists, according to variables such as textile type, geographical origin and destination, and time. Image Database Beyond the archival sources, we are also compiling a database of images featuring textiles that have been re-presented as garments on both “exotic” and European bodies in a colonial context, such as Albert Eckhout’s famous series of so-called ethnographic portraits in Copenhagen; the three portraits of African envoys also in Copenhagen and attributed to Eckhout; Andries Beeckman’s 1661  Castle of Batavia in the Rijksmuseum; the many portraits of VOC governors that would have originally hung in the Casteel Batavia, also in the Rijksmuseum; and Dirck Valkenburg’s 1706  Slaves on a Sugar Plantation in Surinam, among others.  This database of images will provide what the mapped data cannot: a visual record of the ways in which textiles were closely linked to complex categories like race, gender and economic status.  Visual Textile Glossary One of the most important contributions of “Batavia & The Gold Coast” is our visual textile glossary—a much-needed resource that is currently lacking in the scholarship of the early modern period. Our glossary will include each textile’s name, a standard definition, and a visual example of that textile—either in the form of internal links to our database of painted images, external links to textiles housed in museum collections, or both.  Our visual textile glossary will also link to the mapped data, so the user can see—alongside the visual material—how particular types of textiles circulated the world as objects of trade. ADHO 2019: SHORT PAPER PROPOSAL This project fits well with the ADHO’s theme “complexity,” as it visualizes the myriad ways in which textiles were valued in the early modern global world: as trade goods, as mediators in diplomacy, as garments with profound social significance, and as physical objects comprised of raw material from across the globe. In our 10-minute presentation we will introduce our methodological approach; share our in-progress maps, images, and data; and demonstrate the interdisciplinary importance of understanding the dynamic movements and meanings of textiles in the early modern world. ",
        "article_title": "Batavia and the Gold Coast: Mapping Textile Circulation in the Dutch Global Market",
        "authors": [
            {
                "given": "Carrie J.",
                "family": "Anderson",
                "affiliation": [
                    {
                        "original_name": "Middlebury College, United States of America",
                        "normalized_name": "Middlebury College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0217hb928",
                            "GRID": "grid.260002.6"
                        }
                    }
                ]
            },
            {
                "given": "Marsely L.",
                "family": "Kehoe",
                "affiliation": [
                    {
                        "original_name": "Hope College, United States of America",
                        "normalized_name": "Hope College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03chnr738",
                            "GRID": "grid.257108.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-08",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  What is computational thinking in the digital humanities? The question whether digital humanists should learn to code has been highly-contested (Ramsay, 2011: 243–45). The ‘hack’ versus ‘yack’ debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what Davidson terms “collaboration by difference” (Davidson, 2015: 134). Given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? Jeannette M. Wing coined the term “computational thinking” to address the teaching of digital literacy beyond computer science. She contended that “computational thinking is a fundamental skill for everyone, not just for computer scientists” (Wing, 2006: 33). In  Digital Humanities, Berry and Fagerjord comment at length on Wing’s definition, concluding that “a critical understanding of computing at its different levels is a prerequisite for a digital humanist...” (Berry and Fagerjord, 2017: 59). There is little agreement, however, about the best way to teach computational thinking to humanists. We review the potential of visual or “block-based” programming languages for teaching computational literacy in the digital humanities. We argue that digital humanists should learn from these tools’ emphasis on the ludic over the pragmatic. We also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science.    Review of educational programming environments The use of visual or “block-based” programming has become a mainstay for computer science education in the K-12 arena. Block-based programming involves the manipulation of graphical elements to create units of computation. The authors of  Learnable Programming: Blocks and Beyond argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, “chunking code,” and constraining options (Bau et al., 2017: 72–80). These pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults.    Logo. The origins of block-based programming stretch back to Logo, a programming language and graphical environment for computer science education. While not a visual programming language, when paired with Turtle Graphics Logo provides students with the ability to visualize their computations (Papert, 1980: 16–20). Logo has gained renewed popularity among the elementary age set due to Gene Luen Yang and Mike Holmes’  Secret Coders, a series of graphic novels that employs Logo to teach basic computational literacy (Yang and Holmes, 2015).    Scratch. The designers of Scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. The designers stripped away many of the complexities of software development (e.g., linking libraries and compiling binaries) to create what they term a “tinkerable” environment, pioneering the use of blocks for syntax (Maloney et al., 2010: 16:4).   Snap! While Scratch succeeded in developing an extensive community of users in the K-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. The Snap! programming environment emerged from a collaboration between Brian Harvey at Berkeley and Jens Mönig, a software developer currently at SAP. Drawing on long experience teaching functional programming in Scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a Scratch-like syntax (Harvey and Mönig, 2015: 35–38).   NetsBlox. NetsBlox is an adaptation of Snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer (Broll et al., 2017: 81–86). Students can draw on these features of NetsBlox, for example, to place markers representing art museums in the vicinity on a Google Map or to create a shared digital whiteboard. By fostering the ability to communicate beyond the boundaries of the programmer’s laptop, NetsBlox paves the way for creating data-driven digital humanities projects. Data may also be persisted in the cloud, making it possible to preserve state. Currently, NetsBlox comes with the ability to call out to a select number of services. However, the developers envision “adding a lot of new services and data sources to NetsBlox...”(Broll et al., 2017: 86). This raises the question whether a version of NetsBlox could be developed specifically for digital humanists, integrating web-based application programming interfaces (or APIs) for platforms like the DPLA, the HathiTrust, and Europeana, among others.      NetsBlox with prototype RPC block for Wikidata The digital humanities community also embraces visual programming models. Voyant Tools, for instance, provides a graphical interface for scholars seeking to study textual corpora (Sinclair et al., 2016). To date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities.   Programming as ludic rather than pragmatic Is learning block-based programming a means to an end or an end in itself? While computer science students will inevitably move from block-based to text-based programming (Kölling et al., 2015: 29–38), the designers of Scratch claim that many students will fruitfully remain within its environment (Resnick et al., 2009: 66f). At the secondary and post-secondary level, “The Beauty and Joy of Computing” curriculum likewise promotes the enjoyment of programming within the Snap! environment: “having fun is an explicit course goal” (Garcia et al., 2015: 71). Leading digital humanists also acknowledge the playful aspects of programing. In “On Building,” Stephen Ramsay remarks, “Learn to code because it’s fun and because it will change the way you look at the world” (Ramsay, 2011: 245). Nick Montfort argues that motivations for learning programming go beyond the “merely instrumental” (Montfort, 2016: 268), remarking “it is enjoyable to write computer programs and to use them to create and discover” (Montfort, 2016: 277). By customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research.   Prolegomena to any future visual programming environment for the digital humanities What would a digital humanities version of a block-based programing environment look like? By way of conclusion, we suggest how NetsBlox might evolve past its origins in Scratch to provide a shared platform for teaching computational thinking in the digital humanities. We propose three developments: 1. creating default sprites that represent the domains of digital humanities research (i.e. representing books rather than basketballs); 2. establishing libraries of blocks to call commonly-used web-based APIs in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. By developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking.  ",
        "article_title": "Thinking Computationally in the Digital Humanities: Toward Block-Based Programming for Humanists",
        "authors": [
            {
                "given": "Clifford B.",
                "family": "Anderson",
                "affiliation": [
                    {
                        "original_name": "Vanderbilt University, United States of America",
                        "normalized_name": "Vanderbilt University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02vm5rt34",
                            "GRID": "grid.152326.1"
                        }
                    }
                ]
            },
            {
                "given": "Lynn T.",
                "family": "Ramey",
                "affiliation": [
                    {
                        "original_name": "Vanderbilt University, United States of America",
                        "normalized_name": "Vanderbilt University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02vm5rt34",
                            "GRID": "grid.152326.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "pedagogy",
            "computer science and informatics",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  With the increasing availability of large corpora, humanist scholars gain opportunities to choose their material in a more data-driven way. How can we identify texts or text sections relevant to our research question if we abandon prior knowledge as a determining factor? In this paper, we explore the potential of semantic fields for finding text sections about a topic of interest.   We use the term “topic” in the sense of “subject of a text”. We do not want this to be confused with the term used for the results of topic modeling methods. By topic, we do not refer to the topic modelling concept (Blei, 2012), but to different subjects in a text.  Additionally, we want to address the major issue of evaluating a task involving a great deal of interpretation.  The use case we present is the identification of text sections about body and illness. This is motivated by our larger research project that focuses on health (Gaidys et al., 2017). As test data, we use extracts of about 7,000 words from diverse research domains addressed in our research project:  the novel  Corpus Delicti (2009) by Juli Zeh;  the novel  Ellen Olestjerne (1903) by Franziska zu Reventlow;  an interview with a dying patient; and a protocol from the German Bundestag (federal parliament).    Manually annotating topics Our guidelines for the manual annotation are available in German   http://doi.org/10.5281/zenodo.2634297 . They determine that we consider a text span to be about illness or body if it is depicted as such by the text. The exact decision of what to annotate remains somewhat arbitrary, as is unavoidable in hermeneutic annotations. In order to agree on a concept of body and illness across disciplines, both were defined in a narrow way. The annotation was carried out by two independent annotators who did not receive any input beyond the guidelines. We used the annotation tool CATMA (Meister et al., 2018).  We calculate the agreement between the annotators in order to estimate the difficulty of the task and the quality of our guidelines. For this calculation, we compare the annotations sentence by sentence. If any word was annotated, we consider the whole sentence to be annotated. The objective of the task is to identify text sections and not phrases, so this abstraction is adequate. It also facilitates comparison as we do not need to deal with overlaps. In terms of agreement this is a rather tolerant approach. As a result, the agreement is relatively high, given the interpretative nature of the task. The chance-corrected scores range between 0.54 and 0.90, showing the varying difficulty in the texts and topics. Some of the disagreement could potentially be avoided by further refinement of the guidelines.     Corpus Delicti Ellen Olestjerne Interview Protocol   body 0.90 0.86 0.66 -   illness 0.54 0.71 0.74 0.84   Table 1: Inter-annotator agreement, measured by Kappa (Fleiss, 1971) (no mentions of body in the protocol) For the gold standard, the annotators and two other researchers resolved the discrepancies between the two annotations. Table 2 shows the absolute numbers of annotated sentences.    Corpus Delicti Ellen Olestjerne Interview Protocol   body 113 38 38 -   illness 22 27 67 13   Table 2: Number of annotated sentences for the two topics   Semantic field generation We generated semantic fields in the following ways (Adelmann et al., 2019):  The German Integrated Authority File  GND   http://www.dnb.de/gnd (accessed April 29, 2019)  (‘Gemeinsame Normdatei’, Wiechmann, 2012) is a controlled vocabulary with a hierarchy of concepts and references. All hyponyms for body (2,704 words) and illness (11,935 words) were extracted.   GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) is a lexical-semantic net that is structured in hypernym and hyponym relations. We extracted all hyponyms to body (2,170) and illness (2,043). We excluded all hyponyms to illness from the semantic field of body.  We created  word embeddings (WE) with the “gensim” implementation (Řehůřek and Sojka, 2010) of word2vec (Mikolov et al., 2013) on a collection of more than 2,500 full-texts of German literature from around 1900. We took the 100 words most similar (in terms of the cosine similarity of their embedding vectors) to body and illness, respectively.   All words of the semantic fields were expanded by all possible inflection forms using SFST (Schmid, 2005) and the model by (Sennrich and Kunz, 2014). The texts were automatically tagged with the three semantic fields using CATMA’s query function.   Evaluation For the evaluation of the semantic field approach we compare it sentence by sentence with the gold standard. Table 3 shows the results for precision, recall and F1 scores. As can be expected for an annotation task involving much interpretation, not even half the scores reach more than 0.5. The GND semantic field has a better recall than precision as it is very large, especially for illness. GermaNet and WE score higher on precision than recall. The combination of all three semantic fields results in a clear improvement for the semantic field of body.    illness body    Precision Recall F1 Precision Recall F1   GND 0.20 0.41 0.31 0.55 0.69 0.62   GermaNet 0.85 0.23 0.54 0.30 0.13 0.21   WE 0.60 0.34 0.47 0.57 0.23 0.40   mean 0.55 0.33 0.44 0.47 0.35 0.41   combination 0.21 0.45 0.33 0.43 0.79 0.61   Table 3: Results (scores above 0.5 in bold) For example, words like ‘Hand’ (hand) as a part of the body or ‘Virus’ (virus) as an indicator for illness were found both by the manual annotations as by our queries using the semantic fields. Our approach generates false negatives when the topics of interest were mentioned in an indirect way, as it is frequently the case in literature such as ‘zu ihren Füßen’ (at her feet). Additionally, our semantic fields consist of nouns only, so all other parts of speech were neglected. False positives were produced when words about body or illness were used metaphorically as for example ‘aus dem Auge verlieren’ (to lose track of) or mentions of ‘Herz’ (heart) in the context of ‘offenherziges Lächeln’ (open-hearted smile) .   Conclusions The identification of specific topics using existing or automatically generated semantic fields does not fully reproduce what human annotators do. Researchers relying on this method should be aware that they systematically lose texts with specific features such as a more indirect style which results in a biased corpus. There are many false positives that can be manually removed. For scenarios with large corpora, an approach like this is still a feasible one. If we apply the method to identify units of text larger than sentences, the results might improve. We intend to conduct experiments to this end in the future. A higher-level question is how we can adequately evaluate tasks involving a great deal of interpretation. There are many possible ways of operationalizing, the topic body and our annotations guidelines represent only one. We consider our contribution to be a rough first approximation to a solution of this issue.  ",
        "article_title": "Evaluation of a Semantic Field-Based Approach to Identifying Text Sections about Specific Topics",
        "authors": [
            {
                "given": "Benedikt",
                "family": "Adelmann",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Melanie",
                "family": "Andresen",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Anke",
                "family": "Begerow",
                "affiliation": [
                    {
                        "original_name": "Hochschule für Angewandte Wissenschaften Hamburg, Germany",
                        "normalized_name": "Hamburg University of Applied Sciences",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fkqwx76",
                            "GRID": "grid.11500.35"
                        }
                    }
                ]
            },
            {
                "given": "Lina",
                "family": "Franken",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Evelyn",
                "family": "Gius",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Vauth",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Hamburg, Germany",
                        "normalized_name": "Hamburg University of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04bs1pb34",
                            "GRID": "grid.6884.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "semantic analysis",
            "german studies",
            "data mining / text mining",
            "English",
            "cultural studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We present here a project to prepare the digital critical edition of the  Chronicle of Matthew of Edessa, which is due to finish its first stage in April 2019. The  Chronicle is a 12 th century Armenian-language historical text covering events in the Near East during the 10 th-12th centuries. This takes the reader from the apogee of the medieval Armenian kingdoms to the fall of most of them during the eleventh century as their lands were annexed to Byzantium and ultimately lost to the Seljuk Turks. The Chronicle is also an important source for the history of the First Crusade, and particularly the Crusader County of Edessa. There are about 40 known manuscripts that contain the text of the Chronicle in whole or in part, copied between the end of the 16 th century up to the 19 th.  In our workflow we have adopted the stages of digital critical edition suggested by Robinson  (2004) – transcription, collation, stemmatic analysis, edition and publication. We found, in the process, that these stages do not occur in a strict succession; it was quite regularly necessary to move back and forth between stages, refining earlier steps on the basis of later results. One of the central features of our project was to adopt a  continuous integration (CI) system   In this case, the software in question is Concourse ( https://concourse-ci.org/).   in order to manage the work across these stages in a sensible manner. The primary challenge we then had to overcome was the need to ensure that the data was cleanly maintained from beginning to end, as the nature of CI design does not allow for modifications in the middle of the pipeline.  Beginning with the transcription, where we also followed general guidelines proposed by Robinson and Solopova  (1993), diplomatic transcriptions of all available manuscripts were made in T-PEN  (Ginther et al., 2009) according to guidelines maintained in the project’s GitHub repository, and converted from T-PEN’s native Shared Canvas JSON format into valid TEI-XML using a Python library developed for the purpose. A major advantage of the digital approach is the ease with which entire categories of transcription error can be identified and corrected automatically. The CI framework enabled parsing and validation errors to be spotted immediately. Rare cases could be corrected manually in T-PEN (that is, the source of our pipeline data), but widespread mistakes required us to revise our workflow tools to behave in a way more akin to functional programming, so that we could insert custom code to handle peculiarities of our specific texts without compromising the more general design of the tools themselves.   Given transcriptions that passed our litmus tests for validation and basic accuracy of content, we were then ready to collate them using the JSON input functionality of CollateX  (Dekker and Middell, 2011). Here again we relied on custom programming within the CI setup – while programmatically taking TEI files and tokenise the text contents into individual readings is a straightforward task in its basics, the specifics will vary massively from text to text. Our tokenizer software thus provides a number of code plug-in interfaces that can be used to generate a correct token, also in the specific XML context of that reading in the document. The CI setup also allowed us to make, and preserve, a number of custom modifications to how we used CollateX, in order to maximise its accuracy.  Since one of our transcription guidelines was to leave abbreviations unexpanded, we also developed a tool using a combination of base text collation, regular expression logic, and user interactivity to (semi-)automatically expand these abbreviations and store the results in a database, which could in turn be fed into the tokenisation step on later runs of the pipeline. Here too we retained the principles of diplomatic transcription: if the word was abbreviated the way that it could be expanded in a canonical orthography, we did so, otherwise, we tried to follow the intended (or perhaps mistaken) spelling of the scribe.  Given full collations of the chronological sections within the text, our editorial analysis could begin. We have used the Stemmaweb tool  (Andrews and Macé, 2013) both to normalise and to specify classes of relationship between individual readings throughout the text, which in turn eases the stemmatic analysis of the manuscripts. Recent versions of Stemmaweb also provide a means of indicating the ‘lemma’ reading among a set of variants, so that critical edition text can be produced directly. At this time a system for annotation of the textual content is under development, which will enable us to provide a digital commentary on the historical content of the  Chronicle, as well as the edition itself.  Although our project has not extended the CI model past the stage of collation – this would require a system to save and “re-play” editorial decisions concerning the collated text, which would have required more of an engineering effort than we had resources for within the framework of the project – we consider this to be an important direction for producing a critical edition that is truly reproducible from the textual evidence at its base.  ",
        "article_title": "Continuous Integration Systems for Critical Edition: The Chronicle of Matthew of Edessa",
        "authors": [
            {
                "given": "Tara Lee",
                "family": "Andrews",
                "affiliation": [
                    {
                        "original_name": "Universität Wien, Austria",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            },
            {
                "given": "Anahit",
                "family": "Safaryan",
                "affiliation": [
                    {
                        "original_name": "Universität Wien, Austria",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            },
            {
                "given": "Tatevik",
                "family": "Atayan",
                "affiliation": [
                    {
                        "original_name": "Universität Wien, Austria",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "software design and development",
            "scholarly editing",
            "English",
            "computer science and informatics",
            "near eastern studies",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Most antique and mediaeval codices preserved today present one or more layers of complexity. Codices are usually considered “complex” when they contain more than one text; but there are other aspects to be taken into consideration. Sometimes, they are complex objects because several scribes collaborated in copying several texts into one book, at times using different ink or layouts, or because their writing material was reused from pre-existing books. Other times, a manuscript’s owner might also restore damaged leaves or choose to add new texts on new quires. Readers too could contribute to an evolving complexity as they added comments, drawings or small pieces of content in the margins or in blank spots. The historical complexity of ancient codices is in itself also a complex notion (See further Gumbert, 2004; Maniaci, 2004; Andrist et al., 2013).   How can the various levels of complexity of a codex be coded into a database in such a way that the complexity is not only clearly understandable but also correctly searchable?    The aim of this paper is to present a language-independent model for achieving this goal, and some of the new challenges it now faces. It is called the syntactical model and is already in use in several MySQL as well as XML/TEI databases.   The first section of the paper sets out the problem by showing a few slides of a complex codex and explaining the problem one encounters in many current databases. Even though they record accurate information searches with multiple criteria often result in inaccurate answers. Let us for example consider a codex C, which contains both a text by Aristotle copied in the twelfth century, and a text by Chrysostom copied in the fifteenth; let us now imagine a scholar searching databases for witnesses of Chrysostom copied in the twelfth century; by far most of today’s databases will return codex C as a positive match, though it is not! Why is this so? Because the single pieces of information is unrelated to its context of production: these databases fail to consider the codex as an object evolving over time (see Andrist 2014).   The second section presents the principles of the syntactical model. The basic idea is that all the contents are linked to a more or less conscious act of production. Let us imagine that someone orders a new copy of Homer’s  Iliad ; many years later, someone adds some texts by Hesiod to this book; to these a subsequent reader adds the commentaries of Eustathius in the margins along with some notes of her or his own; later still someone takes out the pages containing the Hesiodic texts, binds them together with Pindar’s  Odes  and adds a colophon… every stage of the transformation represents a unique act of production. The Syntactical model holds that each content in a printed or electronic description of a codex ought to be clearly and unambiguously related to its production unit (the theoretical underlying principles are explained in Andrist et al., 2013).   The third section explains how this model has been implemented until now. A standard description according to the syntactical model operates on three data levels: the data level related to the codex as it is today; the data level related to its constitutive production units i.e. its historical parts (which are the main “description units”); and the data level of the pieces of content (mostly texts, but also images or musical pieces), always situated within a production unit. I will show how the syntactical model is implemented in our MySQL shared database (in the framework of the ERC project ParaTexBib submitted by Martin Wallraff in 2013; see Wallraff et al., 2015) and I will also show an example from the XML/TEI Beta maṣāḥǝft  database of Ethiopian manuscripts (unrelated to our project).    This very structured and hierarchical way to describe manuscripts enabled us to develop a new tool, with the specific purpose of both visually relaying the overall structure of a codex, as well as the content of each stratum; elements which may not be immediately evident in a written description. It generates graphic representations on demand from a manuscript description based on the syntactical model (an image of it is available   here  ). The whole codex, as well as its historical parts and their contents, is represented on a single screen; extra details are displayed by clicking on individual elements of this graphic representation. These are created by an open-access web application, developed through a proof-of-concept funded by the ERC (see Dirkse et al., 2019).   A promising future development of this tool is the reconstruction of now dispersed manuscripts; by drawing elements from existing graphic representations, the users will be able to create a new representation by joining diverse parts of various manuscripts and arranging them in order.    The final part of the paper mentions some of the main current challenges:    the main one is a human one, that is to say, when people are unfamiliar with or misunderstand the historical layers of a codex and consequently do not represent them correctly in the database;     from a more technical point of view, the hierarchical way in which the syntactical model is currently implemented in the databases results in very static descriptions and poses certain problems. For example, when the production units of a codex are not easy to identify, or when one wants to simultaneously visualise all the pieces of content currently on one page even though they were produced at different stages. Moreover, other structure levels of the codex within or above the production units are not easy to represent. This is why we are currently thinking of other ways to implement the syntactical model in the databases, which would allow us to visualise the codex according to various interpretation of the syntactical model or, when needed, according to other models (see Andrist, 2015).    Mapping the major layers of complexity of ancient codices in databases is possible with the syntactical model, even though there is a lot room for improvement. We hope that in the future more projects will take advantage of the potential of this model.  ",
        "article_title": "Mapping the Complexity of Ancient Codices in Databases: the Syntactical Model",
        "authors": [
            {
                "given": "Patrick",
                "family": "Andrist",
                "affiliation": [
                    {
                        "original_name": "Ludwig-Maximilians-Universität, Munich, Germany; Université de Fribourg, Switzerland",
                        "normalized_name": "Ludwig-Maximilians-Universität München",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05591te55",
                            "GRID": "grid.5252.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "manuscripts description and representation",
            "English",
            "library & information science",
            "medieval studies",
            "databases & dbms",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The paper will present the results of the analysis and the modelling of the reading process within the Italian publishing house Einaudi (Mangoni, 1999; Turi, 1990) in the aftermath of World War II. The research focuses on the activity of one of the most important contemporary Italian publishers and, ultimately, on the dissemination of Italian literature of the last century. This is part of a larger project, which focuses on narrative works and on Natalia Ginzburg (Iannuzzi, 2012; Saita, 2009), one of the leading collaborators of the publishing house and one of the most original writers of the period.  Reading processes were carried out within the publishing house or thanks to external consultants, and had the purpose of verifying the economic and literary viability of a work; this activity resulted in the production of reading reports, containing a summary and an evaluation of the proposed work. Through these documents, mostly unpublished (a small selection is included in Munari, 2015), the books to be printed were selected. Reading reports embodied therefore cultural models that need to be interpreted according to a specific historical and cultural context. The presentation will highlight the fundamental contribution of tools and methods of Digital Humanities in the context of my research. More specifically, I will discuss data modelling and encoding as speculative activities of epistemological value (Cummings, 2008; Ciula and Eide 2014; Flanders and Jannidis, 2018). In fact, by modelling the reading process, I was able to understand and represent its complexity; in addition, by encoding the documents I was able to focus in a consistent and speculative matter on aspects such as textuality, style, rhetoric, highlighting differences of the communicative process. The potential of the DH tools and methods have yet to be exploited and applied to this research field: at present there are no digital editions of reading reports. This lack of direct reference models is one of the most stimulating challenges of my project. The model was created thanks to a class diagram developed in UML (fig.1). This allowed me, on the one hand, to organize a heterogeneous corpus, and on the other hand it provided the basis for the conceptualization and analysis activities necessary to the creation of a relational database. The database allows to efficiently account for sources, gathering the fundamental information for the description of the corpus in a structured way. It currently contains metadata of a sample of documents, which were then transcribed and encoded according to the TEI XML schema. The metadata in the database and the encoded texts form the basis of the work that will follow: the creation of a digital edition of the documents themselves. To describe the UML scheme (fig. 1), I focus my attention first on the document ( documento), which is stored in a particular archive ( archivio). UML calls their entities classes, where each class can have attributes that describe a series of their characteristics: the class  documento, for example, contains the “date”, “length”, “type of writing” and “form” fields. Each  documento conveys what I have called an editorial text ( testo editoriale). It was fundamental to make the distinction between documentary unit and textual unit, considering also the fact that the same editorial text can be transmitted by several documents: often, in fact, many copies were made of the same reading report, in order to reach different collaborators who were called upon to express their opinion within a collective decision-making process.   Editorial reports ( parere editoriale) are transmitted by different types of editorial texts, among them, of particular interest are minutes ( verbale) of editorial meetings (Einaudi, 2001; Munari, 2011) which report upon the opinions and the sometimes heated exchanges between the various collaborators; these documents allows us to reconstruct the collegial decision-making process, although this reconstruction is, of course, only partial, since it summaries on a much denser oral communication. Other textual typologies that I have identified are:  • Editorial department decisions in which the opinions of the individual collaborators were reported indirectly by the document's compiler ( comunicazione di segreteria/redazione).  • Structured reading reports ( scheda).  • Letters by the individual collaborators, which are the emblem of the dialogic and collegial character of the decision-making process within the publishing house ( lettera).  I distinguish between editorial texts and the editorial reports because a single editorial report doesn’t occupy the entirety of any given editorial texts (except in the rare cases of structured reports), and sometimes, many editorial reports are contained by a single editorial text. The editorial reader ( lettore) had different natures (external or internal collaborators; if internal: collection directors, editorial director, and so on). It is not always easy to identify the reviewer: the documents can be signed or unsigned, sometimes containing an identifier of a given collaborator, but when completely anonymous, an analysis of the handwriting is required (if possible). It is important to underline that, as shown in the diagram, the editorial reader who expresses an editorial opinion may not coincide with the compiler of the document containing it. This is evident in the case of the minutes, where the writer is often a stenographer.   The model I have produced is essential for the re-organization of the corpus and for the rationalization of the reading process. This model is a preparatory work for the study of the corpus itself, which ultimate aim is the valorization of a literary micro-genre (Bricchi, 2010) that deserves to be studied through appropriate tools. The model itself will allow to understand the reasons behind the decisions and the relative weight of any given collaborator, providing an insight on how the Italian literary canon of the aftermath of World War II was built.    FIG. 1 ",
        "article_title": "Modelling The Editorial Reading Process: The Case Of Giulio Einaudi Editore",
        "authors": [
            {
                "given": "Laura",
                "family": "Antonietti",
                "affiliation": [
                    {
                        "original_name": "Université Grenoble Alpes, France",
                        "normalized_name": "Grenoble Alpes University",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/02rx3b187",
                            "GRID": "grid.450307.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "bibliographic methods / textual studies",
            "theory and methodology)",
            "English",
            "text encoding and markup languages",
            "italian studies",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper aims to present the READ-IT project and the first set of case studies collected by DH and HSS researchers. Case studies in reading are one of the first deliverables and results of a joint construction between DH, ICT, LIS and HSS scholars. Case studies occupy a central place in the definition of READ-IT data model and tools, guiding the identification on common issues, dimension of analysis and sources for validating and testing both the conceptual framework and the database. The case studies include different sources, such as social media, students’ diaries and letters, from the 18th up to today, in Czech, French, German, Italian and Dutch. The importance of books and reading is unquestionable in modern society, but at the same time there are still unaddressed questions. Up to now, we can study the circulation of books and therefore the idea they convey, we can identify the factors that may facilitate or impending the reception of such ideas in different cultural groups, but we still cannot comprehend the impact of reading in our history and in our society. Regarding history of reading practices, knowledge has significantly increased over the last decades about what, where and when people read (Murray, 2018). Nevertheless, two major questions remain unanswered: why and how do people read? We still lack a systematic approach and tools to study the experience of reading, what are the effects on readers and their lives, what are the outcomes of reading and what is affecting the reading experience of the general public. The two main research questions indicated above can also be decomposed in a series of subordinated questions: a. What kind of transaction exists between a reader and a text? b. What role does the environment play in this transaction? c. Is it possible to list and model the emotions caused by reading? d. Have these emotions changed throughout time and space in Europe? e. Is it possible to sketch out the portrait of something like the “European reader”? Through a unique large-scale, user-friendly, open access, semantically-enriched investigation tool to identify and share groundbreaking evidence about 18th-21st century Cultural Heritage of reading in Europe, READ-IT (Reading Europe Advanced Data Investigation Tool) project wants to address these questions. It is a 3-year (2018-2020) transnational, interdisciplinary Research and Development project funded by the Joint Programming Initiative for Cultural Heritage. READ-IT consists of a robust consortium of 5 academic partners from 4 European countries (Institute of Czech Literature, Academy of Sciences, Prague; Open University, London, UK including the SME IN2; Utrecht University-DH Lab, Netherlands; CNRS-IRISA, Rennes and Le Mans Université-3LAM, France). The interdisciplinary collaboration between digital humanists, human and social sciences scholars and computer scientists investigates innovative ways of gathering new resources through crowdsourcing and web-crawling as well as linking and reusing pre-existing datasets. READ-IT thus aims to ensure the sustainable and reusable aggregation of qualitative data allowing an in-depth analysis of the Cultural Heritage of reading. The corpus encompassed is a rich “human archive” in multiple media and languages depicting a transaction between reading subjects and reading material from the 18th century up today, to web scraping and social media crowdsourced evidence of reading experiences. With regard to the work plan of READ-IT, the collection of use cases is considered to be the first significant milestone. Use cases collected in READ-IT are challenging the previous approaches adopted in projects such as UK-Reading Experience Database (2006 to date), the ANR-funded Reading in Europe: Contemporary Issues in Historical and Comparative Perspectives project (2014-2017) and the Listening Experience Database project (2012 to present) by going beyond the current state of the art of use cases and by requiring a much deeper analysis of sources.  Specifically, in READ-IT we collected the following seven use cases: 1.  Studying Contemporary Digital Reading Experiences Through Social Media. How can we document contemporary digital reading experiences? Are there any “new” ways of describing reading experiences out there? (Kovač and van der Weel 2018) Can READ-IT help us preserve a collection of present-day heritage for the future? How can we find and collect data about such digital experiences?   2.  Self-reflection. Which readers (in terms of cognitive profiles and/or sociocultural variables) tend to report self-referencing as part of their reading experiences? Which texts prompt self-referencing more than others? (Kuzmičová 2014)  3.  The places where we read. What text genres are read in what physical environments (and on which devices)? What physical environments are conducive to quality reading experiences as perceived/valued by readers? (Kuzmičová 2016)  4.  Four use cases examining   a. the semi-automated extraction of evidence of reading from challenging sources written in inflected languages (Czech, German, Italian, Russian) that have undergone significant historical change, using evidence in non-Latin alphabets, some in manuscript form. 5. whether such extracted evidence can be interpreted by historians of reading both at scale and in detail (Gibbs and Cohen 2011, Towheed et al. 2015) to address questions such as the influence of the state and of censorship, the development of reading in educational contexts, the emergence of a European identity in 19th-century readers.  A preliminary analysis of use cases highlighted a challenging set of requirements to be considered in the modelling of READ-IT data model, such as the ability to study the embodiment of the reader in a physical context, the ability to identify the relation between reading and the personal life of the reader, the patterns emerging from the relation between the content, the type of sources and the socio-political context. This examination of a variety of case study facilitates a dialogue between the ICT and DH scholars who create the underlying data model and the HSS researchers who adopt it (Flanders 2013). This rigorous critique supports the development of a conceptual framework of reading, enables greater interoperability of data sources across different use cases, and allows scholars to address both macroscope and microscope questions (Hitchcock 2014) in terms of geographical scale, time frame and types of sources which are at the core of READ-IT vision.  ",
        "article_title": "Reading in Europe - Challenge and Case Studies of READ-IT",
        "authors": [
            {
                "given": "François",
                "family": "Vignale",
                "affiliation": [
                    {
                        "original_name": "Le Mans Université",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Francesca",
                "family": "Benatti",
                "affiliation": [
                    {
                        "original_name": "The Open University",
                        "normalized_name": "Universidade Aberta",
                        "country": "Portugal",
                        "identifiers": {
                            "ror": "https://ror.org/02rv3w387",
                            "GRID": "grid.26693.38"
                        }
                    }
                ]
            },
            {
                "given": "Alessio",
                "family": "Antonini",
                "affiliation": [
                    {
                        "original_name": "The Open University",
                        "normalized_name": "Universidade Aberta",
                        "country": "Portugal",
                        "identifiers": {
                            "ror": "https://ror.org/02rv3w387",
                            "GRID": "grid.26693.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper outlines an interdisciplinary undergraduate digital humanities course, study abroad trip to Rome, Italy, and consultation with representatives from the non-profit organization Shoot 4 Change (  http://www.shoot4change.eu/  ) focused on teaching students to visualize space critically. Utilizing the Spatial Humanities Kit (  http://spatialhumanitieskit.org  ), deployed via both Molloy College and Hofstra University, we showcase narrative geospatial humanities work, media production, and a simple mix of HTML and GeoJSON as vehicles for our students’ critical analysis.   Our maps prioritize student experience, encapsulated in still images, written description, VR video, and vlogs embedded within them. Our course prioritizes methods for researching and unearthing embattled histories of public space, particularly within architecture, monuments, and urban design. Combined, our maps and critical framework result in a practice of teaching students to visualize cultural conflict that prefigures their experience of the space they inhabit--what is formative of, but currently absent or obscured from, the landscape they engage with.  Context & Background  What does it mean to map something that is no longer present? Inversely, what does it mean to visualize something that has been co-opted and ideologically obscured? These questions guided our course and directed our GIS project. Historical absence, charted through existing architecture, is perhaps easier to visualize. For instance, by researching and visiting the   Domus Palazzo Valentini  , a palace that showaces its ancient history through digital projection and glass floors that reveal earlier structures, students saw how architecture layers over time. What one sees in the present may physically elide what once stood in the exact location in the past. Politically speaking, ideological obfuscation, especially for American students abroad, is often more difficult to understand and visualize. Students were asked to consider contemporary political ideologies that co-opt Roman culture and redeploy it. In August, 2017, for example, famed historian Mary Beard characterized a BBC cartoon depicting a high-ranking black soldier as emblematic of a typical Roman family. As noted by  The Telegraph , Beard argued that the “cartoon was ‘indeed pretty accurate,’” and, “there’s plenty of firm evidence for ethnic diversity in Roman Britain.” What should have been an innocuous observation by an influential historian gave rise to a fierce attack on Beard’s credentials and character by the alt-right. This incident, among others, revealed a trend in alt-right thought now documented through a growing set of popular literature: the alt-right, particularly in the US, consistently appropriates Roman monuments, culture, and history in its effort to garner support for its ethno-nationalist agenda.    This fact reveals itself over and again in the alt-right’s digital footprint. Memes, video, and forum presence form a battleground for manifold acts of alt-right appropriation. Equally as compelling, however, are growing attempts to map and visualize alt-right violence, cultural appropriation, and concomitant modes of exclusion, both historical and contemporary. Several activist map making movements currently document where alt-right organizations congregate as well as the violence they commit. Italian ANTIFA groups have mapped hate crimes perpetrated by neo-nazi movements across the country (  https://bit.ly/2ziNtQ4  ), and similar projects called “Fashmaps” have taken shape in the US (  https://bit.ly/2EL0vYQ  ). Finally, Shoot 4 Change, a European non-profit focused on “humanitarian reportage,” has documented the formation and removal of refugee camps in Southern Italy, a direct result of the country’s closed border policies (https://bit.ly/2DrLA7S).  Parallel to these events, critical GIS work in the humanities, especially geospatial work in DH, has been mobilized to document both present and past inequalities predicated on race and ethnicity. Most recently, the Torn Apart / Separados team visualized the effects of the US’s 2018 “zero tolerance” policy for asylum seekers. DH scholar-practitioners at the University of Iowa have visualized histories of segregation in the US via “Placing Segregation,” and DH scholar-practitioners at the University of Richmond have visualized events leading to slavery’s end during the American Civil War via “Visualizing Emancipation.” In so many words, DH utilizes digital maps to visualize violence, appropriation, and exclusion so as to speak back to dominant histories of national formation and containment.   Taking inspiration from these projects, this paper showcases approachable entry points to humanities GIS work for undergraduates and mobilizes them toward the critical visualization of space. We are specifically interested in the act of locating absence via digital mapping technologies--researching, exposing, and visualizing what has been obscured over time and excluded by nationalist formations in public space, both physical and imagined.  Our paper offers multiple ways in which spatial humanities methods can be made accessible to a layperson audience via interactive and accessible media production tools that document a present space and time as it relates to an array of social, political, and cultural issues. Our focus on pedagogy is primary, and to draw a stark divide between our critical approach and the pedagogical methods we deploy would be anathema to our project-based work.  Map & Production Methods  Our student map of Rome is an outgrowth of prior international trips and deployments of the Spatial Humanities Kit. It demonstrates competence in three areas: basic knowledge of HTML and GeoJSON, media production, and critical visualization.  For ease, we refer to each theme by its essential function in the kit’s deployment: site, narrative, and knowledge produced.    Site: Student training and historical work began months prior to our departure for Rome. Students were introduced to historical texts, maps, and virtual exhibits of the locations we would eventually visit. Concomitantly, students were introduced to contemporary political movements, both US-based and Italian-based, that prioritize appropriation and exclusion toward nationalist goals. On location in Rome, students were prompted by push notifications sent to their phones to reflect on absence and exclusion in one of three areas, depending on the site they inhabited: migration, labor, and infrastructure. Students visited tourist sites, famous monuments, as well as lesser known locations tied to contemporary political turmoil to reflect, gather data, and ultimately visualize absence and exclusion.    Narrative:  Our focus on media production borrows methods from Jason Farman’s  Mobile Storytelling  and non-profit Shoot 4 Change. On the one hand, students were asked to incorporate two methods into their documentation process. First, following Farman’s method for mobile storytelling, students were asked to account for multiple aspects of the locations they documented,  “including its histories, cultural conflicts, communities, and architectures (to name only a few) and makes these aspects foundational for the experience of the space” (Farman 3).  On the other hand, students consulted with Shoot 4 Change to gain awareness around  the sensitivities and necessities of digital storytelling in a global context. In addition to still images taken by students of their experiences, students created vlogs and VR video to capture the times and spaces of their experience, guided by our three areas of reflection above.   Knowledge: Our final area of focus, critical visualization, is an outgrowth of combining site-specific media assignments in Rome with digital mapping technologies.  Our map’s construction combines simple methods for data collection and an introduction to basic coding. Prior to our departure, students learned best practices for organizing and archiving content on three platforms: Google Drive, Omeka, and cPanel. Students were also introduced to video production and editing technologies, while more advanced students focused on the production of VR video. Combined with their narrative training, our students produced a map that both accounts for their own situatedness as American students abroad as well as Rome’s multilayered history. Students draw out Rome’s diverse cultural history in particular as a means of visualizing what is excluded contemporarily by those that would co-opt it for authoritarian gain.   Outcomes As part of our presentation, we will make our syllabi, methods, and maps available to our audience and discuss the importance of digital mapping projects that prioritize criticality. Our work underscores  how historical research, media production, and simple mapping techniques lead undergraduates to view public space as a multilayered and ideologically driven. We demonstrate how digital map making can create critical awareness of exclusion and ideological obfuscation as authoritarian power takes shape.   ",
        "article_title": "Locating Absence with Narrative Digital Maps",
        "authors": [
            {
                "given": "Matt",
                "family": "Applegate",
                "affiliation": [
                    {
                        "original_name": "Molloy College, United States of America",
                        "normalized_name": "Molloy College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0277z0p27",
                            "GRID": "grid.419950.0"
                        }
                    }
                ]
            },
            {
                "given": "Sarah",
                "family": "Evans",
                "affiliation": [
                    {
                        "original_name": "Molloy College, United States of America",
                        "normalized_name": "Molloy College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0277z0p27",
                            "GRID": "grid.419950.0"
                        }
                    }
                ]
            },
            {
                "given": "Jamie",
                "family": "Cohen",
                "affiliation": [
                    {
                        "original_name": "Molloy College, United States of America",
                        "normalized_name": "Molloy College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0277z0p27",
                            "GRID": "grid.419950.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital activism and hacker cultures",
            "multimedia",
            "modeling and visualization",
            "authorship attribution / authority",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "cultural studies",
            "audio",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Among all the diverse typologies of administrative systems, the fiscal-cadastral sources retracing the ownership of lands are undoubtedly the richest and, in a sense, the most coherent records. The need for efficient tax collection processes pushed different administrations and political regimes in the 18th century to develop scale-invariant coherent property tracing system. These systems were progressively generalised at the turn of the 19th century with the abolition of privileges for certain classes of citizens and the establishment of republican administrations for managing cities and countries (Kain and Baigent 1993). The cadastre, by introducing a constant collection of taxes calculated on a fixed percentage, defines a new conception of the city and of the urban space. It marks the transition from the Ancient Regimes to the Modern State. The introduction of a constant, proportional and impartial tax determined a new conception of the city, adapted to statistical computations. In some sense, for the first time, the cadastre transforms the city into a computational object. We use the neologism “cadastral computing” to refer to the operations performed on such primary sources. This article presents a generic approach for processing automatically the information contained in the Napoleonic cadastres. The cadastres established during the first years of the 19th century cover a large part of Europe. For many cities they give one of the first geometrical surveys, linking precise parcels with identification numbers. These identification numbers point to register lines with the names of the parcel’s owners (Fig. 1). As the Napoleonic cadastres include millions of parcels, it therefore offers a detailed snapshot of large part of Europe’s population at the beginning of the 19th century (Clergeot 2007).      Figure 1. Identification numbers in parcels and in the register  To develop a generic approach adapted to the processing of these administrative documents, one needs to solve two difficult challenges: (1) developing algorithms capable of robustly segmenting maps into parcels and administrative tables into cells (2) developing solutions for transcribing handwritten text containing people or places mentions and identification numbers. Until recently, these two problems were considered much beyond the state-of-the-art. The results of this article are based on the important progress recently made on both issues, using deep learning architectures. In 2017, an initial study on cadastre extraction showed promising results in parcel extraction and identifier recognition. However, it was entirely designed as an ad hoc pipeline fine-tuned for a particular cadastre (Ares Oliveira, Lenardo, and Kaplan 2017). A year later, a generic deep-learning segmentation engine, relying on a convolutional neural network (Ares Oliveira, Seguin, and Kaplan 2018) demonstrated that it was possible to design a generic architecture to segment many typologies of documents. In this article we use the genericity of this approach to develop the first fully automatic pipeline to transform the Napoleonic Cadastres into an information system.   Method The automatic processing of the cadastre maps aims at extracting the parcels as geometrical shapes and also at transcribing the parcel’s identification numbers. It is composed of three main steps: 1. Training of the deep neural network on manually annotated data 2. Segmentation of the maps into meaningful regions and objects  3. Transcription of the identification numbers  Our segmentation network is a fully convolutional neural network inspired by the U-Net architecture (Ronneberger, Fischer, and Brox 2015). It uses a ResNet-50 pretrained network (He et al. 2016) as encoder, which speeds up the training, reduces the amount of training data needed and helps generalization. The full details of the architecture and open-source implementation can be found in (Ares Oliveira, Seguin, and Kaplan 2018). The network is trained to extract the parcel contours and text using annotated data from the Venetian cadastre (Fig. 2). The training data corresponds to roughly 1/3 of one map sheet among the 26 maps of the city of Venice (roughly 800 parcels).   Figure 2. Sample of training data for cadastre maps segmentation. Parcels contour are in red, text is in green.  The transcription network is a convolutional recurrent neural network (CRNN) (Shi, Bai, and Yao 2017) which produces a chain of characters when given an image segment containing text. The network is trained on samples of numbers from the Venetian archives and on numbers synthetically generated with MNIST digits (LeCun, Cortes, and Burges 1998) (Fig. 3).   Figure 3. Example of training data for the transcription system. Left: synthetically generated numbers; right: numbers from the Venetian archives.  The segmentation model obtained after the training is able to predict the parcel contours and text region at pixel level (Fig. 4). Watershed by flooding algorithm (Beucher 1979) is applied on parcel contours predictions, which allows the extraction of parcel objects as polygonal shapes. Text regions are cropped, horizontally aligned and converted into grayscale image segments for further processing by the transcription system.   Figure 4. Output of the segmentation network (overlay in purple). Left: text extraction; right: contour extraction.  The image segments containing text are fed to the transcription network, which outputs a prediction of a number, i.e. a chain of digits. Each transcription is then linked to its corresponding parcel. The contours of the parcels (whether they contain an identifier number or not) are saved as polygonal shapes and are exported into JSON format. In our case, since the images have previously been georeferenced, the coordinates are exported as geographical coordinates and can therefore directly be imported in any geographic information system.    Results Two evaluations are performed in order to assess the performances of the system: the geographical accuracy of the extracted parcels and the transcription of the identification numbers.   Figure 5. Visualization of the results of the automatic extraction of parcels. The red rectangle indicates the parcels used as training data for the segmentation network.   Geometrical evaluation The number of geometrical shapes extracted and manually annotated are listed in Tab. 1. After a filtering step, which keeps only shapes which area range from 2 m 2 to 15000 m 2, the total number of extracted parcels is 28711, among which 18138 contain a transcription.   Table 1. Total number of geometries in the automatic extraction and manual annotation. The first three rows relate to automatically extracted parcels, the two last rows show the statistics for manually annotated parcels.  Geometries Number   Geometries extracted automatically 31 342   Geometries remaining after filtering 28 711    ---- with ID number 18 138   Manually annotated geometries 16 946   ---- with ID number 15 634   The quality of the parcel’s extraction is evaluated by measuring the intersection over union (IoU) between the geometries produced automatically and almost 17000 manually annotated shapes. Precision and recall with three different IoU thresholds 0.5 (acceptable), 0.7 (good), 0.9 (excellent) are reported in Tab. 2. The recall value shows that a large majority of parcels are extracted (85% in the most strict case). The low precision value is mainly due to the incorrect extraction of streets, squares, canals, etc. that are currently not filtered out (example in Fig. 6).   Figure 6. Example of false extraction of streets and canals (in blue)   Table 2. Evaluation of the geometrical shape extraction with different Intersection over Union (IoU) thresholds.  IoU Correct parcels Precision Recall   t=0.5 15 999 0.557 0.944   t=0.7 15 292 0.533 0.902   t=0.8 14 440 0.503 0.852     Transcription evaluation We assess the performance of the transcription of parcel’s identifier numbers by computing the number of correct predictions and report the precision and recall values in Tab. 3. The current method assumes that the identifiers are located within the parcel, thus, identifiers partially or completely outside the geometrical shape are not correctly transcribed (Fig. 7), resulting in a lower recall.   Figure 7. Examples of identifiers numbers outside or partially outside the parcel.  In order to increase the precision value and since we can assume that spatially close parcels will have numerically close identifiers, we tried to discard false predictions by determining if a transcription was ‘plausible‘ or not, using information from its spatial neighbourhood. Thus, a transcription is considered as an outlier if the (numerical) difference between the predicted number and the median of its 5 neighbouring transcriptions is greater than 10. This results in a significant increase in precision (up to 93%), but at the expense of a decrease in recall.  Table 3. Evaluation of the transcriptions of parcel’s identifiers numbers   Correct transcriptions Precision Recall   Transcriptions 11101 0.612 0.710   Transcriptions after outlier detection 8070 0.927 0.516      Perspectives This initial study demonstrates on a particular case that 90% of the urban geometrical structure and more than 50% of a city population can be automatically remapped with high precision using only generic pipelines. Even if these numbers need to be confirmed on the basis of other case studies, the genericity of the methods used makes us optimistic about the possibility of conducting a large-scale study in the coming years. Such datasets call for a confrontation with the large number of historical hypotheses that have been formulated on the urban society at the beginning of the 19th century based on much smaller sets of evidence. Thanks to the standardisation processes of the Napoleonic administration, we hope in the coming months to extend this systematic processing beyond Venice to a large part of Europe.  ",
        "article_title": "A deep learning approach to Cadastral Computing",
        "authors": [
            {
                "given": "Sofia",
                "family": "Ares Oliveira",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Isabella",
                "family": "di Lenardo",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bastien",
                "family": "Tourenc",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frederic",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "artificial intelligence and machine learning",
            "English",
            "computer science and informatics",
            "OCR and hand-written recognition",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The field of digital humanities has expanded rapidly over the past decade, with new centres, associations and activities proliferating worldwide, including in Australia and New Zealand. Digital research in the humanities is linking scholars and practitioners across Australasia, Asia-Pacific and globally. In Australia, government policies for research funding and infrastructure, including most recently the  2016 National Research Infrastructure Roadmap, have acknowledged the importance of digital humanities as a collaborative framework that connects the arts and sciences and is bringing together researchers and information technology experts with collecting institutions and the public to address complex social and cultural challenges in new ways.   This paper traces the development of digital humanities in Australia, with reference to major projects and events leading to the founding of the Australasian Association for Digital Humanities (aaDH). It discusses national exemplar projects, as well as significant activities and initiatives that formed a basis for the Australian field. It outlines the history of the establishment of aaDH as a regional association, reflecting on its directions over the past decade, and describes the parallel development of large-scale infrastructure that has supported the field’s further growth. Looking back, it becomes clear that many projects and activities were pointing in the same direction without necessarily being linked directly, but as they progressed and connected with each other, the dispersed and disparate work across disciplines, institutions and sectors began to cohere with a common purpose—and from this confluence the Australasian Association for Digital Humanities (aaDH) was formed. A range of large-scale digital projects and events provided the key context and impetus for the development of the digital humanities field in Australia. Projects discussed in this paper include those of the Centre for Literary and Linguistic Computing at the University of Newcastle; the Archaeological Computing Laboratory (later Arts eResearch) at the University of Sydney; the eScholarship Research Centre at the University of Melbourne (and its predecessors); the Consortium for Research and Information Outreach (later Centre for Digital Humanities) at the Australian National University;  AustLit;  AusStage;  Design and Art Australia Online;  Paradisec; the  Australian Dictionary of Biography; and the  Trove digitisation and aggregation project of the National Library of Australia. As is the case in many parts of the world, such institutional activities tend to be associated with particular research centres or groups, and are typically collaborative enterprises. There were many centres, teams, projects and individuals that played a part in the long-term development of digital humanities in Australia prior to the establishment of the Australasian Association for Digital Humanities. The intention is to provide a representative sample.   The paper highlights the role of foundational projects and events for the development of the digital humanities field in Australia, and in this context, it traces activities of the Australasian Association for Digital Humanities as a peak body representing the interests of researchers and connecting them with an international community. aaDH has been active for almost a decade and is now expanding its engagement with the next generation of scholar-practitioners in universities and the GLAM (Galleries, Libraries, Archives and Museums) sector in Australasia. Meanwhile, since the association’s establishment, the field has expanded exponentially to become a major mode of connection between research communities operating in very different national contexts, serving as a means of global linkage, promoting and opening up possibilities for a higher degree of international collaboration. In Australia, as in other parts of the world, libraries have played a pivotal role in supporting the evolution of digital humanities as a field and set of activities that spans academia and GLAM. National and state digitisation programs led by collecting institutions are increasing researcher and public access to a wealth of otherwise hidden resources. Institutional projects have helped to determine priorities and interests for the field as a whole and to foster communities of practice. In historical and disciplinary terms, many can be linked directly back to the humanities computing tradition that predated and provided the foundation for digital humanities as it is now known. Leading projects have facilitated cooperation across geographical, cultural and disciplinary boundaries and integrated data in ways that enhance capacity, scale and accessibility. Through these characteristics they have been able to stimulate, generate and support new kinds of research activity. Smaller-scale projects can be equally innovative, and in the Australian context they have also contributed to the expansion of the digital humanities field over time. However, the aim here is to highlight a selection of the most visible and active projects that can be regarded as iconic exemplars. They are large-scale collaborative initiatives that have been influential in the field, and through their high profile and long reach, they have played a significant role in raising awareness of new digital approaches and possibilities.  Linking the examples discussed in this paper is a common theme: they all have the goal of capturing, preserving, building upon and articulating the richness of Australian culture and history for current and future generations. Digital humanities points to a future in which researchers will be able to utilize comprehensive global data from many different sources and countries, and across languages, to form diverse and inclusive infrastructures and methods for data sharing and analysis, to further advance knowledge and understanding and to develop new skills in the broad fields that the humanities and social sciences represent worldwide. ",
        "article_title": "Tracing the Development of Digital Humanities in Australia",
        "authors": [
            {
                "given": "Paul",
                "family": "Arthur",
                "affiliation": [
                    {
                        "original_name": "Edith Cowan University, Australia",
                        "normalized_name": "Edith Cowan University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/05jhnwe22",
                            "GRID": "grid.1038.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-19",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "digital ecologies",
            "libraries",
            "museums",
            "communication and media studies",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This short paper will give practical advice about the British Library’s digitisation planning process for scholars who wish to use digitised resources in their research. The information will help scholars understand the institutional context, the roles involved in digitisation, the preparation stages and documentation required, typical timelines and the decision-making that happens at different stages. With this knowledge it is hoped that Digital Humanities (DH) scholars will be better prepared for the process and will factor it in their research funding proposals. They will also gain an understanding of the Library’s considerations and policy for making available for reuse existing digitised resources and how scholars could request this for their projects. In making the policy and processes at the institution more transparent, the presentation will expose some of the hidden labour undertaken by cultural heritage staff to enable DH research, as well as the opportunities to use the research outputs for the enrichment of culture heritage collections. Institutional context The British Library has a mixed-economy approach to drive digitisation and works with external partners from across the commercial, academic and GLAM sectors. The Library’s collections and areas of expertise are wide-ranging and we have a significant experience of collaborating on research projects in the humanities and other disciplines. Collaborations with DH scholars contribute to the Library’s digitisation priorities to ensure a more complete digital offering and an enhanced research potential of the collections to the benefit of future users and audiences. A timely engagement with the Library means proposals are carefully reviewed by relevant staff and lead to mutually beneficial outcomes. Digitisation planning process New proposals for digitisation are assessed and approved by a group of stakeholders representing strategic and operational teams at the Library. The digitisation planning process involves significant information gathering through the coordination of a number of assessments by different teams to establish the suitability and readiness of a collection for digitisation. The data captured relates to the provenance and physical state of the collection, level of descriptive metadata, copyright status and sensitivity issues, evaluation of the imaging process and necessary preparation of the material, post-processing and output file requirements, resource capacity and scheduling, and overall costs. During the final stages of approval strategic considerations are made about the optimal approach for digitising the collection, its fit with the Library’s overall digital offering, the most appropriate partnership model, opportunities for the enrichment and innovative use of the data, and the value for the intended users groups. Legal compliance is an essential stage in the digitisation planning and approval of new proposals, and standard due diligence procedures are in place for undertaking copyright assessments and rights clearance, data protection and sensitivity checks, and the assignment of usage terms. Facilitating easy access and onward reuse of the digital collections is set out as a clear objective in the Library’s long-term vision and strategy, and where appropriate and permissible digital content is released under the most open licences. The paper will outline the Library’s policy with regards to access and reuse of its digital content, the process in place and the stakeholders group involved. Scholars will understand how decisions are made and will be able to apply this knowledge when planning for the use of the Library’s digitised resources in their research. Challenges and opportunities Digitisation at the British Library provides an excellent opportunity for staff to contribute to digital culture heritage studies. This paper will bring to light some of the research and innovation that underlines the digitisation activities and how this benefits scholars. Despite efforts to standardise the digitisation process, the range of formats and content types in the Library’s collections, the items dimensions, age and condition present challenges and require much original thinking and experimentation in order to create the best digital surrogates. Careful investigation and evaluation of the digitisation approach from the outset guarantees that digital assets will be of the highest quality and long-term value. Therefore, decisions made in the planning process about the preparation of material, use of equipment, image capture and properties, post-processing and presentation influence the outcomes of digitisation and would have an impact on how the digitised resources are used in research. With its vast and significant holdings the British Library plays a major role in supporting digital research in the humanities. Members from different teams across the Library, including the Digital Scholarship department, contribute to the digitisation process and enable the scholarly use of digitised resources. Whilst DH projects often help fund new digitisation and make the Library’s collections more accessible, they generate a deeper understanding of culture heritage and enable the enrichment and reuse of digital collections. In explaining the institutional context, the digitisation planning process and the involvement of different stakeholders, this short paper will help scholars appreciate the significant investment in digitisation by the Library and also the opportunity DH projects present for the Library to develop its collections. In this way research collaborations between culture heritage institutions and DH scholars will be strengthened, ensuring that outcomes are of greater mutual value and paving the way for future innovative use of digitised resources. Bibliography  British Library (2019). “Digital Scholarship.”  https://www.bl.uk/subjects/digital-scholarship (accessed 29 April 2019)   British Library (2018). “Heritage Made Digital.”  https://www.bl.uk/projects/heritage-made-digital (accessed 29 April 2019)   British Library (2019). “Proposing a new research collaboration with the British Library.”   https://www.bl.uk/help/proposing-a-new-research-collaboration-with-the-british-library (accessed 29 April 2019)  ",
        "article_title": "A National Library’s digitisation guide for Digital Humanists",
        "authors": [
            {
                "given": "Rossitza Ilieva",
                "family": "Atanassova",
                "affiliation": [
                    {
                        "original_name": "The British Library, United Kingdom",
                        "normalized_name": "British Library",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05dhe8b71",
                            "GRID": "grid.36212.34"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "GLAM: galleries",
            "archives",
            "cultural studies",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Norse Perception of the World project is creating an online, searchable index with mapping capabilities of foreign place names and other spatial references in medieval Swedish and Danish texts. East Norse (Old Swedish and Old Danish) literature is a mine of information on how foreign lands were visualised in the Middle Ages: What places were written about and where? Are some places more popular in certain text types or at certain times? How do place names link different texts? Is there a shared concept of spatiality? How is space gendered? Spatial humanities, the spatialisation of literary studies, and cognitive mapping are growing fields within digital humanities, but the study of spatial thinking and knowledge in medieval Scandinavia and its development as an area of enquiry are hampered by a dearth of information on place names in literary texts. Any research aiming to uncover what pre-modern Scandinavians understood about places abroad requires as a minimum an index of foreign place names in East Norse literature, an infrastructure that has not existed until now. The overall aim of the project is to create an infrastructure in the form of an online, open access searchable index and mapping of foreign place names found in medieval East Norse texts. This is accomplished by a bespoke back-end MySQL database containing the data (i.e. the index), Norse World (2018–2020), an interactive search and mapping resource based on the Leaflet library, and a REST-API, a separate back-end application that allows end-users to access the database. The REST-API uses JSON as its open standard format and is compatible with both GeoJSON and JSON-LD. The data are freely accessible to scholars and other interested parties around the world under a CC-BY 4.0 licence and can also be downloaded as a CSV-file, usable in for instance Microsoft Excel or offline GIS applications as QGIS. The data are extracted from a corpus of all Swedish and Danish literary, non-biblical medieval texts extant in a handful of medieval runic inscriptions as well as around 210 manuscripts, manuscript fragments, and early prints (from before 1515), such as romances, travel stories, pilgrim guides, saints’ lives, devotional literature, revelations, prayers, didactic works, and sermons as well as encyclopaedic works. Some of these texts exist in editions of varying quality while others are as yet unedited and housed in different archives and libraries in Scandinavia. The project is thus collecting data that are scattered and otherwise difficult to access, sorting and presenting them in a uniform manner. It also means that it is hard to estimate how many entries the finished database will hold. Outside of the East Scandinavian context several similar resources exist: Recogito (Pelagios Commons), World Historical Gazetteer (Grossner, 2019), and Icelandic Saga Map (Lethbridge, 2019). One of the challenges of the project is to represent the complexities of the data accurately from an onomastic and philological point of view. First of all, geo referencing itself can be an issue. If a text was translated in the late 14th century, is preserved in a manuscript from around 1450, and was read until the end of the medieval period (1515 for the project’s purposes), it is impossible to decide what borders for instance France would have had in the context of a reader’s reception of the text. For this reason, among others, we have chosen primarily to make use of a modern gazetteer, GeoNames (https://www.geonames.org/). Furthermore, just because Paris, for instance, is mentioned, that does not mean that the reader had any clear idea of where it was or how to get there, but its context would have impacted how it fitted into the reader’s world-view. A further complication is place name identification, both when a name is hard to interpret and when the same place name has different geographic meanings. The former can be studied by using the filter Level of certainty on the Norse World map. For the latter we create different locations by closely following the context of the text. If for instance a place, ’Paris,’ is called a lake, it will be distinguished from the city of Paris.  Secondly, the different forms of place names and non-name spatial references play a major role in structuring the data. The Original form, which is closest to the source in presentation, is transcribed according to diplomatic principles and also contains the immediate textual context. To facilitate orthographic, morphological, and onomastic investigations there are two normalised forms based on the Original form. The Variant from is slightly normalised, for instance, place names begin with a capital letter but spelling variation is preserved. The Lemma form is normalised further so that spelling variation is not preserved. A new Lemma form is created when there is a new place-name formation. The top level is Standard form, usually a modern English form of the place name or spatial reference. The structure, Original form > Variant form > Lemma form, runs parallel for Old Swedish and Old Danish, but the Standard form ties them together. It is the Standard form that has assigned coordinates from GeoNames. In some cases the Lemma forms might be completely different from the Standard form, e.g. ‘Bern’ and ‘Verona’ for the city of Verona. By having a Standard form the user does not have to be aware of all possible variations of a place name or non-name spatial reference to use the resource. To add further layers to the data every Original form is also linked to information on the work and its dating and description; the source, whether a runic inscription, manuscript, or early print, and its dating, material composition, and repository; edition used; and links to available electronic digitised sources or editions. Every Original form is thus at the core of a complex network linking it to other medieval texts and sources, modern digital resources and linguistic information ready to be used by researches and students as a starting point to study the world-view presented in East Norse texts. ",
        "article_title": "Norse World – The Complexities Of Spatiality In East Norse Medieval Texts",
        "authors": [
            {
                "given": "Agnieszka",
                "family": "Backman",
                "affiliation": [
                    {
                        "original_name": "Uppsala University, Sweden",
                        "normalized_name": "Uppsala University",
                        "country": "Sweden",
                        "identifiers": {
                            "ror": "https://ror.org/048a87296",
                            "GRID": "grid.8993.b"
                        }
                    }
                ]
            },
            {
                "given": "Alexandra",
                "family": "Petrulevich",
                "affiliation": [
                    {
                        "original_name": "Uppsala University, Sweden",
                        "normalized_name": "Uppsala University",
                        "country": "Sweden",
                        "identifiers": {
                            "ror": "https://ror.org/048a87296",
                            "GRID": "grid.8993.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-15",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "medieval studies",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 1959, C.P. Snow gave an influential lecture on the split between the two academic cultures – the sciences and the humanities (Snow, 1959). Snow deplored the growing divorce between these worlds and their inability to enter into a dialogue. Nonetheless, around the same period, collaborations between art, science and technology began to develop (Debatty and Grover, 2011) . Snow’s address, which was widely publicized and commented upon (Lee, 2004; Goodyear, 2004), shows that these collaborations cannot be taken for granted. From the 1960s to the present, collaborations between art, science, and technology flourished, leading to the establishment of dedicated cultural institutions and programs, university departments, and academic journals. Among them is  Leonardo (1968-present), published by MIT Press, which aims to be a forum for interaction, collaboration, and the sharing of ideas between artists, scientists, and engineers across the globe. The spirit of the journal was inspired by the multifaceted activities or Renaissance polymath Leonardo da Vinci, and by the modern utopian vision of bringing together the best minds of humankind.  Leonardo is the leading international peer-reviewed publication on the relationship between art, science and technology, making it an ideal dataset to analyze the emergence and dynamics of such complex collaborations.  This research studies how interactions between art, science, and technology in  Leonardo evolved between 1968 and 2018. We use the approximately 3,100 articles and around 7,800 illustrations in the 232 digitized issues of the journal and their metadata as our source data set. Each document is available as a PDF file with full-text searching and an accompanying XML file containing the document’s publication metadata.  While text analysis remains dominant, image and other media analysis is an emerging field that may offer unexpected insight, especially where the arts are concerned (Fyfe and Ge, 2018; Wevers and Smits, 2019). This short paper relies on metadata, text (articles, captions) and images (photographs, diagrams, sketches) to offer a fuller examination of the development and interactions networks between communities of researchers and artists in the journal. We applied multiple analytical approaches to the different materials in the dataset. For example, we analyzed illustrations, topical evolution and semantic transformations within the text, and cross-referenced respective results. We also studied the shifting dynamics of the author/institution network interactions collaborations over time. To study the visualizations, we isolated 7,836 illustrations from all issues of the journal using the PDF Figures 2.0 open-source package, which is based on PDFBox tool for parsing scholarly publications   . We examined the features of the images in order to determine which modes of representing (and actually doing) art, science or engineering were represented in the images. To this end, we used visual and conceptual clustering based on automatic feature extraction from the images. We ran the PixPlot tool (figure 1) on the images to perform the feature extraction and feature-based similarity clustering; PixPlot uses an  Inception3 Convolutional Neural Network, trained on the  ImageNet 2012      data set, and projects its similarity inference results into a two-dimensional manifold with the  UMAP algorithm      such that similar images appear proximate to one another     . The algorithm allowed us to identify 16 individual clusters that could be considered as lying along a visual genre continuum from “art” to “science,” including archetypes such as technical diagrams, portraits, installations, and sketches. Some clusters are strongly oriented towards “science” or “art,” while others exhibit a mixture of both genre polarities, such as one that weaves kinetic art displays together with more scientific optical imaging devices like radar readouts. Some clustering is representative of periods of time: for example, illustrations from the 1970s exhibit a strong association of technical computer science and engineering diagrams with the notion of “science.” Other features remain constant from the 1950s to the present: for example, portraits of the people involved in the projects, photographs of the installations, standard scientific figures. To understand the longitudinal evolution of our image corpus we will adapt some functionalities of the SIAMESE toolkit     , which visualizes changes in advertisements over time.  As a next step, we will tag the images according to their clusters to determine, as with a latent text topic modeling approach, the proportion of each “class” of images that is present in each paper. This information can subsequently be paired with the information generated through text mining methods. We will use both unsupervised (LDA, NMF) and supervised (Naive Bayes) clustering techniques to determine to what extent a text fell within the arts or science domain   SKlearn and Spacy were used for clustering and NLP. . Moreover, we will extract entities (locations, organizations, people, works of art) and other linguistic features from the text. Pairing the information gathered from the image and text analysis will allow us to answer questions like: in an article describing an art/science collaboration, is it customary to include representative “art” and “science” images, and if so, do these types appear in similar or dissimilar proportions? Also, we will be able to see whether specific actors or networks of actors were responsible for advancing particular visual styles.       Figure 1: Overview of the  Leonardo images dataset as visualized via PixPlot.  ",
        "article_title": " The Leonardo Code: Deciphering 50 Years of Artistic/Scientific Collaboration in the Texts and Images of Leonardo Journal, 1968-2018  ",
        "authors": [
            {
                "given": "Clarisse",
                "family": "Bardiot",
                "affiliation": [
                    {
                        "original_name": "Université Polytechnique Hauts-de-France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Peter",
                "family": "Broadwell",
                "affiliation": [
                    {
                        "original_name": "UCLA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Mila",
                "family": "Oiva",
                "affiliation": [
                    {
                        "original_name": "University of Turku",
                        "normalized_name": "University of Turku",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/05vghhr25",
                            "GRID": "grid.1374.1"
                        }
                    }
                ]
            },
            {
                "given": "Pablo",
                "family": "Suarez",
                "affiliation": [
                    {
                        "original_name": "UNAM",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Melvin",
                "family": "Wevers",
                "affiliation": [
                    {
                        "original_name": "KNAW Humanities Cluster",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "art history and design studies",
            "content analysis",
            "English",
            "cultural analytics",
            "cultural studies",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Drawing on recent experiences of the Letters 1916-1923 team in re-designing their project website, this paper will elaborate how changing user expectations, academic standards and special requirements of source material can be reconciled in the creation of database-driven interfaces designed for public humanities projects. Studies in the field of Human-Computer Interaction (HCI) and usability, which first flourished in the 1990s (cf. Laurel, 1993; Johnson, 1997), have been a major concern of the digital humanities for the past decade (e.g. Drucker, 2013). Digital Humanities scholars agree that interfaces are “part of the design” (Drucker, 2013) and need to visually tell the project’s story. But despite extensive theoretical discourse on “user-centered designs” (Gibbs and Owens, 2012; Boukhelifa, Giannisakis, Dimara, Willett, Fekete, 2015; Shneiderman and Plaisant, 2009), DH projects tend to fall short in practice.   A lack of long-term funding and the interdisciplinary nature of DH projects often result in the development of websites, databases or mobile applications that make many compromises and are no longer updated when user expectations and technical possibilities change. In their 2016 conference paper “Usability in Digital Humanities”, Natasa Bulatovic and her co-authors found “that more resources need be spent on testing of digital tools and infrastructure components and that it is especially important to conduct user tests covering the whole knowledge process” (Bulatovic, Gnadt, Romanello, Stiller, Thoden, 2016, abstract).   Jason Slipp (Slipp 2015) has analysed a broad range of digital editions, online archives and biographical websites, pointing out their failure to permanently engage their users. One of the projects he discussed was the William Blake archive, which won the 2003  Modern Languages Association Prize for a Distinguished Scholarly Edition and was last updated in May 2015. Slipp notices that the site offers “a vast amount of content, but [lacks] the aesthetic layout” to help users find and process this information without frustration. In Slipp’s opinion, a consistent site navigation, basic information about the objectives of the project on the landing page, and a video or other visualization explaining the usage of the site are key elements of a successful digital humanities project.    The Letters 1916-1923 project, first launched in 2013, also faced the challenge of re-designing its user interface in 2018 to engage a broad range of users from school children to professional researchers, both as end users of the project and as contributors. In contrast to some of the projects analysed by Jason Slipp, Letters 1916-1923 had to cope with a potentially unlimited, constantly expanding dataset whose metadata were not only added by members of staff but also by students and members of the public. Therefore, extensive data cleaning and careful data management was an inherent aspect of the website redesign, and compromises had to be made when presenting such complex sources.  The fact that many women had different maiden and married names, that Irish republicans often used Irish variants of their English names, and that letter-writers fearing censorship and criminal prosecution resorted to pseudonyms had to be taken into account, and a balance had to be struck between respect for the variety of spellings and names used in the original sources and the technological necessities of a user-friendly digital archive (see evaluation tools suggested by Stiller, Thoden, Zielke, 2016).   Onsite user testing provided insights how website visitors would interact with the site and how self-evident navigation would be. One important result of the user testing was that our clean and reduced design overwhelmed users with little IT experience. An additional section was added to the project’s landing page to offer alternative ways of accessing information on how to participate in the project. Menu categories were renamed, and the number of keywords in “browse” filters was limited.   Every step towards simplification was always balanced by the introduction of spaces for the ambiguity and complexity of content elsewhere (e.g. continuously growing “highlights” and “news” sections on the landing page). Feedback received from the focus-group revealed that the average website user is more willing to accept complexity at least in the design of search results. Frequent contact with sophisticated search engines in the world of online shopping, streaming services or digital newspapers have prepared users to handle large amounts of search results they themselves need to narrow down. When the Letters 1916-1923 project first invited members of the public to test its website in 2015, this type of display was rejected by most users. In only two years, the public expectations of how to navigate search results have changed considerably.    By describing some of our initial problems and design choices in the light of recent user testing, we provide a case study from which similar public humanities projects may benefit .  ",
        "article_title": "Complexities And Compromise – User-Centred Interfaces For Public Humanities projects",
        "authors": [
            {
                "given": "Monika Renate",
                "family": "Barget",
                "affiliation": [
                    {
                        "original_name": "Maynooth University, Ireland",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Schreibman",
                "affiliation": [
                    {
                        "original_name": "Maynooth University, Ireland",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Pádraig",
                "family": "MacCarron",
                "affiliation": [
                    {
                        "original_name": "Maynooth University, Ireland",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "open content and open science",
            "theory and methodology)",
            "English",
            "public humanities and community engaged scholarship",
            "databases & dbms",
            "scholarly publishing",
            "information architecture and usability",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Many academic projects have produced digitised archives that provide public access, but public engagement is difficult to sustain, and museums are integrating digital media into their visitor experiences to engage the public. As Schreibman (2017: 281) argues, public participation in cultural heritage can generate new ideas and could challenge the top-down division between the researchers and them. In a move towards convergence of academia and the public, this paper proposes a novel approach to producing interactive digital narratives (IDN) that combines expert-produced content and user-generated content (UGC) to create participatory cultural heritage narratives. Heritage is socially constructed and communicated; heritage meanings are not frozen in time, they are the result of constantly evolving values, beliefs, knowledge and traditions (Giaccardi, 2012: 2). Thus, it is important to allow heritage narratives to keep evolving as society changes which calls for public participation. Some contemporary constructions of heritage have been criticised as being overly selective, presenting glamourised, nostalgic presentations of the past and excluding many social groups within a multi-cultural society (Aitchison et al. 2000: 109). The democratisation of knowledge-building is growing because audiences are not passively consuming content produced by expert historians and digital media has allowed anyone – with the access and ability – to produce, record and display representations of the past (Cauvin & O’Neill, 2017: 5). The affordances of new media create a bridge for potential collaboration between experts and the public. Participatory heritage is a form of citizen scholarship, which could result in more engaged access to digitised cultural heritage content (Carletti, 2016). It is hypothesised that IDNs present an opportunity to increase the breadth of the social groups (e.g., visitors or tourists) interested in heritage, the types of histories (e.g., different social groups) shared in the digital space, and allow for evolving interpretations and continued public contributions to cultural heritage narratives.  Interactive digital narrative is a term used by scholars who work in the areas of intelligent narrative technologies, interactive drama, interactive storytelling, and narrative games (Monfort, 2015: x). It encompasses the many different formats of digital narratives, such as interactive fiction, transmedia storytelling, web documentaries and video games (Koenitz et al., 2013). If IDN is separated into its three components – namely interactivity, the digital medium, and narrative – each part has historically been researched in different disciplines. However, IDN is “a marriage of computation and narration” that brings together perspectives from computer science and the humanities (Koenitz, 2017a: 361). IDNs combine technical developments, advances in artistic expression, and the expansion of analytical perspectives (Koenitz, 2017a). IDNs can be created for different cultural heritage applications and audiences such as, museum experiences, location-based storytelling, or other heritage installations. Cultural heritage IDNs could help preserve local history, uncover lost cultural stories and customs, and allow cultural heritage tourists to explore their own and other cultures as facilitated by a narrative framework. Examining the potential of IDN formats for creating cultural heritage narratives involves many facets of complexity. Firstly, the concept of a participatory cultural heritage narrative is complex as it involves different interpretations, representations, perspectives (some of which may be controversial) and participants. Secondly, UGC is difficult to verify for authenticity and factuality, thus the extent to which it is incorporated into a non-fiction IDN is challenging. Thirdly, the different perspectives or levels of complexity on the subject matter can be designed into an IDN system, which are designed to tell complex stories (Koenitz, 2017b). More specifically, creating a cultural-heritage IDN involves considering: how the non-linear digital narrative should be structured, the type and method of delivery of multimodal content (e.g., text, image, video, etc.), and how content from different corpora – such as that produced by galleries, libraries, archives, museums (GLAM) and governments, creative-industries, and UGC – can/should be incorporated into a new complex narrative to achieve a participatory cultural heritage while maintaining narrative control.  This conference paper will discuss these complexities in the context of how cultural analytics (Manovich, 2007) can be used to create remixed IDNs. A beta IDN system will be built as a “reverse-engineered” or remixed, web-based transmedia narrative drawing upon data acquired through cultural analytics of numerous sources of cultural heritage content. The results of user testing will inform whether the convergence of a top-down and bottom-up approach is feasible to produce a participatory narrative and inform whether IDNs may be sustainable forms for the future of cultural heritage storytelling. ",
        "article_title": "Creating Complex Digital Narratives for Participatory Cultural Heritage",
        "authors": [
            {
                "given": "Nicole",
                "family": "Basaraba",
                "affiliation": []
            }
        ],
        "publisher": null,
        "date": "2019-04-05",
        "keywords": [
            "libraries",
            "museums",
            "communication and media studies",
            "interdisciplinary & community collaboration",
            "English",
            "cultural analytics",
            "GLAM: galleries",
            "archives",
            "cultural studies",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper looks at YouTube as a platform for the circulation of far-right pseudo-intellectual ideas and focuses on its alleged tendency to act as a machine for radicalization. Whilst social media platforms such as Google have promoted the notion of 'connectivity' as an unalloyed social good (Schmidt, 2013), it has recently been observed that such connectivity, mediated, for example, by recommendation systems, is equally capable of strengthening communities bound by narrow preferences for the same information, including political ideas. Its resulting so-called ‘filter bubbles’ would be creating communities bound by common aversion for users with different informational diets -- particularly political and ideological others (Zuckerberg, 2018, 3, Pariser 2011, Sunstein 2002). While a significant amount of attention has of late been focused on the general problem of 'fake news' within digital humanities research (Bournegru et al, 2017, Venturini, 2018), this abstract takes an empirically-focused approach to the broader, seemingly philosophical, problem of epistemological relativism in the landscape of contemporary social media (McIntyre, 2018; Kakutani, 2018). Specifically, it considers how it is that social media platforms, in this case, YouTube, has become a site of dissemination and development of ideas originating from far-right subcultures, particularly via the medium of pseudo-intellectual debate.  Recently referred to as the 'Dark Intellectual Web' (Weiss, 2018), YouTube is today replete with what had in the past been referred to as 'balkanized speech markets' (Sunstein, 2001, 69). These extreme speech markets, as we will refer to them here, traffic in a variety of ideas considered to be 'too controversial' to be discussed by the academic 'establishment'. While they may be understood to have deep roots in the subcultural margins of the web (notably in web-fora such as 4chan’s /pol/ \"Politically Incorrect\" board or Wikipedia alternates such as Metapedia    http://metapedia.org    ), they can be seen to extend across mainstream social media platforms, and perhaps nowhere more so than on YouTube (Tokmetzis 2019). But while YouTube can justifiably censor imminent threats of physical violence, extreme speech -- hitherto marginal -- rarely fits the platforms’ actionable descriptions of ‘hateful’ content (YouTube, 2018a, 2018b). This thus leaves watchdogs and authorities to doubt where, exactly, to place the distinction between expressions of ideological extremisms -- which have been further normalised into popular content, and which many platforms have sought to tolerate in deference to American free speech principles -- and simply illegal expressions of ethnic, religious and sexual hatred. While the latter discourse might perhaps be ignored if it remained on fringe websites, extremism experts and media scholars have argued that the design of YouTube's recommender system may provide users with the affordances to develop these ideas into inconspicuous intellectual expressions — making Youtube an accidental machine for normalising political extremes within closely-knit political communities (Holt, 2017; Tufekci, 2018).   The problem in question, then, is at the intersection of computational linguistics and intellectual history. As hate speech has largely been defined as products or by-products of far-right political thought, and as content associated to this political culture has been largely normalised as popular content, to what extent would hate speech detection equate ideological persecution or censorship?   The problem of censorship poses theoretical and methodological challenges. Social Media platforms, and their comment space in particular are notorious for their opaque censorship policies. Based on our preliminary results, however, we observed that videos are more rigorously monitored than comments. In a related study on the vaccine debate, we noted that extreme content, promoting the anti-vaxxers point-of-view, was more likely to be removed than pro-vaccines channels.  Would it not be inadvertently aggravating contentions between polarised communities? Instead, how can hate speech detection be sensible to the intellectual history and transformation of ideas substantiating ‘traditional’ hate speech? Discussions of ideas traditional to far-right political thought is widespread enough on Youtube to, as this paper will assess, call for computational techniques sensible to the ambiguity of such “online vitriol” (Pohjonen and Udupa, 2017, 1173).   In this respect, this paper adopts an approach that might be considered a variation of reception studies traditions in studying extreme speech on YouTube. Our approach is a response to Lewis’s analysis of far-right political culture on YouTube: it seeks to follow the audience as opposed to profiling the uploaders of content — the latter whom Lewis referred to as the “Alternative Influence Network” (Lewis, 2018). While in dialogue with Lewis’s research, we take issue with her use of the concept of “influence” — informed here by a critique from reception studies of the media effects tradition. With this point of distinction in mind, our paper looks at how we can detect  hate speech terms uttered by channels and commenters on Youtube, as defined by a number of international hate speech watchdog datasets, by associating them to intellectual corpora having substantiated their discriminatory viewpoints. While racially discriminatory slangs -- to name the deeply inflammatory term, ‘nigger’ -- may not be detectable on YouTube comments and video transcripts, terms originating from nineteenth-century biological racism -- ‘negroid’ -- are instead abundant. We consider how and whether such terms appear in channels and commenters presenting known for debating their arguments in such pseudo-academic format. We then attempt to understand what happens when commenters who have previously been using terms associated to ‘hate speech’ come into contact with intellectualised extreme speech markets.  It is worth mentioning that YouTube has often been characterized as an important hub and source of intellectual resources for new far-right users and talking heads (Holt, 2017; Tufekci, 2018; Weiss, 2018; Lewis, 2018). Self-described 'sceptics' and 'freethinkers' engage in discussions of topics such as ‘race and IQ’ or the role of Jewish elites in the ‘new world order’ as matters of genuine intellectual concern. While the substance of these discussions appears as hateful or conspiratorial to both experts and laymen users, their participants tend to see their points of view as alternative, both ideologically and intellectually.  Within the digital humanities, there is a growing interest in the automatic identification of political ideology and ideas, often relying on techniques created within the field of natural language processing (Azarbonyad et al. 2017; Kenter et al. 2015; Iyyer et al., 2014). The latter discipline has tended to focus on the detection of hate speech as though it were a category distinguishable from “normal” or “acceptable” speech. Following Pohjonen and Udupa’s critique of this “binary” conception of hate speech, our approach is concerned with a broader range of equally problematic forms of speech that do not necessarily constitute hate speech. With this problem in mind, the abstract explores text mining techniques to monitor attitudes of users over time. This yields a computational lens on the behavioural effects of the platform, i.e., how the beliefs and behaviours of users shift as a result of the content they engage with. Our approach of following the audience thus differs from most YouTube research which centred on content creators and largely relies on standard metrics made available through the API (such as channel content, related videos, etc. — see Rieder, 2018). We thus map the debating culture of the new right on YouTube as a process resulting (in part) from the interaction of commenters and channels. By measuring the word usage of the audience of what has elsewhere be called the “Alternative Influence Network” or the “Dark Intellectual Web”, our approach may thus be understood as mapping the debating culture of the new right on YouTube from below in order to determine how audiences become associated to the cause.  Methodologically, we have developed quantitative techniques for gathering all the comments made by individual users (whose identities have been anonymized) in order to map their movement through and across YouTube channels. In collaboration with journalist and extremism expert Dimitri Tokmetzis (Tokmetzis, 2018), we have processed 1027 right-wing YouTube channels initially harvested using the YouTube Data Tool (Rieder, 2018).  The data was collected based on a list of actors that extracted from a database created by the Dutch right-wing watchdog organization KAFKA. The resulting set has a broad an international (European) focus, though the majority of actors are Anglo-American. The data retrieved via the YouTube API record consists of video titles, channel references, tags and comments. To analyse this content -- and establish the extent to which users agreed with extreme opinions -- we defined two vocabularies: one is a set of intellectual ‘extreme’ notions originating from two, main sources. One source comes from tags in right-wing videos and commenters, e.g. 'eugenics', ‘race and IQ’ and ‘white identity’. The second source is a collection of frequently mentioned terms from Metapedia’s article on race and intelligence. The second vocabulary is made of 267 common English language comments that indicated a commenter’s consensus with a video (including comments such as 'well said', 'agreed' and 'brilliant'). Using such comments, we identified those commenters agreeing with videos tagged as ‘extreme speech’. We then identified the five preceding comments by those users, which we visualised in a directed network that indicates where and when users have commented in videos from our selected channels. To find movements between hate and extreme speech, we have developed a similar list of keywords to identify ‘hate speech’ comments.   We define hate speech based on a corpus of terms as collected by the hatebase.org, which bills itself as the world's largest structured repository of regionalized, multilingual hate speech, and which has been compiled by a recognized Canadian NGO. We manually selected items from that database to make up the list.   Applying these to comments allows us to compute ‘extreme’ and ‘hate speech’ scores for users over time. Comparing movements over time in terms of both ‘hate speech’ and ‘extreme speech’ will provide us with the data to test the hypothesis of whether commenters move from ‘hate speech’ to ‘extreme speech’.    Figure 1: Scientific racism and hate speech in right-wing comments and transcripts, 2006-2018  At the crux of the problem then is the attempt to define what counts and does not count as hate speech — which is itself a concept that has come under considerable criticism from the subject being studied in this case study. Our objective is thus to bring critical perspectives to those digital spaces where users consume and reproduce intellectual content. More specifically, our objective is to monitor pseudo-intellectual debates in the comment section and explain them in terms of user behaviour. It is thus our hope that this approach will help researchers concerned with hate speech to contextualise concepts having to do with various intellectual processes online, including radicalization, indoctrination, and (partisan) education. ",
        "article_title": "The Intellectualisation of Online Hate Speech: Monitoring the Alt-Right Audience on Youtube",
        "authors": [
            {
                "given": "Marc",
                "family": "Tuters",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Emillie",
                "family": "de Keulenaar",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Ivan",
                "family": "Kisjes",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Bach",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Kaspar",
                "family": "Beelen",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "social media",
            "digital ecologies",
            "communication and media studies",
            "English",
            "digital communities and critical infrastructure studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The application of new technologies to cultural heritage has led to important methodological changes in the protection and enhancement of monuments. This new approach is stated in the objectives of the International Council on Monuments and Sites, 1 which aims to restore meaning and preserve the memory of historic buildings, promoting the application of technology in the assessment of monuments: this is particularly interesting with regard to the recovery of archaeological heritage.   The European Commission report, New Technologies for the Cultural Heritage Sector, 2 states that virtual reality will provide “an impressive, immersive and involving product”. Within this context, archaeoacoustics is being used as a new method for the analysis of historical heritage, enabling the evaluation of the sound quality of a space (Scarre and Graeme, 2006; Suárez et al., 2016) 3 by using auralisation techniques which allow cognitive and physical elements to be reproduced and combined (Eneix, 2014). 4  The STESICHOROS 5 project aims to assess the cultic theatres acoustics through the acoustic reconstruction of a particular case study at Selinunte in Sicily. It is also hoped that the results might provide some foundations from which to create experimental interpretative reconstructions of what the cultic theatres might have sounded like, using acoustics and digital technology in order to recover their lost intangible heritage.   The Cultic Theatre at Selinunte and its Acoustics  The ancient site of Selinunte is recognised today as one of the most important archaeological sites of the Greek period in Italy. In 2006, the Institute of Fine Arts at New York University began a research project on the acropolis in collaboration with the Archaeological Park of Selinunte. The project consists of a new, systematic and interdisciplinary study of the archaeology and architecture of the main urban sanctuary, beginning with its southern sector.  The investigation included a systematic programme of documenting the buildings in the area and its digital reconstruction (Fig. 1). Several elements suggest the identification of the ‘South Building’ as an impressive theatral viewing area with particular acoustic qualities (Marconi and Scahill, 2015). 6  This building belongs to a group of theatral structures found in various regions of the Greek world. Many of these structures were not proper “theatres”, but rather primitive rows of seats, with linear and non-circular  theatra and/or  orchestra.   The acoustics of the linear  theatra in the Greek world have never been analysed (Blesser and Salter, 2011): 7 no study has focused on the acoustics of these theatres in order to understand how and why these spaces were chosen for auditory and synaesthetic experiences (Schofield 2014; Mattioli et al., 2017). 8      Fig. 1. Digital reconstruction of the theatral area in Selinunte (Sicily)    (Marconi and Scahill, 2015, fig. 11).  Research Objectives  The STESICHOROS project has emerged to address the gap in international scholarship. Previous acoustic research has concentrated only on circular theatres in the Greek world. It will establish a new line of research to complete a study on the acoustics of the linear and non-circular  theatra.  The overall objective of the STESICHOROS project is the assessment and recovery of the lost intangible heritage of cultic theatresʼ acoustics through acoustic reconstruction of these spaces of the past. Using archaeoacoustics as an emerging discipline that involves the study of ancient sites (Bertolis and Bisconti, 2013), 9 the reconstruction will recreate what the cultic theatre in Selinunte might have sounded like. The project aims to verify whether it is possible that this building was built in a precise place for its acoustical qualities.   Research methodology and approach  In order to analyse the acoustic characteristics of the cultic theatre at Selinunte, we plan three main tasks:   1. current acoustic simulation with the creation of a geometric 3D model of the building in its current state;    2. simulation, acoustics and auralisation with a creation of geometric 3D models of the building (non-existent). This process allows the use of visual and sound virtual reality to perceive and experience the sound response of a non-existent space which we have recreated thanks to digital technology; 10   3. acoustic parameters, simulations, auralisations includes a comparative study and perception of the acoustic variations in the cultic theatre during its history.   Expected results This project will provide the first acoustic model for the study of acoustical properties of cultic theatres in the ancient Greek world. It is also hoped that the results will provide some foundations from which to create experimental interpretative 3D reconstructions integrating acoustic models. The results will establish a new framework, which future researchers can use to advance their knowledge of the  application of 3D technology to virtual acoustics in order to offer an innovative research method in visualisation and immersive experiences related to the reconstruction of archaeological sites and their soundscapes.  ",
        "article_title": "Towards a New Approach in the Study of Intangible Cultural Heritage",
        "authors": [
            {
                "given": "Angela",
                "family": "Bellia",
                "affiliation": [
                    {
                        "original_name": "National Research Council, Italy",
                        "normalized_name": "National Research Council",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/04zaypm56",
                            "GRID": "grid.5326.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-19",
        "keywords": [
            "multimedia",
            "archaeology",
            "English",
            "3D/4D modeling",
            "modeling",
            "simulation",
            "audio",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  With ca. 17 million visitors annually to culture and heritage sites in Glasgow and Edinburgh alone, tourism is a major driver of the Scottish economy - it is worth some £6bn annually, ca. 5% of Scottish GDP; and supports 196,000 jobs . The centrality of cutting-edge immersive experiences for tourism, the heritage industry and audience development has been increasingly evident in recent years, with the development of  Ars Electronica Linz   (1996); the  Robert Burns Birthplace Museum   (2009);   The Battle of Bannockburn   (2013), and other venues.   Immersive experiences describe all forms of perceptual and interactive use of technologies and physical spaces in order to create a  hybrid reality , in which visitors feel “part of the experience as a whole, encompassing all spheres of attention” – immersion can be Sensory (audio-visual, olfactory, haptic elements), Challenge-based (interaction) and/or Imaginative (narrative and interpretation)  .    For tourism and cultural heritage, immersion represents a pathway towards a  mixed-mode experience economy , which reflects the nuances of differing experience dimensions embodied by different elements of a site . In this mixed-mode experience economy, visitor engagement combines activities across the “Realms of an Experience”.   Despite the opportunities afforded by immersive experiences – and relevant investment in such experiences in Scotland – there has been a lack of substantive evidence (from scholarship and practice) to evaluate current approaches and guide future developments:   How successful are the approaches to immersive technologies at major heritage sites in Scotland, in terms of outcomes against business plan expectations and in terms of visitor response? What kinds of future development are supported by existing evidence?    The Scottish Heritage Partnership   These questions are the remit of the  Scottish Heritage Partnership  project. The Scottish Heritage partnership is a 2018-19 Engineering and Physical Sciences Research Council/Arts and Humanities Research Council funded project at the University of Glasgow, aiming to address the existing practice and future potential of immersive experiences and technologies in the culture and heritage industry in Scotland.    The team is led by Professor Murray Pittock, with Professor Lorna Hughes and Dr Maria Economou as the Co-investigators and Dr Agiatis Benardou and Dr Leo Konstantelos as Research Associates. The National Trust for Scotland, Glasgow Museums and The National Library of Scotland are key partners in the project and so is our industry partner, Soluis Heritage.    The objectives of the project are to:   Build on and expand existing partnerships to explore the efficacy of immersive experiences at major heritage sites.   Build a decision-making tool and gather evidence for policy development.   Explore how we can best harness and shape cutting-edge digital technology and develop effective, meaningful content into leading edge inclusive and impactful immersive experiences.   Outline the kind of social/group experiences facilitated or limited by immersive technology, and study how these affect the visitor experience overall.   Examine the consequences of service-wide adoption of immersive technology in Scotland's leading heritage and collections resource provider, Glasgow Life.   Produce a website, a digital decision-making tool, a policy paper and a risk assessment together with:   An evidence-based market model for use with Scottish Government, VisitScotland, local tourist authorities and nongovernmental agencies   A route to developing suitable immersive technologies which can be scaled/developed to meet the criteria identified under (a), in the process benefiting our digital partner, Soluis.   Support National Trust for Scotland, National Library of Scotland and Glasgow Museums' strategic use of their collections in interpretation and exhibitions development.     Our methodology   With support from our partners at Glasgow Museums, the National Trust for Scotland, and the National Library of Scotland, we have designed, collected and processed a substantial corpus of empirical evidence. To date, hundreds of questionnaires have been completed by visitors at six major heritage sites across Scotland, including the Battle of Bannockburn; Culloden Battlefield; the Robert Burns Birthplace Museum; the Riverside Museum; the Kelvingrove Art Gallery and Museum; and the NLS at Kelvin Hall. We have conducted observations of visitor experience at Riverside and RBBM, harvested and processed visitor comments from online websites, and we are using secondary analysis of existing visitor experience data to answer the project’s research questions.   Development of an evidence-based, decision-making model was completed. Formulated as a policy and risk assessment document, the model is meant to help heritage institutions identify the kinds of future immersive experiences that are supported by our evidence; as well as assess how to develop effective, meaningful content into leading edge inclusive and impactful immersive experiences.   Our digital partner, Soluis Heritage, developed a visualisation of both the model and project findings, as a decision-making tool to illustrate the wider implications for policy and good practice, making the project’s findings accessible and clearly showing the underlying data and empirical evidence used. This output was made freely available online, as a resource illustrating the creative and critical processes, and key decision points, of developing immersive technologies in a cultural heritage environment.     What have we found:   Our findings provide valuable pointers and evidence on how cultural heritage institutions in Scotland (and beyond) can prepare for dynamic change in the immersive experiences economy. Based on visitor feedback and other data sources, we have investigated the components of perceived value in a digital experience; the audiences most interested in immersive technologies; as well as characteristics of good immersive design and content.    A summary of our findings was presented in a decision-making visualisation, while the entire corpus of evidence informed the development of a policy document for decision-making in incorporating immersive experiences in the cultural heritage sector.   To stay up-to-date with our work, follow us on Twitter  @scotmimmersives  ; and visit our  project website  .   ",
        "article_title": "A World of Immersive Experiences: The Scottish Heritage Partnership",
        "authors": [
            {
                "given": "Leo",
                "family": "Konstantelos",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Murray",
                "family": "Pittock",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Agiatis",
                "family": "Benardou",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow / ATHENA R.C.",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Maria",
                "family": "Economou",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Lorna",
                "family": "Hughes",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-08",
        "keywords": [
            "digital humanities (history",
            "user experience design",
            "interface",
            "gamification",
            "theory and methodology)",
            "English",
            "cultural studies",
            "virtual and augmented reality"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  As methods of immersive experience, virtual and augmented reality, artificial intelligence, as well as mixed methods (ie analogue and digital combined), are all means of memory re-composition in the cultural heritage domain.   Immersive experiences describe all  forms of perceptual and interactive use of technologies that blur the line between the physical world and a simulated or digital world, ie create a hybrid reality aiming at embracing all spheres of the user’s attention. Immersion can thus be sensory (audiovisual, olfactory, haptic elements), challenge-based (interaction), and/ or imaginative (narrative and interpretation).    The beginnings of immersive experiences can be traced at the end of 1960s, when philosopher and cinematographer Morton Heilig invented Sensorama, a simulator for up to four people of a motorcycle ride in Brooklyn, which created an illusion of reality through a three-dimensional film with stereo sound, vibrations of the seats, wind on the face and even smells of the city. Around the same time, Ivan Sunderland (MIT, University of Utah) developed the   Sword of Damocles   the first VR system with interaction possibilities. Since then, technologies which create immersive experiences have developed radically, primarily in the context of gaming and thematic parks, and have thus emerged as new interactive and narrative means.  Some of the most creative applications of immersive experiences appear nowadays in museums and cultural heritage sites, proposing innovative narratives and interpretations through an impressive range of digital methods. Moreover, scenarios complementing immersive experiences as well as personalized storytelling have transformed visitors experience in cultural heritage sites immensely.  Difficult heritage was a term coined by Sharon Macdonald (ICMAH Annual Conference 2007) which was defined as “concerned with histories and pasts that do not easily fit with self-identities of the groups of whose pasts or histories they are part. Instead of affirming positive self-images, they potentially disrupt them or may threaten to open up social differences and conflicts. Difficult heritage deals in unsettling histories rather than the kinds of heroic or progressive histories with which museums and heritage sites have more traditionally been associated.” Difficult heritage is relevant not only to museums focusing on the recent past, but also to all archaeological and historical sites which may present controversial and sensitive subjects. Immersive technologies are so far limited to very few difficult heritage sites: In 2017, a virtual reality experience titled   The Day the World Changed   was to commemorate the work of the International Campaign to Abolish Nuclear Weapons. The experience began with an explanation of the conception, development, and implementation of the atomic bomb and then proceeds to a second part focusing on the aftermath of the attack. Visitors could walk through the ruins of the city and examine artifacts from the bombing. Similarly, a  virtual reality experience developed in 2016 allows visitors to Jerusalem to see the city as it looked during the heyday of the   Second Temple  .   In Europe, immersive technologies have not been employed widely in historically contested sites. In Scotland, however, immersive experiences are used to enhance visitor experiences in two difficult heritage sites, Bannockburn and Culloden. In 1324 in Bannockburn, near Stirling, the Scots under Robert the Bruce defeated the English army  marking a significant Scottish victory in the First War of Scottish Independence, and making the site a landmark in Scottish history. The Battle of Bannockburn immersive experience takes visitors through a series of 3D films depicting the events during and surrounding the battle, and culminates in a visit to the Battle Room in which they have the option to participate in their own interactive battle game and are even give the chance to turn the course of history. In Culloden, east of Inverness, the 1746 battle marked the end of the Jacobite rising. The site’s visitor centre is comprised of an immersion cinema, real-life reenactments of everyday life and a game. In Britain amidst Brexit and in the context of a possible new Scottish independence referendum, both sites bear characteristics of difficult heritage and immersive experiences shape new, objective yet diverse narratives around them.   Α study by G. Yair (2014) showcased that, due the trauma the Third Reich engendered, German memorials appear to be reluctant to advocate, direct, or lead, thus shying away from manipulative educational strategies that they associate with the Third Reich. They intentionally avoid manipulating emotions or directing visitors to arrive at pre-determined moral conclusions, and refrain from using expressive props to create authentic experiences. Yair argued that despite the many possibilities for introducing visitors to authentic and challenging exhibits and artefacts, German memorial sites and documentation avoided using emotive cues. Consequently, without a more challenging approach, these sites are unlikely to create key experiences with lasting effects on visitors’ identities.   Despite the fact that the Nazi Occupation (1941-1944) in Greece has influenced public debates, inspired artistic displays and performances, the sites of memory remain invisible. The proposed talk will use the infamous Block 15 of the Haidari Concentration Camp in West Athens, the largest and most notorious concentration camp in wartime Greece, as a case study of a largely neglected site of difficult heritage and will attempt to showcase that immersive technologies would be best fit to make accessible, highlight and re-interpret both the site and the narrative surrounding it. The Block 15 showcase will employ a mixed methodological approach, in which digital methods will be applied in the re-composition of difficult memory through immersive technologies, ethnography, history and digital narratives. It will be argued that, through original scenarios based on primary and multimedia archival sources, immersive experience developed (such as interactive 3D and VR) could not only bring back to life the actual Block 15, currently an endangered monument, but could also function as reminders of the atrocities of prisoners, in an attempt to reintroduce a historically and politically contested site to heterogeneous audiences, both in situ as well as in sites outside the Concentration Camp.  ",
        "article_title": "Immersive Experiences And Difficult Heritage: Digital Methods As Re-interpreters Of Historically Contested Sites",
        "authors": [
            {
                "given": "Agiatis",
                "family": "Benardou",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow / ATHENA R.C., Greece",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-09",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "multimedia",
            "user experience design",
            "interface",
            "gamification",
            "theory and methodology)",
            "English",
            "cultural studies",
            "audio",
            "cultural artifacts digitisation - theory",
            "video",
            "virtual and augmented reality"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The provision of digital humanities training to graduate students who have no previous experience of the field is a challenging task. In the United Kingdom, such training is still unevenly distributed across universities, varying from dedicated Masters and Doctoral programmes to more informal seminars and research groups. For smaller or less research-intensive institutions, the establishment of a digital humanities programme usually begins with the recruitment of a single specialist lecturer (Cordell 2016), who may however struggle to cover the breadth of expertise required to provide suitably wide-spectrum teaching. This paper will address these challenges in relation to a specific training programme, including our pedagogical approach to addressing these challenges, and how we can develop DH syllabi that simultaneously address the individual needs of the learner alongside a broader understanding of DH as a field of practice. The 11 Doctoral Training Partnerships (DTPs) funded by the UK Arts and Humanities Research Council offer the possibility of pooling the expertise of several DH specialists to provide more extensive coverage. The Consortium of Humanities and Arts South-East England (CHASE) DTP, funded in 2014, comprises nine institutions: the universities of East Anglia, Essex, Kent, Sussex; Goldsmiths, University of London; the Courtauld Institute; The Open University; Birkbeck College and the School of Oriental and African Studies. Since 2015, CHASE has employed its Cohort Development Fund to pioneer an innovative doctoral training programme, Arts and Humanities in the Digital Age (AHDA), which has to date trained over 80 students. The programme is built around a three-day winter school, a series of optional methods-based workshops, a two-day mid-project residential, and a final plenary session. This programme is supported by compulsory group work, where each group is tasked with producing a wireframe for a proposed digital humanities project. Methodology We faced two key challenges in developing the course: first, the DTP is comprised of a diverse range of institutions with broad remits across the arts and humanities; and second, we do not know the disciplinary backgrounds of our self-selecting cohort until they sign up. These students have already identified the relevance of the programme to their training needs, but they arrive with very different concepts of what the digital humanities actually are, and very different learning objectives. Some want to address a personal skills deficit; others want to develop existing skills to address a specific research goal; and others want to engage with DH as a field of study. This has necessitated an iterative approach to programme development that has cohered into a teaching philosophy focused on introducing digital humanities as a reflective methodological space. The broad possibilities of DH can necessitate a self-deterministic element to skills development (Rhody et al., 2018), and this is reflected in our approach. It is important that our students understand the central methods that are considered core to DH (McCarty and Short, 2002), so that they may situate these methods in relation to their own disciplinary practices. The four iterations of the programme so far have trained students in the following: Text Encoding; Text Analysis; Information Visualisation; Digital Images; Web Authoring; Databases; Project Management. Our approach has therefore been twofold: to establish a series of workshops on key methods in digital humanities; and to support this work with a residential school that introduces a degree of meta-discussion (Cordell 2016) alongside an explicit self-reflective component framed in terms of threshold concepts. This understanding is then developed through group work that allows students to situate their work within a broader sense of collaboration in digital humanities.  We have experienced relatively high variation in workshop attendance in the programme, which can be attributed in part to the difficulty of addressing this latter point. Our approach foregrounds individual skills development, while not necessarily making explicit that the methodological component of the programme reflects “communities of practice” of DH that draw together several disciplinary groups and ways of knowing (Siemens, 2016) into a so-called “methodological commons” (McCarty and Short, 2002). For new entrants to the field, the meta-discussions that shape our understanding of interdisciplinarity in DH are neither obvious, nor necessarily relevant insofar as interdisciplinarity can be understood to combine approaches or methods from more than one discipline (Klein, 1990). The extent to which the engagement of the digital humanities with various disciplines is truly interdisciplinary has been contested on the basis that it is not clear how the field contributes to a larger shared agenda with other fields (Liu, 2013). This has led others to propose alternative models: Svensson, for instance, introduces the notion of “trading zones” (2012) where different traditions are maintained while still carrying out intersectional work. Such approaches foreground the importance of methodology in defining the nature of these intersections.  Outcomes AHDA has always introduced the domain of digital humanities through methodology as a reflective, iterative process. It does so because this is specifically a research-level programme. As such, developing confidence in the affordances and limitations of certain methods of analysis, and the quality of data itself, is recognisable to most research degree students. It echoes the conceptual underpinnings of their own, very specific, research projects, a key doctoral criterion (Berman and Smyth 2015). In the final presentations, participants are able to diagnose the technical and conceptual issues as they relate to their own research context, but also the methodological limits of their group project. By this process of reflection, students identify what could and could not be achieved, and in the course of cohort discussion, what approach might improve the reliability and validity of their project. The second outcome is that, through this reflective approach, students are developing a degree of  academic socialization. That is, not only what constitutes an appropriate method or technique for the data, but how it relates to a discipline or across disciplines. For example, in presenting their research findings, our students are not simply evidencing the work they have undertaken, but are also modelling scholarly communication. This might involve explicitly articulating the method, demonstrating clarity and accuracy in terminology, and being able to defend their choice of project design within the timescale and resources at their disposal.  The key challenge in the second outcome - as Cordell (2016) notes in relation to undergraduate DH teaching - is that it is very difficult to develop those meta-discussions about digital humanities as a field. Programmes often fail to do this because the domain knowledge of the novice is insufficient, and frequently tied to their home discipline. For our programme, it is possible that these two broad outcomes have limited potential for those intent on progressing beyond the methodological. For the majority of participants, practical application and a critical approach to analysis may be sufficient; in short, addressing a skills deficit. However, the kind of academic socialization noted above may not tackle the epistemological or hegemonic dimensions of digital humanities practices. These are better likened to Lea and Street’s (1998) concept of  academic literacies, which allow the student to better understand and critique the social nature of knowledge production in terms of what counts as known, and who or what makes those decisions. The challenge for programme design is embedding these literacies while still enabling students to achieve other key learning objectives.  Conclusion The application of the pedagogical framework of AHDA has been continuously refined through student feedback. The four iterations of AHDA so far have privileged a self-directed style of learning, giving students the freedom to choose among a number of elective workshops. Student feedback has been positive, with those who complete the programme stating that it addressed their learning needs effectively, and that the cohort development aspect of the course helped them to take ownership of their learning within a supportive network of common interest. However, the teaching team has noted that the highest levels of engagement have been with those topics, such as Information Visualisation, that have the greatest cross-disciplinary applicability. This feedback, combined with the difficulty of providing specialised teaching for an always changeable student cohort, has led the teaching team to innovate the 2018/19 provision, which will therefore focus on leading the students through the lifecycle of a DH project. Through Open Source tools and platforms, students will learn how to clean, manage, store and analyse either their own data or those provided by the British Library; how to present their results and understand the nature of the DH community of practice; how to make informed decisions about the extent of their participation in DH, whether through deep engagement or in the form of “legitimate peripheral participation” (Lave and Wenger 1991, Ridge 2014). We therefore will require mandatory attendance to all workshops in order for students to engage with each step of the project development process. In this presentation we will introduce the development of the programme and report and evaluate this year’s iteration in relation to the pedagogical framework described above. The unique contribution of this presentation will be to consider how theories of self-reflective learning can combine with an understanding of DH as a community of methods to inform the development of syllabi for postgraduate researchers. ",
        "article_title": "Developing a Community of Practice: The CHASE Model for Digital Humanities Researcher Training",
        "authors": [
            {
                "given": "Francesca",
                "family": "Benatti",
                "affiliation": [
                    {
                        "original_name": "The Open University, United Kingdom",
                        "normalized_name": "Universidade Aberta",
                        "country": "Portugal",
                        "identifiers": {
                            "ror": "https://ror.org/02rv3w387",
                            "GRID": "grid.26693.38"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Gooding",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Sillence",
                "affiliation": [
                    {
                        "original_name": "University of East Anglia",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "pedagogy",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper stems from the analysis of multiple poetic resources that were available on-line, as well as the results of methodological discussions with scholars of European Literature. The goal was to retrieve the informational needs of all these different sources in order to build a common data model for European Poetry. Thus, by implementing a reverse engineering method, we have created the Domain Model for European Poetry, which is an important milestone for making existent poetry resources interoperable. In this paper, we will present some of the challenges we encountered while conceptualizing the information relevant to poetic analysis and how we have worked around them.    Rationale During the history of European Literature, there have been different cultural centers that have irradiated their influence. Some traditions, due to historic and socio-political reasons, have leaned at some point in their history on other literary models (Even-Zohar 1978, 48). Thus, the relations between the different literary traditions are many and heterogeneous. This poses some difficulties for literary research, since these relations are not always easy to trace. An additional handicap is that it demands for researchers to closely know traditions and languages other than the ones of their specialization and the accumulation of all that knowledge is not always humanly possible.  We can find many poetic resources on-line. However, the access to these resources is fragmentary: there is no way to retrieve all relevant information at once. Researchers need to look for multiple resources and then, for each one of them, carry out different queries in order to retrieve the required information.   To work around this problem, the project POSTDATA Poetry Standardization and Linked Open Data (POSTDATA) is an ERC-funded project. Please visit the project’s website for more details:    has a proposal that depends on two key concepts: standardization and interoperability, according to the linked open data paradigm (LOD).   After presenting some brief notes about the objectives, this paper will focus on modeling issues.    Contextualization Linked data must endorse a semantic model before being published. This semantic model can take different formats one of them being an ontology. Considering that one of the main aims of POSTDATA is to provide a means to publish European poetry (EP) data as LOD, this project is building an ontology for this domain. With ontologies, shared and distributed knowledge can be managed in such a way as to allow the integration of information from different data sets (Davies et al. 2003).  The starting point of the ontology construction was the analysis of different databases with contents related to one or more EP traditions in order to represent the informational needs of the community of practice, that is, the EP one. For a detailed exposition about how these informational needs were elicited and other methodological aspects, please see Bermúdez-Sabel et al. (2017).  Our goal is to enhance interoperability between existent repertoires and to facilitate the creation of new poetry resources (González-Blanco et al. 2018). With such an ambitious objective in mind, we must be very exhaustive when eliciting the data needs of our target.  Our sources to draw out the informational needs of the EP community were, on the one hand, a representative sample of existing resources and, on the other hand, a survey that allowed us to consult the EP community. See the map available at    to see the projects that have collaborated with us. In Curado Malta et al. (2018) there is more information about all the resources that were analyzed and what type of study was done to each one of them.  In addition, there were different validation processes through which we received the direct input of experts in EP in order to refine the model. To learn about the validation processes, please see Curado Malta et al. (2018).  We are dealing with miscellaneous sources of information that incorporate data of multiple languages and cultures. This matter complicates the process of modeling. In the following section we will present some of the issues we encountered.   Modeling challenges The creation of a data model that covers all required concepts to analyze any European poem causes some difficulties.  a) Multilingualism: The most obvious problem we ran into arises from working with a multilingual reality. The modelers had to analyze on-line resources in languages they are not familiar with. The perfect team would have an expert on every poetic tradition, that is, a scholar for every European language and literary period. Regretfully, it is hard to find a project in Humanities with that type of resources. This knowledge gap is covered with either the documentation translated to English by the project being analyzed, or with additional bibliography. Nevertheless, the direct contact with the people in charge of that resource is at times inevitable but the response and willingness to collaborate is, for the most part, very positive.   b) Polysemic terms: Occasionally, the difficulties are due to ambiguities in the same language. For instance, we find that many European languages have a term derived from the Latin  versus to describe the poetic line. In English, however, the term “verse” can describe either the line of poetry, a bigger division like the stanza or the whole poetic composition (‘Verse’ 2011).   c) Synonymic (or quasi-synonymic) terms: Literature scholarship is a field with thousands of years, which means that some of the concepts we are analyzing have been defined for many centuries and from different perspectives. The domain experts of the team cannot prioritize any school of thought or theory. On the one hand, we may find different terms for similar concepts, but the use of one term over the other is related to philological schools. In these cases, the less aligned term is selected. On the other hand, we may find the same term in different technical vocabularies but with distinct meanings. For example, the term “dieresis” in syllabic verse traditions describes the separate pronunciation into two syllables of two sounds which usually form one syllable (‘Diérèse’ 2017). However, in the quantitative verse, a “dieresis” expresses the pause that occurs when the end of a foot coincides with the end of a word (‘Dieresis’ 2011). Therefore, it is unavoidable to rank the suitability of certain terms since absolute neutrality is unattainable. In the aforementioned issue with the word “diaeresis,” we selected that term to describe the type of pause and used “hiatus” to express the separation into two syllables, taking the term from its counterpart concept in Linguistics.  d) Semantic interoperability: Like in any other process of semantic modeling, there is some tension between interoperability and semantics. For instance, poetry of the Western world is most commonly divided between qualitative and quantitative meter (Aroui and Arleo 2009, 11–12). Thus, meter may depend on the length of syllables and their distribution, or on the pattern created by stressed syllables coming at regular intervals. In the case of qualitative meter, instead of demanding a fixed pattern of all the stresses, some traditions only care about the position of a certain stressed syllable, like the last one.  Some of the types of qualitative verse have many attributes that are interoperable with the quantitative ones. Therefore, we decided to make a conceptual division between metrical schemes that depend on patterns and those that are defined by “counting” elements (such as counting how many syllables are there before the last stressed one). In this manner, little semantics are lost, because other properties make the distinction between qualitative and quantitative. However, with this conceptualization we enable the comparison between types of meter that, even if they focus on different linguistic properties, have many things in common.   The development of a data model that expects to serve a whole community of practice in the LOD ecosystem entails a great complexity. The type of final user that will consume that data is very diverse. Also, the applications that might be built with these data are many and very heterogeneous. This factor complicates the elicitation of the functional and no functional requirements, thus arising very interesting issues during the modeling process .      Acknowledgments   The authors would like to credit Mariana Curado Malta (Polytechnic of Oporto, Portugal) as the semantic modeler who engineered the method and its implementation.  We would also like to credit Clara Martínez Cantón (UNED, Spain) for her examination of the metrical concepts which are part of the data model referenced in this paper.  Finally, would like to thank the delegates of the analyzed repertoires for their availability and willingness to share information and to discuss issues related to their projects with the POSTDATA team.  Research for this paper has been achieved thanks to the Starting Grant research project Poetry Standardization and Linked Open Data: POSTDATA (ERC-2015-STG-679528) obtained by Elena González-Blanco. This project is funded by the European Research Council ( ERC) under the research and innovation program Horizon2020 of the European Union.    ",
        "article_title": " Towards a Common Model for European Poetry: Challenges and Solutions  ",
        "authors": [
            {
                "given": "Helena",
                "family": "Bermúdez-Sabel",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia, Spain",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "María Luisa",
                "family": "Díez Platas",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia, Spain",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Salvador",
                "family": "Ros",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia, Spain",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Elena",
                "family": "González-Blanco",
                "affiliation": [
                    {
                        "original_name": "Coverwallet, Spain",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "standards and interoperability",
            "English",
            "semantic web and linked data",
            "philology",
            "literary studies",
            "ontologies and knowledge representation",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This long paper presents the  Digital Fragmenta Historicorum Graecorum (DFHG) project ( http://www.dfhg-project.org). The  DFHG is the digital version of the five volumes of the  Fragmenta Historicorum Graecorum (FHG), which is the first big collection of ancient Greek historical fragments published by Karl Müller (1841-1873). The  FHG is a corpus of quotations and text reuses (fragmenta) of 636 ancient Greek fragmentary historians preserved by Classical sources. Fragmentary authors date from the 6th century BC through the 7th century CE and, except for the first volume, are chronologically distributed. Fragments are numbered sequentially and arranged by works and book numbers with Latin translations, commentaries, and critical notes.  The  DFHG is not a new edition of ancient Greek fragmentary historians, but a new digital resource to provide textual, philological, and computational methods for representing fragmentary authors and works in a digital environment. The reason for choosing the  Fragmenta Historicorum Graecorum depends not only on an interest in Greek fragmentary historiography, which provides a rich collection of complex reuses of prose texts, but also on the necessity of digitizing printed editions and preserving them as structured machine readable corpora that can be accessed for experimenting with text mining of historical languages. Moreover, the  FHG is still fundamental to understand recent editions of Greek historical fragments and in particular  Die Fragmente der griechischen Historiker (FGrHist) by Felix Jacoby, who spent his life to change and improve the collection created by Karl Müller. Finally, the corpus of the  FHG is open and big enough to perform computational experiments and obtain results.  This paper presents tools and services that have been developed by the project, not only for accessing the entire collection of the  FHG, but also for providing a new model that can be applied to other collections of fragmentary authors in order to visualize and explore their complex data and connect it with external resources for further developments. The presentation is organized according to the following topics:    Visualization of DFHG contents. The  DFHG appears as an Ajax web page automatically generated by a PHP script querying an SQL database of the entire  FHG, which is accessible by browsing the whole collection or single volumes through a slide in/out navigation menu. The navigation menu allows scholars to navigate the  FHG with a comprehensive and detailed view of the structure of the entire collection and to jump to the relevant section without reloading the page. This kind of visualization is very helpful because the printed version of the  FHG doesn’t contain detailed tables of contents of its volumes, but only short and sometimes incomplete lists of authors published in the collection.   Access to the DFHG. The  DFHG Digger filters the whole collection according to authors, works, work sections, and book numbers, while the  DFHG Search function is performed on fragments, translations, commentaries and source texts. Results show the number of occurrences in each  DFHG author and searched words are highlighted in the text. They also display, when available, the lemmatization of inflected forms and the disambiguation of named entities through external resources. The  DFHG provides a web API that can be queried with author names and fragment numbers. The result is a JSON output containing every piece of information about the requested fragment (e.g.,  http://www.dfhg-project.org/DFHG/api.php?author=ACUSILAUS&fragment=10). The  DFHG exports data to CSV and XML format files (both as EpiDoc XML and well formed XML).   Integration with external resources. One of the main goals of the project is to make the  DFHG part of a bigger infrastructure of processed data. This is the reason why the  DFHG is integrated with external resources such as textual collections, authority lists, dictionaries, lexica and gazetteers. These resources are fundamental for disambiguating and annotating  DFHG data, which in turn offers a collection of parsed texts for enriching external libraries of Greek and Latin sources. The  DFHG is currently connected to different resources that provide morpho-syntactic information and named entities disambiguation of textual data of the  FHG. The  DFHG provides also a  Müller-Jacoby Table of Concordance, which is a complete correspondence between fragmentary historians published in the  FHG and in  Die Fragmente der griechischen Historiker including the  continuatio and the  Brill's New Jacoby ( http://www.dfhg-project.org/Mueller-Jacoby-Concordance). The goal of this resource is to go beyond the  FHG corpus and produce a complete catalog of fragmentary authors of Greek literature published in different digital editions. This resource is progressively ingested into the  Perseus Catalog ( http://catalog.perseus.org).   Data citation. It is possible to retrieve and export citations of  DFHG fragments and source texts down to the word level using URN identifiers. These URNs are combinable with a URL prefix (http://www.dfhg-project.org/DFHG/#) to generate stable links. The syntax of each URN represents the editorial work of Karl Müller, who has arranged the fragments in a sequence and has attributed them to fragmentary authors, works, work sections and book numbers (e.g., urn:lofts:fhg.1.hecataeus.hecataei_fragmenta.genealogiae.liber_secundus:350). The  DFHG provides also CITE URNs according to the guidelines of the CITE Architecture ( http://cite-architecture.org/).   Source Catalogs. The  DFHG includes a  Fragmentary Authors Catalog and a  Witnesses Catalog that have been created from  FHG data. These catalogs allow users to search and visualize the 636 Greek fragmentary historians of the collection and each of their witnesses (i.e., authors who preserve quotations and text reuses of the fragmentary historians). Data from both catalogs has been used to generate charts for visualizing chronological distributions and statistics of  FHG authors and their source texts. This data integrates also  Pleiades identifiers with geo-locations that have been used for producing maps that visualize the geographical distribution of  FHG authors and their witnesses.   Text Reuse Detection. The  DFHG project offers experimental text reuse functionalities for automatic text reuse detection of  FHG fragmentary historians. This resource allows users to automatically detect text reuses (fragmenta) of  FHG authors in their witnesses. Users can insert an XML file URL or select one of the  PerseusDL or  Open Greek and Latin editions available in the  DFHG. Results display quotations and text reuses of  FHG authors within their source texts. The  DFHG allows scholars to download complete XML files of the source texts of the fragments with dfhg attributes that mark up the presence of  DFHG text reuses in the relevant passages of the source texts.  DFHG text reuse detection is based on the Smith-Waterman algorithm that performs local sequence alignment to detect similarities between strings.   OCR Editing. The digital version of the  DFHG has been produced starting from the OCR output of the printed edition of the  FHG. Even if it is possible to obtain very good results when OCRing 19th-century editions of ancient Greek and Latin sources, OCRed texts still contain errors. The  DFHG offers an interface for manual OCR correction of source texts, fragments, Latin translations and commentaries. Corrections are validated or rejected by the project team through an administration page.   Further developments of the  DFHG project aim at implementing named entities recognition in the texts of Greek and Latin  fragmenta and in contributing to enrich the number of lemmata and inflected forms of Greek and Latin thesauri. The final goal of the project is to offer a new methodology based on digital and computational approaches to represent complex historical text reuse data. The  DFHG also offers an open collection of quotations and text reuses of Greek fragmentary historians. This resource provides the community of scholars and students with machine processable data for historical and computational research.  ",
        "article_title": "Digital Fragmenta Historicorum Graecorum (DFHG)",
        "authors": [
            {
                "given": "Monica",
                "family": "Berti",
                "affiliation": [
                    {
                        "original_name": "University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-12",
        "keywords": [
            "corpus and text analysis",
            "digital textualities and hypertext",
            "open content and open science",
            "linguistics",
            "English",
            "databases & dbms",
            "scholarly publishing",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " No cultural artifact lasts forever in a single, static, universal form. Texts change with editors and audiences, and tracking their changes challenges our ways of reading and understanding, heightening awareness of mediations, censorships, interventions, reinventions. Yet, telling the story of change over time in a variorum edition can be tedious, disruptive of joy in the reading experience, alien to the popular experience of “getting lost in a book.” As Brett D. Hirsch and Hugh Craig have discussed of Shakespeare editions, the print variorum has yet to be surpassed for its “highly developed economy of presentation.” (Hirsch and Craig, 2014: 26-27) Can we find a humane way—attuned to readerly impatience and excitement in the reading process—to build an edition that teaches a non-specialist how to read change over time, without sacrificing the standards of textual scholarship? Must digital methods that comprehensively document variation at their best mimic the printed critical edition in burying the experience of change over time to the footnote or the end space? Or, can we design a variorum edition that engages not only scholars but also fans and new readers in the dynamic complexity of a changing text? Since the early days of hypertext, Mary Shelley’s novel  Frankenstein has inspired experiments using electronic texts that invite new ways of reading. The novel about assembling a new monstrously superior life from dead bodies easily becomes, in the age of computers, a metaphor for attempting to unify a super-text complexly assembled from multiple variant pieces, since the process of computationally comparing texts necessitates editors to identify the smallest meaningful units to align and process. Like Victor Frankenstein's assembly of the Creature from multiple corpses, variorum editors aspire to animate a new composite reading experience. Such was the experiment to make the 1818 and 1831 editions of  Frankenstein simultaneously readable in the Pennsylvania Electronic Edition (PAEE) produced by Stuart Curran and Jack Lynch in the mid-1990s. Designed in a framework of small pieces like the hypercard novel popular at the time, the PAEE encouraged the reader to wander between two editions, explore hyperlinked ancillary material, and resequence the novel at will (see  http://knarf.english.upenn.edu/Colv1/f1101.html ). The transfer of this edition to Romantic Circles, a scholarly web resource in British Romanticism, conserved its textual data without exceeding its early intertextual vision of a complex, particulated reading experience with many reading paths always available (see  https://romantic-circles.org/editions/frankenstein).   Our project to design a new Variorum of  Frankenstein began with an encounter with these earlier digital editions. Originally called the “Pittsburgh Digital  Frankenstein project,” and inspired by the worldwide bicentennial celebrations of  Frankenstein’s first publication, we started work in October 2016 as a voluntary collaboration of Pittsburgh-area digital humanists with the Maryland Institute of Technology in the Humanities to update the  Frankenstein edition on Romantic Circles. The original goal was to revise and update the TEI code of the Romantic Circles edition to find ways to connect it with the   Shelley-Godwin Archive   's    (S-GA)    diplomatic edition of the    Frankenstein    notebooks . The mission shifted to design a variorum edition to collate five editions of the novel produced in the author’s lifetime, rather than simply to update and interlink the two most famous earlier editions. Our team from the University of Pittsburgh, Carnegie Mellon University, and the University of Maryland are now producing a new variorum edition of the novel encoded and published in TEI XML. Inspired in part by Barbara Bordalejo’s  Online Variorum of Darwin’s Origin of Species, the  Frankenstein Variorum incorporates annotations and visualizations that highlight change over time to guide readers to discover the \"hotspot\" moments of most intense variation in the novel.  The  Frankenstein Variorum is developing a scholarly edition that augments but does not replace previous print and digital editions. Our project pulls earlier editions into a new comparative perspective, to view the elaborately encoded S-GA edition of the manuscript through a variorum interface designed to illuminate where the manuscript contains material lost in later editions, and where later editions preserve or adapt its original content (see Fig.1). At DH2018 and at the Balisage Markup Conference in June and July 2018, Beshero-Bondar and Viglianti presented on the first phase of these efforts: preparing the differently-encoded source documents for computed collation with CollateX and building the foundational structure of the Variorum as a \"spine\" of standoff pointers into the separate edition files (Beshero-Bondar and Viglianti, 2018). This DH2019 paper addresses a new phase of work: developing a browser-based user interface that incorporates accessibility and responsive design features, inviting visitors to read specific editions, review variants, support edition-specific and cross-edition annotations, and visualize GIS data of the movements of characters, author(s) and co-editors in space and time.   One of the most complex elements of the  Frankenstein Variorum is its application of stand-off XML to point to rather than recreate the diplomatic TEI encoding of the 1816 manuscripts in the S-GA. Our interface should guide readers in navigating the complexity of the edition, how to find its most interesting moments of variation, and how to read the richly encoded textual information about the writing process available in the markup. To help guide the reader, the annotations team on the  Frankenstein Variorum seeks to balance contextual annotations (informing about sciences, gender and political contexts, literary allusions relevant to the novel) with a more innovative commentary designed specifically for navigating the Variorum: a set of “tunneling” annotations to highlight internal changes across the five editions. These annotations will augment previous scholarship on the persistence of various hands in the composition, emendation, and revision process. Incorporating these annotations with the variant apparatus poses a challenge for designing our interface that we look forward to discussing in our paper.  The  Frankenstein Variorum must support traditional, linear readings of individual editions along with nonlinear explorations of textual comparison through collation units and “tunneling” multi-edition annotations. To achieve this, we build our web interfaces directly on the TEI without transforming it to HTML first. We use CETEIcean, a JavaScript library for publishing TEI documents in the browser, to display the texts, visualize the linkages among  Frankenstein’s sources, and scholarly annotations to the TEI-encoded texts (Cayless and Viglianti, 2018). We aspire to the principles of Universal Design (UD) to increase access for those with visual limitations or who require a less cluttered and more responsive interface. We acknowledge the near-impossibility of addressing the variety of user needs and devices used to access web-based content (Godden and Hsy, 2016); yet, if we cannot meet the ideal, we can nevertheless draw on UD principles to expand our audience and inform our creation of vision-sensitive color palettes and adaptive stylesheets for those with difficulty differentiating particular colors or low visual acuity. The UI will be optimized for those using tablets and larger displays, applying Responsive Design to prioritize tablet delivery, limit navigation clutter, and highlight primary features including the text reader, variations table, and annotation window.   We are refining a method of signaling the level of “heat” or variance of a selection across editions, based on Levenshtein distance (or edit distance), the mathematical calculation of the minimum number of edits required to change one text string into another. We are aware of the limitations of Levenshtein for evaluating intensity of variation, since a change of a single letter or mark may totally change the meaning of a passage, such as the edit distance of 1 between the words “went” and “wept”. Nevertheless, as a convenient method for surveying intensity of edits on a macro scale, edit-distance values prove helpful. The tunneling annotations and overrides to the computed color codes will help to intervene where we discover the Levenshtein values to be less meaningful. Our color-coding system applies the maximum edit-distance based on pairwise comparison of each set of divergent witnesses at given variant locations in the novel, as represented in Figure 1 below.    Fig. 1: Prototype view of the Frankenstein Variorum interface. Features Depicted in Fig 1:  A single text reading window with identifying text and navigation elements for edition, volume, chapter, and page. (Left side) Alternate reading frame to review collation sections with navigation by collation unit and source. (Left side, alternate) Variations frame holds a table that displays the strings from each edition in a collation table. An annotation frame will display relevant annotations for the text selection, including those shared by multiple editions,  Color intensity indicates intensity of the variance across a human-mediated ten-value range. Words and passages are encoded using this scale.   At DH2019 we will share a more complete view of the interface, its annotation apparatus, and visualizations that will help guide the reader to sections where the most intensive editing took place. We look forward to conducting usability testing to enhance the interactivity and responsiveness of the digital interface in the coming months. We hope to provide a user experience that invites Frankenstein fans, new readers, and scholars alike to take pleasure not only in reading multiple  Frankensteins but also in reading the story of how this novel changed over time.  ",
        "article_title": "The Frankenstein Variorum Challenge: Finding a Clearer View of Change Over Time",
        "authors": [
            {
                "given": "Elisa",
                "family": "Beshero-Bondar",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh at Greensburg, United States of America",
                        "normalized_name": "University of Pittsburgh at Greensburg",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02b394754",
                            "GRID": "grid.447541.7"
                        }
                    }
                ]
            },
            {
                "given": "Rikk",
                "family": "Mulligan",
                "affiliation": [
                    {
                        "original_name": "Carnegie Mellon University, United States of America",
                        "normalized_name": "Carnegie Mellon University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05x2bcf33",
                            "GRID": "grid.147455.6"
                        }
                    }
                ]
            },
            {
                "given": "Raffaele",
                "family": "Viglianti",
                "affiliation": [
                    {
                        "original_name": "University of Maryland, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "digital humanities (history",
            "digital textualities and hypertext",
            "user experience design",
            "scholarly editing",
            "interface",
            "gamification",
            "theory and methodology)",
            "English",
            "text encoding and markup languages",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The CITE Architecture is a generic framework for identification, retrieval, and alignment of information about things humanists study. The challenge of a  generic framework lies in how it can handle the (literally) innumerable specific kinds of data likely to appear in any non-trivial digital library. CITE allows abstraction of data from specific encodings of that data, while maintaining  scholarly identity. This allows a digital library an open-ended ability to incorporate new formats or retrieval methods in a self-documenting plain-text serialization that maintains backwards compatibility. This paper will describe the implementation of  Discoverable Data Models and  Extended Text Property Types serialized in the CEX format and implemented in applications. Specific examples will be ( a ) geo-spatial data, in which a place can be represented by a URI to a gazetteer, a latitude/longitude pair, or a geoJson string, ( b ) textual data represented as plain-text, Markdown, or as a TEI-XML fragment, and ( c ) image collections, where the same image may be exposed as a JPG on a filesystem, via the IIIF-API, or as a DeepZoom file. CEX, the plain-text exchange format, can serialize collections and allow an application or service to discover these extended types or ignore them gracefully. All tools and data for these examples will be downloadable from GitHub.  Citation of Versioned Collections The acronym in the title of the CITE Architecture stands for Collections, Indices, Texts, and Extensions. ‘Texts’ are CTS-compliant texts, that is, texts canonically citable with machine-actionable CTS URNs because they implement the OHCO2 model, “an ordered hierarchy of citation objects.”(Smith and Weaver) ‘Indices’ are simple URN to URN relationships, subject-verb-object relationships akin to RDF triples, with the proviso that subject, verb, and object are canonically citable by machine-actionable URNs. ‘Collections’ are of data objects and may be ordered or unordered. The CITE Architecture allows collections of objects to be cited at many levels of abstraction and specificity.  urn:cite2:botcar:catesbySpecimen: A citation to a notional collection of of botanical specimens collected by Mark Catesby.   urn:cite2:botcar:catesbySpecimen.2018: A citation to a specific version of a collection of of botanical specimens collected by Mark Catesby.   urn:cite2:botcar:catesbySpecimen.2018:223 A citation to a specific specimen in a specific version of the collection.   urn:cite2:botcar:catesbySpecimen:223 A citation to a specific specimen in  any version of the collection. This recognizes that the  specimen is an object of study that might have different expressions in data.  A version of a CITE Collection is defined by its properties and their values. Each property is citable by URN:  urn:cite2:botcar:catesbySpecimen.2018.label: The  label property in a specific version of a collection.   urn:cite2:botcar:catesbySpecimen.2018.binomial: The  binomial property in a specific version of a collection.  An notional object is instantiated in a versioned collection by the sum of properties and their values:  urn:cite2:botcar:catesbySpecimen.2018.label:223 The  label property for object  223 in a specific version of a collection.   urn:cite2:botcar:catesbySpecimen.2018.binomial:223 The  binomial property for object  223 in a specific version of a collection.  Two versions of a notional collection need not have the same properties. Properties are  typed, at a very low level. CITE defines valid types as:  String (optionally with a controlled vocabulary) Boolean Number Cite2Urn CtsUrn Compositions of Scholarly Primitives The CITE Architecture defines  scholarly primitives: texts, or objects in versioned collections. Objects in versioned collections consist of a set of typed properties, with a very limited number of types.  This makes the CITE Architecture flexible and relatively simple: libraries for working with two types of URN, libraries for manipulating corpora of texts, and libraries for dealing with objects in collections, libraries for managing relations of URN to URN. All CITE data—texts, objects, and relations—can be expressed in plain text, and CEX, the Cite Exchange format, can serialize a digital library, or a part of a digital library, as plain text. CITE-aware services or applications can load data from CEX.   the Homer Multitext’s interactive web-application , the   Homer Multitext’s microservice  and   more specific applications exposing digital libraries for teaching , all read CEX files as their input.  Extensions I: Connecting to the Physical World The ‘E’ in CITE is “Extensions”, additional discoverable information providing richer interaction with the basic scholarly primitives. A CITE Collection can describe a collection of images. A very basic image collection might have the properties  label,  license, and  caption.  Clearly, while we can serialize this information easily as plain-text in CEX, resolving a URN to binary image data requires a connection to the physical world. A notional ‘image’ might be resolved to a JPG file, to data delivered by the IIIF API, to a DeepZoom file, or to any combination of these. CITE and CEX solve this problem by means of “discoverable data models”, additional data (expressed as generic CITE collections) that can identify specific collections of images as being served by one or more binary image services. In this case, an additional Binary Image Service collection associates a collection with: A type of image service (JPG file, IIIF-API, DeepZoom) A URL to a service hosting images from the collection Filepath information necessary to resolve an image’s URN to files on the server. A working example of this is   the Homer Multitext’s interactive web-application . The   CEX of the HMT’s data release  identifies image collections as being expose both as DeepZoom files and via the IIIF-API. The web-application takes advantage of both of these to provide thumbnail views and interactive zooming views.  Extensions II: Different Expressions of Textual Data An object in a version of a collection might have a property of type  string, and that is easily discoverable with the basic CITE tools. But of course, a  string might be plain text, Markdown, some form of XML, or some other encoding. It is easy to imagine a project publishing a version of a collection of comments as plain-text, and subsequently publishing a new version that adds some markup to those comments.  Because the CITE2 URN allows identification of notional collections, versioned collections, individual properties in versioned collections, in each case across the collection or filtered by an object’s identifier, we can expose additional information about the nature of a property of type  string.  By means of a discoverable data model, just as we associated whole collections of images with different binary image services, we can associate properties with different encodings, without losing scholarly identity. This paper will demonstrate a notional Collection of comments on the text of Herodotus, expressed in three different versions: one with comments in plain-text, one with comments formatted as Markdown, and one with comments formatted as TEI-XML fragements. These  discoverable extended string property types are ignored by any application that is unaware of them, but exploited for human display by applications that are aware of them.  As a working example of  discoverable extended string property types, we can point to a CITE Library of lexicon data, based on the openly licensed XML versions of the   Liddell-Scott-Jones Greek Lexicon , the   Lewis & Short Latin Dictionary , and   Strong’s Hebrew Lexicon . Data for each of these serialized in a CEX file, and served through a Microservice. Querying the service shows that the lexicon entries, of type  StringType can be read as plain-text:   http://folio2.furman.edu/lex/objects/urn:cite2:hmt:lsj.chicago_md:n2389 . But because this collection identifies that property as extended by  Markdown, an aware application can process the plain-text expression and apply formatting:   http://folio2.furman.edu/lsj/index.html?urn=urn:cite2:hmt:lsj.chicago_md:n2389 .  Extensions III: Different Expressions of Real World Data Digital Gazetteers such as the Pleiades project[^pleiades] have solved the problem of scholarly identity across historically diverse placenames, so ‘Naulochon’, ‘Smyrna’, and ‘Izmir’, different names for the same place, are canonically citable as  https://pleiades.stoa.org/places/550771. But for very sound technological reasons, we might want to express the location of Izmir by  550771, by its full Pleiades URI, by  38.440912, 27.14781, by  27.14781, 38.440912, by  {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [27.14781, 38.440912 ] } } ] }, or by  <?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\"><Document><Placemark><ExtendedData></ExtendedData><Point><coordinates>27.14781,38.440912</coordinates></Point></Placemark></Document></kml>.  A collection of, for example, “ancient places mentioned in Herodotus”, as a publication should separate the notional scholarly objects—'Sardis', ‘Athens'—from any particular technology for locating those objects on a map. The CITE extended string property types allows different versions of such a collection to record locations in any of a variety of formats, or to mix formats within a single expression,  e.g. latitude and longitude for a simple point, with geoJson for describing regions. The presentation will include a demonstration dataset illustrating this.  Conclusion CEX, as a line-based, plain-text serialization of diverse data—texts, collections, relations—is a convenient, future-proof, and open means of data exchange for small projects and large project. ( The  Homer Multitext 2018g release is published as a 73,000 line CEX file. ) With  discoverable-data-models and  extended-text-property-types, CEX can serve data in a variety of current and future formats; these formats are discoverable by applications that, but degrade gracefully back to generic, plain-text in generic CITE applications. The talk will point to example CEX files and applications, on GitHub, demonstrating these capabilities with scholarly data.  ",
        "article_title": "Discoverable Data Models and Extended Text Properties in the CITE Architecture",
        "authors": [
            {
                "given": "Christopher William",
                "family": "Blackwell",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America; The Homer Multitext, Center for Hellenic Studies of Harvard University, United States",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            },
            {
                "given": "David Neel",
                "family": "Smith",
                "affiliation": [
                    {
                        "original_name": "College of the Holy Cross, United States of America; The Homer Multitext, Center for Hellenic Studies of Harvard University, United States",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "linking and annotation",
            "English",
            "computer science and informatics",
            "digital archives and digital libraries",
            "data models and formal languages",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "    Fig. 1: A relational perspective on the Hausmann collection centered around a selection. Link to the prototype: https://uclab.fh-potsdam.de/hausmann   Akin to digitization processes, cultural heritage institutions are digitizing their inventories and are looking for approaches to devise meaningful digital representations of their collections. In contrast to traditional systems that use keyword search as the main mode of accessing collection items, usually requiring familiarity with the collection, a growing body of work demonstrates how visual interfaces can support more exploratory and serendipitous modes of accessing collections (Dörk et al., 2011; Thudt et al., 2012; Walsh and Hall, 2015; Whitelaw, 2015). Interactivity is often used as a strategy to traverse the extent and complexity of information visualizations (Cockburn et al., 2008; Van Ham and Perer, 2009; Meirelles, 2013). Despite the growing variety of visualizations of cultural collections (Windhager et al., 2018), researchers like Drucker argue that many of them are transferred uncritically to the humanities, ignoring fundamental aspects of humanistic research, such as interpretation, ambiguity or uncertainty, and the specificity and situatedness of a given point of view (Drucker, 2011; Drucker, 2016). While visualizations that focus on providing one overview are suitable for the display of patterns across entire datasets (Shneiderman, 1996), they often fall short of visualizing relations and similarities at the level of individual items. Additionally, the limitation of using only one view implies a loss of information and may lead to distorted perceptions. Referring to the relational importance of an individual’s social context, Latour et al. (2012) conceptualize the individual relations in large social networks as monads. Here, monad refers to a conceptual point of view that defines an entity through its many particular relations to all other entities of a dataset. Monadic exploration is a visualization technique for relational information spaces, in which each node can be selected as a navigation point to trigger changes in flexible layouts (Dörk et al., 2014). Following the premise that for cultural data the individual relations can be just as important as an overview of the whole, we investigate the potential of relational perspectives in the visualization of cultural collections. We engaged in an iterative prototyping and research process in close collaboration with the  Berlinische Galerie – museum of modern art, using their  Raoul Hausmann collection as a case study (see Fig. 1).  Based on the exchanges with our collaborators, a co-design workshop and prior work on collection visualizations, our research ambitions can be summarized by these design goals: 1. Provide multiple, flexible views on the data. 2. Reveal the individuality and diversity of the artifacts. 3. Promote open exploration and serendipitous discovery. 4. Expose the temporal context of artifacts.  5. Acknowledge uncertainties and gaps in the data.   Taking a relational perspective on a collection To examine the potential of relational perspectives in comparison to overviews, the design of the visual interface contrasts two approaches: 1) an  overview of the collection, and 2)  multiple perspective views, each centered around one artifact.  The  overview serves as a landing page providing a comprehensive representation of all artifacts arranged by media type/genre and contextualized by visualizations of the temporal and social relations. Although we are particularly interested in relational perspectives, we acknowledge the benefits of an overview as an entrance point to a collection (Shneiderman, 1996; Whitelaw, 2015). The overview consists of three connected visualizations (see Fig. 2), a vertical timeline (left), artifacts displayed by genre (center), and a people/relations diagram (right).  In the center, all artifacts are organized in small thumbnails by genre of the respective items. Hovering over an item displays a bigger thumbnail. Titles of the genres also function as a legend for the colors of the other visualization parts.     Fig. 2: Overview: a) top-left: no selection, absolute timeline format; b) top-right: person selected, absolute timeline format, hovering contextual information; c) bottom-left: year and person selected, relative timeline format, hovering an element; d) bottom-right: keyword selected, relative timeline format In the timeline, which supports absolute and relative scales, each year inside the time frame of the collection (+undated) is visualized by a horizontally stacked bar consisting of small aligned rectangles, one for each artifact in the color of the corresponding genre, representing the total quantity of each genre per year. Small icons next to a year reveal additional contextual information. The arc diagram displays all persons and relations that are linked to artifacts within a selected time frame. Each person that is connected to one of the items is represented by a circle, whose size represents the total number of artifacts connected to the respective person in the current selection. The ordering, font size, and size of the nodes depend on the number of related artifacts. Selections of a year in the timeline (see Fig. 2b), a person in the relations diagram (see Fig. 2c), or a keyword via the search bar (see Fig. 2d) trigger highlights or activate filters. A click on an artifact triggers a switch to the  perspective views, starting with the attribute-based perspective (see Fig. 3a). The perspective views are primarily based on individual, yet relational viewpoints anchored by a selected artifact and its unique connections with other items. The prototype offers three views, two faceted perspectives and a temporal perspective, each enabling access to a detail view (see Fig. 3d).  The core functionality of the two faceted perspectives is to display related items in order to stimulate open-ended exploration and increase the likelihood of serendipitous discoveries. While the  attribute-based perspective (see Fig. 3a) displays general metadata of the object, the  content-based perspective (see Fig. 3b) is based on manually authored and automatically extracted keywords.      Fig. 3: Perspective views: a) top-left: attribute-based, b) top-right: content-based; c) bottom-left: temporal; d) bottom-right: detail view Both views are structured around a selected item, displayed in the center of the page along with an image and basic information. Other data attributes/keywords are displayed once above and once below the selected item; related artifacts sharing the same attribute/keyword are displayed above/below these labels, roughly ordered by their temporal distance to the selected object. All artifacts created in the same year as the selected item or in later years are displayed above the selected element, all artifacts that were created in the past are displayed below, followed by undated elements. In addition to the ordering, temporally close elements are larger than those that lie further in the past or future (see Fig. 4). Additional encodings help to identify genres (colored circles), the number of shared attributes with the selection (number of rings), duplicates in other attribute columns (lines), and uncertainty (striped texture). The selection of another artifact causes a rearrangement of the visualization based on the artifacts and attributes related to the new selection, resulting in many unique visual and semantic fingerprints.      Fig. 4: Close up of a faceted perspective view     Fig. 5: Close up of temporal perspective view In contrast to faceted perspectives, the aim of the  temporal perspective is to induce iterative navigation from one collection item to the next along temporal vicinity by revealing precise dating of artifacts, including uncertainties (see Fig. 3c & 5). The selected element is positioned on the left side of the interface, marking the moment in time it was created. All remaining elements of the collection are represented in a scrollable vertical timeline around the selection. While a dot indicates a precise point in time and a bar represents an uncertain time interval, dating based on uncertain assumptions is marked by a texture (see Fig. 5).     Conclusion Based on the observation that many common visualization types cannot represent the complexity and uniqueness of individual artifacts of cultural collections, we developed the idea of multiple visualizations acting as relational perspectives. Findings from a co-creation workshop, frequent exchanges with our collaborators, and a thorough literature review indicated that such individual perspectives on artifacts may offer a richer and more diverse approach to cultural collections. To examine the potential of such an approach, we carried out a case study using the Hausmann collection with the intention to put focus on the individuality of each collection element by providing a variety of visualizations, in particular an overview of the entire collection and three relational perspective centered around individual artifacts.  Usage logs suggest that, while many visitors actively used the perspective views as exploration tools, the provision of an overview still proved to be useful for others.  The present research suggests that a deliberate consideration of a diversity of situated views holds promise for the digital exploration of cultural collections. By displaying relational perspectives of each individual artifact and by acknowledging uncertainties we propose an approach to promote exploration, user-dependent interpretations, and furthermore we emphasize the consideration of perspective in future projects.  The conceptual ideas are very much open for further development, but we hope that this research contributes to the exciting research carried out by a growing transdisciplinary community at the intersection between design and the humanities, where critical, aesthetic, and functional considerations converge.  ",
        "article_title": "Relational Perspectives as Situated Visualizations of Art Collections",
        "authors": [
            {
                "given": "Mark-Jan",
                "family": "Bludau",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marian",
                "family": "Dörk",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Heidmann",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-02",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "user experience design",
            "libraries",
            "interface",
            "gamification",
            "museums",
            "theory and methodology)",
            "English",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The introduction of computerized methods for philological analysis is compounded by the richness, complexity and multidimensional nature of literary texts. As it has been coined by Ju.M. Lotman, the arts should be considered as a secondary modelling system, whilst the natural language is a primary one. (Lotman 1967). Computational models allow to extract and analyse linguistic features of the text - POS tagging, syntactic structures, word frequency, and semantic domains of word classes. Meanwhile the most important elements of textual poetics remain outside the scope of application of these tools. As a result, the most rapid development of computational methods for literary study is observed in the field of computational stylistics and stylometry (Burrows, 2002), (Rybicki, 2006), (Eder, 2015), (Franzini et al., 2018), and thematic modelling (Jockers, 2015), (Schöch, 2017), which may be deployed using bag of words approach only without any specific textual mark-up.  It seems that one of the most important reasons why distant reading methods are still not regarded as a mainstream in literary scholarships and are often looked at with suspicion by traditional philologists is their extremely difficult access to textual complexity, the layers of the secondary modeling system. In fact, even in the cases of modelling some complicated phenomena such as social networks of characters or plot elements extraction, the studies are more concerned on engineering but not on the research issues, which means elaboration of computational but not literary analysis methods.  This paper aims to introduce a new approach to the task of capturing textual complexity. We use five methods to model the character system in a novel, each one is aimed to discover one of the layers of this system. The combination of these layers gives as a result a complex view on the novel's composition enriched by computationally obtained data, quantitative and statistical metrics and graphical schemes and networks We apply slylometric and alternative non-lexical analysis to characters' direct speech, two alternative methods of network analysis to model characters interactions and clustering method for comparison of portrait descriptions in Leo Tolstoy's “War and Peace\". The Tolstoy's great novel which counts hundreds of characters among which several dozen may be viewed as prominent, serves a perfect material for such a study. We claim that with the help of  the complex layer analysis we can reveal some new structural constituents of the novel composition, that could not be captured by traditional (close reading) interpretations of Tolstoy's poetics.    Preliminary preparations  The complex layer analysis of the character system requires thorough and precise mark-up. All automatic or semi-automatic mark-up has been checked and corrected manually if needed. First of all, all the characters have been encoded with TEI labels. This procedure was also important as far as the characters may be referred to by several different names (сf. Pierre, Bezuchov, Petr Kirillovich) and anaphorical pronouns. Secondly, all the dialogues in the novel have been identified and connected to speakers (characters) and their TEI labels. Finally, all the portrait descriptions have been elicited with the help of semantic mark-up, borrowed from the National Corpus of the Russian Language (Toldova et al., 2008). Next stage of the portraits mark-up involved encoding one of the four types of Tolstoy's way of designation of his characters' appearances - metaphorical, emotional, portrait, and value expression. Each sentence of the description was also supported by the integral sentiment assessment (positive, negative, oxymoron and neutral)    Methodology  Three major traits of literary characters have been studied with the help of computational means: speech, portrait and social interactions. Two methods have been used for character speech analysis. The first method exploits basic stylometric analysis (delta). It measures the distance between direct speech of different characters by calculating the distribution of top keywords for speech sentences of every prominent character. The stylometric method refers to the layer of topical connections between the characters in the novel. The main oppositions which are set by this method are the oppositions between men and women, and between the Moscow and Saint-Petersburg circles.  The second method presents an alternative to the stylometric approach. It compares non-lexical characteristics of the direct speech sentences, such as ratio of words and punctuation in a speech sentence, exclamation and question marks, frequency of discourse words and readability score. These parameters differentiate the characters by their manner of speech, in particular distinguishing \"oral\" and \"written\" types of sayings as a principal opposition. Each considered character has been defined by a vector, which accumulates the mean value of all the parameters. The clustering model brings together the vectors with closest distance. The PCA analysis shows that this layer is sensitive to family similarity, as it opposes Natasha and Nikolay Rostovs to Andrey Bolkonsky and his sister Princess Mary.  The analysis of portraits was also based on vector clustering. This method builds the vectors for each prominent character out of two metrics: the first one combined the normalized frequency values of each type of portrait description, the second referred to the frequency values of four types of sentiment assessment. The hierarchical tree of this layer brought together the vectors of the main pairs of the novel: Pierre and Natasha, and Nikolay and Marya. Surprisingly the analysis also revealed intrinsic similarity between the two \"bad guys\" of the novel: Napoleon and Dolokhov. This layer concerns concealed parallelism in Tolstoy’s way of thinking and describing his characters.  The social interaction has been measured by two semantic networks built on different bases. The first method sets the connection between the characters that talk to each other. This network reveals the main communities of the novel, the characters that have intensive communications cluster together. One could suppose that the layer captured by this network reflects the family dramatical part of the plot.  The second method sets as a connection the fact of two characters mentioned together. The two networks differ in a crucial way, in particular within the war parts, where the dialogue communications happen less often then in the peaceful parts. This layer stands for the epic part of the novel, thus Napoleon and Kutuzov form one cluster here, though they never communicate and are at utmost distance from each other on the previous network.    Discussion  The five layers of the character system in ”War and Peace\" are defined by the five different methods of computational analysis of literary text. All the methods are in no way new to digital humanities: stylometry, clustering, PCA, corpus and network analysis. The important conclusion needs to be emphasized: all the methods when applied to clean, well marked up data give different results and lead the researcher to different interpretations. At the same time the masterpiece contains all the interpretations within itself. To say more the inner architecture of the text supports and interconnects in a very sophisticated way all the interpretative layers. This point may be proven by the following example. If we consider the whole novel excluding the epilogue, we may see that the speech and network layers group the five main characters of the novel (Natasha, Pierre, Andrey, Nikolay and Princess Mary) differently, but no grouping reflects the romantic relationships of the novel.     Figure 1. Connections/proximity of the five main characters, revealed by the 4 layers of speech and interaction analyses of the novel without the epilogue: no romantic relations reflected on these layers     Figure 2. Connections added by the analysis of the epilogue (dashed lines) reveal the romantic connections of the main characters) It is in the epilogue where the whole construction gets the stability and the romantic attractions of the characters are openly demonstrated to the reader. The Prince Andrey, though not alive in the epilogue, is still mentioned a lot in the talks of Natasha and Pierre, thus the romantic triangle which has never been defined explicitly during the novel is resolved in the epilogue. Amazingly, the fifth layer not mentioned above, which reflects the way Tolstoy depicts and presents his characters, sets the connection between the members of the two happy families of the epilogue from the very beginning.   ",
        "article_title": "The Complexity of Character-building: Speech, Portraits, Interactions in Leo Tolstoy's \"War and Peace",
        "authors": [
            {
                "given": "Anastasia",
                "family": "Bonch-Osmolovskaya",
                "affiliation": [
                    {
                        "original_name": "National Research Unversity Higher School of Economics Moscow, Russian Federation",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            },
            {
                "given": "Skorinkin",
                "family": "Daniil",
                "affiliation": [
                    {
                        "original_name": "National Research Unversity Higher School of Economics Moscow, Russian Federation",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-06",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "data mining / text mining",
            "English",
            "network analysis and graphs theory",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Among the performing arts, opera is undoubtedly the one that presents the greatest complexity, since it is made up of a variety of 'texts' that differ in nature (literary, musical, choreographic and visual), but which are closely coordinated.  As a result, documents relating to opera come in multiple forms (verbal, musical, iconographic, sound, and visual documents) and cover the entire life cycle of an opera: the verbal texts (librettos), the musical score, materials for the opera’s performance in the theatre (contracts, scene sketches, etc.), and documents that relate to the performance itself (audio and video recordings, stage photos, reviews) (Bianconi et al., 2004). The conservation of these heterogeneous sources is usually entrusted to different institutions (libraries, archives, museums) according to the type of material (books, administrative documents, artistic objects) and catalogued using methodologies that vary according to the type of material. The documentation relating to an opera is thus to be found in different institutions and is described in different information systems which are often not compatible. The material is therefore not always easily accessible for consultation. Our initial approach was to adopt Functional Requirements for Bibliographic Records (FRBR) (IFLA Study Group, 1998) as a conceptual solution for a unified management of documents and their contents (Pompilio et al., 2005). The result is a highly specialized model in which information is collated and analysed in depth. But, to fulfil the functional perspective prescribed by FRBR, cataloguing must take account of knowledge of the domain, and the interests and expertise of different categories of users. One of the aims of the Corago LOD   The project web site address is: http://corago.unibo.it/lod  project was thus to demonstrate that by adopting DL it would be possible to express domain knowledge at different descriptive levels within a single coherent semantic model (Bonora and Pompilio, 2019).  The FRBR’s conceptual hierarchy is defined by the three core classes: Work, Expression and Manifestation. In the domain of opera, these three classes are represented by the headings Opera, Event and Document, which become entry points for accessing information about repertoire, chronology and documentary sources.    Diagram 1- From Works to Performances and Librettos The path that leads from the conceptual content of operas to its two expressions, namely performances and librettos, can be expressed using the FRBR Object Oriented (FRBRoo) model. Moreover, by transposing knowledge from the original relational model to RDF, using CIDOC Conceptual Reference Model (CIDOC CRM) and FRBRoo as main reference ontologies (Bekiari et al., 2015), it was possible to overcome constraints deriving from the original implementation of FRBR by using a proprietary model such as data interoperability (Le Boef, 2012). Access to information contained in catalogues, archives or directories has often been limited by a lack of interoperability provided by the information system used. From the users’ point of view, a knowledge based facility such as Linked Open Data (LOD) lowers the access threshold because alignment is at a basic semantic level (Bizer et al., 2009). Increased interoperability also leads to a wider dissemination and circulation of information. As this information becomes part of the knowledge held in other systems, its availability would be guaranteed even beyond the capabilities or aims of the original project. As a result, LOD interoperability both facilitates \"universal\" access to digital resources and is a viable strategy for preserving information in the long term. Conversely, the adoption of formal ontologies often produces data structures that are complex and cannot be browsed in a user-friendly way. The project faced two major factors of complexity introduced by RDF data representation: the deeper level of analysis and the morphological intricacy of the graph. The analyticity is directly linked to the fine-grained accuracy of the ontology adopted for the definition of the dataset, while the complexity of the graph structure derives both from the fine-grained conceptual model and the amount of information that the repository contains. As a result, providing synthetic and effective representations of data to end users becomes challenging. The solution presented exploits some basic mechanisms of the DL such as generalization and inheritance. Through generalization it is possible to build progressive levels of abstraction on top of basic RDF triples. These intermediate layers act as semantic lenses which are capable of focusing the functional requirements expressed by users. While the traditional design of cataloguing applications based on RDBMS requires that data extraction and representation tasks follow a business logic, the transition to semantic models allows intermediation to take place on a conceptual level. An intermediate layer can be formally introduced within the semantic model of the repository to provide a user-friendly representation of the domain entities. This layer would be designed by the domain expert according to user needs.    Diagram 2 - Multi layer semantic architecture The same approach has been adopted with regard to access to domain knowledge. The goal in this case is to make information from diverse sources accessible through search criteria which start go from plain text level up to criteria specific to the field of music. The aim of the project was also to identify semantic references between results and search criteria. We realised this goal by introducing a full-text semantic index within the dataset. Indexing is based on a specific class of properties that identifies the relationships between the core entities of the domain and terms that would be used to search each class of entity. The system can thus provide the reference context that produced the results displayed. This allows users to navigate the results in a more controlled and accurate way. This kind of semantic indexing becomes a tool for managing both the complexity of the domain, and for making with information more accessible in a distributed environment such as the LOD ecosystem. Finally, in our experience, identifying the layers of abstraction used when searching and navigating knowledge constitutes a kind of conceptual modelling which, since it is part of the semantic model of the knowledge base, also becomes part of the competence of the domain. In other words, modelling the way opera history is being accessed and represented becomes a possible way of reducing the intrinsic complexity of the domain as experienced by users. ",
        "article_title": "E Pluribus Unum: a Uniform DL Solution for Historical Data Management, Archiving and Exploitation of Opera",
        "authors": [
            {
                "given": "Paolo",
                "family": "Bonora",
                "affiliation": [
                    {
                        "original_name": "Department of Cultural Heritage, University of Bologna, Italy",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            },
            {
                "given": "Angelo",
                "family": "Pompilio",
                "affiliation": [
                    {
                        "original_name": "Department of Cultural Heritage, University of Bologna, Italy",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "musicology",
            "English",
            "library & information science",
            "semantic web and linked data",
            "ontologies and knowledge representation",
            "digital archives and digital libraries",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The medieval civilization can only be investigated by means of the study of traces that have survived to our times. The best source of our knowledge is the texts, preserved in huge quantity and variety. Written mainly in Medieval Latin, they have not benefited from recent advances in computational linguistics and digital humanities in general. The paper briefly presents and reports on the early development of the project  VELUM – Visualisation, Exploration et Liaison de ressources innovantes pour le latin médiéval.  The ANR-funded project (2018-2022) is a first step towards an innovative digital environment for the study of the language and culture of medieval Europe. It has a challenging and ambitious goal: to build foundations for empirical research in medieval culture, language and history, by providing an appropriate environment for textual source analysis. We aim at building, firstly, a large (100 M tokens) balanced corpus of Medieval Latin texts composed between 500 and 1500 AD all across Europe. In order to assess the population that we would like our corpus to reflect, we rely mainly on the most comprehensive list of Medieval Latin texts, namely the  Index Scriptorum Mediae Latinitatis (Blatt, 1973; Bon, 2005).  A representative corpus should make it possible to avoid the bias that affect existing text collections, resulting from uneven and unjustified distribution of sources of different date, genre and place of composition. For example, medievalists have at their disposal big digital text collections, but none has been conceived in order to give an idea of the whole production in Medieval Latin in its variety (see References: Text Corpora). They tend to focus either on specific domains (e.g. theological texts vs. diplomas and charters) or territories (e.g. Catalan vs. Polish texts). As to the chronological variation, the texts written before 1200 are generally far better represented than the texts of the Later Middle Ages. The corpus will also attempt at remedying another shortcoming of existing text collections: since they usually are not balanced, they do not lend themselves to any sound statistical analysis (both syn- and diachronic). Apart from wide geographical and temporal coverage, the corpus will also reflect the variety of genres practised in the Middle Ages, as well as the functional richness of the medieval textual culture. In order to enable automatic processing, the texts will be annotated with part-of-speech, lemma, time and place labels. The compilation and annotation of the corpus, albeit extremely important, will be only a first step of the project. Secondly, a corpus search engine will be built with the help of the CQP-Web software. The users will be able to query the texts and benefit from their rich linguistic annotation through a user-friendly interface. Thirdly, the project aims at developing a set of efficient statistical analysis and data visualisation tools that researchers would embed in their own workflows. Written mostly in R, scripts, programs, wrapper functions will allow for advanced study of Medieval Latin vocabulary, but will be applicable to other languages as well. Both the texts and the tools will be made freely available to the scientific community through the project’s website and public code repositories. After the brief presentation of the project’s goals, we will report on the workflow and the first results of the project that has started in March 2018. During less than two years, we intend to cover every aspect of corpus compilation from digitizing paper edition to linguistic annotation of the machine-readable texts. The first stage started with intensive work of cleaning and structuring the texts; some of them already are available in interoperable formats, but many are not. Our first work consisted, then, in selecting and retrieving about 1500 scholarly editions of Medieval Latin texts, among others from the Web. Image files which drastically differed in quality needed to be cleaned and standardized. To that purpose we employed a set of optimization scripts and tools such as ScanTailor. The TIFF files were next sent to the OCR engine. We chose the Tesseract-4 as our OCR engine for both its accurateness and performance. The quality of the OCRed text was optimized through error profiling and selective manual proofreading with the latest version of the Post Correction Tool software. The PoCoTo software addresses the problem of recurrent OCR errors by making easier both their detection and batch correction by even less technically-oriented users.    Figure : Using PoCoTo for OCR correction  The next phase of this early development will consist mainly in separating Latin from non-Latin text. The text blocks will be automatically annotated with ‘Latin’ or ‘Non-Latin’ labels by using a simple classifier. We will next employ Transkribus to verify the results of the automatic classification and to manually remove redundant text blocks such as editorial introductions, footnotes, indexes etc. After that, the texts will be exported to the TEI-XML format and annotated with metadata. Once a rough representativeness and balance of this corpus is achieved, the texts will be lemmatized and PoS-tagged using the Treetagger with the parameters developed for Medieval Latin during the “Omnia” Project ( http://glossaria.eu).  ",
        "article_title": " VELUM : Towards Innovative Ways of Visualising, Exploring and Linking Resources for Medieval Latin  ",
        "authors": [
            {
                "given": "Bruno",
                "family": "Bon",
                "affiliation": [
                    {
                        "original_name": "IRHT (CNRS), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Renaud",
                "family": "Alexandre",
                "affiliation": [
                    {
                        "original_name": "IRHT (CNRS), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Krzysztof",
                "family": "Nowak",
                "affiliation": [
                    {
                        "original_name": "IJP (PAN), Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Laura",
                "family": "Vangone",
                "affiliation": [
                    {
                        "original_name": "IRHT (CNRS), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-28",
        "keywords": [
            "corpus and text analysis",
            "English",
            "text encoding and markup languages",
            "philology",
            "medieval studies",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Although it is well known that word meanings evolve over time, there is still much to discover concerning the causes and pace of semantic change . In this context, computational modelling can shed new light on the problem by considering at the same time a large number of variables that are supposed to interact in a complex manner. This field has already given birth to a large number of publications ranging from early work involving statistical and mathematical formalism (Bailey, 1973 ; Kroch, 1989) to more recent work involving robotics and large-scale simulations (Steels, 2011).  We consider that semantic change includes all kinds of change in the meanings of lexical items happening over the years. For example, the word  awful has dramatically changed in meaning, moving away from a rather positive perspective equivalent to  impressive or  majestic at the beginning of the nineteenth century to a negative one equivalent to  disgusting and  messy nowadays (Wijaya and Yeniterzi, 2011).   In this work, we address the question of semantic change from a computational point of view. Our aim is to capture the systemic change of word meanings in an empirical model that is also predictive, contrary to most previous approaches that meant to reproduce empirical observations. We will first describe our methodology, then the experiment and our results, before concluding.    Proposed methodology Our goal is to train a model representing semantic change over a certain period and, from there, to predict potential future semantic changes. The evaluation will thus be based on the observation of the gap between actual data and predicted data.  Our model is based on two main components:   1- Diachronic word embeddings representing the meaning of words over time-periods, following Turney and Pantel (2010). Word embeddings are known to effectively represent the meaning of words by taking into account their surrounding contexts. The representation can be extended to include a diachronic perspective: word embeddings are first trained for each time-period and then aligned temporally, so as to be able to track semantic change over time, see Fig. 1. For our study, we used the pre-trained diachronic word embeddings released by Hamilton et al. (2016): for each decade from 1800 to 1990, a specific word embedding is built using the word2vec skip gram algorithm. The training corpus used to produce these word embeddings was derived from the English Google Books N-gram datasets (Lin et al., 2012), which contain a large number of historical texts in many languages (we used 5-grams with no part-of-speech tags). Each word in the corpus appearing from 1800 to 1999 is represented by a set of twenty 300-dimensional vectors, with one vector per decade.     Figure 1. Two-dimensional visualization of the semantic change in the English word “ cell” using diachronic word embedding. In the early 19th century the word cell was typically used to refer to a prison cell, hence the frequency of  cage and  dungeon in the context of  cell in 1800, whereas in the late 19th century its meaning changed as it came to be frequently used in a scientific context, referring to a microscopic part of a living being (see  protoplasm,  ovum, etc. in the 1900 context).   2- Recurrent Neural Networks (RNNs) modelling semantic change itself. RNNs are known to be powerful at recognizing dynamic temporal behaviours in diachronic data (Medsker and Jain, 2001). In this experiment, we used the word embeddings representing the semantic space of each decade from 1800 to 1990 as input for the RNN, and from this we predicted the embedding corresponding to the 1990-1999 decade. Our RNNs have a long short-term memory (LSTM) and are implemented through Tensorflow.   To explore different scenarios, we ran several experiments with different vocabulary sizes (restricted to the 1,000, 5,000, 10,000, 20,000 and 50,000 most frequent words). We used the stratified 10-fold cross-validation method to estimate the prediction error (i.e. 90% of the words were used for training, and 10% for testing). The overall prediction accuracy is taken as the average performance over these 10 runs.    Experiment, Results and Discussion To get an overall estimation of the prediction accuracy, we compare each predicted embedding to the ground truth obtained from real data. Though it is impossible to predict exactly the vector corresponding to a given word “w”, as we are working in a continuous 300-dimensional space, one can assess the accuracy of the predicted meaning by extracting the closest vectors, i.e. the closest neighbours of a given word over time.  If the word “w” is actually the nearest semantic neighbour to the predicted vector, then it is considered to be a correct prediction. Otherwise, it is considered to be an error (a false prediction). The results are summarized in Table 1.    Vocabulary Size Accuracy   1000 91.7%   5000 86.1%   10000 71.4%   20000 52.2%   50000 25%    Table 1. Results of prediction accuracy measured for different vocabulary sizes. The training and the prediction using the RNNs model were performed on embeddings derived from the Google N-gram corpus.  The results show that the model can be highly effective at capturing semantic change, and can achieve a high accuracy when predicting the evolution of word meaning through distributional semantics. As one can see from Table 1, the model was able to achieve 71.4% accuracy when trained and tested exclusively on embeddings based on the 10,000 most frequent words of the corpus. The model was even able to correctly predict word embeddings for words that have radically changed their meaning over time such as  awful,  nice,  cell and  record (Wijaya and Yeniterzi, 2011).   The results also show better results when using smaller vocabulary sizes containing top frequent words. The decrease of performance with large vocabularies is due to the fact that infrequent words do not have enough occurrences to derive meaningful and stable enough contexts so as to observe reliable evolutions. It is thus fundamental to use large corpora for this kind of experiments, but also to adapt the size of the vocabulary to the size of the corpus.    Conclusion We have proposed a new computational model of semantic change. Although this model is (partially) successful at representing this evolution, it can still appear to be too simple compared to the complexity of language change in general and semantic change in particular. For now, it may remain hard to understand precisely how this type of computational modelling can be combined with more traditional methods of linguistic analysis. However, we strongly believe that such empirical approaches based on diachronic vector-based representations can considerably help to refine and clarify theoretical insights on the foundations and mechanisms of semantic change, as well as provide an accurate empirical evaluation.   Acknowledgements This work is supported by the project 2016-147 ANR OPLADYN TAP-DD2016. Thierry Poibeau is also supported by the CNRS International Research Network “Cyclades”. Our thanks go to the anonymous reviewers for their constructive comments.  ",
        "article_title": "A Predictive Approach to Semantic Change Modelling",
        "authors": [
            {
                "given": "Mohamed Amine",
                "family": "Boukhaled",
                "affiliation": [
                    {
                        "original_name": "Laboratoire Langues, Textes, Traitements informatique, Cognition (Lattice, CNRS, ENS & Université Paris 3)",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Benjamin",
                "family": "Fagard",
                "affiliation": [
                    {
                        "original_name": "Laboratoire Langues, Textes, Traitements informatique, Cognition (Lattice, CNRS, ENS & Université Paris 3)",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thierry",
                "family": "Poibeau",
                "affiliation": [
                    {
                        "original_name": "Laboratoire Langues, Textes, Traitements informatique, Cognition (Lattice, CNRS, ENS & Université Paris 3)",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "corpus and text analysis",
            "semantic analysis",
            "artificial intelligence and machine learning",
            "linguistics",
            "data mining / text mining",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The CWRC-Writer XML/RDF editor is the centerpiece of the Canadian Writing Research Collaboratory (CWRC) platform for the production, hosting, and dissemination of digital humanities scholarship. In development since 2011 and launched with the platform in 2016, the browser-based editor has reached maturity and stability. Well prior to this, the team had begun strategizing towards sustainability. We outline this strategy while highlighting features of the editor. Compared to some outcomes of digital dumanities tool building – such as gaining new insights into one’s own research – the effort of turning a tool into a sustainable, generalized service is less glamorous, more laborious, and less acknowledged. Tool-building is considered part and parcel of the scholarly work of DH  (Schreibman and Hanlon, 2010) and is beginning to be recognized by academic reward systems. Yet scant support and rewards accrue once software is up and running. This situation has changed little over the years, despite increasing concern regarding digital infrastructure sustainability generally  (Eghbal, 2016; Maron and Pickle, 2014) and attention to “care and repair” within DH  (Nowviskie, 2015; Sayers). Like all software, DH tools require maintenance, enhancement and updates, which is to say, continued funding and expertise.   Pursuing uptake seems like a natural approach to the sustainability dilemma, since:  it is easier to demonstrate the success of a tool and to justify further resource allocation in light of increases in use; and adopters of a tool are invested in its survival and might put resources towards sustainability.  However, uptake is no guarantee of sustainability. As observed by Cameron Neylon, many scholarly infrastructures are public goods, and “Finding sustainability models to support them is a challenge due to free-loading, where someone who does not contribute to the support of an infrastructure nonetheless gains the benefit of it”  (2017: 3). Nevertheless, unused tools are poorly positioned to request continued funding or support.  The uptake or adoption of existing DH software by new users is far from guaranteed, even if it fulfills a need that it is well-documented in the research community where it originates. Fred Gibbs and Trevor Owens crystallize the ways in which tool uptake is hindered by multiple factors  (2012). Significant problems include:    managing expectations, while also scaling up functionality from local to more general needs (Koeser and Hicks, 2018);  limited  learning resources (examples, user documentation);  unintuitive or complex  user interfaces that discourage novice users;  lack of support for  standards and interoperability.  community building  Together with more mundane but important activities like code maintenance, stable hosting, and systems administration, these factors create challenges that can prove fatal to promising technologies. Some are proclaimed at digital humanities conferences but seldom heard from again, while others like Paper Machines  (Guldi and Johnson-Roberson, 2012) show immense promise but do not develop into fully robust tools. Even mature tools with uptake from a wide range of users, such as Gephi, live quite precarious lives  (Jacomy, 2018). In short, the challenges of sustaining tools are manifold. We use the above points as a rubric for reflecting on CWRC-Writer’s engagement with the challenges of uptake.   Scaling features and expectations  The modular CWRC-Writer exists in several types of installation to suit users from novices to technical experts:   CWRC-Writer: available to researchers within the CWRC platform, where it is integrated with an Islandora repository,  Git-Writer ( cwrc-writer.cwrc.ca): uses GitHub’s file storage, versioning, and authentication to allow anyone to edit GitHub-hosted XML documents (Fig. 1).  Installations by third parties in other software stacks.       Fig. 1. Git-Writer document loading interface To support a wide variety of users, CWRC-Writer provides these core features:  an interface that renders XML in a human-readable layout using CSS (Fig. 2); XML tagging, with or without tags showing, with validation and error identification; raw/source XML editing for experts; entities tagging in XML and/or Web Annotation RDF with built-in authority lookups.  Members of the DH community, as well as literary and cultural studies scholars using XML for their texts, were involved from the beginning in the design of the tool. The user group comprises both power-users – researchers with decades of experience in markup – and novice or occasional users with little familiarity with DH. CWRC-Writer was designed from the outset as a light-weight editor to allow novices to tag XML documents and link them to named entity authorities, such as the Virtual International Authority File, in a manner that would avoid the steep learning curve associated with other, more complex editing tools  (Brown, 2015).   This lightweight usage is our main use case. CWRC-Writer does not aim to replace a full-featured XML editor for heavy-duty markup or transformations. The complexity of managing XML through an HTML front-end mean that major restructuring, for instance, is very tricky. To ensure that available affordances are aligned with the needs of the users, CWRC-Writer offers three different editing modes:   A default combined XML & RDF mode creates both XML tags and Web Annotations identifying entities in the same span of text;  external named entity identifiers are mapped onto the equivalent tags within supported XML schemas - which include established   TEI customizations   and other schemas employed by CWRC-supported projects.   RDF-only mode for Web Annotations that leave the body of the XML file untouched. XML-only mode for tagging without adding any Web Annotations.      Fig. 2. CWRC-Writer document showing application of CSS In conjunction with particular user communities, we are extending CWRC-Writer functionality based on a document’s schema declaration; for example, for EpiDoc files, a popup editor for translations will allow users to create or tag a translation while viewing it side-by-side with the original. To support transcription, side-by-side display of the XML and images allows transcribers to view the scanned manuscript within the tool (Fig. 3).     Fig. 3. Editing interface (XML & RDF mode) with side-by-side display of manuscript scan     Learning resources We mitigate the challenge of a new interface by providing extensive, searchable   user documentation  (produced with the DITA documentation standard) and   tutorial videos , as well as virtual office hours for real-time support. Learning to apply markup is a major challenge for the uninitiated, so there are sandbox templates for fooling around. Projects can create customized document templates that can be used to kickstart content creation and editing. These can provide highly detailed instructions, in order to promote consistency and best practices.     A user-friendly interface  From 2012 on, CWRC-Writer has undergone successive rounds of user testing, which have informed feature development and UX improvements. Two extensive rounds of survey-based user-testing were conducted before 2016, followed by numerous informal consultations and feedback from users and workshop participants. CWRC-Writer code is available in GitHub and a ticketing template allows adopters to submit both feature requests and bug reports. Formal announcement of the GitHub version in 2019 will be followed by another round of systematic user testing.    Standards and interoperability  CWRC-Writer editor adheres to the standards for both markup and Web Annotation. An integrated XML validator allows users to validate against the declared schema as they work on the document. TEI is supported in all version of the editor. RDF annotations adhere to the Web Annotation Data model, a W3C Recommendation that is being widely adopted within DH and in the scholarly publishing community as a standard for annotation data.   Promoting a community of users  In addition to passive adopters, who employ CWRC-Writer as made available through CWRC or GitHub, we have projects joining CWRC primarily thanks to its integration of the editor with other tools. There is growing interest from members of the DH community considering it for use in TEI editing projects, as components of library-based DH tool suites, or for teaching XML. The Center of Digital Humanities Research at Texas A&M has produced a containerized version and has installed it on top of Fedora 4 as part of a larger toolkit. Bucknell University is installing a version of the Git-Writer to support diverse local DH projects, and other institutional installations are planned. External partners were consulted for the development of Git-Writer, and the code is configurable, modular, and well documented in order to permit installation in a range of software environments. Users currently cohere around specific projects. We hope a broader CWRC-Writer community will develop as numbers grow, and be joined by a community of developers familiar with and willing to contribute to upkeep. However, the experience of other projects indicates that this is a major challenge.   Future developments CWRC-Writer has for several years now, since its launch within CWRC, been thinking hard about how to promote uptake and long-term sustainability. Our development roadmap is constructed around current and oncoming user needs. We will continue to adapt our strategy in response to insights gained from further user testing and feedback from the community following the launch of the Git-Writer to the DH community.  ",
        "article_title": "CWRC-Writer Design and Survival Strategies: Observations from the Post-Launch Trenches",
        "authors": [
            {
                "given": "Mihaela",
                "family": "Ilovan",
                "affiliation": [
                    {
                        "original_name": "Canadian Writing Research Collaboratory, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "School of English and Theatre Studies, University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "scholarly editing",
            "sustainability and preservation",
            "English",
            "literary studies",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Creating an ontology for feminist literary history with a view to its extensibility to other literary and cultural work involves significant decisions. The ontology described here supports the linked open data (LOD) strategy of the  Canadian Writing Research Collaboratory (CWRC)’s online platform. CWRC launched in 2016 as “An online infrastructure for literary research in and about Canada designed to meet the challenges and embrace the opportunities of the digital turn” (About CWRC, 2018). It pursues this mission in part through contributing LOD to the Semantic Web initiative to make Web resources more discoverable, shareable, and interoperable by making them machine-readable (Oldman, Doerr, and Gradmann, 2016; Smith, 2012). It promotes linking amongst CWRC-hosted projects through use of LOD identifiers both in metadata and within XML documents through an online XML editor and Web Annotation creator, CWRC-Writer (Brown et al., 2019). The next step is to push its LOD extracted from CWRC content into the Web. The  Orlando Project (Brown, Clements, and Grundy, 2019) is our pilot dataset for extracting more complex LOD.  The Orlando Project has published since 2006  Orlando: Women’s Writing in the British Isles from the Beginnings to the Present (Brown, Clements, and Grundy, 2006-2019), an online textbase for the research and discovery of women’s writing. Its 8 million words of richly-encoded text describe writers’ lives, careers, and contexts using a bespoke XML schema. The textbase has been acclaimed as initiating a new breed of information resource and for having “changed the parameters of the scholarship and teaching of British women’s writing” (Bowers, 2012). However, its utility is impeded by its data structure and paywall. LOD offers a means to make much of the knowledge embedded in Orlando more accessible and interoperable.  The first requirement was an ontology. No single extant ontology covers the range of biographical and literary relationships that CWRC needs to represent. We therefore sought to adapt as much as we could from elsewhere while filling gaps through an ontology of our own (to echo Virginia Woolf [1929], whose ground-breaking feminist analysis inspired the Orlando project’s name). We embrace Gruber’s widely accepted definition of a computational ontology as “A specification of a representational vocabulary for a shared domain of discourse”; an “explicit specification of a conceptualization”, where a conceptualization is understood as “an abstract, simplified view of the world that we wish to represent for some purpose” (Gruber, 1993). We outline here some key aspects of our ontological strategy, many of which are more fully described in the published preamble to the ontology itself (Brown et al, 2019).   Ontology Design Process Our process was pragmatic: we aimed to mobilize the data as soon as feasible, adopting where we could in accordance with best practices and extending or creating where we could not. In using other ontologies we sometimes importing (e.g. the Organization Ontology [Reynolds, 2014]), sometimes cherry-pick terms or relationships, and sometimes base definitions on external ones and cite them (e.g.  DBpedia,  Getty) so we can relate terms to each other within our structure. The ontologies from which CWRC draws include  FOAF,  Organization,  BIBFRAME,  TIME,  Web Annotation,  Dublin Core,  PROV, and  CIDOC-CRM, as well as the  SKOS vocabulary.  The ontology is designed to support open-world datasets like Orlando‘s: we develop terms on an as-needed basis rather than trying to be exhaustive. We are not trying to map the world but to represent what we have. The ontology is dynamic, expanding as needed, and the development process iterative. In some respects, this ontology strategy resembles lean startup and agile development processes (Cummings and Stacey, 2018). We here outline the major principles and decisions that have informed the work thus far.   Antifoundationalism and representationality The ontology reflects what Alan Liu calls a “lightly antifoundationalist” epistemology; it emerges from feminist theory and science and technology studies, with an emphasis on situated knowing and a wariness of the consequences of classification (Liu, 2016; Haraway, 1991; Braidotti, 2006; Bowker and Star, 1999; see also Smithies, 2014). Ontologies can be understood as approximating the world and “thus capturing some truth about it, without enjoying a one-to-one correspondence with categories of entities as they exist completely independently of human languages or human practices” (Alcoff, 2000). Rather than viewing ontologies as involving “reality representation” (Smith, 2004), we understand the CWRC ontology as deeply representational. Structuring it with the Web Ontology Language (OWL) and devising Shape Constraint Language (SHACL) rules for our data will provide our data with the tactical benefits of ontological rules including intelligibility, error detection, processability, inferencing, and interoperability. This work aims to intervene, with experimental models, in the knowledge structures of our time. We therefore need to accommodate some aspects of those structures in order to make an argument and to be intelligible, even if the aim is to move beyond some categories. So we take as our touchstone the core concerns of the datasets themselves. Some priorities were covered by existing ontologies, but sometimes there was nothing to approximate what we needed, so we invested significant resources there.   Deferred upper-level ontology The team considered and debated adopting an upper-level ontology that lays out categories and relationships at the most general level. After reviewing leading candidates, we held off because none align closely with our approach. DOLCE Light is closest, but orphaned, and the CIDOC-CRM is very event-oriented, whereas much of our data is not. We are therefore deferring the question to see whether it seems necessary and until we have a wider range of use-cases, including ones related to space and time, so as to better evaluate the implications (cf. Posner, 2015 on Cartesian representation).   Provenance and citation Every LOD assertion links back to the encoded source (in this pilot instance, the Orlando textbase), connecting it to nuanced prose as well as the specific XML markup from which the relationship was extracted. The data is also designed to point to the specific scholarly sources from which claims within the source text are derived.   Annotation The Web Annotation Data Model (Sanderson et al, 2017) links identifications of entities to their source, as noted above. We also use the WA model to characterize the properties associated with a writer as descriptions of that person, refuting positivist claims. Framing claims as annotations motivated by description is important, given that the creators of the markup did not anticipate this use of the data, and given that only dates have any kind of certainty value attached to them.   Ambiguity, diversity, and nuance Rather than being disambiguated, leaky cultural categories are generously documented and represented as mutually constitutive with specific discursive frameworks or “Context” classes of annotations (Brown et al., 2017). “Textual labels” group together related terms emerging from different discourses. Rather than mash complex and contested terms into a single identity, the ontology retains distinctions made between them within the encoding and groups them together with these labels – so, for instance, the jewishLabel instance and different constructions of Jewishness are linked using the relationship “represents”.   Linking to legacy terms The labelling strategy and “represents” predicate help handle problematic legacy terms, along with another custom predicate called hasFunctionalRelation, which signifies that a term has served in a parallel way in other datasets, but has no semantic commensurability with our term. Gender and sex are definitively different, so the OWL sameAs relationship does not work to link terms for sex to womanLabel and manLabel. However, sex can map (inadequately) to gender terms in existing datasets, which often employ the ISO 5218 categories for sex (“ISO/IEC”). The “hasFunctionalRelation” predicate creates a complex but still processable bridge between Orlando data and datasets of women’s writing that employ ISO 5218 values.   Challenges We conclude by demonstrating a couple of challenges arising from this work with visualizations of extracted data using the HuViz LOD explorer (described in a parallel paper). First is the tension between complexity and nuance, on the one hand, and readability and processability on the other. As Rob Sanderson and David Newbury stress, we need Linked Open Usable Data (2017). The Orlando LOD dataset is verbose and complex, raising concerns that it may be incomprehensible and unwieldy despite the ontology. Second is the tension between standards and bespoke terms. Staying close to the Orlando data has meant, for instance, devising a genre ontology (CWRC Genre Ontology) rather than adopting a partially suitable one, making CWRC data more of an island in the LOD stream than it might otherwise be. However, standards can also have unfortunate consequences: adopting the Web Annotation framework means that our assertions are now less direct and easily comprehensible by non-specialists than in our initial model for our data. The CWRC ontology aligns with the strain of critical DH that applies humanities epistemologies to create digital representations: the CWRC ontology works not with data but with capta (Drucker, 2011). The millions of triples in the Orlando Project’s British Women’s Writing Dataset will advance feminist digital literary history, and experiments with the CWRC ontology will help refine strategies for writing feminist literary history, among other complicated stories, into the Web.",
        "article_title": "Linked Literary History, or An Ontology of One’s Own: The Canadian Writing Research Collaboratory Ontology",
        "authors": [
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Joel",
                "family": "Cummings",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Jasmine",
                "family": "Drudge-Willson",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Abigel",
                "family": "Lemak",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Kim",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Alliyya",
                "family": "Mo",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Deb",
                "family": "Stacey",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "feminist studies",
            "standards and interoperability",
            "English",
            "semantic web and linked data",
            "cultural studies",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IsiXhosa is an Nguni language classified in the south-eastern geographical zone of South Africa (Guthrie, 1971:33). It is one of the official South African languages, and one of the most widely spoken (after isiZulu) with approximately eight million mother-tongue speakers. In terms of natural language processing, particularly computational morphology, the Nguni languages including isiXhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. Nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system (Bosch & Pretorius, 2008:97). The principal author’s masters study focused on the representation of women protagonists by male and female authors in isiXhosa dramas. The whole analysis process was done manually, mainly because digitised isiXhosa literature books were not available. This limited the study to only four books. The analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isiXhosa dramas.  When examining the representation of female protagonists in isiXhosa dramas, similar works from other scholars are noteworthy. The first contribution that narrates the same viewpoint as the one investigated here is by Ngqase (2002), in which she examines the representations of women in four isiXhosa drama books. The study highlights the interplay between culture and women's social space. The second contribution by Peter (2010) expresses female character portrayal in various drama works written by males. He concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes (Peter, 2010:15).  As an isiXhosa language researcher at the South African Centre for Digital Language Resources (SADiLaR), the principal author has been introduced to computational methods which could afford new ways to approach the research topic described.  Assessing and reporting on the usability of computational tools when analysing isiXhosa texts   This presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. The computational tools which were utilised include, Voyant Tools and regular expressions (regular expressions) as well as testing the feasibility of BookNLP when used conjunctively with written languages.    The creators of  Voyant Tools  note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available (Sinclair & Rockwell, 2019). Capitalisation in isiXhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. For instance, the “Context” tool in Voyant Tools produces search terms only in lower case.   With  BookNLP , which is specifically built for English texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-Western language and how it could be adapted and/or how a similar pipeline could be developed for isiXhosa using tools developed by SADiLaR. BookNLP was developed by Bamman, Underwood and Smith (2014). The study follows similar approaches to those utilised by Algee-Hewitt, Porter and Walser (2016).   Finally,  regular expressions  will be used as well, as it allows to match patterns and search for very specific character sequences more effectively.  Operationalisation of the research questions  The study has two parts. First, by reporting on the performance of the computational tools on the isiXhosa drama corpus versus an English equivalent. The specific steps for Voyant Tools and differences when using regular expressions will be provided and compared. Second, in terms of research questions focusing on the representation of Xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool.    The paper reports on:   How can computational tools used to analyse Western languages be used for conjunctively written South African languages?  Are authors of isiXhosa literature influenced or led by their gender when writing?   Do authors conceptualise their work with the intention to uplift one gender while diminishing the other?  How can the gap caused by inequality between sexes be bridged through written literature?   A practical example:  If data from an English corpus is analysed using Voyant Tools, the tool would automatically be able to provide word frequencies and links between words. However, with a conjunctive language like isiXhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. For example, gender association in isiXhosa depends solely on the prefix. Only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. Furthermore, only through contextualisation will one know whether that person is male or female, as isiXhosa prefixes are also unisex.    E.g.:  uPeter  u sela amanzi  Peter (he) is drinking water  uSammy  u phunga iti.  Sammy (she) is drinking tea. This research also aims to provide a point of departure for new scholars interested in analysing isiXhosa literary works using computational approaches.  Key words : Conjunctive language, Nguni language, computational methodologies, voyant tools, regular expressions, BookNLB  ",
        "article_title": "The Complexities Of The Representation Of Xhosa Protagonists, Represented By Male And Female Authors In IsiXhosa Dramas Using Computational Methodologies",
        "authors": [
            {
                "given": "Andiswa",
                "family": "Bukula",
                "affiliation": [
                    {
                        "original_name": "SADiLaR, South Africa",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Juan",
                "family": "Steyn",
                "affiliation": [
                    {
                        "original_name": "SADiLaR, South Africa",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "corpus and text analysis",
            "content analysis",
            "african studies",
            "gender studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Introduction: The Beatles and musical complexity  The Beatles are considered to be one of the most influential bands of the 20th century, who still shape and influence pop music today (Everett, 2001). In the course of their creative history, the band has proven an enormous range and variety of individual compositions. One reason for the unusually large musical diversity of the Beatles is that with Paul McCartney, John Lennon and George Harrison three persons were involved in composing the Beatles’ songs (MacDonald, 1995). In addition, the Beatles’ producer George Martin also had a considerable influence on the composition of many songs (Martin & Hornsby, 1979).  In this paper we will investigate how this diversity in composition is reflected in musical complexity in the work of the Beatles by using computational methods. So far, related work studies can be found in Mason (2012), who statistically analyzes the properties of Beatles songs in order to decipher what he calls the “Beatles genome”. The aspect of complexity has already been investigated by Eerola et al. (2000), who use a MIDI corpus to analyze the relationship between musical complexity in Beatles songs and its effect on chart placement. Eerola et al. (2000) also discover a highly significant increasing time-trend in melodic complexity, i.e. the Beatles’ songs became melodically more complex through the course of time. While Eerola et al. (2000) only looked at singular melodies and their complexity, we present a study that takes into account all of the available melodies and also the chords to compute the complexity of a song. We present an exploratory tool for the interactive visualization of musical complexity distributions in the work of the Beatles. The visualizations can be scaled from all of the Beatles’ albums to single songs and composers, to investigate complexity on a more detailed level.   Corpus and Method  Corpus – The corpus used for this study is based on guitar tablatures from the online platform  Ultimate Guitar    Ultimate Guitar portal: https://www.ultimate-guitar.com/, (Note: all URLs mentioned in this paper were last checked Nov. 25, 2018)  , which were downloaded in  GuitarPro    Guitar Pro Tool: https://www.guitar-pro.com/   format and converted to  MusicXML   MusicXML documentation: https://www.musicxml.com/for-developers/   for analysis. This platform has already been successfully used as a data source for other scientific studies (Di Giorgi et al., 2017) and includes 205 songs from the Beatles’ first single in 1962 to their last studio album in 1970.    Normalization – To be able to compare songs with different scales to each other we normalize the note inventory according to Cuddy et al. (1981), who propose the  Roman Numeral Analysis method. With this method all notes of a diatonic scale are represented by Roman numerals, starting from the basic note of the scale as step I. Tones that are not part of the scale are marked with a sharp (#) (cf. Fig. 1).     Example for the classification of tones according to the Roman Numerals Analysis using the scales of C Major /A Minor and D Major / B Minor.   Complexity model – To operationalize the concept of musical complexity we rely on experiments by Krumhansl & Shepard (1979), in which test persons were asked to evaluate how a certain tone completes the tone sequence of a scale. The results show that scale tones (the keynote in particular) are rated better than non-diatonic tones. Building on this previous work, we define four levels of complexity (cf. Fig. 2). We use this complexity model for both, the analysis of single notes as well as for chord progressions.    Classification of tones into different levels of expectation-based complexity using the example of C Maj. / A Min.   Computation of results – Our corpus of MusicXML files is analyzed by means of Python scripts that parse individual notes and chords from the data and count their frequencies. For the recognition of chords, the existing  music21    Music 21 Toolkit: http://web.mit.edu/music21/    library is used, as it provides many useful functions for the analysis of transcribed music. Because both the tone material and the chord material of each song are to be analyzed on the basis of the previously described Roman Numeral categories, it is necessary to identify the scale of a song. In many songs the scale is explicitly annotated by means of global accidentals and therefore can be extracted directly from the MusicXML transcription. For those cases where global accidentals are missing, we apply an existing algorithm for scale detection (Madsen et al., 2007). By assigning notes and chords to the previously introduced four complexity levels, a complexity distribution can be calculated for each song and album.     Results and Discussion  The results can be analyzed statistically, to detect general trends in the development of musical complexity in the work of the Beatles. We conducted a Pearson-Bravais (Pearson, 1895) correlation test to investigate how musical complexity has developed through time. For each year, we calculate the frequency of tones and chords that belong to the complexity levels 1+2 (rather low complexity) and also the frequency of tones that belong to the complexity 3+4 levels (rather high complexity). For the higher complexity levels, we find a weak positive correlation (cf. Cohen, 1988) for both tones (r = 0.208, p=0.005) and chords (r = 0.167, p=0.024). These results indicate a gentle trend toward increased musical complexity over time, but cannot confirm the observation of a highly significant correlation as noted by Eerola et al. (2000). Our results rather seem to correspond with existing research on the musical development of the Beatles, which does not describe a general complexity trend, but rather identifies different phases (of different composers) and individual albums with increased complexity (Everett, 2001). This observation is also illustrated by the following graph (cf. Fig. 3), which does not show a clear trend in the development of complexity levels, but rather goes up and down over time.   Development of tonal complexity levels    The graphs for chord complexity levels largely correspond to the tonal complexity levels. The detailed graphs are available via the online visualization tool.   (level 1 = low complexity, level 4 = high complexity) for the Beatles’ albums over time.   We can also show that a general complexity trend for albums is problematic, as even just a few outlier songs can substantially influence the overall complexity score of an album. This can be best illustrated for the album “A Hard Day’s Night”. While Fig. 1 suggests that tones on the complexity level 4 for the whole album increased as compared to the previous (and also the successive) albums, a closer look (cf. Fig. 4) at the individual songs shows that in fact only two of the 13 songs show high complexity on level 4 (“And I Love Her” = 31%; “When I Get Home” = 42%).    Overview of the distribution of tonal complexity levels (level 1 = low complexity, level 4 = high complexity) for all songs of the Beatles’ album “A Hard Day’s Night” (1964).  These observations reflect the initial notion that the work of the Beatles is extremely heterogeneous, which is also a result of the band’s different composers and their individual musical development. When we look at musical complexity from the perspective of individual composers (cf. Fig. 5), Paul McCartney seems to be the most stable composer, i.e. his complexity curves largely correspond to the overall complexity curves of the Beatles albums (cf. Fig. 3). George Harrison and John Lennon each have several albums where they contribute songs with higher complexity.   Overview of the distribution of tonal complexity levels (level 1 = low complexity, level 4 = high complexity) for the main composers of the Beatles. Note that George Harrison was not involved as a composer for the albums “Please please me”, “A Hard Day’s Night” and “Beatles for Sale” and overall only composed 24 songs.  The absence of a general trend or temporal pattern in musical complexity lead us to present the results of our computational approach in a rather exploratory interface that can be used to look at complexity from different scales. The visualization tool is available online   Visualization tool: https://fuchsflo90.github.io/beatles-analysis/#   and can be used to explore complexity for both tones and chords, for any album of the Beatles filtered by the main composer. In addition to the complexity scores, it also provides information on the frequencies of different scales and rhythms.    Conclusion In this paper we showcased the application of a computational approach to measure musical complexity in a corpus of user-generated transcriptions of Beatles songs. We were able to demonstrate that musical complexity did not consistently increase over time (only a weak correlation was measured) and that high complexity seems to be a situational phenomenon that can occur for single songs rather than for a whole album. The approach presented in this paper can be considered as a case study for further computational studies on musical complexity, thus adding to the branch of computational musicology as part of the Digital Humanities. As next steps we plan to extend the approach to comparative complexity analyses for different bands and genres.  ",
        "article_title": "A Computational Approach to Analyzing Musical Complexity of the Beatles",
        "authors": [
            {
                "given": "Manuel",
                "family": "Burghardt",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Florian",
                "family": "Fuchs",
                "affiliation": [
                    {
                        "original_name": "Regensburg University, Germany",
                        "normalized_name": "University of Regensburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01eezs655",
                            "GRID": "grid.7727.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "musicology",
            "content analysis",
            "data mining / text mining",
            "English",
            "theory and methodology)"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  State of research and problem outline In the course of the continuing upswing in writing process research, source types that document processes of authorial self-organization, notation, or reading have come into focus of literary and cultural studies. These include, among others, notebooks (Hoffmann 2008; Efimova 2018), card indexes (Gfrereis and Strittmatter 2013; Krajewski 2011; Schmidt 2016) and author libraries. While digital presentation techniques based on transcriptions and TEI editions are being developed for notebooks and card indexes (cf. Radecke 2018, Schmidt 2016), the digital presentation of author libraries is currently limited to the provision of either digital catalogs that make library metadata available (cf. Paul Celan’s library) or of simple digital copies, which are offered in a viewer and/or as a PDF download (cf. the Grimm library). These forms of presentation, however, do not succeed in grasping the autographical character of these book collections, whose particularity lies in the reading traces that the author (or heirs, estate administrators, owning institutions, etc.) has left in them. At the same time, the aim of an online presentation of such libraries cannot be an edited text; rather, they are intended to enable the reproduction of creative reading and usage practices. Three objectives can be phrased for such a presentation, with reference to research needs: 1) the provision of the entire volumes of an author's library; 2) the implementation of (re-)search opportunities; 3) the provision of novel applications with which the profile of the collection and reading patterns emerging in it can be discovered, recognised, and researched. For objective 1), common techniques for presenting digitized books (mostly based on METS/MODS) are already available; for objective 2), similarly, solutions are being offered (librarian or archival discovery tools). For objective 3), which constitutes the focus of our project, no solutions or conceptualizations have been presented so far, although there have been calls for the supplementation of search-based interfaces by concepts that enable serendipitous encounters (Dörk et al. 2011; Thudt et al. 2012; Whitelaw 2015). Even if the establishment of so-called explore modes has already partly contributed to the fulfilment of these goals, there is still a lack of flexible navigation along the relations between the objects in digital collections (Kreiseler et al. 2017).   Project, procedure and corpus We present a draft of an explorable and scalable representation of an author's library, developed in a research-based rapid-prototyping process. Our prototype study combines the presentation of author’s libraries, which is usually addressed as a philological problem, with design-oriented research on the visualization of cultural collections (Glinka et al. 2017; Dörk et al. 2017; Windhager et al. 2018). With this research we present a prototypically implemented approach of how to digitally represent author’s libraries (  video demo ). The project takes up two impulses: On the one hand, it is committed to distant reading (Moretti 2013), which looks for possibilities of pattern recognition in (parts of) an author’s library; on the other hand the representations should be scalable (Weitin 2017), which raises questions about transitions between different granularities.  The basis for the prototype is the library of Theodor Fontane (1819–1898), which comprises approximately 170 volumes. The significance of this author's library is primarily due to its provenance. It is irreplaceable because of the marginal notes and glosses written inside the books by Fontane and valuable because of numerous presentation copies (Rasch 2005). In addition to the complete digitization of the holdings according to archival standards, and the indexing of individual volumes according to bibliographic standards, data is collected at both page and corpus level. Access to the data is possible at four levels: overall corpus, object, page, and individual phenomenon (note, comment, etc.). The visualization enables various access and analysis options, which ideally reveal the broad spectrum of thematic and personal clusters and the different uses of commentary within the author’s library.    Visualization concept The visualization places a special focus on continuous navigation through zoom and filters on several granularity levels. This allows for the exploration of individual objects as well as their comparison and broad overviews of the whole inventory. The interactive prototype offers three basic levels for navigating the collection: the level of the individual authors, books, and pages.    Fig.1: Landing page/book level with overview of all books The starting point for the exploration of the visualization is the book level, which provides an overview of all books in the library, sorted by authors—starting with the author with the most books (Fig.1). Each book is represented by a vertical bar, in which each page of the respective book is represented by a segment, ordered from top (first page of the book) to bottom (last page). The page segments are color coded according to the category of occurring reading traces. While pages without traces are displayed in white, pages containing traces are colored, divided into four categories: 1) provenance (grey tones), 2) markings (red tones), 3) marginalia (blue tones), and 4) additional material (yellow). Hovering over a segment displays a preview of the respective page scan and the book title (Fig.2).     Fig.2: Page level with a book selected, filtered by marginalia, and hovering over an element  Giving an overview and resembling individual bar codes for each book, the book level visualizes the number of pages of a book (total length of the bar) as well as the distribution and patterns of reading traces (color coding). Above the visualization, a filter navigation serves as a legend for the color coding and offers the possibility to focus on certain categories of reading traces. The selection of a filter triggers the unfolding of the corresponding subcategories in the filter bar (Fig.2,3). In addition, a search bar enables specific keyword search in all transcribed marginalia, highlighting respective occurrences in the visualization. By selecting a book, the other books are compressed and a detailed view of the selected book is unfolded, revealing additional metadata and details. The browser's scroll function can be used to reach the other two granularity levels of the visualization. Following the principles of semantic zoom (Perlin and Fox 1993), scrolling upwards leads to a higher level of abstraction and downwards to a higher level of detail—thus allowing a change between three main granularities: authors, books, and pages. Scrolling enables continuous, meaningful transitions between the views and offers the possibility to go back and forth at one's own speed with the aim of making transitions between views more comprehensible.    Fig.3: Author level, filtered by marks, the colors of various mark types are unfolded In contrast to the book level, on the author level (Fig.3) all books of an author are combined by presenting the total distribution of the reading traces in form of an area chart. It is therefore possible to compare Fontane's reading traces, distributed over the works of different authors. However, the higher abstraction also allows insight into broader patterns.  Navigating from the book level towards the other direction to the lower page level (Fig.2), the page fragments of books of a selected author are zoomed in, enabling interaction with individual reading traces. Furthermore, selecting a book on this level unfolds a visualization of the transcribed marginalia. All selections, filter options and the selected level of granularity are encoded via a fragment identifier in the URL, allowing the sharing and bookmarking of specifics views and the use of the browser's history functions.   Reflection of results The novel visual approach to author’s libraries developed within the framework of the prototype combines design-oriented approaches to the visualization of cultural collections with philological, archival, and library research questions. By means of visual filters, subsets can be identified and categories can be created that enable pattern recognition within the collection. Users should be enabled to grasp concepts and themes within dedicated categories and across their boundaries. The focus of the project lies on the integration of search functions, visibility and scalability in visualizations, and how further exploration possibilities can be opened up by different access options to a collection. The development of new research questions during the prototyping process and the associated readjustment in the indexing process illuminate the interactions between visual research, metadata management, and philology. It became clear that the collection is to be understood as a construct that is only generated retrospectively. In the process, 'the' collection emerges from the unity of perspective and object. The resulting consequences of offering multiple views of 'the' object(s) are reflected in the various granularity levels of the prototype and result in an observation-dependent visualization of the available source data.   ",
        "article_title": "Scalable Exploration. Prototype Study For The Visualization Of An Author’s Library On The Example Of 'Theodor Fontane’s Library'.",
        "authors": [
            {
                "given": "Anna",
                "family": "Busch",
                "affiliation": [
                    {
                        "original_name": "Theodor-Fontane-Archiv / Universität Potsdam, Germany",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            },
            {
                "given": "Mark-Jan",
                "family": "Bludau",
                "affiliation": [
                    {
                        "original_name": "Urban Complexity Lab, Fachhochschule Potsdam, Germany",
                        "normalized_name": "Fachhochschule Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/012m9bp23",
                            "GRID": "grid.461741.1"
                        }
                    }
                ]
            },
            {
                "given": "Viktoria",
                "family": "Brüggemann",
                "affiliation": [
                    {
                        "original_name": "Urban Complexity Lab, Fachhochschule Potsdam, Germany",
                        "normalized_name": "Fachhochschule Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/012m9bp23",
                            "GRID": "grid.461741.1"
                        }
                    }
                ]
            },
            {
                "given": "Kristina",
                "family": "Genzel",
                "affiliation": [
                    {
                        "original_name": "Theodor-Fontane-Archiv / Universität Potsdam, Germany",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            },
            {
                "given": "Sabine",
                "family": "Seifert",
                "affiliation": [
                    {
                        "original_name": "Theodor-Fontane-Archiv / Universität Potsdam, Germany",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            },
            {
                "given": "Peer",
                "family": "Trilcke",
                "affiliation": [
                    {
                        "original_name": "Theodor-Fontane-Archiv / Universität Potsdam, Germany",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "manuscripts description and representation",
            "user experience design",
            "german studies",
            "modeling and visualization",
            "interface",
            "gamification",
            "spatial & spatio-temporal analysis",
            "English",
            "library & information science",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Feature selection in machine learning is one of the best covered topics in general statistics literature, and, next to classification algorithm, the most important factor to consider. To name but a few, the techniques of identifying the most efficient features include dimension reduction, shrinkage, or penalization (James et al., 2013). However, in stylometric investigations the selection of best performing style-markers is usually narrowed down to the choice between word frequencies, character n-grams, POS-tag n-grams (Hirst and Feguina, 2007; Stamatatos, 2009), etc., without devoting much attention to their statistical properties.  The purpose of this study is somewhat different: apart from identifying meaningful features – or such that facilitate telling apart analyzed classes – we would like to provide some deeper linguistic understanding of the most distinctive features, i.e. discover if words efficient in classification share any linguistic properties.  In stylometry, and particularly in authorship attribution, most frequent words (MFWs), or more specifically, their mean frequencies in the corpus, are traditionally claimed to exhibit strong discriminative power. In a vast majority of studies following a seminal study by Mosteller and Wallace (1964), the feature selection procedure starts with preparing a joint frequency list containing words ordered in a decreasing sequence of the number of their occurrences in the entire corpus, from the most to the least frequent. It has been shown that MFWs are mostly function (synsemantic) words, bearing meaning only in the company of other words, which makes it a very plausible theoretical justification for simply taking a considerable number of top words from a frequency list as potential features. Even if the impact of content (autosemantic) words is also considered important, and as claimed in a recent study (Lestrade, 2017) crucial for the Zipfian distribution of words in a dataset, these words are very sensitive to topic, theme, and other factors that might overshadow the authorial signal.  However, there is no simple answer to the question how many of MFWs should be taken into consideration. Consequently, there is no consensus between scholars when the choice of MFW vector is concerned, ranging from a very limited number of top words (Juola, 2008) to long vectors of 1,000 or so features (Hoover, 2004). Rybicki and Eder (2011) tested hundreds of combinations for both frequent and not-so-frequent words, and came to a conclusion that there is no universal number of features that would lead to satisfying results: this always depends on language and corpus, although a vector between 100 and 1000 MFWs usually reveals acceptable performance. Further studies (Evert et al., 2017) corroborate these findings using various measures of textual similarity. There are several ways to balance the influence of a certain words in the corpora and lessen the impact of less important ones. Undoubtedly, the most popular is tf-idf (term frequency–inverse document frequency), commonly used in information retrieval systems. It assumes that a word which is attested in few documents, but is yet relatively frequent (e.g. onto ,  upon ,  therefore), contributes much more to the general knowledge of the text than a popular word evenly distributed across the corpus (e.g.  the, a, an). This method of weighting by definition culls the most frequent words, and boosts the weights of “keywords”, or unusual words. A possibly unwanted effect is that the “keywords” also include several proper nouns, names, and so on.  A different approach assumes that the use of some words – no matter how frequent they actually are – does not differ significantly across the corpus, whereas some others are over- and underused. The  variability of a given word in a text collection might then be a good indication of its discriminative power. However, since the standard deviation of a given sequence of numbers strongly depends on their actual values, this also holds for word frequencies. E.g., the standard deviation for the set x= {1, 2, 3, 4} is as high as 1.290994, the standard deviation of the set y= {10, 20, 30, 40} is ten times bigger, which, going back to the realm of word frequencies, would mean that the variability of the word  the would be orders of magnitude higher than that of the infrequent words. This can be corrected with the coefficient of variation (CoV), which is the standard deviation of a set of values divided by its arithmetic mean. First attempts to tune the word list according to the CoV were carried out by Hoover (2014).  In the following experiments, we explore the above three basic ways of ordering the features: (i) according to their mean frequency in the corpus, (ii) according to their mean TF-IDF score, and (iii) according to their CoV. Machine learning is always burdened with the problem of over- or underfitting the model: using too few features usually makes it difficult to reveal the signal from the corpus, but relying on too many without any pruning might lead to overshadowing the signal with noise. Rather than looking for the optimal length of feature vector, we approach this problem with the intention of a rearrangement of features from the most to the least meaningful, which can help neutralize the issue of unimportant words landing (high) in the number of considered features. While it is possible to balance the impact of particular, not necessarily very important, features with a proper choice of distance measure and classification method, these solutions require advanced knowledge of the existing practices and the ability to critically assess the method and results. Applying a better method of feature selection, more resistant to words that carry importance only for singular works or authors, makes stylometry more accessible to beginners and provides more objectivity, reducing the opportunity and urge for “cherry-picking”.    Dataset  We used 4 datasets: 100 Polish, 100 English, and 75 French novels for main analysis, and smaller set of 28 English for test purposes, as the computations were time-consuming. The datasets were retrieved from resources used as benchmark by Computational Stylistics Group and Evert et al. 2017 comparison of Delta measures. The texts in these corpora are balanced for the time period (turn of the 19th and 20th century), number of texts per author (3, with additional 1 book for Polish and English) and to some extent style and topic – all canon literary texts, dealing with social issues, with some influence of history.     TF z-scores TF-IDF z-scored TF-IDF   mean TF (=MFWs) x x, discussed in detail x x   TF-IDF x x, discussed in detail x x   CoV x x, discussed in detail x x   Table 1: Considered scenarios, out of which z-score weighting discussed in this paper. Although we tested all above-mentioned scenarios, due to limited amount of space in this abstract, we focus on the behavior of various order of features that are next transformed to z-scores, which proved most interesting in terms of the results. Other weights and their combinations with different arrangements of the features will be discussed in the full-length version of this paper.    Experiment design  In our approach, we performed a series of controlled tests of attribution, using the above-mentioned corpora of known authorship. Over multiple iterations we recorded the number of texts that were correctly attributed by our chosen supervised classifier. To neutralize an impact of any local idiosyncrasies in our corpora, we applied leave-one-out cross-validation scenario, which meant that every single text from a corpus was checked against a slightly altered training set. As for chosen classification method, because of conceptual simplicity and intuitive interpretation of the results, we use  k -NN supervised lazy learner, with k= 1. In stylometric community, this classifier is well known under the name  Delta (Burrows, 2002), and widely used. Since it is somewhat difficult to test multidimensional methods using one feature at a time, we apply a moving window approach, in which for every single feature to be tested, we perform a test in 10-dimensional space for the feature in question and its 9 subsequent features (i.e. a total of 10 words in the order as they appear after  the weighting and ordering procedure). In other words, in each iteration we test the combination of w i + w i+1 + … + w i+9 features.     Results  The evaluation of different rearrangements of the list of features starts with a classic MFW-centric approach, or ordering the features according to their mean term frequency (TF). The results turned out to be surprisingly unsurprising: mere word frequencies outperform other approaches (see Fig. 1).        Fig. 1: Attribution scores for the words ordered based on MFWs (mean TF), weighted with z-scores. One circle represents one window of 10 subsequent features. TF-IDF reveals reasonable predictive power for the features ranked at the top of the list, as evidenced by the first few hundreds of words (Fig. 2). However, the obtained values are significantly lower than for the regular MFWs. Worth noticing is the fact that the end of the feature list spikes up: very frequent function words excluded from the TF-IDF company and clustered at the end of the list actually outperform all the other features in this picture, once again highlighting the relevance of very common and very frequent words for the attribution.      Fig. 2: Attribution scores for the words ordered based on mean TF-IDF, weighted with z-scores. Interesting and counter-intuitive are the results for the coefficient of variation (Fig. 3): the most successful attribution scores alike, or even higher, than those obtained for TF occur only for the features grouped at the end of the considered wordlist. Moreover, the significant features are distributed more densely than in the other case. Essentially, an inverse version of CoV, computed as 1 / CoV, will serve as an efficient feature harvester.      Fig. 3: Attribution scores for the words ordered based on coefficient of variation (CoV), weighed with z-scores. While general attributive success rates are higher for TF than for CoV, the tail of non-distinctive features is longer in the case of CoV than for the TF-weighted list, which suggests that the two methods can harvest meaningful features quite effectively, despite differences. This brings us to the question: What if we combine their potential to extract the right features?  We propose to simply multiply TF and reverse CoV. Knowing that CoV is in fact the standard deviation of a given feature divided by its mean, we can denote the formula as follows:  ω i =  μ i × (1 / (  σ i /  μ i ))  where  ω i is the new weight of a feature  i,  μ i – its mean TF, and σ i – its standard deviation. With a little bit of algebra, we observe that:   ω i =  μ i 2 /  σ i  which, we believe, will further aggregate meaningful features at the beginning of the wordlist. We additionally tested a similar idea of boosting the meaningful features by simply magnifying TF by standard deviations:  μ i ×  σ i.        Fig. 4: Cumulative attribution scores for different scenarios discussed in the paper, for the first 500 features. Inset: an overview of the entire feature set. Results obtained on the test corpus of 28 English novels. The comparison of the above three plus two newly introduced scenarios is shown in Fig. 4, where cumulative sums of attribution accuracies are presented. If the features were spread randomly on the word list, the observed results would follow the grey dashed line. The higher a given trajectory, the better an order of features from most to least meaningful. An overview of the entire feature set (Fig. 4, inset) exhibits a good performance of TF, but a closer look at the top few dozen features (Fig. 4, main) shows that our newly introduced weighting outperforms the time-proven MFW-centric approach. The suboptimal behavior of CoV (meaningful features clustered at the end of the list, see also Fig. 3) is mirrored by its very low trajectory.     Conclusions  In this paper, we experimentally confirmed that the intuition of ordering a list of features according to their decreasing frequencies has solid grounds. An alternative approach – ordering features according to their “keyness”, i.e. TF-IDF scores – turned out to be questionable. The major observation formulated here, however, was that combining the usual MFW approach with an inverse CoV weighting arranges the features even more efficiently.   Acknowledgments The study was conducted as part of  Large-Scale Text Analysis and Methodological Foundations of Computational Stylistics project (SONATA-BIS 2017/26/E/HS2/01019) funded by the Polish National Science Center (NCN), for whose support we are very grateful.   ",
        "article_title": " Feature Selection in Authorship Attribution: Ordering the Wordlist  ",
        "authors": [
            {
                "given": "Maciej",
                "family": "Eder",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language (Polish Academy of Sciences), Poland",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            },
            {
                "given": "Joanna",
                "family": "Byszuk",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language (Polish Academy of Sciences), Poland",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-16",
        "keywords": [
            "stylistics and stylometry",
            "linguistics",
            "authorship attribution / authority",
            "data mining / text mining",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In March 2018 the project Eurasian Latin Archive was started with the aim of building a digital platform for a large corpus of Latin texts and documents from medieval and early modern times. The corpus encompasses text from East Asia, including Latin-Chinese texts, on which a group of researchers began to work addressing specific multilingual issues. The corpus will be available within a digital library provided with tools for textual and thematic analysis. The goal of the project is the comparative linguistic analysis, both internal with other Latin texts from different eras and areas, and external with non-Latin texts on homogeneous subjects. The ultimate purpose of the project is to highlight relationships by extracting and investigating 1) historical-cultural data about religion, law, science, art, and customs; 2) linguistic data, to be compared with homologous values in European Latin literature, in order to determine the specificity of East Asian Latin and investigate overlappings and mismatches between Latin words and their local (mostly Chinese) equivalents. The start-up phase of the project ( DAS-MeMo), which will end by February 2020, is co-financed by Regione Toscana and QuestIT, an IT company specialized in NLP and Artificial Intelligence. The working group is interdisciplinary and brings together medievalists, digital humanists, engineers and IT specialists. The platform is inspired by the digital archive  ALIM (Archivio della Latinità Italiana del Medioevo), which acts as a starting point for the new archive. However, unlike ALIM, the project is geared towards providing textual analysis also with machine learning tools; ALIM focuses instead on digital representation of editions (including editiones principes) and takes into account texts of Italian Latinity exclusively (Ferrarini, 2017; Stella, 2015).    Preliminary complexities: defining and creating the corpus To define the corpus, a first census of over three hundred texts has been made (also thanks to online resources, such as  Sinica 2.0 and  CCT-Database). The amount of documents is due to increase, but it is sufficiently large to collect some preliminary results in the main projects’ tasks. It seems useful to classify documents into four main categories: 1. Born-digital editions, freely available or to request for; 2) modern (critical and non critical) editions that have entered the public domain; 3. editions with possible issues with OCR; 4. manuscripts. With the exception of the born-digital editions, the categories can a) be already digitized and made available by other Institutions; b) still need to be digitized. At this stage, besides the born-digital editions already acquired, seventy more documents of the type 2a and 2b have already been processed. In the case of critical editions, we chose to provide the text without encoding the critical apparatus. Items are being encoded in TEI P5, with a particular attention for the essentials metadata, such as VIAF and Wikidata ID when available, OCLC references, reliability both for external and internal processes of the item (i.e. responsible for digitization and/or OCR, checking, date history of all changes, evaluation of the resource, and so on), possibly opening the way to integration with semantic models (Ciotti, 2018; Ciotti et al., 2016).    As a non-conclusion: a structure that is modular and can grow more complex with time For such an ambitious project it is obviously necessary to proceed by ensuring a modular architecture from the very beginning, so that it can be implemented within this project progressively and can possibly merge, through its interoperability features, with open source projects from other Institutions. For this purpose, a requirement analysis has been carried out, and a list of specifications has been redacted in order to provide a basis of guidelines for the subsequent implementation. The aim of this paper is to display the architecture of the project and some of the early results: a prototype based on the ElasticSearch search engine has been developed, which includes a search module (with multiple filter methods) and a browse module, with the possibility to investigate the corpus by authors, dates and periods, collections, sources, places mentioned within the document, languages used in the text, keywords. A particular attention is being paid to the recognition of entities. For the moment being, we are working on recognition of places, dates and people. For the particular case of places, we also need to take into account that location names often differ significantly from the ones used nowadays, to the point that many researchers actually disagree on the current location of cities mentioned in these texts. More tools will be added to the preliminary ones, in order to pursue a text analysis (Stella, 2018: 72-100; Eger et al., 2015). One of the most important issues remains however the encoding of multilingual documents, currently under examination by handling Prospero Intorcetta’s Sapientia Sinica, which contains chinese text, and transliterations in latin of the chinese terms.  ",
        "article_title": "A Digital Platform for the “Latin Silk Road”: Issues and Perspectives in Building a Multilingual Corpus for Textual Analysis",
        "authors": [
            {
                "given": "Emmanuela",
                "family": "Carbe'",
                "affiliation": [
                    {
                        "original_name": "University of Siena, Italy - QuestIT",
                        "normalized_name": "University of Siena",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01tevnk56",
                            "GRID": "grid.9024.f"
                        }
                    }
                ]
            },
            {
                "given": "Nicola",
                "family": "Giannelli",
                "affiliation": [
                    {
                        "original_name": "University of Siena, Italy",
                        "normalized_name": "University of Siena",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01tevnk56",
                            "GRID": "grid.9024.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "corpus and text analysis",
            "oriental and asian studies",
            "artificial intelligence and machine learning",
            "data mining / text mining",
            "English",
            "medieval studies",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The ISMI project The Islamic Scientific Manuscript Initiative (ISMI) project was founded in 2005 to make accessible information on all Islamic manuscripts in the exact sciences (astronomy, mathematics, optics, mathematical geography, music, mechanics, and related disciplines), whether in Arabic, Persian, Turkish, or other languages from the 9 th to the 19 th century ISMI website:    The ISMI project limits itself to “scientific” manuscripts but it tries to encompass all such manuscripts worldwide regardless of their current location and it tries to record as much information about these manuscripts as available, including reader and ownership marks, annotations and illustrations, making it possible to learn more about structures and practices of knowledge in the islamicate world (Ragep et al., 2008).   The database The database of the ISMI project is a cooperation project by the Max Planck Institute for the History of Science and the Institute of Islamic Studies at McGill University in Montreal. The database has been built up over more than ten years starting from an early personal database project by the involved scholars, extended by corrected information from catalog works like MAMS (Matvievskaya et al.,  1983 ) and personal research by the scholars in the project and outside. It currently contains information about over 4700 texts in 15000 witnesses in 8000 codices and 2500 persons and an accompanying secondary bibliography of 2700 titles and it is constantly being extended.  The database development started in 2006 with a new data model based on the idea of a network of flexible objects and relations. Objects can have arbitrary attributes and the relations between objects are also like objects and can have attributes.    Part of current ISMI data model showing relations between text, witness, person and codex objects.   The basic objects in the data model are for example the TEXT which is abstract, the WITNESS which is a concrete material manuscript and the PERSON (real or imaginary). These objects are connected by relations like  is_exemplar_of which connects a text and its witnesses and  was_created_by which connects a text and a person as its author (see Figure 1). The same person can at the same time also be connected to other witnesses as a copyist or as a dedicatee. This very flexible data model was regularly modified and extended to accommodate changes and refinements that were developed in close cooperation with the scholars entering the data as their understanding of the source material and the technical possibilities of the system changed. Examples of theses unique additions are the possibility to record misattributions of authorship and misidentifications of witnesses in existing literature as well as documented reading events and changes of ownership.  This concept of a network of objects with flexible relations, an attribute-graph, exists today in database products like Neo4J Neo4J:    but those were not available in 2006 which led to the development of a custom database called \"OpenMind\". The database software is Open Source, written in Java, uses a conventional SQL database backend and a Web-based frontend.  A first version of a public website presenting a limited set of 130 codices by the Staatsbibliothek Berlin with digitalizations was published in 2015.   Towards new standards The current database system OpenMind was a custom development which was necessary at the time of its creation but has not aged well and burdens the future development of the project with limited flexibility and high maintenance for software development. The data model was also not created based on existing ontologies due to a lack of usable tools at the time. Both features were acceptable during the development of the project but they pose a problem to the continued maintenance of the project and the reusability of its data. Currently both software and data are migrated to new standard tools in two phases: In the first phase data is still entered in the legacy OpenMind backend but there is a new public web frontend based on the Drupal CMS that is fed by an XML export from the legacy backend. The XML data is also fed into a Neo4J graph database for additional queries and visualisations. This is the architecture for the beta launch in September 2018 and the public launch end of November 2018. In the second phase the data model will be migrated to the CIDOC-CRM CIDOC-CRM:    reference ontology using the FRBRoo FRBRoo:    model and other extensions. All data is converted to RDF following the new data model and a frontend based on the ResearchSpace ResearchSpace:    software and a triple store backend is created for data entry and specialized queries and visualisations. This process is currently under way.    The new ISMI website The new public website presents data on 650 persons (selected chronologically following MAMS), 2300 texts, 6900 witnesses and related objects, representing authors from before 1350CE. The website will be public starting end of November 2018. Additional data publications are in preparation. The new web frontend provides browsable lists of all major object types (persons, texts/works, witnesses, codices, places,…) as well as a general search and searches for specific object types. All objects on the pages are linked which makes it easy to get from a person to all their works and their witnesses as well as to the commentaries on the titles and their supercommentaries. The search has a simple normalization for Arabic and a special normalization for romanized Arabic and is specially tuned to be very forgiving for differences in spelling especially for Arabic names. Feedback for the search and navigation during the beta test phase was very positive. The website also shows currently 104 freely available digitized codices using the IIIF IIIF:    image standard and the Diva.js Diva.js:    viewer (see Figure 2). Most of the codices were scanned by the MPIWG in a cooperation with the Staatsbibliothek Berlin but some exemplars from the Gallica project of the Bibliothéque Nationale de France and the Qatar Digital Library are also present to demonstrate the potential of public IIIF image sources in an area that has been plagued in the past with proprietary data silos and restrictive access conditions making global electronic manuscript databases nearly impossible. We hope to expand the amount of scanned codices in the future.     Display of scanned manuscript (Codex Petermann I 671, Staatsbibliothek Berlin)   The experimental “ISMI Lab” section of the site offers access to the “Query Builder” tool which allows to construct custom queries to the database based on objects, attributes and relations and a full Neo4J graph database console with access to all published data (see Figure 3). These additional tools are very powerful but require some technical expertise and familiarity with the ISMI data model. There is some documentation but this section is more of an experimental offer to also get in contact with interested scholars in the hope that interesting queries and research questions can be exchanged and new, easier to use, tools can be developed in the future.    Experimental Neo4J console showing partial graph of commentary relations.     A never ending project? The history of the project in the last ten years has shown the difficulties of developing and maintaining a project of this complexity – organisationally, in terms of hardware, software, and scholarly support. We think this project shows the potential for a unifying manuscript database that is not limited to singular collections and presents the continually updated and expanded current knowledge of scholars in the field. We hope that scholars in the future will not have to figure out errors in decades-old printed catalogues individually again and again but that they can participate in a common database and share and enhance their individual findings. The collaborative phase of the ISMI database is only beginning and we would like to start the discussion now. We think we have laid the technical foundations to make the database maintainable and adaptable and the data shareable and linkable but the long term value of a shared resource lies in its users and its contributors.   ",
        "article_title": " A Database of Islamic Scientific Manuscripts — Challenges of Past and Future  ",
        "authors": [
            {
                "given": "Robert",
                "family": "Casties",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science, Germany",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "manuscripts description and representation",
            "sustainability and preservation",
            "English",
            "history of science",
            "near eastern studies",
            "databases & dbms",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Summary This paper is a report and critical account of research undertaken in the project  Made by Machine: When AI Met the Archive ( https://www.bbc.co.uk/programmes/b0bhwk3p). In this project we used three computational approaches to analyse and automatically browse the BBC television archive; we then proposed a novel way of combining these approaches through machine learning by fitting their outputs to a recurrent neural network as a time-series.  Through these methods, we were able to generate new sequences out of a dataset of short clips of archive footage. These sequences were then edited and packaged into a television programme broadcast on BBC Four in September 2018. This is, to our knowledge, the first time machine learning has been used in this way to recast archive footage into original prime-time content for television. In this paper, we first frame  Made by Machine as a project inscribed in the political economy of datafied cultural production. We then describe the technological approaches we used to traverse archive space, learn and extract features from video, and model their relations through time. And finally, we introduce the idea of  computational spectatorship as a concept to analyse the objects and practices of automated seeing/editing of moving imagery through machine learning.    The television archive as cultural big data In public discourse we are constantly reminded that how we learn, what we enjoy, and the ways we conceive and exercise power, are all consistently mediated by imagery. In various academic disciplines this pre-eminence of imagery in all spheres of human activity has been referred to as visual culture or the visual turn (Mirzoeff, 2002; Jay, 2002). It has been further recognised that digital and networked technologies have dramatically increased the role imagery plays in society. According to an estimate already 23.2% of files available online in 2003 were images (Lyman and Varian, 2003) and this is before the rise of YouTube and Netflix in the mid-2000s. A more recent report by CISCO predicts that by 2022 video will account for more than 80% of the world’s IP traffic. If this is the case, by then we will be collectively watching through Video-on-Demand (VoD) systems the equivalent to 10 billion DVDs per month (CISCO, 2018: 2). Arguably, out of all the cognitive-cultural production today, moving imagery is the most semantically ubiquitous, the larger in size, and one of the most semiotically complex insofar as it combines visual, linguistic, and aural modes of expression. In their abundance and complexity moving images are in today’s networked societies one of the most challenging aspects of contemporary culture, and one that increasingly demands social and academic attention. In this context, while the volume of digital video grows, and its formats continue to diversify, the use of computational techniques to access, analyse, produce and reproduce vast collections of digital moving imagery becomes a pressing issue for organisations and archives that deal with audio-visual production (see: Kuhn et al., 2015; and Ward and Barker, 2013) One of such collections of is the BBC television archive, which amounts to approximately 700,000 hours of television distributed in about 400,000 programmes in various formats, including items originally stored in film and magnetic tapes that have now been digitised, and new programming which is preserved as born-digital data (British Film Institute, 2018; Lee, 2014). A significant portion of this archive is now available through the internal  Redux system, which allows on-demand access to high-resolution video stream files through an online interface (Butterworth, 2008).  In the rapidly changing landscape of digital entertainment industries, where large corporations like  Netflix or  Amazon are producing original video content and are —in industry parlance— leveraging their own moving image collections as profitable datasets, the BBC is increasingly pushed to recognise the value of their archives as data. In 2017, BBC R&D signed a five-year partnership with eight UK universities to “unlock the potential of data in the media” by creating a framework to make BBC data available for research (Chadwick in BBC R&D, 2017). To unlock such potential can be understood in this context as extracting value from this data, but unlike  Amazon or  Netflix, the BBC is a public corporation funded largely by tax-payers to fulfil public purposes ( https://www.bbc.com/aboutthebbc/governance/charter). The British broadcaster therefore needs to grapple with the questions of what types of data and what kinds of value it extracts from its television archives. These questions require the broader intellectual frameworks of the humanities to place  Made by Machine into context.    Seen by Machine Made by Machine was a project commissioned under a simple premise: to create a TV programme out of archive footage using machine learning. The project was conceived, produced and delivered between May and August 2018, and it aired under the category of documentary on BBC Four in September as part of a two-night experimental programming block called “AITV on BBC 4.1” ( https://www.bbc.co.uk/programmes/p06jt9ng).  Most of  Made by Machine was developed internally by a small group of technologists at BBC R&D, in collaboration with an external researcher from King’s College London. This team designed the computational methods to analyse archive footage and produce new video sequences out of it; the sequences were then packaged and delivered as a documentary by a producer who commissioned a 3D-rendered talking head and a presenter whose commentary bookends the sequences.  First, a Support Vector Machine (SVM) was trained on existing programme metadata “to predict whether or not a given programme was broadcast on BBC Four” (Cowlishaw, 2018). Using this, an ad hoc subset of programmes was selected, and then segmented into several thousand clips that ranged from tens of seconds to several minutes using an algorithm developed internally at BBC R&D (Cowlishaw, 2018). These clips were then used as a dataset to be explored and manipulated using four methods: 1. Object detection 2. Subtitle analysis 3. Visual energy 4. Mixed (generative) model Object detection The  Densecap system ( https://cs.stanford.edu/people/karpathy/densecap/) was used to automatically annotate the dataset of clips and then these annotations were used as metadata to generate the sequences by similarity.  Densecap is a computer vision system designed to “localize and describe salient regions in images in natural language” (Johnson et al., 2015, p.1). It is trained on the visual genome dataset ( https://visualgenome.org/), a crowd-sourced set of densely annotated images.  Subtitle analysis A pre-trained  Word2Vec language model ( https://code.google.com/archive/p/word2vec/) was used to create word embeddings of the subtitles of the programmes, and then term frequency–inverse document frequency (TFIDF) to retrieve clips and generate the sequences based on word salience and similarity.  Visual energy MPEG-2 video encoding was used to extract a frame-to-frame motion vector of pixel colour difference. This signal was then used to rank every clip in the dataset according to this representation of video dynamics (also preserving the variation information at the frame level). Mixed mode The features learned/extracted from the previous three sections were then used as inputs to train a long short-term memory (LSTM) machine (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) to learn a model of the relations between the features across time. The model was later used to generate new similar sequences. Through these four techniques the team aimed to represent digital video in terms of three of its dimensions: depiction (visual), dialogue (linguistic) and motion (plastic). One significant contribution in our approach is to try to model moving imagery as a multi-dimensional time-series problem: analysing video streams as a sequence of para-symbolic units whose ordering in time is integral to the creation of meaning.   Computational spectatorship Finally, we conceptualise this family of machine-learning approaches as machine-seers. These machine-seers mediate our relationship with large audio-visual collections and provide novel ways of automatic browsing as a form of automated editing. We argue that machine-seers enable a new and specific modality of vision, one which depends heavily on machine-learned representations not only of digital video features but, crucially, of spectatorship. Machine-seers are built on top of existing computer vision and other machine learning techniques, but we conceive them as a second generation of systems designed to learn and replicate complex viewing practices in and across media; they activate in audiences new visual dispositions and capacities, and as such they enable particular ways of seeing (see: Berger, 1972).  One significant advantage of framing a project like  Made by Machine in this way is that theories of spectatorship enable us to think of the imagery produced through machine learning systems as images of the systems themselves; their aesthetics include the technological and social conditions for their existence.  We therefore argue that  Made by Machine can be understood as one early instance of a new aesthetic modality: the datamatic (Ikeda et al., 2012). Unlike the cinematic or the televisual before it, the datamatic is first and foremost a form of computational spectatorship: more than novel ways of  creating imagery, machine-seers afford us with novel ways of  enjoying imagery; they fetishise calculation and turn the datafication of society into its own form of spectacle. Through this  spectaculum ex computatio, datamatic watching allows us, potentially, to enjoy sequencing without continuity, narrative without authorship and, ultimately, presence without subject.   ",
        "article_title": "Seen by Machine: Computational Spectatorship in the BBC television archive",
        "authors": [
            {
                "given": "Daniel Alberto",
                "family": "Chavez Heras",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Tobias",
                "family": "Blanke",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Tim",
                "family": "Cowlishaw",
                "affiliation": [
                    {
                        "original_name": "BBC, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jakub",
                "family": "Fiala",
                "affiliation": [
                    {
                        "original_name": "BBC, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Amaya",
                "family": "Herranz Donnan",
                "affiliation": [
                    {
                        "original_name": "BBC, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Man",
                "affiliation": [
                    {
                        "original_name": "BBC, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "methods and technologies",
            "multimedia",
            "artificial intelligence and machine learning",
            "English",
            "film and performing arts studies",
            "cultural analytics",
            "computer science and informatics",
            "audio",
            "cultural artifacts digitisation - theory",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Recent computational work has analyzed the significance of gender in characterization, investigating the extent that character descriptions are sorted along a feminine-masculine axis. Matthew Jockers and Gabi Kirilloff, for instance, tabulate pronoun-verb bigrams, exploring the connection between characters’ actions and their gendered representation in nineteenth-century novels (Jockers and Kirilloff, 2016). They show evidence of a stable relationship between gendered characters and the verbs they perform. Ted Underwood, David Bamman, and Sabrina Lee chart a broader range of character descriptions from the past two centuries, measuring the difference between the words describing fictional men and those describing fictional women. They demonstrate that the implicit differences between gendered characters becomes less and less clear as we move towards the twenty-first century. So while the former study argues that characters’ actions reveal gender’s steady prominence, the latter research suggests that those overarching gender divisions might actually be diminishing. But these seemingly disparate arguments should not be taken as contradictory. Rather, these varying conclusions should reinforce a more complicated sense of how “some forms of gender differentiation…are declining while other forms…are on the rise“ (Underwood et al., 2018). To further explore the varying degrees and modes of gender differentiation, I employ quantitative methods to investigate gender’s prominence in the configuration of characters’ bodies. This research contributes to that ongoing research, analyzing characters’ physical depiction throughout a collection of around 15,000 English-language novels. By producing a model of gender based solely on characters’ physical features, I explore the extent that literary embodiment is defined along a feminine-masculine axis. And I pursue two central claims that complicate existing models of character and gender. The first is that bodily description becomes an increasingly prominent aspect of characterization. In fact, an increasing proportion of character description is devoted to detailing the anatomical features of both fictional men and women. Secondly, those characteristics are increasingly deployed along gendered lines. As we move towards the twenty-first century, men and women are more and more embodied using different words. Even seemingly innocuous attributes such as “blue” and “red” function as consistent signs of gender. Those two patterns form a suggestive parallel: as the body became an expanding aspect of characterization, that dimension was increasingly organized along gender stereotypes.    Methods and Tools In order to gather characters’ physical descriptions, I needed a way to separate characters from each other and tabulate the words embodying them. Bamman et al.’s BookNLP pipeline has worked well for many similar problems, so I modified it using spaCy libraries. First, it uses coreference resolution to identify character names and cluster them with any synonymous markers in each text. The name “Scout Finch,” for instance, gets clustered with “Scout” and any associated pronouns, treating each of those markers as referring to a single character. Then it uses dependency parsing to tabulate a wide range of words connected to each character.  By default, BookNLP extracts actions characters perform, actions that they’re the object of, adjectives modifying them, and nouns they govern (such as body parts, like “her hand”). Taken altogether, we get several words used to describe fictional people. For my purposes, however, additional words are extracted to capture each characters’ physical description. Whenever a character’s body part is mentioned, “his hands” for instance, I also gathered the verbs and adjectives modifying their bodily features, such as “his hands grasped,” “took her wrist,” and “her blue eyes.” As a result, for each novel in the corpus, we get a frequency table of words used to describe men and women (relative to the total number of words in that novel). We can then subset that table to see which words pertain to the description of characters’ bodies. In effect, this process tabulates the same characterizing words as BookNLP, but it additionally procures and counts the words attributed to their physical features.   Similar to previous research on gender’s significance in characterization, my method comes with a few methodological challenges. In order to separate characters from each other and assign them gender identities, spaCy will identify proper names, and I use Lincoln Mullen’s  Gender package to label those names with a grammatical gender. Mullen’s package uses U.S and North Atlantic census data to accurately predict gender of first names, accounting for shifts in time and geographic location. The problem with using proper names to identify character, however, is that characters referred to by generic nouns are excluded, such as “the baker.” This pipeline attempts to account for this by including characters signaled by stereotypically gendered nouns, such as “the queen” or “the father,” but this does not comprehensively account for majority of generic nouns used to produce characters. Moreover, this study also does not provide any robust solutions to the first-person narrator problem. The pronoun “I” does not consistently connote a particular gender identity, so their bodily configurations are excluded from this essay. In effect, there are certain kinds of characters whose physical features will not be counted in this study.     Analysis Taken altogether, the benefit of this approach is that it accurately assigns gender to named entities and consistently extracts their anatomical features, allowing us to explore the gendered distribution of bodily language. By examining how gender impacted this particular aspect of representation, we can then ask questions about forms of gender contingent upon specific registers of characterization. For example, proportionally speaking, how much space did authors allocate to the physical description of men and women? So to what extent is characterization composed of bodily description? Let’s start by simply taking the number of words that physically describe female characters and divide that by the number of all words describing those characters. Then we perform that calculation for each year. In effect, we’re just plotting the proportion of words that describe women’s bodily features out of all the words characterizing fictive women. The same calculation is done for the fictional men and we compare the proportions. When we perform this calculation for each year, two clear long-term patterns emerge. First, body language becomes a growing aspect of character as we get closer to the twenty-first century. For both female and male characters, more and more words describe bodily features and gestures. Second, physical description consistently tends to be a larger proportion of characterizing women than men. In fact, while women’s bodies are regularly described more than men’s, this gap gets wider the further we move back into the nineteenth century.    So, on the one hand, this picture reflects a well-known story: the body becomes a growing aspect of producing characters. These two slopes provide further evidence of Heuser’s and Le-Khac’s claim that the body becomes steadily more important in fiction, showing that it was specifically important to describing characters across the past two centuries (Heuser and Le-Khac, 2012). This isn’t to say that the body was only becoming important during the twentieth century. There’s a lot of evidence to the contrary. Rather, the interest in characters’ physical features, appearances, and actions seems to continually grow over the past two centuries. On the other hand, there is another important trend, characterizing women involves a greater proportion of bodily description than characterizing men. In fact, as we move from the 1850s to the 2000s, that gap remains jarringly stable. The pattern remains intact even when physical characteristics were becoming more prominent for all characters. This suggests that the body has historically played a larger role in representing women. That’s an important facet of literary history, because it underscores the extent that characters’ physical descriptions are imbricated in gender discourses. Feminist scholars have, of course, already captured important parts of this story. Butler’s argument about gendered bodies, for instance, hinges upon her claim that there are “cultural associations of mind with masculinity and body with femininity” (Butler, 12). Figure 2 doesn’t completely verify this alignment of women with the body. Rather, it is congruent with the latter part of Butler’s claim, showing that the representation of women tends to rely more heavily on bodily language. But to what extent were fictive men and women embodied in different ways? In order explore to that question, binary classification methods have proven effective at modelling the weight of ideological categories. At its core, this method tests to see how well individual elements can be sorted into two related categories. In our case, we want to test whether bodily nouns, verbs, adjectives, etc. are consistently attributed to either fictive men or women. This means, first, taking multiple random samples of bodily words from each decade. Then each descriptor is labelled according to a characters’ grammatical gender. By showing a model a large number of these labelled words, we train it to develop a stereotypical sense of what attributes constitute a stereotypically “feminine” or “masculine” body. Finally, we instruct the model to use its sense of gendered bodies to make gender predictions about characters it hasn’t seen yet. If, for instance, physical characteristics are predominantly distributed along gendered lines, this will allow the model to consistently make accurate gender predictions. On the other hand, if body language and gender are generally unrelated, then the model will be less capable of accurately inferring gender from that language. If we train a classifier to see how well it can predict character gender based on physical characteristics, as a proxy for the strength of gender stereotypes, we see two overarching patterns : First, up until the 1970s, the model gradually has an easier time inferring the gender of characters from their physical characteristics. During that period, the model’s percentage of correct gender predictions rises from about 76% to about 83%. What this suggests is that words describing the body are becoming increasingly bifurcated along normative gender lines.  Second, after the 1970s, however, characters’ physical features appear less and less distributed along a feminine-masculine axis. The percentage of correct predictions drops back down to roughly 77% as we reach the 2000s. By contrast, this seems to indicate that the association between body language and gender is changing, and the bodily differences between fictive men and women has recently diminished.     More quantitative evidence will, of course, be needed to feel confident about these figures, but this approach takes a few steps in order to test the strength of this pattern. This figure was produced by running fifteen different models of physical description within each decade. Each of those models is produced using the physical descriptions within 350 randomly sampled novels, selecting 450 characters at a time (225 men and 225 women), balancing the sample to contain an equal number of men and women’s features, classifying them using the top 330 most commonly occurring words. This winnowing strategy comes at the cost of ignoring less frequent physical descriptions. For example, the adjective “emerald” is sparingly used to describe eye-color, so it is often excluded from each model. These sparsely deployed features can certainly be significant signs of gender, but the benefit of this approach is that it analyzes gender’s prominence within pervasively deployed physical features, such as having a “nose” or being attributed with “brown” hair.  ",
        "article_title": "Enumerating Gendered Bodies In Two Centuries of English-Language Fiction",
        "authors": [
            {
                "given": "Jonathan",
                "family": "Cheng",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska-Lincoln, United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "corpus and text analysis",
            "data mining / text mining",
            "gender studies",
            "English",
            "cultural analytics",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A map can be thought as the fixture of certain places in a time to an artifact of indefinite longevity. The fixture was completed at the moment when the artifact had been produced. The places depicted on the map, however, can always be re-interpreted. We are interested in technical arrangements about historical maps upon which new possibilities can be experimented. We report on our experience in re-basing and rendering of early 20th Century land survey maps upon the OpenStreetMap technical infrastructure. Successive layers of digitalization and transformation are performed so as to bring out new representations from the old maps. Populated places, road networks, administrative centers and boundaries, water systems and diverse landscaping areas – surveyed and mapped long ago on paper – become digital objects waiting to be examined and navigated online. These places, connecting us to the past as well as to the present, can now be perceived anew.  Old Maps, New Tech Maps are rich sources of geospatial information. Old maps reveal how people saw the places in their world. Researchers take hints from historical maps to help them connect the places of today to those of the past. Change in place names, the vanished old trails and the emergence of new roads, or the shifting boundaries of settlements, all these details could well be observed when land surveys from different periods are compared for a particular region. There are stories to be told about how these differences come to exist. We distinguish three kinds of digitalization for historical maps. One is built on top of the other in stage. In the first stage, paper maps are scanned into digital images. The change from paper media to digital media makes the maps more accessible and open for other use. In the following stage, geospatial features in the images are identified and extracted into programmable objects. The shift from visual features to digital objects facilitates transformative and analytic use of the information from the maps. In the last stage, the maps, now exist as collections of digital objects, are reprogrammed for new functions and novel cognitions. The three stages can be short-termed as the digital (from paper to image), the informational (from features to objects), and the perceptional (functional and cognitional reconfiguration) stages. In this report we focus on experimentations in the perceptional stage. In particular, we map-out and re-style early 20th Century land surveys of Taiwan on top of the OpenStreetMap infrastructure (the software system, not the map content). Work in the digital and informational stages, that is, image scanning and feature extraction, is performed (and had been reported elsewhere) before the work in this perceptional experimentation can start. Reuse the OpenStreetMap infrastructure has many benefits. The software is open source, so it can be freely modified for experimentations. The modifications to the software are also free to redistribute, so the experimentations and the results can be reproduced by others. The development of OpenStreetMap is by a grassroots community effort where innovations occur quite frequently. In this experiment we use tools from others, and the tools we have developed are also free for others to use. The data format used in OpenStreetMap is quite straightforward (nodes, ways, and relations) and there are converters to and from other data formats. The styling of maps on OpenStreetMap is also customizable. Indeed we rely on this ability in OpenStreetMap in order to render datasets from old maps into new styles.   Taiwan Baotu: Maps of Early 20th Century Land Survey The Taiwan Baotu was a collection of topographic maps published in 1906 (when Japan ruled Taiwan). It is the output of an island-wide land survey, and the collection includes in total 457 maps covering a major part of Formosa and the Pescadores Islands. The maps also incorporated many other types of geographic information. The maps illustrate administration areas with their detailed boundaries. The maps contain a wealth of place names in use in Taiwan at that time. It includes details about land use, transportation, as well as landmark and landscaping information about Taiwan in the early 1900s. An image from the Taiwan Baotu collection is shown in  Figure 1.       Figure 1. Map on the area around Sinhua (新化) Township (high-resolution image available online at:  )    Re-imagining Taiwan Baotu in OpenStreetMap Once the feature datasets have been extracted from the Taiwan Baotu, all kinds of visual styling can be applied to them in the OpenStreetMap to generate new maps. An identical dataset can be rendered into different maps of the same area. We present below a sampling of the styles available in our current implementation. Please note that we merge in all datasets extracted from the individual maps from the Taiwan Baotu collection. In our OpenStreetMap realization of the Taiwan Baotu, there are no longer 457 maps, but one dataset for the entire collection. Further, the map can now be zoomed in and out in different scales. For illustrative purposes, in the following we use the area roughly corresponding to the map in  Figure 1.   Figure 2 shows a new rendering of the area, following mostly the original Taiwan Baotu map style. However, new markers are used to re-style certain keys in the original map. For example, the Chinese characters for trees (林) are now used to replace the symbols for trees. The markers are adaptive to different zoom levels which result in nice visualization. In  Figure 3 , the dataset is rendered to emphasize different landscaping areas and their boundaries.  Figure 4 is the rendering of the Baotu dataset with the default OpenStreetMap style to give the old map a modern look.  Figure 5 is the current OpenStreetMap map of the area. Comparing details in  Figure 4 to  Figure 5 using the same mapping style help us better perceive changes to the landscape of the area.  Figure 6 and  Figure 7 are zoomed out views of  Figure 4 and  Figure 5 respectively.  We experiment with other styles in rendering the Taiwan Baotu datasets.  Figure 8 is the rendering of the Tainan and Anping area, using the Ink style.  Figure 9 is the area rendered in the Green style and  Figure 10, just for fun, is the area in the Ukiyo style. The rendering of Taiwan Baotu datasets into maps of different styles can be  experimented online.       Figure 2. New rendering of the area around Sinhua, original Baotu style mostly      Figure 3. New rendering of the area around Sinhua, matching texture for landscaping      Figure 4. New rendering of the area around Sinhua, the modern OpenStreetMap look      Figure 5. The current OpenStreetMap map of the Sinhua area (c.f.  Figure 4)       Figure 6. The Sinhua area in Taiwan Baotu, in modern OpenStreetMap look, zoomed out to a different scale      Figure 7. The current OpenStreetMap map of the Sinhua area, zoomed out (c.f.  Figure 6)       Figure 8. Rendering of the Tainan and Anping area, in the Ink style      Figure 9. Rendering of the Tainan and Anping area, in the Green style      Figure 10. Rendering of the Tainan and Anping area, in the Ukiyo style   Background and Workflow Our transformation of the Taiwan Baotu proceeds in stages. There are technical choices and styling decisions to be made along the re-imagination process. Our experimentations naturally are limited by the functionalities made available by the OpenStreetMap infrastructure. We depends thoroughly on existing technical tools and map datasets as well. The above factors however should not be considered just as constraints. Rather, upon these tools and datasets we are able to quickly test ideas and build systems. They are accessible and reusable, hence greatly help experimentations. We are very grateful. The initial experiment we set out for ourselves is to re-make the Tainan portion of the Taiwan Baotu. Previously we had access to a collection of Tainan region image files digitized from the Taiwan Baotu paper maps. We were provided with several datasets of vectorized features already extracted from the images. The datasets are in the form of ArcGIS shapefiles. The shapefiles were converted into OpenStreetMap datasets with the help of available tools. They were rendered, in the default OpenStreetMap style, and put online by an OpenStreetMap server hosted by us (Huang et. al., 2015). In the Summer of 2017, we began to re-do the Tainan region with the goals of achieving better data quality and gaining more flexibility in map styling. We (re-)digitalized the entire Tainan region by vectorizing map features drawn from the Taiwan Baotu online imagery service provided by the Center for GIS, Academia Sinica, Taiwan ( ). The features not only are recorded for their geometries (points, lines, and areas) but also for their semantics (map symbols). These features are annotated with the tags used in the OpenStreetMap datasets ( ). OpenStreetMap tags are pre-defined key-value pairs describing specific features of map elements. In a way, we use OpenStreetMap tags to re-model the semantics of Taiwan Baotu map symbols. At the same time, we keep an image catalogue of all the Taiwan Baotu map symbols.  We then use the TileMill editor ( ), with the help of the image catalogue above, to define a Taiwan Baotu map style for use by the OpenStreetMap tile server, so that map features extracted from Taiwan Baotu can be rendered online in a way similar to the original paper map style. This is the Ink style shown in  Figure 8. We produce two more map styles: a Green style mainly for illustrating different types of land use, and a Ukiyo style mostly for style experimentation and for fun. Map styles produced by TileMill are provided as style configurations to the Mapnik toolkit ( ) in OpenStreetMap to render map datasets into map tiles of the prescribed styles. Note that, no additional style configuration is required if datasets are to be rendered with the default OpenStreetMap map style ( ).  For easy comparisons, we use OpenLayers ( ) to overlap Taiwan Baotu, in various styles and all drawn from the same OpenStreetMap server, with other online maps (e.g. modern day OpenStreetMap or Google Map). This results in an experimental Taiwan Baotu (Tainan region only) map service ( ). The overall workflow is shown in  Figure 11 which is itself derived from the component diagram in the OpenStreetMap wiki ( ). The parts marked in red are new or modified for our experimentations.       Figure 11. Workflow diagram, the parts marked in red are new or modified   Discussion We have rendered the Taiwan Baotu datasets, which are extracted from maps produced more than one hundred years ago, using the OpenStreetMap infrastructure. The experimentations are both technical and humanistic. We try out to see how difficult the task is and how the maps look like. We are satisfied with the outcome. Our experimentations are humanistic in the sense that we see such transformative use of historical maps beneficial to researchers, teachers, and students in the humanities. Re-imagining the past with accurate datasets rooted in historical maps, we feel, could well generate interesting ideas for more research. There is a wealth of literature on cultural cartography and visual cognition which we have not touched upon. We wish to draw lessons from these research areas and to continue our experimentations.  ",
        "article_title": " Maps Re-imagined: Digital, Informational, and Perceptional Experimentations in Progress  ",
        "authors": [
            {
                "given": "Tyng-Ruey",
                "family": "Chuang",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taiwan",
                        "normalized_name": "Academia Sinica",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bxb3784",
                            "GRID": "grid.28665.3f"
                        }
                    }
                ]
            },
            {
                "given": "Chih-Chuan",
                "family": "Hsu",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taiwan; National Cheng Kung University, Taiwan",
                        "normalized_name": "National Cheng Kung University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/01b8kcc49",
                            "GRID": "grid.64523.36"
                        }
                    }
                ]
            },
            {
                "given": "Huang-Sin",
                "family": "Syu",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taiwan; National Cheng Kung University, Taiwan",
                        "normalized_name": "National Cheng Kung University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/01b8kcc49",
                            "GRID": "grid.64523.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-01",
        "keywords": [
            "software design and development",
            "methods and technologies",
            "user experience design",
            "modeling and visualization",
            "interface",
            "gamification",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "computer science and informatics",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Collaboration is fundamental to digital humanities work and DH researchers and practitioners spend significant effort, time and resources on collaborative processes. Additionally collaboration is frequently necessary and actively encouraged by funders (AHRC, 2019; NEH-DHAG, 2019) and yet little formal discourse and attention is given to this topic in DH publications and project reports  (Griffin and Hayler, 2018; Lawrence, 2006). In an attempt to address this lack of dialogue, this short paper introduces a project that aims to map and document the collaboration of multiple diverse partners, in a large-scale distributed digital humanities project.   The Georgian Papers Programme (the Programme) is a ten-year interdisciplinary, transatlantic project to digitize, conserve, catalogue, transcribe, interpret, and disseminate 65,000 manuscript items from the British Royal Archives and Royal Library relating to the Georgian period and its monarchs, 1714-1837.  The corpus contains letters, diaries, account books, inventories, household records and more. The Programme is a partnership between the Royal Collection Trust and King’s College London and is joined by primary United States partners the Omohundro Institute of Early American History & Culture and William & Mary Libraries.  It is a collaborative enterprise in almost all facets. This is by design and on occasion, especially with regards to the cataloguing, metadata enhancement and data exchange, by necessity. The whole process is very much a learning curve for all parties concerned and significant teamwork and cooperation is absolutely key.    Project & framework The Programme is a distributed project which involves partners in the areas of archivists, librarians, academics/researchers in a variety of disciplines, and research software engineers.  Partners are responsible for development of specific areas and products that ultimately are expected to be integrated into the wider Programme ecosystem. King’s Digital Lab (KDL) and William & Mary Libraries (W&M), both technical partners, each lead the development and production of core portions of the ecosystem: KDL is producing a digital collaborative workspace and enhancing metadata; W&M is responsible for full-text transcription.  KDL and W&M teams work independently but rely on digitized archival materials from the Royal Archives and on the academic and scholarly community that is being built around the Programme. Coordination among the partners, and support from colleagues outside of the Programme on development, standards, data exchange, timelines, and product delivery is a continuous process. The leads for transcription at W&M, and at KDL for metadata and documentation have set about to study and capture the Programme’s collaboration activities and efforts.  The study began in the first half of 2019, and is scheduled to continue for the life of the Programme. We are using a mixed methods research approach, that includes an online survey and a number of video interviews asking our Programme partners to reflect upon each year of their Programme work.  Online survey questions are based partially on the IDEA Partnership Success Rating Scale (IDEA, 2006), with questions specifically about communication tools and digital humanities added. The interview questions are derived from the INKE e-research collaboration study (Siemens and INKE, 2012), and tailored to this project.  Analysis of the data is conducted using grounded theory methodology. The study focuses on collaboration in two areas: 1) Group to group Exchange amongst digital projects and research groups is commonplace for DH practitioners.  Not always intentional, the interaction with a fellow DH group easily becomes lost over the course of a large project, and in a field that often requires interdisciplinary teamwork and expertise, capturing this aspect of collaboration is vital for documenting the evolution of DH work, and building on a record of impact, alongside more traditional ‘records’ such as reference citations.  Using information visualization techniques, we map groups and projects that we consider, are influenced by, and actively engage with Programme collaborative workspace development, metadata enhancement and transcription work.  A timeline illustrates the time frames of each collaboration, and the mapped collaborations are categorized based on the level of impact, if any, the group has on the development and processes of our Programme work. Transkribus, a Handwritten Text Recognition tool developed as part of the Recognition and Enrichment of Archival Documents Project (READ), is one example of a DH project that is an essential tool for the Programme and consequently has a large impact in terms of its direct contribution of full-text transcriptions to the collaborative workspace. 2) Human to human The human coordination in DH work, especially on large distributed projects, can be the most stressful and rewarding part of the project.  Dialogue on human to human collaboration (Griffin and Hayler, 2018) in DH projects often happens in informal conversations, is reflective and more about the challenges of the work as much as it is directly about what steps to take next (Siemens and INKE, 2012). This research also aims to capture and chart the human relationships side of collaboration. Conclusion Our presentation will present the visual mapping of group to group collaboration from the initial two years of the Programme, and report on the human to human reflection work.  Through analyzing and mapping collaborative activities in a transatlantic collaboration, we believe this work will contribute significantly to the discussion on the topic of documentation of collaboration in the Digital Humanities. ",
        "article_title": " Transatlantic Collaboration in Digital Humanities The Global Georgians ",
        "authors": [
            {
                "given": "Deborah A",
                "family": "Cornell",
                "affiliation": [
                    {
                        "original_name": "William & Mary, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Samantha",
                "family": "Callaghan",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "management",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "library & information science",
            "organization",
            "digital archives and digital libraries",
            "project design"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The New Approaches to Women’s Writing (NEWW) Network brings together scholars from across the globe to research women writers’ transnational collaborations and reception histories from the early modern era to the twentieth century. The aim is not only to recuperate national histories of women’s writing but also to establish how feminist ideas were disseminated as texts crossed national and linguistic borders. This short paper seeks to introduce the NEWW network and its pilot virtual research environment as it seeks to develop this further. In part the creation of this VRE is born out of frustration with more traditional forms of the conference paper and article which tend to lend themselves to single-author case studies. These research outputs often point towards broader patterns of transnational networking and influence, but corroborating and interpreting these patterns demands an overview of the significant amount of data now stored in the VRE. We have therefore created a number of outputs in an effort to visualise the reception trajectories of key feminist writers’ texts as they crossed national borders, often appearing in translation in other countries. The NEWW VRE was created to answer some of these questions, originally as a standalone database, later it was adopted as part of an overarching internally-funded project at Newcastle University called ATNU (Animating Text Newcastle University –  http://research.ncl.ac.uk/atnu). This project has initiated a number of digital humanities projects looking at text in various forms, and this was an important early pilot to map the appearance of translations of feminist literature called “Mapping translations of feminist literature in Europe 1750-1930”. This was initially developed with ATNU using fairly simple methodologies and then later as a full pilot expanded in order to identify the mechanisms and contexts for the transnational development of feminism before the so-called First Wave.   The technological implementation has since been significantly expanded through the use of custom maps built on top of a D3 visualisation library. This allowed for the use of vector images detailing the changing borders of Europe to be used instead of the modern borders available via Google Maps. The initial interface was replaced by a full relational database to support more advanced queries and store more metadata, particularly around the evidence sources. The pilot project selected key early modern feminist writers and plotted their reception data on a European map. The data is categorised by reception type and includes: translations; adaptations; reviews and articles; evidence of reading; and texts where demonstrable influence has been established. This data can be visualised either as distinct markers on the map or as a ‘heat map’, whereby the densest clusters of reception data appear in a range of colours depending on the amount of data attached to them. ‘Hot spots’ highlight areas of particular interest and the connections between the different texts. The map also has a timeline which allows the user to ‘travel’ through time and view the developing reception of a given text or writer. The map is dynamic, changing with the date on the timeline so that the user sees the national borders in Europe shift over the course of the eighteenth and nineteenth centuries. This is important because the significance of particular ‘clusters’ of reception data may be illuminated by political and cultural connections between nations would not be readily apparent on a modern map. Given the relatively small size of the testing dataset, any conclusions drawn from the distribution of results are naturally skewed. In selecting the test dataset, however, questions about the criteria for inclusion in the complete resource have surfaced: what constitutes feminist writing, how to compensate for spotty data in the sources, and how to weigh the different types of reception to more accurately represent the spread of early feminist ideas -- these aspects will be refined and expanded in the course of the larger project which will follow. In addition to introducing the NEWW network and its pilot project this short paper will discuss a number of issues more directly of interest to a DH2019 audience. These include the problems of mapping with historical datasets: although we wanted to provide digital maps, the dataset is historical so merely plotting these points on a modern Google Map would be obviously misleading. Sourcing historical maps in a useful form in itself is a problem – since the date range covers some of the most turbulent years in European history, it has constant border changes throughout the time period of the data. The provision of historic open-source borders in a useful format proved difficult but eventually a series of over twenty maps of Europe after major border changes were sourced from the Leibniz Institute of European History, provided by Andreas Kunz ( http://www.ieg-maps.de/Welcome.html). Each map contained a number of artefacts not relevant to this project, so each was edited using Adobe Illustrator to resize and remove unneeded sections before saving into SVG. The NEWW project also faced additional difficulties, for example, in the categorisation of evidence types into a hierarchy that members of the network could agree upon. For example, a translation was judged to be a more significant reception text than a brief mention in a letter. Moreover, evidence for the existence of translations comes from a number of different sources. Each source type needs to be ranked against all other types to build up a score that represents how certain the members of the network are that any given translation existed at that time and place. As much of the data is fragmentary or has degrees of uncertainty, there were a number of issues with the visualization of this uncertainty.   This short paper will introduce the network, the ATNU pilot project that led to the revamped NEWW VRE, and the resulting website itself. It will look for feedback and seek to open dialogue among DH scholars working on related topics, not only on feminist writers but those encountering similar challenges on the technical development of historical datasets. ",
        "article_title": "New Approaches to Women’s Writing Virtual Research Environment",
        "authors": [
            {
                "given": "Laura",
                "family": "Kirkley",
                "affiliation": [
                    {
                        "original_name": "Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            },
            {
                "given": "Tiago",
                "family": "Sousa Garcia",
                "affiliation": [
                    {
                        "original_name": "Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            },
            {
                "given": "James",
                "family": "Cummings",
                "affiliation": [
                    {
                        "original_name": "Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Turner",
                "affiliation": [
                    {
                        "original_name": "Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-02",
        "keywords": [
            "feminist studies",
            "English",
            "translation studies",
            "cultural analytics",
            "databases & dbms",
            "information architecture and usability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The University of Glasgow’s  Historical Thesaurus of English (HT) arranges all the recorded words in the last millennium of English into almost a quarter of a million concepts. The work of half a century, it is available online (at www.ht.ac.uk) and a second edition is underway. This edition draws upon editorial work conducted by the  Oxford English Dictionary (OED) in its ongoing third edition, and thus a crucial activity in creating the new edition of the  Thesaurus is the meshing of the Glasgow database with the separate data held by the OED. This paper describes the processes developed by the HT editorial team to tackle the complex task of linking these datasets, allowing rapid updates to be made to the HT and the OED. Through these means, it illustrates challenges associated with the linking of complex, structured data comprised of a mixture of text and numerical information, discusses methods developed by the team, and evaluates the effectiveness of the different linking techniques.   Background The HT’s database evolved over forty years of digital humanities work at Glasgow, pre-dating even the concept of a relational database or a primary key (Kay and Chase, 1987; Wotherspoon, 1992). At the time the data was sent to the OED for publication of the first edition, there were no unique IDs yet assigned to pieces of  Thesaurus data. As a result, the production of the online HT and the ‘Historical Thesaurus’ feature of the  OED Online involved the respective teams creating separate key-indexed versions of the database. The linking of HT word senses to OED word senses was a painstaking process which at times required case-by-case analysis by OED editorial staff which occasionally resulted in minor alterations to the thesaurus structure or reassignment of word senses to different categories. On rare instances in which a headword has more or fewer senses in the HT than in the OED, the OED editors had to make decisions about how best to combine the relevant data. This work allowed a one-way connection from the original HT to OED data, but not back to the online HT produced at Glasgow.  However, while the OED’s data is more up-to-date, in places the HT’s data is richer, such as in its 800,000 manually-created complex statements of usage dates. A reciprocal arrangement between the OED and the HT means that the editorial teams now share data, with the result that the OED has access to the classification experience of the HT editors, whereas the HT team receives updates on word senses and their dates of activity as established by the OED team. These data updates form the basis of the  Thesaurus’ second edition, and so in order to bring such updates into the HT database, the editorial team must create a data linkage between the HT and the OED which had not previously been attempted.    The Linking Process A multi-stage linking process was developed. The first stage has involved aligning the hierarchies of the HT and OED datasets and verifying this alignment; the second stage consists in finding matches for the categories which appear not to have been successfully aligned during the first stage. A third stage will involve linking lexemes within categories. The initial alignment stage was achieved using category code numbers which exist in both the HT and the OED data (e.g. ‘01.05 Animals’, ‘01.05.17 Reptiles’ in the HT data). The OED editorial team made adjustments to some areas of the HT hierarchy (e.g. ‘01.10 The Universe’ in the HT became ‘01.01 The Universe’ in the OED structure). These known changes were accounted for by adjusting the HT category numbers accordingly when aligning them to the OED categories. An initial script checked for matches in the category code, part of speech, and text heading; this process confirmed over 200,000 category matches, leaving around 27,000 for which matches either could not be confirmed in this manner or which had no provisional match between the two datasets. The number of confirmed matches between the aligned hierarchies was increased using a suite of techniques allowing for known variation between the HT category and OED category headings (e.g. OED’s house style recommends ‘relating to’ where the HT prefers ‘pertaining to’; OED uses ‘/’ where the HT uses ‘or’). For quality assurance, matches confirmed in this way were required to meet additional criteria, including a minimum number of shared lexemes within the category and matches between elements of lexeme metadata (cf. Fig. 1, below). Following these processes approximately 5,000 categories containing lexemes remained either with unconfirmed matches or entirely without a potential match.    Figure 1 Sample view of category lexeme and metadata used in linking and QA processes In the second phase, new potential matches for these 5,000 categories were sought. Methods used included looking for matches with ‘sibling’ categories at the same tier in the HT hierarchy (e.g. HTOED subcategory ‘03.01.06| 05 elevate or raise to a higher position’ matches HT ‘03.01.06| 08 elevate/raise to a higher position’). Further methods used lexemes which only appeared once in each of the unmatched datasets, as well as the creation of ‘date profiles’ for categories based on first citation dates for their constituent words. An intractable residue of approximately 1,000 categories were manually matched by project assistants.  The linking between the datasets is work in progress and, at the time of abstract preparation, work has concentrated on matching at the category level. The next phase will match lexemes within the now linked categories, accounting for the knowledge that some lexemes may have been moved between categories as the result of editorial work conducted by OED staff in preparing the OED’s 3rd edition. By July 2019 this lexeme matching phase should be complete, and the final paper will also address the methods and challenges involved in this part of the matching process. When both categories and their lexemes are confidently matched, the HT team can begin the important work of using OED3 data to both update the dates for which words can be evidenced as active, and introduce words which have been added to the OED but which are not, as yet, represented in the HT. Accurate linking between the two datasets will allow automation of the update process in future as OED3 updates are periodically released, thus ensuring the continued accuracy and utility of the resource for academic and public use.  ",
        "article_title": "(Re)connecting Complex Lexical Data: Updating the Historical Thesaurus of English",
        "authors": [
            {
                "given": "Brian",
                "family": "Aitken",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow, United Kingdom",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Marc",
                "family": "Alexander",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow, United Kingdom",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Fraser",
                "family": "Dallachy",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow, United Kingdom",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "semantic analysis",
            "linguistics",
            "lexicography",
            "English",
            "philology",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Nowadays, historians gather art historical data from secondary sources, such as cataloguing records issued by cultural institutions and multipurpose sources. Online catalogues of art historical photo archives may include detailed information about authorship attributions. However, sources may provide contradictory information. For instance, according to the Zeri’s Foundation the artwork “Three Graces” is ascribed to Peruzzi Baldassarre. Several bibliographic references support the attribution. A discarded attribution to Bernardino Luini’s workshop is also recorded, supported by Christie’s auction firm (https://tinyurl.com/y9mkrh7r). The Berenson library records the same attribution supported by one bibliographic reference (http://id.lib.harvard.edu/via/olvwork634683/catalog). The Frick Art Reference Library records an anonymous artist and the attribution has not been updated since 1952 (http://arcade.nyarc.org/record=b1107496~S7).  Connoisseurs, i.e. art historians that ascribe artworks to artists, require plenty of documentation for supporting their statements. However, collecting sources is time-consuming and not all of those are relevant. While photo archives often provide details on discarded attributions, museum and gallery catalogues generally do not, and the motivation supporting the accepted attribution is not always evident to final users, who have to compare several sources in order to assess the most authoritative attribution. Furthermore, publishing curated information is expensive for cultural institutions. Lastly, cataloguing records may be affected by information quality issues, such as being incomplete (e.g. Berenson library), not up-to-date (e.g. Frick Art Reference Library), or incorrect.  In this paper we argue that we can rely on quantitative methods and Semantic Web technologies to support users and cataloguers’ tasks, such as (1) to retrieve relevant sources of attributions, (2) to compare sources on the basis of Information Quality metrics, and (3) to support users’ decision-making process by leveraging a documentary, evidence-based approach. The result is  mAuth, a framework for harvesting art historical Linked Open Data and return the ranked list of authorship attributions related to artworks. mAuth supports scholars’ inquiries such as “ what is the most documented artwork attribution?”. The proposed methodology and tool make authorships’ argumentations first-class citizen in catalogues and facilitate metadata quality management.    Related work Aggregators of art historical data, such as Europeana (https://www.europeana.eu) and Pharos (http://pharosartresearch.org), support users in gathering resources. However, available data models (Doerr et al. 2010; Doerr 2003) do not take into account questionable and potentially contradictory information. In addition, aggregators do not support the assessment of reliability of sources. Despite several Information Quality measures exist (Knight and Burn 2005; Naumann and Rolker 2000) and can be applied to Linked Open Data as well (Zaveri et al. 2016), no bespoke studies on how to measure authoritativeness in the Arts field exist yet. So far, methods modelling and reasoning on arguments (Walton 2013) haven’t been considered as part of cataloguing processes. In fact, existing metadata quality assessment approaches mainly focus on functional aspects of cataloguing data (Park 2009), and no effective solutions for supporting the decision-making process in assessing reliability of statements are available.   Ontologies for the Arts domain Existing vocabularies (Doerr et al. 2010; Doerr 2003; Peroni and Shotton 2018; Daquino et al. 2017) and thesauri (Baca and Gill 2015) for the Cultural heritage naturally cover a number of aspects that are peculiar of the Arts. Since most of the statements in the Arts domain are questionable, recording provenance of information is fundamental. The PROV Ontology (PROV-O) (Moreau and Groth 2013) expresses provenance in terms of agents (who produced the object) and derivation (e.g. reuse with modification). Nonetheless, supporting users in assessing reliability of questionable information in the Arts field is an open problem. Aspects such as adopted criteria, the date of the attribution, authority of primary sources and information providers may affect the validity of a statement.  To fill this gap, the HiCO Ontology (Daquino and Tomasi 2015) (prefix hico) was proposed as an extension of PROV-O to represent the aspects required to assess reliability of a statement. In the following picture is illustrated an overview of the HiCO ontology. The main class is the hico:InterpretationAct, which is linked to pieces of information required to validate a statement, e.g. the creation of an artwork performed by a certain artist.    Overview of the HiCO ontology   We developed an ontology-based ranking model and a proof-of-concept web application that leverages the HiCO Ontology for recommending the most authoritative authorship attributions. So doing, we aim at evaluating its expressivity in a real scenario.   The ranking model of authorship attributions We distinguish two types of authoritativeness.  Textual authority (Farahat et al. 2007) refers to the extent to which information is useful, good, current, and accurate. Such features can be quantified and measured by means of a number of Information Quality (IQ) metrics.  Cognitive authority (Rieh 2002; Wilson 1983) refers to the extent to which a source is deemed trustworthy. The latter aspect is strongly domain-dependent and can be addressed by using third-party opinions or citation indexes. In this study we rank attributions on the basis of textual authority, and we present citation indexes of scholars cited as primary sources to support users’ evaluation of their credibility.  We performed a comparative study on three representative datasets, i.e. the dataset of the Federico Zeri photo archive ( http://data.fondazionezeri.unibo.it), the Berenson Library collection called “homeless”, and the Frick Art Reference Library collection of Italian anonymous paintings. The aim is to identify and validate dimensions characterising art historical data providers’ hermeneutic approach and obtain a ranking model. The argumentation around attributions is a peculiarity of historical photo archives, that usually record motivations, documentation and annotations. When photo archives document art historians’ work they provide details on discarded attributions. Museum and gallery catalogues generally do not. Therefore, photo archives are the focus of our case study. In detail, our approach includes the following steps.   Review of cataloguing standards for collecting requirements (Baca and Harpring 2009; Moro et al. 2017). Extraction of a controlled vocabulary of criteria supporting attributions from three datasets. Rating of criteria based on experts’ consultancy. Validation of the rating by means of a comparative data analysis over three datasets. Selection of IQ measures (and related metrics) taken from (Naumann and Rolker 2000) that may affect the rating of criteria, namely: reputation (list of trusted providers), completeness (data parsing), timeliness (data parsing), relevance (number of sources in agreement).  Development of bespoke metrics that represent scholars’ impact in the community (the  artist-related index) and with regard to a certain artist (the  acceptance rating). Such indexes do not affect the final ranking model, instead they are provided as informative source for the end-user.     Overview of mAuth framework mAuth ( ) is a framework based on a semantic crawler that harvests authorship attributions in the Web of Data. The following picture shows an architecture overview.     Overview of the mAuth framework   Since the web application is a proof-of-concept, harvested data sources currently include only the three aforementioned datasets, VIAF ( ), Dbpedia ( ), and Wikidata (  https://www.wikidata.org ). Data fetched are stored in a triplestore and statistics are produced to include citation indexes.  The web application allows users to input the URL of an online record from the Zeri photo archive and retrieve the ranked list of attributions. Results include (i) motivations, (ii) dates of attributions, (iii) number of sources in agreement with the same artist, and, eventually, (iv) bibliographic references, (v) names of cited scholars, and (vi) citation indexes. The ranking model and the HiCO ontology were evaluated by means of a user study ( ). Twenty domain experts performed searches by means of mAuth and similar services. We recorded their user satisfaction by using a Likert scale. In detail, they were asked to agree/disagree with the sorting of attributions, and to state whether the highest ranked attribution was deemed correct in three scenarios. Results show that the tool was able to emulate domain experts’ behaviour two out of three times, ranking the most authoritative attribution on top of the list. In one case users were not able to judge the proposed ranking, namely when only contradictory attributions were provided and these were supported by scholars’ opinions only. The scenario shows that reliable citation indexes would be required when other evidences are not available to support the decision.    Conclusion and future work In this paper we presented an ontology for describing questionable information, a ranking model for addressing textual authority of art historical data, and web solutions for supporting historians and cataloguers’ decision-making process. We address features of authoritativeness that can be measured by means of an evidence-based approach, hence our approach is potentially applicable to similar information (e.g. provenance of artworks) and fields (e.g. philologists’ critical apparatus in scholarly editions). However, sources that can be deemed authoritative (e.g. museums) but do not provide motivations are penalised in the final ranking. Future works include the harvesting of such Linked Data providers, e.g. members of the PHAROS consortium, and the analysis of citation networks in the Arts and Humanities field to address other ways to assess cognitive authority.  ",
        "article_title": " MAuth - Mining Authoritativeness In Art History  ",
        "authors": [
            {
                "given": "Marilena",
                "family": "Daquino",
                "affiliation": [
                    {
                        "original_name": "University of Bologna, Italy",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            },
            {
                "given": "Enrico",
                "family": "Daga",
                "affiliation": [
                    {
                        "original_name": "Knowledge Media Institute, Open University, UK",
                        "normalized_name": "Universidade Aberta",
                        "country": "Portugal",
                        "identifiers": {
                            "ror": "https://ror.org/02rv3w387",
                            "GRID": "grid.26693.38"
                        }
                    }
                ]
            },
            {
                "given": "Francesca",
                "family": "Tomasi",
                "affiliation": [
                    {
                        "original_name": "University of Bologna, Italy",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "libraries",
            "authorship attribution / authority",
            "museums",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "semantic web and linked data"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Scholars of the Harlem Renaissance have long struggled with the generic status of Jean Toomer’s  Cane, particularly as a result of the hybrid forms found in the forty-nine-page closing story, “Kabnis.” In order to narrow the possible field of inquiry, critics have opted to read  Cane as an experiment in autobiography, one that either avows or disavows African American identity (Gunther Kodat, 2000: 1). What strikes us about such criticism is the way these readings of “Kabnis” inadvertently (or, perhaps, advertently) insist on indexing the text along binary oppositions of black versus white despite “Kabnis” itself resisting such easy racial classification: not only is its eponymous character described in mixed racial epithets (metaphors, for instance, like “lemon face” [ Cane, 2011: 111]), but his speech, while occasionally dialectal, toes the line between an African-American Vernacular English (AAVE) register and the register of a standardized English. With the extent of Kabnis’s racialization therefore dependent on how a reader interprets his features and his voice, literature scholars need to establish new methods through which to analyze this text’s phonemical registers.    Methodology  Computational analysis provides us with one method to measure the reception of race in Toomer’s work. In particular, literature scholars can turn to the developing field of sound studies, using visualization tools such as SonicVisualizer (Cannam et. al, 2010) alongside quantitative software like Gentle and Drift (Ochshorn and Hawkins, 2019; Ochshorn, 2019) to ascertain how readers embody or avoid “doing” what scholars have variously called “black voice” in performances of “Kabnis” (Holmes, 2004). SonicVisualiser was developed at the University of London and provides musicologists with a flexible apparatus in which to annotate, track, and edit audio recordings. Gentle, a forced aligner, and Drift, a pitch trace exploration tool, are under development by Robert Ochshorn with support from Marit MacArthur’s ACLS Digital Innovation Fellowship; taken together, they cultivate nonsemantic data that, when run through a Matlab script, can calculate prosodic measures that reflect the salient features of performative speech (MacArthur et. al). Though neither tool can directly map dialect, what they can do is analyze the nonsemantic aspects of speech that AAVE adopts by virtue of its phonology—aspects of speech such as irregular metrical patterns, higher pitch ranges, and a greater overall tonal expressivity (Rickford, 1999:5). Applying them to recordings of “Kabnis” then presents critics with the opportunity to witness first hand not only  what a reader might modify when approaching Toomer’s language, but also  how they might modify it.    To demonstrate the efficacy of sonic analysis to literary studies, we use as our case study the 2013 audiobook of  Cane produced by Dreamscape Media and performed by Sean Crisden. Crisden, a self-identified African American, seems, on first listen, to embody a distinctly racial character. Yet as we progressed through his performance, it became clear that Crisden ultimately avoids the phonemical features distinct to AAVE, resulting in a portrayal of Kabnis that eludes clear raciality. It is difficult to describe the terms of this avoidance without qualitative measures. What prosodic features indicate the affectation of a racially charged voice? What prosodic features indicate the avoidance of a racially charged voice? How can variations in pitch and rhythm be used to advance arguments about identity performance? These ambiguous and deeply difficult questions can begin to be parsed through the literary application of sound studies software.    Results and discussion  To establish a baseline for our analysis, it is important to register subjective responses to Crisden’s performance. When we listened to the Dreamscape audiobook, we recognized that Crisden adopts two distinct registers for its characterization: the first is what might be identified as a “Northern” voice while the second is a “Southern” voice. These two voices are distinct on the page, with Toomer representing the former in a standardized English and the latter in an ambiguous vernacular. Yet Sean Crisden does not fully abide by this textual difference, opting, in his performance, to enunciate “Southern” Kabnis’s words in a standardized English register, even when certain letters are elided. The distinction he makes between the “Northern” and “Southern” voice therefore occurs instead at the level of the nonsemantic. For instance, in our subjective response to the audiobook, we noted Crisden’s “Southern” voice as more expressive than his “Northern” voice; it seems to have a wider pitch range, very few long pauses, and a conversational cadence which sees him playing more with assonance and internal rhyme (e.g. “mold thats branded on m soul” [ Cane, 2011: 151]). Likewise, his “Southern” voice exhibits “upspeak” tendencies, which is a trait many dialectologists associate with vernacular languages (Rickford, 1999: 5).    Qualifying these responses computationally, however, makes clear that such subjective claims are only partially true. As shown in Figure 1, although “Southern” Kabnis has a greater intonation range than “Northern” Kabnis, his expressivity metrics are quite low. While his average pause length of 0.30 seconds might indicate a conversational approach to diction, his words per minute and rhythmic complexity suggest an attention to formality and enunciation at odds with the varied metrical patterns inherent in AAVE, AAVE being characterized by phoneme cluster reduction: certain sounds, most particularly consonants, are dropped from the ends or beginnings of words, resulting in a vocalization in which sentence components are elided (e.g. “telling y about” would be pronounced  telling y’about). If Crisden abided by this phoneme cluster reduction rule, then his performance would have a higher rhythmic complexity factor overall, as these elisions would result in an unpredictable cadence. Likewise, his WPM would also be higher, since phoneme cluster reduction necessitates quicker speech. While we might therefore subjectively interpret Crisden’s performance of “Southern” Kabnis as demonstrating the phonological features of AAVE, computational analysis reveals that the opposite is in fact true: Crisden, at the nonsemantic level, avoids the phonology of black English by regulating his WPM and by enunciating his words despite their being written without certain end sounds.   We can further locate this de-vernacularization at the metrical level by turning to visualization software like SonicVisualizer. Figure 2 demonstrates a wave-form annotation of stress patterns found in brief excerpts of speech from “Northern” Kabnis and “Southern” Kabnis respectively, whereby orange lines indicate unstressed syllables, and red lines indicate stressed syllables. In both cases, Crisden alters the metric pattern inherent in Toomer’s writing to iambicize it, thereby emphasizing syllabic regularity despite the text’s very irregular rhythms. A scansion of the former excerpt as it exists in  Cane would, for instance, view “bull-neck” as a spondaic foot and “and a” as pyrrhic; Crisden, however, alters this rhythm by reading “neck” as an unstressed syllable and “and” as a stressed one, giving way to a cadence that is ordered and controlled. He continues this same metric regulation in the second excerpt by placing a stress on “on,” which leads, in turn, to a line that is forcefully iambicized despite its initial two trochaic substitutions. As with his previous modulation of WPM and articulatory patterns, the consequence of Crisden’s metrical regulation is a sort of phonological standardization, a taming which forces “Southern” Kabnis to speak in no distinctly vernacular fashion. The result is a negotiation of Kabnis’s identity that de-vernacularizes the racial potential encoded into his speech. And what are the implications of such a choice?    Prosodic analyses of three excerpts of \"Kabnis\" through Gentle and Drift: one passage which is distinctly “Northern,” one that is distinctly “Southern,” and one in a neutral Narrator’s voice. Miller’s script calculates twelve prosodic measures, though only six are salient for our purposes; these six measures are listed in the top row. All numbers are rounded to the nearest thousandth for concision’s sake.     Excerpt of wave-form visualizations from Kabnis's speeches, the former from act 1, the latter from act 5. This excerpt has been annotated through SonicVisualizer to demonstrate Crisden’s stress patterns.   Conclusion  Though it is impossible, of course, to claim with any authority Crisden’s intentions, the effect of his de-vernacularization reminds scholars of the value judgements that are placed on languages and on the people who speak them. For “there really is little if anything that […] distinguishes racial prejudice and linguistic prejudice,” writes sociolinguist Sonja L. Lanehart (2001: 2): “The people are not separate from the language.” Exporting the sentiment to “Kabnis,” how individuals choose to interpret Toomer’s eponymous character—and how we, as scholars, choose to interpret portrayals  of this character—therefore depends on how they view the construct of race as it operates not only within the early 20 th-century, but also during the period of interpretation. Perhaps, by rendering “Kabnis” without certain vernacular traits, Crisden is demonstrating a sort of racial prejudice that manifests, covertly, in his linguistic prejudice. Or perhaps, instead, his de-vernacularization is an attempt at equalization—an attempt, in other words, to reduce stigma by demonstrating that the people society often views as deviant are more similar than might once have been thought. That in our subjective analyses, we saw in the “Northern” and “Southern” voice a distinct, dialectal difference further bespeaks a culturally codified perception of racial speech: that African-American Vernacular is socially thought to “sound” a certain way, compared with “standard” Englishes, these “sounds” made clear through Marit et. al’s prosodic measures. Whatever angle a literary critic chooses to take in their argument, computational analysis provides then with both the necessary quantitative data to analyze text-in-performance, and a means of reflexively considering their own stance on linguistic difference  ",
        "article_title": "\"The Mold Thats Branded On M Soul\": A Computational Approach to Racialized Voice in Jean Toomer's \"Kabnis\"",
        "authors": [
            {
                "given": "Jonathan",
                "family": "Dick",
                "affiliation": [
                    {
                        "original_name": "University of Toronto, Canada",
                        "normalized_name": "University of Toronto",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03dbr7087",
                            "GRID": "grid.17063.33"
                        }
                    }
                ]
            },
            {
                "given": "Adam",
                "family": "Hammond",
                "affiliation": [
                    {
                        "original_name": "University of Toronto, Canada",
                        "normalized_name": "University of Toronto",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03dbr7087",
                            "GRID": "grid.17063.33"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "multilingual / multicultural approaches",
            "multimedia",
            "linguistics",
            "English",
            "audio",
            "english studies",
            "diversity",
            "video",
            "speech processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper will discuss a two-and-a-half-day workshop on IIIF, the ‘International Image Interoperability Framework’ that the authors designed and tutored as part of a summer school on analyzing and processing images in Digital Humanities. Across GLAM-institutions (Galleries, Libraries, Archives, and Museums) and image-based Digital Humanities projects, IIIF’s set of shared API (Application Programming Interface) specifications is quickly becoming a standard for enabling interoperability functionality in digital image repositories around the world. Hosting images on a IIIF-compliant image server makes it much easier to parse and share digital image data, migrate them across technology systems, and provide enhanced access to them for scholars, researchers, and general users. Essentially, IIIF allows for the possibility of opening up image collections for extensive sharing and reuse, and all without losing  web-traffic on the host institution’s web servers.  Although the benefits of using IIIF for research and reuse purposes should not be underestimated, the concept behind IIIF is abstract and its implementation can be technically daunting. As such, the complexity of the technology may lead to some hesitancy on the part of representatives of cultural heritage institutions to make the change for their digital image collections. That is further complicated by the fact that IIIF seminars and workshops usually (in our anecdotal experience at least) do not have the time for anything but scratching the surface of the technology’s possibilities: such workshops try to explain the basic concept behind IIIF, show some of the API’s powerful parameters, and let participants play around with IIIF Image Viewers such as Mirador and UniversalViewer. Since we believe that convincing potential users of the real benefits of IIIF requires more of an engagement on the part of the user, we designed a hands-on workshop in order to immerse our participants in the topic: instead of just using the technology, we would take our time and teach the students how the technology works, and how they could install and configure it themselves. The aims of the tutorial were straightforward: first, we wanted to help the students set up their own IIIF compliant image servers; and then, taking it one step further, we wanted to let them collect and reuse each other’s images. This would introduce students to the technology behind IIIF and allow them to unlock its full potential in a classroom setting where they had ready access to help from the tutors. In the course of the workshop, we distributed images of random pages of a single document for the students to host and share on their image servers. In the end, we would teach them how to create a manifest that would link the document’s distributed pages together, and that they could feed into a IIIF Image Viewer to bring their images together in a nice presentation environment. For the teaching materials, we used the draft manuscript of Mary Shelley’s Frankenstein that the Bodleian had recently released on its IIIF-compatible Digital Library. Using the Frankenstein metaphor as a didactic instrument, the class was told they would be assembling their own ‘monster’ from a series of unconnected parts. To get there, however, we needed to introduce some basic related technologies to our students, particularly because the course was open to students with no prior knowledge of IIIF or command line computing. But before we could do that, we first needed the hardware for the students to put their image servers on. That hardware consisted of a gigabit router, further networking hardware, and 15 monitors, keyboards, and Raspberry Pi 3 mini computers. Thanks to the generous support of DARIAH-BE, we were able to acquire enough of these devices to provide every group of two to three students with a credit card sized mini server for the duration of the course. As inexpensive and low-power computers, Raspberry Pis are especially helpful in areas where people face significant constraints on power, network capacity, finances, etc. Using Raspberry Pis also provided us with a useful segue to introduce the students to minimal computing – a subfield of Digital Humanities that aims to rethink DH work for areas in the world where these factors are not a given, or where we want to make less of an impact on our environment. Moreover, Connecting these Raspberry Pis over a local network to turn them into a mini server farm also offered the students a practical way to learn more about and play with fundamental computing concepts. In particular, the students were exposed to issues in physical computational and networking infrastructures; operating a computer without an interface (command line); and communicating with and controlling other computers (in a very similar way to real world server solutions).  After setting up their Raspberry Pis, linking them together in an offline network (our ‘IIIFarm’), and hacking a solution for connecting their server to the internet as well, the students disconnected them from their monitors and keyboards, and used SSH (secure shell) on their own laptops to configure their image servers remotely over WiFi. Practicing some basic commands that they had learned at the summer school only the day before, they now downloaded, created and edited new files on their Raspberry Pis; wrote a web-page in HTML; installed an Apache web server to make it accessible to the rest of the class; converted their high-resolution images to Pyramid TIFFs; and installed a IIIF-compatible image server to share those images with each other as well. Moreover, they performed these tasks via their own laptops’ CLI (command line interface), and turned their Raspberry Pi monitors on as little as possible. When the students achieved these steps, we gave them a short introduction to JSON, which allowed them to read and edit IIIF manifests, and mix together each other’s images using a Mirador viewer that we had installed on a separate Raspberry Pi on the network, where they could behold the ‘little monsters’ they had created.  Throughout the course, we applied the didactic concept of experiential learning. Every step of the configuration process had to be executed individually by the participants, giving them enough opportunities to make mistakes and to learn from them. This was also true for the instructors: this was the first time we had taught this course, and we were fine-tuning it, incorporating the students’ problem solutions and fixing bugs as we were teaching it. Since we had never tested building a network with fifteen Raspberry Pis, we were as happy as the students when they succeeded just in time for the end of the workshop. In this paper, we will start with situating the workshop in the context of the summer school. After introducing the setting, concept, setup, and structure of the course in a little more detail, we will present some of the key lessons we learned organizing and teaching this course. To achieve this we will focus on some of the infrastructure, network, hardware, and software issues we encountered, and disclose how we tried to solve or circumvent those problems. In addition, we will report on the feedback we received from our students (on the course itself, as well as in relation to the rest of the summer school) which they submitted to us anonymously. We will then reflect on how a hands-on and in-depth treatment of a concept as complex and relevant as IIIF can be as rewarding for the teacher as it is for the student. In that spirit, we will end by presenting the tutorial we are currently developing on the basis of this workshop, and which will be available for reuse at the time of the conference. ",
        "article_title": "IIIFarm. Teaching Image Interoperability on a Raspberry Pi Network of IIIF-Compliant Image Servers.",
        "authors": [
            {
                "given": "Wout",
                "family": "Dillen",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Joshua",
                "family": "Schäuble",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "sustainability and preservation",
            "standards and interoperability",
            "pedagogy",
            "English",
            "library & information science",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Answering the call for this year’s edition of the annual ADHO conference in Utrecht, the digital scholarly edition seems like a perfect example of those “complex models of complex realities” that humanists “analys[e] with computational methods and communicat[e] to a broader public” (Ciotti and Pierazzo, 2018). While trying to convey the complexity of textual development, transmission, and transmutation over time, an edition’s editor often faces the challenge of making this wealth of information understandable and accessible to its diverse target audience. Acknowledging that much of this effort depends on how we (as editors) answer questions such as “what do we make accessible?,” “how?,” and “to whom?,” my colleagues and I organized a panel at DH2017 (Montréal) to explore the layered conceptions of access and accessibility as they relate to the theory and praxis of digital scholarly editing (Sichani et al. 2017).  “Access,” we argued, “in all its iterations, continues to shape the discourse of digital scholarly editing as the field grapples with new models and methods” (Sichani et al. 2017, 128). Therefore, the panel would “frame a discussion around a broader definition of the concept in relation to the field of digital textual scholarship, by critically reflecting on its meaning for digital scholarly editions and theorizing how the term relates to issues of accessibility, usability, pedagogy, collaboration, community, and diversity” (ibid.). To gauge the community’s perspectives on these matters in preparation of our panel, we released a qualitative survey on inclusive design and dissemination (Martinez et al. 2017).  Since we were still receiving rich, nuanced data from the community, and because we wanted to use the momentum of the conference to attract even more responses, we decided to leave the survey open for some time after the conference. Of course, this meant that we could not make any firm statements about the survey data at the conference; so instead we used the results we had received so far as a way to open up the discussion, and present the audience with a series of questions rather than answers. Since then, we have closed the survey and analyzed its results, and are now ready to present them to the community.    Survey Description The survey was distributed through a series of relevant mailing lists, social media portals, and via personal emails to practitioners in the field in our own networks. In total we received 219 responses, 109 of which completed every required question in the survey – resulting in a completion rate of 49.7%. Given the length of the survey (with 42 questions distributed over 14 pages, which most respondents took over 40 minutes to complete), this was a healthy completion rate. Taking into account that 65 (or almost 60% of our) respondents expressed their willingness to participate in a follow-up interview, it is clear that the issues raised in the survey are of considerable interest to the community – or, at least, to that portion of the community that we were able to reach with our survey.  The survey was structured around a series of themes relating to aspects of access and/or accessibility. After a demographic section (Q1-3) and a section designed to assess the respondent’s involvement or role in the development or publication of digital scholarly editions (Q4-6), the survey first focused on Open Access and licensing issues (Q7-11); access to the underlying code and software of the edition (Q12-18); cataloging and dissemination of digital scholarly editions (Q19-21); web accessibility (Q22-30); and inclusivity (Q31-37); before ending with a general question about digital scholarly editions, and an inquiry whether the respondent had any additional comments, or was open to the possibility of a follow-up interview (Q38-42). This paper will zoom in on the responses we received in relation to one of the themes that were broached in the survey: namely, ‘web accessibility’.   Scope As I suggested above, one of the main challenges the editor of a digital scholarly edition faces when it comes to the presentation of their research is to walk a fine line between complexity and simplicity. On the one hand, the editor will want to present the user with as much relevant information as possible (to help them in their own research, and allow them to assess the editor’s interpretation of the materials), but on the other hand the editor will want to present this information in such a way that it does not overwhelm or distract the user as they are browsing through the edition. Since this turns interface design into a key aspect of the digital scholarly edition and a necessary tool to convey the editor’s interpretation to the edition’s users (Dillen 2018), we felt the need to examine the concept of “access” through the lens of web and software development in our survey as well.  In this context of interfaces, the term accessibility has a very specific meaning, where it refers to the adoption of strategies that make the web application accessible to all users – including those with (in)visible disabilities. George H. Williams lamented the fact that although “[o]ver the last decades, scholars have developed standards for how best to create, organize, present, and preserve digital information” for future generations, “the needs of people with disabilities” have largely been neglected in this pursuit (2012, 202). And indeed, following Williams, a strong case can be made that while it may not be possible for one edition to cater to all of its potential users, editors should at least try to cater to the broadest possible interpretation of the target audience we have in mind for the edition – which will inevitably include people with disabilities.  Especially in the field of digital scholarly editing,    As opposed to, for example, digital collections hosted by GLAM-institutions (Galleries, Libraries, Archives, Museums), which are typically much more aware of (and have implemented features to minimize) web accessibility issues.   discussions regarding different user needs typically refer to those with non-academic backgrounds (Apollon et al. 2014, 93; Pierazzo 2015, 151) rather than to users with (in)visible disabilities. In addition, as two major (though mainly Western) points of reference in the field, neither Sahle’s (2016) nor Franzini’s (2016) catalogues mention accessibility in their respective lists of criteria for digital scholarly editions. This suggests that otherwise widely adopted standards such as @alt texts for links and images, consistent use of header tags, legibility of fonts, attentive use of colors and contrast, etc. are not sufficiently acknowledged or adopted in the field. In the web accessibility section of our survey, we wanted to test this hypothesis, while also gauging the community’s perspective about making web accessibility a prevailing concern for digital scholarly editions.     Objectives of the Paper After an introduction to the survey, its methodology, reach, and some of our more general survey results, this paper will zoom in on its web accessibility section. Going over this section’s results in more detail, this paper will map our respondents’ awareness of relevant accessibility guidelines, as well as their position towards implementing them; illustrate what kind of accessibility features they offer (or have seen other editions offer); delineate how web accessibility issues are tested, resolved, and incorporated in the edition’s workflow; and discuss in which cases the survey data suggests a regional divide in the answers we received to these questions. Taking some of the possible biases in the survey’s data into account, this paper will then draw its conclusions from our survey results, review their implications for the field of digital textual scholarship, and suggest a way forward. In general, however, we can already say that while the survey suggests an overwhelmingly positive attitude towards making digital scholarly editions web accessible, the community also conversely indicates some marked resistance towards its implementation. This implies that six years after Williams’ essay, there is still a marked lack of field consensus about how or why to practically implement web accessibility features. That is why this paper (and the survey on which it is based) also aims to raise awareness in an attempt to give these issues a more central place in our discussions of digital scholarly editing.  ",
        "article_title": "Web Accessibility in Digital Scholarly Editing: Considerations from a Survey on Inclusive Design and Dissemination.",
        "authors": [
            {
                "given": "Wout",
                "family": "Dillen",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "digital textualities and hypertext",
            "user experience design",
            "scholarly editing",
            "interface",
            "gamification",
            "English",
            "philology",
            "literary studies",
            "diversity"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   In 1839, in Paris, the Maison Didot bought the Bottin company. Sébastien Bottin trained as a statistician was the initiator of a high impact yearly publication, called “Almanachs\" containing the listing of residents, businesses and institutions, arranged geographically, alphabetically and by activity typologies (Fig. 1). These regular publications encountered a great success. In 1820, the Parisian Bottin Almanach contained more than 50 000 addresses and until the end of the 20th century the word “Bottin” was the colloquial term to designate a city directory in France. The publication of the “Didot-Bottin” continued at an annual rhythm, mapping the evolution of the active population of Paris and other cities in France.     Figure 1: Example of the cover page and alphabetical list for the year 1856 The relevance of automatically mining city directories for historical reconstruction has already been argued by several authors (e.g Osborne, N., Hamilton, G. and Macdonald, S. 2014 or Berenbaum, D. et al. (2016). This article reports on the extraction and analysis of the data contained in “Didot-Bottin” covering the period 1839-1922 for Paris, digitized by the Bibliotheque nationale de France. We process more than 27 500 pages to create a database of 4,2 Million entries linking addresses, person mention and activities. The quality of the document analysis process is assessed diachronically and a conservative strategy was chosen in order to populate the database with only information of high confidence. An initial analysis of the data is presented, reporting on the overall statistics of the distribution of professions in Paris and their evolution during more than 80 years, as well a general overview of the diversity of family names through time. Seven case studies corresponding to different streets are briefly compared, showing how information in city directories capture statistically the dynamics of segmentation of the city into functionality differentiated neighborhoods.     Figure 2: Number of pages per city directory year by year. 29 years are missing in the digital collection used for this study.   Method  The document analysed in this article were digitized by Bibliotheque nationale de France and made available online through the Gallica portal. The dataset corresponds to three different published series which are homogeneous in their structure and aims (Bing 1897): Annuaire général du commerce (1839-1856); Annuaire-Didot-Bottin (1857-1907); Annuaire du commerce Didot-Bottin (1914-1922). The documents were associated with an ALTO description containing a structural decomposition of each page into text blocks and lines, associated with a transcription obtained by an Optical Character Recognition (OCR) process. We designed a parsing process converting each line/entry into a record in a database documenting the name, the activity, the place and when relevant the street number (Fig. 3). Only a subset of the entries was successfully parsed (4,2M over 5,6M) and included in the database.    Figure 3: Structure of an entry in the directories A general discussion on quality assessment methods of the OCR for the BnF digital collection can be found in  ( ​Salah - Moreux - Ragot - Paquet 2015) ​ . In order to assess specifically the OCR quality of the inserted data, 14 pages were randomly picked for the years 1839, 1848, 1856, 1857, 1881, 1907, 1921 and manually controlled.      Figure 4: Character error rate, Word error rate and Line error rate for 7 years of the corpus (14 random pages analysed). On well parsed entries, the mean of the character error is of 2.6% with a standard deviation of 0.1%. The error per line is of 21% with a standard deviation of 2.9% (Fig. 4). The overall transcription quality tends to decrease as sources are more recent. This is essentially due to three factors: (1) the insertion at the beginning of the 20th century of special symbols used for saving textual space but unparsed by most OCR system, (2) the increasing thickness of the volumes leading to the curvature issue during the scanning process making line detection and word identification more difficult (3) the use of continuously thinner paper sheets leading to problems of transparency between the verso and the recto of a page (Fig. 5).     Figure 5: Examples of problems of scanning (1) symbols, (2) page straightening (3) transparency and corresponding OCR  The entries of the database for seven cases studies were realigned on the Vasserot cadastre digitized and analysed during the ALPAGE project (Noizet-Bove-Costa 2013) and available online. The Vasserot cadastre is giving a full director of addresses in Paris for the period 1810-1837. Paris Open Data covers the structural change due to the Haussmann period and the evolution of the 20th century. Using these two sources, 89,2% of the addresses were successfully realigned for the seven case studies considered.   Results  In total 4.2M person mentions were extracted for the period 1839 to 1922. This database could be the starting point of numerous studies, we are only giving here a broad illustration of the content of this dataset and discussing their potential relevance for future research. For instance, the diversity of family names, an indirect proxy of the social effects of urbanization, clearly increases during the 19th century and then stabilizes at the beginning of the 20th century (Fig. 6).     Figure 6: Number of unique family names by year during the entire period. Name diversity keeps increasing during the 19th century and then stabilizes.    Figure 7: Family names frequency on the period 1839-1922    Figure 8: Frequency of trade type by family names “Dubois” ; “Martin” ; “Petit” on the period 1839-1922 If one compares the three most frequent family names “Martin”, “Petit”, “Dubois” (Fig. 7) with the typologies of trading businesses, the sale of wine appears to be, by far, the most commonly practiced activity (Fig. 8). If you encounter a Martin, a Petit or a Dubois at the end of the 19th century, there is a certain chance, he’ll be a wine seller. This result corresponds to a general trend, at the urban scale. In the figures 9 and 10 the dominance of the wine business is 2.5 times more important than the one of grocery stores, the second activity on the list. As confirmed by other studies (Pinol-Garden, 2009), the relative proportion of the wine activity keeps increasing during the 19th century. This is in line with other figures like the wine consumption (one million hectolitre in 1800, 5 million in 1920, about 150-200 litres per person/per year) and the construction of the Bercy wine hall in 1869 (42 hectares on the Seine bank) and their extension in 1910 (Gallet 1939; Thillay-Reynald 2004). Grocery professions, tailor, hairdressers, bakers are equally represented, alongside liberal professions like doctors. Architects, cabinet makers and carpenter are also well represented, a sign of an important building activity on a city scale.    Figure 9: most common trades over the whole period    Figure 10: most common trades by decade The seven chosen case studies correspond to a specific analysis crossing historical information and urban space, focusing on very ancient streets (Rue Montmartre-Rue Saint Denis-Rue Saint Antoine-Rue Saint Jacques) organising a variety of activities over a deep urban “palimpsest” (Corboz 2001) and others, much more recent, with a strong characterization by trade businesses (Rue Richelieu-Rue, Saint Anne) or residential vocation (Boulevard Saint Michel) (Fig. 11). Fig 12 compares the situation of seven cases studies showing the diversification of activities by neighborhood. The ancient roman streets (Rue Saint-Denis, Rue Saint-Antoine, Rue Saint-Jacques) are characterized by the equally distributed presence of services for the population, reflecting a residential activity and a mix of activities acquired over time. Only the mention of the ‘bookbinders’ on the Rue Saint Jacques, highlights the presence of the university district strongly linked to book production. Rue Montmartre, mentioned since the 13th century, includes many more activities than other cases, without clear differentiators, except for the presence of the tailors. Indeed, this axis is the interface to the 'Richelieu District' considered as a place of production of fabrics and clothes. Rue Richelieu and Rue Saint Anne have a concentration of activities linked with fashion (“tailleurs”, “couturière”, “bottiers”). This denotes the original vocation not residential but specific to these trades. The distribution of activities on the Boulevard Saint-Michel constructed after the transformation from Baron Haussmann is significantly different from all the others with higher level of owners and liberal professions (doctors, architects).    Fig. 11: The seven case studies:  Rue Montmartre; Rue Saint-Denis; Rue Saint-Antoine; Rue Saint-Jacques; Rue Richelieu; Rue Saint Anne; Boulevard Saint-Michel      Fig. 12: Street trades (1839-1922)  These preliminary results on Paris hope to demonstrate the potential of city directories to conduct large-scale urban analysis at different level of granularity. The automatic extraction process designed for this article permits to envision to easily conduct similar studies on the population of many other important cities in the world. Provided that the quality of the extraction process is monitored, such kind of massive datasets will open new avenues to study the transformations of the urban structure at different geographical and temporal scales during the ongoing industrialization and other significant societal transformations of the 19th century, connecting these large datasets from the past with the ones of the present.   ",
        "article_title": "Repopulating Paris: massive extraction of 4 Million addresses from city directories between 1839 and 1922.",
        "authors": [
            {
                "given": "Isabella",
                "family": "di Lenardo",
                "affiliation": [
                    {
                        "original_name": "DHLAB-EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Raphaël",
                "family": "Barman",
                "affiliation": [
                    {
                        "original_name": "Institut National d'Histoire de l'Art",
                        "normalized_name": "National Institute for Art History",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/045cwqj90",
                            "GRID": "grid.462402.6"
                        }
                    }
                ]
            },
            {
                "given": "Albane",
                "family": "Descombes",
                "affiliation": [
                    {
                        "original_name": "Institut National d'Histoire de l'Art",
                        "normalized_name": "National Institute for Art History",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/045cwqj90",
                            "GRID": "grid.462402.6"
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "DHLAB-EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "data mining / text mining",
            "English",
            "geography and geohumanities",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "      From 1947 to 1974 Shell Oil Company sponsored a public relations program that engaged single and married women drivers. They especially targeted married women who helped plan leisurely road trips for their families, and single “gals” who wanted to see the country. Over twenty different women portrayed its figurehead, the pseudonymous Carol Lane. For most of the program’s twenty-seven years, two or more Carol Lanes split the United States and Canada geographically and lectured concurrently, while a third or more, at times, played her on television or radio. No single face represented “the one.” Each Lane had her own biography for press releases in what seems to have been an effort to individualize the new “girl” and highlight what special areas of expertise she brought to the character. At the same time traits essential to the composite biography carried over. What I aim to discover is who Carol Lane was to the women who played her, the PR department who created her, and her audiences.  Precedent I situate Lane in the company of two other better-known American “living trademarks,” Aunt Jemima and Betty Crocker, who were also both played by multiple women, sometimes simultaneously. Aunt Jemima and Crocker were the face of domestic products, and Lane an attempt to domesticate products—automobiles and gasoline—not necessarily associated with the home or women. All three had radio and television presences, well-defined skill sets or expert knowledge, and public personas that shifted over time. Caroline Iverson, who developed the Lane persona while employed on Shell’s public relations team, cited Crocker as a direct influence in correspondence with her departmental directors (Ackerman papers, 1927-2004). In crafting biographies for each of the women who portrayed Lane, I am following Marilyn Kern-Foxworth’s lead on her history of Aunt Jemima (Kern-Foxworth, 1994). In short paragraphs, she honors each woman who played Aunt Jemima, who embodied a living brand that was so racially charged yet represented employment and perhaps fleeting celebrity. I also turn to a number of authors who collocate an incomplete collective biography of Crocker including Carolyn M Goldstein, Susan Marks, and Laura Shapiro. I have yet to discover another scholarly endeavor that uses prosopography as a method to study a phenomenon like Carol Lane or Betty Crocker, let alone a digital humanities project focused on collective biography about women in the public relations field.  Methodologies My digital dissertation combines textual and visual data, all displayed, searchable, and readable on the ArcGIS-based website. The combination of prosopography (Stone, 1971), mapping/graphing, close reading, and display of primary documents is methodologically appropriate and necessary to advancing my analysis of the Carol Lane phenomenon on macro and micro scales. On the macro level I develop how Shell public relations visually, rhetorically, strategically, and physically positioned Lane in relationship to her audiences and as part of company’s larger marketing efforts. I also place these relationships in the context of the larger petroleum and public relations industries. Mapping her routes and grouping her audiences thematically and locationally also falls under the macro level. On the micro level, I assess richer, more granular details about audiences’ socioeconomic standing and their race- and class-orientations, as well the individual women who played the role of Carol Lane. While Lane received considerable press compared to similar women-fronted PR programs, the women who portrayed her, Lane’s audiences, and the people in the PR department who created her -- collectively Lane’s known associates and network -- did not. Information about them is tucked away in newspaper articles, photographs, corporate and personal papers, genealogical databases, yearbooks, obituaries, television footage, census records, and more. The remarkable amount of labor already put into scanning and otherwise facilitating the discoverability of newspapers and other resources – long before I began my research -- make prosopography, or collective biography, tenable.  I bring together disparate bits of information from a large-scale database into a composite view of Carol Lane. Prosopography can shed light on lives whose footprints may be documented in only ephemeral and fragmentary ways. On the ArcGIS platform, I present, for others to use, a subset of the total data (4000+ magazine and newspaper articles) I have collected to date, and input myself. The assembled ephemera are displayed in an online archive exhibition space, as will be textual analysis and video essays. Thus far I have created a fully articulated website with biographical sketches, print advertisements, complete copies of Carol Lane publications and films, a crowdsourcing form for sourcing information on unidentified Lanes, methodologies, video essays, how-to information, etc. I have input data for over 1500 sources that underpin the site’s visualizations and maps, and will add a selection of approximately 500 more before I submit my final project to my committee (adding more as I continue with the project). I am also in the early stages of creating the contextual and analytical narrative for each chapter on the site (see a video screen capture here:  ). In future, the site will be available at  http://carollaneproject.com.  My hybrid method takes a more feminist approach in visualizing related data in more ways than a straightforward relational database. Because I cannot anticipate who my (hopefully multiple) audiences may be, my project presents my analysis, and the raw data itself so visitors to the site may come to different conclusions than I. Of course with that I will include an explanation of the decisions behind how the data is structured, visualized, and categorized. ",
        "article_title": " Changing Lanes: A Reanimation Of Shell Oil’s Carol Lane  ",
        "authors": [
            {
                "given": "Melissa",
                "family": "Dollman",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill, United States of America",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "multimedia",
            "feminist studies",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "prosopography",
            "audio",
            "databases & dbms",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Beginning in the 1950s Americans sponsored international exchange programs for Polish scientists and professionals, believing that exposure to the West would undermine Communism. In 1989, Poles underwent a negotiated revolution. Were these extensive American public diplomacy efforts successful? Can pathways of influence and shifts in perception within specific epistemic communities be measured, mapped, and visualized longitudinally to better understand exogenous influences on Eastern Europe’s democratization process? Based on an approach to quantifying individuals’ lives based on their “institutional affiliations,” our interdisciplinary team has designed an interactive social network analysis visualization app, built in R Statistical Software using the Shiny package. The app allows users to interactively explore the overlapping networks of political revolution and international exchange, and illustrate how these connections shifted over time. This provides insights into Poland’s specific experience, as well as a model for studies of other complex, longitudinal networks. ",
        "article_title": "Mapping the Indefinable: Designing a Social Network Analysis Shiny App to Explore the Influence of East-West Exchanges on Poland’s Political Transformation",
        "authors": [
            {
                "given": "Gregory Frank",
                "family": "Domber",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Kelly",
                "family": "Bodwin",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-16",
        "keywords": [
            "digital humanities (history",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "network analysis and graphs theory",
            "prosopography",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Complexity in the Social Sciences and Humanities (SSH) can take the shape of the fragmentation of research fields, across many disciplines and subdisciplines, usually grounded in regional, national and linguistic specific communities. SSH data are fragmented themselves across different types, formats, languages, disciplines, resulting in a major impediment to their discovery and to their reuse from outside the specific and often small communities where they were produced. “Big data” does not apply to SSH, where data can often need to be very precisely qualified, described, curated and managed: they are smart and small data, which means they have to be specifically managed, all the more so in the perspective of being integrated in the European Open Science Cloud (EOSC) landscape, as a major component of the IFDS. At the same time, SSH disciplines have undergone a major change in their communication practices, driven by the development of digital technologies and the Open Science paradigm: the boundaries of the “scholarly record” are now blurring, and the research monograph -  the traditional primary form of research dissemination - is being challenged by technical innovations such as text and data mining, open annotations, data embedding, and collaborative writing. This short paper will present CO-OPERAS - Open Access in the European Research Area Through Scholarly Communication -, which is an Implementation Network (IN) within the GoFAIR initiative. CO-OPERAS IN aims to build a bridge between SSH data and the EOSC, widening the concept of “research data” to all types of digital research outputs linked to scholarly communication that are, in SSH, part of the research process, whereas the concept of “continuous communication” underpinning the SSH research lifecycle holds an immense potential as an inspiring model of Open Science with direct societal impact. CO-OPERAS IN is based upon a solid international framework, grown through strong collaboration, and years of consensus building between more than 36 partners from 13 countries, representing diverse stakeholders and service providers encompassing the entire cycle of scholarly communication in SSH.  CO-OPERAS IN aims to bring the FAIR principles into the SSH research environment, leveraging on existing scholarly communication services and platforms to connect them as components of an emerging EOSC, and more broadly to the global SSH communities. This short paper will try to explain how “integration” is a way to face fragmentation. This is the keyword of CO-OPERAS IN, while its core strategy is coordination rather than competition, nurturing existing realities. The first part of the paper will be dedicated to highlight this CO-OPERAS unique approach based on synergies between researchers, publishers, and libraries and the complex organization of the IN, with special regard to the number and varieties of partners through the presentation of its work plan and scheme of governance. The second, more techinal part will present the CO-OPERAS IN building blocks for the Discovery, Certification, Research for Society services and the tools to support the FAIRification of the research process and resources in the SSH, which is of utterly importance to bring Humanities and Social Sciences into the European Open Science Cloud. The paper wil provide some insights on identification and certification processes, metadata enrichment, interoperable standars, discovery tools based on a multilingual approach, and licensing practices. ",
        "article_title": "CO-OPERAS IN: Integration And Cooperation To Face Fragmentation And Address Complexity In The SSH",
        "authors": [
            {
                "given": "Suzanne",
                "family": "Dumouchel",
                "affiliation": [
                    {
                        "original_name": "Huma-Num (CNRS) - OPERAS, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Elena",
                "family": "Giglia",
                "affiliation": [
                    {
                        "original_name": "University of Turin",
                        "normalized_name": "University of Turin",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/048tbm396",
                            "GRID": "grid.7605.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-09",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "digital ecologies",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In an interview he gave in the year 2000, the well-known comics author Alan Moore made a remarkable observation about the graphic novel. Although he was critical of the term, which is commonly used to refer to book-length comics narratives, Moore acknowledged that canonical titles such as Art Spiegelman’s  Maus and his own  Watchmen could legitimately be described as novelistic on the basis of their higher “density” (Moore). Moore thus implicitly hypothesized that critical appreciation may have a formal basis. As we understand it, Moore’s brief reference to density—which he does not expand on in the interview—may be reformulated as complexity: the visual and textual cues that make it comparatively easy or difficult to comprehend and interpret a given narrative. Yet, this notion of complexity introduces further complexities for a scholarly understanding: On the one hand, Moore’s hypothesis accords with recent attempts in DH to find a new middle ground between the older formalisms and a cultural studies emphasis on the discursive construction of literary concepts. In practice, this reorientation necessitates a combination of computation and cultural sociology (English & Underwood). On the other hand, anyone who attempts to operationalize a concept such as density in multimodal media also faces technical challenges, in our case the automatic recognition of handwritten comics fonts.   In this paper, we describe the operationalization of Moore’s concept of density with the help of six textual and visual measures. We then present a pilot study of 40 graphic novels and memoirs, which are taken from the first representative corpus of English graphic narrative, or GNC (Dunst, Hartel & Laubrock). Six of these can be described as canonical given their frequent discussion in academic scholarship. The relatively small number of titles can be traced to the aforementioned technical hurdles. DH research on comics and visual media more generally has made significant progress in recent years (Dunst, Laubrock & Wildfeuer). Yet, existing computer vision methods still need to be adapted to the structural features of comics, such as individual panels, speech bubbles, and non-perspectival drawn images. Existing OCR software based on static and adaptive character classifiers leads to poor results in recognizing highly individualized and frequently handwritten comics fonts. This paper builds on early results of applying neural network-based automatic text recognition (ATR) to graphic narratives and may constitute the first computational analysis of comics text.    Dataset & Methodology The brevity of Moore’s reference to density does not give any indication of his precise understanding of the term. However, our previous research has shown that Shannon entropy, a measure for the visual agitation of a page, and the number of shapes are useful indicators of style in comics (Dunst & Hartel 2018). These measures also capture central elements of basic visual processing, which distinguishes variations in color and brightness and establishes discontinuities between shapes (Lefèvre). In addition, we include the number of individual panels in our formalization. Most comics pages consist of several individual images that are framed by drawn borders or white space to suggest a sequence. Therefore, the number of panels per page indicates whether a page consists of one single image, or is constructed from the complex arrangement of many. We currently achieve the most promising results recognizing comics text with the open-source Tesseract 4 software, which is based on a long short-term memory (LSTM) recurrent neural network (Smith). As described in earlier work (Hartel & Dunst), we use the similarity measure of the “Bag Error Rate” (BER) to decide whether the ATR software produces plausible results for text analysis based on a Bag-of-Words (BOW) approach. For each graphic novel, we manually annotated around 10% of its pages and compared this gold standard to the results of the ATR. Only if the BOW of the gold standard and the recognized texts are similar enough (a BER smaller than 40, for details see Hartel & Dunst), do we consider the graphic novel appropriate for text analysis. Research on the complexity of written texts often uses simple word-based measures. Standardized reading tests such as the Gunning fog index or Flesch-Kincaid count the length of individual words and sentence lengths. While our ATR does not reliably recognize punctuation at this point and is thus unable to count sentence length, we include the number of overall words on a page, word length by number of characters, and normalized type-token ratio in our calculation of textual complexity. In order to weigh all six textual and visual measures equally, we normalized each measure by computing the quotient of the value for each graphic novel and the maximum value for all graphic novels.  The designation of certain graphic novels and memoirs in the GNC as canonical is based on the frequency with which they are mentioned in the Bonn Online Bibliography of Comics Research. Figure 1 gives an overview of the 20 titles with the highest number of mentions and includes all six titles that were part of our study ( A Contract with God,  Fun Home,  Jimmy Corrigan,  Maus,  V for Vendetta,  Watchmen).    Fig. 1: 20 titles in GNC with the most mentions in Bonn Bibliography of Comics Research     Results & Discussion   Our pilot study provides empirical evidence that supports Moore’s hypothesis that critically esteemed, or canonical, graphic novels are characterized by higher density. Despite the comparatively small number of titles analyzed, figure 2 shows that the results are highly significant, with p<2*10 -16.     Fig. 2.: Distinction in density between canonical and non-canonical graphic narratives   The results introduce a number of finer distinctions that are, by necessity, absent from Moore’s brief mention of density. Figures 3 and 4 compare four genres: the umbrella category graphic fantasy, which includes science fiction, fantasy, horror, and superhero narratives; graphic memoirs; graphic novels in the narrower sense of the word as fictional, literary narratives; and graphic non-fiction. Both information channels present in graphic novels show significant differences for canonical and less celebrated titles. If we look at different genres, we see that graphic memoirs are less complex visually than other titles but show the highest score for textual density. Graphic fantasy emerges as the most visually complex genre.   Fig. 3: Genre comparison for visual density. All categories show statistically significant distinction, with p  < 0,05.     Fig. 4: Genre comparison for textual density. The pairings graphic novel–graphic fantasy, graphic novel–graphic memoir, and graphic novel–graphic non-fiction are statistically significant, with p  < 0,05.    The difference between textual and visual density contributes to our empirical knowledge about narrative. The visual density of graphic fantasy is due to higher entropy and number of shapes. Work in progress indicates that these titles also tend to be more colorful and more irregular in their layout. Titles such as Moore’s  Watchmen and  V for Vendetta are thus visually highly complex, possibly because of the emphasis on spectacle and entertainment in these genres. In contrast, the textual density of graphic memoirs might contribute to their frequent discussion in academic scholarship, with  Maus amassing 15% of all mentions in our corpus. Textual complexity arguably appeals to literary and cultural critics who have been schooled to appreciate titles that allow for ambiguity and subtle interpretations. However, many graphic memoirs (including  Maus) are published in black and white—a feature that leads to lower entropy and, in our current operationalization, to somewhat lower visual density. Finally, a combination of high visual and textual density seems to augur well for the success of graphic narratives. As figure 5 shows, four of the six canonical examples included in our study can be found among the highest scoring titles on both counts.   Fig. 5: Scores for overall density, with canonical titles marked green   Conclusion & Outlook We’ve detailed the operationalization of a concept, that of density or complexity, that was anecdotally connected to social processes of canonization by a leading comics author. Similar processes might be at work in multimodal media, including film and television. Generally speaking, higher-level concepts that combine information channels may provide useful research hypotheses for multimodal analysis. In contrast to more exploratory analyses of correlation between verbal and visual measures, these concepts can easily be connected to qualitative scholarship and sociological metadata. In a next step, we will increase the number of titles to the total of 250 included in the GNC. This will allow for a representative overview of graphic narrative. In addition to Tesseract 4, we are currently training Transkribus ATR software (Kahle et al.) on comics fonts and are working on enhanced text spotting, so that we will likely be able to present a more comprehensive version of this study in time for DH 2019.  ",
        "article_title": "Quantifying Complexity in Multimodal Media: Alan Moore and the “Density” of the Graphic Novel",
        "authors": [
            {
                "given": "Alexander",
                "family": "Dunst",
                "affiliation": [
                    {
                        "original_name": "University of Paderborn, Germany",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Rita",
                "family": "Hartel",
                "affiliation": [
                    {
                        "original_name": "University of Paderborn, Germany",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "corpus and text analysis",
            "multimedia",
            "communication and media studies",
            "English",
            "cultural analytics",
            "cultural studies",
            "audio",
            "image processing",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Latent Dirichlet Allocation (LDA) topic modeling (Blei, 2012) is a statistical method that discovers hidden themes and topics from a text corpus, and it has been widely applied in digital humanities over the past several years. My survey on applications of topic modeling have found 53 studies from the books of abstracts of the annual international conference of the Alliance of Digital Humanities Organizations between 2011 and 2018   Collection of abstracts from the last decade was initially planned. Unfortunately, due to a broken link, the abstracts of DH2009 could not be obtained, and studies related to topic modeling could not be found in the abstracts of DH2010.  . Topic modeling-related approaches are increasing (Figure 1) and most of them are related to historical studies (18 approaches) and literary studies (17 approaches). It has also been used for religious studies, digital archaeology, etc.      Figure 1. Distribution of studies over time. The Problem  The standard use of LDA topic modeling is to browse corpora through topics and data visualizations, while it is more complex than just training and visualizing topics in practice. Results of topic modeling can be influenced by several factors such as the LDA hyperparameters, topic number, chunk-length of documents, number of iterations of model-updating as well as hyperparameter optimization (Schöch, 2017). As far as I know, a common understanding on handling these factors seems yet to be established. Using text chunking as an example, Jockers (2013) divided novels into chunks in order to capture transient themes which only appear at certain points of a novel. In contrast, Nichols et al. (2018) writes: “Following common practice using LDA on texts, we did not chunk or split the texts in our corpus for analysis.” Stop words removal is another example, where more coherent topics are able to be obtained in general by removing stop words with no contents. Most approaches remove stop words before training the model, however discussions regarding the effectiveness of removing stop words after the modeling process have surfaced (Schofield et al. 2017).   In order to provide a comprehensive overview of how the majority of humanities scholars understands and uses topic modeling, a survey on above mentioned 53 approaches in detail has been done. In this paper I therefore propose to look at these approaches in the following aspects:  Preprocessing: text processing procedures before topic modeling. What kinds of preprocessing is used and why? Modeling: what are the parameters, which control a topic modeling process. How can they influence the results and how are they been used in different approaches?  Postprocessing: What method has been used for the interpretation of topics? How were the quality of a topic model and the topics evaluated?     Preprocessing:  The common preprocessing procedures include lemmatization, part-of-speech (POS) tagging and document chunking. By transforming words to their base form, the topic model can become more concentrated on the semantic structure. Through POS tagging, words with less contents could be identified and removed from corpus, in order to get more coherent topics. Chunking allows us to capture topics which only appear at certain points. My survey pays particular attention to the reasons of applying (or not) a preprocessing procedure in practice. For example, lemmatization is often applied when the corpora are in highly inflected languages like German or French. Document chunking is very diverse: the chunk-size could be several hundred or several thousand words, or a page of a book, or to split a book into ten equal segments. But almost no approach explained the reason of their chunking choices.   Modeling:  The LDA hyperparameters, the number of topics, the number of iterations of model-updating, the hyperparameter optimization control the modeling process. As a matter of fact, no approaches have been reported on setting the hyperparameters, while two approaches reported their number of iterations (Schöch, 2015; Maryl & Eder, 2017) and two approaches applied the hyperparameter optimization (Goldstone, 2014; Falk, 2016). Other approaches are more focused on the visualization and analysis on topic models. In 23 approaches, the choice of topic number has been reported, but only 8 of them explained how the numbers were determined.    Postprocessing:  After the modeling process, it is important to evaluate the topic model and the topics. Only 5 approaches reported their evaluation methods. Although the topic quality can be evaluated by measuring topic coherence (Röder et al., 2015) and topics can also be automatically labeled (Magatti et al., 2009). Only one approach used topic coherence for the evaluation of the trained topics (Rhody, 2014). It is more common to label topics manually and to highlight the correlation between interesting topics and metadata. More than half of all approaches applied data visualization for exploration purposes.  Conclusion This survey intends to provide an overview regarding the common use of topic modeling in digital humanities. It presents the situation that DH-community not always report how topic modeling was applied: Within 53 approaches, around 74% didn't report how their corpora were prepared; more than 70% didn't report which tool was used to train their topic models; almost 57% didn't report how many topics were trained and about 90.5% didn't report how their topic model were evaluated. Without reporting the technical details, the scientific reproducibility and the stability of their research could be questionable.   In addition, the lack of interest (or knowledge) on the complexity of topic modeling itself may indicate non-optimal application of this method. The standard parametrization of the topic modeling tool could be used, but because texts in humanities (for example literary texts) are more complex, it is not always clear how much previous understanding of topic modeling from computer science can be beneficial. One good example of the variation is the subgenre classification using topic modeling in Schöch, 2017. The accuracy difference between the worst and the best models is 17%. To obtain a better understanding of topic modeling, series of systematic investigations into the impact of factors on topic modeling will be my next steps to proceed.   ",
        "article_title": "A Survey On LDA Topic Modeling In Digital Humanities",
        "authors": [
            {
                "given": "Keli",
                "family": "Du",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "content analysis",
            "data mining / text mining",
            "English",
            "theory and methodology)"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The proposed paper aims to present the development of  Oralia diacrónica del español   Funded by MINECO/AEI/FEDER, UE (reference: FFI2017-83400-P).   (ODE corpus, University of Granada), a new digital resource for the study of historical dialectology, thanks to TEITOK, “a web-based framework for corpus creation, annotation, and distribution, that combines textual and linguistic annotation within a single TEI based XML document” (Janssen, 2016).   The digital resource consists of a diachronic corpus of Spanish texts from the south of Spain (mainly, from the old Kingdom of Granada, comprising of the current provinces of Granada, Málaga and Almería) written between 1492 and 1833. These texts, characterised by communicative immediacy or conceptional orality (Koch and Oesterreicher, 2007), include inventories of goods, witnesses’ testimonies in criminal trials and surgeons’ reports on the state of an injured or dead person, where doctors and surgeons use both colloquialisms and learned words. Furthermore, there are texts from different archives in the south of Spain, which makes the corpus an excellent source for historical dialectology studies.  The corpus follows the successful model of the ERC-funded project  Post Scriptum: A Digital Archive of Ordinary Writing, based on TEITOK. This model allows the combination of two methodological approaches, which represent two subsequent stages in the creation of the corpus (Vaamonde, 2015; Carvalheiro, 2016: 177-78):   1) A philological approach that involves the digital edition of the manuscripts (selection of documents and metadata, transcription based on TEI-XML). The texts have been encoded following the  TEI P5 Guidelines. Furthermore, as proposed by CHARTA, the texts in the corpus can be visualised in three different formats: images of the manuscripts, diplomatic transcriptions and critical editions. Each text is presented with metadata, such as date, place and text type.  2) A corpus linguistics approach, in which texts are tokenized, normalized and annotated by PoS (Janssen, 2014), based on the international standard for European languages EAGLES (Expert Advisory Group on Language Engineering Standards), although the tagset has been adapted. NeoTag, a PoS tagger (Janssen, 2012), has been trained with another corpus of early modern Spanish: Post Scriptum. When a considerable amount of data has been annotated and manually corrected in ODE, this will be used as the training corpus to automatically annotate new texts, improving this way the accuracy of the PoS tagger. Thanks to the user-friendly interface offered by TEITOK, it is possible to revise and modify the following information online: TEI tags, metadata, lemmas and PoS. Encoded information can be retrieved and visualised in different ways, such as KWIC, indexes and maps (visualised on Open Street Map). It is possible to search and browse by different filters (including text typology, place, date, archive), which can be applied simultaneously, and combined with other filters like lemma and PoS. A friendly-interface query builder allows the exploration of the corpus by a general audience with no background in computational linguistics (Janssen et al., 2017). Finally, we would like to emphasize that the new online corpus has successfully overcome the following difficulties: a) It combines digital textual scholarship (TEI) and corpus linguistics (based on the EAGLES international standard for morphosyntactic annotation and lemmatisation). b) It allows working in a single edition that can be visualised in different formats by the end user in the digital resource. c) Furthermore, it permits independent management, since scholars can upload and edit their work, having control over their own research without the need for an external person in charge of the digital resource. ",
        "article_title": " An Online Corpus For The Study Of Historical Dialectology Oralia diacrónica del español (ODE) ",
        "authors": [
            {
                "given": "Miguel",
                "family": "Calderón Campos",
                "affiliation": [
                    {
                        "original_name": "University of Granada, Spain",
                        "normalized_name": "University of Granada",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/04njjy449",
                            "GRID": "grid.4489.1"
                        }
                    }
                ]
            },
            {
                "given": "Rocío",
                "family": "Díaz Bravo",
                "affiliation": [
                    {
                        "original_name": "University of Granada, Spain",
                        "normalized_name": "University of Granada",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/04njjy449",
                            "GRID": "grid.4489.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "corpus and text analysis",
            "linguistics",
            "English",
            "spanish and spanish american studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The idea that Digital Humanities practitioners might provide a translational capacity within and between the arts, humanities, information and computer science, easing collaboration between these disciplines and enhancing shared results, is not a new one: in fact, there is a long tradition of conceptualising at least some digital humanists as “intermediaries,” (Edmond 2005) “translators” (Siemens et. al., 2011) or “hybrid people” (Liu et al 2007, Lutz et al 2008 cited in Siemens et. al., 2011). As the long-predicted mainstreaming of digital humanities and digital methods into arts and humanities research advances, we might expect this transformation of the digital humanities from a disruptive to a supportive force to continue. Furthermore, while some within the academy certainly view the potential industrial relevance of the digital humanities with suspicion (Allington et. al., 2016), there are also many voices from industry itself calling for the development of a more humanistic, critical dimension in the work of the ICT industry (Hern, 2018; Madsbjerg, 2017; Hartley, 2017; Copenhagen Letter, 2017; Centre for Humane Technology, 2018). While it may therefore seem timely to explore, as Liu (2012, 2016) has called for, how the digital humanities might deliver a linchpin set of critical competencies for and reflections on the techno-social interface, how this cultural intervention into technology development might resonate with of the core tenets of DH remains unclear. This paper will introduce such a frame of reference by exploring the implications for digital humanities to be found in a corpus of 38 linked interviews about big data research. The project that developed this material, an EU-funded collaboration known as Knowledge Complexity, or KPLEX for short (  www.kplex-project.eu ), explored in depth the perspectives of and attitudes toward big data found among computer scientists, collections holding institutions, and an interdisciplinary research community reaching from philosophy to fMRI studies (emotion research). The project originally focussed on understanding unconscious bias in such research, but they also expose the depth of the misalignment between approaches to how knowledge is generated and validated across contributing disciplines.   The data the project produced therefore offers much food for thought to those of us who identify as digital humanists, as it points toward a number of key barriers commonly faced and ideally negotiated within our hybrid research space. When viewed from the perspective of the KPLEX project’s data, six distinct points of ‘aporia’ arise, places where the interviewees explicitly or tacitly exposed gulfs in epistemic culture that are clearly at the heart of the tensions between disciplines as they seek to collaborate. These gulfs in goals and understanding echo the work of digital humanists, but also expand upon and throw into relief the underlying tensions in their research. While none of these findings presents, strictly speaking, an insoluble problem, the KPLEX interviews clearly illustrate the embeddedness of these challenges in the foundations of the contributing disciplines. This entanglement with professional identities and values raises them above the level of mere barriers, to a status where a more fundamental reconsideration of the scholarship produced within such collaborations may be required. In these fundamentals we may find future avenues for DH to grow in its own right, but also to expand and reconsider its potential impact. This paper will focus its exposition on the nature of and evidence for these gaps given in the interviews, which can be briefly described as follows:  Language matters. In particular the interviews with computer scientists showed a resistance to discussing what certain key terms might mean or imply, a lack of precision that would draw criticism in a purely humanities context. This impulse weakens the potential for self-reflection in computer science but also greatly impedes successful interdisciplinary work, which may progress for extended periods on a falsely constructed sense of common understanding. While this obscurity had already been observed by Borgman (2015), the KPLEX project results provide not only empirical evidence of the phenomenon, but also of its eventual negative consequences.    Context matters. Datafication implies decontextualization, and this data/context-tradeoff is only rarely reflected in data-driven methodologies (for a notable exception see Nelson, 2017). But in humanistic enterprises context is indispensable: for an historian, for example, provenance is an all-important facet in the understanding of any source. But that which is a potentially harmful data ‘modification’ for one community is a neutral, or in fact positive, process of data ‘cleansing’ for another.    Tools and standards are  pharmaka , giving much but taking as well. In particular, information scientists can see how the availability of certain dominant tools (like keyword searches and metadata standards) are both liberating and limiting in equal measure. Data and metadata standards can be perceived by humanists as handcuffs, limiting possible iterative adaptation of parameters, but the resulting variability and complexity stand in opposition to interoperability, aggregation, and scaling (Saklofske et al., 2015).   Data without theory is as problematic as theories without evidence. A popular notion has been proposed that big data may have delivered us to the ‘end of theory’ (Anderson, 2008), but researchers actively working at the edges of big data can see clearly that this is not the case. That said, the lack of a critical frame merely pushes much of the transparency around complex phenomena into a black box with an authority based on a potentially flawed algorithm.    The power structures of technology inhibit accommodation of analogue or hybrid narratives. Much of the humanistic source landscape is still measured in kilometres of shelving rather than terabytes of data. Because of this, digital humanities practices must be well-adapted to resisting the Matthew Effect (Merton, 1968), by which research becomes concentrated on the limited, potentially flawed data - this is not always the case outside of the humanities, however. Moreover, the struggle between ‘archival thinking’ and ‘computational thinking’ evidenced in the interviews and the conceit of routinisation raises questions of who will control cultural heritage knowledge in the future.   Humanistic competences are not taught in conjunction with digital approaches. Critical, speculative, and hermeneutic thinking - the hallmarks of the humanities - are not taught alongside empirical methodologies, and critical approaches are not systematically implemented in computational studies -- Jonathon Morgan’s analysis of the Alt-Right Movement on Twitter (2016), and the Digital Humanities’ Now ‘Editor’s Choice’ Project ‘Torn Apart / Separados’ (2018) are two rare and enlightening exceptions.   The paper will conclude with a series of reflections on how digital humanities researchers could move within their disciplines and beyond to become uniquely able to negotiate some of these critical conversations. It will also address crucial points DH can share with all interdisciplinary collaboration, such as shared data formats and structuring approaches, how misconceptions are surfaced and resolved, the place of self-reflection and methodological discussions, and the incommensurability of research questions and methodologies. In conclusion, it will offer recommendations for how each of the six aporias might be met and used to create a stronger digital humanities community and culture, fulfilling its potential as both a disruptive and productive force. ",
        "article_title": "Digital Humanities, Knowledge Complexity and the Six ‘Aporias’ of Digital Research",
        "authors": [
            {
                "given": "Jennifer C",
                "family": "Edmond",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin, Ireland",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Jörg",
                "family": "Lehmann",
                "affiliation": [
                    {
                        "original_name": "University of Tübingen, Germany",
                        "normalized_name": "University of Tübingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03a1kwz48",
                            "GRID": "grid.10392.39"
                        }
                    }
                ]
            },
            {
                "given": "Mike",
                "family": "Priddy",
                "affiliation": [
                    {
                        "original_name": "DANS, KNAW, Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-11",
        "keywords": [
            "digital humanities (history",
            "digital ecologies",
            "management",
            "content analysis",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "organization",
            "project design"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. Recording the visit of these ‘forgotten’ spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites.  Urbex destinations are located worldwide and include a wide range of abandoned sites. Belgium has been a very popular destination for urban explorers and Château de Noisy, a neo-gothic castle in Belgium dating back to the 19 th century was a very famous destination which was demolished in 2017. There is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, Facebook, YouTube, and Flickr. The latter has become a significant repository of urban exploration photographs.   Regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. Considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected. This research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. The unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. Hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived.  To explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. Moreover, focusing on Château de Noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of ‘distant documentation’ by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. To reach these objectives, focusing on Flickr and using the Flickr API service, two Flickr Dataset are collected: One of general photos related to urban exploration on Flickr (from 2000 to 2017) and another which includes the specific photos of Château de Noisy on Flickr. To collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: Python Scripting Language (collection and preparation), Tableau Desktop (visualization and analysis), Voyant (textual analysis), ContextCapture (creation/reconstruction) and WebStorm (presentation via the creation of a website).  Terminology of the urbex Flickr photo titles and visualizing the distribution of the urbex Flickr photos, led to interesting insights into the urbex scene. Furthermore, the collected and downloaded images of Château de Noisy from Flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials (and pathology), structure and context over the course of many years.  Château de Noisy was demolished without being given the chance for detailed documentation through advanced  in situ techniques. Using the ContextCapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the Château de Noisy Flickr Dataset, a 3D mesh model of the building and its immediate context is created. This scalable 3D reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. A digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the ‘fairytale castle’ building that once stood in Celles. The experience of such distant documentation of Château de Noisy can also be implemented in other heritage sites which are demolished or inaccessible. For buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival.     Figure 1. Image extract from the reconstructed 3D model of Château de Noisy ",
        "article_title": "Digital Documentation of Abandoned Heritage. The Case of Château de Noisy",
        "authors": [
            {
                "given": "Negin",
                "family": "Eisazadeh",
                "affiliation": [
                    {
                        "original_name": "KU Leuven, Belgium; University of Liège, Belgium",
                        "normalized_name": "University of Liège",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/00afp2z80",
                            "GRID": "grid.4861.b"
                        }
                    }
                ]
            },
            {
                "given": "Barbara",
                "family": "Bordalejo",
                "affiliation": [
                    {
                        "original_name": "KU Leuven, Belgium",
                        "normalized_name": "KU Leuven",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/05f950310",
                            "GRID": "grid.5596.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-06",
        "keywords": [
            "corpus and text analysis",
            "methods and technologies",
            "art history and design studies",
            "sustainability and preservation",
            "English",
            "3D/4D modeling",
            "modeling",
            "cultural studies",
            "simulation",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The evolution of the digital, and its intersection with the traditional role of the humanities, has impacted academic and non-academic modes of communication, as well as research practices including collaboration, knowledge dissemination, and engagement. As the scholarly landscape evolves, so does the nature of the institutions, labs, centers, and other places and spaces of research, including those of digital humanities. Engaging with these transformations in knowledge creation, but also continuously expanding and evolving with them, is the Electronic Textual Cultures Laboratory (ETCL) at the University of Victoria, Canada. This paper is based on the premise that there is a correlation between the developing knowledge landscape and the structure of an intellectual center, especially when it is committed to ‘open’ values (open access, source, data, knowledge, and others); the development of the former necessarily affects the structure of the latter, especially over time. The ETCL operates in both physical places and virtual spaces; on campus, the ETCL serves as an intellectual research center that facilitates on-campus DH community building and off-campus engagements and networking; it simultaneously operates in physical and virtual spaces through research, skills training, and community-oriented initiatives. While existing in both modes, however, we are conscious not to perpetuate the general criticism that often identifies enclosed places of knowledge production, especially in a university setting and located on campus, as mechanisms of exclusion of the public.  While ‘digital humanities’ has existed in some form or other under various titles such as ‘humanities computing’ for some decades now, Matthew Kirschenbaum in “What Is Digital Humanities and What’s It Doing in English Departments?” identifies the 2001 debate about the title for A Companion to Digital Humanities (Schreibman, Siemens, and Unsworth 2004) as the inception of the term that amalgamated different modes of scholarly inquiry under ‘digital humanities’ (2010, 55). The ETCL, founded in 2004, falls among the earliest waves of digital humanities labs, and has evolved and expanded in multiple directions in the 14 years since its inception. Earlier this year, select current and past members of the lab came together to formally reflect upon the developments and infrastructural changes of the ETCL. The models that we considered to define the infrastructure of the lab, as well as the mission, mandate, and all ETCL-related work, tie to our values—namely our dedication to: community-driven scholarship that recognizes collaborative models of knowledge sharing; open practices in digital research, production, and dissemination; the intellectual development and well-being of our communities; shared mentorship, accountability, and support, across multiple disciplines, professions, and groups; and inclusive and ethical practices, as outlined in the DHSI Statement on Ethics and Inclusion. These values reflect the ETCL’s main direction over the last years — to understand and practice ‘open social scholarship,’ which builds on evolving modes of knowledge creation and communication, and seeks to create and disseminate research and research technologies to a broad audience of specialists and active non-specialists in ways that are accessible and significant to a broad audience. These are expressed in the three main constituents of the lab: Implementing New Knowledge Environments (INKE) Partnership, the Digital Humanities Summer Institute (DHSI), and the Canadian Social Knowledge Institute (C-SKI), that correspond to research, skills training, and community-oriented initiatives, respectively. In our paper, we consider the evolving landscape at the intersection of the digital and the humanities, specifically with reference to the lab place and space of the ETCL. Our research questions are the following: how does a DH lab that is primarily meant as a place and space for creating, enabling, and exploring open social scholarship, put into practice this goal? What type of lab infrastructure model, such as lab as incubator versus lab as branching tree structure, can best facilitate open social scholarship and support ETCL’s projects, present and future? In this paper, our engagement with DH lab infrastructure operates on two levels: 1) lab infrastructure in physical and virtual settings to correspond to and reflect the evolving knowledge landscape and 2) lab practices that engage open social scholarship in physical place and virtual space, across research, teaching and service.  The ETCL is a digital humanities research lab currently led by Dr. Ray Siemens as Director, Alyssa Arbuckle as Associate Director, Randa El Khatib as Assistant Director (Open Knowledge Initiatives), and Luis Meneses as Assistant Director (Technical Development). The lab serves as an intellectual centre for the activities of ~20 local faculty, staff, students, and visiting scholars. Through a series of highly collaborative relationships, the ETCL’s international community comprises over 300 researchers. The ETCL welcomes more than 800 students per year through their organization of the DHSI, and will be hosting its 19th annual training institute in the summer of 2019. The lab also supports the activities of the multidisciplinary INKE Partnership, which has involved over 42 researchers and consultants, 53 graduate research assistants, 4 staff members, 19 postdoctoral fellows, and 30 partners and associates. C-SKI actively engages issues related to networked open social scholarship. Representing, coordinating, and supporting the work of INKE, C-SKI activities include awareness raising, knowledge mobilization, training, public engagement, scholarly communication, and pertinent research and development on local, national, and international levels.  The two main models that are pertinent to the ETCL structure are lab as incubator and lab as tree — to reflect a structure that is at once expanding and dynamic, but that stands on solid ground to provide steady support for expansion. Lab as incubator reflects one of our central values: a nourishing environment. The incubator metaphor also reflects the idea that a lab should be a positive space for growth — in our case, one that has facilitated INKE, DHSI, and C-SKI to develop into their current forms. The second model — lab as tree — represents something living, that grows, and at once serves as foundation and support for new growth. A tree also depends on communication between all of its constituent parts, and more accurately reflects the sub-branches that grow out of, and are the fruits of, the separate branches. Additionally, the tree as a strategy to aid thinking exists in many disciplines, and is often used in computing. In terms of practicing open social scholarship in the lab place and space, we launched a number of initiatives over recent years. With respect to place, the ETCL is in its third year of hosting the Open Knowledge Practicum Program, consisting of four-month fellowships that support projects proposed by university affiliates and members of the community to be carried out in the lab, which offers some local support. Fellows contribute to Wikipedia and publish their projects in online, public venues, and are also involved in the day-to-day lab life and events on and off campus. The practicum is meant to open ETCL doors to the rest of the university and the larger local community. A global community is also involved in the “place” of the lab annually at DHSI by spending two weeks on campus for the duration of the institute. Many attendees are awarded significant tuition scholarships by DHSI in order to offset the cost of participating in a training intensive. In addition, an open social scholarship course stream seeks to facilitate further knowledge development in this area; so far, DHSI has hosted ten courses in this stream annually, including courses related to open access, public humanities, feminist DH, accessibility in digital environments, queer DH, and other pertinent topics in the contemporary DH landscape. Other forms of community building include speaker series that span practical and theoretical topics related to digital scholarship in interdisciplinary settings, such as the Nuts & Bolts and the Digital Scholarship on Tap speaker series that are open to university and community members. The ‘space’ of the lab has been engaging open social scholarship in numerous ways as well. For example, the annual Open Scholarship Awards for emerging and established scholars in any institution globally recognize contributions to open scholarship through projects or publications. This award is also meant to more appropriately recognize work related to open scholarship in a present scholarly framework where many institutions are yet to formally acknowledge this type of scholarly work. Additionally, the ETCL has authored three bibliographies that engage with the topic of social knowledge creation and open social scholarship. A virtual space for Canadian humanities and social science researchers to connect is the Canadian HSS Commons, an INKE / ETCL project inspired by the Modern Language Association Humanities Commons platform. The Canadian HSS Commons provides a platform that encourages a culture of sharing, accessing, re-purposing, and developing scholarly data, tools, and resources, thereby aiming to provide an open platform for virtual collaboration for scholars working in different institutions.  Through its structure and initiatives the ETCL engages, facilitates, and promotes cross-community digital initiatives in local and virtual contexts by cultivating the practices and values of open scholarship. ",
        "article_title": "An “Open Lab?” The Electronic Textual Cultures Lab in the Evolving Digital Humanities Landscape",
        "authors": [
            {
                "given": "Randa",
                "family": "El Khatib",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Alyssa",
                "family": "Arbuckle",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Ray",
                "family": "Siemens",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Luis",
                "family": "Meneses",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-02",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "digital ecologies",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Dynamic encounters with three-dimensional assets make virtual reality (VR) an attractive medium for innovations in teaching and research. One of VR’s most compelling advantages is an immersive experience: researchers and the public are no longer separated from digital artifacts; instead, they share a space with them. Such experiences engage the whole body. They provide a 360° event, eliminating the restrictions of computer screens. For the classroom and individual researchers, cost has previously hindered widespread adoption of VR in classrooms and by individual researchers. Fortunately, prices have fallen as technical solutions have evolved. Another obstacle, perhaps more complex and profound, is building useful academic VR systems. VR platforms are not equally capable; different systems tend to facilitate radically different actions and engagements. Tools, features, and types of engagements need identified, imagined, and built. Our presentation explores approaches to these system-design challenges while building the Oklahoma Virtual Academic Laboratory (OVAL), a scholar-oriented, human-centered VR system.   OVAL is the first generation of general-purpose, multiuser academic VR systems, and is free to download and use. Our first speaker, Matt Cook, explains the importance of collaborators across campus and institutions when designing and testing VR systems like OVAL. External to the University of Oklahoma, OVAL has hosted a range of multi-campus virtual tours, including early archaic caves in Arizona, Syrian ruins at Palmyra, and 3D-scanned (live) sea turtles from around the world. Internally, OVAL has been used across a range of academic disciplines to test the pedagogical impact of immersive visualization. Each of these unique implementations represents an iterative hardware and software design process, ultimately providing today’s students the means to quickly engage with complex 3D data in a way that preserves embodied interfacing and naturalistic visual depth-cues.  Matt discusses the motivations, hardware & software considerations, usability testing, and documented pedagogical impact of OVAL across a range of associated academic disciplines.    Importantly, specific disciplinary needs preclude a definitive version of the OVAL software and make designing a VR system an intricate puzzle. Different disciplines require different types of engagements and functionality, and some applications demand stringent preservation practices (e.g. Law School). For architecture, as part of the creative process, student needs to move through the structures they design, the experience revealing previously overlooked design flaws associated with accessibility, layout, and scale. For biology, students studying various protein molecules need to analyze VR content from the outside, turning and magnifying it, examining its features, and recording their analysis as a distributable video output. Beyond presenting the history of this open access VR system, then, Matt discusses how sorting through these divergent disciplinary considerations results in a more robust VR system with numerous academic applications. He also discusses human subjects testing and its need to refine and validate design features, as well as metrics and instrumentation useful to understand and document the value of VR in the classroom.    Our second speaker, Bill Endres, from the English department, specializes in medieval manuscripts. With funding from the OU Humanities Forum, Bill built a 2-person travelling VR workstation. He has loaded 3D models of the eighth-century St Chad Gospels into it, including multispectral and post-processed renderings for recovered content. For comparing artistic techniques, he includes metal work, Pictish stone carvings, and related pages from other illuminated manuscripts. Bill discusses how the VR workstation allows him to engage experts and the public to test features and imagine new possibilities for VR. During the presentation, Bill will quickly demonstrate the traveling VR workstation and make it available throughout the conference. Before DH 2019, Bill will have presented the workstation at the Medieval Academy’s Annual Meeting, International Congress on Medieval Studies, and at the University of Glasgow.    Medieval studies has numerous artifacts that benefits from encounters in VR, including manuscripts,  mappae mundi , swords, tapestries, cathedrals, and stone crosses. Bill discusses the simple benefit of having VR’s 360° field of visual. For example,  mappae mundi are quite large. On a computer screen, it is impossible to explore details without losing track of the whole. VR’s 360° visual field eliminates this problem.    Because such possibilities regularly hinge on the human element rather than the technical, Bill discusses the complexities of human sensory experience. Human sensory experience is anything but simple. Rather than the popular notion of the Aristotelian five senses, neuroscientists have determined that humans have twenty-two to thirty-three senses, depending on how specific each sense is defined. For example, the sense of proprioception allows someone to sense where the parts of their body are, necessary input for performing simple tasks. It is the sense evaluated in the common sobriety test: with eyes closed, touch the tip of your nose. Proprioception provides significant information when encountering physical objects, such as a sense of size, as it relates to the human body. Size offers clues about artifacts like a manuscript, signifying whether it was meant for display, study, or travel. When examining high-resolution photographs on a computer, proprioception provides information about the body in relations to mouse and screen, not to the manuscript.    But human sensory experience has a further complexity: it is constructed. For example, when sitting on an airplane, if you glimpse the walkway being withdrawn, you might experience the airplane backing away. In reality, the plane is still. The constuctedness of sensory experience allows for two possibilities. One, to translate sensory experience into VR, ratios and ratios of different sensory data can be used. For example, cold makes an object feel more rigid. This reflects learned experiences: cold generally makes an object stiffen. Therefore, to reproduce the experience of stiffness when turning a page of parchment, rather than manipulating pressure, manipulating temperature might prove more advantageous. Two, sensory experience can be generated in alternative ways. Researchers at the University of Bristol have invented a device that uses ultrasound to generate geometric shapes mid-air, which can be felt. Ultrasound shows promise for providing haptic experiences of parchment, such as feeling its contours and layered pigments. Bill discusses work with Julie Williamson at the University of Glasgow on this possibility.    Our final speaker, John Grime, provides further disciplinary range to the team. John has a PhD in physics and chemistry. His background includes developing algorithms to explore biomedical phenomenon. John develops the platform architecture for OVAL, and he discusses some of the technical underpinnings for building different multidisciplinary tools from the same base code of the program. But thinking across disciplines is likewise highly productive. For example, in OVAL we are designing a feature that allows someone to experience the sky and its effects on lighting at Neolithic sites; such control over lighting is also important for an advanced imaging technique called reflectance transformation imaging (RTI). RTI software generates a single file from a series of photographs taken with different directional lighting. The file allows control over lighting to reveal surface details. Whether for a Neolithic site or RTI, code to control lighting for one contributes to code for the other.    Furthermore, digitally grounded sensory experiences are not limited by the rules of sensory input in the physical world. John discusses one of the latest features in OVAL: “Janus,” which dynamically warps 360° of visual content to fit within the field-of-view of VR headsets. This allows users to analyze their environment or collection of artifacts as if their vision extends behind their heads.    From its earliest days, the digital humanities (humanities computing) have been collaborative. Nowhere is this more evident or important than designing VR. Sorting out how humans construct experience and how to reconstruct it in dynamic ways in VR is complex and challenging. However, as with all things complex and challenging, there are extensive rewards. VR provides unprecedented ways to engage in research and teaching by presenting content in a 360° environment that enables body-centered interactions and expanded representational characteristics for objects. It teaches us about ourselves, our artifacts, and our world. As with other research in the digital humanities, designing and building VR invites us to understand human experience more fully.  ",
        "article_title": "Sublime Complexities and Extensive Possibilities: Strategies for Building an Academic Virtual Reality System",
        "authors": [
            {
                "given": "Bill",
                "family": "Endres",
                "affiliation": [
                    {
                        "original_name": "University of Oklahoma, United States of America",
                        "normalized_name": "University of Oklahoma",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02aqsxs83",
                            "GRID": "grid.266900.b"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Cook",
                "affiliation": [
                    {
                        "original_name": "University of Oklahoma, United States of America",
                        "normalized_name": "University of Oklahoma",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02aqsxs83",
                            "GRID": "grid.266900.b"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Grime",
                "affiliation": [
                    {
                        "original_name": "University of Oklahoma, United States of America",
                        "normalized_name": "University of Oklahoma",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02aqsxs83",
                            "GRID": "grid.266900.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "software design and development",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "library & information science",
            "modeling",
            "simulation",
            "virtual and augmented reality"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Theatre ‘constitutes itself through disappearance’ (Phelan 1993) and its ephemerality poses methodological problems for researches. Before the twentieth century, most theatre scholarship focused exclusively on texts, whether they were produced before or after a performance. This is true for many theatre traditions around the world (including Javanese theatre, the case study in this paper). A textual model of theatre has serious limitations, as many of the social and improvised aspects of performance are rarely reflected in the texts. Audiovisual documents constitute better (if still imperfect) records, but they have yet to be adopted as authoritative critical editions in theatre studies. Several online platforms offer full length recordings of key theatre performances (for example digitaltheatreplus.com), but they don’t usually include the level of detailed annotation found in literary editions, where individual words or phrases are annotated to report their genesis, elucidate interpretations and trace variations across versions. Audiovisual theatre resources are often accompanied by interviews with performers or introductory notes, but there are no standard formats to annotate specific moments in a performance (the intonation of a word, the movement of a performer, the laughter of the audience) and explain their significance within larger historical and cultural contexts. A scholarly infrastructure for the critical annotation of audiovisual documents has yet to emerge, even though relevant resources and technologies exist. We suggest that a digital philology of performance can be used to imagine new formats for scholarly analysis and communication, at the intersection of theatre studies and digital humanities.  Why philology? Conventionally, philology has been associated with the study of literary material and the production of textual editions. However, the principles of philology can be used to interpret all aspects of a theatre performance: the audiovisual, social, and kinesthetic aspects of a performance can all benefit from a philological perspective. Theatre studies tends to be presentist, placing emphasis on novelty rather than tradition (Arps 2016). A philological perspective offers a principled method to study the historical layering of a performance (contemporary performance included), countering this narrow focus on the present. There are many ways in which a text-based philological edition of a performance can document the emergent, interactive and multimedia aspects of a performance, by using notational conventions to represent vocal parameters (an approach pioneered by Tedlock in 1978), tinkering with the spatial arrangement of text on a page, and using extensive notes to describe emergent and interactive aspects of a performance. However, the potential of philological editions can be more fully realized in digital editions that can combine audiovisual sources with careful philological attention.  There are calls for born-digital scholarship in performance studies (Mee 2018) in response to impressive growth of digital archives that offer full-length recordings of performances around the world (Caplan 2016). However, the authoring platforms suggested by Mee (such as blogs or Scalar) are not sufficiently malleable to accomplish the level of critical attention required by a scholarly, multimedia edition of performance. For example, it is important for scholars to link specific sections of audiovisual media to textual transcription and translations, in ways that transcend subtitles. These different media should all be amenable to meticulous cross-reference and annotation in ways that are sustainable, findable and reusable. There is no straightforward way to achieve these objectives with most available tools. How does a digital edition of a theatre performance look? What should it seek to achieve? Textual editions are standard critical objects that have benefited from a long history of continuous experimentation in both print and born-digital formats. There is an extensive corpus of influential digital editions and an extensive literature that explores how digital editions modify and continue traditions of textual editing (for example Dsicoll et al. 2016). But this level of experimentation and theoretical discussion has yet to be extended to multimedia editions in theatre studies.  To sketch a prototype for such scholarly, multimedia editions, the present authors embarked on a collaborative journey of creativity and discussion. Both authors have an interest in the Javanese tradition of wayang kulit (shadow puppet theatre). A has worked as a scholar of Javanese language and culture for more than three decades; B is an early-career digital humanities scholar and web developer. In 2016, A published a philological, annotated translation of the work of an influential wayang kulit artist, based on the recording of a performance. The first version of this translation was published in book format. A and B are currently collaborating on an interactive, multimedia version of this translation. The development of a digital portal for this purpose is not just a matter of ‘adding’ audiovisual materials but a dialogical experimentation with the format and possibilities of a digital philology of performance.   Conceptualizing multimedia editions Spatz (2015) suggests that video can document several aspects of performance, such as training (2015). Although he refers to these videos as ‘editions’, it is unclear how they constitute scholarly interventions. As Sahle (2016) notes, an edition without additional material that makes the document understandable or accessible is just a facsimile or an item in an archive. A critical attitude is required to determine what additional materials are required, and how they should be included. Their inclusion should follow rules derived from the relevant scholarly context, and these rules should be transparently and rigorously applied. An example from A’s print edition is that the symbol • indicates that the dhalang (the puppet-master in wayang kulit) knocks a mallet against a wooden box. The specific sequence of such knocks is of great significance to a performance: it might constitute a cue to the gamelan musicians or indicate that a different personage is speaking, while also contributing to the aural aesthetic of the performance. The transcription of these sounds is surrounded by explanations, and linked to detailed notes (indicated by the symbol ⓐ). For example: [T]he dhalang raps the puppet chest to signal an accelerando and sforzando in the gamelan. ⓐ At the appropriate point in the structure of the piece he raps the pattern •• • as a cue to the gamelan to play slowly and pianissimo.  In the print version, the symbols substitute for the experience of listening to the actual sounds of these rhythmical pattern. In the multimedia version, the passage above is time-linked to the recording. The user can play the recording, and the appropriate segment of the transcript will be highlighted in a different color (Fig. 1). The user can also click on any portion of the transcript to navigate trough the audiovisual recording. This description is no longer a stand-in for an absent sound, but an interpretive scholarly layer. People who are not familiar with the tradition might not be able to identify the •• • pattern just by listening to the recording. Thus, the co-presence of audio and annotation, linked through time-based playback directs the attention of the users, making the material more accessible, understandable and usable for future research.  This example shows that even the simplest inclusion of audiovisual material is never just an appendage. The audiovisual material changes the function and potential of scholarly annotation. We are at the early stages of discovering the full implications of linked transcripts, annotations and audiovisual documents. Besides producing a specific web portal for this wayang kulit performance, we are documenting our process and producing an open-source software package that can be adapted by other scholars to tackle the problems a performance philology poses for other theatrical traditions. We aim to develop tools that are usable by theatre scholars (even if they are not interested in web development) in ways that are citable, reusable and sustainable. The transcripts, translations and annotations of our edition are all TEI-complaint and we are working with both an academic publisher and a digital archive to preserve our edition and to manage its metadata records. We are also committed to making our materials available as data: this will enable the perusal of the materials online through customizable portals, as well as their eventual integration within computational, data-driven research projects. We believe that more collaborative work on this area will open new avenues for the digital transformation of theatre scholarship.       Figure 1. As the recording plays, the appropriate section of the transcript is highlighted.  ",
        "article_title": " Scholarly Multimedia Editions for Theatre Studies  ",
        "authors": [
            {
                "given": "Miguel",
                "family": "Escobar Varela",
                "affiliation": [
                    {
                        "original_name": "National University of Singapore, Singapore",
                        "normalized_name": "National University of Singapore",
                        "country": "Singapore",
                        "identifiers": {
                            "ror": "https://ror.org/01tgyzw49",
                            "GRID": "grid.4280.e"
                        }
                    }
                ]
            },
            {
                "given": "Bernard",
                "family": "Arps",
                "affiliation": [
                    {
                        "original_name": "Leiden University, The Netherlands",
                        "normalized_name": "Leiden University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/027bh9e22",
                            "GRID": "grid.5132.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-07",
        "keywords": [
            "oriental and asian studies",
            "open content and open science",
            "multimedia",
            "scholarly editing",
            "English",
            "film and performing arts studies",
            "scholarly publishing",
            "audio",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As an interactive, pluri-directional and multimodal realm, the cybersphere is characterized by the incessant production and sharing of information content, with an ever-growing number of bottom-up discourse formations and disseminations (KhosraviNik and Unger 2016). One of the most significant and complex drawbacks of this unprecedented proliferation of user-generated content, and the so-called democratization of access to symbolic recourses, is the acutely increasing incidence of online hate or cyberhate. Hostility is a complex social, cultural and psychological phenomenon: motives behind people’s hate are various, different and often obscure, and the fluid and widely unregulated nature of the cybersphere seems to have added to further complicate an already thorny matter. One of the key scholarly assumptions on the issue is that social media affordances seem to act as a force multiplier, both in terms of sheer quantity and vitriolic quality of interactions. Both social psychologists and criminologists have attempted at sketching haters’ underlying motivations and strategies by drawing on psychological theories and research. These studies have provided useful insights into how some features inherent to Computer Mediated Communication - e.g. perceived anonymity (Joinson 2003) and physical separation (Weisband and Atwater 1999) - contribute to trigger social practices online - e.g. dishinibition and de-individuation (Thurlow et al. 2009), polarization (Wallace 2016) and mob dynamics (Citron 2009). As a result, the recognition of strong psychological features in antisocial behaviours like hate speech is basically entrenched in the differences between face-to-face communication and online interactions. One of the dangers of relying on these scholarly interpretations is the relatively straightforward establishment of a cause-effect relation between the affordances of the participatory web and practices of hostility online, highlighting the role of the digital medium and downplaying socio-political structures and power hierarchies.  This paper advocates a Social Media Critical Discourse Studies (SM-CDS) approach to online hostility. As “a socially committed, problem-oriented, textually based, critical analysis of discourse (manifested in communicative content/practices)” (KhosraviNik 2017: 586), SM-CDS deals with discourse as its central object of analysis: it is not only interested in what happens in the media per se as a closed loop but also in how it may shape and influence the social and political sphere of our life worlds and vice versa. Such an approach would deliberately steer away from media determinist accounts as well as from universalist understandings of social media effect: communication is to be regarded as a human endeavour, irrespective of the sophistication of the medium used. Despite difficulties in demographic and geographic accounting of online communities, macro-contextual aspects, including materialities of sociocultural categorizations (class, ethnicity, gender, age, dis/ability, agency, cultural capitals, as well as cultural positioning, stereotypes, power structures, histories etc.) are to be carefully taken into account and not to be distilled “into a bland cybernetic metaphor” (Couldry 2012: 117).  In particular, this paper focuses on gender as a source of hate in its own right which has not received sufficient institutional and academic attention. While the dangers and risks of the digital world are well acknowledged, we still lack a clear grasp of what it actually entails being a woman navigating the cyberspace, and which specific threats and troubles this journey can bring about. In approaching online gender-based hostility, we would always start from the assumption that any online form of gendered violence replicates and extends the gender and power relations that pre-exist digital communications technologies and vice versa. Digital misogyny is to be regarded as a purposeful discursive strategy to maintain a gendered asymmetry of power by threatening, discrediting and ultimately silencing women in a way that it has historically regimented (Butler 2009). The domain of online misogyny as a digital discursive practice would be, therefore, conceptualized at the intersection of digital media scholarship, discourse theorization and critical feminist explication, with audacious interdisciplinarity (KhosraviNik and Esposito 2018) and substantial intersectionality (Lykke 2010) representing the epistemic way forward. This paper presents a number of epistemological considerations in relation to digital media, discourses of hostility and critique, grounded in the results of a multi-lingual pilot study conducted in the context of a project funded by the European Commission (H2020-MSCA-IF-2017). The study investigates phenomena of online misogyny (such as gender-based hate speech, rape threats and image-based sexual harassment), against highly visible, political and institutional female figures in Europe. More specifically, it maps the multimodal discursive strategies of online hate against women in the public sphere by collecting and analysing a corpus of user-generated comments on Social Networking Sites (namely, Youtube and Twitter) from three different linguistic landscapes and political cultures in Europe, namely Italy, Spain, and the U.K. In order to locate abundant relevant foci of data, the preliminary phase has been characterized by a digital ethnographic stance (see Androutsopoulos 2008). Criteria of inclusion encompassed: degree of digital presence, critical ‘moments’ or events of particular relevance or visibility, as well as overall number of views, likes and comments, (regarded as indices of audience attention and likely to yield a high occurrence of polarized content. The selection of case studies has been also guided by the typology of online sexual harassment by Powell and Henry (2017), to include instantiations of: a) gender-based hate speech; b) rape threats; and c) image-based harassment.  The multimodal nature of data has called for an integrated methodology, encompassing: 1) Corpus Linguistics tools (Baker and Egbert 2016), for a quantitative identification of linguistic patterns (e.g. key-keywords, collocations and semantic prosody); 2) Critical Discourse Analysis, for a close qualitative and critical analysis mapping the vast number of discursive strategies and rhetorical devices through which online misogyny is realized, in four different heuristic levels of context (Reisigl and Wodak 2001). 3) Visual Content Analysis, for a multimodal analysis of the videos containing image-based harassment by means of the four-layered framework proposed by Rodriguez and Dimitrova (2011). Emerging results in all the three linguistic and socio-political contexts in exam are showing: 1) From a more ‘micro’ linguistic perspective, the different degrees of formulaicity and creativity of the multimodal discursive strategies of online hate. This is in line with the already demonstrated algebraic and tediously predictable quality (see Jane 2017) of digital misogyny, but also highlights digital creativity as an integral part of mob dynamics and mentality online, often resulting in an ever-escalating competition to produce the most abusive content.  2) From a more ‘macro’ social perspective, the profoundly intersectional nature of online gender-based hostility. In particular, the analysis points toward the interaction of mutual and intertwined factors both triggering and stoking hate such as: class, race, gender identity or behaviour, age as well as feminist activism. These results contribute to a more in-depth understanding of gender-based hostility against women in politics as an extremely multi-faceted and multi-layered phenomenon, where gender is not the only factor at play. They also call for the further integration and development of the concept of Digital Intersectionality (Noble and Tynes 2016), which would allow to further question the organization of social relations embedded in digital technologies and foster a clearer understanding of how power relations are organized through them. ",
        "article_title": "Towards a Critical Approach to Digitally-Mediated Discursive Practices of Gender-Based Hostility",
        "authors": [
            {
                "given": "Eleonora",
                "family": "Esposito",
                "affiliation": [
                    {
                        "original_name": "University of Navarra, Spain",
                        "normalized_name": "University of Navarra",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02rxc7m23",
                            "GRID": "grid.5924.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-03",
        "keywords": [
            "corpus and text analysis",
            "social media",
            "digital ecologies",
            "linguistics",
            "gender studies",
            "English",
            "digital communities and critical infrastructure studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Efforts to measure the  organization of human activity in terms of complexity  have a long history (Chick 1997 ).  The recent  proliferation  of  computational tools has  accelerated  the creation of  rigorous  means of modeling and assessing   the development of  human  collectivities ,   with particular reference to  the   emergence  of  social  complexity   ( Sawyer 2005;  Miller and Page 2007:  Murphy and Stoeger 2007;  44ff ;   Jörg 2011:  36ff ) .   This paper argues that  r ecent studies of historical social complexity have  neglected to account for the   emergence  of social complexity   because of a reliance   on  a reductionist approach.  In place of the reductionist  approach , we  put forward a prototype of a NoSQL database architecture for studying the emergence of populated with data from historical Arabo-Islamic sources.   In the face of pervasive critiques of narrativist historiographical techniques in historical scholarship (White 1973; Clark 2004),  computationally-oriented approaches to human collectivities   have turned to more “scientific” approaches. Frequently, this involves the use of a  reductionist method, breaking down various non-narrative data related to a collective into various constituent parts under the belief that each of these can be scientifically described  and analyzed  (Gallagher and Appenzeller 1999) .  R ecent studies by Preiser-Kappeler (2018) and Turchin  et al  (2017) exemplify this approach.  The former paper applies statistical tests to a single, node-level variable (computed from “distance decay effect” between cities) to model the long-term “resiliency” of transport networks in the pre-modern Roman and Chinese empires, whereas the latter proposes nine “complexity characteristics”  that are coded into an RDF (graph) database, the analysis of which reveals a  “single dimension” that accounts for measures of complexity across a variety of civilizations.    E ach of these studies makes a successful, independent case for the evolution of complex social structures;  the use of a RDF database by Turchin  et al  moreover significantly increases the explanatory value of the convincing statistical analyses put forward by Preiser-Kappeler.   However,  the reductionist approach  adopted in each  assumes , rather than demonstrat es , the prior emergence of  these  complex features.  Furthermore, both studies   show how the limitations of some datasets limit the explanatory power of the reductionist approach in the first place. For instance,   Preiser-Kappeler’s study  also assumes “long term stability of core elements” of the Roman and Chinese empires, which precludes an investigation into the emergence of such “core elements” in the first place. Similarly, T urchin  et al ’s limitation of analysis to “the appearance of politically centralized societies” (ibid: 2), stands in  stark  contrast to a central tent of theories of complex systems which states that the are composed of “networks of components with no [mechanism of] central control” (Mitchell, 2009: 13). The task of describing the  actual emergence of  complexity  within  human collectives thus seems to require  the following elements :  (1) time-sensitive  data  related to   (2)  entities that are substantially independent from a central control mechanism, and  (3)  a sufficiently robust  data storage system,  such as a RDF database.   We propose that the above-mentioned conditions can be met using data from a historical Islamic religious movement known as Sufism, which first appeared in Iraq during the 9 th  century (Nicholson 1914; Massignon 1975; Schimmel 1975; Knysh 2010; Melchert 2013).  No study of this movement has given a conclusive answer as to the emergence of Sufism, largely because previous research has failed to analyze a key feature of early literature written by Sufis: the  “pathway of transmission” (Ar.  isnad ; lit. “citation”) used to introduce narrative  content. An example of this form would appear as follows: I heard [X] say he heard [Y] say that he heard [Z] say […].” Although there have been numerous attempts to undertake the analysis of abundant relational data found in non-Sufi Islamic sources (Şentürk 2005; Romanov 2014), these data are either not publicly available, or exclude the narrative component of these data because of the limitations of relational (SQL) databases in which they were compiled.   In this paper, I outline the architecture needed to leverage the complexity of the relational data in early isnad-based Sufi literature with as network data NoSQL database framework. Within this network, the breadth and depth of node level data (i.e. individuals involved in transmitting Sufi sayings) can vary widely with regards to: names, occupations, birth and death dates, destinations of travels, teachers, students, intellectual specialties, and matrices that record other figures’ opinions about the figure, to name only a few. On the edge level (i.e. the structure of the isnads that connect the nodes), features are more standard, and can include: place of transmission, date of transmission, method of transmission (via book, face-to-face meeting, etc.), as well as the narrative content of the saying that is transmitted via the isnad. At present, the architecture of these data is being prototyped using the data from one early Sufi work (al-Khuldi 2011) on the MongoDB NoSQL database. This decision was based on both the flexibility of the JSON scheme that is native to MongoDB, as well as the ability to easily transfer this data onto a graph database scheme, such as Neo4J.  ",
        "article_title": " From Reductionism to Complexity: A Digital Corpus for Sufism Using a NoSQL database for studying the emergence of complexity ",
        "authors": [
            {
                "given": "Jeremy",
                "family": "Farrell",
                "affiliation": [
                    {
                        "original_name": "Emory University, United States of America",
                        "normalized_name": "Emory University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03czfpz43",
                            "GRID": "grid.189967.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "corpus and text analysis",
            "English",
            "prosopography",
            "near eastern studies",
            "databases & dbms",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Although there have been some infrastructural developments of late, the main  modus operandi in digital literary studies is still to apply a certain research method to an ephemeral corpus. In a best-case scenario, the results are  somehow reproducible, in a worst-case scenario they are not reproducible at all. At best, there is an openly accessible corpus in a standard format such as TEI, another markup language, or at least TXT. At worst, the corpus is not even accessible, i.e., the research results cannot be questioned.  However, there are signs that this is slowly changing. Some projects provide interfaces that allow for multiple ways of access to corpora. One of these projects is DraCor, an open platform for research on (European) drama, which will be introduced in this paper (accessible at   or via its GitHub repositories or its API). DraCor transforms existing text collections into 'Programmable Corpora' – a new term we bring into play with this talk.    Building Blocks  Vanilla Corpora Similar to the COST Action on European novels (Schöch et al. 2018), the DraCor project seeks to establish a bundle of multilingual drama corpora encoded in basic TEI as basis for digital comparative studies. To date, the platform enables access to a Russian-language ( ) and a German-language corpus of plays ( ). Similar to Paul Fièvre's collection \"Théâtre classique\", these corpora are designed as vanilla corpora, which initially contain hardly any special markup beyond the necessary, but are freely available and can therefore be forked, enriched and expanded. To demonstrate that other corpora can be easily linked to the platform, we forked the Shakespeare Folger Corpus and the Swedish Dramawebben corpus and connected it, and all existing extraction and visualisation methods of the platform are readily applicable to the newly added corpora (  and  ). Other corpora of dramatic texts are to follow; the only prerequisite is that they are encoded in TEI.  The advantages of a freely available corpus hosted on GitHub are obvious. Not only can the corpus be cloned and loaded directly into an XML database like eXist-db. Using the SVN wrapper from GitHub, the entire corpus can also be downloaded directly, in its current state and without version history if this is not needed: svn export https://github.com/dracor-org/rusdracor/trunk/tei An openly accessible GitHub repository also means that pull requests for error correction are possible and welcome.   XML Database (eXist-db) and Frontend DraCor relies on eXist as XML database to process TEI files and to provide functions for researching the corpora. The frontend is built with React ( ), it is responsive and easily extensible. However, the focus is not on the GUI, but on the API (on the general differences between these two approaches to interfaces cf. Bleier/Klug 2018).    API To come close to the ideal and the possibility of applying \"all methods to all texts\" in a simple manner (Frank/Ivanovic 2018), it takes more than open corpora. The article by Frank/Ivanovic advocates SPARQL endpoints (for which there is also a readily-available app for eXist-db:  ).  DraCoroffers such endpoint, but also features a rich general API documented and explained via Swagger ( ). In a subarea of corpus philology, the digital scholarly editions, discussions about more proactive use of APIs have already begun (for background information cf. Bleier/Klug 2018), the Folger Digital Texts API may serve as an example ( ). The advantage of a more modern solution like Swagger is that API queries can be executed live and directly and that the output can be controlled more precisely.  A simple use-case scenario would look like this: using RStudio you can throw a quick glance into a corpus with just a few lines of code, maybe regarding the development of the number of characters in Russian drama between 1740 and 1940, stored in the metadata table ( ). This table, which can be obtained in JSON or CSV format, is read into a Data.Table, whereupon the values of two columns (year of publication and number of speakers) can simply be visualised via ggplot (Fig. 1).     Figure 1: Number of characters per play in chronological order (source: RusDraCor). This very simple example is able to show the starting point of a decisive structural diversification of the Russian drama landscape. Pushkin's historical drama \"Boris Godunov\" (1825), result of his reading of Shakespeare, features speech acts of 79 characters, a number previously unthinkable in Russia drama.  However, the possibilities are not limited to using ready-made API functions. New research ideas always create new needs for easily obtainable and reproducible data and metrics; the API can be extended accordingly, i.e., new research ideas can be implemented centrally in the API layer. This is made even easier by the fact that Apache Ant can be used to rebuild the entire development environment on your own system. In addition to structural data and metadata, full texts without markup can also be obtained, e.g., if methods such as stylometry or topic modelling are the purpose, i.e., methods that need a \"bag of words\" and do not require markup. All in all, the structure and documentation of open APIs makes it much easier to reproduce research results, which up to now has often been a time-consuming (or impossible) process.   Shiny App An example of the versatility of the DraCor API is the Shiny App created by Ivan Pozdniakov ( ). Shiny is a framework based on R, which makes it possible to display interactive visualisations in the browser. The DraCor Shiny App does just that, relying entirely on the DraCor API for data retrieval. Thus, visualisations of the current database can be used for teaching and research purposes, but also for easier data correction.    Didactics The formalisation of literary texts, for example via markup, is not self-explanatory. Although the community can rely on some standards, every operationalisation depends on the actual research question. To give an example: if you would like to extract network data based on character interactions in a literary text, you would have dozens of different ways of doing this (e.g., Grayson et al. 2016 test different extraction methods for novels and compare the results). This also applies to plays. In order to sharpen the senses for this in teaching, we developed the tool \"ezlinavis\" ( ) and integrated it into the DraCor toolchain. Network data can be extracted from literary texts manually, also to raise the awareness for the contingency of this process, an important preliminary step to the eventual step of operationalisation.  In addition to an approach to the gamification of the process of correcting TEI-encoded corpora (Göbel/Meiners 2016), we also developed a card game for teaching purposes in order to playfully train the understanding of network values (Fischer at al. 2018). These didactic tools wrapped around the platform are an integral part of the whole project as they are based on the project data and operationalisations. While building the platform, it was important to recognise that data can take several forms and be equally important for research and teaching.   Linked Open Data The TEI files contain GND [Integrated Authority File of the German National Library] and Wikidata identifiers for both authors and works. In this way, various data and facts that lie beyond one's own corpus work can be included. Something like an automatically created gallery of authors has a more illustrative character (de la Iglesia/Fischer 2016). But using the same identifiers, we can also determine if a corpus has a regional bias. Via Wikidata, we can easily display the distribution of the authors' places of birth and death on a map (by doing so, we could rule out that our German-language corpus GerDraCor has a regional bias, cf. Göbel/Fischer 2015). Similarly, the Wikidata ID of the plays can be used to find out where they were first performed (example query:  ), i.e., aspects of the performance history can be switched on, even though they are not the focus of the core project and based on data curated elsewhere.    Infrastructure Instead of Rapid Prototyping Projects like DraCor seek to provide the digital literary studies with a reliable and extensible infrastructure so that the research community can focus on research questions. An important conclusion for us was that we would give up the further development of our all-in-one Python script collection \"dramavis\" (Kittel/Fischer 2014–2018 and Fischer et al. 2017), which we have been developing for four years now. From here on, we would rather devote our time to the API. \"Dramavis\" followed the idea of rapid prototyping and had to do all by itself, including the preprocessing of data (Trilcke/Fischer 2018), which is not untypical in the Digital Humanities. The code base has grown quite a bit in the meantime and its maintenance has become difficult and often enough led away from actual research questions.    Outlook In allusion to the project \"ProgrammableWeb\" – which maintains a database of open APIs and whose slogan is: \"APIs, Mashups and the Web as Platform\" (accessible at  ) – we propose the term 'Programmable Corpora' for research-oriented corpora providing an API.  Programmable Corpora facilitate the implemention of research questions around corpora. It is to be expected that infrastructural efforts of this kind will pay off for the entire community with effects such as those listed by John Womersley in his presentation at the ICRI2018 conference in Vienna: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction. The are numerous ways to connect to Programmable Corpora, no matter if you don't want to code at all and only need a CSV file for Excel or LibreOffice Calc or a GEXF file for Gephi, if you want to research a corpus via its connections to the Linked Open Data cloud or just want to get specific data for your R or Python script without having to worry about the corpus and its maintenance and reproducibility (all this remains an option, though). Programmable Corpora make it easier to decide on which level of the platform your own research process starts.  ",
        "article_title": " Programmable Corpora: Introducing DraCor, an Infrastructure for the Research on European Drama  ",
        "authors": [
            {
                "given": "Frank",
                "family": "Fischer",
                "affiliation": [
                    {
                        "original_name": "National Research University Higher School of Economics, Moscow",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            },
            {
                "given": "Ingo",
                "family": "Börner",
                "affiliation": [
                    {
                        "original_name": "Austrian Centre for Digital Humanities at the Austrian Academy of Sciences",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Mathias",
                "family": "Göbel",
                "affiliation": [
                    {
                        "original_name": "Göttingen State and University Library",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Angelika",
                "family": "Hechtl",
                "affiliation": [
                    {
                        "original_name": "Vienna University of Economics and Business",
                        "normalized_name": "Vienna University of Economics and Business",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03yn8s215",
                            "GRID": "grid.15788.33"
                        }
                    }
                ]
            },
            {
                "given": "Christopher",
                "family": "Kittel",
                "affiliation": [
                    {
                        "original_name": "University of Graz",
                        "normalized_name": "University of Graz",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/01faaaf77",
                            "GRID": "grid.5110.5"
                        }
                    }
                ]
            },
            {
                "given": "Carsten",
                "family": "Milling",
                "affiliation": [
                    {
                        "original_name": "National Research University Higher School of Economics, Moscow",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            },
            {
                "given": "Peer",
                "family": "Trilcke",
                "affiliation": [
                    {
                        "original_name": "University of Potsdam",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-20",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "literary studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The techno-political manifestations that seek to breach the established order abound in the north or south of the globe. Tactics and strategies (Certeau, 1984) are used by subaltern groups to express their opinions or deviant practices. In challenging socio-economic environments such practices can assume a wide variety of forms: for instance, by using the digital as both discourse and practice of subversion to established orders. In this article, we choose a single expression of such subversions to represent one of the ways through which the digital take part in everyday lives. The practice we discuss in this paper is what we call “Gambiarra”: a term applied to a myriad of improvisations, usually material and technical ones as a result of scarcities of all sorts. Gambiarras are normally the technical expression through which people overcome everyday obstacles from the most ordinary ones to the outmost complex environments. Rosas (2006) and Clinio (2011) define Gambiarra as a “do it yourself a la brasileira”, in which the technical limitations are overtaken through creativity in proposing innovative solutions. As a synonym for improvising in a popular culture realm, this is a “inventive process of repossession, adaptation and transformation of available materials in an alternative design form, which allows the creation of improvised solutions for real demands” (Clinio, 2011, p.76, translated). Widely used in the Brazilian quotidian, Gambiarra has a meaning in the daily lives of people that tactically adapt its (digital) apparatuses in order to resist to daily-life problems of all sorts. As such, the paper is supported on two case studies. The first is “Gambiarra Favela Tech    Website:   http://www.gambiarrafavelatech.org/    ”, an artistic residence held in July and December 2015 in the Maré Complex, in the northern area of Rio de Janeiro, through a partnership of the Olabi makerspace and the “Favelas Observatory”. The initiative brought together 12 young people from the local community to propose new usages for obsolete materials. Taking as its motto the improvisation and the inventiveness that transform realities, around 40 hours of workshops were provided in order to practice the usage of materials in line with gambiarra rationale: \"to take something that is used in a traditional way and to use it in another way, in a way that nobody would imagine\" as one of the young participants commented    The first testimonial of the project summary video, available at:   https://www.youtube.com/watch?time_continue=18&v=h54_A5fXk0o    ”.  Gambiarra Favela Tech's proposal was anchored in three main aspects: (a) developing environmental awareness, which indicates that better than producing something new and discarding another product in the environment would be to recycle non used products; (b) losing the fear of opening the black boxes, by discovering how technical objects work through a playful  manner; and (c) “sevirismo”, a brazilian expression that represents \"the science of dealing with what one has,\" - a term that can be seen as a synonym for gambiarra.  Such project interacts with the history of the word gambiarra as a concrete example of a\"badly done\" improvisation – also typical of favela environments as already mentioned. The improvisations in this case used few resources, trash and unused objects in order to produce works of art that could relate to the favela community. By rearranging materials and giving alternative means to waste, bricolages and gambiarras are put forward as a way to produce art in a scarce context. The success of the project in that local context was, thus, a product of symbolic re-ordering of materials. The second case is “Hacker Clubs” activities in Raul Hacker Club,“a group of people interested in using, re-using and sharing technology, learning, fun and culture in a collaborative and indiscriminate manner”. Located in the northeast Brazilian coastal area, this hackerspace defines itself as an assemblage of different people managing a non-hierarchical space, without influences of public or private institutions (Martins, 2017). The name honors a well-known Brazilian singer-songwriter born in Bahia, Raul Seixas, whose songs praised, among other themes, the alternative ways of living. Some of the projects carried out by this group are the  Criança Hacker    Infant Hacker, in English  , series of activities of education free of informatics and basic electronics for children;  Data Laboratory for Citizenship Hacker, a collaborative space for research, work and discussion on scraping public data for journalistic purposes; among other diverse activities of learning and production of projects in the areas of electronics, free software, open data and hacker culture.  The approach of the hacker culture - of which hackerspaces like Raul are some of the main representatives – has for many years included the idea of a gambiarra such as the creation of a network called “Meta-Reciclagem    Meta-Recycling, in English.  ”: a movement that brings together hackers, students and artists who propose the deconstruction of technology for social transformation, which relates to a gambiarra rationale as an ideological practice of resistance to the dominant order. One of his projects, no longer in activity, was called the \"mutirão gambiarra    Mutirão means a group of people that join efforts towards a specific goal in the form of a task-force. Also described as a “communal work”.  \" ( mutgamb) and constituted as an \"editorial collective that articulates collaborative publications on themes such as creative appropriation of technologies, experimental digital culture and collaborative networks\". The network has helped to popularize the term \"gambiology\", a fusion of gambiarra with \"ideology\" in Portuguese, which can also be interpreted as \"science of gambiarra\", a term that shifts the expression out of its pejorative meaning by entailing gambiarra as a localized practice of technological innovation with few resources”.  In hackerspaces, gambiarra is a quintessential form of a “hacker culture    We support our claims on the notion of “hacker” based on Coleman’s (2015) writings about the term.  ” as if the term would entail the brazilian form of hacking materials, codes and ideas. As a typical hackerspace suggests, learning is based on collaboration in a “hand-on” modus operandi. Teaching, living and working with computers and electronics is an example of hackerspace that can be better understood by framing such practices as gambiarras.  Every technology represents a cultural invention in the sense that it produces a world (Escobar, 2016b). It was our intention to shed light on practices that produce a world and, therefore, entail a form of getting to know the reality though the improvisation. It would be possible to argue that improvisation is not only related to materials and artifacts, but also ideas. We draw on Le Breton (2017) as well to argue that the mind-body division that such assumption would suggest is not feasible in gambiarra terms - solely because even though our examples of gambiarras are technical and material applications of digital humanities, they are a result of ideation and creative improvisation that cannot properly work without planning. As such, we argue that applying gambiarras might evolve into an epistemology, a way of reaching reality through its manipulation. Gambiarra, therefore, is a combination of practices tailored to solve practical problems of an everyday life. Although not typically restrained to global-south areas, we argue that typically brazilian gambiarras resignify the material usage into new symbolic realms. Even though overcoming the myth of modernity (Escobar, 2004) is not a new argument in itself, the combination of such endeavour with the practices that shape everyday life in global-south regions was also the goal of our essay. Finally, as means to propose empirical continuations of this paper, the ethnographic and anthropological investigation of gambiarras in all sorts of forms and places is likely to make emerge a myriad of practices from the outskirts of the world, where the digital finds a way to exist despite its possible restrictions – socio-economic ones mostly. By showing the richness of gambiarras, we argue that it is possible to theorize digital humanities as mostly applied to the people that make use of it in its daily applications. ",
        "article_title": "Towards An Epistemology Of “Gambiarra”: Technical Resignification In Brazil",
        "authors": [
            {
                "given": "Leonardo Feltrin",
                "family": "Foletto",
                "affiliation": []
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital activism and hacker cultures",
            "anthropology",
            "bibliographic methods / textual studies",
            "English",
            "cultural studies",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Over the last years various corpus projects have started all over the world documenting sign languages. The purposes of such corpora focus primarily on the linguistic study of the languages as well as the preservation of the languages themselves. As Drew and Ney deduct (Dreuw and Ney, 2008), the processing and storing phase of these corpora require a textual representation of the signs. Although different notation systems have been created over the years, gloss notation seems the prevalent one. Instead of using an annotation system with components representing the main formal components of a sign, ID glosses are typically used. These consist of a uniquely identifying spoken language word (written in capitals) that by definition refers to a particular sign form. During the annotation process the researcher has to determine the precise time a sign occurs and properly identify and gloss it. As a result, the annotation process is extremely labor intensive, but it is a condition for a reliable quantitative analysis of the sign language corpora. The focus of this project is the development of a tool for automatic annotation of sign occurrences in video corpora as a first step towards fully automatic annotation. This study presents a new approach to automatic annotation for sign languages using as little data for training as possible and taking advantage of a state-of-the-art pose estimation framework for a robust and unbiased tracking.   Literature review Recent developments in the field of sign language recognition depict the advantages of machine and deep learning for tasks related to recognition and classification (Agha et al., 2018; Pigou et al., 2015; Masood et al., 2018). However, they require a vast amount of data to be trained and they are bounded in the sign language they have been trained on (hard to generalize in other sign languages) Additionally, approaches in automatic annotation for sign languages require manual annotation of the hands and body joints prior to the training process of the recognizer models (Pfister et al., 2015; Aitpayev et al., 2016). Furthermore, most studies apply skin color and motion detection algorithms (Kumar, 2017) that are prone to errors and possibly skin color bias. It is also often the case that in order to assist the hand tracking model, corpora are compiled using colored gloves for the subjects (Masood et al., 2018) or captured using Kinect (Pigou et al., 2015) making the result of such studies unusable in real-life conditions in the corpora. Pose estimation, as a technique to detect human figures in images and video, showed enormous improvement over the last years. OpenPose (Cao et al., 2017) is the state-of-the-art framework when it comes to accurately detect human body and hands keypoints. The model takes as input a color image or video and through a 2-branch multi-stage Convolutional Neural Network predicts the 2D locations of keypoints for each person in the image. This framework was chosen to be used in this study as it has been trained on the Multi-Person (MPI) and COCO datasets making it exceptionally robust and fast.   Methodology A data-set of 7805 frames in total (approximately 4 minutes of videos) has been compiled and labeled as signing or not signing. The dimensions of the frames were 352 by 288 pixels and were extracted from the Adamorobe and Berbey sign language corpora (Nyst, 2007; Nyst et al., 2012). These corpora portray an additional challenge as they are extremely noisy and low quality. Furthermore, they contain signing from one and two people at the same time. The original data-set was split into a training and testing set of 6150 and 1655 frames respectively. Using OpenPose, the positions of the hands, elbows, shoulders and head were extracted from each frame. The positions of the rest of the body joints were disregarded as most of the time they were out of the frame bounds. It is important to compare the performance of multiple different machine learning algorithms consistently. Thus, four different classification methods were used and optimized, namely: Support Vector Machines, Random Forests, Artificial Neural Networks and XGBoost. The majority of these algorithms have been extensively used in machine learning studies as well as in sign language applications (Agha et al., 2018). Performance was measured using the metric of Area Under the Receiver Operating Characteristics (AUROC).   Results All classifiers performed adequately well. However, the best AUC score was found in XGBoost (0.92). Figure 1 presents the AUROC curve after a 10-fold cross-validation. The Artificial Neural Network was found to perform sufficiently well (AUC: 0.88). While the performance of the model is satisfactory, it is important to explore the features that contribute to the classification task. Figure 2 shows the importance of each feature as measured by the classifier. The result is reasonable as the position of the dominant hand (i.e. right) has the highest importance on how the classifier weights the features. To account for multiple people signing in one frame, an extra module was added. The module creates bounding boxes around each person recognized by OpenPose, normalizes the positions of the body joints and runs the classifier. This process makes it possible to classify sign occurrences for multiple people in a frame irrespective of their positions (figure 3). Once all the frames have been classified, the \"cleaning up\" and annotation phase starts. A sign occurrence is annotated only if at least 12 consecutive frames have been classified as \"signing\" frames. This way I account for the false positive errors. This sets the stage for the annotation step. By using the PyMpi python library (Lubbers and Torreira, 2013) the classifications are translated into annotations that can be imported directly to ELAN. Figure 4 shows the result of the overall output.   AUROC curve of XGBoost after a 10-fold cross-validation.    The importance of each feature based on XGBoost classifier.    Recognition module in multiple people.    Final output of the tool as seen in ELAN.    Conclusion This is the first step towards fully automated sign language annotation. The results show that a frame-to-frame classification using XGBoost is a promising tool for the annotation of sign occurrences in a video. The significance of this study lies on the fact that the tool created can be easily adjusted and used in any kind of sign language corpus regardless of its quality, the sign language presented or the number of people in the video. Furthermore, one needs approximately only 4 minutes of annotated video in order to retrain the model making the process as easy as possible. Finally, the tool has the potential to be extended and used in gestural corpora as well.  ",
        "article_title": "To Sign or not to Sign: Automated Generation of Annotation Slots for Sign Language Videos using Machine Learning",
        "authors": [
            {
                "given": "Manolis",
                "family": "Fragkiadakis",
                "affiliation": [
                    {
                        "original_name": "Leiden University Centre for Linguistics; Leiden Centre of Data Science Research; Leiden University Centre for Digital Humanities",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "corpus and text analysis",
            "linking and annotation",
            "multimedia",
            "artificial intelligence and machine learning",
            "linguistics",
            "English",
            "computer science and informatics",
            "audio",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper presents ongoing foundational theoretical and practical work on the application of ontology-based modeling to represent and visualize the complexity of knowledge disseminated in historical narratives. In short, the new approach combines modeling informed by philosophical ontology and philosophy of history with semiotically founded visualization of historical processes in order to support historical understanding. The following quote from Munslow (2007) lends itself as an appealing summary of the character of historical narratives: “In writing a history for the past we create a semiotic representation that encompasses reference  to it, an explanation  of it and a meaning  for it.” What role could information visualization tools play in this context? As Champagne (2016) remarks, “[h]istorians occasionally use timelines, but many seem to regard such signs merely as ways of visually summarizing results that are presumably better expressed in prose.” He challenges this view and argues that timelines could support the historian in gaining novel historical insights. The main cognitive funtion of timelines is according to Champagne (2016: 40) the “logical conjunction by visual juxtaposition”. Furthermore there is also the potential of abductive reasoning: “Timelines, however, are more likely to surprise us, by showing us past events that we would have never otherwise considered chunking. Hence, in addition to historical scholarship expressed in regular prose, consulting diagrammatic signs can foster the discovery of patterns essential to a fuller understanding of the past” (Champagne, 2016: 40). This is especially more likely if there are synchronoptic timelines showing historical events of different categories—i.e. not only political events, but also economic or cultural events etc. (That is the approach conducted by Peters and Peters (1952) in their  Synchronoptische Weltgeschichte.) For example, such parallel timelines could possibly be used as a tool to support periodization (see possible use case reported by Luyt (2015)).    Problem statement and objectives A “visual historiography” (Roegiers and Truyen, 2008) can quite easily be done via the temporal, spatial, and thematic context of information about historical events, but without explicitly stated relations between events it is questionable how useful that could be in supporting historical research. The big advantage of that approach is of course, “that one is able to represent the complexity of a historical subject, without having to fill out the gaps, or having to choose between different interpretations, but using an [information integration] architecture that places the subject in its context(s)” (Roegiers and Truyen (2008: 70) as cited in Sabharwal (2015: 57)). The problem with such a ‘visual historiography’ is that it cannot support visual contextualization done by the historian during conceptualizing complex interrelations of historical events—including not only temporal, spatial, and thematic relations, but also causal relations (incl. the motivation and roles of historical actors involved in the events), mereological, and constitutive relations of complex (e.g. composite) events. Anyhow, explicit modeling of event structure and relations is necessary because without a more fine-grained representation of the structure and interrelations of events visualization tools are indeed limited to bare juxtaposition. Digital history demands information visualizations beyond simple timelines (see for example Drucker and Nowviskie, 2003). Diagrammatic approaches for multiperspectival analysis and synopsis of historical sources are needed. There are rarely technical implementations and just a few theoretical approaches to the development of such tools for multiperspectival exploration of historical sources (cf. Shaw, 2008: 90). See for example Drucker (2011) in “Humanities Approaches to Graphical Display”: “At best, we need to take on the challenge of developing graphical expressions rooted in and appropriate to interpretative activity.” Jänicke (2016) argues that the reason for this slow progress is the still deficient collaboration between researchers in information visualization and digital humanities. This brings us back to the announced modeling approach, because conceptualizing can be considered as modeling: “the more schematic the conceptualization in a discipline, the more its practitoners are likely to engage with models rather than concepts” (McCarty, 2005: 25). Historians construct concepts in order to understand historical events. The method for this kind of explanation is known as colligation—the construction of colligatory concepts—in philosophy of history. Walsh (1951 S. 59) defines colligation as “the procedure of explaining an event by tracing its  intrinsic relations to other events and locating it in its historical context” (Walsh (1951 S. 59) as cited in Shaw (2010 S. 11)). An adequate modeling of colligatory concepts and the relations between the concepts—the colligatory relations (Shaw, 2008)—is the precondition for “semantic tools” (Shaw, 2013) based on such an explicit representation of the past.    Related work The digital edition (Behrendt et al., 2010) of Peter’s (already mentioned)  Synchronoptische Weltgeschichte (Peters and Peters, 1952) visualizes parallel timelines showing historical events from different categories (political events, economic events, cultural events, etc.). However, it does not show the inner structure of complex events or processes and does not provide typed relations between events (see Shneiderman, 1996). Interestingly the tool provides visual contextualization of events based on the knowledge organization in its event database (see Fig. 1): related events are retrieved based on their common index terms.     Fig. 1 Screenshot of  Der Digitale Peters (Behrendt et al., 2010) showing contextualized historical events   The older tool  SemTime (Jensen, 2003) provides a solution to these typical shortcomings of timeline visualizations by introducing Semantic Timelines to visualize complex timelines with sub-timelines and different types of relations between historical events. Newer projects concentrate not only on the granularity of events but also on the the details of biographies, i.e. the modeling and visualization of the roles of (historical) persons in events (Trame et al., 2013; Hyvönen et al., 2018).  The VICODI (Visual Contextualisation of Digital Content) project (Nagypál et al., 2005) used semantic web ontologies as basis for visual contextualization. Based on the top-level ontology DOLCE (Gangemi et al., 2002) the SHO (Spatial History Ontology) was developed by Grossner (2010) to overcome the shortcomings of exisiting ontologies in the modeling of spatial information in event ontologies. HERO (Historical Event Representation Ontology) is also founded on DOLCE and focuses on the modeling of different types of roles (thematic, social, and also perspectival) (Goy et al., 2018). DOLCE and DOLCE-DnS Ultralite (DUL) respectively contains the Descriptions and Situations (DnS) Ontology Design Pattern (ODP) (Gangemi and Mika, 2003). DnS allows the modeling of different perspectives on entities. A CRM (Doerr, 2003) based alternative to model perspectives or interpretations is the MIDM (Multiple Interpretation Data Model) (Ruymbeke et al., 2017). A much simpler modeling approach for different perspectives is SEM (Simple Event Model) (Hage et al., 2011).   Approach A crucial requirement for the approach presented here is the representation of different perspectives on historical events. Perspectival explanation or “synoptic judgement” (Mink, 1987) is a main task of the historian (cf. Levy, 2001: 70). I have chosen DUL as top-level ontology for the modeling examples described in the following paragraphs because of its constructivist design principles and especially because its DnS ODP fits very well to our requirement of modeling colligations and the different perspectives or interpretations of historical events. Thus, a Description represents the conceptual relations which were grasped by the historian in a synoptic judgement. In biomedical ontologies (so far the main domain of applied ontology) Descriptions are used to represent medical diagnoses. There is indeed an interesting similarity between the synoptic judgements of a historian and the medical diagnoses of a physician: “The best analogy I can suggest for the way in which synoptic judgments are reached is that of a physician’s diagnosis—a combination of broad medical knowledge, relevant evidence drawn from various tests, a knowledge of various theoretical possibilities for explanation, and skill in seeing which interpretation of the evidence works best in a particular case—the difference being, of course, that the physician deals primarily with law-bound physiological processes, the historian primarily with human conduct and purposive action” (Schroeder, 1997).  Fig. 2 shows a screenshot from an experimental tool that draws diagrams of causal narratives. The example is from Theda Skocpol’s  States and Social Revolutions: A Comparative Analysis of France, Russia, and China. Skocpol (1979) describes the historical process which led to the French revolution in narrative form. In order to visualize the historical process consisting of three sub-processes, the mereological and causal relations had to be represented in a knowledge base according to the reconstruction of Skocpol’s narrative done by Mahoney (1999). I am planning to adapt Semantic MediaWiki (Krötzsch et al., 2007) extensions (e.g. Weller and Maleshkova, 2016) for the creation of ‘ontological hypertext’ as base for visualization using constraint-based layout algorithms (e.g. inspired by visualizations in biology as in Hoffswell et al., 2018).     Fig. 2 Diagram of the causal narrative based on the reconstruction and visualization by Mahoney (1999: 1166) (The nodes represent events extracted from the narrative and the edges represent causal links between them. The causally linked events 33 and 34 for example are summarized as follows in Mahoney’s narrative analysis: “Pressures for creation of the Estates-General” and “King summons the Estates-General”. 37 finally marks the state collapse and the outbreak of the revolution.)   Skocpol (1979) combines macrosocial and idiographic historical research (cf. Mahoney, 1999: 1189). Fewer idiographic detail is represented in the example from the CEWS (Conflict Early Warning Systems) project (Schmalberger and Alker, 2001a) (see Fig. 3). CEWS focuses on the phase sequences (escalation and de-escalation) in conflict processes. The CEWS Explorer was developed as a tool to visualize and compare conflict phase sequences and different perspectives on them as described in causal narratives about conflicts. In my talk I argue that a remake of this approach can benefit from an ontology-based representation of conflict phases and different perspectives on phases and causes for change of phases as seen from different conflict parties or other actors involved in the conflict.    Fig. 3 Diagram of the first two episodes of the Transnistria conflict (according to the narrative by Vorkunova, 2001) (Nodes represent conflict phases and edges represent transitory events. Note that divergent perspectives on conflict episodes can be added via synchronoptic views.)   According to Peirce’s diagrammatic reasoning approach a diagram should be constructed under the rules of a “system of representation” (CP 4.418). The ontology-based knowledge representation of historical events provides as well a framework for the construction of such representation systems. Similar to the so-called two-level theory of social revolutions in the first example (see diagrammatic representation of the theory in Goertz and Mahoney (2005: 509)) the so-called visual grammar of possible conflict phase sequences in Fig.  4 is a representation system that pretends all possible sequences of different types of conflict phases within conflict episodes. The system is used as “system of diagrammatization” (NEM IV:318) in order to construct diagrams for specific conflict trajectories.    Fig. 4 Visual grammar for possible sequences of conflict phases (according to Schmalberger and Alker, 2001a)   As there is less granularity, i.e. no complex composite events, a simpler ODP can be used to represent conflict phase transitions. Fig. 5 shows an exemplary phase transition modeled with the ODP Transition   . Based on the ‘system of diagrammatization’ and some empirical data about conflicts diagrams of possible conflict sequences similar to the famous diagram of the Cuban missile crisis could be created (see Fig. 6). The CEWS Explorer was also built for counterfactual analysis of conflicts (see Schmalberger and Alker, 2001b).     Fig. 5 Modeling the change of a conflict’s state with the Transition ODP and classification of the conflict phase types and the transitory event type with knowledge organization systems      Fig. 6 Diagram for counterfactual exploration of possible decisions of the conflict parties and resulting outcomes of the Cuban crisis according to the game tree based counterfactual analysis by Joxe (1963) (from Bertin, 1967)     Conclusion There are two feasible use cases for the presented modeling and visualization approach—followed by a more demanding one:  preparation and communication of research results, knowledge representation and knowledge visualization for public history, and generation of new insights with information visualization tools for digital history.  The added value for the diagrammatic communication of research findings originates in the explicit representation of historical knowledge: A complex historical process can be visualized for better communication of research results on the base of the explicitly represented entities and their relations. As Lange (2013: 46) notes in his textbook on comparative-historical methods, it is recommended to use “diagrams to represent clearly the argument of causal narration, to make the causal claim more explicit”. Supported by a ‘system of diagrammatization’, a (public) historian is enabled to represent and visualize the essential event relations of a “basic story” (cf. Perfetti et al., 1995: 2)—excluding more granular expert knowledge about the historical process in focus. In knowledge visualization projects for public history additional diagram types could be used in order to provide the “collateral knowledge” (Hoffmann, 2005) necessary for the public audience (non-historians) to better understand the historical events—e.g. via concept maps or knowledge mapping in general (Davies, 2011). It has been shown in public history’s neighboring discipline history didactics, that concept maps “can help learners to analyse and synthesise knowledge” (Tzeng, 2010). This is especially valuable to provide insight into the different perspectives on an issue (for example in divergent conflict histories or national memory narratives). The third use case lies beyond knowledge visualization. It is aimed at interactive information visualization tools in the sense of a “knowledge generator” (Drucker, 2014): using diagrams to support historians during research in order to create new knowledge. Formalization of the complexities and subtleties of expert knowledge is the necessary precondition for building diagrammatic reasoning tools for historical understanding. See Frank (2018) for more details about the semiotic foundations of this systematic diagrammatic reasoning procedure by means of systematically experimenting on a knowledge base (created by ontology-based knowledge modeling) in order to infer new knowledge. Hence, the requirements of “total explicitness and absolute consistency” (McCarty, 2005: 5) for formal knowledge modeling are large obstacles towards such ‘thinking tools’. Case studies in digital history have to reveal if the proposed diagrammatic reasoning approach is suitable. Besides historical conflicts the ontology-based modeling approach is planned to be used to prepare a collection of historical travelogues to be published as digital editions enhanced with interactive map-based visualizations. We will apply ODPs to model journeys and historical routes described in the travel narratives. See also the Linked Places project’s conceptual model of historical geographic movement:      ",
        "article_title": " Diagramming the Complexities of Historical Processes: From Ontology-based Modeling to Diagrammatic Reasoning  ",
        "authors": [
            {
                "given": "Ingo",
                "family": "Frank",
                "affiliation": [
                    {
                        "original_name": "Leibniz Institute of East and Southeast European Studies, Germany",
                        "normalized_name": "Leibniz Institute for East and Southeast European Studies",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/039s64n79",
                            "GRID": "grid.462368.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-09",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "modeling",
            "semantic web and linked data",
            "public and oral history",
            "simulation",
            "ontologies and knowledge representation",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Presentation Materials:    https://sites.google.com/haverford.edu/crim-project/crim-dh-utrecht   The allusiveness of musical discourse is so fundamental to the Western tradition that it is hard to imagine a work that does not in some way make reference to some other composition, type or topic. Indeed, music that refers to other music has been a constant in the European tradition of the last 1000 years, from the layered polyphony of 12th-century Notre Dame de Paris to the rampant borrowing (from himself and others) of George Frideric Handel, and from the topical allusions of film music to looped sampling heard in rap. Thanks to the advent of new technologies for encoding and addressing symbolic music scores, we can now begin to explore these complex cultures of citation with both new scope and precision.  Citations:  The Renaissance Imitation Mass (CRIM) [Freedman and Fiala, 2017] focuses on one important but neglected part of this allusive tradition: the so-called Imitation or Parody Mass of the sixteenth century, in which a composer transformed a short sacred or secular piece into a long five-movement cyclic setting of the Ordinary of the Catholic Mass:  Kyrie, Gloria, Credo, Sanctus, and Agnus Dei. The resulting works are far more than collections of quotations. The sheer scope of the transformation–in which a work that lasted perhaps five minutes was recast as a cycle lasting thirty minutes or more–required the composer to thoroughly re-think the model, adapting pre-existent melodies to fit new words, and shifting, extending, or compressing them to new musical contexts and expressive purposes. Indeed, if counterpoint is a craft of combinations (as two or more vocal independent melodies come together in a polyphonic weave), then the Imitation Mass involves the art of  recombination on a massive scale.   Musicologists have considered the intertextual relationships of these Masses from a number of vantage points. At a cultural level, for instance, they have been read in the context of debates about whether secular sounds of models might be elevated by the sacred lyrics and contexts of the Mass, or conversely whether the sacred words and purposes of the Mass were corrupted through secular sounds. But the chief challenge of measuring the genre has been dampened by two basic factors: the sheer number of possibilities for contrapuntal elaboration, and the idiosyncratic ways in which individual scholars have sought to explain and exemplify them. The CRIM Project, with its digital capacities for managing citations, claims, and counter-claims in a collaborative environment, answers both of these key challenges in ways that will transform our understanding of the repertory, and set the stage for the investigation of related corpora as well. CRIM builds upon recent developments in the digital domain for music scholarship, implementing for the first time a new kind of quotable text for music. XML encodings of symbolic music scores are the foundation of this work. Built according to the open-source  Music Encoding Initiative (MEI) standard (  https://music-encoding.org/ ), these texts provide dynamic scholarly editions that are readable by musicians and computational systems alike. Scores of related works are presented in a novel citation engine (using  Verovio, a Javascript rendering system that works in any internet browser without additional software), from which analysts can directly select any combination of notes, in any combination of staves or measures. These selections are stored as durable addresses following the Music Addressability API [Viglianti, 2016], which in turn can be used to return MEI representing the selected notes (as well as their original context) to any subsequent user (sample relationship at  https://crimproject.org/relationships/135/). Such digital citations inaugurate a new kind of durable, quotable text for musical scores that we invite others to use.  CRIM citations are also critical assertions:  statements made by particular analysts about relationships between musical patterns. For these, too, we are proposing durable ontologies of various sorts. Within the confines of the CRIM project itself we have defined various controlled vocabularies that describe the workings of Renaissance counterpoint as it migrates from one work to the next. Participants record not only a collection of notes, but also a range of metadata that detail the specific kind of patterns (the order of voice entries, their relationship in musical time and space, etc), and the particular kind of transformation that has been applied to the model as source material is compressed, prolonged or recombined in imaginative ways. The specifically musical portions of the observations, moreover, are surrounded with information about the person responsible for the assertion, and other relevant data about its motivation and status. All of these data are exposed using Open Annotation ( http://www.openannotation.org) and Linked Open Data standards ( http://linkeddata.org).  These complex, structured annotations (CRIM participants have created over hundreds of them to date) are assembled in a database and Django web application that permits users to discover related patterns in disparate works, and to deploy dozens of citations in collaborative discussions and narrative arguments about style and practice. We expect that digital publications of this sort–joining durable citations with open annotation–will have important implications for scholarly discourse about music. This in turn points towards the wider perspectives of the project: enhancing music digital edition and analytical annotation tools; improving the understanding of citation, imitation and, more generally, charting the processes by which pre-existing materials are transformed in the course of creative musical expression. We view the Digital Humanities community as the ideal forum in which to expand the reach of these technologies, and to continue the collaborative spirit of our work.  ",
        "article_title": "The Quotable Musical Text in a Digital Age: Modeling Complexity in the Renaissance and Today",
        "authors": [
            {
                "given": "Richard",
                "family": "Freedman",
                "affiliation": [
                    {
                        "original_name": "Haverford College, United States of America",
                        "normalized_name": "Haverford College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04fnrxr62",
                            "GRID": "grid.256868.7"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Fiala",
                "affiliation": [
                    {
                        "original_name": "Université de Tours, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Micah",
                "family": "Walter",
                "affiliation": [
                    {
                        "original_name": "Harvard University, United States of America",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-15",
        "keywords": [
            "linking and annotation",
            "musicology",
            "renaissance studies",
            "English",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Oral transmission of musical material plays an important role in many African-American music cultures, such as in blues and in jazz (Berliner, 1994). This does not only pertain to entire songs but also to smaller musical units which are often called licks, formulas, or patterns. Due to the importance of improvisation in jazz, there is a certain need to command a personal vocabulary of patterns, which are musical snippets of a few tones. This greatly facilitates the improvisation process, particularly in fast tempos, by reducing the cognitive load. An overall heightened level of virtuosity became common in jazz with bebop in the 1940s (DeVeaux, 1997; Frieler, 2018). In this context, characteristic “bebop lines” were invented by players like Charlie Parker and Dizzy Gillespie, amongst others. Since the musical features those long lines are rarely found in other Western musical styles, they became a token of jazz. Certain patterns and licks are building blocks of those “bebop lines” and, hence, have become important components of jazz improvisation. Therefore, they deserve closer scrutiny, lending itself to the use of computers as it is not easy to discern patterns by listening or analyzing transcriptions manually.    Pattern mining and search The international Dig That Lick project (DTL) is dedicated to investigating the usage of patterns and licks in monophonic jazz solos using search algorithms on a large database of jazz solo transcriptions. These transcriptions are created automatically using state-of-the-art melody extraction algorithms based on neural networks and advanced signal processing techniques.(Basaran et al., 2018) The transcriptions are equipped with extensive metadata based on a specifically designed semantic model. N-grams, i.e., melodic sub-sequences, are extracted from the transcriptions using pitch and interval representations and stored in a database. Similarity algorithms, which are grounded in music psychological research, are used to retrieve patterns instances for a given query and similarity threshold. Additionally, exact patterns can be extracted using regular expressions. This system allows tracing patterns and its variants across the whole database while combining it with the available metadata to make further inferences.    A case study To demonstrate the viability of this approach, we present in the following a small case study exploring a typical bebop pattern. The pattern was found with the help of the Pattern History Explorer (Frieler et al., 2018), which contains over 600 interval patterns in over 11,000 instances pre-mined from the Weimar Jazz Database   Publicly available from http://jazzomat.hfm-weimar.de  (WJD; Pfleiderer, 2017) for exploration.    Procedure The chosen interval pattern [‑1,‑2,‑1,3,3,3,‑1,‑2] (measured in semitones, cf. Frieler, 2017) can be considered a typical bebop pattern with a distinctive recognizable structure. The pattern can be found as patterns M20 and M40 in Owens work on Charlie Parker (1974). The pointwise self-information (logarithm of expected to observed frequency) of this pattern is about 11 bits, which means it occurs about 3000 times more often than it could be expected based on a 0th order Markov model, which shows its significance. In order to find variants of the pattern, it was submitted as a query to the DTL similarity search system currently working with the Weimar Jazz Database which contains 456 solo transcriptions by 78 soloists (Pfleiderer, 2017). A similarity threshold of .7 and a maximum length difference of 2 was used. This resulted in a set of 768 similar (including 12 identical) pattern instances. Next, consecutive stretches of instance locations were filtered by using maximum similarity first, length matching and left-most precedence. This filtering left 184 patterns in total. Aural control of sample instances showed, however, that patterns without the ascending seventh chord (or an inversion of it) in the center are usually not perceived to be similar to the query. After filtering these out, a final set of 100 instances was left. The pattern nuclei were classified by the seventh chord they represent, and prefixes and suffixes of the nuclei were frequency ranked. This allowed to construct unique tags of the form  nn-X-mm, where  nn ( mm) is the frequency rank of the prefix (suffix), and X is the nucleus code: D for a diminished chord [3,3,3], D’ for its first inversion [-9,3,3], H for half diminished chord [3, 3, 4,], 7 for a dominant seventh chord [4,3,3] and 7’ for its first inversion [-8,3,3], and m7 for a minor seventh chord [3,4,3].    Pattern structure and variants Out of 4 3 = 64 possible ascending seventh chords with inversions (i.e., combinations of ascending minor/major thirds and descending minor/major sixths), only six occurred in our result set as nuclei, with the original [3,3,3] being the most common with 70 instances, followed by its first inversion [-9,3,3] with 14 instances. No sixth was found on any other than the first position. Together, this is a first indication for the stability and specificity of the pattern. For the prefixes, 21 different versions could be found with the original [‑1,‑2,‑1] the far most common with 63 instances. The suffixes are more varied with 26 forms and the original [-1,‑2] the most common with 35 instances. A pattern network using Edit Distance-based similarity of all patterns can be found in Fig. 1. Here, a similarity cut-off of .8 was used and node size is chosen proportional to Freeman centrality. The original pattern (01-D-01) is in the center, as expected.    Figure 1. Similarity network the patterns with similarity cut-off of 0.8 for adjacency.  For further structural analysis, we extracted chord contexts, metrical positions, absolute pitch values and chordal diatonic pitch class information (Frieler, 2017) for the first tones of the nuclei. This showed a remarkable consistency. 55% of all nuclei start on a beat, most frequently on the third and the first beat of a 4/4 bar. The most common chordal diatonic pitch class is the third of the chord, whereas the most common chord is a C 7 chord, followed by G 7, D 7 and F 7. Generally, the dominant seventh chord was the most common chord type with 65% of all instances. From these most common traits, a prototypical version of the pattern can be constructed, which, however, cannot be found as such in our results. The closest to such a prototype is an instance of [‑1,‑2,‑1,‑9,3,3,‑1,‑2] depicted in Fig. 1. The only difference to the virtual prototype is the nucleus D’ instead of D.     Figure 2: Nearly prototypical pattern instance [-1,-2,-1,-9,3,3,-1,-2] (01-D’01), found in m. 6 of Charlie Parker's solo on “My Little Suede Shoes” (1951).    Oral transmission In Fig. 3 a timeline plot of all instances of the pattern variants found in the WJD is depicted. Striking is the number of instances by Charlie Parker, nearly exclusively with a D or D’ nucleus (cf. Fig. 1). Dizzy Gillespie is another heavy and early user, with four instances along in one solo (on “Be-Bop”, 1945). Sonny Rollins, Dexter Gordon, Sonny Stitt and Phil Woods are also fond of this pattern. Interestingly, more recent post-bop players such as Michael Brecker, Chris Potter and Wynton Marsalis have this pattern in their repertoire. However, the pattern variants are not equally popular across the main jazz styles as annotated in the WJD (χ 2(7) = 91.8  p < 0.001), as it is much more likely to be found in bebop and hard-bop solos than in any other styles. This justifies post-hoc the denomination of the pattern as a “bebop lick”. However, the earliest instance can be found with swing tenor sax player Chu Berry (in his solo on “Body Soul”, 1938).    Figure 3: Timeline plot of all instances of all pattern variants according to the recording year of the containing solo, sorted by performer on the y-axis. Labels and colours correspond to nucleus type, point size represents number of instances.     Discussion In this case study, we found several interesting results. Firstly, Charlie Parker and Dizzy Gillespie seem to have been, not unexpectedly, the main users and popularisers of this pattern and its variants, even though they themselves might have taken inspiration for it from the earlier swing players. Secondly, many other players from the bebop era, known to be influenced by Parker and Gillespie (Berliner, 1994; DeVeaux, 1991), also used the pattern quite frequently, indicating a direct transmission. Thirdly, modern post-bop players used it also quite often, which is indicative of their mastery of the bebop tradition, though it might also be a direct and deliberate reference to their bebop forebears (e.g., Michael Brecker using it over “Confirmation”, a well-known composition by Charlie Parker). Fourthly, the pattern variants nearly always appear in a specific metrical configuration and certain harmonic contexts, which indicates that metrical and harmonic aspects might be stored along with the pattern in a player’s memory. However, it can also be adapted to different harmonic context without losing its musical shape which opens further questions about pattern construction and memorisation.    Conclusion This case study demonstrates that computer-based methods are useful to address research questions at the interface of historical, cultural and psychological aspects, leading to new results which could not have been be gained without the help of digital tools. While the case study builds on the rather small sample of 456 solos contained in the WJD the much larger database of jazz improvisations which is currently under development by the DTL project will very likely corroborate the results and provide further insights.  ",
        "article_title": "Anatomy of a Lick: Structure & Variants, History & Transmission",
        "authors": [
            {
                "given": "Klaus",
                "family": "Frieler",
                "affiliation": [
                    {
                        "original_name": "Hochschule für Musik \"Franz Liszt\" Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            },
            {
                "given": "Höger",
                "family": "Frank",
                "affiliation": [
                    {
                        "original_name": "Hochschule für Musik \"Franz Liszt\" Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            },
            {
                "given": "Pfleiderer",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "Hochschule für Musik \"Franz Liszt\" Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-16",
        "keywords": [
            "corpus and text analysis",
            "multimedia",
            "musicology",
            "data mining / text mining",
            "English",
            "audio",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Normalisation can be produced with various solutions (Baron and Rayson, 2008; Bollmann et al., 2011; Pettersson et al., 2013a; Pettersson et al., 2013b; Sánchez-Martínez et al., 2013; Porta et al., 2013; Scherrer and Erjavec, 2013; Pettersson et al., 2014; Bollmann and Søgaard, 2016; Ljubešic et al., 2016; Tjong Kim Sang et al., 2017; Domingo et al., 2017), but recent research have demonstrated that neural machine translation (NMT) is the most efficient (Korchagina, 2017; Domingo and Casacuberta, 2018a). However, moving from test to production of a working tool is not an easy task, because of the amount of training data required for machine learning. This paper present a solution to create a parallel corpus and deliver an NMT-based normaliser for early modern French.  A first test corpus A first test has been made with the 1668 edition of  Andromaque of Jean Racine (Racine, 1668) and the 1624 edition of the  Lettres of Jean-Louis Guez de Balzac (Guez de Balzac, 1624).     Author Text Date Lines Tokens Characters   Corpus Guez de Balzac Correspondance 1624 1723 49,589 298,486    Racine Andromaque 1664 1756 13,884 86,612   Total    3479 63,473 385,098   This proto-corpus is deliberately heterogeneous to test our workflow. Guez’s  Correspondance is a collection of short letters in prose using a graphic system from the first half of the 17 th c. Racine’s  Andromaque is a play in verse with a graphic system from the second half of the 17 th c.  Transcriptions have been produced directly from PDF files (Fig. 1) with a model specifically designed for 17 th c. prints (Gabay, 2019). It has been trained on both low-quality (72 DPI) and high-quality (400 DPI) images of books using various fonts and the extracted text preserves abbreviations ( ẽ…) and special characters ( ſ…) but not ligatures ( ﬁ…).     Fig. 1 Racine,  Andromaque, Paris, BNF, RES-YF-3206, p. 2    Pre-processing Following previous successful experiments (Bollmann, 2012), a rule-based system for pre-orthographic French has been developed (Riguet, 2019). It is based on two lexical resources: Morphalou, an open lexical database of inflected forms of contemporary French (Romary et al., 2004), and LGeRM, an open morphological lexicon for middle French (Souvay and Pierrel, 2009) now covering also 17 th c. French (Diwersy et al., 2017). Based on these two databases, the normaliser applies transformations on each token, before a manual correction of the result.  Normalisation consists of aligning 17 th c. graphic systems (source) to 21 st c. orthography (target)    Source Target    Sur tout ie redout ois cette Mélancolie   Surtout je redout ais cette Mélancolie    Où j’a y v eu ſi long -temps v oſtre  Ame ense uelie.  Où j’a i v u si longtemps v otre  Âme ensevelie.     Ie craign ois que le Ciel, par  vn cruel  ſecours,   Je craign ais que le Ciel, par  un cruel  secours,      First results with an NMT-based normaliser We have decided to use NMTPYTORCH (Caglayan et al., 2017). The baseline model is composed of a 2-layer bi-directional GRU (Cho et al., 2014) encoder and a 2-layer conditional GRU (Sennrich et al., 2017) decoder with MLP attention (Bahdanau et al., 2015). The encoder and the decoder both have 256 hidden units and their initial hidden state is initialised to 0. The embedding dimensionality is also set to 256. Two versions of the system have been trained. The first one is a word level system and the second one uses the byte pair encoding (BPE) (Sennrich et al., 2015) which operates at the subword level. The corpus has been divided into two parts: 90% of the lines have been used for training and 10% for testing.    Lines Tokens Characters   Train 3,133 5,6825 348,098   Test 346 5,959 37,000   Total 3,479 62,784 385,098   Five trainings have been made with different initialisations on the two different models: words and subwords ( i.e. BPE units). Accuracy of the result is calculated with BLEU scores (Papineni et al., 2002).    Model Average BLEU Best BLEU   Words 79.27 82.960   BPE 75.79 77.070   These BLEU scores still have to be used with extreme care considering the limited size of our corpus. They are however promising enough to engage in the production of a large-scale corpus for a NMT-based normaliser.   Future developments To be as universal as possible, our training data must reflect all the lexical and graphic variety of 17 th c. French. We are therefore engaging in the construction of a representative corpus of early modern French, including excerpts of literary (plays, novel, poems…) and non-literary texts (theology, medicine, law, science…), in verse and in prose, spread diachronically across the century, and taken from original editions, reprints or illegal prints. Along this compilation phase, the OCR model and the rule-based normalising solution will be regularly improved to increase their efficiency before a final open source release.  The final corpus, expanded with back translation (Domingo and Casacuberta, 2018b), will be used for the training of an NMT-based solution. On top of words and subwords, character-level NMT will also be tested to provide the most efficient tool. A special model, trained to normalise the results of the rule-based system rather than the raw OCRised text will be tested, to evaluate the efficiency of a hybrid system using both technologies.  ",
        "article_title": " A Workflow For On The Fly Normalisation Of 17th c. French  ",
        "authors": [
            {
                "given": "Simon",
                "family": "Gabay",
                "affiliation": [
                    {
                        "original_name": "Université de Neuchâtel, Switzerland",
                        "normalized_name": "University of Neuchâtel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/00vasag41",
                            "GRID": "grid.10711.36"
                        }
                    }
                ]
            },
            {
                "given": "Marine",
                "family": "Riguet",
                "affiliation": [
                    {
                        "original_name": "Université Paris-Sorbonne/Labex Obvil",
                        "normalized_name": "University of Paris-Sud",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/028rypz17",
                            "GRID": "grid.5842.b"
                        }
                    }
                ]
            },
            {
                "given": "Loïc",
                "family": "Barrault",
                "affiliation": [
                    {
                        "original_name": "Le Mans Université",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-18",
        "keywords": [
            "corpus and text analysis",
            "machine translation",
            "translation studies",
            "English",
            "philology",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " It is usually understood that literary authors have style, and numerous papers have been published about the usefulness of digital approaches and techniques for identifying stylistic specificities of a writer, for confronting styles, for attributing texts, etc. (see Holmes, 1994; Brunet, 2004; Herman et al., 2004). However, another traditional type of stylistic analysis in literary studies has been less operationalized in a digital paradigm, aimed at observing how the characters speak (see however Brynes 2010 and 2012). This paper contributes to the testing of digital tools for such an approach; in other words, it tries to answer, with digital tools, whether literary characters have a style, or if the signal of the author that creates them is prevalent over all other kinds of linguistic specificities.  In order to answer this question, the paper focuses on theatrical texts from the 17 th and 18 th French centuries. To the contrary of what happens in narrative texts, character’s discourses have clear boundaries in plays, and can be easily extracted from an XML/TEI encoded text. Also, characters in plays have been somewhat less studied with digital tools than characters in novels (Jockers and Kiriloff, 2016; Jockers 2013).  A sample of 8 comedies staged approximately from 1630 to 1740 has been put together; the sample tries to balance well known play writers, such as Molière, Corneille Destouches and Marivaux, with more obscure authors (Dancourt, Regnard, Boissy, Boursault). However, all the plays are “grandes comédies” in 5 acts and in verses, with comparable lengths and quite similar numbers of characters. A total amount of 82 discourses has been extracted using an XQuery under BaseX. First, the plays as a whole (but without the stage directions or any kind of meta-discourse) have been submitted to a PCA using the stylometric library under R written by (Eder and al., 2016). As expected, differences between author styles are well underlined by their distribution on the graph, with Molière in the middle and Regnard closer to him than to the authors from the 18 th century.     This first analysis has been conducted only to confirm that the tool works on the type of texts it has been fed to, and yields sensible results. Second, character’s discourses have been submitted to the same kind of analysis. After further adjustments, such as the exclusion from the corpus of too short roles that were skewing the general distribution on the graph, and the testing of various algorithms (covariance, correlation or cluster analysis? Classic Delta, Canberra or Eder’s Delta?), the following representation has been obtained:    As it can be observed, characters do not group by “origin” (i. e., more often than not characters from a play do not appear together), nor do they display a clear historical split - to the difference to what was happening with the authors. This tends to confirm that characters do have a style, whose parameters are to be further identified and tested. Moreover, when merging in a same txt file discourses of feminine, respectively masculine, characters from the same play/ author, significant differences appear in certain cases, with Molière’s Agnès and other feminine characters being the most intriguing case:    In a third stage, an analysis of characters’ speeches is conducted with TXM (see Heiden, 2010), so as to delineate the stylistic differences pointed to by the PCA and to attempt an explanation. Verbs do not seem to be useful discriminators, even when the texts are lemmatized. The calculus of specificities shows that personnel pronouns, names and possessives are the most discriminant features. To these, one may add the adjectives, which appear more frequently as a characteristic of male speeches according to the table of preferred words built with the “oppose” function under stylo library in R. While the importance of “pronouns of dialogue” for characterizing plays has already been underlined (Muller, 1979), the other features are a bit more surprising, since one would have expected, for instance, verbs to play a more prominent role, related to the actantial position of the characters. Also, it is not clear why male characters would use more adjectives than their feminine counterparts; sensibility and a trend towards pathos are more often evoked in relation to the second ones. After further inquiries with a new set of discourses, from other plays by the same authors, but also from other authors, so as to confirm the above mentioned phenomena, the paper will try to propose some explanations based on a close analysis of the contexts. ",
        "article_title": "Stylometric Analyses of Character Speeches in French Plays",
        "authors": [
            {
                "given": "Ioana",
                "family": "Galleron",
                "affiliation": [
                    {
                        "original_name": "U. Sorbonne Nouvelle, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "corpus and text analysis",
            "french studies",
            "stylistics and stylometry",
            "content analysis",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Background It has been almost 25 years since Gibbons et. al (Gibbons, 1994). introduced the idea of ‘Mode 2’ research, and Boyer asked us to reconsider our idea of scholarship (Boyer, 1990) as encompassing not only discovery, but teaching, application and integration as well.  These two contemporaneous works encouraged higher education to re-evaluate the way it envisioned the skills profile of a researcher, and imagine ways to meet these skills needs.  Yet, in the intervening decades, neither Boyer’s ideas nor those of Gibbons and his co-authors have had much impact on the arts and humanities, where the most common mode of scholarship is still single authored, basic research, and the organisation of higher education is still highly disciplinary.  One place where these new modes of research are taking hold in the arts and humanities, however, is in the emergence of the uniquely European class of large-scale, transnational collaborative research infrastructures constructed as ERICs, or European Research Infrastructure Consortia. Because the ERICs’ primary mission is to provide a higher baseline for all contributing researchers, both transdisciplinary application and integration find a natural home within their activities, regardless of the communities of practice they serve.  These modes of research do not replace the traditions of discipline-based investigation, but complement them, bringing to the fore new skill sets that are useful for the economy   See, for example, the Institute for the Future’s ‘Future Work Skills’ proposal, which highlights transdisciplinarity, collaboration and new media literacy among their growth areas. ( http://www.iftf.org/futureworkskills/, retrieved 26th Nov 2018)   and for the organisation of research itself   See, for example, the emphasis on skills in the EU report on the long term sustainability of RIs, Sustainable European Research Infrastructures, a Call for Action,  https://publications.europa.eu/en/publication-detail/-/publication/16ab984e-b543-11e7-837e-01aa75ed71a1/language-en (retrieved 26th Nov 2018)  .  In spite of these developments, the view of Higher Education Institutions (HEIs) as the primary delivery mechanisms for training and education is understandable.  Certification and validation of skills acquired requires its own infrastructure, one of policies and processes, and one which universities have developed over many centuries.  Yet the complementarity between these two modes of knowledge creation presents a clear opportunity for convergence and creation of added value for both kinds of institutions, not to mention students.  For this reason, the PARTHENOS project, a cluster project of Digital Research Infrastructures (RIs) and large projects within the Humanities and Social Sciences, based one of its areas of shared development on an investigation of, and pilot programme in, the shared transmission of knowledge between RIs and HEIs.   Scoping the Relationship between Higher Education Institutions and Research Infrastructures In order to ensure the approach taken matched the requirements of the variety of courses already in existence across HEI programmes throughout Europe, the PARTHENOS team commenced their work with a survey of Digital Humanities (DH) programmes; first through desk research and a survey, and then through two ‘course providers’ workshops’, which discussed DH training provision, and how RIs were included (if at all) within the course syllabus. Results from the survey and desk research, complemented by the course providers’ workshops, gave an overview of the level of granularity many of the courses took to specific RI training.  The majority of responses to the survey came from course providers (approx 80%), with around 20% of responses provided by students or recent graduates of DH courses. Initial indications showed that course providers are keen for more reusable teaching materials around RIs to enhance their existing training in the subject, and that while some did already provide some manner of experiential learning such as an internship or a more hands-on practical element to their course, the majority mostly included RIs either as something that is integrated throughout the course (particularly in the case of those course providers who were already very engaged with RIs) or as a one-off lecture.  The responses during the course providers’ workshop echoed this, revealing similar approaches: either by incorporating information about RIs regularly throughout the course training programme or by tackling it in one lecture. Where training around RIs was not included, it was either through lack of confidence in the subject or lack of knowledge of how to approach on the part of the course provider. Provision of reusable training materials, such as those created by PARTHENOS, therefore attempts to address the requirements of DH course providers, certainly across Europe.   Designing an optimised course  The PARTHENOS project applied the knowledge gathered during this exploratory phase to the design of a module for students undertaking a Master’s programme at King’s College London.  The module, developed by Dr. Kristen Schuster, incorporated materials specifically around the issues of research data management, an area in which both HEI-based research programmes and research infrastructure-based training have developed significant complementary expertise.  The module was 10 weeks in duration and used a two-pronged approach, with some classroom-based training materials and lectures being used in the first 5 weeks of the course, and a practical element forming the majority of the training in the second 5 weeks of the course. The PARTHENOS training materials in particular used throughout the module included video lectures with accompanying downloadable presentation slides (made available on SlideShare) and shorter videos around basic concepts in Research Infrastructures, and links to content in sections of the “Manage, Improve and Open Up your Research Data” PARTHENOS module, which was given as required reading in preparation for lectures in class.  The students participating in the course were mostly non-native English speakers, and many of them came from outside Europe.  Based on the feedback we received from course providers and students, we used two questions to develop lecture themes and seminar activities: what is data, and what do we mean by research infrastructures?  Asking ‘what is data?’ helped to contextualise the functional requirements for creating, describing and preserving structured information. Over the course of the module, students explored standards and policies that inform the development of best practice guidelines for handling structured information. Students gained introductory knowledge to metadata, database management and protocols for data exchange.  Understanding theories and practices for defining and managing data segued to descriptions and analyses of RIs. Asking ‘what do we mean by research infrastructures?’, allowed the students to explore different requirements for developing policies, protocols and technical specifications for collaborative endeavours to create, use and re-use research data.  Readings, discussions and activities provided students with opportunities to explore technologies, workflows and documentation that support research data management from a variety of perspectives. This combination of activities required students to practice the skills discussed during lectures and seminars and build a professional vocabulary for discussing research data management as a practice that supports the curation, preservation, use and re-use of digital assets. Overall, gaining a foundational knowledge of data management through this module prepared students to manage digital assets that support cultural heritage work and research.   The Course Provider’s Experience Introducing broad themes through questions like ‘what is data?’, ‘what are the social and technical roles of RIs’ and ‘how can research data management improve the impact and function of RIs in the humanities’ engaged students in discussions and debates, which enabled students to work through their own areas of interest and come to conclusions supported by a body of research and expertise.  Learning was supported through creative problem solving in a two-part group project. By working with a teaching fellow and external partner at the Royal United Service Institute (RUSI) it was possible for students to develop and revise research data management plans and present wireframes for data portals and guidelines for institutional repository participation in RIs. In general, students progressed from confused to curious to engaged over the course of five weeks, and began their group projects with a great deal of confidence .     Student Evaluation of the course materials To evaluate the usefulness of PARTHENOS training materials within the course, we also asked the students to complete two surveys: one at the end of the theoretical classroom-based section in weeks 1-5; and a second at the end of the practical section in weeks 6-10.  We were interested in how the students found the materials from a classroom learning perspective, and how the materials might be received and referred to in a task-based setting. Preliminary results point towards a dual-format approach of accessible videos used in conjunction with written content as the ideal approach, especially given the non-native English speaking cohort within the student group.  While the more ‘mobile-friendly’ materials such as videos maybe useful for those who are comfortable with spoken English, the additional support of written content to provide context and allow for longer rumination is of benefit to those who are less comfortable with spoken English and all its dialectal varieties currently presented within the PARTHENOS training materials.    Outline and proposal for a Long Paper This long paper will set a context for training around Research Infrastructures, and their inclusion in formal Higher Education courses.  Using PARTHENOS materials as a case study, we shall discuss the rationale for and process of designing the course to incorporate training materials around RIs.  It will then discuss in more detail the evaluation process we have undertaken, and look to lessons learned and recommendations for the incorporation of training materials from Digital Research Infrastructures.    ",
        "article_title": "Developing and Integrating Training Materials on Research Infrastructures into Higher Education course design: The PARTHENOS Experience",
        "authors": [
            {
                "given": "Jennifer",
                "family": "Edmond",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin, Ireland",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Vicky",
                "family": "Garnett",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin, Ireland",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Helen",
                "family": "Goulis",
                "affiliation": [
                    {
                        "original_name": "Academy of Athens",
                        "normalized_name": "Academy of Athens",
                        "country": "Greece",
                        "identifiers": {
                            "ror": "https://ror.org/00qsdn986",
                            "GRID": "grid.417593.d"
                        }
                    }
                ]
            },
            {
                "given": "Kristen",
                "family": "Schuster",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Ulrike",
                "family": "Wuttke",
                "affiliation": [
                    {
                        "original_name": "Fachhochschule Potsdam",
                        "normalized_name": "Fachhochschule Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/012m9bp23",
                            "GRID": "grid.461741.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "pedagogy",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Libraries are sources of large-scale data: both in terms of their collections and the information they collate on their spaces, users, and systems. These data provide opportunities to explore technical, methodological, and ethical questions from the valuable interdisciplinary perspective of Data Science and the Digital Humanities. In light of this, we will explore our analysis of library datasets using  Subjectify   The code and documentation for  Subjectify is available on Github at  https://github.com/mbennett-uoe/librarytools.   , an automatic classification matching tool developed to assist analysis of UK Non-Print Legal Deposit (NPLD) collections. NPLD regulations were introduced to the UK in 2013 to support legal deposit libraries to collect electronic publications  (2013). Access restrictions mean that readers may only use these materials on fixed terminals within the physical walls of the six legal deposit libraries  (see British Library, 2014 for details). The resultant web logs are therefore unambiguous sources of NPLD collection usage within UK legal deposit libraries.   Our study is part of an established tradition of user studies in the digital humanities. To date, these have focused on user behaviour with digital resources  (Warwick et al., 2008; Ross and Terras, 2011; Sinn and Soares, 2014). Web log analysis has been used successfully in this context for over twenty years  (Almind and Ingwersen, 1997; Nicholas et al., 2005; Gooding, 2016). These studies adopt methodological approaches and topics of study that contribute directly to our understanding of information sources in the digital humanities. However, there have been fewer studies that address critical humanistic perspectives to inform approaches to the data itself. This paper addresses that gap by describing our research into the users of NPLD materials in the United Kingdom, and the implications of automatic classification matching for library dataset analysis. It will address the following questions: what insights into users of digital library collections can be derived from automatic classification matching? What limitations are introduced by the use of existing classification schemes? And, in light of ongoing debates on responsible data curation in DH  (Weingart, 2014; Brown et al., 2016), how might DH and LIS scholars collaborate to inform ethical analysis of large-scale library datasets?    Methodology Our analysis follows Bates’ observation that scholarly communication practices function differently across domains, and that “these many differences  do make a difference”  (1998: 1200) to information access and use: as such, we should be able to identify differences in behaviour by studying the subjects requested by users. To this end we were provided with two datasets comprising title-level NPLD access logs from the reading rooms of the six UK legal deposit libraries   The six libraries are the British Library, National Library of Scotland, National Library of Wales, Bodleian Libraries, Cambridge University Library, and Trinity College Dublin Library. . The anonymous logs contained only bibliographic records of NPLD materials accessed by users, excluding both identifiable information about users and interactions with discovery systems and other materials. The first dataset comprised metadata for all eBook requests (total 91,809) from 31 st July 2015 to 31 st March 2017. The second dataset comprised metadata for all eJournal article requests (total 36,506) over the same period. Each dataset contained the following metadata: date and time of access request; originating legal deposit library; title of book or article; journal title (where applicable); publisher; and ISBN or ISSN. Each dataset was provided as a CSV file, then cleaned by the research team in OpenRefine to address metadata errors.   Although our dataset contained no identifiable information about users, it may still be possible to infer information about users from the works they consult. We therefore decided to abstract our data and undertake a macroanalysis of user behaviour. To achieve this, we created a small Python-based tool called  Subjectify. This tool uses the OCLC Classify2 API service   http://classify.oclc.org/classify2/api_docs/index.html   to automatically obtain Dewey Decimal (DDC) and Library of Congress (LCC) classmarks from CSV files using key data fields such as title, author, and ISBN. It additionally provides for different options to locate relevant fields to allow input from different data sources.  Subjectify found a matching classmark for 76.42% for eBooks, and 55.53% for eJournals. This was partly due to missing key data fields in records for eJournals, and partly because many records did not have a corresponding classmark: time-consuming manual classification samples via OCLC Classify2 achieved only slightly higher accuracy rates. We discarded unclassified records used the remaining records to represent patterns of usage by DDC subject. Due to the large number of repeat requests due to system timeouts, we split the remaining records into unique titles (each title counted once regardless of number of requests), and total results (including repeat requests). Our results show that findings were not unduly influenced by repeat requests.     Findings The following charts show the most commonly accessed subject, by DDC, for titles viewed by users of eBook and eJournal materials. We found that usage by DDC differs distinctly from the spread of classmarks across, for instance, the BL’s entire collections   The sub-subroutine blog has produced a fascinating visualisation of the BL’s collections, which is worthy of comparison  (sub-subroutine, 2015).  :        Social Sciences texts were notably among the most commonly accessed titles for both formats. The most common subject for eJournals was Technology, whereas for eBooks both Social Sciences and Literature subjects were more frequently accessed. Our findings reflect differing information behaviour across domains: books, for instance, remain a vital source for the Arts and Humanities  (Stone, 1982; Palmer and Cragin, 2008) whereas technology and science subjects rely on journals  (Talja and Maula, 2003), which tend to provide faster publication of new research. Indeed, Stone’s flagship early study noted that “retrospective coverage may be more important to the humanists than having access to current material”  (1982: 296). The Social Sciences, on the other hand, are shown by our findings to be more hybrid in their reading patterns.    Analysing individual subject categories can derive a better sense of the difference between institutions. The following charts show usage of books in the DDC600-699 classmark (Technology), for the Bodleian Libraries, British Library and Cambridge University Library.        The British Library receives proportionally fewer requests for NPLD medical and health materials. Our interviews with staff at the Bodleian and Cambridge University libraries showed that local medical school staff were a key user group, so we can trace a direct correlation between the user communities of these libraries, and the subjects of the NPLD material used.   Finally, the following table demonstrates usage of 800-899 (Literature) sources to underline a key problem with DDC for automatic classification: Library classification is a subjective process undertaken by humans within the biased frameworks provided by existing classification schemes  (Mai, 2010). Here, for instance, DDC provides distinct categories for English, American, and classical European schools of literature, while lumping the rest of the world under “other literatures” (800-899). This bias emerges from the nineteenth century North American perspective embedded within DDC  (Kua, 2008). By relying on automatic matching, we inevitably embed problematic perspectives into our data: while our case study uses UK legal deposit collections, which comprise works represented strongly by DDC, the applicability of this method for library datasets in other parts of the world is questionable. Each time we zoom in with the macroscope, the bias of our chosen classification scheme becomes increasingly evident in the resultant data structures – yet in order to report on literature from without the established canon, we have to do precisely this. The use of established classification schemes is therefore both a methodological and epistemological problem, and future work will be needed to refine our approach.     Conclusion Our results demonstrate that  Subjectify was successful in allowing us to analyse user behaviour at scale. It contributes to macroscopic analysis of library data in two ways: first, it allows us to report on bibliographic library users while maintaining privacy through data abstraction; and second, this abstraction allows us to identify patterns of user behaviour with NPLD materials. We believe this approach would work for subject-based analysis of similar collections of bibliographic data, and that it does so in a way that closely reflects how collections are represented in libraries. However, we are also aware that automated classification introduces the biases of those classification schemes into our own data  (Adler, 2017). This is an unfortunate side effect of the growing scale of library data. Weingart (2014) notes that the role of the humanities is to tie the very distant to the very close, in order to become ethical stewards of our data. It is therefore essential, when viewing library datasets from a humanistic perspective, to consider the ethics of data representation in our own work. Our priority for future work is to consider how a fruitful conversation between DH and Information Science might develop more nuanced approaches to representing  (in the sense meant by Unsworth, 2000) of library data. This should include further consideration of the consequences of how bias in library classification schemes affects microanalytic approaches to bibliographic datasets.   ",
        "article_title": "Subjectifying Library Users to the Macroscope Using Automatic Classification Matching",
        "authors": [
            {
                "given": "Paul Matthew",
                "family": "Gooding",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow, United Kingdom",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Linda",
                "family": "Berube",
                "affiliation": [
                    {
                        "original_name": "University of East Anglia, United Kingdom",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            },
            {
                "given": "Mike",
                "family": "Bennett",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Richard",
                "family": "Hadden",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "digital humanities (history",
            "libraries",
            "museums",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The main goal of the   correspSearch web service    https://www.correspsearch.net , developed and hosted by Berlin Brandenburg Academy of Sciences and Humanities, is the aggregation of open and freely accessible metadata from printed and digital editions of correspondences.   For a general overview see Dumont (2016).  Over the last four years we have successfully aggregated metadata for about 52.000 letters. Most of the data is obtained from external contributions ranging from a wide variety of scholarly editions and institutions. With this recap we aim to infer successful recipes and practices for the decentralized aggregation of domain specific open metadata. Furthermore we will show the possibilities which arise from the aggregation of such metadata on a bigger scale and discuss ways to manage as well as explore the complex realities of our data.  Gathering a large quantity of suitable metadata about published letters is the precondition and one of the basic functions of the web service  correspSearch. The format used in a data aggregation project such as ours has to fulfill a wide array of requirements, ranging from interoperability with existing standards to ease of use and a straightforward creation process. The TEI Correspondence SIG   https://www.tei-c.org/activities/sig/correspondence/  developed a format for the purpose of simplifying correspondence metadata and thus offering a way for a standardized exchange of data, the Correspondence Metadata Interchange Format (CMIF)   https://correspsearch.net/index.xql?id=participate_cmi-format&l=de . The flat hierarchy and simple node structure of CMIF offers an easy way to process metadata not only in a machine readable way but also in a way that does not require much prior knowledge to get in touch with (see also Stadler et al., 2016). Given the specific nature of correspondence metadata, the focus of CMIF lies on people, places and dates. The format also encourages the extensive usage of authority control data. By relying on metadata as the core of the exchange format, it is not only of use for editions that are already present in a digital form, but can also be employed to print editions (see Stadler, 2014).  Since the aggregated data is stored decentralized with each edition, participating editions and data providers preserve full ownership and control over their data. Based on the TEI-XML standard, CMIF follows strong guidelines and utilizes the strengths of XML to prevent faulty or ambiguous data while taking into account the heterogeneity of metadata. Furthermore, CMIF requires the usage of authority files for names and places in order to account for their ambiguity and linguistic heterogeneity (see Stadler, 2012). All in all, CMIF successfully provides a model for describing correspondences in an easy and rather flat hierarchical way, while relying on a strict ruleset in favor of standardized and machine-processable data. As the metadata aggregated by our service is licensed under CC BY, it remains open and thus stands for an Open Access-based approach to correspondence metadata aggregation. Open Data provides an easier ground on which research can be conducted without having to take care of licensing beforehand and with a much larger data pool. The analysis of a specific network of letters, as for example in  csLink, can only benefit from this approach. In addition and in accordance to the licensing for the aggregated data, all software developed by  correspSearch is published as Free Software.  The   CMIF Creator    https://correspsearch.net/creator/index.xql, https://github.com/correspSearch/CMIF-Creator  that was released in its second version in 2018 pursues our approach to an easy handling of CMIF-based data. It enables the creation of correspondence metadata without any prerequisite knowledge about XML/TEI or the CMI format. With the  CMIF Creator we offer a clean graphical user interface to enter available metadata and transform the entered data to valid XML. Lowering the barrier of generating and contributing data has been a key factor for successful and lively external data contributions in the last years. As the  CMIF Creator is implemented as a browser-based application, all entered and processed data is saved locally and thus stays within the control of the user at any time. As CMIF heavily relies in authority control data, authority files data for names and places can be acquired directly from GND   https://www.dnb.de/  and GeoNames   https://www.geonames.org/  via the lobid   https://lobid.org/gnd/api  and GeoNames APIs (see Tasovac et al., 2016). The  CMIF Creator also offers a validation service as well as the option to locally save drafts in JSON format so that work can be continued at any time. The final CMIF files can then be provided for aggregation into the  correspSearch database. The benefits of using the  CMIF Creator are obvious when it comes to the actual experiences we made: Besides the low barriers for data entry – it does not require any prior knowledge and experience of TEI-XML – the average time for a student assistant to process and enter the metadata of a single letter out of a printed letter edition is approximately 30 to 60 seconds, depending on the necessity to further disambiguate authority control data. Thus, large quantities of letters can be processed in a reasonable amount of time. The output format is standardized and does not deviate from TEI specifications, which reduces the incidence of errors in the final XML.  Another potential of the open data approach with a rather analytical purpose is followed in our development of the application   csLink    https://github.com/correspSearch/csLink .  csLink is a widget for websites that can be implemented and included in existing digital editions of letters. It establishes a “network of letters”, displaying the correspondence network of a certain person and reaching beyond the scope of a single edition of letters. Customized by optional parameters given,  csLink provides a list of other letters from the same network of letters, as well as a list of persons, who are part of the wider correspondence network. By relying on CC BY licensed metadata the widget is available for anyone interested in such a visual representation of the correspondence network belonging to a person. Being able to acquire a visual impression of different letter networks and the corresponding persons offers immediate epistemological gains in the study of complex network relationships. Utilizing the aggregated metadata enables  csLink to situate letters in a single edition within a wider context of correspondence and communication. Examples for applications of  csLink are the digital editions   humboldt digital    https://edition-humboldt.de  , as well as   Weber Gesamtausgabe    https://weber-gesamtausgabe.de/de/Index . We are currently developing additional ways to visualize the aggregated data and to make it more accessible for analysis and valuable for academic purposes. This necessarily includes a critical discussions of the interpretational character of visualisations, as for example in which ways a network visualisation already is making statements based on the composition of nodes on a canvas (see Dunne, 2013; Biehl et al., 2015; and Szafir, 2018).  In opposition to closed data services, the open data approach of CMIF and its potential in enabling any edition (be it printed or digital) to provide their very own correspondence metadata is extremely beneficial to mapping wider networks of communication. Since any letter network to be explored (for example in  csLink) only shows the information that is available as data already entered into  correspSearch from letters that are already edited and in some way published, a larger base of people actually committing reliable data substantially improves the database and reliability of epistemological gains from this data (see Grüntgens/Schrade, 2016). The development and usage of the  CMIF Creator has proven very valuable for increasing the amount of aggregated metadata. Further addition of data and increasing connections between the letters in our data form a kind of crowd based validated open metadata that does not rely on a single contributor or institution. It is thus not only beneficial for network analysis in a strict analytical sense but also contributes to and implements an agenda to further establish open data principles in the digital humanities.  As a leading provider for the decentralized aggregation of metadata from letter editions with the purpose to facilitate research on letter editions and correspondence networks on the basis of a standardised and open XML-format,  correspSeach, together with  CMIF and  correspDesc, were awarded with the Rahtz Price for TEI Ingenuity 2018.  ",
        "article_title": "Four Years of correspSearch – Challenges, Potentials and Lessons of Open Data Aggregation",
        "authors": [
            {
                "given": "Stefan",
                "family": "Dumont",
                "affiliation": [
                    {
                        "original_name": "Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                        "normalized_name": "Berlin-Brandenburg Academy of Sciences and Humanities",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05jgq9443",
                            "GRID": "grid.420264.6"
                        }
                    }
                ]
            },
            {
                "given": "Sascha",
                "family": "Grabsch",
                "affiliation": [
                    {
                        "original_name": "Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                        "normalized_name": "Berlin-Brandenburg Academy of Sciences and Humanities",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05jgq9443",
                            "GRID": "grid.420264.6"
                        }
                    }
                ]
            },
            {
                "given": "Jonas",
                "family": "Müller-Laackman",
                "affiliation": [
                    {
                        "original_name": "Berlin-Brandenburg Academy of Sciences and Humanities, Germany",
                        "normalized_name": "Berlin-Brandenburg Academy of Sciences and Humanities",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05jgq9443",
                            "GRID": "grid.420264.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "copyright",
            "open access",
            "digital humanities (history",
            "open content and open science",
            "theory and methodology)",
            "English",
            "semantic web and linked data",
            "philology",
            "licensing",
            "scholarly publishing",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Network interpretation is a widespread practice in the digital humanities, and its exercise is surprisingly flexible. While there is now a wide variety of uses in different fields from social network analysis (Ables et al., 2017) to the study of document circulation metadata (Grandjean, 2016) or literature and linguistic data (Maryl and Elder, 2017), many projects highlight the difficulty of bringing graph theory and their discipline into dialogue. Fortunately, the development of accessible software (Bastian et al., 2009), followed by new interfaces (Rosa Pérez et al., 2018; Wieneke et al., 2016), sometimes with an educational dimension (Beaulieu, 2017; Xanthos et al., 2016), has been accompanied in recent years by a critical reflection on our practices (Weingart, 2011; Kaufman et al., 2017), particularly with regard to visualisation. Yet, it often focuses on technical aspects. In this paper, we propose to shift this emphasis and address the question of the researcher’s interpretative journey from visualisation to metrics resulting from the network structure. Often addressed in relation to graphical representation, when it is not used only as an illustration, the subjectivity of translation is all the more important when it comes to interpreting structural metrics. But these two things are closely related. To separate metrics from visualisation would be to forget that two historical examples of network representation, Euler (1736) and Moreno (1934), are not limited to a graphic reading (the term “network” itself would only appear in 1954 in Barnes’ work). In the first case, the demonstration was based on a degree centrality measurement whereas in the second case the author made the difference between “stars” and “unchosen” individuals while qualifying the edges as inbound and outbound relationships. This is why this paper propose to examine the practice of visual reading and metrics-based analysis in a correspondence table that clarifies the subjectivity of the translation while presenting possible and generic interpretation scenarios.  Visual approach: making the global structure readable The way we read networks has changed over time. Historically the question of network readability was asked in terms of aesthetic criteria. In the word of Jacob Moreno “the fewer the number of lines crossing, the better the sociogram”. Even in the nineties, when giving birth to the modern layout algorithm, Früchterman and Reingold (1991) aimed at “minimizing edge crossings” and “reflecting inherent symmetry”. However these criteria do not seem so crucial to practices observed nowadays in digital humanities (and beyond).     Fig. 1 Different contexts for network visualisation in DH2016, DH2017 and DH2018 abstracts.  Looking at recent papers in digital humanities, networks appear to have a wide range of usages. Their visualisations are either self-sufficient [fig. 1.a.] (Algee-Hewitt, 2018; Pino-Diaz and Fiormonte, 2018; Verhoeven et al., 2018; Marraccini, 2017), an optional help to understanding [fig 1.b.] (Colavizza et al., 2016) or strongly connected to the text. Some authors use them to highlight the position of a specific node [fig. 1.c.] (Moretti et al., 2016), to compare layouts [fig. 1.d.] (Sozinova, 2016) or the layout of the same graph in time [fig. 1.e.] (Wright, 2016). They may aim at visualising communities [fig. 1.f.] (Rybicki et al., 2018; Torres-Yepez and Zreik, 2018), mapping a general structure [fig. 1.g.] (Gao et al., 2017), tracking density patterns [fig. 1.h.] (Gao et al., 2018) or monitoring algorithms like modularity clustering [fig. 1.i.] (Choinski and Rybicki, 2017). These usages reveal a different perspective in network visualisation where we expect the visual to translate underlying relational structures. It helps to give different names to these two different approaches. We call  diagrammatic the perspective where the network is a diagram that we read by following paths. We do not want the edges to cross and we use aesthetic criteria to bring clarity. It was Moreno’s perspective, and is still relevant to small networks and local exploration. Then we call  topological the perspective where the network is a structure that we read by detecting patterns. We expect the visualisation to help us retrieve structural features like clustering or centralities. It is a common practice in digital humanities, more holistic and relevant to larger networks. Aside or in complement, classic data visualisation is also employed to visualise non-relational structures (node attributes, etc.).      Fig. 2 Various layouts do not follow a force-driven algorithm to make non-relational dimensions of the data explicit.  In the topological perspective, a standard procedure is to assign nodes a position using a force-driven algorithm. This family of algorithms is known for displaying clusters that match a widely used measure of community detection, modularity clustering (Noack, 2009). Its translation remains however difficult to interpret locally, as we can never give a simple explanation for a node’s position. Classic data visualisation also translates non-relational structures, by itself or combined with a relational perspective. Different structural features may require different visualisations: the examples of fig. 2 shows curated visualisations using categories [fig. 2.a boys and girls, in the famous example of (Moreno, 1934)], temporality [fig. 2.b] (Jänicke and Focht, 2017) or hierarchy [fig. 2.c] (Grandjean, 2017). Though very different from force-driven placement, they display better certain structural features.   Objectifying the structure with metrics Often opposed to visual interpretation, of which they would be a more objective and reliable representation, centrality measures have a history that goes back to more than half a century and shows that they are not immutable and require constant adaptation to usage Moreover, Freeman (1979) insists on the fact that the notion of “centrality” is the result of several intuitive conceptions. To remind that these metrics are based on “intuition” means to recognize that they have no meaning in themselves and that their interpretation must be rediscussed - and therefore translated - according to the context. This paper thus proposes to list and evaluate to which extent these metrics are applicable to humanities and social science data and can, if necessary, be “translated” into this language to complement visual analyses.     Fig. 3 Three levels of interpretation that can be articulated: visual analysis (examples top left), use of global metrics (examples bottom right) and use of local metrics (highlighted nodes).   Global properties Statistical analysis allows for comparing networks across multiple dimensions at once (Tank and Chen, 2017). For instance, comparing the  number of nodes and edges of different graphs of the same type (Trilcke et al., 2016) can be a ranking tool that is directly translatable into natural language. In addition to that, studies suggest that  density (the number of edges in relation to the number of nodes) is relevant to analyse character networks, especially when compared within a homogeneous collection (Evalyn and Gauch, 2018; Grandjean, 2015). This is also the case when measuring  average path length (Trilcke et al., 2016).    Local properties With regard to local measures, the  degree (number of neighbouring nodes) is the simplest  centrality, and the only one systematically used between the late 1950s and early 1970s, before the development of more diversified metrics (Freeman, 1979). Its simplicity allows for a transparent translation: in a literary network, for example, it counts the number of times one character speaks to another (Jannidis et al., 2016).  The notion of  betweenness centrality disrupts the conception of what the “centre” of a network may consist of. Its ability to reveal structural elements bridging large, immediately visible clusters makes it popular in the social sciences since the emergence of Granovetter’s concept of “weak ties” (Granovetter, 1973). Betweenness is very closely linked to the notion of circulation: it counts the shortest paths to detect intermediate “bridges” or “key passages” capable of opening or locking certain parts of the network (Tayler and Neugebauer, 2018). Depending on applications, these are therefore both positions of power and vulnerable places.  The  closeness centrality allows to highlight the “geographical” middle of the graph. In networks of a certain density and when they are not divided into several distinct communities, the closeness is generally fairly evenly distributed and allows a good translation of the notions of “center” and “periphery”.  For its part, the  eigenvector centrality is quite complicated to translate since it works iteratively and is very much dependent on the structural context at short and medium range around a node. “Prestige” or “influence” centrality, named “power” centrality by its author (Bonacich, 1972), it qualifies a node’s environment while operating in cascade: a well connected node gives its neighbours a part of its authority capital, and so on. It is therefore particularly useful when trying to analyze the hierarchy of the nodes in a graph (Piper et al., 2017). The most well-known use of this measure is the backbone of the Google search engine: the PageRank algorithm (Brin and Page, 1998).     Towards mixed approaches This contribution proposes a table of correspondence between the concepts of graph theory and the practice of visual network analysis in the social science and humanities. This effort must not be understood as a demarcationist attempt at telling the right method from the wrong. The “dictionary” is not exhaustive and only aims at helping to bridge two worlds that have more in common that what meets the eye. By focusing on translating methods, we want to stress that crossing points are real even though they do not come without issues, and thus require our methodological attention.  We also note that the analysis should not be limited to a catalogue of well-known methods (basic centralities, etc.) but that approaches combining several of those should be encouraged to obtain an optimal and innovative “translation”. In this way, we could compare metrics (Escobar and Schauf, 2018) or combine them to establish rankings (Fischer et al., 2018; Grandjean, 2018: 328). Furthermore, the enrichment of the networks by means of categories that are not dependent on the structure, like the gender of individuals in a social network (Dunst and Hartel, 2017) or the discipline of projects in a scientometric analysis (Grandjean et al., 2017), allows to test translation and interpretation hypotheses by avoiding the blind approach of testing all possible graph metrics.  ",
        "article_title": " Translating Networks Assessing Correspondence Between Network Visualisation and Analytics ",
        "authors": [
            {
                "given": "Martin",
                "family": "Grandjean",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne, Switzerland",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            },
            {
                "given": "Mathieu",
                "family": "Jacomy",
                "affiliation": [
                    {
                        "original_name": "Aalborg University TANTLab, Denmark",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "network analysis and graphs theory",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper presents the guiding principles and ongoing development of  Manuscripta, a digital catalogue of medieval and early modern manuscripts in Sweden, which started as a project specific database but has since evolved to become a national infrastructure. The manuscript descriptions are encoded in  TEI, which is a highly suitable metadata format for detailed, scholarly catalogues since the hierarchical structure of TEI corresponds to the four parts traditionally used in cataloguing: description of contents, codicological description, provenance, and bibliography. The digitised manuscripts are presented using the  IIIF API, and the images are available free of restriction under the CC0 public domain dedication. The infrastructure is built entirely using open source software, and the source code, together with the TEI-files, are available on  GitHub.    Cataloguing and encoding principles Medieval and early modern manuscripts are seldom monographs, but often contain multiple texts by different authors. Furthermore, they have usually gone through several stages of production and use, e.g. been expanded, taken apart, lost certain parts, or been rebound with new additions. Occasionally new texts may have been inserted on previously blank pages. This historical stratigraphy has not, until recently, been taken into account in manuscript cataloguing, but recent scholarship and methodological developments have shown that the notion of codicological units is an essential requirement for detailed, scholarly catalogues, not least to distinguish different dates and provenances of various units, and to clearly indicate this information for each particular unit, which has previously not been the case (Gumbert, 2004; Andrist et al., 2013). The manuscript descriptions are therefore structured around the notion of codicological units, and the customised TEI encoding schema, which also provides cataloguing guidelines with examples and documentation of the elements and attributes used, has been tailored to this end. This is one of the first digital catalogues to implement this state-of-the-art research, which at times has been called a codicological revolution. There have so far only been a few printed catalogues offering this form of stratified cataloguing. The structure of the TEI-encoded descriptions follows as closely as possible the hierarchical structure of the manuscripts: the intellectual content, physical description, and history, where applicable, of each unit are described in separate <msPart> elements, whereas information common to all units, e.g. the binding, provenance, and bibliography, is described one level higher, directly under the <msDesc> element. The <msPart> element is always used, even when a manuscript consists of only one codicological unit, to reflect the notion of the ‘monomerous codex’, a term coined by Gumbert (Gumbert, 2004:26).   Technology The infrastructure is built entirely using open source software which is an essential requirement for transparency and for ensuring long-term maintainability. It runs on an XML database called  eXist-db, which offers advanced indexing and search functionality through XQuery, and built-in functions for converting TEI to HTML and PDF. The digitised manuscripts are displayed adjacent to the descriptions, with page references linked to the digitised manuscript, enabling immediate access to specific locations in the manuscript. Part of the description is given in running text, and TEI provides a variety of possibilities for tagging words and phrases, e.g. information such as names, places, dates, writing material, and watermarks. These tags then allow for advanced text search queries. In addition, names, places, and bibliographical references are linked to authority files which have information on alternative name forms, short biographical and geographical data, and links to other external resources. Since TEI is based on XML, it can easily be converted into other formats like HTML, PDF, MODS, JSON, RDFa. This is an essential requirement for the data to be transferable between different platforms and to enable Linked Open Data.  Cataloguing is done using a web-based editing interface, which does not require any knowledge of TEI encoding and therefore simplifies and reduces the time required for the cataloguing process. The interface is built with  React.js. Previously, it has been necessary to use an XML editor which had a steep learning curve, and was time-consuming as well as error prone, even when validating with an encoding schema and using detailed cataloguing guidelines.     Future plans More descriptions will be added to Manuscripta continuously, not only born digital descriptions, but also descriptions from legacy databases like Microsoft Access and FileMaker, and printed manuscript catalogues, using OCR and data extraction. These will then be encoded into the same schema as the born digital descriptions. Further development will include adding a controlled vocabulary for technical terms used in the manuscript descriptions and authority files for works preserved in the manuscripts; enriching the HTML with  Microdata, i.e., machine-readable data for the semantic web which would enable search engines to give users more accurate search results; creating stylesheets for converting the TEI of the authority files into linked open data formats like RDF and JSON-LD.   ",
        "article_title": "A Digital Catalogue of Medieval and Early Modern Manuscripts",
        "authors": [
            {
                "given": "Patrik",
                "family": "Granholm",
                "affiliation": [
                    {
                        "original_name": "National Library of Sweden, Sweden",
                        "normalized_name": "National Library of Sweden",
                        "country": "Sweden",
                        "identifiers": {
                            "ror": "https://ror.org/049bh0z35",
                            "GRID": "grid.435908.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "manuscripts description and representation",
            "English",
            "library & information science",
            "medieval studies",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In my presentation I will explore how online audiences experience time in digital museum communities, and how these experiences change their cultural perceptions and identities. The project will focus on the online museum case study: Singapore Memory Project (2011-2015). It is an online national initiative for public memory preservation. It was facilitated by the National Libraries and Museums Board in 2011 to collect and provide access to Singapore’s culture through crowdsourcing. It aimed to tell the “Singapore Story” through the voices of ordinary people in the live stream of their shared memories.  The construct of time appears to be the most important in the Singapore Memory Project, which even in its title speaks through the language of time. The recreation of the cultural past is an important political task for Singapore. Being a young country, the government strives to establish the country’s legitimacy through representations of evidence of its very existence in time, as this brings more leverage when negotiating identity and interests with various parties in the global context. Crowdsourcing techniques, involving ordinary people in recreating national memory, appears to be a quite sophisticated technology of manipulating the construct of time within the national consciousness. Under the pressure of national identity crises, the memory project worked as an important step to naturalize public perceptions and understanding of the nation within their political and cultural history.  Due to extremely rapid economic growth in the last fifty years, which completely reshaped physical, social, and cultural landscapes of the country, many important cultural places, symbols, and objects of significance were destroyed or lost. In this situation, it was imperative for Singapore to rebuild its cultural dimension through revitalizing and nurturing cultural memory and reconstituting national cultural identity. Aiming to rebrand the cultural image of Singapore, the government aimed to build upon public efforts of collective memory construction.  Employing interviews with governmental officials and museum managers, as well as content discursive analysis of the online memory portal, in my research I analysed how this digital space reconstructed time through museum narratives communicating political messages across borders. I also explored how these narratives were aligned with national and foreign policy objectives of the country revealing social and cultural complexities of the memory crowdsourcing exercise.  The presentation will contribute to the topic of the conference by providing an analysis of the contemporary politics of chronology through the lenses of digital diplomacy implemented within online museum environments changing human perception of time. The project will draw on the conceptual framework of French critical philosopher Bernard Stiegler, who explored the technologies of human consciousness manipulation in his seminal series,  Techniques and Time (Stiegler 1998, 2009, 2010). He elaborated on the processes of “industrialization” and “externalization” of memory which in digital networks operate through much shorter circuits of informational flows and exchanges on a global level. The increased speed of communication in online networks makes it both easier and less expensive to deliver texts, music, symbols, and images to people around the world, thus accelerating global consumption of information and formation of cultural environments. According to Stiegler, the mechanisms that are in place in the reconstruction of human experiences through interaction within digital networks can be explained by the ability of digital communication to represent the pasts of others while being in the present for an individual. As a result, history, traditions, and communities can be “instrumentalized” and transmitted, thus creating possibilities for reconstructing historical past through digital narratives changing human perceptions of time.  The presentation will identify and outline political implications of reconstructing time and memory through an empirical analysis of the Singapore Memory Project. This online museum portal provided an excellent illustration of the theoretical paradigms developed by Stiegler to describe the modern political technologies modulating public consciousness in the digital realm. More importantly, though, the Singapore Memory Project provided a platform where the power of digital technologies to change human perception of time could be tested. Through a focused analysis of the public engagement with the portal my research reveals that a crowdsourcing activity can actually turn into a political machine of inciting nationalism and constructing citizenry. ",
        "article_title": "Re-inventing the Past Through Singapore Memory Project: Socio-political Complexities of Digital Crowd-sourcing Techniques",
        "authors": [
            {
                "given": "Natalia",
                "family": "Grincheva",
                "affiliation": [
                    {
                        "original_name": "University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "crowdsourcing",
            "methods and technologies",
            "content analysis",
            "libraries",
            "museums",
            "communication and media studies",
            "English",
            "GLAM: galleries",
            "archives",
            "cultural studies",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper reports on World-Historical Gazetteer (WHGazetteer), a three-year project funded by the US National Endowment for the Humanities, now two-thirds complete.   Goals and Purpose WHGazetteer is a scholarly infrastructure project intended to support historical research across many disciplines. It is principally a web-based software system for aggregating open access data about places and linking it with data about historical entities associated with those places. We have seeded the system with a global ‘spine’ dataset of some 30,000 places, developed expressly for this purpose. There are existing repositories of historical place data, and many more are in development. A few are explicitly termed gazetteers   E.g.  Pleiades;  Vision of Britain  , some are historical GIS web resources   E.g.  China Historical GIS ;  HGIS de las Indias  , and others comprise place data tables developed and published in the course of other research projects. Typically, each concerns a particular geographic area and a particular temporal scope   E.g.  Map of Early Modern London ;  Syriaca.org  .  Bu contrast, WHGazetteer is soliciting and aggregating attestations of historical places for all regions and periods, along with annotations of records about historical entities with identifiers for those places. The project is conceived from a world-historical perspective. By this we mean several things. First, that it is intended to facilitate representations of connection and movement; second, that it scales up to global processes and the longue durée; and finally, the places for us includes ethnonyms for cultural regions, physical features, and ecoregions, helping ensure coverage for all parts of the globe and providing geographic context. In many respects WHGazetteer follows and extends the pilot implementation of  Peripleo, the linked data gazetteer system developed by the  Pelagios Commons project. We are working closely with the Pelagios group to leverage their accomplishments, maintaining and extending the trajectory they established.     Places and Traces WHGazetteer will solicit contributions of two kinds of place-related data. One is attestations of place names found in historical texts and maps:  Places. The other is annotations of records about any sort of historical entity (artifacts, creative works, persons, events, etc.) with place identifiers from published gazetteers:  Traces.    Places WHGazetteer catalogues place references associated with a time period, which may be a date of publication for an historical source or temporal information about a name as in a modern historical atlas. In either case, a historical gazetteer records attestations from sources that assert that a name existed at a certain time. This approach to temporal scoping differentiates us from modern gazetteer data sources like GeoNames and Wikidata. A single place may (and ultimately should) have many linked attestations for different periods, where names, spellings, place type, relations, and geometry vary.   Traces Traces are assertions of spatial and temporal scope for historical items of almost any kind: the footprint of an individual’s lifepath; the findspot of a coin hoard; the itinerary of a journey; the extent of a war; the places referred to in any sort of text, from a treaty to a novel; and so on.    Standard Data Formats We are developing two standard contribution formats conforming to linked data requirements. The first,  Linked Places format, is essentially complete and is being tested with early contributions.  Linked Traces format, based on the W3C Web Annotation Data Model, will be completed in late spring, 2019.    Contribution Pipeline Projects contributing to WHGazetteer come in all sizes and have varying levels of technical capability. Some have elaborate web interfaces and maintain permanent URIs for thousands of place records. These projects will have little difficulty exporting abbreviated records transformed to Linked Places format. Others have dozens or at most hundreds of place references drawn from sources specific to their domain of interest and are unable to stand up per-place pages with permanent URIs. The WHGazetteer contribution pipeline (Fig. 1) allows both groups to a) upload compatible Linked Places or CSV files, b) reconcile their records against Getty TGN, DBpedia, GeoNames and Wikidata as well as the WHGazetteer index itself, c) review and validate results of that matching process, thereby augmenting their records with closeMatch or exactMatch links, and d) contribute reviewed records to the index, published under permanent URIs provided by WHGazetteer.   Backend Stores and Middleware The WHGazetteer system backend is comprised of a PostgreSQL relational database and multiple Elasticsearch index stores. These and the APIs for internal use and public access are managed with Django, a Python-based web development framework.    Interfaces WHGazetteer will provide a public API supporting machine queries to all indexed data. A graphical web interface will support contribution activities for authenticated users, and provide search and mapping capabilities for both Places and Traces. Linking Place and Trace data in a single backend allows us to present linked data ‘portals’ for all indexed places, which will grow richer over time as our indexes expand. Using either the public API or graphical interface one might discover, for a given place: its names, shapes, and relations with other places over time; people whose lifepaths have intersected it; journeys for which it has been a waypoint; and texts and artworks for which it is a subject.  Our contribution pipeline interface will enable several kinds of pedagogical applications. For example, students and instructors will be able to create custom Traces associated with course material. Advanced students will be able to upload CSVs of gazetteers they have created, and use the WHGazetteer reconciliation tool to augment their records with information in the WHGazetteer index.   Status Version 1 of WHGazetteer is scheduled for launch in January 2020. A beta version will be available by July 2019.  We have established data partnerships with roughly a dozen contributors of large datasets covering a range of regions and periods, and also have a queue of many smaller projects. Data pipeline functionality to receive these is nearly complete (Fig. 1).   Sustainability WHGazetteer has been designed to require minimal hand-holding for contributions, and tools for efficient curation and maintenance. University of Pittsburgh’s World History Center is committed to maintaining the system for the foreseeable future.    Figure 1 – Data pipeline (partial): (a) upload dataset(s); (b) perform reconciliation against modern authorities; (c) review/validate reconciliation ‘hits’  ",
        "article_title": "World-Historical Gazetteer",
        "authors": [
            {
                "given": "Karl",
                "family": "Grossner",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh, United States of America",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            },
            {
                "given": "Ruth M.",
                "family": "Mostern",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh, United States of America",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "library & information science",
            "semantic web and linked data",
            "databases & dbms",
            "information retrieval and query languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Statistical topic models are increasingly and popularly used by Digital Humanities scholars to perform distant reading tasks on literary data (Navarro-Colorado, 2018), (Hettinger et al., 2016). It allows us to estimate what people talk about. Especially Latent Dirichlet Allocation (LDA), see (Blei et al., 2003), has shown its usefulness, as it is unsupervised, robust, easy to use, scalable, and it offers interpretable results. In a preliminary study, we apply LDA to a corpus of New High German poetry (textgrid, with 51k poems, 8m token) and interpret salient topics, their trend over time (1575–1925 A.D.), and use the distribution of topics over documents for a classification of poems into time periods and for authorship attribution.   Corpus The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form (Vanscheidt et al., 2016). It was mined from  http://zeno.organd covers a time period from the 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work). We find that around 51k texts are annotated with the label ’verse’, not distinguishing between ’lyric verse’ and ’epic verse’. We will further call this verse portion TGRID-V. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Furthermore, the poems are distributed over 229 authors, where the average author contributed 240 poems with a median of 131. A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French and Dutch (around 200 poems). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work.     Figure 1: 25 year Time Slices of Textgrid Poetry (1575--1925)     Experiments We approach diachronic variation of poetry from two perspectives. First, as distant reading task to visualize the development of clearly interpretable topics over time. Second, as a downstream task, i.e. supervised machine learning task to determine the year (the time-slot) of publication for a given poem. We infer topic distributions over documents as features and pit them against a simple style baseline. We use the implementation of LDA as it is provided in genism (Rehurek and Stoja, 2011). LDA assumes that a particular document contains a mixture of few salient topics, where words are semantically related. We transform our documents (of wordforms) to a bag of words representation, filter stopwords (function words), and set the desired number of topics=100 and train for 50 epochs to attain a reasonable distinctness of topics. We choose 100 topics (rather than a lower number that might be more straightforward to interpret) as we want to later use these topics as features for downstream tasks. We find that wordforms (instead of lemma) are more useful for poetry topic models, as these capture style features (rhyme), orthographic variations ('hertz' instead of 'herz'), and generally offer more interpretable results.  Topic Trends We retrieve the most important (likely) words for all 100 topics and interpret these (sorted) word lists as aggregated topics, e.g. topic 27 (figure 2) contains:  Tugend (virtue), Kunst (art),  Ruhm (fame), Geist (spirit), Verstand (mind) and Lob (praise). This topic as a whole describes the concept of ’artistic virtue’.  In certain clusters (topics) we find poetic residuals, such that rhyme words often cluster together (as they stand in proximity), e.g. topic 52 with: Mund (mouth), Grund (cause, ground), rund (round).  To discover trends of topics over time, we bin our documents into time slots of 25 years width each. See figure 1 for a plot of the number of documents per bin. The chosen binning slots offer enough documents per slot for our experiments. To visualize trends of singular topics over time, we aggregate all documents d in slot s and add the probabilities of topic t given d and divide by the number of all d in s. This gives us the average probability of a topic per timeslot. We then plot the trajectories for each single topic. See figures 2–6 for a selection of interpretable topic trends. Please note that the scaling on the y-axis differ for each topic, as some topics are more pronounced in the whole dataset overall.    Figure 2: left: Topic 27 'Virtue, Arts' (Period: Enlightenment), right: Topic 55 'Flowers, Spring, Garden' (Period: Early Romanticism)      Figure 3: left: Topic 63 'Song' (Period: Romanticism), right: Topic 33 'German Nation' (Period: Vormärz, Young Germany)      Figure 4: left: Topic 28 'Beautiful Girls' (Period: Omnipresent, Romanticism), right: Topic 77 'Life & Death' (Period: Omnipresent, Barock)      Figure 5: left: Topic 60 'Fire' (Period: Modernity), right: Topic 42 'Family' (no period, fluctuating)      Figure 6: Most informative topics for classification; left: Topic 11 'World, Power, Time' (Period: Barock), right: Topic 19 'Heaven, Depth, Silence' (Period: Romanticism, Modernity)   Some topic plots are already very revealing. The topic ‘artistic virtue’ (figure 2, left) shows a sharp peak around 1700—1750, outlining the period of Enlightenment. Several topics indicate Romanticism, such as ‘flowers’ (figure 2, right), ‘song’ (figure 3, left) or ‘dust, ghosts, depths’. The period of 'Vormärz' or 'Young Germany' is quite clear with the topic ‘German Nation’ (figure 3, right). It is however hardly distinguishable from romantic topics. We find that the topics 'Beautiful Girls' (figure 4, left) and 'Life & Death' (figure 4, right) are always quite present over time, while 'Girls' is more prounounced in Romanticism, and 'Death' in Barock. We find that the topic 'Fire' (figure 5, left) is a fairly modern concept, that steadily rises into modernity, possibly because of the trope 'love is fire'. Next to it, the topic 'Family' (figure 5, right) shows wild fluctuation over time. Finally, figure 6 shows topics that are most informative for the downstream classification task: Topic 11 'World, Power, Time' (left) is very clearly a Barock topic, ending at 1750, while topic 19 'Heaven, Depth, Silence' is a topic that rises from Romanticism into Modernity.   Classification of Time Periods and Authorship To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slots (instead of 25) to ease the task. For each document we determine a class label for a time slot. The slot 1575–1624 retrieves the label 0, the slot 1625–1674 the label 1, etc.. In total, we have 7 classes (time slots). As a baseline, we implement rather straightforward style features, such as line length, poem length (in token, syllables, lines), cadence (number of syllables of last word in line), soundscape (ratio of closed to open syllables, see (Hench, 2017)), and a proxy for metre, the number of syllables of the first word in the line. We split the data randomly 70:30 training:testing, where a 50:50 shows (5 points) worse performance. We then train Random Forest Ensemble classifiers and perform a grid search over their parameters to determine the best classifier. Please note that our class sizes are quite imbalanced.  The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%. The mose informative features (with information gain) are: Topic11 (.067), Topic 37 (.055), Syllables Per Line (.046), Length of poem in syllables (.031), Topic19 (.029), Topic98 (.025), Topic27 ('virtue') (.023) and Soundscape (.023). For authorship attribution, we also use a 70:30 random train:test split and use the author name as class label. We only choose the most frequent 180 authors. We find that training on stanzas gives us 71% Accuracy, but when trained on poems, we only get 13% Accuracy. It should be further investigated is this is only because of a surplus of data.     Conclusion & Future Work We have shown the viability of Latent Dirichlet Allocation for a visualization of topic trends (the evolution of what people talk about in poetry). While most topics are easily interpretable and show a clear trend, others are quite noisy. For an exploratory experiment, the classification into time slots and for authors attribution is very promising, however far from perfect. It should be investigated whether using stanzas instead of whole poems only improves results because of more available data. Also, it needs to be determined if better topic models can deliver a better baseline for diachronic change in poetry, and if better style features will outperform semantics. Finally, only selecting clear trending and peaking topics (through co-variance) might further improve the results.  ",
        "article_title": " Diachronic Topics in New High German Poetry  ",
        "authors": [
            {
                "given": "Thomas Nikolaus",
                "family": "Haider",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for Empirical Aesthetics, Frankfurt, Germany; Institut für Maschinelle Sprachverarbeitung, University of Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-06",
        "keywords": [
            "natural language processing",
            "authorship attribution / authority",
            "data mining / text mining",
            "English",
            "cultural evolution",
            "computer science and informatics",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Context and collaboration The Applying AI to Storytelling project was a high-risk research and development project at the intersection of the digital humanities, computer science, and the creative industries, funded by Innovate UK. The project embedded a university-based Research Software Engineer (RSE) from a Digital Humanities (DH) lab in the core development team of a start-up company pioneering interactive storytelling techniques, with a particular focus on character-based chatbot technologies powered by artificial intelligence. The project therefore sits at a crossroads between academic DH, literary-critical analysis, the creative industries, computer science, and software engineering. In doing so it represents the emergence of a new strand in DH practice that seeks to take lessons learned over decades of incubation within universities and cultural heritage organisations into the wider world. The work is also representative of a wider socio-cultural convergence of advanced technologies with creativity and academic research. 1  Research Software Engineering (RSE)  is growing rapidly in the United Kingdom, United States, and Australasia and is increasingly being used as an umbrella category that DH practitioners can contribute to 2. That is the case at our university, where DH teams work within a wider RSE community comprising specialists in bio-informatics, imaging science, and data analysis. 3 While RSE teams in the science and technology sector have long-established relationships with industry partners--creating a significant traffic in ideas, methods, and personnel across HE and industry boundaries-- DH RSE teams have until now focused mostly on developing relationships with the cultural heritage sector rather than industry.  Embedding a DH RSE within a creative industry SME as a connection point between industry and academia aims to facilitate a similar traffic in ideas and methods between the Creative Industries, RSEs and academics while at the same time demonstrating the value of DH to higher education and government sectors that are increasingly focused on the creation of tangible economic value.   DH methods are well suited to such an undertaking, providing an important element in both the creative and technical processes, and assisting with the translation of research in digital literary studies, 4 media history, 5 post-humanism, 6 and narrative design 7 to a commercial product that has captured the interest of companies  in the United States and Europe. The partner company involved in this project is developing a system that accelerates the creation of AI-driven storytelling,  recommending possible dialogue to the author using natural language processing and visually managing the complex branching structures these narratives require.  This system is a product that is currently in the beta stage and already licensed to external companies for the creation of interactive stories, as well as film and television productions.   Our collaboration extends established techniques in immersive storytelling and ‘extended reality’ (XR) into the commercial world of product design and development. 8 This kind of work is being encouraged by the UK government through a variety of funding mechanisms, as it aims to maximise the country’s natural advantages in cultural and creative production. 9 The partnership used the company’s core product to produce a ground-breaking translation of an immersive theatre piece into an AI-driven game written in the Unity engine.  The show relies on audience participation, assigning them tasks and roles and changing the direction of the narrative based on their choices. The adaptation places audience members with a cast of AI characters, using machine learning to inform their personalities, intent, and emotional interactions with the player.    The problem of memory The key focus of the collaboration with the DH team is the ‘problem’ of memory that results from characters who ‘remember’ what the player has said and done.  This question is what brought the tech company CEO to our DH team in the first instance: they had reached a point where their technology had moved beyond their writers’ understanding of narrative. Their goal in partnering with a DH RSE was to better understand the literary-critical issues associated with digital characters who can ‘remember’ information given to them by readers, and improve the writing product they aim to sell into the creative industries. Perhaps surprisingly, given the long history of ‘network fiction’ 10 that stretches back to the electronic literature movement of the early 1990s and beyond to ‘Multi-user Dungeons’ of the 1970s, they were struggling to find writers (even when collaborating with writers with experience designing video games and interactive fiction) comfortable with the radical level of emotional interaction support for memory creates. The persistence of memory in narrative has been noted as an affordance of even simple hypertext fiction, 11 but holds additional micro-level implications for interactions with players, and profound macro-level implications for the overall direction of a narrative, when ‘super-charged’ with AI.  AI characters in the game remember, misremember and outright lie, challenging the player’s decisions, and even their own memory of what has transpired in the game.   Maximising the potential for this to support high quality narrative is non-trivial, as is developing an elegant writing tool capable of supporting such new modes of imagination and reader interaction. In building this prototype the project faced a number of overlapping, multidisciplinary challenges.  The narrative authoring system had to be comprehensible to non-technical users -- especially writers -- to be used effectively, while at the same time retaining technical scalability and sustainability. The optimisation of the machine learning algorithms to create convincing character interactions required a critical examination of their output in both literary and social science terms, identifying how conscious and unconscious bias informs the author’s conception of the character and the player’s responses to it. This convergence of technical, creative, academic and commercial factors make the project a natural fit for Research Software Engineers working within Digital Humanities.  Difficulties in structuring the dialogue progression within the UI, or in how the algorithm follows particular narrative paths and not others, are often technical, conceptual and cultural questions all at once.  These interrelated problems require not only the range of skills provided by a partnership between academia and the creative industries, but the high-risk, experimental nature of the work demands the strong connective tissue that Digital Humanities provides in mediating across scholarly, technical and business languages.     Versatile and eclectic RSE The unique nature of this collaboration, and evidence of the significant value DH skills and mindsets offer the commercial product development process, is demonstrated by the fact that the lead RSE is also a writer who has worked in immersive theatre.  This relatively rare set of skills enabled him to work with the show’s author on the adaptation; developing characters, designing narrative, and writing and editing dialogue. At the same time he provided granular, detailed feedback to the development team on the usability of the UI, and the challenges that both he and the writer encounter in structuring a shared immersive experience as a single player interactive game.   Rather than being only a conduit for the academic and creative sides of the collaboration, therefore, the lead RSE was involved in both the creative and technical processes that created the final product.   As the project evolved, changing priorities and deadlines led the RSE to take on a more creative role, becoming the primary writer of a showcase demo for the immersive theatre project and developing the narrative structure for the adaptation of a novel into the system.  As a result of this change in responsibilities, the RSE identified issues as a writer that he then helped solve as a developer. As an example, in order to better test the narratives he was creating, he developed a tool to automatically run a set of player responses, effectively creating a unit test for the story.  He then used this tool as a writer to edit and refine the narrative paths inside the AI-driven system. This model of collaboration created an intermediate space where DH methods allow seamless movement between the roles of developer and writer, often on the same question. This ability to not just translate between the creative spaces of game development and writing, but to inhabit both simultaneously, is essential to the success of creative collaboration, and more accurately reflects the current state of digital creativity in the independent sector.   A participatory model of creative co-production An important secondary aspect to the research is the development of a model capable of defining optimal modes of working between DH RSE teams and the creative industries. The model was created through an ethnographic approach pioneered in the Science and Technology Studies (STS) community by researchers such as Latour and Woolgar, Knorr-Cetina, 13 and Fujimura 14 in conjunction with industrial theories such as lean production (Ohno) 15 and disruptive innovation (Christensen). 16 The project’s Principal Investigator (PI)  developed the model by embedding himself in the creative process, observing interactions between writers, technologists, business people, and the DH RSE over the course of 12 months. The insights gained will be used to generate a best practice model to facilitate a mutually beneficial partnership between academia and industry, one that can be used by other DH RSE teams, increasing capability across the global community and demonstrating the special value DH has for companies and governments increasingly struggling to recruit and retain staff with convergent skill sets spanning creative and technical fields.  Effective collaboration between universities and industry is essential for the higher education sector to maintain its leading status in research and innovation, and to ensure industry can resolve the tension between convergent creative and technological pressures. Alongside this industry innovation, of course, is the equally important parallel development of a critical infrastructure within the humanities to effectively engage, modify and critique these emerging forms of narrative.  This project provides an example of how Digital Humanities RSE teams can act as key participants in this process, using their blended culture of development and research to avoid an excluded middle, where academic and creative teams lack the vocabulary, practical experience, and cultural experience to collaborate effectively together.  ",
        "article_title": "Misremembering Machines: A Creative Collaboration on Memory in AI-driven Storytelling",
        "authors": [
            {
                "given": "Elliott",
                "family": "Hall",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "James",
                "family": "Smithies",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "software design and development",
            "user experience design",
            "artificial intelligence and machine learning",
            "interface",
            "gamification",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "literacy and creative writing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A digital image of a painting is not the painting. A digital image of a folio is not the folio. It is an artifact in its own right that conveys, or fails to convey, the information one would want to study. Just as human perception is full of complexities, the construction of a digital facsimile is full of complexities. A very simple image may be recognizable as a representation of the original, while a very advanced image may fail to satisfy the questions of research. The project of “digitization” requires reflection on the nature of scholarly perception of the artifact and the digital tools suited to capture and represent the pertinent information. It is no longer enough to consider spatial resolution (pixels per inch or centimeter). This presentation considers first the modes of perception in scholarly investigation of artifacts such as manuscripts including the importance of texture and interactivity for humanities questions. Second, we will consider the ability of “spectral imaging” (including narrow-band reflectance, fluorescence, and transmission) to meet and surpass the capabilities of the human eye on first-hand inspection. Third, we will consider the tools for capturing and representing texture and interactivity with raking light photography and Reflectance Transformation Imaging (RTI). Finally, we will present the results of a recent project, funded by the U.S. National Endowment for the Humanities, to integrate Spectral Imaging and RTI (Spectral RTI) in capture, processing, and visualization. The software and documentation created by the project are freely available online and accessible for use “off the shelf” by imaging teams. Scholarly investigation utilizes a variety of modes of perception depending on the questions brought to the artifact. Often the interest is in reading text written on the artifact, which can be difficult if the manuscript was deliberately erased for reuse or otherwise damaged. The difficulty only increases if the writing was secondary in the first place, as with marginal or interlinear notes or quire numbers. Some scribal markings, such as scoring lines, are not even inked in the first place. Similarly, dry-point notation could be an important object of study, yet completely invisible in diffuse-light photography. Limited considerations for digitization may be appropriate if a manuscript is valued only as a text container. Different considerations for digitization are appropriate if a manuscript is valued as an artifact of scribal cultures. To all these examples from manuscripts many more could be added for other media, such as paintings. The study of such objects in person, without digital or other mediation, involves first and foremost movement. The scholar moves the relationship between the eye, the object, and the light, and changes the lighting. The movement could mean moving closer or farther, to a different angle, or even holding the object up to a light to see if light passes through. Just as no one still position satisfies every inquiry, no one still photograph creates an adequate facsimile for scholarly investigation. Indeed, the experience of interaction surpasses the sum of individual moments of perception. The value of an interactive experience is strong for research, and even stronger for teaching. For all these reasons it is still common to hear “there is no substitute for first-hand experience.” While experience with a digital surrogate may never be identical, the number of questions that can be answered from the surrogate depends on the methods of digitization. The limits of digitization are not absolute. When justified, the use of spectral and texture imaging may allow a digital copy to approach and in some ways surpass first-hand experience. The following images show the same page digitized six ways. The object is Biblioteca Ambrosiana C73 inf page 110, a palimpsest with the Testament of Moses overwritten with Eugippius’ anthology of Augustine (CC BY-NC-SA).    (1) 1-bit black and white by way of a print edition     (2) grayscale by way of microfilm     (3) DSLR camera 2011     (4) Multispectral accurate color 2017     (5) Accurate color with raking light     (6) Multispectral enhanced color with raking light.  Spectral imaging expands the range and resolution of color perception of the human eye. The human range spans from violet to red, excluding ultraviolet on one end and infrared on the other. Human color resolution is limited to three receptors, so all the colors we see are combinations of intensity of three elemental colors, red, green, and blue. Digital spectral imaging measures reflectance of pixels in an image at specific wavelengths of light. These wavelengths can extend to ultraviolet and infrared, and resolve colors more finely than the three receptors of the human eye. Just as someone with only two kinds of receptors can be called “color blind,” all the more so a person with three receptors is limited compared to the sixteen or more possible with spectral imaging. In addition to reflectance, spectral imaging today often captures fluorescence and transmission, comparable to use of a blacklight and holding a light behind the object, respectively. Because the capture is digital, the numerical values of light intensity at each picture element under each of dozens of conditions can be processed in relationship to each other. As a result, color can be captured and rendered more accurately than a simple “color” camera, and algorithms can enhance contrasts, such as erased ink on parchment, that could never be perceived with the unaided eye. Texture imaging is most simply accomplished with raking light. That is, the light is positioned from one direction at a low angle to the object. Any readable image of a coin, inscription, or cuneiform tablet uses this approach. The main limitation of raking light imaging is the judgment of the photographer in anticipating all the light positions that would be necessary to interpret the artifact. One solution is capture a complete set of raking light images from forty to sixty different positions. Again, since the information is digital and can be easily processed by algorithms, such a data set could be processed into a dynamically relightable image. This technology, called Reflectance Transformation Imaging (RTI), can extrapolate to light positions not directly captured, can enhance the texture, and is fully interactive. Imaging the texture of a surface is distinct from, but complementary to, 3D imaging. 3D imaging, using techniques such as laser scanning or photogrammetry, digitizes the boundary structure of an object. Texture imaging digitizes the reflectance properties of the surface as a function of light position, usually at much higer resolution. 3D imaging is well suited to capturing the shape of an object. For any one surface on that shape, RTI is well suited to capturing the fine texture, roughness, and specularity (shininess). The two technologies are entirely compatible, as a 3D engine can render both the shape of many surfaces (represented as triangles) and the shader properties of each surface. The first image following is derived from a 3D model created by laser scanning. The second image is photographed with raking light. Each has advantages and can be interactive in different ways. The object is Cyprus Museum 1885, a Cypro-Minoan tablet with an undeciphered writing system.     Digital spectral imaging and RTI have been in development since the beginning of the century when high-quality digital imaging became widely available. The recent development is the publication of the technique, open-source software, and documentation for integrating spectral and reflectance transformation imaging (Spectral RTI). A pair of grants from the U.S. National Endowment for the Humanities supported the experiments and development of the technique, followed by full-scale implementation, documentation, and open-source software that performs the processing entirely within a graphical user interface. The fundamental premise of the integration is that chrominance and luminance can be combined in color spaces such as YCbCr. Spectral imaging is concerned with chrominance and strives to avoid highlights and shadows by using diffuse illumination. RTI is concerned with luminance variation as a function of light position (i.e., highlights and shadows generated by light from specific angles) and pays no more attention to color than what is captured from a conventional camera with a Bayer filter. All the data can be captured with one camera and combined in post-capture processing. All the required software is freely available, most notably the SpectralRTI_Toolkit plugin for ImageJ. ",
        "article_title": "Deep Digitization: Considerations and Tools for Imaging Cultural Heritage Beyond the Basics",
        "authors": [
            {
                "given": "Todd R.",
                "family": "Hanneken",
                "affiliation": [
                    {
                        "original_name": "St. Mary’s University, United States of America",
                        "normalized_name": "St. Mary's University",
                        "country": "Ethiopia",
                        "identifiers": {
                            "ror": "https://ror.org/02hzmab58",
                            "GRID": "grid.470988.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "manuscripts description and representation",
            "methods and technologies",
            "archaeology",
            "English",
            "cultural artifacts digitisation - theory",
            "digital archives and digital libraries",
            "image processing",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  1 After the East Japan Great Earthquake on 11 March, 2011, rumors about an explosion at a petrochemical complex owned by Cosmo Oil spread rapidly on twitter. Stories of oil tanks exploding and releasing harmful substances into the air caused widespread panic until official government news releases corrected the misinformation the following day. This story demonstrates the importance of fake news detection: while an enormous real disaster (the earthquake) was unfolding, rumors of imaginary disasters spread misinformation and diffused attention on social media to imaginary dangers.  The story of the fake Cosmo Oil fire provides an example of a situation in which fake news detection is important, and also provides a test case for studying the characteristics of fake news on twitter. We propose a method for fake news detection based on topic diversity. Our method computes topic diversity by a micro-clustering approach that makes clusters smaller than those produced by conventional clustering methods. We begin by extracting micro-clusters, small sets of keywords that represent topics, using a data polishing algorithm (Uno 2015; Uno 2017) from tweets about the event. Next, we analyze clusters over time using visualization methods to understand how these topics change. We observe that a diversity measure for clusters, a measure of both the number of clusters and the number of words, in one cluster, shows topic transition. This observation has implications in the measure of the truthfulness of a story. Users tweeting about real news stories often show sudden changes in opinions, which causes a drastic increase in the diversity of the opinions expressed. Even if the number of tweets on the topic does not increase, topic diversity rises. On the other hand, users tweeting fake news stories are often not thinking critically about a topic, so there is no change on the number of clusters even as the number of tweets increases.    Figure 1: Outline of Our Method     2 Topic Extraction using Micro-Clustering Figure 1 illustrates our method. First, we build a graph of tweets: each tweet is a node, and an edge between two nodes represents two tweets whose Jaccard similarity is greater than 0.3. This is our Input Data. Then, cliques are extracted by micro-clustering using  Data Polishing (Uno 2017).  Micro-clusters represent small topics within a larger set of tweets. Micro-clusters are groups of records that have high similarity—in our case, tweets that include similar sets of words. To create micro-clusters of similar tweets, we perform maximal clique enumeration (MCE). A maximal clique is a clique included in no other clique within the graph. A maximal clique is not necessarily the largest clique in a graph, so the size of maximal cliques in the same graph can vary significantly.  Because there are usually a huge number of maximal cliques in a graph, MCE is a computationally intractable problem. Data polishing reduces the complexity of MCE. It makes an edge between pairs of nodes if they seem to belong to the same cluster, and removes edges between nodes that do not seem to belong to the same cluster. It clarifies the graph's cluster structures, and thus makes MCE far simpler.  We eliminate edges using the following procedure. If nodes  u and  v are in the same clique of size  k,  u and  v have at least  k − 2 common neighbors. Thus, we have  |N u  ∩ N v | ≥ k, so  u and  v are in a clique of size at least  k. If  u and  v are in a sufficiently large pseudo-clique, they belong to a pseudo-clique with a high probability of being semantically meaningful; otherwise, they do not. To compute these nodes’ similarity, we compute the Jaccard coefficient,  J, of their neighbor sets. We set the threshold  s’ and consider each pair of nodes in the graph. If  J(N u , N v ) > s’, we add an edge between  u and  v. Conversely, if  J(N u , N v )  <  s’, we remove any edges between them.  Micro-clustering produces a set of topics, each made up of one or more clusters. Next, topic transitions are analyzed by calculating the diversity of clusters that constitute the corresponding topic. Micro-clusters are groups of similar or related records.  In a graph, micro-clusters correspond to dense subgraphs, and non-edges in the dense subgraphs are ambiguities. We also consider edges not included in any cluster to be ambiguities. Our data polishing approach for micro-clustering consists of adding edges for these non-edges, and removing these edges from the graph.    3 Target Data Our target data set is the over 200 million tweets sent around the time of the Great East Japan Earthquake on March 11, 2011. We obtained this dataset from the social media monitoring company Hotto link Inc. (Hottolink). Hotto link tracked users who used one of 43 hashtags (including #jishin, #nhk, and #prayforjapan) or one of 21 keywords related to the disaster. Later, they captured all tweets sent by these users between March 9th (2 days prior to the earthquake) and March 29th. This dataset offers a significant document of users’ responses to a crisis, but its size presents a challenge. We show our experimental result for tweets from 00:00 on March 11 to 24:00 on March 15, a total of 120 hours. We began by creating the sequence of tweet-word count matrices for our dataset for every 30 minutes, that means we had 240 slots. For example, the matrix for 30 minutes on March 11 before 14:30 (before the earthquake), contains 60,000-80,000 tweets. On the other hand, the matrix for 30 minutes on March 11 after 15:00 (after the earthquake) contains 300,000-500,000 tweets. The number of tweets increased dramatically after the earthquake. The size of each matrix after 15:00, March 11 is around 15MB and they were all quite sparse.    4 Target Topic We selected the fake news about the petrochemical complex explosion that happened just after the earthquake. The story can be divided into four stages:  Fact: around 15:00 on March 11, just after the earthquake, the Cosmo Petrochemical Complex in Chiba caught fire. Fake: Around 19:00 on March 11, the following fake stories appeared as tweets and were retweeted frequently:  “Radiation and harmful chemicals are leaking into the air from the petrochemical complex. Be careful!”  “Don’t go out! Because the rain includes radioactivity and harmful materials by the petrochemical complex explosion.”    Correction: Around 15:00 on March 12 (the day after the earthquake), the company’s website and the local government’s twitter account explained that there had been no explosion.  Convergence: At night on March 12, the topic was converged.   The fake news about the oil tank emitting harmful substances scared users as it spread. Finally, the government released a report correcting the misinformation and the fake news disappeared from Twitter. To evaluate the progress of the target topic, we investigated micro-clusters with the phrase “Cosmo Oil” over time. We examined the target topic transition and the diversity of clusters in each time period to show our method’s effectiveness.     Figure 2: # of Tweets vs # of Micro-Clusters that include the word ”cosmo oil” (base 10 log-log plot)        Figure 3: Topic diversity for all tweets 5 Fake News Detection  The progress of the fake news is shown in Figure 2. The graph shows the relationship between the number of tweets and the number of micro-clusters. Each circle on the graph shows one half-hour time period.  Fake news stories show low topic diversity: Figures 2 and 3 illustrate the difference between a real story and a fake story. Both graphs plot the correlation between the number of micro-clusters that contain a phrase and the number of tweets that contain a phrase over each half-hour period. Figure 2 shows the topic diversity of a fake news story, tweets that contain “Cosmo Oil;” in contrast, Figure 3 plots all tweets from our data set during the same time. We use Figure 3 as an index of a known real story, the Great East Japan Earthquake, in contrast to the false rumor of the Cosmo Oil explosion.  For a real news story, the relationship between tweet count and micro-cluster count is linear. Figure 3 shows the nearly linear relationship observed on the log-log plot for a real news story, which implies a power law relationship between the number of tweets and the number of micro-clusters. We can make the hypothesis that the line is the upper bound of topic diversity: that is, when a topic emerges independently, the total number of topics is equal to this upper bound.  However, for a fake news story, the micro-cluster count is much lower relative to the tweet count in many time periods, and so the relationship is non-linear. In some time periods of Figure 2, highlighted in red, the number of micro-clusters is lower than the expected number. Figure 2 does not show a similar correlation between the number of micro-clusters and the number of tweets that contain the phrase “Cosmo Oil.” We suggest that a fake news story is more likely to have a lower topic diversity because there are fewer facts to report in such a story, and, as such, the story is likely to change little over time.    Figure 4: Dynamics of the fake topic transition about ”cosmo oil” (base 10 log-log plot)   We evaluate the progress of the fake news over time (Figure 4). In Figure 4(a), first, the “fact” topic occurred. Then the “fake” topic appeared as rumors of the explosion spread, shown in Figure 4 (b). Compared to the “fact” and the “correction” periods, the “fake” period shows low diversity: the measurements. Figure 4(c) shows the “correction” period, when the government corrected the fake story and the correction overtook the spread of the rumors. At that time, the number of tweets and the number of clusters grew together, and so the diversity increased. Finally, In Figure 4 (d), the convergence happened: the Cosmo Oil story started to disappear, while gthe diversity stayed high, but gradually the number of tweets decreased. The progress shows the dynamics of the topic transition and a kind of topic life cycle.  Through the experiment, we confirmed that our method can extract quality micro-clusters by data polishing. In addition, we realized that micro-clusters can show dynamics of the topic transition using real tweets.    6 Conclusion This paper proposed a fake news detection method based on micro-clustering using data polishing. It showed that fake news stories follow a certain lifecycle. Furthermore, it suggests that topic diversity measures can help to detect fake news before an official correction is issued, as in the case of the Cosmo Oil story.   Acknowledgements This work was partially supported by JST CREST JPMJCR1401, JSPS KAKENHI 19H01133 and 19K12125, 18K1143, and 17H00762.  ",
        "article_title": " The Narrow Scopes of Fake News: Detecting Fake News Using Topic Diversity Measures  ",
        "authors": [
            {
                "given": "Dave",
                "family": "Shepard",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles, USA",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            },
            {
                "given": "Takako",
                "family": "Hashimoto",
                "affiliation": [
                    {
                        "original_name": "Chiba University of Commerce, Japan",
                        "normalized_name": "Chiba University of Commerce",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/02qn0vb48",
                            "GRID": "grid.443770.3"
                        }
                    }
                ]
            },
            {
                "given": "Kilho",
                "family": "Shin",
                "affiliation": [
                    {
                        "original_name": "University of Hyogo, Japan",
                        "normalized_name": "University of Hyogo",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/0151bmh98",
                            "GRID": "grid.266453.0"
                        }
                    }
                ]
            },
            {
                "given": "Takeaki",
                "family": "Uno",
                "affiliation": [
                    {
                        "original_name": "National Institute of Information, Japan,",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tetsuji",
                "family": "Kuboyama",
                "affiliation": [
                    {
                        "original_name": "Gakushuin University, Japan",
                        "normalized_name": "Gakushuin University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/037s2db26",
                            "GRID": "grid.256169.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-07",
        "keywords": [
            "social media",
            "artificial intelligence and machine learning",
            "content analysis",
            "communication and media studies",
            "data mining / text mining",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  TXM is a software platform offering textual corpora analysis tools and services. It is delivered as a standard desktop application for Windows, Mac and Linux and as a web portal server application (Heiden, 2010), < >.  TXM provides a consistent set of analysis tools combining qualitative (or close reading) tools such as word frequency lists, concordancing or text edition hypertextual navigation with synthetic quantitative (or distant reading) tools such as factorial analysis, clustering, keywords or co-occurrence statistical analysis. To work on texts, the platform first imports the corpus sources to create a rich XML-TEI based internal pivot representation via the following general workflow:   first the “base text” of each text is established: this operation implements “digital philology” principles and consists of decoding information in the various formats of the source documents to decide primarily where are the text limits, possible internal textual structures boundaries and the words of the text. To do this, TXM can analyze and represent three main types of corpora:  corpora of  written texts, possibly including paginated editions including display side-by-side of the transcription and the images of facsimiles;   record transcriptions corpora, possibly time synchronized at the word or at the speech turn level with the audio or video source to provide playback;  and parallel  multilingual corpora aligned at the level of textual structures such as sentences or paragraphs.   The result of this operation is represented in a pivot XML format especially designed for TXM called “XML-TEI TXM” extending the standard encoding recommendations of the Text Encoding Initiative consortium (TEI Consortium, 2017);  then, natural language processing (NLP) tools are optionally applied to the base text to automatically add linguistic information like sentence boundaries, grammatical categories (pos = part of speech) and lemma of words by eg TreeTagger (Schmid, 1994). As NLP tools generally don’t take XML format as input, the pivot representation is first converted to plain text for NLP processing and results are injected back into the XML-TEI TXM representation; finally a specialized representation of the texts is built into TXM for efficient execution of its different tools (by indexing for search engines or by rendering in HTML 5+CSS+Javascript for text editions navigation).  From a methodological point of view:  the XML tags of the initial XML-TEI TXM representation in a) can be seen as  manual annotations added to the base text (or raw text), typically philologically edited with the help of specialized XML editors (like Oxygen XML Editor  https://www.oxygenxml.com ) outside of TXM when the source is XML native, or as  automatic annotations added by TXM when converting the sources from other digital formats (like TXT, DOC, etc.) into XML-TEI TXM.  NLP tools processing results in step b) can be seen as  automatic annotations added to the initial XML-TEI TXM representation of texts built in work step a);  All TXM tools can then be applied indiscriminately to all types of annotations through a unified textual corpus data model regardless of their origin (automatic or manual).  Thus, so far TXM has implemented a traditional digital philology workflow combining an initial “text source encoding and annotation” step to a following “application of analysis tools on annotated texts” step. The text analysis tools use text annotations (for example word pos or some internal textual structure) to offer their services and produce their results (for example the frequency index of all infinitive verbs found in a section). The workflow is unidirectional and the whole of it must be passed through again completely if any annotation needs to be corrected. To add or correct annotations, the user has to edit the sources or the annotations outside of TXM. For example word properties can be exported from the XML-TEI TXM representation in a file in tabulated format, edited in a spreadsheet and injected back into the texts before re-import into TXM  see for example this tutorial based on TXM macros:  . .  This paper introduces new services developed in TXM to annotate directly texts from within the results view of specific tools for a better integration of philological and analytical work. Indeed, results views are great places to be aware of annotation errors or annotation needs, and to access what needs to be corrected or annotated.   Interactive annotation services in TXM The three new annotation services concern both adding and correcting information, and all the annotations edited are meant for further exploitation by usual TXM tools.  Concordance based SyMoGIH entities annotation The first service, developed in partnership with the LARHRA research laboratory in history   http://larhra.ish-lyon.cnrs.fr  , is based on the annotation of concordance pivots: any sequence of words composing the pivots can be annotated with any semantic category  pivotscan also optionally be annotated with simple keywords or with key-value pairs, managed by TXM in a local repository. of the SyMoGIH    historical knowledge base (Beretta, 2015). In this architecture, the SyMoGIH platform hosts the ontology of historic facts and knowledge, and TXM concordances provide the user interface to link identifiers of those data to text spans for further analysis.  As an illustration, see figure 1 the annotation of the “Faculté de droit d’Aix” entity (of id CoAc13562) in unverified OCRed texts of the “Bulletin administratif de l'Instruction publique\" corpus  see the Bibliothèque historique de l'éducation (BHE) project:   .     Figure 1. TXM screenshot of a Concordance of a “Faculté de droit d’Aix” word sequence pattern to annotate (top) and of browsing SyMoGIH semantic categories to find the CoAc13562 identifier to use for the annotation (bottom).  TXM internal management of those annotations is equivalent to a re-import of the current pivot representation with the new annotations. After re-import (after saving annotations) the new annotations are available for all TXM tools to work on like any original “annotation” of the texts (with internal textual structures and their properties).   Concordance based word properties annotation The second service is based on the annotation of words of concordance pivots: a word present in the pivots  in TXM, pivots of concordances can be composed of a sequence of words. of a concordance can be annotated with any property. The primary goal of that service is to annotate and correct pos and lemma properties of words, but it can help to annotate any property at the single word level.  As an illustration, see figure 2 the correction of the “pos” property of some “vers.” words used in biblical references in Hobbes works lemmatized by Morphadorner (Burns, 2013).     Figure 2. TXM screenshot of a Concordance to set the “pos” property to the “n-ab” value of two occurrences of the \"vers.\" word, selected by their concordance line.  TXM internal management of those annotations is equivalent to a re-import of the current pivot representation with new annotations encoded in XML-TEI TXM at the word level.   Full text URS annotation in text edition The third annotation service is based on manual annotation of sequence of words inside text editions with elements of a Unit-Relation-Schema (URS) annotation model (Widlöcher & Mathet, 2009). URS type annotations are designed to encode complex discourse entities like co-reference chains in texts (Schnedecker et al., 2017). As an illustration, see figure 3 the annotation of the “ses loix” sequence of words with a URS unit of type MENTION, having its grammatical category to the value “GN.POS” and its referent to the value “les lois de la divinité”, in the first chapter of the 1755 edition of  De l'esprit des lois by Montesquieu.     Figure 3. TXM screenshot displaying the first page of an edition of  De l'esprit des lois highlighting in light yellow all URS units of type MENTION and in bold the unit currently selected (top window), and displaying the current unit properties value input form (bottom window): CATEGORY property at value “GN.POS”...  TXM import/export management services represent those annotations as XML-TEI stand-off annotations anchored to the word elements of the XML-TEI TXM representation of texts (Grobol et al., 2018).    Discussion By using a common XML-TEI TXM pivot representation for internal management of corpora for all the annotation services, TXM unifies transcription, encoding and annotation activities in a single framework. In this framework, annotations can represent manual (user), semi-automatic (machine+user) or automatic (machine) interpretation results used further for analysis and interpretation work. The reflexive nature of the resulting text analysis workflow is schematized in figure 4. Texts are first digitized by OCR, transcribed or converted from digital formats. They are then possibly philologically corrected and established through XML-TEI manual encoding. Then automatically processed by NLP tools while being imported into TXM to produce the TXM internal corpus model. Corpus analysis is then assisted by TXM tools applied to the corpus model. The pivot representation that gathers all annotations produced by annotation tools is figured as the node labeled « Pivot rep. » and the interpretation workflow itself is figured as a digital hermeneutic circle.    Figure 4. Digital hermeneutic circle integration into TXM.  Legend   blue box: manual annotation activity black box: tool   red box: automatic annotation activity green box: TXM corpus data model   purple disk: data representation black arrow: activity   green arrow: annotation equivalence       Conclusion All the new annotation services integrated into TXM are building a comprehensive annotation-based digital text corpora analysis platform. From an epistemological point of view, the integration in TEI of the different annotation models and tools into the platform helps its users to better define and trace what comes from the source corpus they analyze and what comes from their own or from others interpretation work. This work was funded by the ANR and the DFG under grant numbers  ANR-15-CE38-0008 (DEMOCRAT project) and  ANR-14-FRAL-0006 (PaLaFra project).   ",
        "article_title": " Coping With The Complexity Of The TXM Platform Annotation Services With A Unified TEI Encoding Framework  ",
        "authors": [
            {
                "given": "Serge",
                "family": "Heiden",
                "affiliation": [
                    {
                        "original_name": "ENS de Lyon, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "natural language processing",
            "theory and methodology)",
            "standards and interoperability",
            "English",
            "text encoding and markup languages",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  There are three groups of stakeholders, when it comes to research data: Those who make data, those who use data and those who build infrastructure to match those two. In the literature, we find a lot of research on how to build infrastructure and how to share data (often written by the same group of people), yet there is relatively little research (but see Caria and Mathiak, 2018, Kern and Mathiak, 2015, Porter, 2016, Tenopir et al., 2011,2015, Warwick et al., 2008) on what the third group, the users, or rather re-users, actually want and what they do. Most of these studies do also not focus on the Humanities. While for other area of studies, research data sharing and reuse through data archives or journals is far more institutionalised (for different subject culture, see Tenopir, 2011, 2015), this is not so in the subjects covered by Humanities.     Methodology   To study the practices and attitudes towards data sharing and reusing of researchers in the field of Humanities, we did an online survey on research data management practices and needs. It consisted of three sections on describing the data worked with or produced, reusing and sharing experiences, practices and attitudes, and knowledge and needs in the area of research data management. It was a follow up study of our research data management survey 2016 (Mathiak and Kronenwett, 2017) adding a part on reusing and sharing experiences, practices and attitudes that was partly adapted from a survey conducted by the Specialised Information Service Social and Cultural Anthropology (Imeri and Danciu, 2017).    The online survey was available between 06 June and 08 July 2018. It was conducted by the Data Center for Humanities (DCH), University of Cologne, in collaboration with the Cologne Competence Center for Research Data Management (C3RDM) and the Deans of the Faculties for Arts and Humanities and Human Studies.    Data Center for the Humanities (DCH),   http://dch.phil-fak.uni-koeln.de  , Accessed 27.11.2018; Cologne Competence Center for Research Data Management (C3RDM),   https://fdm.uni-koeln.de  , Accessed 27.11.2018.   It was consisted of 36 closed questions, categorial items always offered the possibility to add additional ones. The sample consists of 268 data sets, some of them did not answer all questions. The sample covers all subject groups and departments of two humanities faculties. For this paper, we are focussing on the questions on reusing and sharing experiences, practices, and attitudes.     Results   Reuse of Data  Over 80% of our participants indicate the scientific benefit of searchable and reusable research data with rather high/high/very high for their field of study (cf. fig. 1).     Figure 1: Rating of the scientific benefit of searchable and reusable data for the scholars’ field of study (N=240).   Purpose of reuse  There are three aspects being rated highest in relation to the individual field of study: Over 85% of the participants have specified that reuse of data is important for reconstructing results, generating questions and comparison with similar data. In contrast only 59% indicate they would reuse data for reconstruction purposes personally. And even generating new questions and comparison with similar data is rated lower within the personal perspective (cf. fig. 2).     Figure 2: Purposes, scholars would like to reuse data for (N=167).   Access to reusable data If researchers reuse data, only 22% have found it in an archive, while the more common way to find data is personal contact, either within the own research group (56%), personal contact (26%) or even complete strangers (34%) (cf. fig. 3). Over all less than 4% of our sample scholars categorically rejected the use of secondary research data.   Figure 3: Experience with secondary data use by source (N=219).    Handling research data  Only 34% have stored data in an archive, at least 72% consider doing so and only 0,5% cannot imagine storing data in an archive. Nevertheless only one quarter of the 34% that have stored their data in a data archive do so in a openly accessible way (cf. fig. 4).     Figure 4: Accessibility of stored data in data archives (N=70).    Reasons for not storing The main reasons for not storing data in a data archive is a lack of knowledge that this is possible (38%) and not finding an appropriate one (24%) (cf. fig. 5).   Figure 5: Reasons for not having used a data archive so far (N=142).    Conditions for an adequate archive  When asked what conditions an archive would have to fulfill in order for researchers to save data there, the most important factors rated highest in very important would be data security, followed by factors of archive’s confidentiality and professionality (nearly congruent). Looking of the combined important rates (very important/important/rather important) manageable effort for data curation, explicit agreements on licensing and usage, and the quotability of data are also ranked higher than 90%. Explicit (precise) agreements on licensing and usage of data came next. The next important factors were data must be clearly quotable and there should be specific security mechanisms for single information as well as coverage of the additional costs for data curation and storing by research funding organisations.   The factors that were rated least important (combining the important rates) even though still more than 70% indicated them as being very important/ rather important/ important, were sophisticated access restriction, information on who uses the data for what and indexing the data set in different systems (looked at least important of all items).    Figure 6: Rating of factors if considering archiving own data in a data archive (N=208).  If a data archive would fulfill all requirements, nearly 50% of all the scholars answered that all data should be stored. Surprisingly, all research data was ranked highest (cf. fig. 7). That indicates a basic willingness to store data.    Figure 7: What kind of data should be stored in a data archive - if it fulfills all requirements (N=213).    Information sources for the evaluation of data archives  Finally, even though data archives see themselves as information broker between producers and data reusers: if researchers decide on concrete archives to use, recommendations of colleagues and scientific organisations are most influential, followed by the popularity and reputation of the organisation that funds the archive. (cf. fig. 8). Networks seem to play the major role for choosing a data archive.    Figure 8: Important factors for choosing a data archive (N=215).      Conclusions   Sharing data is common and important in the Humanities, but that doesn’t mean that the data ends up in a data archive. Instead, most sharing happens in research groups, with personally known colleagues and even strangers, while data archives only broker 20% of the research data transactions. This is similar to what other disciplines have found (cf. Fecher et al, 2015 for an excellent survey of this topic). For those who could share, there is a conflict of interest between adding to the knowledge commons and self-interest.   In recent years, several policies have been set in place to encourage data sharing outside of the social network, e.g. the requirement of third-party funding agencies to submit a data management plan (European Commission, 2012, NSF, 2018) and through journal data submission policies (McCullough, 2009 and Savage and Vickers, 2009). Bibliometric studies have shown that sharing research data increases citation rate (Piwowar et al., 2007). Yet, in the humanities, these are not the decisive motivations for publishing data. Third-party funding is not as prominent as in the “hard” sciences and neither is pressure to publish in high-ranking journals or getting cited a lot. As a consequence, data archives cannot rely on scholars to seek them out for data deposits. But there are other options.  Scholarly communities are key when it comes to finding and sharing data, but too much of the data gets lost, due to insufficient storage policies. Data archives can help with storing and managing data, but they have to be integrated in the community as indicated by Fig. 8. Awareness of suitable archives is still not as high as one would like (cf. fig. 5), which also can be improved with community-based measures.  For our data center, we have decided to concentrate on the PhD students in our graduate school. What we have found is that they have quite different questions and problems than more senior scholars. While experienced scholars usually have a setup of tools and research data from previous projects as well as an established network of collaborators, PhD students have to cold-start their research in most cases. This gives us the opportunity to introduce them to the possibilities of re-using and sharing research data, while also educating them on digital tools and data management in general.  In our talk, we will introduce some more of the results from the survey and discuss more thoroughly the mechanisms of data sharing and non-sharing. We will also report on our experiences with addressing PhD students and discuss some of the other policies to raise awareness.  ",
        "article_title": "\"The Role Of Data Archives In The Humanities At The University Of Cologne\"",
        "authors": [
            {
                "given": "Brigitte",
                "family": "Mathiak",
                "affiliation": [
                    {
                        "original_name": "University of Cologne, Data Center for the Humanities (DCH), Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Katja",
                "family": "Metzmacher",
                "affiliation": [
                    {
                        "original_name": "University of Cologne, Data Center for the Humanities (DCH), Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Patrick",
                "family": "Helling",
                "affiliation": [
                    {
                        "original_name": "University of Cologne, Data Center for the Humanities (DCH), Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Jonathan",
                "family": "Blumtritt",
                "affiliation": [
                    {
                        "original_name": "University of Cologne, Data Center for the Humanities (DCH), Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "theory and methodology)",
            "English",
            "library & information science",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   This project aims to mine two centuries worth of digitised newspapers in four languages, and to propose a methodologically sound, reusable approach to carry out quality historical research on the changing vocabulary relating to nationhood. The newspapers stem from different sources and countries, and are available in different formats. Massive digitized newspaper collections are increasingly used to address historical questions through mining textual data.    For recent examples and further discussion see for instance Bos & Gifford (2016); Brandzæg, Goring & Watson (2018);   Buntinx, Bornet and Kaplan (2017  ). For a discussion of the role of digitization of newspapers for historical research see Cordell (2016); Milligan (2013).     They are more seldom used for comparative projects cross linguistic and national boundaries. In this paper, we address the methodological challenges the use of newspapers from different political contexts, languages and datasets poses, and lay out our approach to tackle a comparative study for the Netherlands, Finland, Sweden, and the UK.    Working with historical newspapers from different countries to look for the evolution of a concept poses several methodological challenges. A first problem is actually getting the data, and shaping it in a way that makes its use possible. For the UK, we use the Burney and Nichols collections and the British Library 19th Century Newspapers, both provided by Gale, and accessible through an API, OCTAVO.    Tolonen  et al   (2017).     The full texts of the Dutch newspapers, as well as their corresponding metadata, are retrieved through the Delpher API.     ,  accessed August 2018 . We would like to thank Dr. Steven Claeyssens at the Royal Library of the Netherlands. The original script used to query the API has been written by Juliette Lonij.     Finally, Finnish (including newspapers in Swedish from Finland) and Swedish newspapers are first queried through the KORP interfaces made available by the language banks of those two countries, respectively the Kielipankki     https://korp.csc.fi  ,  accessed August 2018.     and the Språkbanken,     ,  accessed August 2018.     and then fetched via their API. The full datasets were used for the UK, Sweden, and Finland. The colonial newspapers were filtered  out of the Dutch dataset, so as not to bias the comparative analyses.   The justification is twofold: first, only the Dutch dataset has an extensive coverage of colonial newspapers. Second, Dutch colonial newspapers “showed a great uniformity” because “their news supply was unique and controlled by the official news agency, ANETA”. (Our translation and paraphrasing of Witte 1998:18)     In addition, Finnish newspapers published outside the historical borders of Finland were also disregarded from our analyses.  After getting access to the different data and shaping it in a way that a single pipeline can be reused for all languages and historical realities comes the trade-off between the computational, distant reading of the text, and the actual research question. We focus on the process of nation-building in Europe, and to achieve that goal we utilise several methods. Whilst historical processes or concepts do not appear as such in texts, and thus cannot be the object of a mere tallying across time, it is obvious that words do. We thus use words as a proxy to study the process of of nation-building, and carry that out in several ways. In doing this we also limit the study of nation-building to the development in which the nation became a self-evident frame for social and political affairs.  As a first  step in exploring  this idea, we look at how bigrams   We borrow this methodology from Hill  et al  (2018), who studied the public sphere in 18 th  century Britain.    starting with the adjective “national”    Or  nationale ,  nationaal ,  nationell ,  nationella , and  kansallinen . For the sake of clarity, the remainder of this abstract will use English terms as examples. In the case of bigrams containing “meaningless” words such as conjunctions (eg:  nationell och ,  nationale en , “national and”), we expand the query until we arrive at the noun modified by the adjective. For English and Finnish, languages for which a surface form of the substantive shares the same spelling as the adjective, those occurrences of nouns are filtered out. Frequent compounds were decompounded and where needed harmonised, eg:  nationalbiblioteket   ->  national biblioteket ;  kansalliskirjasto   ->  kansallinen kirjasto , etc.     behave in our datasets, in terms of absolute and relative frequencies. This paints a picture of how common the idea of something “national” is mentioned in newspapers in different countries at different periods. We complement this picture with an analysis of the creativity and productivity     The definitions of “productivity” and “creativity” are fluid within subfields of linguistics, as already discussed in Lyons (1977: 77). In this paper, we use “productivity” in its corpus linguistics sense, i.e. the proclivity of a linguistic unit to be (re-)used. “Creativity”, on the other hand, will characterise this unit’s  new   forms: in the case of a bigram, any new bigram following the construction “ national + _ ” .      of the  national + noun   bigram: by looking at how “creative” writers are with the linguistic unit, and by looking at how its use evolves across time, we have a glimpse at the vocabulary of the nation, and can identify key junctures in the transformation of this vocabulary. We notice that the French revolution, the political ruptures of 1848 and the Franco-German War of 1870 were particularly important for the diversification in the vocabulary of “national”, in all of our cases, but can also show how local political and publishing conditions produced local reactions. The differences also point out how events abroad affected domestic vocabulary, making the development a transnational one (cf. Bos and Gifford 2016). By focussing on bigrams, we can trace the domains in which the word “national” was used. In doing so we do not trace the theoretical development of the concept of nation or even the intentional processes of shaping Dutchness, Britishness, Swedishness,  or Finnishness, but rather focus on a much more implicit process in which the nation became a natural frame for conceptualizing the societal issues or -- to speak with Benedict Anderson (2006) -- an imagined community that became inescapable for citizens of any state.   Approaching bigrams is limited to mere counts, and whilst it hints at change, it does not qualify it. To remedy this weakness, we cluster bigrams by themes, in two different ways: on the one hand, domain experts assign a theme to the top-300 bigrams. Those themes, viewed in a diachronic way, add more colour to the simple tallying of bigrams, and of the creativity and productivity of the construction. Analyses based on our manual annotation point toward a hypothesis that, on a general level, the vocabulary of “national” tended to be focused on economic discourse in the late eighteenth century, but soon gained a stronger presence in relation to political issues and ultimately also entered the domains of culture and social affairs during the course of the nineteenth century.  The other approach, which we believe will be a useful one for researchers wanting to reproduce our methodology, is data-driven, and should help reduce a researcher’s bias. Clustering words semantically in a data-driven way is a challenge. Indeed, most current approaches rely on topic models to assign some “sense” to documents, but not directly on words. On the other hand, relying on external knowledge, such as Wordnet -- a database that groups lexical items into “synsets”, i.e. synonym sets -- proves itself to be difficult, due to the varying quality of the OCR. Additionally, Wordnet does not allow for a more fine-grained relationship between words than a dichotomous answer to the question “are they part of the same synset or not?”. To circumvent those problems, we train word embeddings on the full texts of our corpora, and then calculate the “semantic” distances between each of the top 1000 nouns that appear next to “national”. As such, we believe this approach is similar to the one proposed by Wevers  et al   (2015). We then use k-means clustering on the 0.5 million different distances     Prior to the distance calculation, the vectors have been unit-normalised, which allows us to use k-means clustering. Embeddings were trained on tokens with a frequency threshold of 300, a CBOW architecture, 100 dimensions, and with a window of 5. Furthermore, mimicking Kim  et al   (2014), the embeddings were trained on different time slices, where embeddings for slice  t+1   are initialised with the embeddings for slice  t , hence bypassing the need for a “temporal” alignment of the vector space.      to generate semantic clusters of words. Each word is subsequently assigned a label -- its “centroid sense”, which allows us to look into the thematic distribution of “national” across time. An advantage of using word embeddings is that words with OCR errors also get distributional similarity. The weakness of such an approach is that only the primary sense of a word is captured, making the technique sensitive to frequency (Dubossarsky  et al   2018; Iacobacci  et al   2015). To make sure that the data-driven clustering is not mere chance, we attempt to replicate the results in two ways: first, we use SCAN  (Frermann and Lapata 2016), a dynamic topic model that infers, for a specific target word, word senses across time. Second, we calculate word mover’s distance (WMD, Kusner  et al   2015) between 11-grams (“KWICs”) in which “national” appears -- the same input as for  SCAN --, and cluster them using the same method as for the top words, in a move to go slightly beyond the “primary sense” limitation of the word embeddings. Data-driven clustering further confirms our hypothesis of the broadening domains of national, but, more importantly, also paints a clearer picture of how the politicization and culturalization of “national” took place. Further, it shows how the word national moved from being an abstraction of terminology such as “French”, “German”, “Dutch” to becoming indicative of a political community, and thus more often used similar to adjectives such as “public”, “common”, or “international” in referring to state institutions.   A further way of better qualifying our findings is using available metadata to zoom in on periods of instability and ruptures in creativity and productivity curves and tie these empirical findings to theories of semantic change (for a discussion see, Hengchen 2017: 11-24). Another way of making our analyses more precise includes a more linguistically- and culturally-aware preprocessing of the texts so as to go beyond the  national + noun   bigram: different cultures refer to a comparable reality differently (eg: Swedish  riksarkivet   (“archives of the kingdom”) vs English “national archives”; the same word is used in Swedish from Finland, but not in Finnish, despite Finland  not   being a monarchy). To successfully implement more advanced preprocessing, future work will rely on the comparative findings of the present study.  ",
        "article_title": " A data-driven approach to the changing vocabulary of the ‘nation’ in English, Dutch, Swedish and Finnish newspapers, 1750-1950  ",
        "authors": [
            {
                "given": "Simon",
                "family": "Hengchen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Ruben",
                "family": "Ros",
                "affiliation": [
                    {
                        "original_name": "Utrecht University",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Jani",
                "family": "Marjanen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "multilingual / multicultural approaches",
            "natural language processing",
            "theory and methodology)",
            "English",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In November 1675, Spanish writer Agustín de Salazar died leaving unfinished the play  La Segunda Celestina (SC), which he was writing on commission for the birthday celebrations of the Spanish Queen. The play was probably finished by an anonymous writer and performed the following year (Sabat de Rivers, 1992). In 1989, Guillermo Schmidhuber published a newly discovered ‘suelta’ of SC with the anonymous ending he claimed had been written by Sor Juana Inés de la Cruz (SJ), a prominent Hispanoamerican writer of the time, whom he also thought to have made significant changes to the original.    Literary hypothesis In 1700, the editor of the works by SJ, Castorena y Úrsua, mentioned that she finished and improved a literary text by Salazar. When her entire works were published in 1957, one of the editors, Salceda, connected Castorena’s mention with SJ referencing a comedy about Celestina in her own play  Los empeños de una casa, and concluded that the said play could be SC. Later, Schmidhuber tried to prove it making historical, linguistic and even primitive stylometric arguments (Schmidhuber de la Mora, 1991). After a comprehensive research, Georgina Sabat de Rivers concluded that although there was not enough evidence to make it a fact, it was highly probable that SJ indeed wrote the ending, but not edited the whole text (Sabat de Rivers, 1992).    Dataset In the course of our study, we found that availability of digitized Spanish texts, especially historic ones, poses a great problem, due to few resources or repositories, and poor state of digitization – for Spanish works it mostly means scanning images of early editions (especially manuscripts or old prints), and their typographic variance makes OCR results not very useful. As a result, our corpus was composed based on various sources. The text of SC was extracted from a digital edition (Schmidhuber de la Mora, 2016), and converted into plain text. Other dramatic works by SJ were extracted from the Cervantes Virtual Library. Salazar’s texts were much more difficult to find, as there are no digital editions. We had to use the image digitization of his texts offered by the ‘Biblioteca Digital Hispánica’. The OCR provided by the library and our software (ABBYY-FineReader 12) produced so many irregular errors that we decided to transcribe  El amor más desgraciado and  Más triunfa el amor rendido.  To place the problem in a broader perspective, we used the Canon-60 corpus (Oleza Simó, 2014), a collection of digitized Spanish Golden Age plays that includes canonical baroque works. However, this corpus is imbalanced – some authors are overrepresented, whereas for others – less famous or relevant for the literary history – there is only one play.  The final corpus combining the Canon-60, SJ’s and Salazar’s texts lacked balance in terms of genre (a mix of secular and religious plays, as well as comedies and tragedies), gender (only two women in our corpus: SJ and María de Zayas, showing that the relevance of other Spanish baroque female writers needs further studies), and, finally, nationality – all the authors are Spanish-born, except for SJ, born in ‘Nueva España’ (i.e. Mexico). We therefore limited our corpus to only one genre: ‘la comedia de capa y espada’.   Analysis  Works from New Spain in the Spanish Baroque perspective We approached the issue of verifying SJ’s authorship in a multi-step study, starting with a distant look on the literary surrounding of SC. With the primary network analysis conducted on the large corpus (Canon + SJ + Salazar) with Bootstrap Consensus algorithm for 100-1000 most frequent words, as implemented in the stylo package (Eder et al., 2016), we determined optimal settings granting stable results – deciding against using culling which completely distorted any authorial signal and relying on Cosine Delta distance measure. Importantly, network analysis confirmed some existing inspiration links between authors, e.g. SJ’s work bearing similarities to Calderón and Salazar, whom Paz cites as ones she mimicked in her youth; and unveiled new connections, e.g. to Antonio de Solís and Juan de Vera Tassis. What also supports claims of SJ extraordinary talent surpassing her times is the fact that while works of other authors cluster mostly together, her works span across the whole corpus.      Fig. 1 Network of all works in the corpus (each work divided into 2000 word samples). SJ is marked with black edges.   Strength of authorial signal and determining authorship Preliminary authorship attribution and verification tests showed very unstable classification results. For cross-validation with SVM, Delta and NSC, and verification with the so-called “Imposters method” (Kestemont et al., 2016; Koppel and Winter, 2014) varying on settings a number of candidates were recognized as the author of anonymous part – from Calderón and Moreto to SJ, Solís and de Vera Tassis.  We decided to examine the strength of authorial signals in our corpus, which led us to excluding those who could not author the anonymous part for objective reasons such as the time of its creating (e.g. Lope) or being hub authors – strongly connected to every text in the corpus (Moreto). Inspired by Eder’s evaluation of authorial signal in short samples (2017) and thanks to his courtesy in making the script from the study available to us, we conducted a series of evaluation tests on our corpus until we were left with two authors beside Salazar: SJ and Solís. Of the three considered authors SJ had the most stable signal (see Fig. 2-4).              Fig. 2-4 Accuracy of recognition of particular authors by classification algorithm. In the final part of our examination we once again performed cross-validated classification and verification on the small corpus consisting of one-genre works by the mentioned 3 authors against anonymous part of the text. As the anonymous part is only 4863 words, we used sequential sampling of 1000 words. In this case, SJ was attributed as the author in almost all settings (some of the results pointed to Solís influence in the last two thousand words), with the most reliable results produced by SVM and 100-500 MFWs scope (from 54.8% to 81.2% accuracy, with the average of 72.75%). Interestingly, parts of works by other authors were consistently misclassified as SJ, which might indicate either/both her domineering style or her taking inspiration from either of the authors, of whose works she must have been aware.   Editorial influence in the non-anonymous part and the ending This problem of authorship requires detecting multiple authorial voices as we know for sure Salazar wrote significant part of the play before other person finished it, which is why we apply Rolling Classify (Eder, 2016) to detect authorial takeovers. This allows to discover both who authored the ending, and if this author made significant changes to the rest of the play. We marked two important points in the whole play: mark ‘b’ represents the place at which, according to Vera Tassis, Salazar left the play unfinished. As it can be observed in the figures 5, 6 and 7, the ending is attributed to SJ in SVM and NSC, and to Solís in Delta. The most surprising thing is that Salazar’s signal is not detected at all, which may be related to the weakness of his signal detected in previous analyses, and Sor Juana seems to dominate the rest of the play. Could it be that, if it was SJ who finished the play, she altered the rest of the text to the extent that we are not able to see Salazar anymore? This would confirm the claims of some of the scholars who defend her authorship (Schmidhuber de la Mora, 2016; Paz, 1990). We also marked the beginning which portrays the first encounter between protagonists: doña Beatriz and don Juan, with the mark ‘a’. It is retold and alluded to several times in the play, something unusual in Golden Age theatre. The actual first encounter is a very feminist confrontation, later in texts belittled in don Juan’s retelling. It seems to betray a female writer which is why we examined it. As it can be observed in the figures, in all tests this fragment is attributed to SJ.                Figures 5-7. Rolling SVM, NSC and Delta on SC. 500 MFW and 5000 words per slice.    Conclusions    Our experience emphasizes the need for and usefulness of taking corpus evaluation steps in all analyses, and especially in the case of historic works, for which it is impossible to create a truly balanced corpus. Various authors seem important for the text and the situation is quite blurry. Second best candidate, well above chance level, Solís, is a new discovery, and his possible relation to the SC and SJ, especially in terms of influence or themes, may also be of interest to future studies. However, quantitative analysis and literary evidence history show that the influence of SJ is definitely the strongest, supporting the theory of her being the author of anonymous part and editor of the whole text.    Further Materials See our corpus, evaluation of the OCR difficulties which led us to transcribe the texts, and list of plays in our GitHub:     ",
        "article_title": " Challenging Stylometry: The Authorship of the Baroque Play La Segunda Celestina  ",
        "authors": [
            {
                "given": "Laura",
                "family": "Hernández Lorenzo",
                "affiliation": [
                    {
                        "original_name": "University of Seville, Spain",
                        "normalized_name": "University of Seville",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/03yxnpp24",
                            "GRID": "grid.9224.d"
                        }
                    }
                ]
            },
            {
                "given": "Joanna",
                "family": "Byszuk",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language, Polish Academy of Sciences",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "gender studies",
            "English",
            "spanish and spanish american studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Pre-Conference Workshop “DLS Tool Criticism. An Anatomy of Use Cases”    The current panorama in DLS presents a plethora of tools, protocols and practices for processing, analysing and visualising data. This diversity of practices and tools originating from different areas (e.g., stylometry, NLP, literary studies, corpus linguistics) has resulted in a rich, but atomised situation.  Using a broad definition of “tool” understood as method, the ADHO-Special Interest Group “Digital Literary Stylistics” (SIG-DLS) organizes a workshop that taps into the  DLS Tool Inventory (DLS-TI), which is a first attempt to gather information on the practices of the various traditions present in DLS. The DLS-TI features methods and suites for data analysis, including desktop GUIs, online Virtual Research environments and libraries for R or Python, as well as general purpose tools such as Excel spreadsheets. As tools have the power to reify theoretical a prioris (Jannidis & Flanders, 2015; McCarthy, 2005), the community needs a handle for gauging their validity, applying a sense of craft (Piper, 2017) from the perspective of tool criticism (van Es et al., 2018; Koolen et al., 2018).   Building on the DLS-TI, in concert with other initiatives ( LRE-map, Calzolari et al., 2012;  DIRT directory;  Catalogue of Digital Editions;  IDE), we aim at taking stock, but also reflect on, our methods. Three use cases representing different types of digital tools will undergo an “anatomy”: Textométrie, Stylometry, and Semantic Text Mining. Three scholars will each present a use case, elucidating (a) reasons for choosing the method; (b) the method’s impact on the analysis and literary modeling; (c) advantages and limitations.   The discussion will also address traditions of Digital Literary Stylistics between “digital” (computational linguistics, text mining, corpus linguistics) and “analog” (formal and subjective approaches), addressing the fit of data and method to literary modeling (Piper, 2019; Underwood, 2019; cf. Da, 2019).  Anatomy of tools: A closer look at “textual DH” methodologies Based on what has emerged so far from the DLS-IT and further observations of research practices, we have identified a three groups of tools that can be covered in this half-day workshop and will be represented by an exemplary use case:   1.Textométrie Textometry is a traditionally French approach to statistical text analysis, often based on methods such as Correspondence Analysis, which has produced a number of tools, alongside a productive body of research in the domain of stylistics and corpus linguistics. Textometry and stylistics: which tools and practices for literary interpretation? ( Clémence  Jacquot, Université Paul-Valéry Montpellier 3, France)   “Textometric tools” are widely used by researchers to explore literary corpora. This session intends to propose a  critical feedback of experience on a stylistic analysis guided by a textometric exploration of Apollinaire's poetic corpus with TXM. Several issues will be discussed: first, how to analyze in concrete terms the evolution of the poetic writing of a single author, Apollinaire, between 1898 and 1918, from a diachronic perspective? In a contrastive study, which are the advantages of TXM. We propose to review the importance of specificities calculations and the various visualizations proposed (AFC, for example), as well as the corpus scores allowed by an annotation of the structural units of the poetic corpus. Finally the methodological and epistemological contributions of a tool such as TXM for stylistic analysis will be discussed from a critical point of view. What observable results does it provide? What does it make visible? How to interpret the salience of certain results? What silence can he throw on other stylistic points of the text?     2. Stylometry  Stylometry uses a series of tools and methods for the statistical analysis of style, based on advanced calculations on word frequencies, including multi-dimensional measurements and machine learning techniques. Their main applications have been both authorship attribution and distant reading. Initially developed through the use of spreadsheets, they have been fully implemented into programming languages such as R and Python, and integrated by a wide variety of visualizations, derived from research fields such as philogenetics and network theory.  Less than countless. Options to move beyond word counting in stylometry   (Mike  Kestemont, University of Antwerp, Belgium)  In a fairly dramatic critique of computational literary studies, Nan Z. Da recently made a controversial case against the application of quantitative methods to literary texts. She argues that much work in this field essentially boils down to \"counting words\". This view is somewhat reductive but not without merit: it certainly applies to much of the present-day approaches that are dominant in stylometry and, consequently, to many of the tools that are available. While this methodological focus (if not poverty) is to some extent justified by previous empirical work, I will reflect on under-explored options for stylometry to move beyond naive word counting. Stylometrists, for instance, often take pride in the fact that their tools typically work on raw texts that require little preprocessing. In this, stylometry ignores much of the achievements of literary theory in the twentieth century, such as the importance of focalization (perspective) or the (actual, individual) reader. Richer (pre)processing pipelines, that also tap into syntax and discourse, might allow stylometry to revitalize its connection with literary theory, but comes with significant barriers for non-Anglo-Saxon literatures. In this talk, I intend to review some of the less conventional work in stylometry that leads the way in this respect.   3. Semantic Text Mining   Semantic Text Mining applies tools for text analysis and visualization based on semantically enriched and co-occurrence methodologies, such as sentiment analysis, topic modelling, and word embeddings. They offer the potential of addressing key questions in literary theory and narratology, from the identification of genre to the visualization of plot. These emerging approaches are now beginning to broaden the scope of computational literary studies and to open up new, still-unexplored potentialities. LDA Topic Modeling for Semantic Corpus Analysis ( Steffen   Pielström, Würzburg University, Germany)  Topic models based on Latent Dirichlet Allocation (LDA) and Gibbs Sampling are a tool for exploring and analyzing the content and semantic structure of digital text corpora that has become popular in digital humanities research in recent years. They allow researchers to model a corpus' content in terms of so called \"topics\", groups of apparently semantically related words, and show the distribution of these topics within the corpus. Thanks to an increasing number of available tools and libraries, the method is, by this day, accessible to a wide range of users. In contrast to this technical accessibility, the methodology of topic modeling is rather intricate, and users cannot generally use them without making a number of decisions that require some deeper understanding. Additionally, there are aspects of topic modeling that are still in need of systematic methodological research. The session will give a hands-on introduction on goals and method of LDA topic modeling, demonstrate how to experiment with topic models using a simple desktop tool, and address open methodological issues.    4. Discussion  During the discussion we will pose questions on methodological and epistemological levels – including humanistic enquiry vs. data science, explorative vs. confirmative, and qualitative vs. quantitative approaches; as well as the range of research questions, from text similarity to aesthetic effects. The aim of each use case-anatomy is to give an overview of the usability and strengths of the tool (group of tools) in research, as well as pointing out problems and formulating specific avenues for further development. Results will be documented on a special SIG-DLS webpage. Through this, we will produce a guide for DLS-scholars’ orientation, as well as the beginning of a roadmap for further tool development.  The target audience is scholars interested in methods/tools and their linkage to literary modeling. We welcome newbies who look for initial orientation, old hands who wish to progress methodological development (“application”), and any scholars interested in modeling, i.e. epistemological aspects of style-tool criticism (“theory”).  Calzolari, N., Del Gratta, R., Francopoulo, G., Mariani, J., Rubino, F., Russo, I. and Soria, C. (2012). The LRE Map. Harmonising Community Descriptions of Resources. Proceedings of LREC 2012, Eighth International Conference on Language Resources and Evaluation. Istanbul, Turkey, pp. 1084–1089  http://lrec.elra.info/proceedings/lrec2012/pdf/769_Paper.pdf  Da, N. Z. (2019). The Computational Case against Computational Literary Studies. Critical Inquiry, 45(3), 601–639.  https://doi.org/10.1086/702594  Es, K. van, Wieringa, M. and Schäfer, M. T. (2018). Tool Criticism: From Digital Methods to Digital Methodology. Proceedings of the 2Nd International Conference on Web Studies. (WS.2 2018). New York, NY, USA: ACM, pp. 24–27 doi: 10.1145/3240431.3240436. http://doi.acm.org/10.1145/3240431.3240436  Franzini, G. (2012). Catalogue of Digital Editions Zenodo doi: 10.5281/zenodo.1161425. https://zenodo.org/record/1161425#.XCoDJhNKh8c (accessed 31 December 2018).  Flanders, J., & Jannidis, F. (2015). Flanders, J., & Jannidis, F. (2015). Knowledge Organization and Data Modeling in the Humanities. A Whitepaper.  https://www.wwp.northeastern.edu/outreach/conference/kodm2012/flanders_jannidis_datamodeling.pdf   Koolen, J., Gorp, M. van and Ossenbruggen, J. van (2018). Lessons Learned from a Digital Tool Criticism Workshop. Proceedings from DH Benelux 2018. Amsterdam, The Netherlands. McCarty, W. (2005). Humanities Computing. London and New York: Palgrave. Piper, A. (2017). Think Small: On Literary Modeling. PMLA, 132(3), 651–658.  https://doi.org/10.1632/pmla.2017.132.3.651  Piper, A. (2019). Enumerations: data and literary study. Chicago: The University of Chicago Press. Underwood, T. (2019). Distant horizons: digital evidence and literary change. Chicago: The University of Chicago Press. ",
        "article_title": "Pre-Conference Workshop “DLS Tool Criticism. An Anatomy of Use Cases”",
        "authors": [
            {
                "given": "J. Berenike",
                "family": "Herrmann",
                "affiliation": [
                    {
                        "original_name": "Basel University Switzerland, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Francesca",
                "family": "Frontini",
                "affiliation": [
                    {
                        "original_name": "Université Paul-Valéry Montpellier 3, France",
                        "normalized_name": "Université Paul-Valéry Montpellier",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/00qhdy563",
                            "GRID": "grid.440910.8"
                        }
                    }
                ]
            },
            {
                "given": "Simone",
                "family": "Rebora",
                "affiliation": [
                    {
                        "original_name": "Verona University, Italy; Basel University Switzerland, Switzerland",
                        "normalized_name": "University of Verona",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/039bp8j42",
                            "GRID": "grid.5611.3"
                        }
                    }
                ]
            },
            {
                "given": "Jan",
                "family": "Rybicki",
                "affiliation": [
                    {
                        "original_name": "Jagiellonian University, Kraków, Poland",
                        "normalized_name": "Jagiellonian University",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/03bqmcz70",
                            "GRID": "grid.5522.0"
                        }
                    }
                ]
            },
            {
                "given": "Anne-Sophie",
                "family": "Bories",
                "affiliation": [
                    {
                        "original_name": "Basel University Switzerland, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "stylistics and stylometry",
            "natural language processing",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Narrative perspective is the fascinating, but poorly understood in linguistic terms, phenomenon whereby literary texts often present events through the eyes, or minds, of a character in the story. (1), from Jane Austen’s Emma, for example, is presented from within Emma’s consciousness: (1) The hair was curled, and the maid sent away, and Emma sat down to think and to be miserable. – It was a wretched business, indeed! – Such an overthrow of everything she had been wishing for. – Such a development of everything most unwelcome! – Such a blow for Harriet! – That was the worst of all. Narrative perspective is even more difficult to investigate in texts written in ancient languages, such as Ancient Greek, for at least two important reasons: one, the absence of the stylistic device of Free Indirect Discourse which is one the main ways of manipulating narrative perspective in modern literature (Bary and Maier, 2014); two, the impossibility of querying native speaker intuitions about fine nuances of meaning and use. In response to these difficulties, in this paper we present a quantitative approach to studying narrative perspective in the Attic historian Thucydides, an author who today is still highly esteemed for his dramatic uses of perspective shifts. His manipulation of perspective is perceived as very subtle and nuanced and classicists up to today have tried to get a grip on it. (Recent studies include Bakker, 1997; Gribble, 1998; Grethlein, 2013a, 2013b; Allan 2013, 2018.) We provide a new quantitative approach to this question based on statistical analyses of the distribution of vocabulary in direct speech reports (“character text”) and outside of such reports (“narrator text”). The grounding of narrative perspective in the use of language at the level of lexical choice has been observed in narratology and (text) linguistics (cf. Fludernik, 1993; Sanders, 1994; Dancygier, 2012; Eckardt, 2015, to mention but a few important studies), but quantitative studies are rare. The most important contribution for Ancient Greek in this respect is constituted by de Jong’s (2001 and 2004) seminal narratological studies of Homer. As one element of her analysis, de Jong contrasts “character language” with “narrator language” in the Homeric epics. “Character language” comprises expressions that appear predominantly in speeches and rarely in narrator text. They are typically charged, evaluative or emotive, words. Their scarcity outside speeches contributes to the impression of Homer’s narrator as impersonal and objective. When words from character language do - infrequently - appear in narrator text, they still are to be interpreted as conveying a character’s perspective, and they are essential to the creation of certain narrative effects. This may happen in indirect discourse, as ἀλείτης ‘the sinner’ in (2), but also outside speech representation, as ἐνηής ‘gentle’ in (3): (2) φάτο γὰρ τίσεσθαι  ἀλείτην ‘he (Menelaos) thought to himself that he would take revenge upon the  sinner’ Homer, Iliad 3.28  (3) κλαίοντες δ᾿ ἑτάροιο  ἐνηέος ὀστέα λευκὰ ἄλλεγον  ‘weeping, they (the Greeks) collected the white bones of their  gentle companion Patroclus’ Homer, Iliad 23.252-3  Both ἀλείτης ‘the sinner’ and ἐνηής ‘gentle’ are to be evaluated from the character’s rather than narrator’s perspective, as de Jong (2004) argues on the basis of their distribution in the text. We apply a similar analysis to Thucydides. However, we proceed in a highly automated way: rather than deciding first which words are potentially interesting evaluative and emotive words and then counting their occurrences in narrator and character text (as de Jong did), we identify character language by looking at the relative frequencies of all words within and outside character speeches and identifying those whose distribution is the most skewed. They need not correspond only to highly charged vocabulary that a narratologist conducting a manual analysis would identify beforehand. This procedure makes it possible to uncover even subtler perspectival effects achieved by the narrator with the use of expressions that could intuitively seem entirely ‘descriptive’ or non-perspective-sensitive. For our analysis we preprocessed the text in the following way. We created a lemmatized version of the text, so that we could retrieve frequencies of lemmas rather than inflected wordforms. The lemmatization was done with the help of GLEM, operating by combination of lexicon look-up and memory-based learning, which has been found to out-perform, for Ancient Greek, lemmatizers using only one of these components (Bary et al., 2017). We divided the text of Thucydides’ Histories into separate files corresponding to chapters in modern editions, and segregated those files into three subsets: CT - character text, containing passages of direct speech, QT containing (purported) quotations made by Thucydides from existing documents and literary sources, and NT - all the rest, i.e. narrator text. We disregarded QT as passages in which lexical choice was not in Thucydides’ full control. (CT, by contrast, contains speeches which, even if based on real utterances, were written up and stylized by the historian.) Splitting  Histories into separate chapter-files and two subsets meant treating the text of Thucydides as a corpus divided into two sub-corpora, which allowed the application of corpus-like methods.  Our investigation proceeded in two steps. First, we calculated relative frequencies of all lemmas in the sub-corpora and compared those that occurred in both, looking to identify those that are importantly more frequent in CT than NT, but do sometimes appear in NT, as these would be, by hypothesis, the character language words that may contribute to narrative perspective. We assessed the differences in distribution of lemmas between CT and NT using log-likelihood ratio (cf. Dunning, 1993; Rayson and Garside, 2000) and ranked them according to how strongly skewed the distribution was in favor of CT. Lemmas that exhibit a most skewed distribution can be called “character language” words; they are those words that appear predominantly in character speech, and as such they are the precise object of our interest here. We then proceeded to categorize and analyze the NT occurrences of the top ten of highest-ranked lemmas (excluding proper names, function words, extremely rare and extremely frequent words) focusing on their role with respect to narrative perspective in the textual context in which they appear. Table 1: Top 10 character language lemmas (content words only)    Lemma # NT  # CT  Nrel Crel Ratio Crel:Nrel  LLratio Crel:Nrel    1 δίκαιος  dikaios ‘just’  14     50     1.2     15.9     13     93.2       2 ἀδικέω  adikeō ‘do wrong’  44     74     3.8     23.5     6.1     93       3 ἀγαθός  agathos ‘good’  68     81     5.9     25.8     4.3     76.5       4 χρή  khrē ‘should be’  49     66     4.3     21     4.9     69.7       5 κίνδυνος  kindunos ‘danger’  41     59     3.6     18.8     5.3     65.9       6 ἀμύνω  amunō ‘defend’  46     62     4     19.7     4.9     65.5       7 ἀρετή  aretē ‘virtue’  8     31     0.7     9.9     14.1     59.6       8 ἴσος  isos ‘equal’  37     51     3.2     16.2     5     54.9       9 αἰσχρός  aiskhros ‘shameful’  6     27     0.5     8.6     16.4     54.6       10 πάσχω  paskhō ‘suffer’  35     49     3     15.6     5.1     53.5       Second, we hypothesized that character language lemmas may cluster together in the text in passages that are especially perspectivally rich or dramatic. To test that we treated each chapter of NT as an individual document and ranked them based on how many occurrences of most salient character language lemmas they contain, relative to size (a set of such lemmas was identified through application of additional filters excluding extremely rare and extremely frequent words; additionally the lemmas were weighted based on the LL ratio of their CT vs NT frequency). We then studied the content of the highest-ranking chapters with respect to their narrative mode and perspectival effects. We found that character language lemmas in NT (i.e. in Thucydides narrator text) tend to occur predominantly in indirect reports, both at the level of individual occurrences and chapters - this is to be expected, as indirect reports attribute thoughts and words to the perspective of a character rather than that of the narrator. More importantly, we also found that where character language appears in NT outside of reportative contexts it is also most often used to express a perspective or mode different than that of the default objective narrator - either the perspective of a character (in context that can be described as “focalized” in narratological terms) or that of Thucydides commenting on the events or on general truths. Furthermore, we found that our automated and quantitative approach allowed us to identify as character language words that would not  prima facie stand out as evaluative or subjective in any way, but are in fact used by Thucydides in a perspectivally “charged” way. One prominent example is κίνδυνος  kindynos, meaning ‘danger’, as in the following passage, where it refers to Nikias’ reasoning and therefore his perception of potential dangers.  (4) ...ὁ δὲ τὰ κατὰ τὸ στρατόπεδον διὰ φυλακῆς μᾶλλον ἤδη ἔχων ἢ δι᾿ ἑκουσίων  κινδύνων ἐπεμέλετο. [Nikias] attended to the affairs of his army, keeping it from this time on the defensive to avoid any unnecessary  dangers.’ Thuc. 7.8.3  In conclusion, we saw that quantitatively identifiable “character language” contributes to narrative perspective in Thucydides text, and our method allowed us to identify otherwise elusive aspects of the historian’s narrative style. Moreover, our findings are important for linguistic theories of perspective-sensitivity, suggesting that it may be a matter of pragmatics and patterns of use at least as much as of semantics. As much as being a study of Thucydides, this paper provides a proof of concept for a method of addressing narratological questions with the use of quite simple, but powerful quantitative corpus-based techniques. ",
        "article_title": "Quantifying narrative perspective in Ancient Greek: Narrator language and character language in Thucydides",
        "authors": [
            {
                "given": "Leopold",
                "family": "Hess",
                "affiliation": [
                    {
                        "original_name": "Radboud University Nijmegen, Netherlands, The",
                        "normalized_name": "Radboud University Nijmegen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/016xsfp80",
                            "GRID": "grid.5590.9"
                        }
                    }
                ]
            },
            {
                "given": "Corien",
                "family": "Bary",
                "affiliation": [
                    {
                        "original_name": "Radboud University Nijmegen, Netherlands, The",
                        "normalized_name": "Radboud University Nijmegen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/016xsfp80",
                            "GRID": "grid.5590.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "semantic analysis",
            "stylistics and stylometry",
            "English",
            "literary studies",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Is an animal a person? The question is far from idle; it is in fact fraught with urgent ethical and legal consequences for both animal and environmental rights and shapes scientific norms for the study of animal behavior. It remains a constant theme in Western philosophy from Rene Descartes through present-day Critical Animal Studies. However, lawyers, philosophers, and ethologists are not the only deciders in this question: cultural representations of animals also mediate their relation to personhood. Fiction, for instance, excels in the representation of human individuality, interiority, and action; complex, “round” characters of course populate the long history of prose fiction. How, then, does fiction engage with the personhood of animals? In fiction, is an animal a  character? What do animals do in the pages of fiction? Do they make decisions, have feelings, express interiority? Do animals function more similarly to human characters, or to things, objects, and machines?  In what follows, we approach these questions with computational methods in one of the first attempts to apply digital methods to animal studies.    For an experiment on biodiversity archives and species extinction, see Ursula Heise,  Imagining Extinction: The Cultural Meanings of Endangered Species  (Chicago: University of Chicago Press, 2016), 55-86.   In a variety of corpora—from popular natural history to scientific writing about animal behavior to animal-driven fictions historically accused of anthropomorphism—we compare the semantic and syntactic footprints left behind by animals and humans. We discover that, from a computational standpoint, animals in fiction are indeed recognizable as characters, albeit characters who register intentionality through physical movement over speech and display a mental paradigm delimited by instinct and associative learning. Natural history writing, on the other hand, narrates animals in ways that seem surprisingly human-like when compared to animal representations in fiction more broadly.    Corpus Around the turn of the twentieth century, reading audiences in the United States developed a penchant for fiction about nonhuman animals. These so-called “wild animal stories” appeared in popular magazines and short story collections by writers such as E.T. Seton, Charles G.D. Roberts, William J. Long and Jack London. Despite their popularity, the stories proved to be a flash-point for scientific controversy over the appropriate way to narrate animal behavior. Over a period of several years, prominent public intellectuals—including the naturalist John Burroughs and then-president Teddy Roosevelt—would attack the writers of wild animal stories for attributing inaccurately human-like qualities to their nonhuman characters. The entire debate would come to be known as “The Nature Fakers” episode.    For a detailed literary-historical account, see Ralph Lutts,  The Nature Fakers: Wildlife, Science, and Sentiment , Fulcrum Publishing, 1990.   Though concerned a minor and moderately pulpy literary sub-genre, the controversy marks a moment when science and fiction butt heads over the meaning and appropriateness of anthropomorphism, as natural historians attempt to police the limits of fictional character. This corpus thus offers a high-stakes proving ground for the very concept of anthropomorphism in narrative fiction and science writing.  We have assembled 54 texts from the writers involved in this controversy, comprising short story collections and novels by eight of the most prominent animal story authors, published between the 1870s and the 1930s and concentrated in the first two decades of the twentieth century when the Nature Fakers debate was at its most intense. As a point of comparison for these fictions, we have also assembled 17 of John Burroughs’ natural history monographs, which detail the doings of wild animals in the idiom of popular science. Finally, we selected approximately 400 American novels published between 1870 and 1930 as a control corpus with no particular interest in nonhuman animals.    These 444 novels derive from two sources. For the late nineteenth century, we turned to a collection of about 325 American novels published between 1875 and 1905. Compiled by Marissa Gemma, these texts were selected based on their inclusion in the  Annals of American Literature  (Richard Ludwig and Clifford Nault, Oxford: Oxford University Press, 1989) and their availability in Project Gutenberg. Twentieth-century novels are those compiled by Mark McGurl and Mark Algee-Hewitt in “Between Canon and Corpus: Six Perspectives on 20th-Century Novels,”  Stanford Literary Lab  8 (2016), https://litlab.stanford.edu/LiteraryLabPamphlet8.pdf.      Methods Our digital methods collect the words that characterize animals, humans, objects in fiction. We base our methods on BookNLP, a Java program which clusters fictional character names together (“Elizabeth” and “Elizabeth Bennet”), and then collects the words associated with each character in specific ways: verbs for which each character is either a subject (“Elizabeth wondered”) or an object (“Mrs. Gardiner ... reminded Elizabeth”); as well as the nouns each character possesses (“Elizabeth’s wishes”), the adjectives attributed to each character (“Elizabeth was watchful”), and all words spoken by the character in moments of dialogue.    David Bamman, Ted Underwood, and Noah Smith, “A Bayesian Mixed Effects Model of Literary Character,”  ACL 2014 , http://www.cs.cmu.edu/~ark/literaryCharacter/. For a recent application of BookNLP to historical fictional practices of gendering human characters, see the recent article by Ted Underwood, David Bamman, and Sabrina Lee, “The Transformation of Gender in English-Language Fiction,”  Cultural Analytics  (Feb 2018), DOI: 10.31235/osf.io/fr9bk.   We applied BookNLP to our corpus of wild animal stories, and then annotated its returned characters for whether they were human or animal in 15 of the cleanest texts. BookNLP was in general accurate at identifying named animal characters—for example, in London’s  White Fang, BookNLP failed to recognize only one repeatedly occurring named character, White Fang’s father One Eye. The results of our annotations are recorded below in Table 1.    Author #Stories #Words in stories #Chars (Animal) #Chars (Human) #Words (Animal) #Words (Human) #Words (per char) %Effect on Totals   Ernest Seton 5 162,595 70 59 6,830 4,072 85 42.2%   William Long 3 117,069 21 5 768 684 56 5.6%   Charles Roberts 2 129,408 0 6 0 1,394 232 5.4%   Harriet Miller 2 113,588 2 6 76 502 72 2.2%   Jack London 2 103,676 20 26 4,049 5,107 199 35.4%   Clarence Hawkes 1 33,784 3 5 1,372 1,009 298 9.2%    Table 1 . Statistics regarding the small corpus of 15 texts whose BookNLP-identified characters have been annotated for their species (whether human or animal). Sorted by the number of stories they contribute, these authors vary widely in the number of word-to-character associations they generate. For instance, although William Long contributes more stories, words, and characters than Jack London, his effect on the aggregate data is smaller (6% to London’s 35%): this is because he attributes on average only 56 words per character, whereas London attributes on average 199 words to each character. This is likely owing to Long’s heavier usage of non-proper nouns, often referring to characters as “the mother,” “the cub,” etc.  BookNLP has the obvious limitation of overlooking unnamed characters—both human and animal alike. To address this limitation, we designed a Python program to extend BookNLP’s logic to all nouns. Our program collected approximately the same information: the verbs of which each noun is a subject or object; the nouns it possesses; and the adjectives it is modified by.    Our Python program ran the corpus through spaCy, a syntactic dependency parser for Python, and collected all moments when a noun had a syntactic relation to another word in a sentence of any of the following kinds (shown in parentheses are the syntactic dependency tags): subject (nsubj); passive subject (nsubjpass); direct and indirect objects (dobj and dative); possessives (poss); and modifiers (amod [adjective], compound [noun compound], appos [appositive], and attr [predicate]). We used SpaCy's default English-language model (en_core_web_sm), trained on web discourse. That this model was trained on the hyper-modern form of language of the internet is of course regrettable, given the historical material of our project, but unfortunately this is common and largely unavoidable problem in literary text mining.   We then produced a lists of nouns for animals and humans, which were drawn from the Harvard General Inquirer lists of those names.    Roger Hurtwitz, “General Inquirer Home Page,” http://www.wjh.harvard.edu/~inquirer/Home.html (accessed July 22, 2018). Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith and Daniel M. Ogilvie,  The General Inquirer: A Computer Approach to Content Analysis  (Cambridge: MIT Press, 1966).   Uncertain or ambiguous entries were pruned, reducing an original list of 930 words to a refined list of 371 words.    For example, the refined list of 287 human words included words like detective, ambassador, pope, freshmen, executive, commoner, manager, scientist, and human; the refined list of 96 animal words included words like turtle, owl, shark, oxen, grouse, roachback, moth, crow, hare, and jackrabbit.   Whenever a noun from these lists appeared in the parsed sentences of the corpus, we record its appearance along with its associated syntactic relation.  Finally, to determine whether a word is (statistically) significantly more likely to associate with a human or animal character, we conduct a Fisher’s exact test on a 2x2 contingency table: (# of times a given word appears in the context of an animal/human character) x (# of times an animal / human character appears.) This weights our expectation for the number of times words appear by the number of times characters appear. The Fisher’s exact test returns an odds ratio (the odds of a word’s human association / its odds of animal association) along with a p-value. Words with a p-value of less than 0.1 are shown in the results below.   Results Due to limited space, we present only a brief summary of some of our findings in experiments that draw on BookNLP and named characters (Experiment 1) and on our own program’s collection of words associated with animal and human nouns (Experiment 2). Finally, we apply machine learning techniques to assess the overall distinctiveness of human and animal nouns (Experiment 3).  Experiment 1: Comparing how animal and human characters are narrated in the Wild Animal Stories using Book NLP As seen in Figure 1, human characters are more likely to be the subject of speaking verbs such as “asked,” “called,” “cried,” “announced,” and, most especially, “answered.” Even in these wild animal stories, animals do not (as a rule) speak. Instead, animal characters in these texts are far more likely than humans to register intentional response through embodied activity--such verbs as “fought,” “bristled,” and “licked.” Emotionally, these results suggest that animals are characterized more by aggression (“hated”), humans by sociability (“love”).   Think of London’s dog heroes, who only learn to love toward the end of their stories, when paired with the appropriate human master, but who never experience love in the company of other dogs.   More subtly, animals “learn” and “know” things, while a human character is more likely to have “thought,” a result that could suggest something about the prevailing paradigm of animal mental life, characterized by instinct and associative learning, rather than rational reflection.     Figure 1: The verbs for which animal and human characters most often act as subject. Shown are words that are statistically significantly distinctive of animal or human characters (p<0.1). The farther to the left, the more strongly a word is associated with animal characters; the farther to the right, the more to human characters.   Experiment 2: Comparing how animal nouns are narrated differently in Wild Animal Stories and Natural History writing Our comparison of Burroughs’ natural history writings with the Wild Animal Stories does not, on the face, suggest that animals are any less person-like in Burroughs anti-anthropomorphic paradigm than they are in fiction. Indeed, as seen in Figure 2 below, Burroughs’ animals are statistically more likely to “know,” “make,” and “become” than their fictional counterparts. Many of the most distinctive active verbs between these two corpora might be explained by their relative interest in different species--for example, Burroughs writes about birds more frequently, so his animals are more likely to “fly” and “sing.” On the other hand, the strong influence of Jack London in the fictional corpus results in a high incidence of sled dogs, who are statistically more likely to “draw” a sled.    Figure 2: The verbs for which animal nouns most often act as subject. Shown are words that are statistically significantly distinctive of wild animal stories or of natural history writing (p<0.1). The farther to the left, the more strongly a word is associated with the former genre; the farther to the right, the more to the latter.   Experiment 3: Classifying human and animal nouns in wild animal stories, natural history, and novels The computer had only a slightly harder time distinguishing between humans and animals in the fictional corpus than in Burroughs’ natural history texts, with median accuracies at 70% and 71% respectively. Comparatively, when fiction with no particular interest in non-human animals is added to the mix, the computer was able to distinguish humans from animals with a median classification accuracy of 82%. This discrepancy suggests that, contrary to the terms of the Nature Faker debate, both wild animal stories and natural history construct a semantic similarity between humans and animals.    Figure 3. Classification accuracy rates to determine whether a noun is a human or an animal. An identical number of instances of human and animal words were sampled from each genre (8,800). All human and animal words with more than 10 instances were used, leaving anywhere from 97 to 145 words depending on the corpus. In one hundred runs per genre, 30 animal words and 30 human words were randomly sampled. These were classified for their species (animal vs. human) by a logistic regression, trained and tested according to a leave-one-out classification model.   ",
        "article_title": "Digital Animal Studies: Modeling Anthropomorphism in Animal Writing, 1870-1930",
        "authors": [
            {
                "given": "Victoria",
                "family": "Googasian",
                "affiliation": [
                    {
                        "original_name": "Stanford, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ryan James",
                "family": "Heuser",
                "affiliation": [
                    {
                        "original_name": "Stanford, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-01",
        "keywords": [
            "artificial intelligence and machine learning",
            "natural language processing",
            "eco-criticism",
            "English",
            "cultural analytics",
            "literary studies",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Although semantic web technologies are gradually introduced in the digital humanities and cultural heritage institutions the representation of linked data is still very abstract and hardly allows for interactions by researchers or other users. SPARQL, for instance, is not a user-friendly language to query the Semantic Web and representations of Big Data, of historical networks or of Geographical Information Systems (GIS) are still just too “flat” to exploit them in depth for historical research in the humanities or for the disclosure of cultural heritage. (Bodenhamer 2010; Presner 2014). Humanities scholars have challenged the use of digital analytical methods of patterns in Big Data (Drucker 2011; 2013). Valuable as these studies are for close readings of small data, they do not provide solutions yet for handling Big Data and visualizations hereof. How can we embrace complexity, ambiguity, and uncertainty in the analysis and visualization of big data? This question is central in the project  Virtual Interiors as Interfaces for Big Historical Data Research (start September 2018.) It builds upon the current NWO-large infrastructure project  Golden Agents: Creative Industries and the Making of the Dutch Golden Age (2017-2022) that aims at analyzing interactions between various branches of the creative industries and between producers and consumers using a combination of semantic-web and multi-agent technologies and circa 2 million scans of notary acts, such as probate inventories of the City Archives of Amsterdam. Here we present the first experiments with the creation of complex 2D/3D/4D interfaces on top of the Semantic Web, that express uncertainties in/allow users to interact critically in multiple ways with data.   The 2D interface aims to preserve and present the complexities rooted in historical sources through deep mapping. Deep mapping creative industries in Amsterdam embraces the uncertainties to see, experience, and understand space in all its complexity, and enable the visualization and analysis of migration pattern of the creative individuals within the city. Methodologically, this project proposes a mechanism of translating the descriptions of location-related information in historical sources, which are often incomplete or imprecise, into concrete-georeferenced locations. The physical, geo-coded locations, as vectorized in the first cadastral map of Amsterdam by the HisGIS project, serves as a basis in this geo-translation process and as an anchor for the alignment of pre-cadastral maps, archival materials, and modern databases like biographical database such as  ECARTICO and visual collections like  RKDimages, creating a multi-layered deep map of the early modern Amsterdam.   This research develops a framework to analyze the fabrics of painting and other creative industries in an urban space and to understand their choices of location within the framework of location theories from economic geography (Williams, 2018; Isard, 1956). Data concerning the features of urban space, such as the accessibility to public service, market, and customers and the housing price, are collected and analyzed to contextualize the complex living environment of artists. The first experiment focuses on Rembrandt’s neighborhood. The deep maps of Rembrandt’s neighborhood represent the spatial and social connections among Rembrandt’s neighbors and the rental value of housing in the areas Rembrandt lived before and after his bankruptcy. Reconstructing the historical road system of Amsterdam during Rembrandt’s lifetime, the accessibility of Rembrandt’s neighborhood is evaluated to revitalize the physical surroundings of the artist.  The 3D/4D interfaces, which will be anchored at the GIS map layer, will act as a hub to connect the heterogeneous data that are available on 17th century creative industries in a spatially coherent context. Specifically, this part of the project focuses on the creation of virtual reconstructions of domestic interiors on the basis of the information provided by probate inventories and other notary acts, surviving material culture, and structural information from the houses’ building history. Documents with the richest descriptions are compared to archival sources, building floor plans and elevations, surviving objects, archeological finds and contemporary images to find suitable matches. Since this match is certainly not always possible, we need the big data of the Golden Agents project and other collections to select feasible case studies for the creation of some demonstrators. These demonstrators offer us a lens to zoom in into how individuals created, used, displayed and experienced cultural goods in their homes over time, and serve as spatially enhanced interfaces to existing and  ad-hoc developed databases on the creative industries of the Dutch Golden Age. The real-world measurements in which the virtual environments are created allow us moreover to engage with the physicality of the reconstructed domestic space and to use them as exploratory tools to test and show alternative hypotheses about the use of space and the positioning of paintings and other objects within each room.  Although the possibility of cross-referencing information from the abovementioned interdisciplinary data sources enhances our ability to create a reliable reconstruction, a varying degree of ambiguity will still remain. Uncertainty regarding e.g. the house layout and the appearance or position of furniture and objects belonging to the household calls for a structural solution. One of the aims of this project, which makes it relevant to any application in the field of digital humanities beyond this specific temporal and geographical context, is indeed to develop a consistent way to express uncertainty, to describe the source selection criteria and to explain the reasoning behind the 3D reconstruction process in a transparent way. Concerns about the reliability and the powerful agency of virtual reconstructions have been raised since their inception in the historical and archaeological domains (see e.g. Ryan 1996). Despite the fact that this discussion has resulted in recent years in issuing guidelines for best practices (Denard 2012), a reliable and widely applicable practical workflow for 3d/4d interfaces in which these issues of complexity and uncertainty are systematically taken into account is still missing. The case studies that will be presented here will show the work in progress towards an implementation that aims to fill this gap.  ",
        "article_title": "Embracing Complex Interfaces Linking Deep Maps and Virtual Interiors to Big Data of the Dutch Golden Age.",
        "authors": [
            {
                "given": "Weixuan",
                "family": "Li",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands, The Netherlands",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Chiara",
                "family": "Piccoli",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, The Netherlands",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Charles van den",
                "family": "Heuvel",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands, The Netherlands; University of Amsterdam, The Netherlands",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "user experience design",
            "libraries",
            "interface",
            "gamification",
            "museums",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "GLAM: galleries",
            "archives",
            "semantic web and linked data",
            "modeling",
            "simulation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  With this proposal we explore the question: How can we characterize disciplines by looking at the discursive flows between scholars in university departments, and thus describe interdisciplinarity amid shifting topologies of knowledge? Taking as a provocation the premise that “every field of knowledge is the centre of all knowledge” (Frye 10), we explore paths of connectedness between disciplines, as constructed from a dataset of approximately 5,000 theses and dissertations (ETDs), in order to elucidate the boundaries, shapes, and concentrations of disciplinary knowledge in the making.   Analyzing the content and metadata of our institution’s collection of ETDs has allowed us to draw suggestive connections about disciplinary groupings locally and in broader contexts, highlighting both points of sturdy disciplinary borders and points of porousness, where boundaries are fading or non-existent. Boundaries, but also passages, between disciplines form and re-form depending on the mode or scale of analysis, which can shift between an entire corpus of disparate texts, a single ETD as rhizomatic agglomeration of its author’s disciplinary (and extra-disciplinary) experience, and the shared metadata features linking them together.    Thus far we have cleaned and normalized metadata, and built network graphs based on shared features among dissertations, and among academic programs. Still to come is a text-analytic component to model similarity among the full text. The resultant maps of interdisciplinary connectedness have been rendered in terms of specific features, including the ETD’s author-defined keywords (text freely entered by the author to describe their work), author-selected topical descriptions (choices from a controlled vocabulary of options), librarian-supplied Library of Congress subject headings, departmental affiliations, shared advisors and committee members, and topic models generated by analyzing the full text of all ETDs. The messy or chaotic connections that emerge are in contrast to the neatly hierarchical model of colleges, departments, and programs that is used to define the disciplinary structure of the university at an administrative level. Yet network graphs themselves constitute knowledge models that belie the complexity of a  terrain , reducing slopes, rifts, and dunes to a set of nodes and edges, circles and lines, often, with a limited set of visual features.   Attempts have often been made to classify disciplines based on theoretical “knowledge categories,” wherein “hard-pure” disciplines, such as physics, are described as “cumulative, atomistic (crystalline/tree-like), concerned with universals, quantities, simplification, resulting in discovery/explanation,” while the “soft-pure” disciplines, such as history, are described as “reiterative, holistic (organic/riverlike), concerned with particulars, qualities, complication, resulting in understanding/interpretation” (Becher 278). As pleasing or poetic as descriptions such as these may be, there has been a strong trend toward seeing disciplines as more or less  social  groupings with shared discursive traits: If the “essence of discipline formation and evolution is self-referential communication,” as Weingart suggests, then for interdisciplinary projects to be successful, there must be ways of breaking into hermetic epistemic groupings (8). Groupings are not just based on shared knowledge structures or methodology, but something like a culture, “each with its own tradition of thought and practice,” meaning that succeeding in interdisciplinary work is “much about coming to an understanding of cultures that are different from one’s own” (“Interdisciplinary Research”), and navigating the structures of power that maintain it. Undertaking such a mission of cultural outreach is especially difficult if Latour’s description of group formation applies, wherein the “spokesperson looks rather frantically for ways to  de-fine  them…rendering the group definition a finite and sure thing, so finite and sure that, in the end, it looks like the object of an unproblematic definition” (33). Breaking down these cultural boundaries is just the precondition to setting up new ones, in an ongoing cycle of shifts and ruptures.    Fuller coins the term “deviant interdisciplinarity” to describe projects that “aim to recover a lost sense of intellectual unity, typically by advancing a heterodox sense of intellectual history that questions the soundness of our normal understanding of how the disciplines have come to be as they are” (50). If we admit that any discipline could (in theory) be connected to any other, that the terrains of knowledge we construct are contiguous, then what are the divisions which prevent these maps from actually existing? Can the features around which disciplines coalesce at this moment be changed? Our analysis of ETD metadata will mediate between high-level disciplinary constructs and the “on-the-ground” reality, exposing the blind-spots of each. Visualizing these connections in network graphs has allowed us to highlight the difference, for instance, between the “potential” and “actual” disciplinary collaborations underway, and to measure the distance between Mode 1 disciplinary grouping, wherein “increasingly specialized disciplines are seen as the natural outgrowth of the knowledge production process” and Mode 2, which is the result of exposing the “collective blindspots” of the Mode 1 process (Fuller 51).  An initial interactive graph highlights several possible models of interdisciplinarity instantiated through an ETD collection, partially revealing that underneath the tidy departmental websites and faculty listings are a multitude of micro-cultures in the form of collaborations, research labs, and interdisciplinary courses: the makings of deviant irruptions of non-disciplines and future-disciplines. ",
        "article_title": "Disciplinary Topologies: Using dissertations to map deviant interdisciplines",
        "authors": [
            {
                "given": "Devin",
                "family": "Higgins",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": "Scout",
                "family": "Calvert",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": "Shawn",
                "family": "Nicholson",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "English",
            "network analysis and graphs theory",
            "library & information science",
            "digital archives and digital libraries",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper makes use of complex bibliographic metadata – the English Short Title Catalogue (ESTC) – to create a dataset which is analysed with quantitative tools in a way allowing for novel insights into historical perceptions of authorship and the structural backdrop for them. In doing this it demonstrates the relevance of both these tools and datasets for humanistic research.  Background Historical conceptions of authorship, despite perhaps initially seeming obvious in meaning, are far from clear. Theories of authorship have evolved from the Romantic “isolated, originary author”, through the Foucauldian author as a discursive formation, to the New Bibliography’s definitional idea of authorship attribution, and finally to a point where scholarship has come to regard “authorship and the status of the author not as ahistorical givens but as contingent constructs and institutions whose changing shapes represent responses to particular social, cultural, and economic pressures” (Hirschfeld, 2001: 610). Recent research into the early modern period (Kastan, 2002; Feather, 2011) builds upon this perspective by emphasizing the role of the book trade in the development of the concept of authorship. Dobranski (2014: 3), for example, highlights the paradox of the “author’s growing symbolic presence versus early modern writers’ limited practical authority”; while authors were increasingly named in publications, which sometimes even included a likeness and biographical information, they did not have control over the publication itself. It was the publishers – and, to some extent, printers, booksellers, and other actors in the book trade – who controlled publication, decided which details about authors to print, owned the copyright, and reaped most of the financial benefits. In this way, one could not be an author without being part of a more complicated network of actors within the book trade. The relationships between authors and these other book trade actors were not abstract. By a large margin, the majority of early modern publications in English came from London, and the book trade was made up of unique interconnected networks (Raven, 2007: 155–157). Even internationally, London played a key role in the book trade – as publishing established itself within North America, imports from Britain increased more quickly than domestic production (Green, 2009). Thus, the importance of the changing historical relationships which made up the book trade as a whole cannot be ignored when examining authorship as a constituent aspect of it. And while scholarship agrees that the situation changed over time, and that the institutions and relationships which made up the book trade were key to these changes, the details of this transformation remain debated and murky (Dobranski, 2014; Rose, 2009).    A Quantitative Approach Partly due to lack of suitable data, quantitatively oriented studies of early modern authorship have been sparse (Crawford, 1985; Stanton, 1988; Raven, 2000). However, advances in the digital analysis of traditionally humanistic resources – both bibliographic catalogues and full-text databases – are facilitating new quantitative approaches (Lahti et al., 2015; 2019; Underwood et al., 2018). This paper, therefore, turns to the historical records of the book trade – specifically bibliographic metadata in the form of a harmonized and enriched version of the ESTC – to digitally reconstruct the historical details of authorship in a way which allows for quantitative measurement.  While the process has been complex, with tens of thousands of lines of code required to parse the catalogue, the outputs are promising: we have extracted over 800,000 actors involved with roughly 480,000 printed documents, from which a total of 52,917 unique authors have been identified. Additionally, the data has been enriched with information including publishing location, years of activity, and pseudonyms - and by making use of open data resources (the Virtual International Authority File) and building a custom name-gender dictionary out of UK parish records, gender has also been attributed. In doing this we have been able to construct a dataset which can be used to test previous historical claims – both quantitative and qualitative – and, by making new historical claims, demonstrate the value of these digital methods and approaches when combined with traditional, yet novel, historical data. To this end, the paper has two approaches: testing the relationship between the data and historical reality, and making new quantitative claims.   Testing Previous Claims As noted, there is existing research with regard to the history of authorship and the book trade. However, this research has been built upon smaller datasets (often subsets of the ESTC) which has meant simpler quantitative measurements were sufficient for analyses. This does not mean the claims are incorrect, however. Thus, we begin by making use of both our dataset and new quantitative methods to test previous research claims. Specifically, we follow up on the early quantitative findings by Crawford (1985) and Stanton (1988) on female authorship in the seventeenth and eighteenth centuries. These include the influence of the civil war, population growth, and literacy rates on the number of female writers. Additionally, Rose (2009) suggests that changes in copyright legislation, such as the elimination of perpetual copyright in 1774 (to which we may want to add the 1710 Statute of Anne) encouraged publishers to support new authors to acquire copyrights.  There are also more general historical events which impact our data, such as the civil war and great fire of London (Figure 1). To demonstrate the relationship between this quantitative data and historical reality we also turn to these specific historical moments as case studies.    FIGURE 1: Actors involved in the book trade 1500–1800.   New Quantitative Approaches Built upon previous research claims which note the key role the book trade itself played in concepts of authorship (Hirschfeld, 2001; Bell, 2002; Dobranski, 2014), we make use of our data to develop new quantitative descriptions of historical authorship. Specifically, we demonstrate that complex analyses are necessary to develop competing representations of authorship both over time and during a given moment. With regard to the former, we are able to create a historical typology of authors which measures the transformation of the role through, for example, the number and type of professional relationships authors had, and how this changed over time (Figure 2).    FIGURE 2: Authorial connections by actor-type 1500–1800. With regard to the latter – for example, the eighteenth century debate between authors as “hacks” and “gentlemen” (Rose, 2009; Griffin, 2009) – we can move beyond making qualitative claims and instead recognize distinct categories of authorship within our data. Additionally, we are able to measure how these changes are reflected in distinct categories of authorship (i.e., gender, political or religious affiliation, genre, etc.). For example, with network analysis it is possible to identify distinct categories of authors through their relationships with outsiders (Figure 3).     FIGURE 3. Subsection of intellectual communities (1650–1659) as detected in the ESTC. Quakers: blue; poets: green; main book trade community: red. All of this allows us to recognize a number of interesting details within the data which could not be assessed previously. For example, it becomes possible to statistically identify individuals whom we consider historically important in ways which more basic analyses of publication records are unable to. Figure 4, for example, is a graph constructed out of four network centrality measurements and overall publication counts. It can be seen as a set of competing historical timelines constructed out of the statistical significance attached to particular authors during the early modern period. What is of particular methodological interest here is that the “Most Published” category is generally the worst metric for constructing this type of timeline. Thus, we are able to use the data in a way which satisfies historical intuition and addresses methodological concerns.    FIGURE 4: Timelines based on categories of authors identified using network centrality measurements.   Conclusion Although throughout the paper we aim to show how these types of analyses are able to draw accurate conclusions with regard to both qualitative historical tradition and quantitative historical data, it is not the aim of this work to simply make historically accurate quantitative claims which have been extracted from a non-traditional dataset. Instead, we hope to demonstrate how the structural frameworks of the book trade were directly linked to the complex act of authorship, and that these relations are a part of the way one could be conceptualized as an author at a given time. That is to say, the transformation from the “hack” to the “professional”, or the profound shift of women from an underrepresented group to exemplars of the 19th century novel, are changes tied as much to historical structures of authorship as they are to conceptual changes. It is these structures – as they are identifiable within the data – which allow us to recognize this complexity, and it is in this way that the paper makes both methodological and historical contributions: By demonstrating how historical metadata can be transformed into historical records with their own particular, and important, insights into the concept of authorship, we demonstrate ways in which existing, yet complex, data and historical knowledge can be used to make new historical claims.  ",
        "article_title": "Patterns of Early Modern Authorship: Using Metadata as Historical Record",
        "authors": [
            {
                "given": "Mark J.",
                "family": "Hill",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Tanja",
                "family": "Säily",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "bibliographic methods / textual studies",
            "history and historiography",
            "English",
            "network analysis and graphs theory",
            "theory and methodology)",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  We examine the discussions on the Facebook Page of the Czech Reality TV show  Výměna manželek ( Wife Swap). A commercial TV Nova acquired the originally British program for the Czech market in 2005. In 2018, the show is in its 10th season and consistently ranks among the most popular prime-time programs. We intend to find out if the show’s viewers active on social media partake in the shaming of lower class participants on the show. Specifically, we map the semantic space of “shame” in the comments associated with negative sentiment, interrogate the space for class-based content, and compare it with the alternatives.    Related work The global uptake of the Reality TV format in the last decades has been associated with a “demotic turn” that allowed more people to appear in content previously exclusive to the elites (Turner, 2010), although empirical evidence suggests that the increased working class presence is coupled with overrepresentation of the upper classes (Stiernstedt and Jakobsson, 2017). By showing “ordinary people” in what appears, despite heavy scripting and editing, to be everyday situations, Reality TV shows – such as ( Wife Swap), in which two wives exchange their homes for one week and both receive a reward for completion of the required swap – alter the way in which mass media represent social hierarchies. But more is not always better and a higher number of working class or poor characters does not guarantee their favorable representation. Skeptical authors relate the success of the Reality TV genre to its affinity with dominant neoliberal values, such as entrepreneurship and individual responsibility (McCarthy, 2007; Ouellette and Hay, 2008). In the framework of neoliberalism, low social status becomes a signifier not of class identity, but of individual failure.  Various on-screen events in a reality show may trigger politically charged response among audience members (Graham and Hajru, 2011; Graham, 2012), but class issues on the Reality TV are typically not discussed explicitly despite their centrality to the orchestrated narrative of the show. Instead, class distinctions are marked through lifestyle differences and tastes (Piper, 2004; Matheson, 2007) or supposedly individual moral shortcomings (Hirdman, 2016). Purposeful positioning of individuals outside of their value system (Lyle, 2008) underscores class divisions by subjecting members of the lower class to middle-class gaze. In Swedish reality shows, working class participants systematically become an object of ridicule (Eriksson, 2016). The competitive environment in the market-driven media helps to cultivate content producers who are more likely to generate negative portrayals of welfare and poverty (De Benedictis et al., 2017). The treatment of lower class individuals in Reality TV effectively resembles the practice of shaming - enforcement of norms through the generation of negative collective affect and public identification of a trespasser. Originally a punitive measure (Garvey, 1998), shaming has proliferated recently thanks to the social media platforms and ranges from benign vigilantism to criminal bullying (Trottier, 2018). People with middle class status are more likely to engage in this practice (Hou et al., 2017).   Data and methods Using Facebook Graph API, we collected 5-years worth of “postings” of all available types: posts by page (   n  =1273), comments (   n  =26383) and replies (   n  =28459) from January 2012 to March 2017 (Figure 1A). For further analysis, we obtained lemmas and part-of-speech tags using NLP tools for Czech language and retained only content-bearing words. After data collection, we used sentiment analysis to structure our corpus into groups based on the prevailing emotional polarity of postings. Next, we trained word embeddings for each of the sentiment groups. Finally, we generated the semantic neighborhood of “shame” from the trained vectors.  To choose between the two public language resources available for sentiment analysis in the Czech language, we manually tagged 150 randomly sampled postings with sentiment on the negative-neutral-positive scale. Neural Monkey classifier (Helcl and Libovický, 2017) performed worse than the most frequent class scenario (47% against 43% accuracy). To test SubLex (Veselovská, 2012), postings were assigned to one of the three classes by the following rules: if the numbers of positive and negative words are equal (including zero) the tag is “neutral”, more positive words yield “positive” tag, and more negative words result in “negative” tag. This approach reached 52% accuracy on three classes problem. In the absence of alternatives, we chose the sub-optimal lexicon-based method. We note that the satisfactory specificity of 88% reached by SubLex ensures that we can at least be reasonably confident that the negative postings are correctly identified as such. After performing sentiment analysis, we trained word2vec models (Mikolov et al., 2013) of dense word vectors with the sliding window parameter set to 10, one model per sentiment segment. Reducing data and comparing word vectors across slices is effective in tracing semantic shifts (Jo and Algee-Hewitt, 2018). In the end, we added the vectors for “shame” and its several synonyms together and extracted 50 terms closest to this aggregate vector. These terms delimit the semantic spaces of “shame” in our paper. In future work, we will build a bespoke sentiment classifier that would provide more accurate segmentation of our corpus. We shall also test the robustness of the obtained word vectors. Currently presented results are therefore preliminary.   Results    Figure 1  Figure 1B shows the results of the sentiment classification, although the proportions of the sentiment classes need to be read against the poor performance of the classifier. Still, the increase in the proportion of negative postings that corresponds to the overall influx of participants and the growing use of the “reply” feature on Facebook could indicate that a part of negative sentiment is due to the interactions of the discussants themselves (cf. Graham, 2012). The intersections of the semantic spaces of “shame” ( ostuda in Czech) show essentially no overlap (Figure 2). In contrast, for a sanity check, we extracted 50 most frequent words in each segment, which resulted in 39 overlaps. We have therefore good reasons to believe that “shame” is constructed differently based on the dominant sentiment in the discussion.  When “shame” occurs in connection with negative sentiment, a noticeable cluster of meanings related to hygiene emerges. Out of 50 words, more than 10 relate to dirt (“dirt”, “make dirty”, “stinky”) or cleanliness (“washed”, “clean teeth”, “hygiene”) and include interjections of disgust such as “yuck!”. Expression of disgust over mess thus appears as the main distinctive feature with which viewers can scold the participants in the Reality TV shows as inferior. This result provides support the notion of the middle class gaze, as order and tidiness are its integral components. Furthermore, in the semantic space of negative sentiment the word for “laziness” also appears and expands the discussion to neoliberal values of diligence and constant self-improvement.   Figure 2  We focused on a very specific aspect of social media audience response to the Reality TV genre and found that the viewers engage in the shaming of reality show characters by affirming personal hygiene as the demarcation line between acceptable and unacceptable poverty.  ",
        "article_title": "Semantics of Shame in Social Media Discussions of Reality TV Fans",
        "authors": [
            {
                "given": "Radim",
                "family": "Hladík",
                "affiliation": [
                    {
                        "original_name": "Institute of Philosophy of the Czech Academy of Sciences ; National Institute of Informatics",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Markéta",
                "family": "Štechová",
                "affiliation": [
                    {
                        "original_name": "Charles University",
                        "normalized_name": "Charles University",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/024d6js02",
                            "GRID": "grid.4491.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-01",
        "keywords": [
            "corpus and text analysis",
            "social media",
            "semantic analysis",
            "content analysis",
            "communication and media studies",
            "English",
            "cultural studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Cuneiform characters have been described using various systems in the past and the varieties of systems used in the literature as well as in daily work varies from language to discipline. Commonly, sign lists (Borger 1971, Borger 2004, Ruster 1989, Deimel 1947) are created and published in the form of dictionaries in a non-machine-readable form. Similarly, for computers, the only way to distinguish cuneiform characters is currently to assign them different numbers in a list (e.g. Unicode (Unicode Staff, 1991)) and consider a distinction on this level. Therefore we are left with many systems and numbers to describe the same cuneiform sign. (Figure 4). Contrary to listing cuneiform signs, (Gottstein, 2012) took another approach in creating a searchable cuneiform character encoding based on wedge types which would be implemented in applications such as CuneiPainter    (Homburg, 2015). Character image recognition has also been performed in the past (Mara, 2010), but never yielded a machine-readable representation of a cuneiform characters paleographic information which could have been useful as a means of validation for machine learning recognitions. This publication therefore introduces Paleo Codage, a paleographic distinct machine-readable description inspired by the Manuel de Codage encoding (Van den Berg, 1997) for Egyptian Hieroglyphs.   Motivation A machine-readable paleographic description despite yet representing another encoding scheme could link all systems of cuneiform character descriptions, as it directly describes the characters shape and positioning parameters. Scholars could register newly found characters easily in a machine-readable way and provide the basis for computational analysis on the paleographic shapes of cuneiform characters. Such paleographic information would ideally be integrated into currently emerging Semantic Dictionaries for cuneiform (Homburg, 2017, 2018) to enrich linguistic linked open data and thereby profit the respective scholars. In addition a machine-readable paleographic description provides the basis to capture sign variants of characters currently described in unicode. It is very common for on unicode codepoint to have many sign variants describing the same meaning over the centuries in which cuneiform has been written. Those sign variants have never been assessed digitally (only as sketches in books) and could provide valuable insights for philologists.    Approach Paleo Codage builds on the description of (Gottstein, 2012), by using simple character descriptions for certain wedge types and by extending it with a Manuel de Codage (Van den Berg, 1997) inspired set of relational descriptions. Cuneiform wedges are distinguished as follows:  Vertical wedge 𒀸 (a)   Horizontal wedge 𒁹 (b)   Diagonal wedge 1-4 𒀹,𒀺 (c,d and mirrored e,f) Winkelhaken 𒌋 (w)   The system encodes relations between wedges as shown by the following most frequent examples:  Wedges that pass through other wedges situated right to them (-) (e.g. MIN 𒈫 -> a-a , three AŠ 𒐁 → b-b-b )  Wedges that do not pass through other wedges situated right to them (_) (e.g. ŠU 𒋗 -> b:b:b:b:b_a , GIŠ 𒄑 -> b::b_a ) Wedges under another wedge possibly passing through other wedges (:) (e.g. U2 𒌑 → B::B-a-a-a-a , AŠ2 𒀾 → b:b:b:b-a ) Wedges under the current wedge not passing through other wedges (;) (e.g. BAR 𒁇 → ;b-a ) Diagonally under another wedge (.) (e.g. GAM 𒃵 → c.c ) Wedge inversion (!) (e.g. IDIM 𒅂 → !b:b )  In addition size variations of cuneiform wedges are common and can be encoded as follows:   Capital letters signify a bigger version (e.g. A instead of a), wedges prefixed with a small s a smaller version (e.g. sa instead of a) (e.g.  A x A  𒀁  →  a-sa-sa:sa-a:a ,  ŠE  𒊺 →  W:W-w:w-w:w-w:w-W:W  )      Lastly, angles of diagonal cuneiform characters may vary between characters which required angle modifiers to be added to the encoding.  The angle between the diagonal wedges in (e.g. IR 𒅕 → c;d-a-a-a) is bigger than the angle between the diagonal wedges in (ARKAB 𒀶 → |d;|c_A ). The angle can be halved by using the | operator.  While the order in which cuneiform wedges were drawn is not always agreed upon by the respective scholars (Devecchi, 2015), PaleoCodages’ order independent of this dispute is from left to right and then from up to down in order to avoid ambiguities concerning cuneiform sign definitions. In order to facilicate the representation of displaced wedge groups PaleoCodage also includes the following positioning modifiers (/ half the size down, ~ half size to the left, # half size to the right, as well as < and > as rotation modifiers, rotating the whole glyph). Further operators could be added if needed by glyphs which can currently not be modeled.  Proof Of Concept A proof of concept is provided on a representative subset of 200 cuneiform unicode characters  https://en.wikipedia.org/wiki/Cuneiform_(Unicode_block)  which were analysed to infer the relations described section Approach. Table 1 includes further encoding examples.    Image Unicode Main Transliteration Borger Gottstein Paleo Codage   𒁹 U+12079 DIŠ 748 a1 a   𒀸 U+12038 AŠ 001 b1 b   𒀹 U+12039 AŠ ZIDA tenû 575 C1 c   𒀺 U+1203A AŠ KABA tenû 647? c1 e   𒌋 U+1230B U 661 d1 w   𒈦 U+12226 MAŠ 120 a1b1 :b-a   𒁇 U+12047 BAR 121 a1b1 ;b-a   𒇲 U+121F2 LAL 750 a1b1 a-b   𒈨 U+12228 ME 753 a1b1 a-:b   𒃵 U+120F5 GAM 576 c2 c.c   𒋻 U+122FB TAR 009 a1c2 c.ca   𒌀 U+12300 TIL 114 b1c1 bc   𒉽 U+1227D PAP 092 b1c1 C:d   𒂢 U+120A2 EZEN x A 288 a7b6 :sa-:sb::sb-ab;b-:sa-:sa:sa-a-:sb::sb-:sa   𒅈 U+12148 IGI RI 726 a4b2d2 :w-a-:b_-:b-a-a-:::w-a    Table 1: Cuneiform Encoding Examples A generated similarity graph for verification purposes (Figure 2) using the new encoding method shows the applicability of the encoding to identify subglyphs that are included in other glyphs which in turn is useful information to be included in (Semantic) dictionaries. Further similarity measures on the encoding (String Similarity) could reveal additional connections between cuneiform character representations.    Figure 2: Cuneiform Character relations as graph (excerpt): Only by verification of the encoding the computer can e.g. now recognize that the glyph IMIN3 (b:b:b_b:b:b_b) is contained by the glyph ilimmu3 (b:b:b_b:b:b_b:b:b). Using the Gottstein System such a conclusion could not be made as they would be classified as b7 and b9 respectively.     Application  Given the paleographic information encoded in a standardized way users have the ability to draw a rudimentary shape of the character in order to detect the character they are seeing in front of them (e.g. on a picture or a tablet). This functionality is currently being implemented in CuneiPainter   , improves its accuracy when matching cuneiform characters and will be ready as a showcase for DH2019. A showcase in JavaScript (Figure 3) highlighting all currently encoded characters is already available for testing   , allowing users to verify and create their own encodings easily. In addition, the testing tool allows to export created cuneiform characters as SVG and as OpenType fonts in-browser, creating the basis for an easier automated font creation for cuneiform characters.     Figure 3: Paleo Codage Input (JavaScript Application)     Figure 4: Cuneiform Numbering Systems: Semantic Dictionary for Ancient Languages   ",
        "article_title": " Paleo Codage - A machine-readable way to describe cuneiform characters paleographically  ",
        "authors": [
            {
                "given": "Timo",
                "family": "Homburg",
                "affiliation": [
                    {
                        "original_name": "Mainz University Of Applied Sciences, Germany",
                        "normalized_name": "Hessische Hochschule für Polizei und Verwaltung",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01hwhm419",
                            "GRID": "grid.466449.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "linking and annotation",
            "software design and development",
            "linguistics",
            "natural language processing",
            "lexicography",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A translator normally replaces almost all the original author's vocabulary except proper nouns. Most authorship attribution methods are based on the frequencies of the most frequent words or n-grams, the latter themselves derived from the sequence of words. Given these facts, one might expect attributions of translations to identify them by translator rather than author. Yet that is not the case. Rather, despite the replacement of the original author's language by that of the translator, translations are normally attributable to their original authors, rendering the translators virtually invisible. Jan Rybicki, himself an accomplished translator, has presented some important discussions of this peculiar state of affairs (Rybicki 2006, 2012), but a further investigation of this curious phenomenon seems worthwhile (see also Burrows 2002, Rybicki and Hedel 2013).  As a first step, consider a test of twenty texts by Chekhov translated by five translators (or pairs of translators). Figure 1 shows a Stylo bootstrap consensus tree (Eder, Rybicki, and Kestemont 2016), based on cluster analyses of the 200-2,000 most frequent words (pronouns deleted) in increments of 100 words and with culling from 0% to 100% in increments of 20% (0% retains all words; 100% retains only words occurring in all texts), consensus .5 (at least 50% agreement is required to group texts).    Fig. 1 Chekhov Translations by Multiple Translators Here multiple translations of the same text rather than multiple translations by the same translator cluster consistently, suggesting that text identity is a stronger signal than translator (on the strength of various signals, see Jockers 2013: 79-81). Note, however, that three of the four Garnett translations of texts not translated by any of the other translators group together. Next, consider the bootstrap consensus tree of multiple translations of five Russian authors by Constance Garnett seen in Fig. 1 (same stipulations as above), which does an excellent job of grouping authors even without the effect of multiple translations of the same text.     Fig. 2 Garnett Translations of Multiple Authors The strength of the original author's signal in translations can be tested more thoroughly using Stylo's Classify function. For the first test, 30 texts form the training set: 5 Chekhov texts by 4 translators, 9 Dostoevsky texts by 7 translators, 5 Gogol texts by 4 translators, 7 Tolstoy texts by 3 translators, and 4 Turgenev texts by 2 translators. The test set contains 47 texts by the same authors: 10 Chekhov texts by 4 translators, 13 Dostoevsky texts by 6 translators, 8 Gogol texts by 3 translators, 9 Tolstoy texts by 4 translators, and 7 Turgenev texts by 4 translators. No translations of the same text appear in both groups, eliminating the signals of individual texts. Thus the task is to attribute a set of test texts (sometimes including multiple translations of a single text by different translators) to the original authors of a different set of training texts. Based on the 100-2,000mfw (with an increment of 100), with 40% culling and pronouns deleted, NSC (nearest shrunken centroid) classification is 94.5% accurate (888 of 940 correct attributions) and SVM (support vector machine) classification 96% (902 of 940 correct attributions). These results would be strong even on texts that had not been translated. A second much stricter test involves 34 training texts: 7 Chekhov texts by 2 translators, 7 Dostoevsky texts by 2 translators, 10 Gogol texts by 2 translators, 6 Tolstoy texts by 2 translators, and 4 Turgenev texts by 1 translator. The test set contains 44 texts by the same authors: 5 Chekhov texts by 4 translators, 14 Dostoevsky texts by 6 translators, 10 Gogol texts by 4 translators, 8 Tolstoy texts by 2 translators, and 7 Turgenev texts by 4 translators. These texts were chosen so that no translations by the same translator for the same author appear in both training and test sets. Thus the task is to attribute a set of test texts to the original author when the translators of the training texts by that author are different from the translators of the test texts by that author. The results on this test (same settings as the previous test) are naturally less accurate, but NSC classification is still 85.8% accurate (755 of 880 correct attributions) and SVM 87.6% (771 of 880 correct attributions). This seems almost incredible: the original author of a set of English translations by one group of translators is usually correctly identified as the author of a different set of that author's texts translated by a different set of translators. In spite of the strength of the author's signal, however, further analysis shows that the translator can be made visible again by filtering out the author's signal. Consider a different kind of test. The training set contains 6 translations of Tolstoy by Garnett and 5 translations of Dostoevsky by Pevear and Volokhonsky (Garnett is treated as the author of Tolstoy and Pevear and Volokhonsky as the author of Dostoevsky). The test set contains 33 texts: 10 translations of Chekhov, 1 of Goncharov, and 9 of Turgenev by Garnett, and 13 translations of Gogol by Pevear and Volokhonsky. With authorship neutralized, the translator becomes startlingly visible again. On these tests (same stipulations as above), NSC is 81.2% accurate (536 of 660 correct attributions) and SVM 93.9% (620 of 660 correct attributions). Clearly Garnett's translations of Tolstoy are similar enough to her translations of Chekhov, Goncharov, and Turgenev that she can readily be identified as their \"author.\" The same is true of the translations of Dostoevsky and Gogol by Pevear and Volokhonsky. A final test can begin to show how this is possible. Zeta analysis identifies the characteristic vocabulary of these two translators–words consistently used by each and avoided by the other (Burrows 2002). It contrasts two groups of texts by measuring the consistency of inclusion and exclusion of a large set of words in large groups of sections of text of the same size by the two translators. For this test, Garnett's translations of Chekhov and Turgenev are treated as her \"authorial\" set and the Pevear and Volokhonsky translations of Dostoevsky and Tolstoy as their \"authorial\" set. An initial analysis showed that many proper names appeared in the characteristic vocabulary, and that British vs. American spellings and Garnett's use of hyphenated forms of words like  to-day, to-morrow, to-night, etc. had a significant effect, so I manually culled out more than 4,000 such words and retested, with the result shown in Fig. 3. Given the proven strength of the author's signal, Fig. 3 makes an important point. None of the Garnett Ind. Sections or P and V Ind. Sections influenced the distinction between the two translators, and all these texts are by Gogol. Many of them (in bold) are translations of the same work. Nevertheless, they are easily placed near the texts by their translator and separate from each other.      Fig. 3 Zeta Analysis of Garnett vs. Pevear and Volokhonsky The 40 most distinctively characteristic words for the two translators shows some interesting patterns: Consistently used by Garnett and avoided by Pevear and Volokhonsky: till, fancy, passed, drawing-room, upon, air, flung, answered, muttered, walked, scarcely, cap, sound, slowly, hair, expression, hardly, every, fellow, near, silence, instant, distance, white, low, soft, bent, walking, deal, sky, grew, poor, shoulders, lips, fond, rather, dark, ought, haste, country, black, faint, beside, suppose, window, observed, continually, clever, creature, sank Consistently used by Pevear and Volokhonsky and avoided by Garnett: therefore, everyone, precisely, also, finally, I'll, despite, maybe, became, anyone, especially, decided, terribly, you're, having, start, impossible, I'm, unable, obviously, main, I'd, someone, contrary, he's, moment, until, started, order, situation, I've, didn't, because, terrible, firmly, front, silently, purpose, earlier, otherwise, immediately, certain, understood, let's, barely, they're, lit, you'll, former, you've The words  till for Garnett and  until for Pevear and Volokhonsky are a classic authorship pair. Pevear and Volokhonsky clearly use a less formal style, as indicated by the large number of contractions among their markers. They also use more -ly adverbs, with nine in the list above compared to only four for Garnett, and only they have indefinite pronouns in their list (a trend that continues far beyond the 40 most distinctive words). By contrast, Garnett's list contains many concrete nouns, while Pevear and Volokhonsky's list contains none. It also contains many more full verbs and adjectives than Pevear and Volokhonsky's. There is no space here to investigate these differences fully, but this analysis suggests new ways to study the elusive signal of the translator.  The seeming paradox of the invisible translator can be resolved: although the strength of the author's signal normally renders the translator's individual style invisible, the translator's own signal is quite strong enough to allow the attribution of translations to their translators once the author's signal is eliminated. ",
        "article_title": "The Invisible Translator Revisited",
        "authors": [
            {
                "given": "David L.",
                "family": "Hoover",
                "affiliation": [
                    {
                        "original_name": "New York University, United States of America",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-05",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "authorship attribution / authority",
            "translation studies",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  As part of a larger project in distant reading nineteenth-century British poetry, a method for detecting line-end rhymes was devised that utilizes rhyme dictionaries published in the eighteenth and nineteenth centuries. This method was proposed in order to account for historical debates about the definition of poetic rhymes in English as well as historical changes in pronunciation. This paper describes an evaluation of this approach that compares it to a method commonly used in computational analysis, which is based on the  CMU Pronouncing Dictionary, in order to understand what significant differences occur.    Historical context  Rhyme in English poetry is generally defined as the connection between two syllables “that have identical stressed vowels and subsequent phonemes but differ in initial consonant(s) if any are present” (Brogan and Cushman, 2012: 1184). Line-end rhyme is the most common use of rhyme, and it contributes to the structure and effect of particular poetic forms, like the sonnet and triolet, and to stanza patterns like the Spenserian stanza.  One syllable, or masculine rhymes, predominate in English poetry, as do “perfect” or exact rhymes, in which the vowel sounds are identical: cat/hat. Yet many poets have also used “imperfect” or near rhymes, in which the vowels are somewhat different: young/song. Literary critics in the nineteenth century frequently debated the rules for rhyme, either pointing to such examples as justification for a relaxed definition, or deriding them as bad poetry.  Alongside these debates, many different rhyme dictionaries were published in the nineteenth century, which offered critical definitions and examples of poetic rhyme, as well as lists of rhyme syllables and rhyming words in English. These dictionaries were aimed at would-be poets, students of poetry, and those wishing to improve their pronunciation of English. Rhyme dictionaries can thus serve as a data source for understanding both historical theories about rhyme and historical British pronunciation.    Rhyme detection with historical rhyme dictionaries In previous work, a method for rhyme detection using John Walker’s “Index of Perfect and Allowable Rhymes” was demonstrated. Each entry consists of a rhyme syllable, a list of words that end with that syllable, other perfect rhymes, and a list of allowable rhymes:  AM Am, dam, ham, pam, ram, sam, cram, dram, fam., sham, swam, epigram, anagram, &c. Perfect rhymes, damn, lamb. Allowable rhymes dame, lame, &c. (Walker, 1824: 642) A key-value table was created from these entries. The rhyme detection script uses the key-value pairs to identify perfect rhyme words and syllables first, followed by allowable rhyme words and syllables within the poem. Rhyme patterns are also visualized as a sequence of capital letters (ABABCDCD), as is standard in literary studies.  This method makes possible the detection of rhyme words, rhyme syllables, and rhyme patterns in large document sets. This method for computational historical poetics can compare different historical theories of rhyme as well as use them to evaluate rhyme usage in large document collections. This method also contributes to the study of rhyme’s effects on poetic vocabulary more generally in the nineteenth century.   Rhyme detection with the  CMU Pronouncing Dictionary  The  Carnegie Mellon University Pronouncing Dictionary is “an open-source machine-readable pronunciation dictionary for North American English that contains over 134,000 words and their pronunciations” (http://www.speech.cs.cmu.edu/cgi-bin/cmudict). It is widely used for a variety of language analysis tasks and is available through NLTK.   Several researchers have based their work on rhyme detection on the  CMU Pronouncing Dictionary (Hirjee and Brown, 2010; Kao and Jurafsky, 2012; McCurdy, et al., 2015). The  rhyme-plus package for node.js (https://www.npmjs.com/package/rhyme-plus) and the  pronouncing package for Python (https: //pypi.org/project/pronouncing/) include functions for rhyme analysis based on the  CMU Pronouncing Dictionary. This wide availability has made it standard for dictionary-based digital humanities work involving pronunciation. (It should be noted, however, that some other projects use speech transcription or speech synthesizer programs as an alternative to dictionary tables (Clement et al, 2013; Malmi et al, 2016).)     Evaluation In this evaluation project, rhyme detection using historical rhyme dictionaries is compared to rhyme detection using the  CMU Pronouncing Dictionary. First, the rhyme syllable and word pairs from Walker’s rhyme dictionary are compared against the CMU dictionary to discover which rhyme word pairs are found in both dictionaries; which rhyme word pairs are found only in the CMU dictionary; and which rhyme word pairs are found only in Walker’s dictionary.  Preliminary evaluation with a random sampling from Walker’s dictionary suggests that a significant proportion of historical rhymes labeled as perfect, as well as most of those labeled as “allowable” by Walker, are not discovered by using the CMU dictionary. Several reasons are suggested for this: pronunciation differences between American and British English; vocabulary differences between literary and general English; and vocabulary and pronunciation differences between nineteenth-century and contemporary English. A second phase of evaluation tests the historical dictionary method and the  CMU Pronouncing Dictionary over a corpus of 1500 British poems published between 1800-1900 to evaluate how significant the differences between the dictionaries are for the analysis of a literary corpus.    ",
        "article_title": "An Evaluation of Rhyme Detection Using Historical Dictionaries",
        "authors": [
            {
                "given": "Natalie M",
                "family": "Houston",
                "affiliation": [
                    {
                        "original_name": "University of Massachusetts-Lowell, United States of America",
                        "normalized_name": "University of Massachusetts Lowell",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03hamhx47",
                            "GRID": "grid.225262.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "English",
            "literary studies",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A large number of digital humanities projects focuses on text. This medial limitation may be attributed to the abundance of well-established quantitative methods applicable to text. Cultural Studies, however, analyse cultural expressions in a broad sense, including different non-textual media, physical artefacts, and performative actions. It is, to a certain extent, possible to transcribe these multi-medial phenomena in textual form; however, this transcription is difficult to automate and some information may be lost. Thus, quantitative approaches which directly access media-specific information are a desideratum for Cultural Studies. Visual media constitute a significant part of cultural production. In our paper, we propose Deep Watching as a way to analyze visual media (films, photographs, and video clips) using cutting-edge machine learning and computer vision algorithms. Unlike previous approaches, which were based on generic information such as frame differences (Howanitz 2015), color distribution (Burghardt/Wolff 2016) or used manual annotation altogether (Dunst/Hartel 2016), Deep Watching allows to automatically identify visual information (symbols, objects, persons, body language, visual configuration of the scene) in large image and video corpora. To a certain extent, Tilton and Arnold’s Distant-Viewing Toolkit uses a comparable approach (Tilton/Arnold 2018). However, by means of our customized training of state-of-the-art convolutional neural networks for object detection and face recognition we can, in comparison to this toolkit, automatically extract more information about individual frames and their contexts.  Research object The focus of our project is Ukrainian nationalist Stepan Bandera, who during World War II collaborated with Germany and tried to forcefully establish a Ukrainian national state against Polish and Russian opposition, and his instrumentalisation in the recent Ukraine conflict by both the Ukrainian and the Russian side. In the Russian narrative, Bandera is used as an example for Ukrainian fascism, whereas for Ukrainian nationalists he symbolises the uncompromising fight for national independence. New media such as video clips uploaded to YouTube are used extensively to disseminate these contradicting interpretations of Bandera; a first study showed that this instrumentalization is present in all major digital media and was already immanent before 2014 (Fredheim et al. 2014). Our paper builds on this preliminary work and traces Bandera’s image and position within cultural memory in Poland, Ukraine, and Russia from the Euromaidan in 2013 up until now.   Methodology We use the first 200 Youtube search results for the terms “Stepan Bandera” and “Степан Бандера” as a corpus. Because of overlapping search results, our corpus comprises 274 videos, uploaded to Youtube between 2007 and 2017 with a total length of 3 days, 11 hours, 49 minutes, and 16 seconds. It should be noted that YouTube’s search engine does not provide direct access to its database, but rather adapts the result list according to country, browser, and other details of the user. The 274 videos are split into their individual frames. In order to analyse the corpus, we trained Detectron, an open source framework developed by Facebook AI Research (Girshik et al. 2018), to recognize 12 emotionally charged symbols, which help identify in which context Bandera is presented and thus, hint at different instrumentalization. Table 1 presents these symbols within their four different main classes and the corresponding numbers of manual training annotations used.    Table 1: List of all 12 symbols within their 4 distinct classes  To create the training set we manually annotated 793 images with 1731 annotations, i.e. an average of 144 annotated objects per symbol. An annotation consists of point coordinates indicating the outline of the object and the corresponding name of the symbol. Between 1 and 13 annotations are assigned to an image, on average 2.2; the median is 1. For proper testing and evaluation, the corpus is randomly divided into training, testing, and evaluation data using a ratio of 70/15/15. We use Intersection over Union (IoU) as the evaluation metric for symbol recognition. IoU covers the interval 0 to 1, 1 being a perfect match between proposed and predefined region. In our experiment, we reach an average IoU of 0.68. On closer inspection, our results show that objects which have not been recognized in one frame are likely to be recognized in a subsequent one. Hence, the recognition of symbols is even better then the test result suggests. A sample visualization of recognized symbols in a single frame can be seen in Figure 2.      Figure 2: Recognized symbols in a single video frame, taken from https://www.youtube.com/watch?v=axFz-SU8cIM (accessed 27 November 2018). Unfortunately, our instance Detectron is not optimized for individual face recognition; trying to recognize distinct persons (in our case Bandera and Adolf Hitler) in individual frames led to a high error rate within this class. Hence, we decided to combine Detectron with OpenFace (Amos/Ludwiczuk/Satyanarayanan 2016), an implementation of the FaceNet algorithm (Schroff/Kalenichenko/Philbin 2015). We are currently evaluating recognition accuracy in a test corpus and will present the combined results of Detectron and Facenet at the conference.   Results As Figure 3 shows, symbols related to Poland (the Polish eagle and the Falanga) and Russia (Ribbon of St. George and Hammer & Sickle) are seldom encountered in our corpus, whereas the flags of Ukraine and UPA are rather common, as is the Ukrainian coat of arms. The Ukrainian flag, for example, shows up in 2% of all video frames in the corpus (i.e. for 1 hour and 40 minutes). Also common, albeit less frequently occurring than their Ukrainian counterparts, are the symbols of the Third Reich. This distribution suggests that Bandera is presented in a Ukrainian nationalist context and his connections to the Nazis is underlined, whereas his position in the Polish and the Soviet context does not play a big role. This interpretation becomes even more clear when symbol co-occurrences (i.e. symbols showing up in the same frame) are plotted (Figure 4). Both the Ukrainian and the Nazi symbols not only co-occur within their group but also with the respective other groups. This finding hints at the dominance of a Russian nationalistic discourse on Youtube, which frames Bandera as an example of Ukrainian fascism.      Figure 3: Mean percentage of occurrence for each symbol       Figure 4: Symbol co-occurrences in 274 videos, adjusted for symbol frequency      Figure 5: Total symbol occurrences over time The next step is to combine the detection results from Detectron with the appearances of Hitler and Bandera in our corpus. What is more, we plan to compare the results discussed above with a second video corpus about Bandera, which was collected in 2013 as part of previous work. This comparison may uncover how the Ukraine crisis changed the way Bandera is represented in Youtube videos and will be presented at the conference. A first glance at diachronic symbol occurrences is presented in figure 5; this visualisation suggests that specific symbolic discourses rise and fall in the course of time, and most symbols peak in 2014 when the conflict is in the most heated stage.   Discussion and outlook Recognizing specific symbols allows for new ways to study large visual corpora. Nonetheless, this approach is tied to a specific research question because a RCNN has to be trained to recognize predefined symbols. This limiting factor led us to experiment with a more general approach which focuses on visual depictions of human bodies as embodied signs, a key question of Cultural Studies. We are currently evaluating additional algorithms to automatically recognize specific people and assess both their posture and mimic on a sample corpus of 1000 trading cards from the 1930s depicting German-American actress Marlene Dietrich.  Body postures can be analyzed on the basis of keypoints (Bourdev/Malik 2009) such as hands, feet, head, etc., which result in different postures. (Figure 6) These postures can be used to study symbolic meanings communicated through the body (Impett/Moretti 2017). The connection between postures and gender stereotypes is much discussed (Mühlen-Achs 1998); in the case of Marlene Dietrich's androgynous self-staging which relies on elements connoted as “male”, a quantitative analysis of postures and their changes over time allows new insights.    Figure 6: Posture detection with Detectron (left), Face analysis by OpenFace (right)  Generic information on Faces can be extracted by the algorithm OpenFace 2.0 (Baltrušaitis et al. 2018) which extracts three-dimensional orientation and keypoints such as eyes or nose from a given digital image. (Figure 6) Moreover, these keypoints are compared with a standard face model defined by the Facial Action Coding System (FACS, Ekman/Friesen 1978), which describes facial expressions. In the case of Marlene Dietrich, the expression outer brow raiser is often encountered, which can be explained by the makeup trends of the 1930s. Thus, our approach can to a certain extent assess fashion and style-related questions in a quantitative manner. By means of combining various algorithms to automatically identify symbols, objects, faces, posture, and mimics, we propose a potent framework to study large corpora of visual media. We are convinced that Deep Watching will advance the quantitative methodology of Cultural (and Media) Studies significantly.  ",
        "article_title": " Deep Watching: Towards New Methods of Analyzing Visual Media in Cultural Studies  ",
        "authors": [
            {
                "given": "Bernhard",
                "family": "Bermeitinger",
                "affiliation": [
                    {
                        "original_name": "U St. Gallen, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Sebastian",
                "family": "Gassner",
                "affiliation": [
                    {
                        "original_name": "U of Passau, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Siegfried",
                "family": "Handschuh",
                "affiliation": [
                    {
                        "original_name": "U St. Gallen, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Gernot",
                "family": "Howanitz",
                "affiliation": [
                    {
                        "original_name": "U of Passau, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Radisch",
                "affiliation": [
                    {
                        "original_name": "U of Passau, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Malte",
                "family": "Rehbein",
                "affiliation": [
                    {
                        "original_name": "U of Passau, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "multimedia",
            "data mining / text mining",
            "English",
            "cultural analytics",
            "computer science and informatics",
            "cultural studies",
            "audio",
            "image processing",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " “The ties that bind’: The creation, use, and sustainability of community generated histories  The use of digital content, tools and methods allows new insights into historical research, through enriched engagement with primary sources via digitisation and datafication, and the use of data analysis, visualisation, and immersive approaches. In response to these digital opportunities, the commemoration of the Centenary of the First World War has seen a digital 'big bang': more than any other historical period, the research community has access to a large number of publicly accessible digital resources: outputs of projects created by universities, libraries, museums, archives, and community groups. In addition, numerous organisations and initiatives have created opportunities for individuals and communities to create and/or share digital Community Generated Content (CGC), making accessible ‘public’ historical content from personal collections, or providing expertise and knowledge to collection or catalogue descriptions. While many of these initiatives simply present personal collections and content alongside ‘official’ archives, collections, and narratives, they can also present an opportunity to explore the potential of community histories and content to challenge notions of professionalism and the authority of the ‘expert’ voice.    This paper will address ways that digital CGC has been used in digital First World War initiatives across Europe, focusing on three core aspects of this approach.    “Everybody is your neighbour, everybody is your friend”  The first is the significance of community generated history to the commemoration of the First World War. Due to political, educational and public interest in the ‘decade of commemoration’ of significant centenaries in twentieth century history, there has been an enormous interest in community digitisation projects that allow members of the public to digitise their family collections that relate to the First World War. The first project to do this was the Oxford Great War Online project (2008). This methodology was adopted by Europeana 14-18, which has generated ca. 200k items related to the First World War at workshops around Europe. Smaller, national/local projects have also carried out workshops to generate content within communities (including Cymru1914.org, the People’s Collection Wales, and the AHRC-funded  Living Legacies Centre for the Centenary of the First World War in the UK). In this presentation, we will examine motivations for developing community generated content, and how analysis of these activities shows the value of participating in digitally-based activities to develop CGC can increase engagement with primary sources and provide a strong example of the power of digital heritage to facilitate experiential value, opportunities and benefits.   However, the interaction of creators and producers to produce meaningful ‘experiences’ in the digital heritage sector is poorly theorised, particularly where the community is both the creator and the producer. We argue that those involved in creating CGC in the digital heritage sector can experience value in four different areas: firstly, the value of producing distinctive content (ranging from providing incremental additions to knowledge, to providing hidden new resources, to giving new attention to under-privileged voices); secondly, the value of a feeling of useful participation (which can include supporting or challenging the ‘authoritative voice’, and which gamification research has provided insights to); thirdly, the value of the transformation such involvement can engender in the participants (including their new understanding of history, their community, the content they provide, or their own value and perception), and finally, the value of CGC in helping coalesce a (sub)community through the work itself. It is within this matrix of value that existing discussions of empowerment, identity production, and challenges to the ‘expert’ voice (amongst others) sit, and these issues often span multiple parts of our matrix.  A fuller understanding of such values and a fleshed-out model of each aspect is needed for us to understand the ‘gaps’ a participant may have between their expectations and their experience when becoming involved in CGC, and to establish a concrete analysis of the value of CGC for historical research.    “The time has come to let the past be history”  The second strand is the use and reuse of this material for research. The most recent Call for Europeana Research Grants  invited early-career researchers to explore the Europeana 1914-1918 Collection to address digital humanities research questions in projects that were transnational in scope. But the development of the Europeana 14-18 Collection, and most other GCG initiatives, did not intend research as the end goal. Rather these resources were developed with the objective to mobilise communities; to stir and engage. But they leave in their wake a vast corpus of heterogeneous data that can be used beyond the initial design of these initiatives, and when non-professionals create data that can be used by communities and professional researchers alike, the transformative nature of this material is inevitably magnified.  This issue raises several questions. Principal amongst these is: do the methodologies of creation, and the value-based motivation of those developing projects and people who submit material to these CGC projects, support re-use? What, therefore, is the research potential for this content? Are they just localised Wikipedias or blogs, designed for superficial non-professional reference (and, if so, does the intentionality of their creation enable them to have greater re-use as digital outputs once metadata, copyright and other aspects of the critical framework for digital heritage are linked and deployed)? Once this data is (re)used for research, then how is the past reconstructed and reinterpreted in the digital domain by historians (which connects to the issue of digital history often being focused on tool generation and use rather than on re-theorising history itself) – in short, does CGC subvert or just supplement existing paradigms of digital information use in history? How can we take the existing model of landmark commemorations (such as the centenary of the First World War), with their significant peaks of public engagement, and systematise our findings?  In the paper we will point to some contextual answers to these questions. Our initial conclusion is that the meshing of CGC and research has not delivered on its transformational promise, primarily due to structural factors in the engagement of researchers with this material, and so this lack of research ‘value’ must be understood as having a negative impact on participants, as outlined in the matrix of value described in section 1, above.   “Some say forget the past, and some say don't look back  But for every breath you take, you'll leave a track”    The final strand is an examination of the parallels between community generated digital content and the establishment of community archives and ‘History from below’, the basis of a significant body of archival research. The real parallel is the fragility of the digital content, as much CGC ends up as effectively ‘orphan’ content – the poor likelihood of digital sustainability parallels exactly the fragility of similar analogue content, and raises similar challenges of post-custodial care.    This directly mirrors the fragility of community archives generally. The Digital Preservation Coalition have rated community archives and CGC as ‘critically endangered ’ (  https://www.dpconline.org/our-work/bit-list/critically-endangered  ) .     While the sustainability of such content is itself an issue, as with our research concerns above it is essential to recognise that the fragile sustainability of CGC negatively affects its value to participants. In fact, the interplay of participant value and future sustainability should be recognised, alongside infrastructural issues of preservation : the value of the content may be incidental to the experience of creating it. To quote a respondent in the Europeana 16-18 study, their largest takeaway from the experience of CGC was that ‘my neighbours and I have the same history’. However, while acknowledging this, we must also challenge the digital heritage community to communicate their expectations and responsibility for sustainability of CGC to participants in their projects.     The authors will explore the three stands and the value of bringing them together in synthesis, drawing on their research into digital approaches to the First World War Centenary generated through the Europeana Research initiative, and two UK Arts and Humanities Research Council projects,  Living Legacies and  Reflections on the Centenary of the First World War. By digging into the centenary of the First World War and the digital ‘big bang’ of content it has engendered, it is possible to create a detailed case study to address the value and digital legacy of community generated content that is, methodologically, of significance to broader issues around using and sustaining digital histories.   ",
        "article_title": "“The Ties That Bind': The Creation, Use, And Sustainability Of Community Generated Histories",
        "authors": [
            {
                "given": "Lorna",
                "family": "Hughes",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Agiatis",
                "family": "Benardou",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow and Athena RC",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-06",
        "keywords": [
            "digital humanities (history",
            "crowdsourcing",
            "methods and technologies",
            "sustainability and preservation",
            "theory and methodology)",
            "English",
            "public humanities and community engaged scholarship",
            "cultural artifacts digitisation - theory",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Congressional investigations and testimony before Congressional committees is a commonly used source in labor history (as well as, of course, in historical and political science scholarship more generally.) Congressional investigations into the causes of economic depressions (e.g. 1878-79), the problems of industrial relations (1900-01, 1913-15), anti-labor practices (1936-41), and alleged malfeasance within labor unions (1957-60) have provided an important body of evidence for students of labor-capital relations, working-class culture, business ideology, and the like (see e.g. Auerbach, 1966; Greene, 1998; McCartin, 1997; Witwer, 2003.) As a subject in its own right, however, testimony before Congress has received only limited attention from labor historians, and little enough from other scholars, though there is relevant work on specific committees and general lobbying (Clemens, 1997; Harris and Tichenor, 2009; Herring, 1929; Tichenor and Harris, 2002-2003; Tichenor and Harris, 2005). The focus in this paper is on what these hearings can tell us about the relative power of workers in the society over time. The U.S. has no labor party, and American unions have traditionally been ambivalent about electoral politics and legislation (Greene, 1998; Archer, 2010). Yet they have sought to be heard in Congress. This paper combines metadata about testimony at Congressional hearings and data about union membership and strike frequency in the U.S. to argue that this effort has been most successful when union penetration of the civil society as well as union involvement in electoral politics have been strongest, emphasizing the efficacy of a multipronged and organizing-based approach .   Basics about the data and processing The data set used here is extracted from the ProQuest Congressional database, which contains metadata on hearings and witnesses for all Congressional hearings. These are proprietary, but all publishable data as well as scripts will be available at  https://github.com/vhulden/congressionalhearings/ (along with more detail on the technical procedures.)  The full data set contains between about 62,500 and 85,000 unique hearings (the first is by unique title and the second by unique hearing ID) and a total of 941,302 instances of testimony (of course, many witnesses appear multiple times.) The average number of witnesses (or testimonies) per hearing is 12.2 (it is higher in earlier years and stabilizes to about 10.5 witnesses per hearing after 1900.) The subset under closer examination here consists of those hearings that concerned organized labor, wages, jobs, working people, labor management, and the like. The basic data was further processed to attempt to assign witnesses into broader categories by their organizational affiliation, which the data contains for 83 percent of the instances of testimony. Since the focus in the current paper is on what hearings data can tell us about workers and their relative strength vis-à-vis business representatives, the main categories considered here are labor, companies, and trade associations; in addition, I have included the two largest categories of witnesses, the federal bureaucracy and the political parties (usually Congressional representatives themselves.)   Labor topics and witnesses Figure 1 suggests that Congressional attention to matters related to work and labor has been fairly constant.    Figure 1: Hearings related to work and labor as percentage of all hearings, 1877-1990 Unsurprisingly, labor has been better represented at hearings on labor topics than at the average Congressional hearing, as shown in Table 1, which displays what percentage of the testimonies come from different witness categories.    Witness category In full data (percent) At labor-related hearings (percent)   Labor 2.5 5.9   Companies 7.2 5.9   Trade associations 2.0 1.9   Federal government 10.6 7.5   Political parties 12.0 7.5   Table 1:  Instances of testimony at Congressional hearings, 1877–1990  Companies and trade association representatives dominate over labor ones in both data sets, but much less so at the labor-related hearings. Moreover, plotting the representation of different groups over time reveals that these percentages have not held constant.    Figure 2:  Representation of witness groups over time, by percentage of instances of testimony  In two ways, this chart emphasizes the significance of electoral politics. One, the sheer number of testimonies from inside the federal bureaucracy and the political parties emphasizes that even at Congressional hearings, succeeding in electing friendly politicians mattered. Two, it is clear that labor representation was strongest in the periods when organized labor concerned itself with electoral politics. The New Deal era, when labor unions formed an important part of the Democratic coalition, forms the only period when labor testimonies were consistently on a rough par with business and trade association testimonies. Similarly, significant spikes of labor representation around the turn of the twentieth century and in the Progressive Era coincide with the American Federation of Labor’s campaign to support labor’s friends in either party, while some spikes in the 1920s perhaps relate to the brief but significant challenge to the major parties from the Farmer-Labor Party.     Strikes, union density, and labor representation at hearings How well labor has been represented at hearings also seems to correlate with labor’s strength outside the electoral context in the form of strikes and union densities.    Figure 3:  Number of strikes, percentage of strikers of labor force, and union density correlated with labor representation at Congressional hearings  Note in particular how labor’s representation stabilizes as strike activity becomes more consistent in the 1930s (and union density reaches a plateau.) However, note also how the downward trend of labor representation at Congress tracks declining union density and the decline in “strike density,” despite a spike in the number of strikes in the 1970s.     Discussion and future work The data at hand is, of course, limited and imperfect. Nevertheless, at the very least these charts underline that labor’s power is consistent mainly when labor is strong by a number of measures, from strike activity to union density to electoral participation; individual strike waves around the turn of the twentieth century did not correlate with consistent labor representation at Congressional hearings, despite occasional spikes in the prevalence of labor testimony. The lack of impact from the spike in the number of strikes in the 1970s also seems to suggest that incidents of labor strife are insufficient by themselves, if the penetration of labor (strike “density” and union density) is low or declining. More careful statistical analysis is needed (on a cleaner data set) to tease out a more exact relationship, but overall, these correlations seem to suggest a multipronged, organizing-intensive strategy to increase representation. Future work might consider electoral campaigns and legislative outcomes to further elucidate labor’s fortunes in Congress.  ",
        "article_title": "Labor Witnesses at U.S. Congressional Hearings: Historical Patterns",
        "authors": [
            {
                "given": "Vilja",
                "family": "Hulden",
                "affiliation": [
                    {
                        "original_name": "University of Colorado Boulder",
                        "normalized_name": "University of Colorado Boulder",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02ttsq026",
                            "GRID": "grid.266190.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-22",
        "keywords": [
            "data mining / text mining",
            "English",
            "history and historiography",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In Tibetan culture, treasure texts refer to those written scrolls hidden secretly centuries ago by the great master Padmasambhava in the Yarlung Dynasty (8 th C.) and discovered by later generations. Those who have the ability to detect these hidden objects ( gter ma) are called treasure revealers ( gter ston). This practice is especially significant in the Nyingma School, one of the four major Schools in the Tibetan Buddhist tradition. It was a creative invention in response to the other three schools, who claimed that their doctrinal texts were translated from Sanskrit, and thus of pure Indian origin.    The  Rin chen gter mdzod is a large corpus compiled by 'Jam mgon kong sprul blo gros mtha' yas (1813-1899, hereafter Kongtrul), a famous prolific writer and one of the propagators of the non-sectarian movement in the East Tibetan region in the 19 th century. Kongtrul made efforts to collect scattered treasure texts to prevent their loss. It contains the works of 108 treasure revealers under the classifications of Mahayoga, Anuyoga and Atiyoga. This project focuses on texts classified in the Mahayoga section, in total, 54 out of 72 volumes. This project will shed some light on the meaning of treasure rediscovery and textual invention and reuse in Tibetan religious culture. Considering the amount of data, we try to implement digital technology to compare each phrasing in order to detect reused sentences, thus we can further interpret the so-called intertextuality of Tibetan treasure literature.  Although Tibetan scholars have already noticed the phenomenon of text reuse in the treasure literature ( gter ma), it remains difficult to conduct a large scale comparative reading and further identify repeated sentences and locate their origin. Deducing from previous studies, we estimate that there might be greater intertextuality  embedded in the writings of treasure texts than has already been noted. There has been no systematic analysis of large Tibetan textual collections in academic circles so far, thus we propose to apply digital textual analysis technology to deconstruct the great corpus of Tibetan treasure— the Mahayoga section of the  Rin chen gter mdzod. After a trial period spent on this research project, we find it is an approachable goal.   Detection of Text Reuse  The data to be analyzed in this study is the content of the  Rin chen gter mdzod. We use the XML file published by the BDRC organization on its website   The files of the  Rin chen gter mdzod published on the BDRC (https://www.tbrc.org/#!rid=W1KG14) is digitalized based on the Shechen edition, which consists of 54 volumes. We numbered each document by volume and by document in each volume, and obtained a total of 2,643 files from 1-1 to 54-40.   as the primary source. The contents of these files are in Tibetan. In order to simplify the difficulties in the subsequent processing, we have used the Wylie transliteration system (Wylie 1959) to transliterate the Tibetan texts of the documents into the Roman alphabet. Here is an example of Wylie Transliteration.     In the Tibetan writing system, the double perpendicular stroke (Tib.  nyis shad) is used to separate complete sentences (represented by // in Wylie Transliteration). Therefore, we can use the punctuation to separate the text into sentences.   In order to be able to effectively find the part of textual reuse between sentences, we adopt the Local Alignment algorithm, which is commonly used in the field of bioinformatics to perform matching of long DNA sequences. The biggest advantage of Local Alignment is that it can efficiently find the maximum matching area of two strings, and consider possible insertions and deletions of characters in the strings. An example of the comparison result is as follows:    The “○” in the comparison result denotes inserted characters of the string (b+h+rU~M 'od du zhu form no.22-36, sentence no.6), and “ ◎” denotes a deletion of original string ( here is oM form no. 4-24, Sentence no.6). After comparing all sentences, we found 14,478 paired sentences with more than 20 words repeated.    Database and Web Interface Construction In order to assist other researchers who are also interested in exploring this topic, we have created a database with an easy-to-use web interface   http://syda.dila.edu.tw/RCGM/  . This website is still under development, yet currently provides the following two functions:    When a user enters a complete section of a text, the system will highlight those sentences which repeatedly occur in other texts. For example, the figure below shows the content of the 401 st sentence of text No.5-5 has two other repetitions: the 715 th sentence of text No. 30-19 and the 350 th sentence of text No. 26-20.  The user may further query the details of a compared result of two highly overlapped sentences. For example, the figure below shows the content of the 401 st sentence of text No. 5-5, and the 715 th sentence of text No. 30-19 in the same window. In addition, the contexts of the two sentences are also displayed on the screen, which makes it convenient for the user to assess.         Preliminary analysis According to our preliminary analysis, we obtained the following results. Firstly, reused sentences do recur among the works of different revealers in different times. Taking two important revealers from the 12 th-13 th centuries as example, there are 242 matches between the works of Nyang ral nyi ma 'od zer (1124-92, hereafter Nyi ma 'od zer) and Gu ru chos kyi dbang phyug (1212-70, hereafter Guru chos dbang). Other detected and highly duplicated sentences are derived from works of revealers in the 14 th, 15 th and 17 th centuries. In particular, we noticed that the  Eight Collected Teachings of Sugata ( bka' brgyad bde gshegs 'dus pa) was composed by Guru chos dbang, Rig 'dzin rgod ldem (1337-1408) and Gyur med rdo rje (1646-1714) respectively. This study enables us to take a closer look at their actual contents and editing methods. Secondly, regarding textual comparison, we randomly chose a sentence, as follows:   “ 'on kyang bla med thun mong ma yin pa'i skabs 'dir dam ye gnyis med dbugs rngub pa'i sbyor ba dang bstun rang la  ◎ pas mi shigs pa'i thig ○ ◎ ro gcig tu ○ ○ bsams kyang legs so”.   We notice that this sentence has appeared both in text No.14-25,  The Pacifying Homa-teaching on splendid great bliss from the Collected Teachings of Sugata ( bDe gshegs 'dus pa'i zhi ba'i sbyin sreg bde chen rab 'bar)    Volume 14 (ཕ) / Pages 745-762 / Folios 1a1 to 9b4.   and text No.31-23,  The Blaze of the Sharp Blade of Vajrakilaya – The Homa of quick bestowal of pacification and enrichment ( rDo rje phur pa yang gsang spu gri 'bar ba'i zhi rgyas kyi sbyin sreg grub gnyis myur sbyin)    Volume 31 (ཀི) / Pages 489-516 / Folios 1a1 to 14b1.  . Both can be traced back respectively to Nyi ma 'od zer’s and Guru chos dbang’s writings. It is not clear for us yet why Kongtrul classified them in different categories, nevertheless, through close reading we find that there is an entire duplicated ritual section, which is far beyond the scope of the detected sentence. Further analysis will rely on close reading. Yet it is the power of artificial intelligence that has brought us to this domain.    ",
        "article_title": "Exploring Intertextuality in the Mahoyoga Section of the Rin chen gter mdzod",
        "authors": [
            {
                "given": "Ching-Hsuan",
                "family": "Mei",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            },
            {
                "given": "Jen-Jou",
                "family": "Hung",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "corpus and text analysis",
            "theology and religious studies",
            "digital textualities and hypertext",
            "content analysis",
            "data mining / text mining",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Traditional Chinese Phonology, lacking of alphabetic system of phonetic notation such as IPA, had to deal with large written materials in Chinese characters, and used Chinese characters as a tool to analyze sounds of words. This brings up a significant feature of its study, that is, the relationships of words’ sounds are more important than their phonetic values. Xìli án (literally: \"inter-link\") is one of the most important methods in traditional Chinese phonology. Its fundamental is to build networks of Chinese characters having same syllabic elements. This paper takes Xìli án of Fǎnqiè in  Gu á ngyùn as an example to show how to use network analysis and visualization software to improve traditional Chinese phonology study.  In general, Chinese characters are monosyllabic. A Chinese syllable can be divided into three parts: the Initial (Shēngmǔ), the Final (Yùnmǔ), the Tone (Shēngdi ào). The final can be further subdivided into the Medial (Jièyīn), the Main Vowel (yùnfù) and the Coda (yùnwěi), while the Medial and Coda are optional.     Fǎnqiè is a Chinese method to indicate the pronunciation of a monosyllabic character by using two other characters. The first one, known as Fǎnqiè-Sh àngzì, has the same initial as the desired character, known as Bèiqièzì. And the second one, known as Fǎnqiè-Xi àzì has the same final and tone as Bèiqièzì. Here is an example. “端(du ān), 多(du ō)官(gu ān)切”. The Bèiqièzì is “端”. The Fǎnqiè-Sh àngzì is “多” indicating that the initial of “端” is “d”. The Fǎnqiè-Xi àzì is “宗” indicating that final of “冬” is “u ɑn” and the tone is “-”.  Naturally, any Fǎnqiè was meaningful when it was created, but may not keep its perfection as time goes by, due to phonetic changes. For example, “東(dōng), 德(dé)紅(hóng)切”. “德” still has the same initial as “東”, while “紅” has the same final but different tones. Thus, the systematic use of such Fǎnqiè in ancient rime dictionaries is an invaluable resource for the work of historical linguists.  Guǎngyùn is a Chinese rime dictionary compiled in 1008AD. It is a revision and expansion of  Qièyùn, the influential rime dictionary published in 601AD. It is generally accepted that  Qièyùn recorded the voice of Chinese at that time, maybe not as a spoken language, but rather how characters should be pronounced when reading the classics. So  Guǎngyùn, as the most accurate available account of  Qièyùn in the past, was used by traditional scholars as a major source on the reconstruction of  Qièyùn system, the code name of Middle Chinese.   Guǎngyùn is split into four tones in five volumes (Two for Píng tone, and one each for other three: Shǎng, qù, rù). Each tone is split into rimes, with total of 206 rimes (including final and tone). Each rime is divided into groups of homophonous characters, with a character as the representation, and the pronunciation of each group given in  Fǎnqiè formula.  It is Chénlǐ, in his masterpiece  Qièyùn-Kǎo published in 1842, who first introduced Xìli án method in the study of Fǎnqiè in  Guǎngyùn. The Principle of Fǎnqiè-Xìli án comes from the idea that the relation between Bèiqièzì and Fǎnqiè-sh àngzì or Fǎnqiè-xi àzì is an equivalence relation.  In mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. Any equivalence relation provides a partition of the underlying set into disjoint equivalence classes. Two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class. That means, if we look for the Fǎnqiè-sh àngzì of each Fǎnqiè-sh àngzì and link them together with one another, we can obtain equivalence class of Fǎnqiè-sh àngzì representing a same initial. By systematically applying this method, it becomes possible to make classes of Fǎnqiè-sh àngzì for the initials, and Fǎnqiè-xi àzì for the rimes (including the final and tone) of  Guǎngyùn. When two classes were unable to link each other by any method, it may conclude that they represent distinct initials and distinct rimes.    Figure 1: A page in  Guǎngyùn   In old days, Fǎnqiè-Xìli án of Guǎngyùn would cost a lot of time and the process is hard to display. So faced the same materials, researchers had to put lot of energy in repeating works to verify others’ results. It was often difficult to find out what went wrong when there were disagreements. Today, with the help of the network analysis and visualization software package like Gephi, it becomes much more easily to display one’s own work or review other’s work in both researching and teaching. Fǎnqiè-sh àngzì (or Fǎnqiè-xi àzì) are the nodes. The equivalence relation between them are undirected links. Then the components in the undirected network are equivalence classes of Fǎnqiè-sh àngzì (or Fǎnqiè-xi àzì) which represent different initials (or rimes).  The process is simple and repeatable. The first step is to convert the text of  Guǎngyùn to a structural form, see Table 1. The second step is to find the no repeated Fǎnqiè-sh àngzì (or Fǎnqiè-xi àzì) appeared in Guǎngyùn. The third step is to find the Fǎnqiè-sh àngzì of each Fǎnqiè-sh àngzì (or Fǎnqiè-xi àzì of each Fǎnqiè-xi àzì) in step 2, see Table 2. The fourth step is to transform the equivalence relations between Bèiqièzì and Fǎnqiè-sh àngzì or Fǎnqiè-xi àzì into links between nodes representing these characters. The fifth step is to convert those links in step 4 to a network with a network analysis and visualization software package, like Gephi, see Figure 2. The sixth step is find all the components in the network, Figure 3 shows an example.  However, it is not the end of our study of Middle Chinese Phonology, but only the beginning. Over more than 100 years, there is still no consensus on exactly how many initials and finals in Middle Chinese. The reasons are complicated, partly due to different versions of Guǎngyùn, partly due to various origins of Fǎnqiè in Guǎngyùn. Owing to digital method, it becomes much more convenient to demonstrate the works of key scholars and to locate the exact Fǎnqiè that caused their disagreements.   Table 1: The Structural Table of  Guǎngyùn   Rhyme Homophonous Group  F ǎ nqiè-sh à ngzì     F ǎ nqiè-xi à zì    上平1東  東菄鶇䍶 𠍀 倲 𩜍𢘐 涷蝀凍鯟 𢔅 崠埬 𧓕 䰤  德 紅   上平1東  同仝童僮銅桐峒硐 𦨴𧱁 筒瞳㼧 𤭁 罿犝筩潼曈洞侗橦烔䴀挏酮鮦㼿 𦏆𦍻 眮蕫穜衕 𩍅𢈉 䆚哃 𢏕 絧 𨝯𨚯𪔝𩦶𪒿  徒 紅   上平1東  中衷忠 𦬕  陟 弓   上平1東  蟲沖种盅爞 𦬕 翀  直 弓   上平1東  終衆(眾)潨 𣧩𧑄𩅧 䝦䶱䈺螽鼨蔠柊鴤泈  職 戎   …… …… …… ……    Table 2: The Fǎnqiè-sh àngzìof each Fǎnqiè-sh àngzì in  Guǎngyùn   ID  F ǎ nqiè-sh à ngzì in  Guǎngyùn   F ǎ nqiè-sh à ngzì of F ǎ nqiè-sh à ngzì    1 德 多   2 徒 同   3 陟 竹   4 直 除   5 職 之   6 敕 恥   7 鋤 士   8 息 相   9 如1 人   …… …… ……     Figure 2: The Network of Fǎnqiè-sh àngzi     Figure 3: A Component in the Network of Fǎnqiè-shàngzi  ",
        "article_title": "Using Network Analysis to Do Traditional Chinese Phonology Study",
        "authors": [
            {
                "given": "Jiajia",
                "family": "Hu",
                "affiliation": [
                    {
                        "original_name": "Beijing Normal University, China, People's Republic of",
                        "normalized_name": "Beijing Normal University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/022k4wk35",
                            "GRID": "grid.20513.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-10",
        "keywords": [
            "linguistics",
            "English",
            "network analysis and graphs theory",
            "philology",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Abstract. This paper presents a new knowledge-based approach for finding serendipitous semantic relations between resources in a knowledge graph. The idea is to characterize the notion of \"interesting connection\" in terms of generic ontological explanation rule patterns that are applied to an underlying linked data repository to instantiate connections. In this way, 1) semantically uninteresting connections can be ruled out effectively, and 2) natural language explanations about the connections can be created for the end-user. The idea has been implemented and tested based on a knowledge graph of biographical data extracted from life stories of 13100 prominent historical persons in Finland, enriched by data linking to collection databases of museums, libraries, and archives. The demonstrator is in use as part of the semantic portal BiographySampo of interlinked biographies.   Approaches to relational search   Serendipitous knowledge discovery (Baker et al., 2007) is one of the grand promises and challenges of the Semantic Web. This paper concerns on the problem of discovering serendipitous relations (a.k.a connections, associations) in semantically rich, linked Cultural Heritage (CH) data (Hyvönen, 2012), i.e., Knowledge Graphs (KG). In particular, we focus on the problem of finding \"interesting\" (Silberschatz and Tuzhilin, 1995) connections between the resources in a KG, such as persons, places, and other named entities. Here the query   consists of two or more resources, and the task is to find semantic relations, i.e., the query results, between them that are of interest to the user.  This problem has been addressed before in different domains. The approaches reported in the literature (Cheng et al., 2017) differ in terms of the query formulation, underlying KG, methods for finding connections, and representation of the results. Some sources of inspiration for our paper are shortly reviewed below. In (Sheth et al., 2005) the idea is applied to association finding in national security domain. Within the CH domain, CultureSampo (Hyvönen et al., 2009)(Mäkelä et al., 2012) contains an application perspective where connections between two persons were searched using a breath-first algorithm, and the result was a list of arc chains (such as student-of, patron-of, etc.), connecting the persons based on the Getty ULAN knowledge graph of historical persons. In RelFinder (Lohmann et al., 2010)(Heim et al., 2010)(Heim et al., 2009), based on the earlier \"DBpedia Relationship Finder\" (Lehmann et al., 2007), the user selects two or more resources, and the result is a minimal visualized graph showing how the query resources are related with each other, e.g., how is Albert Einstein related to Kurt Gödel in DBpedia/Wikipedia. Both gentlemen, e.g., worked at the Princeton University. In WiSP (Tartari and Hogan, 2016), several paths with a relevance measure between two resources in the WikiData KG can be found, based on different weighted shortest path algorithms. The query results are represented as graph paths. Some applications, such as RelFinder and Explass (Cheng et al., 2014), allow filtering relations between two entities with facets. From a methodological perspective, the main challenge in these systems is how to select and rank the interesting paths, since there are exponentially many possible paths between the query resources in a KG. This problem can be approached by focusing only on \"simple paths\" that do not repeat nodes, on only restricted node and arc types in the graph (e.g., social connections between persons), and by assuming that shorter, possibly weighted paths are more interesting than longer ones. For weighting paths, measures such as page rank of nodes and commonness of arcs can be used. The graph-based works above make use of generic traversal algorithms that are application domain agnostic. In contrast, this paper suggests an alternative, knowledge-based approach to finding interesting connections in a KG. The idea is to formalize  the notion of \"interestingness\" (Silberschatz and Tuzhilin, 1995) in the application domain using general explanation patterns that can be instantiated in a KG by using graph traversal queries, e.g., SPARQL. The benefits of this approach are: 1) non-sense relations between the query resources can be ruled out effectively, and 2) the explanation patterns can be used for creating natural language explanations for the connections, not only graph paths to be interpreted by the end user. The price to be paid is the need for crafting the patterns and queries manually, based on application domain knowledge, as customary in knowledge-based system. In the following, a case study of applying this approach is presented in the Cultural Heritage domain by using a KG of biographical data. In conclusion, lessons learned are discussed, and further research suggested.    Finding semantic relations in a biographical knowledge graph  In historical research, one is often interested in finding out relations between certain types of things or persons, such as Finnish novelists, and larger areas, such as South America. Our tool, Faceted Relator, can be used for solving such problems. Faceted Relator combines ideas of faceted search (Tunkelang, 2009) and relational search. The idea is to transform a KG into a set of instances of interesting relations for faceted analysis. A relation instance has the following core properties: 1) a literal natural language expression that explains the connection in a human readable form. 2) a set of properties that explicate the resources that are connected. For example, the following illustrative example of a tertiary relation <X, Y, Z> connects Leonardo da Vince to Vince and to year 1452 based on the explanation pattern “Person X was born in place Y in Z” for birth events: :c123 a :BirthConnection; :explanation \"Leonardo da Vinci was born in Vince in 1452\"; :place :vince; :time 1452; :person :Leonardo_da_Vince . :BirthConnection rdfs:label \"Person X was born in place Y in time Z\" .  Relation instances like this can be searched for in a natural way using faceted search, where the facets are based on the properties of the instances, that can often be organized hierarchically. In this case, there would be a facet for explanation types (such as :BirthConnection), and facets for places (in a partonomy), persons (that may be organized into a hierarchy based on, e.g., occupation or nationality), and times (in a partonomy). By making selections on the facet hierarchies, the result set is filtered accordingly and hit counts in facets recalculated. This method was tested in the context of BiographySampo (Hyvönen et al., 2019), a linked data service and semantic portal aggregating and serving biographical data. The knowledge graph of this system includes several interlinked datasets:  Biographical data extracted in RDF form from 13144 Finnish biographies. The data includes, e.g., 51937 family relations, 4953 places, 3101 occupational titles, and 2938 companies.   HISTO ontology of Finnish history including more than one thousand historical events. Data for the events includes people and places related to the event. The data was available in RDF format. The Fennica National Bibliography is an open database of Finnish publications since 1488. The metadata includes, among other things, the author of the book and the subject matter of the book, which can include places. BookSampo data covering virtually all Finnish fiction literature in RDF format, maintained by the Finnish Public Libraries consortium Kirjastot.fi. The Finnish National Gallery has published the metadata about the works of art in their collections. The metadata is described using Dublin Core standard and was available in JSON and XML format. The collected works of the J. V. Snellman portal includes the texts written by J. V. Snellman, the national philosopher of Finland. The data includes, e.g., 1500 letters. We transformed the data into RDF.  The focus in our demonstrator is on finding relation instances describing connections between people and places in Finnish cultural history. The relations listed in Figure 1 were created using SPARQL CONSTRUCT queries with natural language explanations. For example, the following template is used for explaining artistic creation relations related to painting collections: \"< person name> has created a work of art called < painting name> in the year < year> that depicts < place name>.\"      Relational instances extracted from the data      Demonstrator at work Faceted Relator was published as part of the BiographySampo portal, and is in use as a separate application perspective in it. Figure 2 depicts the user interface of the application. The data and interface are in Finnish, but there is a Google Translate button in the right upper corner of the interface for foreign users available. Faceted Relator can be used for filtering relations with selections in four facets seen on the left: 1) person names, 2) occupations, 3) places, and 4) relation types. The system shows a hit list of the relation instances that fit the selected filtering criteria in the facets. The user is not required to first input a person and a place, but can limit the search at any time with any facet. This allows searching for relations between groups of people and larger areas instead of single places. Each relation instance is represented in a row that shows first a natural language explanation of the relation, then the related person, place, and data source as links to further information, and finally the relation type. Different types of relations are highlighted in different colors and have their own symbols in order to give the user a visual overview of relations found. At any point, the distribution of the hit counts in categories along each facet can be visualized using a pie chart—one of them can be seen in the left upper corner of Figure 2.     View of the user interface    For example, the question \"How are Finnish painters related to Italy?\" is solved by selecting \"Italy\" from the hierarchical place facet and \"painter\" from the occupation facet. Any selection automatically includes its subcategories in the facet. For example, places such as Florence and Rome are in Italy, and Vatican further in Rome. The result set in this case contains 140 connections of different types whose distribution and hit counts can be seen on the connection type facet. In the same way, the person facet shows the hit count distribution along the person facet. Any facet could be used to filter the results further, if needed. In this case the 140 hits include, e.g., connection \"Elin Danielson-Gambogi received in 1899 the Florence City Art Award\" and \"Robert Ekman created in 1844 the painting 'Landscape in Subiaco' depicting a place in Italy\". In a similar way, the question \"Who has got most awards in Germany\" can be solved by selecting the connection type \"Received an award in a place\" and \"Germany\" from the place facet. The hit distribution along the person facet shows that general Carl Gustaf Mannerheim is the winner with eight German awards. The demonstrator is based on an architecture with the server side consisting of a Apache Jena Fuseki graph store and the client side consisting of an application written with AngularJS. The faceted search was implemented with the SPARQL Faceter (Koho et al., 2016) tool.   Discussion An informal initial evaluation and testing of the demonstrator showed that it works as expected in test cases, and that a layman can potentially learn new information by using the system. However, more testing is needed to find out how interesting and surprising the results are for an expert of CH and how a system like this can be used for DH research. We also found out needs to improve the usability of the system. For example, the demonstrator now sorts results based on firstly the name of the person and secondly on the name of the place. The user should probably be offered the possibility to sort the relations freely along any facet.  Acknowledgements Our research was supported by the Severi project, funded mainly by Business Finland. The authors wish to acknowledge CSC – IT Center for Science, Finland, for computational resources.   ",
        "article_title": " Relational Search in Cultural Heritage Linked Data: A Knowledge-based Approach  ",
        "authors": [
            {
                "given": "Eero",
                "family": "Hyvönen",
                "affiliation": [
                    {
                        "original_name": "Aalto University, Finland; University of Helsinki (HELDIG)",
                        "normalized_name": "Aalto University",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/020hwjq30",
                            "GRID": "grid.5373.2"
                        }
                    }
                ]
            },
            {
                "given": "Heikki",
                "family": "Rantala",
                "affiliation": [
                    {
                        "original_name": "Aalto University, Finland; University of Helsinki (HELDIG)",
                        "normalized_name": "Aalto University",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/020hwjq30",
                            "GRID": "grid.5373.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-16",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "library & information science",
            "semantic web and linked data",
            "ontologies and knowledge representation",
            "information retrieval and query languages",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Music Information Retrieval offers new options for musicological research, particularly for methodologies which are hard (or impossible) to carry out manually, e.g., large corpus studies and measurings of acoustical properties. One such field of application is the mining of patterns. Patterns – and repetitions in general – play an important role in nearly all music styles and are thus of interest for many sub-fields of musicology. In particular, melodic patterns – or ‘licks’ as patterns are often called in jazz parlance - form a crucial component of jazz improvisation (Berliner, 1994; Norgaard, 2014; Owens, 1974). Given the significance of patterns and licks in jazz, several research questions arise, concerning historical issues, e.g., the oral tradition of melodic patterns and the development of a typical jazz language, or more systematic issues, e.g., the psychology of the creative process, where patterns can be regarded as necessary to accomplish the highly virtuoso feat of modern jazz improvisation:  To what extent are patterns used to shape an improvisation? When and by whom are patterns created and how are they transmitted between players over time (pattern archaeology)? Does pattern usage change with time and styles? Is there an influence of jazz education on pattern usage (e.g., by published pattern collections)? How are patterns used to build phrases, e.g., to construct a typical bebop line? Which role do external musical influences such as quotes, and signifying references play in jazz improvisation?  This paper presents three novel user interfaces for investigating the pattern usage in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation.   Related work Several web-based melody search engines have been developed in the past, e.g. Themefinder ( http://www.themefinder.org) which allows searching for patterns in both classical and folk music and  Musipedia ( https://www.musipedia.org ), a user-generated database of melodies, providing more sophisticated and user-friendly search interfaces like a virtual piano keyboard for entering melodic patterns and a query by tapping interface for rhythm search. A more thorough overview of search engines can be found in (Frieler et al., 2018). Unfortunately, many of these projects are discontinued or use out-dated web technology. An example for an up-to-date search interface including metadata filters is the RISM catalogue search ( https://opac.rism.info ).    User Interfaces The  Pattern History Explorer is a Shiny application that allows exploring a set of 653 patterns that are most commonly used by eminent players ( ). The  Pattern Search is a web interface for a general two-staged pattern search Martin2019-04-30T10:24:00 Nicht nur dort!featuring regular expressions ( https://dig-that-lick.hfm- weimar.de/pattern_search). The  Similarity Search faces the challenge of finding and grouping similar patterns, i.e., patterns that differ in one or several tones ( ). The applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.  The main goal of the  Pattern History Explorer is to enable the exploration of interval patterns in jazz solos by providing information from many different angles. It maintains an overview of interval patterns frequently used by a selected subset of performers and traces their usage in the Weimar Jazz Database (Pfleiderer, 2017), allowing for the discovery of cross-artist and cross-temporal relationships. Currently, 653 interval patterns with 11,630 instances are included. The pattern corpus was created by mining for interval patterns in solos of eminent performers using the partition mode of the  melpat module from the MeloSpySuite (Frieler, 2017). Subsequently, all instances of these patterns were searched for in the Weimar Jazz Database, and the results were included in the application. In general, the user of the Pattern History Explorer selects a certain interval pattern from the overall set of 653 patterns. Several options are available in order to filter the pattern set or to change the ordering of the patterns according to several criteria (e.g., filtering by performer, length, intrinsic characteristics such as Huron contour (Huron, 1996), tonality type, or tonal content). For the selected pattern, various kinds of information can be accessed in several tabs.  While pre-computing a set of patterns is helpful for exploratory investigations, searching for instances of arbitrary patterns of any length and frequency of occurrence within a database requires a different approach. The web-based  Pattern Search interface provides most of the functionality of the  melpat search module of the MeloSpySuite while also extending it with audio and score snippets (both as isolated patterns and within their melodic context) for visual and aural inspection. To execute a basic search, the user can enter a pattern on a virtual keyboard or as a space or comma separated list of elements and choose a corresponding transformation, that is, a mathematical mapping of the basic melodic representation (for details, see Frieler, 2017). Currently, ten pitch-related transformations for primary search are offered (e.g., MIDI pitch, semitone intervals, extended chordal diatonic pitch class). An additional 18 transformations, such as duration, inter-onset interval classes and various structural markers, are supplied for the optional secondary search. Additionally, the search space can be constrained by several metadata categories, like performer, style, or recording year. Search patterns can be regular expressions (in a hybrid syntax depending on the selected transformation) which allows searches for variants in a single run. The secondary search can be used to refine the result space, e.g., by filtering out certain rhythmic or metrical configurations or by confining instances to a single phrase. The underlying search algorithm is built upon the basic Python regular expression module, which is fed with virtual Unicode strings constructed from the different melodic representations (transformations) with different alphabets. Scores are generated with the help of Lilypond, while audio snippets are directly extracted from the solo audio files using  ffmpeg.   The latest addition to the web-based toolset is the  Similarity Search web application. Basically, the interface follows the design and concept of the  Pattern Search , however, with some significant differences. Not only identical pattern instances, but also similar patterns, that differ in one or more tones from the query, can be searched for. The calculation of search results differs from that of the  Pattern Search in that the actual similarity calculation is done using the underlying PostgreSQL database system implementation of the Levenshtein  distance (edit distance). This distance measure has been shown in various studies (e.g., Frieler & Müllensiefen, 2006; Grachten & Arcos, 2004; Gulati, 2016) to be a good approximation to similarity judgements of melodies by human experts. The similarity search operates on a database of the complete set of pitch and interval n-grams of up to 20 elements that were previously extracted from the Weimar Jazz Database using the  melpat module. This amounts to about 4 million n-grams (instances) for MIDI pitch and 3.5 million for interval n-grams. By entering a pattern (n-gram) as a space or comma separated list of elements and choosing a transformation, similar n-grams can be retrieved from the database. To further control the result set, the search interface provides options for parameters such as ‘minimum similarity’ (calculated using the inverse of Levenshtein distance), ‘maximum length difference’ (allowing for n-grams of differing length), or the preservation of melodic contour and pitch range. All searches can also be refined using metadata filters for performers, instruments, etc.    Visualizations Search results are presented in tabular form together with two graphical representations allowing for visual inspection – an n-gram network graph (Figure 1) and a timeline chart (Figure 2), both generated using the  D3.js data visualization library (Data-Driven Documents, https://d3js.org/). In the case of the network graph utilizing a radial layout, n-grams can be grouped, e.g., by metadata attributes or same similarity value and n-gram class, resp.     Network graph for interval pattern -1,-2,-1,3,3,3,-1,-2 grouped by performer; the biggest bunches stand for those patterns played by Charlie Parker, Dizzy Gillespie, Sonny Rollins and Dexter Gordon. Node colours denote identical patterns which are connected by edges. Node size represents the degree of similarity, where bigger means more similar.   The timeline chart depicts when and by which performer pattern instances were played. Audio snippets for all n-grams found are generated and can be listened to by clicking on the nodes in both the network graph and the timeline chart as well as via the play buttons in the result table.    Timeline chart for interval pattern -1,-2,-1,3,3,3,-1,-2. Node colours denote identical patterns. Node size represents the degree of similarity, where bigger means more similar.     Discussion The three applications are already usable interfaces for the Weimar Jazz Database and serve as prototypes for applications to explore large databases, which are going to be automatically extracted from large collections of jazz recordings, as aimed at within the transatlantic research project “Dig That Lick: Analysing Large-Scale Data for Melodic Patterns in Jazz Performances” (DTL). All three tools can be primarily viewed as bespoke interfaces for the specific needs of jazz researchers, but could easily be transferred to other melodic corpora, too. They could also be of interest to teachers, students and music fans as well as to training courses in computational music analysis.  ",
        "article_title": " Digging Into Pattern Usage Within Jazz Improvisation (Pattern History Explorer, Pattern Search and Similarity Search)  ",
        "authors": [
            {
                "given": "Frank",
                "family": "Höger",
                "affiliation": [
                    {
                        "original_name": "University of Music “Franz Liszt” Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            },
            {
                "given": "Klaus",
                "family": "Frieler",
                "affiliation": [
                    {
                        "original_name": "University of Music “Franz Liszt” Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Pfleiderer",
                "affiliation": [
                    {
                        "original_name": "University of Music “Franz Liszt” Weimar, Germany",
                        "normalized_name": "University of Music FRANZ LISZT Weimar",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/000evwg49",
                            "GRID": "grid.461658.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-04",
        "keywords": [
            "software design and development",
            "multimedia",
            "musicology",
            "data mining / text mining",
            "English",
            "computer science and informatics",
            "audio",
            "video",
            "information retrieval and query languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The problem of political repression in the Soviet Union, despite the interest of a number of researchers, remains insufficiently studied. Some aspects of the implementation of repression or the life of the repressed were left outside of the research focus. For example, the problems of repression against the Volga Germans during the Second World War remain on the sidelines of research due to the following reasons. Firstly, these are not popular topics for modern Russia, and secondly, access to sources is rather difficult today. There are not so many documentary and oral sources, numerous of those people who were repressed left for Germany or stayed in endangered villages in the Urals and Siberia, so it is really difficult to gather materials for the research. At the same time, it is customary for the historical discourse of modern Russia to include the 1940s in the discourse of the Second World War winners (Golovashina, 2017). The great construction of unity among the winners leaves no place for dialogue on traumatic experiences, which includes repression on political and ethnic grounds.  As a result, the historiography of repressions against the Volga Germans is quite fragmentary. And today, scientist researcher should give an answer not only to the questions of the academic community but rather to the questions of the repressed and their descendants, who are asked in an interview the only question «For what?». This project is particularly relevant in terms of preserving the memory of the repressed Germans. Most of the Germans were repressed between 1941 and 1946 when the USSR entered the war with Nazi Germany. There was virtually no real investigation or trial, in accordance with personal directive by I. Stalin (Merten, 2015). The only reason for deportation was the ethnicity of Germans. The purpose of this study is to identify the characteristics of the repressive practices of the USSR government regarding the ethnic group Volga Germans, as well as the analysis of everyday life of repressed people in special settlements and concentration camps in the Molotov region (currently the territory of the Perm region). Also, the study analyses memories from the perspective of the trauma phenomenon and the role/ability of victims and witnesses. The chronological framework of the study covers mainly the period of the Second World War and the 1950s, when the liberalization of the regime in the USSR occurs. The study is based on the approaches of historical anthropology, as well as methods of digital humanities and oral history. In addition, important aspects of the study are the consideration of these events, as well as the memories of the repressed adults and children in terms of the theory of historical memory and the phenomenon of historical trauma and secondary historical trauma.  The study uses the methods of descriptive statistics, information modelling, and content analysis. In addition, an electronic map is being developed, where German deportation routes, special settlements and concentration camps on the territory of Nyroblag will be visualized (the system of camps in the Molotov region, which contained political repressions of Russian, Ukrainian, Jewish, German origin, as well as representatives of other minorities). All objects of the electronic map were equipped with a historical information, which is compiled on the basis of archival documents and interviews. In the process of the research, there was one ethnographic expedition to the north, to the old villages, where we managed to collect unique materials, interview the repressed, as well as the children and grandchildren of the repressed Germans. The method of realization of the study comes, first of all, from the idea of synthesizing sources of various origins (such as official documents, diaries, correspondence, interviews, etc.). Initially, a dictionary with lemmatized word forms is created in several subject groups, all other words are entered into the stop list. Then analyze the statistics on the words count (by interviewing groups: victims and witnesses), this option can help to understand the overall context of the problem. Also, it could be helpful for the understanding the difference between victim’s and witness’s vocabulary that we use for describing of the repression’s experience. The information obtained is divided into four main blocks: events, place names, daily life, and political situation. For conducting content analysis, the software MAXQDA-12 is used.  The electronic map contains more than 150 objects, which are accompanied by historical information. In the future, the map will be further replenished. However, the main special settlements of the repressed and blocks of concentration camps have already been marked. Special settlements that have disappeared from the surface of the Earth are also partially introduced, they were found thanks to the help of local residents and children of the repressed. Also, the main routes of the repressed movement from the Volga region to the Molotov region are created. For maps creation we used the web-service Carto.com. The final point of the project will be the publication of maps, photo and video content in an open portal created on the basis of WordPress, which will partially solve the problem of commemorative practices in modern Russia about traumatic scenes in the history of the 20th century (Povroznik, 2015). Thus, an important step towards public rehabilitation of the repressed the Germans will be done. As mentioned above, self-awareness is still preserved as repressed, not only among those who went through it, but also their children and grandchildren (Assman, 2016). ",
        "article_title": "Remember How: The Place of Visualization in Preserving the Memory of Repressions of the USSR Against the Volga Germans",
        "authors": [
            {
                "given": "Iuliia",
                "family": "Iashchenko",
                "affiliation": [
                    {
                        "original_name": "Perm State University, Russian Federation",
                        "normalized_name": "Perm State University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/029njb796",
                            "GRID": "grid.77611.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-09",
        "keywords": [
            "content analysis",
            "libraries",
            "museums",
            "geography and geohumanities",
            "English",
            "GLAM: galleries",
            "archives",
            "semantic web and linked data",
            "public and oral history",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The aim of analytical bibliography is to understand books and other printed objects as artefacts, and, most importantly, how they were produced (Tanselle, 1977). Systematic methods can unlock patterns otherwise hidden and provide an overall view of the publishing history (Eliot and Rose, 2009). Hence, bibliographic metadata can represent important historical trends (Tolonen et al., 2015; Lahti et al., 2018) as well as resolve long standing issues, such as the ordering of editions and impressions of books (Hawsam, 2014). In this paper we present the state of the art analytical approach for determining editions and their ordering. This enhances the applicability of metadata towards bibliographical analysis, and provides a systematic quantitative perspective on early modern publishing. Furthermore, it will be a great aid for projects aiming to do large data set oriented text mining, by providing harmonized data and information on historical developments in book production. Such analysis will be essential for recognizing how books and other textual artifacts have organically evolved over time, and for delivering a broader context for full text mining and interpretation.  State-of-the-art Contemporary text mining approaches typically ignore edition level information, or provide very generic solutions that omit many details. Related earlier work includes the “Commonplace Cultures” project (Morrissey, 2016), where large-scale text mining of the ECCO dataset was carried out, though with only the earliest edition information present. Other projects include BookSampo, a semantic portal which draws from FRBRoo ontology (Riva et al., 2008). It covers metadata on Finnish fiction literature, however at the work level and does not have complete edition information (Mäkelä et al., 2011). Additionally, commonly used analysis algorithms such as Latent dirichlet allocation are inherently time agnostic (Blei et al., 2003), and although newer approaches such as Topics over Time can include time spans (Wang and McCallum, 2006), they are not applicable to the problem of edition detection, due to their focus on topics. Hence these methods may not be able to contextualize historical developments in book printing and publishing in a chronological fashion. Effectively, these projects are limited in their scope by providing a simplistic and static view into the nature of book production.    Data We have demonstrated these ideas based on The English Short Title Catalogue (ESTC, 2018), which provides a wealth of knowledge with regard to the books, their publication and editions. However, it follows the Machine Readable Cataloging (MARC, 1999) standard, which is unsuitable for research in its raw form. This is a common characteristic of the metadata in general (Nilsson, 2010). To overcome this limitation, we have developed dedicated and semi-automated harmonization techniques that convert free-form textual information into more coherent and consistent entries that are readily amenable for statistical analysis. This required robust handling of differences in title texts, spellings, and more, hence going beyond simple textual comparisons.    The harmonization approach Developed in the popular statistical environment R, our approach begins with cleaning up edition information present in the edition statement field, MARC Field 250, provided by ESTC. Unfortunately, this information is unavailable for the majority of entities and hence the publication date, MARC Field 260, is used to provide a starting point for editions. Next, a new work field is constructed to collect titles into a small collection or work, by using title uniform, which is MARC Field 240, or a cleaned up title of the book.  Considering the variety of titles in the ESTC, the spelling variations, and different styles of writing, we developed an algorithm to handle these issues. It iteratively builds up a title for the collection using the initial work field and collects similar books into the collection. The algorithm performs sub string matching using a variety of methods such as grep, fuzzy or exact matching, on a word by word basis. Alongside manual corrections, the algorithm allows for gaps in the matching and uses a coverage metric to determine if two titles are similar. The benefit of this approach is then realized as similar titles are grouped together despite variations in word spellings, title length and more. At the moment, the algorithm is being worked on to improve its applicability across different genres.   In between automatic and manual The project has been supported by manual checking and corrections. While we are looking at the whole of ESTC, our priorities are focused on published books. Considering the scale of the ESTC dataset, with the number of documents going beyond 460,000 (Tolonen et al., 2015), a smaller sample of the works of 7 authors were selected, keeping in mind the diversity present in ESTC as well as the popularity of these authors. The list of authors consisted of William Shakespeare, David Hume, Jonathan Swift, John Locke, Isaac Watts, Alexander Pope and Daniel Defoe. The dataset sample was then used for development of the cleanup techniques and algorithms. Finally, the harmonized entries were manually checked to determine the corrections needed for the cleanup.  Different techniques may be required for different genres. Such as the case with works concerning poetry or religious sermons, which contrast significantly with the works consisting of popular books. The issue is further complicated by the fact that different spellings have been used for the same words, titles have been written in different manner across different genres, and therefore may have no universal clear pattern.    Validation Validation of the algorithms is essential in order to assess the performance and the ability to generate correct data. In the context of this work, it is imperative that such algorithmic techniques are robust, so as to develop a correct view of the underlying data and obtain reliable conclusions. Therefore, in the interest of reproducibility, a gold standard is developed for the purpose of validating the harmonization process. We sampled a total of 250 authors, each with 5 to 50 publications with unique titles in the ESTC. Then we manually evaluated these samples to determine those titles, which should be grouped together as the same work based on the non-material content of the entries. Each of the records are considered as a distinct impression or edition. This way we did not have to bother ourselves with publication years or page counts signifying reprints: the chronological ordering of the entries computationally is straightforward once the correct grouping is known.   Construction of the gold standard for validation We created several edition layers for the gold standard. The first one was a simple straight-forward layer, with all the works with exactly the same content but with spelling mistakes or obvious occasional word replacements in the title defined as a single work. The second layer combined the first print with new revised editions and those with added content as a singular work. Also we connected the multi-volume works with each volume annotated separately together with the works containing all the volumes in the same entry. On the third layer we regarded the same content with differing time periods as one work. This type of layer was required for including calendars designed and marketed for a specific area as the same work, as well as music performance handouts for different dates.  We also added yet another level signifying that the work is a collection of other works. Seeking out collections allows to research which part of an author's curriculum actually was revered at a given time. Additionally, we made a rudimentary classification of the genre, so that automating exclusion from subset based on the record type would be possible, even if the genre field had not been annotated for the record. For example, formally structured documents, such as proclamations, court case reports, meeting minutes or dictionaries, in subsets designed for word embeddings would skew the outcome.   Conclusion Our overarching aim is to provide a transparent and reproducible ecosystem of bibliographic data and algorithms that can be used to harmonize and analyze various library catalogues. This project complements the overall analysis by investigating the harmonization of the edition field, and by providing the first harmonized version of the data. A quantitative perspective on early modern publishing would be greatly improved by combining the edition level information with publisher data. Accurate description of the publishing network and the various changes it had undergone in the eighteenth century would then become available. Combining the edition level information from ESTC with text mining of large datasets such as ECCO would provide a finer description of what was the first edition of a book as well as the subsequent changes between it and the later editions. This would be supplemented by text reuse approaches, enabling a more detailed account of the evolution of the written text during the early modern period and hence can serve as the foundation for more descriptive analysis.  ",
        "article_title": "Analytical Edition Detection In Bibliographic Metadata",
        "authors": [
            {
                "given": "Ali Zeeshan",
                "family": "Ijaz",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Hege",
                "family": "Roivainen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Leo",
                "family": "Lahti",
                "affiliation": [
                    {
                        "original_name": "University of Turku",
                        "normalized_name": "University of Turku",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/05vghhr25",
                            "GRID": "grid.1374.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The present contribution has a twofold aim: on the one hand it will seek to demonstrate how the use of digital tools and methods enabled the reconstruction of the road network in Crete, Greece during the Roman period (1st century BCE - 4th century CE), while on the other hand it will showcase how the rapid developments in digital tools often deems research in the field of the Humanities outdated or obsolete. Back in 2005, when I first started working on digitally modeling land communication in Roman Crete, the puzzle I was trying to put together was looking for the bits and pieces of relevant and useful information within a variety of diverse and scattered sources: ancient written sources (like, for example, Strabo’s  Geography and  Stadiasmus Maris Magni, an ancient Roman periplus detailing the ports on the shores of the Mediterranean Sea), archaeological evidence (paved road remains, bridges, miliaria, i.e. Roman milestones, and Roman sites), historical maps (like, for example, the  Tabula Peutingeriana, a medieval copy of an itinerarium pictum, i.e. a painted itinerary, showing the layout of the road network of the Roman Empire, and Venetian maps of Crete), travel literature (15th-20th century travelers’ accounts) and topography (largely my own surveys).   Material heterogeneity and diversity of information and data collected called for sophisticated modeling; that is, an information schema that would capture and document any determinant detail, and, when implemented, be able to facilitate correlation and answer my research questions: which settlement types (for example cities, sanctuaries, farmsteads) were connected through the road network, what were the distances between them, how long it took to travel and by what means of transportation (e.g., by foot, horseback, mules, etc.), which route was followed by which traveler and for what purpose, do the routes mentioned by different travelers change across time, how accurate or credible is the information provided by ancient sources with regard to distances between settlements, what parameters affected the course of the route and the planning of the road, to what extend Cretan topography determined the route direction, what were the local topographic characteristics that affected transportation on the island, and can we reconstruct the original trace of the Roman road network with the use of digital tools and methods even though the archaeological evidence is scarce and fragmentary? The other challenge was to combine the use of Geographical Information Systems in order to restore visually the spatial data and exploit the GIS functionality to check and assess parameters that affected the planning of the road network in question.  The digital tools that made this endeavor feasible were two. The first one was ArcGIS (ArcView 9.0), a commercial GIS software still largely in use, for the spatial analysis of geographic data. The selection of the second one, the tool that would enable me to manage, store and correlate all historical and archaeological data, has proved more challenging and changed over the years. From MS Access, for the implementation of a relational database, back in 2005, to BetaCMS, an open source web-based content management platform, which used XML schema definitions to represent content, in 2010. The BetaCMS, later called Astroboa, was developed by a Greek IT company, Beta Concept, and allowed fast and easy modeling, storing and querying of all data, thus providing what it seemed to be, at the time (2010), a suitable alternative to switch into. The alternate and combined use of both systems (GIS and database) allowed for a long and intriguing iterative process from which a network of optimal paths emerged as a result. This enabled me to propose a reconstruction of the original trace of the public road network connecting the major cities and settlements of Roman Crete, and in specific cases test it against field trip data, with very promising results.  Such an initiative as the integration, connection and modeling of complex data on Roman road networks in the digital domain was indeed quite innovative in 2005. Had this venture been undertaken manually, i.e. without employing any digital methods or tools, it would undoubtedly have taken longer and it would most probably not been as accurate. Correlating the data so that they become meaningful and usable for my analysis and making more realistic calculations over geographic space, could not have been performed as efficiently relying on analogue methods alone. However, an analogue approach would still have been up to date and re-usable, unlike my 2005 and 2010 end-results. Sustainability of my Roman roads modeling project has proven to be a great challenge, as BetaCMS is not supported any longer, while ArcGIS is not an open source tool. Therefore, one could argue that, what the digital so generously offered my work, it has taken it back rather fiercely.  In this short presentation I will be going through my methodology and results of the Roman Crete land communication modeling and will be raising questions as to their usefulness, curation, re-usability and sustainability as well as the implications to Humanities research, also looking into the prospect of the employment of current innovative digital methods and tools that are available openly. While there is a considerable number of studies regarding digital preservation strategies and planning on an institutional level (e.g. libraries and archives), my intention is to address those very issues from the individual scholar’s perspective. ",
        "article_title": "And The First One Now Will Later Be Last, For The Times They Are A-changin': Modeling Land Communication In Roman Crete",
        "authors": [
            {
                "given": "Maria",
                "family": "Ilvanidou",
                "affiliation": [
                    {
                        "original_name": "ATHENA R.C., Greece",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "archaeology",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Computer vision necessarily embodies a scientific theory of vision. Since the early 1980s, this has largely been a neuroscientific theory of human vision, chiefly with reference to David Marr’s posthumous Vision (Marr 1982). This metaphorical link has been even more explicit since the dominance of Convolutional Neural Networks as a general technique for image-understanding in the 2010s, with commonplace metaphors (e.g. Kietzmann 2018) between artificial and human neurons and the subsequent hierarchical visual processing structure they entail.  This paper reports the prototyping of a computer vision which is instead founded on early modern theories of optics, vision and visual perception. In doing so, we create not only an experimental apparatus to investigate those theories; but also a computational lens through which to see the image production of the time. In that sense, it is an attempt to construct a computational version of Michael Baxandall’s ‘Period Eye’ (Baxandall, 1988) - a lens through which we can escape our habitual ways of seeing images.  In Shadows and Enlightenment, three decades after he had introduced the notion of Period Eye in Painting and Experience, Baxandall himself had drawn parallels between early modern theories of vision and computer vision; the possibility of a computer vision based on those early modern theories was implied, though no implementation was attempted. Baxandall’s rationale starts from Molyneux’s problem, a 17th century thought-experiment which asks: could a person born blind, and taught only by touch to differentiate between spheres, cubes etc, recognise these objects visually if suddenly given the ability to see?  Baxandall se es computer vision as a contemporary equivalent to Molyneux’s thought-experiments: “If one is going to draw on modern thinking about vision, it must first be clear that the newly sighted man of Molyneux has been displaced from his central part in the thought experiment. The exemplary figure addressing the cubes and spheres now is often an array of electronic sensors, feeding light measurements into one or another conformation of circuitry controlled by such-and-such a program” (Baxandall 1997, p.41).   The period in which Baxandall was writing Shadows and Enlightenment - the mid 1990s - coincided with a particular crisis in artificial intelligence. The failure of symbolic or rules-based intelligence, also known as GOFAI (Good Old-Fashioned Artificial Intelligence), led to an interest in alternative approaches: embodied computation, human-machine interaction, machine learning. Baxandall was not only aware of the two competing paradigms of artificial intelligence research in the mid 1990s - he saw them as embodying differing historical theories of vision. In discussing different algorithms that predict 3D shape from 2D shading, he differentiates between a  “parallel distributed processor (or neural network or connectionist system) with a learning algorithm, not a serial symbolic system with a processing algorithm... There is no symbolic language, no pre-set procedure except the network structure itself” (ibid, p.46-47). Baxandall frames the tension between symbolic AI and machine learning as a continuation of Enlightenment debates around the Molyneux problem; going so far as to suggest that, as a solution, neural networks “would have pleased Condillac more than Locke” (ibid, p.160).   Baxandall then puts his oppositional thought-experiment into practice, in the reading of an early 18th century drawing by Tiepolo. Having discussed the relative mechanics of bottom-up and top-down computer vision, he sees Tiepolo’s drawings as containing  “two models of perception in incomplete relation”. The central part is scale-free, and might as well be a crumpled paper bag; the head, single hand and single foot hint at a “schematic mental mannekin”; and neither of these two readings being quite resolved, there results a “persisting element of flicker between readings” (ibid, p.51).   This project aims to go beyond what Baxandall had hinted at in Shadows; not only to use different computer vision techniques as metaphorical thought-experiments, but to use their technical implementations as experimental apparati. A first attempt at this can be seen in Figure 1 - details from the Tiepolo example are processed by object detection and caption generation, and the peripheral elements (‘person’; ‘close up of a person’) are indeed more immediately schematisable than the scale-free robes (‘kite’; ‘close up of a half eaten sandwich’).         Figure 1. Detected objects and generated captions on Baxandall’s Tiepolo example, using convolutional neural networks (in the vein of Condillac). The detections are ‘Person: 63%’ (in green) and ‘Kite: 62%’ (in brown). Caption generator from (Xu, 2015), code available at   https://github.com/DeepRNN/image_captioning  ; object detector from Huang (2017), code available at   https://github.com/tensorflow/models/tree/master/research/object_detection  . (The author)  Taking our historical perspective beyond Locke and into the sixteenth century, we use Giovanni Paolo Lomazzo’s Temple of Painting (1590) as the scaffold on which to build our Early Modern Computer Vision. An influential text in Italian mannerism, it is both well-ordered, scientifically explicit, and specifically directed towards the visual arts: it intersperses optical theory with practical recommendations for painting. In contrast to Molyneux’s imaginary subject, Lomazzo had practiced as a painter before becoming blind.     Figure 2. Giovanni Paolo Lomazzo’s colour system (the author) Lomazzo’s Temple of Painting (along with his earlier Trattato) contains a scientifically and artistically substantial theory of colour; which, according to Barasch, “make Lomazzo a turning point in the history of color concepts in the theory of art” (Barasch 1979, p.160). The backbone of this theory of colour is a colour scale, ranging between white and black, in a single sequence - as dictated by the Neoplatonic thought of the time (Marsilio Ficino and Gerolamo Cardano have similar scales).  We have no shortage of colour-systems today, but all digital-display systems use three channels (or four, for opacity). The neoplatonic imposition of a one-dimensional scale through three-dimensional colour-space can therefore be visualised as a single vector path from white to black - Figure 3 shows an example in the HSL (Hue-Saturation-Luminance) double-cone.     Figure 3. Lomazzo’s colour system as a path through the Hue-Saturation-Luminance double-cone (the author). Lomazzo specifically urges the painter to avoid stark contrasts in adjacent colours in his sequence: a colour is ‘friendly to one that stands next to it… while it is hostile to a color separated from it by other shades’ (Barasch 1979, p.183); contradicting Leon Battista Alberti’s earlier advice in De Pictura (1435) to set adjacent robes in contrasting colours, to give a greater impression of clarity.  Using the path in Figure 3 not only as a visualisation but as an interpolation, Lomazzo’s harmonic sequence can become a digital colour space in the technical sense: any colour image, including Lomazzo’s own work, can be translated to points in the Lomazzo colour-space (Figure 4, centre). Colour-harmonic relationships inherent to Lomazzo’s scale are shown, would be incompatible with current notions of a ‘colour-wheel’: red and ultramarine, for instance, are almost adjacent.     Figure 4. Giovanni Paolo Lomazzo, Madonna e santi, chiesa di San Marco a Milano. Seen in original colour (left) and interpolated using the 1-D Lomazzo colour system (centre), then rendered back to RGB (right). (The author).  Barasch, M.  (1979). Light and color in the Italian Renaissance theory of art. New York: New York University Press.   Baxandall, M.  (1988). Painting and experience in fifteenth century Italy: a primer in the social history of pictorial style. Oxford: Oxford University Press.  Baxandall, M.  (1997). Shadows and enlightenment. New Haven, Connecticut: Yale University Press.  Huang, J, et al.  (2017). Speed/accuracy trade-offs for modern convolutional object detectors. Proceedings of the IEEE conference on computer vision and pattern recognition. New Jersey: IEEE.   Kietzmann, T. C., McClure, P., & Kriegeskorte, N.  (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504.  Marr, D.  (1982). Vision: A computational investigation into the human representation and processing of visual information. Cambridge, Massachusetts: MIT Press.   Lomazzo, G. P.  (1590). Idea del tempio della pittura. Milan: Paolo Gottardo Pontio.   Xu, K, et al.  (2015). Show, attend and tell: Neural image caption generation with visual attention. Proceedings of the 32nd International Conference on Machine Learning. ICML.   ",
        "article_title": "Early Modern Computer Vision",
        "authors": [
            {
                "given": "Leonardo Laurence",
                "family": "Impett",
                "affiliation": [
                    {
                        "original_name": "Bibliotheca Hertziana Max Planck Institute for Art History, Italy",
                        "normalized_name": "Bibliotheca Hertziana – Max Planck Institute for Art History",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/04g6zen34",
                            "GRID": "grid.461961.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "multimedia",
            "art history and design studies",
            "communication and media studies",
            "English",
            "media archaeology",
            "audio",
            "video",
            "digital art"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Background We provide a survey over the main strategies to harmonize and to integrate TEI/XML documents and Linked Open Data resources. As a highly popular community standard, the Text Encoding Initiative (TEI) provides a the most frequently adopted model for the semantic markup of text data in the Digital Humanities. Likewise, applications of Linked Open Data technologies and resources in Digital Humanities are manifold, and where commonly used LOD and RDF technology is employed, the scientific challenges involved are comparable to those in other areas of application. A scientific problem specific to Digital Humanities is, however, how these technologies can be related to the TEI as the current de facto standard for computational philology.  While benefits of LOD technologies have long been recognized in the DH community, and lead to the formation of a LOD SIG in 2014, there is no agreement on possible technological bridges between TEI/XML and LOD technology. With this paper, we provide an overview over existing solutions and their characteristics, and contribute to the discussion of the further standardization — and possibly, revision — of these possibilities. We focus on in-line XML, because Web Annotation (Sanderson et al., 2017) already provides a convenient and established W3C standard for establishing LOD as a standoff layer over XML documents.   Complementary Intentions: The Text Encoding Initiative and Linguistic Linked Open Data Founded in 1987, the TEI is the authoritative body that develops and maintains an XML-based interchange format for textual data, in particular for the electronic edition of printed (or printable) publications. Beyond its historical focus on literary science and linguistics, the current edition of the TEI guidelines, P5, represents a de facto standard for the entire field of Digital Humanities. The TEI defines an XML format that aims to provide a compromise between a formal description of layout elements (e.g., italics) and their abstract function (e.g., emphasis): Its markup elements are given interpretable names, but the provided definitions are informative only, not normative, as the TEI standardizes only their form, context and structure using standards such as modular DTDs, Schematron or RelaxNG. For practical applications, the TEI thus takes a document-centric, and text-driven approach: the form, content and structure of the underlying text is preserved, and are enriched by markup elements. Markup elements can be validated with respect to syntactic constraints, but not directly with respect to their semantics.  In application to objects of linguistic and philological interest, Linked Open Data and RDF technology have been applied with increasing intensity in the last years – as manifested in the Linguistic Linked Open Data (LLOD) cloud   See  http://linguistic-lod.org/ for the LLOD diagram; for specifications for lexical data, see  https://www.w3.org/2016/05/ontolex/; for annotations, see  https://www.w3.org/TR/annotation-vocab.  , — but with a clear emphasis on semantics and data structures rather than markup: The original sequential structure of a document can only be preserved in RDF if explicit data structures (relations) are being created. In opposition to that, sequential structure is inherent to and implicit in XML.   The focus of XML technologies on structure (rather than semantics) also affect its modes of validation: XML technology is capable of validating structure, and semantic information can only be validated in terms of constraints for their occurrence. In opposition to that, LOD technology, on the other hand, is based on knowledge representation by means of ontology languages (RDFS, OWL, SKOS) and graph technology (RDF, ShEx, SHACL), and allows to perform inferences (and, by extension, validation), resp. pattern validation over semantic data structures — regardless of their sequential and hierarchical organization. In this regard, LOD and XML can be considered complementary approaches for the digital philologies. Not in the sense of contradiction, but in the sense that they specialize in different aspects and that synergies can be expected from their harmonization. Such a harmonization, however, requires the development of interfaces, and the re-interpretation or the modification of TEI vocabulary elements whose introduction preceded the emergence of Linked Open Data.   Three Paths to Go, and How to Choose As it has previously been extended by necessary modules as they were needed, the TEI today provides a very rich vocabulary of markup elements, in the current TEI P5 guidelines (literally, ‘proposal’) covering 569 XML elements and 231 attributes. The analogy-driven extension of semantics and syntax of markup elements (‘tag abuse’) is a common strategy to counter the unrestricted growth of the TEI vocabulary. However, this is not a recommended practice, as it leads to compatibility issues (as the same information can be represented in different ways) and semantic indeterminism (if the same markup is used for two distinct functions, the intended meaning cannot be automatically recovered). In addition to TEI-native strategies to represent RDF and Linked Data, we thus describe an alternative approach whose first implementation was reported in 2018: The generic extension of TEI with W3C-recommended vocabulary elements to represent RDF in XML attributes (RDFa). We will give an overview over these three common strategies to formalize LOD references and data within the TEI, we present their original use cases, inherent limitations for each of them, and the implications of these limitations for future use. For reasons of brevity, this is summarized in in bullet points, only:  Representing LOD references with TEI attributes, e.g.,  @ref in the “Text Database and Dictionary of Classic Mayan” (Diehr et al., 2017)  Emulating RDF triples with TEI elements, e.g.,  <relation> in “Sharing Ancient Wisdoms” (Tupman, 2013)  TEI+RDFa: extending att.global.linking with  @about,  @property, and  @resource, e.g., in the “Diachronic Spanish Sonnet Corpus” (Ruiz Fabo et al., 2018)   We analyze these approaches along two main dimensions: TEI P5 compliance and LOD expressivity and discuss implications and advantages of the respective limitations for prospective use cases, with main results as given in the following table:    Strategy 1 allows to refer to LOD resources, it does not support typed links. Strategy 2 allows to refer to LOD resources with typed links (i.e., to emulate RDF triples), but is not compliant with LOD standards or technologies. Strategy 3 allows to publish LOD data directly as TEI, and to provide an LOD view on TEI data within a single data source. In the following table we present different strategies to encode triples in the TEI:    In our presentation, we discuss the impact of the different modelling choices in terms of their support by existing infrastructures, off-the-shelf databases and APIs.   Acknowledgements We would like to thank the anonymous reviewers for valuable insights and feedback. This abstract is a result of discussions that have been on-going in the TEI community for about a decade already, and to which the first author contributed since 2013. We would like to thank the members of the TEI discussion list (TEI-L@listserv.brown.edu) for their input and feedback to our earlier inquiries about the topic. The research presented in this paper was primarily conducted in the context of the independent research group “Linked Open Dictionaries (LiODi)”, funded 2015-2020 from the eHumanities programme of the German Federal Ministry of Education and Science (BMBF).  ",
        "article_title": "Linking the TEI: Approaches, Limitations, Use Cases",
        "authors": [
            {
                "given": "Christian",
                "family": "Chiarcos",
                "affiliation": [
                    {
                        "original_name": "Goethe University Frankfurt",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            },
            {
                "given": "Max",
                "family": "Ionov",
                "affiliation": [
                    {
                        "original_name": "Goethe University Frankfurt",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "linking and annotation",
            "theory and methodology)",
            "standards and interoperability",
            "English",
            "text encoding and markup languages",
            "semantic web and linked data"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Oral history plays a significant role for historians to understand the experience shared by the people from their past. One of the crucial benefits of oral history interviews, such as survivor interviews, is that it can shed light on important issues that might not have been present in previous historical narratives. Such interviews involve complex social interactions and different factors highly influence the interview situation such as complexity of human lives, age, intelligence, personal quality, etc. (Bornat, 2010). Both the interviewer and the interviewee contribute to these components during this dialogical process. In survivor interviews about holocaust, an interview goal is to better understand the interviewee’s related experiences. While such experiences are often associated with negative emotions such as fear, anger, or sadness and the interviewee is not willing to recall that memory and talk about it, the interviewer often needs to get to that and have the interviewee engage in this reflection. Tension can thus occur, as a result of conflict of interest or uneasiness. Researchers study these moments to gain an understanding of the conversational dynamics in these interview processes. For example, tension can be shown as reticence in the interview. Layman (2009) studied how reticence can cause conversational shifts by interviewees therefore putting a constraint on the responses the interviewees will offer to the interviewer. Layman (2009) discussed the importance of being aware of these situations so that the interviewer can better judge whether to press the interviewee.  In this work, we aim to identify the tensions in transcribed and translated interview transcripts of Rwandan genocide survivors. To approach this, we explored a list of potential indicators of tension including the use of hedging and boosting in the language, the sign of reticence, and emotion of the interviewees from their interview responses. Hedging refers to the technique of adding fuzziness to the propositional content by a speaker. People are known to use hedging to deal with controversial issues in conversations (Ponterotto, 2018). For example, the use of “I think…”, “Well, …” in interviews give interviewees the authority to shape their narratives. Phrases such as “In other words”, “In my understanding” can also be used to shift a topic either completely or partially. It can be used as a filler or delaying tactic. Boosting, using terms such as “absolutely”, “clearly” and “obviously”, is a communicative strategy for expressing firm commitment to statements. It restricts the negotiating space available to the hearer. It plays a vital role in creating conversational solidarity (Holmes, 1984) and in constructing an authoritative persona in interviews (He, 1993). Interestingly, if booster words are preceded by negated words such as (not, without), it can act as hedging (e.g., not sure). We identified lists of boosting and hedging words and phrases in this study to help us identify the tensions. We also developed a hedging detection tool to automatically identify the occurrences of hedging in the interview transcripts and achieved accuracy of 85.4% and F1-score of 81.9% for hedging sentences.  Tension can be defined as an emotion of physical and psychological strain joined by discomfort, unease, and pressure to look for alleviation via talking or acting (Nugent, 2013). To analyze whether and how the interviewee’s emotional aspect indicates the tension during the interview, we developed an emotion recognition tool to recognize the interviewee’s emotions from the interview transcript. Emotion recognition in computational linguistics is the process of identifying discrete emotion expressed by humans in text. Emotions lead directly to the past and bring the past somatically and vividly into the present (Misztal, 2003). Leveraging the high performance of deep learning compared to other machine learning approaches (Kim, 2014; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), we used a multi-channel convolutional neural network (CNN) model to recognize the emotions from the transcript. We achieved relatively high F1-scores in the emotion categories, compared to previous studies. For example, we achieved F1-scores of 72.6%, 73.6%, 76.8% and 46.0% for happiness, sadness, anger and fear, respectively.  We also considered prosodic cues such as laugh and silence as signs of reticence, but also acknowledge that further exploration is needed to interpret these cues. For example, while “laughter” may indicate invitations for the next question sometime, it may also represent hesitation or nervous deflection, i.e., the tension. We explored the traces of tension in the situations when an interviewee gives unusually long or short answers for a specific question type, shorter or longer than three standard deviation of the average length of responses of that question type (e.g., wh questions, yes/no questions, etc.).  Our algorithm of identifying the tensions in the interview transcripts combines all these aforementioned components. To evaluate the performance of this algorithm, oral history researchers first annotated tension points in the interviews, which provides us the “ground truth”. Then, we applied our algorithm to identify tensions from these interviews. With the two human labelled interviews available to us, the algorithm is able to identify six out of seven tension places in one interview, and three out of four in the other. The recall of the algorithm is thus satisfying. However, the algorithm suffers from low precision and we have 76 false positives in one interview and 56 in the other. On the other hand, given that the two interviews are very long (over 17,000 words for each) but there are only a few tension places identified by the researchers, this algorithm may work as a filtering tool in the tension annotation process. First, the algorithm identifies possible tension places, and then the researchers review them to identify the actual tension occurrences.  In conclusion, this study is interested in developing computational techniques to analyze where the interviewee’s tensions can be detected in the interview data in the context of survivor interviews about Rwanda Holocaust. This contributes to a better understanding of where tensions occur and how in the interviews. Such understanding helps researchers in the exploration of dialogical space created by the interviewer and interviewee in these conversational interviews. ",
        "article_title": "Tension Analysis in Survivor Interviews: A Computational Approach",
        "authors": [
            {
                "given": "Jumayel",
                "family": "Islam",
                "affiliation": [
                    {
                        "original_name": "The University of Western Ontario, Canada",
                        "normalized_name": "Western University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02grkyz14",
                            "GRID": "grid.39381.30"
                        }
                    }
                ]
            },
            {
                "given": "Lu",
                "family": "Xiao",
                "affiliation": [
                    {
                        "original_name": "Syracuse University, USA",
                        "normalized_name": "Syracuse University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/025r5qe02",
                            "GRID": "grid.264484.8"
                        }
                    }
                ]
            },
            {
                "given": "Robert E.",
                "family": "Mercer",
                "affiliation": [
                    {
                        "original_name": "The University of Western Ontario, Canada",
                        "normalized_name": "Western University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02grkyz14",
                            "GRID": "grid.39381.30"
                        }
                    }
                ]
            },
            {
                "given": "Steven",
                "family": "High",
                "affiliation": [
                    {
                        "original_name": "Concordia University, Canada",
                        "normalized_name": "Concordia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01qxhf360",
                            "GRID": "grid.448967.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-18",
        "keywords": [
            "corpus and text analysis",
            "artificial intelligence and machine learning",
            "natural language processing",
            "cognitive sciences and psychology",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Author attribution is the task of identifying the writer of a text of unknown/disputed authorship. Automated attribution is based on selecting a set of stylistic features (e.g. function words, n-grams, etc.), which capture the intuitive notion of an author’s style. The frequencies of use of these features in known works are used to train machine learning classifiers, which can be applied to recognizing the style of a document of unknown or disputed authorship. For an overview of the field, the reader is referred to (Stamatatos 2016, Stamatatos 2009, Juola 2008). Recently, there has been interest in stylistic features based on prosody. Lexical stress for attribution was studied in (Ivanov et al 2018, Ivanov and Petrovic 2015, Dumalus and Fernandez 2011). In (Ivanov 2016), the role of alliteration for attribution was investigated. In this paper, we compare the usefulness for attribution of two other prosodic features – assonance and consonance. We present results from several machine learning experiments, based on extracted assonance and consonance patterns from a historical corpus of 18th Century works, the popular Reuters corpus, and two small poetry corpora.   Assonance and consonance  Definitions Assonance is defined as the use of a repeated vowel or diphthong sound in nearby words. Consonance is the use of a repeated (combination of) consonant sound(s) in nearby words. Examples of assonance and consonance abound in literature:  Blake's \"Tyger, Tyger burning bright in the forest of the night\" illustrates the use of the assonant \"ai\" sound. The sentence \"I have a craving for scrambled eggs and marble rye toast\" illustrates the use of multiple consonant blocks (\"kr\", \"mbl\", \"s\", \"r\", etc.). There has been little work on assonance and consonance except for a paper ( Addanki and Wu 2013) on rhyme identification in hip hop. A few earlier works (Genzel et al 2010, Byrd and Chodorow 1985) also consider rhyme identification. To the best of our knowledge, neither assonance nor consonance have been used for authorship attribution prior to this study.     Extracting assonance and consonance We developed algorithms for extracting assonance and consonance sequences from text. Both algorithms use a modified version of the Carnegie Mellon University (CMU) pronunciation dictionary, augmented with word-pronunciation pairs from our historical corpus.  The assonance algorithm takes as input a text and several user-specified parameters: The maximum inter-vowel distance (max-IVD) parameter defines how far apart assonant vowels/diphthongs can be. The second parameter specifies the scope for the assonance search: within sentences or within paragraphs. The third parameter indicates whether only primary stressed vowels or any vowels should be considered. The fourth parameter indicates whether only the longest or the two longest assonance patterns per block should be used. For each text block (sentence or paragraph), the algorithm determines the (two) longest assonance sequence(s), and labels them with the vowel/diphthong they represent plus a \"short\", \"medium\", \"long\", or \"very-long\" tag, e.g. \"AE_vl\". The label(s) are entered into a map, which tracks the number of times different sequences appear in the text.  The consonance algorithm accepts as input a text and two parameters: the maximum inter-consonance-distance (max-ICD) and the maximum number of consonant-sound/frequency patterns to be output. The algorithm considers all possible combinations of nearby consonant sounds and complexes. For example, in the phrase \"extremely strong\", the algorithm keeps track of the \"str\" complex, the \"st\" and \"tr\" sub-complexes, and the individual 's', 't', and 'r' consonant sounds. Both deliberate and accidental use of consonance is taken into account. Using the modified CMU dictionary, the algorithm converts a text into a string of syllables, and processes them using a table, which keeps track of the consonant sounds, their count, and their ICDs. The consonant sound patterns and their frequencies are output at the end. When all corpus files have been processed, separate programs create training/testing vectors, and write them to ARFF files for the WEKA data mining software (Hall et al, 2009).   Experiments with the 18 th Century Historical Corpus  Our historical corpus consists of 224 documents authored by 38 American and British 18 th Century political figures. The number of documents per author varies between 2 and 21, and the size of the documents varies between 959 and 19101 words.   Baseline experiments The baseline experiments were conducted using JGAAP (Juola, 2009). We used all 38 and subsets of 15, 10, and 7 authors. Our set of stylistic features is described in Table 1. The classification was performed with WEKA support vector machines with sequential minimal optimization (SMO) and multilayer perceptrons (MLP).  Table 1: Baseline accuracies (historical corpus)       Assonance and consonance experiments All assonance experiments and most consonance experiments used leave-one-out (L1O) validation. Only the 38-author consonance experiments used 10-fold validation. For assonance, we experimented with all combinations of the following parameters: a max-IVD of 5, 10, and 15, sentence and paragraph search boundaries, the longest- and the two-longest assonance sequences per block, and the all-vowel (stressed and non-stressed) option. For consonance, we used max-ICD of 5, 10, 15, and 25, outputting the top 100 consonant-sound/frequency pairs. In all experiments, we used MLP and SMO classifiers. The maximum accuracies achieved are presented in Table 2:  Table 2: Consonance vs assonance accuracies (historical corpus)     In the assonance experiments, for larger author sets, the use of the two longest assonance-sequences-per-block produced the strongest results. The results in the 7-author experiments were obtained using the longest-sequence-per-block only. With two exceptions, the maximum accuracy was achieved using a max-IVD of 15. The sentence boundary consistently produced higher accuracy compared to the paragraph boundary. In terms of classification methodology, SMOs outperformed MLPs in two-thirds of the experiments.  For consonance , MLPs routinely outperformed SMOs, yielding stronger results in all but one experiments. The optimal max-ICD was 10 in all consonance experiments. Larger max-ICD values led to rapidly deteriorating results.     Experiments with the Reuters corpus The Reuters Corpus v.1 (RCV1) (Lewis et al, 2004, NIST) is an expansion of the popular Reuters-21578 corpus for text categorization. We selected a random 20-author subset of RCV1 for our second set of experiments. Each author had 50 texts with an average length of about 700 words.  Baseline experiments The baseline results are listed in Table 3:  Table 3: Baseline accuracies (Reuters RCV1 corpus)       Assonance and consonance experiments We repeated all experiments using the RCV1 corpus. The assonance and consonance parameters were varied as described in section 2.3.2. The maximum accuracies obtained are presented in Table 4. Both assonance and consonance performed at least as well as the baseline averages, outperforming most traditional features when the number of authors was relatively small. In all except one case, the maximum consonance accuracy was obtained with a max-ICD of 10. Assonance results were somewhat surprising: As in the case of the historical corpus, the highest accuracy was obtained using a sentence boundary, However, the max-IVD parameter did not affect the accuracy - identical results were obtained using an IVD of 5, 10, and 15. We suspect that this is due to the equal per-author distribution of documents and their sizes in the Reuters corpus. Interestingly, Random Forest (RF) classifiers outperformed both SMOs and MLPs in all experiments.   Table 4: Consonance vs assonance accuracies (RCV1 corpus)        Experiments with poetry We performed a third set of experiments with two small poetry corpora - 18 th Century American poetry (8 authors, 4-6 texts/author), and a mixed 19 th-20 th Century poetry (7 authors, 8-19 texts/author). All parameters of the assonance and consonance algorithms were as in the previous experiments. Table 5 present the assonance/consonance maximum accuracies. The baseline accuracies were 39.48% for both corpora.   Table 5: Consonance vs assonance accuracies (poetry corpora)     Once again, consonance and assonance performed at or above the baseline average. If fact, assonance may have a slight edge in performance in poetry experiments. However, we refrain from drawing conclusions since the corpora are too small to have any statistical validity. We are currently constructing a larger poetry corpus using the resources of (Project Guttenberg 2018).    Conclusion and future work We presented an experimental study of the effectiveness of the assonance and consonance as stylistic features for authorship attribution. Both features exhibit a good performance for smaller author sets. Consonance appears to work better for attributing historical texts, while assonance may perform better on poetry. The performance of both features is affected by the predisposition of authors to using prosody: Certain author styles (e.g. Paine, Wollstonecraft) are readily recognizable through the use of consonance and assonance, while other authors yield weak results. The most promising use of assonance and consonance is in ensemble classifiers, which use traditional features to carry out the initial classification, and prosodic features to fine-tune the attribution hypothesis.  ",
        "article_title": "Comparing Assonance and Consonance for Authorship Attribution",
        "authors": [
            {
                "given": "Lubomir",
                "family": "Ivanov",
                "affiliation": [
                    {
                        "original_name": "Iona College, United States of America",
                        "normalized_name": "Iona College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00m79dm66",
                            "GRID": "grid.419406.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-06",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "authorship attribution / authority",
            "theory and methodology)",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The ADHO Special Interest Group for Digital Humanities Pedagogy and Training offers a special mini-conference that builds upon the DH2019 conference theme, “Complexities,” we are eager to foster a discussion about the many ways in which DH is taught and learned in academic contexts across languages, borders, cultures, and academic structures. This half-day conference will consist of presentation and breakout sessions. The presentation format will focus on lightning talks of 5-8 minutes in length. Proposals are welcome on any topic informing or treating Digital Humanities Pedagogy and Training, including (but not limited to) those focusing on DH training in an international context, i.e. how we articulate/coordinate/collaborate across international boundaries, and what we can learn from our differences and what do we share; how does teaching DH in a global context reveal the complexities of intercultural communication and pedagogy; developing a multilingual lexicon for teaching DH. During the mini-conference we will also break into groups to identify opportunities and possibilities for the SIG to engage in during the coming year. A Call for Proposals will be issued separately. Conveners of the Digital Humanities Pedagogy and Training Special Interest Group: Ray Siemens (University of Victoria), Diane Jakacki (Bucknell University), Katie Faull (Bucknell University), Brian Croxall (Brigham Young University), Walter Scholger (University of Graz) ",
        "article_title": "ADHO SIG for Digital Humanities Pedagogy and Training Mini-conference and Meeting: \"The Complexities of Teaching DH in Global Contexts\"",
        "authors": [
            {
                "given": "Diane Katherine",
                "family": "Jakacki",
                "affiliation": [
                    {
                        "original_name": "Bucknell University, United States of America",
                        "normalized_name": "Bucknell University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00fc1qt65",
                            "GRID": "grid.253363.2"
                        }
                    }
                ]
            },
            {
                "given": "Ray",
                "family": "Siemens",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Brian",
                "family": "Croxall",
                "affiliation": [
                    {
                        "original_name": "Bucknell University, United States of America",
                        "normalized_name": "Bucknell University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00fc1qt65",
                            "GRID": "grid.253363.2"
                        }
                    }
                ]
            },
            {
                "given": "Katherine",
                "family": "Faull",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Walter",
                "family": "Scholger",
                "affiliation": [
                    {
                        "original_name": "University of Graz, Austria",
                        "normalized_name": "University of Graz",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/01faaaf77",
                            "GRID": "grid.5110.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "pedagogy",
            "globalization & digital divides",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   In recent years, the term  tool criticism found its way into the Digital Humanities. Blog posts, journal articles   E.g., Underwood: New methods need a new kind of conversation. 28 February 2018. URL: https://tedunderwood.com/2018/02/28/raising-the-standards-for-computation-in-the-humanities/; Van Es (et al.): Tool criticism: From digital methods to digital methodology. Datafied Soiety Working Paper Series. 28 May 2018. URL: https://datafiedsociety.wp.hum.uu.nl/tool-criticism/.   and workshops   E.g., Tool Criticism in the Digital Humanities co-organized by the Centrum Wiskunde & Informatica, the eHumanities group of KNAW and the Amsterdam Data Science Center (2015); DH Benelux workshop on Digital Tool Criticism (2017).   discuss the necessity of the deliberated exposure to computational methods. The aim is to understand the potential and limitations as well as the scope of application of the tools, leading to a sharpened awareness of the methodology. Sentiment analysis is one of the most popular methods among humanists expanding their research into the digital field. The technique can be easily implemented with tools developed for scholars without programming skills. While such tools are alluring in their applicability, their performance must be interpreted with caution which, in return, can contribute to developing new standards within the field and beyond. This research provides a case study which illustrates the complementation of manual and automated analyses when the possibilities and limitations of both approaches are considered.     Research objectives  To establish tool criticism within DH, the project serves as a basis for discussing certain computational tools frequently used for literary research. In more detail, experiments with different sentiment analysis tools are conducted on a self-designed corpus of dystopian novels; the outputs, then, complement the manual investigations of the texts and lead to further experiments.    Data  The corpus is composed of 102 dystopian novels dating from 1836 to 1979 in the languages American English, British English and German   These languages were chosen because the dystopian genre emerged simultaneously in America and England and inspired German authors decades later (Zeißler 2008, Kumar 1987).   (Table 1). A comprehensive body of secondary literature constitutes the background for deciding which novels were incorporated into the corpus. The categorisation of the works can be unclear, though, because there is no consensus about the texts containing enough distinctive features. Reading into the novels, therefore, was another necessary step in designing the corpus. The most prominent concepts of dystopian novels are a totalitarian regime, an oppressed society, the protagonist(s) rebelling against the authorities and surveillance (Mohr 2005, Baccolini, Moylan 2003, Zeißler 2008, Kumar 1987). The genre was chosen because it carries both distinctive features, e.g. a totalitarian society, as well as debatable characteristics, e.g., the exploitation of extraterrestrial life. This arouses certain assumptions which point the analysis in a specific direction, while also leaving room for findings that have not been prioritised by literary researchers yet.      Language   Novels  Tokens  Types   Token-type ratio    American English  39  3,167,702   136,954  23.1    British English  35  2,660,983   112,012  23.8     German  28  1,872,969   98,479  19.1    TOTAL  102  7,701,654    331,391 4   23.2     Table 1: Overview of the research data.     Methodology   Manual analysis  Studying dystopian fiction qualitatively includes the thorough investigation of both secondary literature as well as other extensive sources like the frequently updated  The Encyclopaedia of Science Fiction (Clute et al. 2018). The concepts defined within all secondary sources are the background for interpreting the output obtained through the quantitative analyses.     Sentiment analysis  Dystopian works are characterised as pessimistic narratives, thus, we hypothesise that sentiment analyses will provide empirical evidence for dystopian novels being a ‘negative’ genre. The Stanford Sentiment Annotator (Socher et al. 2013) and the Berlin Affective Word List Reloaded (Võ et al. 2009) were used to investigate the research data.     Results   Stanford Sentiment Annotator  Analyses undertaken using the Stanford Sentiment Annotator show the ratio between five classifications of V ery positive,  Positive,  Neutral,  Negative and  Very negative sentences in the English part of the corpus due to the tool only correctly identifying English texts. The method works sentence-based: A deep learning model computes the sentiment based on how words compose the meaning of longer phrases which delivers an analysis with an accuracy of 80.7% (Socher et al. 2013).   Figure 1 and 2 represent the sentiment analyses for the novels written in American (Fig. 1) and British English (Fig. 2). The outputs are similar: Slightly more than half of all sentences are defined as  Negative, which proves true the classification of dystopian novels as primarily pessimistic. Besides, less than 20% of all sentences are classified as  Positive and close to 30% as  Neutral, while the percentages for  Very positive and  Very negative sentences are comparatively low. It is noticeable, though, that the British texts have a tendency of being slightly more positive than the American ones. The rare occurrence of ‘extreme’ emotions can be explained due to not every utterance carrying a strong sentiment as well as the authors’ aims of primarily telling a story and not inculcating the readers with strong statements.    Berlin Affective Word List – Reloaded  The Berlin Affective World List Reloaded (BAWL-R) is a dictionary of more than 2,900 German words. These words were chosen by Võ and her colleagues based on their representation potential for negative, neutral and positive affective valences. The dataset was then annotated by 200 psychology students. Since the BAWL-R is a word list and not a tool per se, we wrote an algorithm to analyse the German part of the corpus. It scans the texts and searches for the terms the BAWL-R consists of. Then, it analyses the terms in the different categories based on the annotation in the list.  For the current research, the values for emotionality, arousal and imageability are of interest. Emotionality is rated between -3 ( very negative) and 3 ( very positive), arousal ranges between 1 ( low arousal) and 5 ( high arousal) and the imageability is measured on a scale from 1 ( low imageability) to 7 ( high imageability) (Võ et al. 2009). With an emotionality mean of 0.50, the novels are categorised as rather positive texts, but the dictionary’s emotionality mean is still higher. The arousal value of the novels is higher than in the dictionary, which hints at dystopian texts issuing themes the reader feels personally connected with and touched by. It could be assumed that a relatively high arousal value is attended by a rather high imageability value, too. In the case of our German dystopian novels, this assumption holds true partially: The mean value is slightly below the dictionary’s imageability mean, but still relatively high (4.3). This can be interpreted in the direction of German dystopian novels being written in a relatively vivid manner, so that the reader can imagine the contents well.     Outlook  Based on the findings presented above, an experiment with test persons will be conducted. This experiment is further grounded on the hypothesis that sentiments depend on a person’s cultural and social background (Scollon 2011, Chodorow 1999). Moreover, the emotionality a text can potentially arouse is never isolated, but it is tightly connected to a recipient’s personality and emotional state (Kagan 2007, Sergerie & Armony 2006). With the example of Aldous Huxley’s dystopian novel  Brave New World (1932) we can illustrate these hypotheses: A person who values personal freedom is more likely to interpret the novel negatively than a person who enjoys living in a well-structured society that cares for its citizens while simultaneously cutting off their individuality. To prove the aforementioned assumptions, the experiment will be designed as follows: Parts of the novel  Brave New World will be annotated by test persons. Through a crowd sourcing platform, people will be reached globally. Like that, it is possible to work with a diverse group of annotators, representing different parts of different societies. The task will be twofold: The probands are asked to assign their sentiments, ranging between  Very negative,  Negative,  Neutral,  Positive and  Very positive, to each sentence. The sentence-based method enables the reader to take textual context into account. Besides, they are asked to give some demographic information, which covers the aspect of the non-textual context. This information will help us to interpret the correlation between a proband’s background and his or her annotation.   ",
        "article_title": "Towards Tool Criticism: Complementing Manual with Computational Literary Analyses",
        "authors": [
            {
                "given": "Melina Leonie",
                "family": "Jander",
                "affiliation": [
                    {
                        "original_name": "University of Göttingen, Germany",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "digital textualities and hypertext",
            "multilingual / multicultural approaches",
            "data mining / text mining",
            "standards and interoperability",
            "English",
            "theory and methodology)",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  After two decades of steady increases in image resolution through technical advances in image sensors, we are now also witnessing a significant growth in comprehensively tagged image collections. Concurrently cultural institutions have been digitizing their collections, while cultural scholars have been investing considerable efforts into the annotation of images to denote iconographic details and historical context. Despite these developments, existing interfaces to access image collections do not harness the possibilities provided by rich visual details of high-resolution images and detailed tags associated with them.  A particularly promising development, however, is the growing research interest in visualization to support the analysis and exploration of cultural heritage data [Windhager et al., 2018]. In this context, art historians are experimenting with digital methods, in particular visualization [Bailey and Pregill, 2014], to explore their potential for expanding the scale and scope of art history [Drucker, 2013; Manovich, 2015]. In these experiments, digital methods tend to be equated with a distanced perspective on the phenomenon [Moretti, 2013] with the result that many visualizations provide high-level overviews that diminish the intricate and intriguing details of individual artifacts [Hochman and Manovich, 2013; Hristova, 2016]. With this research we present an approach towards visualization that is challenging the understanding of overview and detail as something inherently opposed. We introduce a technique that clusters iconographic details of images in order to reveal visual patterns prevalent in a collection.   An Overview of Close-ups We worked with a collection of glass plate negatives that were created around 1900 in an attempt to document the inventory of the Museum für Kunst und Gewerbe Hamburg (MKG) for publications and for internal use [Kreiseler, 2018]. Since their creation the glass plate negatives have been reinterpreted from internal material to collection objects themselves. Subsequently, they are being digitized and tagged with iconographic terms. Based on these resources we introduce a visualization technique that arranges close-ups into frequency-based collages. The aim is to a) highlight specific details of the collection objects and create an awareness for their iconography, b) represent the thematic and aesthetic patterns across the collection and c) support open-ended exploration at varying granularities. The visual interface is composed of three views, each illuminating different characteristics of the collection, and encouraging a different mode of accessing and appreciating the artifacts.      Figure 1: Close-up Cloud   Close-up Cloud  The initial stage, the Close-up Cloud (see Fig. 1), provides a high-level overview of the entire collection. It displays a cloud-like collage of close-ups, each representing one occurring tag. Akin to word clouds which vary in font size, the area sizes of the close-up images represent the respective tag’s relative frequency within the entire collection, i.e., a larger area indicates more occurrences of that tag. Over time the close-ups’ content changes randomly, constantly creating a different assemblage of depictions. The visual arrangement of close-ups constitutes an abstract-concrete visualization of the collection, which attempts to (re)present both the texture and the structure of the collection items. The positioning of images is based on semantic similarity. While the initial view does not include any labels, one can reveal the respective tags by hovering over an image. The close-ups of the motifs function as navigation elements, allowing exploration along visual details.      Figure 2: Tag view    Tag  After selecting a close-up, the Tag view (see Fig. 2) unfolds in a conventional image grid system, displaying all close-ups from the same tag. One can now confidently navigate through the results and select areas of interest.      Figure 3: Object view    Object  In the Object view (see Fig. 3) the originating image in its entirety is visible for the first time. It is gradually zoomable and pannable. These interactions can expand the image to fill the complete size of the canvas, and in this way maximize the level of visible detail (see Fig. 4). A column of tags lists all identified close-ups in the selected object. Hovering over a tag element highlights the corresponding regions in the image by setting non-corresponding regions to be semi transparent. A number next to the tag indicates the overall frequency of the tag across the collection. It links to the respective Tag view, enabling further exploration of tangential parts of the collection.      Figure 4: Object view, zoomed in     Technical implementation  In order to illustrate the core concepts of our technique we have devised a rudimentary web-based prototype using the JavaScript libraries Pixi.js and D3.js. We prepared an exemplary set of 43 data objects. The close-ups of iconographic details were manually identified, cropped and embedded as images in a folder structure. Metadata and coordinates of the close-ups are stored in JSON files that link visual material and tag data.    Conclusion  We presented a visualization technique for the exploration of digitized cultural collections that proposes a novel approach towards the overview. The centerpiece of our approach is a Close-up Cloud that presents a high-level overview of the collection’s iconography, while at the same time facilitating close viewing of details. Thus, it enables visual exploration along iconographic patterns across the whole collection (Close-up Cloud view and Tag view) or the details of individual objects (Object view). All stages of the interface are interlinked for open-ended exploration at multiple granularities. We implemented the concept as a web-based interface and evaluated the potential of the approach with collection experts and people interested in photography. The feedback that we received suggests that the technique allows both a serendipitous exploration of the collection that requires no prior knowledge, as well as the scholarly examination of iconographic patterns. While the Close-up Cloud is still at an early prototypical stage, the unique integration of symbolic patterns and figurative details may have the potential to bridge distant and close viewing of image collections. For future work, the development of a semi-automatic annotation technique through the integration of computer vision could be a promising approach to visualizing significantly larger collections of various kinds.   Acknowledgements We thank the Museum für Kunst und Gewerbe Hamburg (MKG) for the collaboration and the Brandenburg Centre for Media Studies (ZeM) for funding this research.  ",
        "article_title": " Close-Up Cloud: Gaining A Sense Of Overview From Many Details  ",
        "authors": [
            {
                "given": "Pauline",
                "family": "Junginger",
                "affiliation": [
                    {
                        "original_name": "University of Potsdam, Germany",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            },
            {
                "given": "Dennis",
                "family": "Ostendorf",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Barbara",
                "family": "Avila Vissirini",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Anastasia",
                "family": "Voloshina",
                "affiliation": [
                    {
                        "original_name": "Academy of Art Berlin Weissensee, Germany",
                        "normalized_name": "Kunsthochschule Berlin-Weißensee",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00zbhen47",
                            "GRID": "grid.466261.1"
                        }
                    }
                ]
            },
            {
                "given": "Sarah",
                "family": "Kreiseler",
                "affiliation": [
                    {
                        "original_name": "Leuphana University of Lüneburg, Germany",
                        "normalized_name": "Leuphana University of Lüneburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02w2y2t16",
                            "GRID": "grid.10211.33"
                        }
                    }
                ]
            },
            {
                "given": "Marian",
                "family": "Dörk",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "user experience design",
            "libraries",
            "interface",
            "gamification",
            "museums",
            "theory and methodology)",
            "English",
            "cultural analytics",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "    In digital humanities applications, tag clouds are often used as a means of distant reading (e.g., Beaven 2011, Koch et al. 2014, Hinrichs et al. 2015, Montague et al. 2015, John et al. 2016). By dissolving the structure of texts, thus, splitting it into words, the frequencies of different words, in the following denoted as  tags,   can be determined. Typically, tag clouds take the most frequent N tags of a text corpus, and by mapping frequency to font size and arranging the tags in a random manner on the screen, the observer gets a quick and intuitive summary of the textual content of the corpus. At least since Wordle (Viégas et al. 2009) has been offered to the public to generate tag clouds on demand, they enjoy great popularity and are widespreadly used.   Nevertheless, there are crucial theoretical problems in the design of tag clouds (Viégas et al. 2008) that question their benefit for text analysis tasks. Finding a single word without assistance is hard, long words receive more visual attraction than short ones as they cover more space, and font sizes, thus the frequencies of words, are difficult to compare. Furthermore, a tag cloud usually does not display all words in a corpus, thus, neglecting less frequent words can lead to misinterpretations. In this paper, we evaluate the value of several tag cloud visualization techniques that have been designed to support research tasks in various digital humanities scenarios. Continuing prior works of the co-authors (Jänicke et al. 2014, Jänicke and Geßner 2015), we base our analysis on the Bible known as the most often read and researched books, thus, well-suited for evaluation purposes. We chose the King James Bible being the most influential English translation.    TagVenn Diagrams  TagPies have been designed to compare the contexts of different words (Jänicke et al. 2018). For a set of M different terms, TagPies generate M+1 different tag sets for shared (1 set) and non-shared vocabulary (M sets). TagVenn diagrams are an extension of TagPies aiming to compare co-occurrences more precisely as all combinations of shared vocabulary are considered. Taking three terms a, b and c and their respective sets of co-occurring tags A, B and C as an example, TagVenn diagrams visualize the following tag sets: A\\(B∪C), B\\(A∪C), C\\(A∪B), (A∩B)\\C, (A∩C)\\B, (B∩C)\\A and A∩B∩C. The tags are arranged in a Venn diagram style with a set of colors reflecting the cut sections. As the human ability to distinguish colors is limited, a maximum of four texts (generating 15 sets) can be analyzed with Tag Venn diagrams. A similarity in topics and writing style can be seen looking at the King James Bible: It is known that the books of the four Evangelists have a lot in common with John differing the most from the others. This can clearly be seen by comparing John with Marcus and Matthew with the minimum number for occurrences set to four (Figure 1). John (JOH) and Marcus (MAR) have significantly less words in common than Mark and Matthew (MAT). It is interesting to see here that Matthew has the biggest number of words only used in this book instead of John as might be expected. Especially worthy for further investigation in the close reading were words like “truth” (27x) and “true” (13x) are frequently used by John, and less than 4x used by the other two Evangelists. In the current version the scholar has to set a minimum number of occurrences, which may not always give accurate results. Then, the diagram might show a word as only occurring in one book of the Bible although other books may also include that word with a too small amount of occurrences. Setting a high number of occurrences will not always be interesting for a researcher since very frequently used words are not necessarily relevant for determining the content of a text. A workaround to avoid unwanted results could be extending the number of stopwords on demand. Also, adding the option to set a “maximum number of occurrences” would give more opportunities to research different questions. Maybe even considering classes of frequency could bring interesting results and open up an interesting field of research.    TagVenn Diagram showing the most frequent words in Mark, Matthew and Johannes     MultiCloud  MultiCloud has been developed as a flexible approach to show different documents in a merged word cloud visualization (John et al. 2018). It depicts each document with a colored circle around the word cloud shape (Figure 2) and it applies a force-based layout in combination with a collision detection algorithm to use the available space as optimally as possible. This way, the positions of the tags represent the occurrences in the different documents. Additionally to the position in the layout, the font size represents the overall occurrences in the documents and color saturation indicates how relevant each tag is in the respective text documents. Furthermore, MultiCloud offers the possibility to control the number of displayed tags: Analysts can set both the number of displayed tags that occur in more than one document and the number of displayed words that occur only in an individual document. Both options can be combined and trigger an immediate update of the visualization as depicted in Figure 2. Since MultiCloud uses the largest qualitative color scheme of ColorBrewer (Harrower & Brewer 2003), which comprises twelve distinct colors, it supports the analysis of twelve different documents.   By scanning the first tag cloud (Figure 2A), users get a quick and intuitive overview of the textual content that all the chosen books have in common, like “lord” and “jesus” as well as differences in topics. Looking at words only appearing in one of the chosen books shows that the word “esaus” (Figure 2B, orange) might be a spelling mistake in book Genesis or a problem of normalization, if the apostrophe is filtered out, for the Biblical name “Esau”, which is used more often in different books of the Bible. Also interesting is the name “abram”, which is only used in one of the chosen books (Genesis, orange), for this is the rarely used former name of the very frequently mentioned “abraham”. Determining whether a word is relevant for a specific research question depends strongly on the books chosen to compare. The third tag cloud visualization (Figure 2C) offers an interesting overview of tags that describe the seven subparts and the individual documents. Document-related tags, for example, like the city of “babylon” only being mentioned in book  Jeremiah (green ) as well as tags that characterize all loaded parts such as “glory” or “prophet” can be easily identified.     Three different word cloud variations of seven subparts of the King James Bible using: only tags that occur in multiple documents, tags that only appear in the individual documents, and a combination of both.       TagSpheres   TagSpheres have been designed within a digital humanities project to support the analysis and classification of a term's co-occurrences according to their clause functions (Jänicke et al. 2017). The analyzed term T is placed in the center, and the co-occurrences are placed on concentric discs around it. The depth of the analysis, thus the number of levels, is user-configurable. The farther away from the co-occurring tag in the text, the farther away it is also placed in TagSpheres. A divergent cold-hot color map is implemented to transmit the notion of distance. T is colored red, the tags of the outer disc are colored blue, and a color on the gradient between red and blue is assigned to intermediate levels. Clicking a tag X lists text passages in which T and X have the associated word distance.  Choosing the word “righteousness” with a maximum distance of 6 in all Bible books shows an interesting amount of times this word itself and variations of it are reoccurring in the near proximity: The adjective “righteous” appears once two words apart, four times three words apart, once four words apart, twice five words apart and twice six words apart. Also, the word itself is repeated several times (i.e. six times at four words apart) as well as its opposite “unrighteousness” etc. This case shows that the chosen word is frequently being used in a repetitive, sermon-like style of writing, e.g. in Psalms and especially in the Sermon on the Mount in Matthew.  Increasing the minimum number of occurrences can massively change the result. Currently, stopwords are omitted, but researchers might want stopwords taken into consideration when trying to detect interesting chains of words like sayings, proverbs etc. It could also be interesting to look for a single word re-occurring in the span of a work and visualizing this or looking for a more flexible span of words between two re-occurring words indicating (e.g. rhetoric or stylistic) habits of the author.      TagSpheres showing the co-occurrences for the word “righteousness” dependent on word distances from 1 to 6      Summary The three case studies outline that, despite the above mentioned theoretical problems, tag clouds can be–if they are carefully designed–valuable tools to support different research inquiries. As opposed to Wordle, all presented visualizations use tag color and position to express a tag’s set relations. It was important for the literary scholar in all scenarios to interactively get access to the underlying texts to examine upcoming hypotheses. Furthermore, different parameter sets shall be provided to generate multifarious views on the text in question.  ",
        "article_title": " The Value of Tag Cloud Visualizations for Textual Analysis  ",
        "authors": [
            {
                "given": "Stefan",
                "family": "Jänicke",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Markus",
                "family": "John",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Annette",
                "family": "Geßner",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "corpus and text analysis",
            "user experience design",
            "modeling and visualization",
            "interface",
            "gamification",
            "spatial & spatio-temporal analysis",
            "English",
            "computer science and informatics",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Over the past decades, the increase in the use of digital resources and the growth of research conducted in digital environments has transformed academic scholarship. Yet, as the employment of digital resources increases, so does the necessity to understand user behaviour and provide digital infrastructure tailored to the needs of researchers. Through this paper, we aim to explore how the design of digital libraries and resources can be improved to better facilitate information discovery and use in art history; for this purpose, we will look at scholars’ creative encounters with information and present the implications for resource design.  Art history is a highly creative discipline in terms of its interaction with information. Thinking about the beginning of the research process and, therefore, the seeking of the needed information, this is to a great extent linked with the scholar’s intuition and memory. These two qualities, which are associated with connoisseurship, apply especially to the case where research starts from the examination of the artwork. Brilliant (1988:121-122), for example, noted that scholars in the field, after mainly relying on their visual memory to examine a work of art, attempt to search for related information objects. In fact, artworks can often inspire the initiation of the art historical research process through enabling the discovery of the research subject and the generation of research questions; these questions, in combination with the experience of the researcher lead to the searching of the required material. According to Shneiderman’s Genex framework (Shneiderman, 2000), which is considered appropriate for understanding creative information work, using information as a source of inspiration rather than just as research evidence constitutes a characteristic of creative disciplines (another example is architecture, as in Makri and Warwick, 2010). In the digital age, the abundance of information available through digital libraries and resources offers unparalleled opportunities for art historians to engage creatively with a variety of information objects and gain inspiration for research and teaching projects; for instance, digital images have been found to stimulate the thinking process of scholars in the field (Graham and Bailey, 2006: 22). Yet, in order to facilitate creative scholarly processes which can lead to the production of knowledge, information discovery and related phenomena, such as serendipity, which have been found to trigger inspiration (e.g. see Race and Makri, 2016) need to be enabled. We argue that understanding art historians’ information behaviour and needs in terms of accessing and using digital resources is necessary in order to design systems that can positively affect the whole scholarly workflow.  This study employed an ethnographic approach to the study of scholarly practices by conducting semi-structured, in-depth interviews with twenty art historians at different career stages, as well as observation of their physical and digital personal research collections in order to develop a sound understanding of their interaction with information at different stages of the research process. We were particularly interested in creating a pool of interviewees consisting of two groups; one where scholars worked on commonly studied areas (e.g. various areas of European art, like Renaissance art) or employed traditional art historical methods (e.g. stylistic analysis, historical investigation) and another where the topics examined (e.g. non-Western art, digital art) or the methods employed (e.g. quantitative, digital) were considered less traditional. Identifying any similarities and differences between these two different groups of scholars could provide a better insight into the needs that art historians in different areas of the field have in terms of resources and tools. Accessing and interacting with digital libraries and resources was an important part of our participants’ daily work routine; in fact, most tended to start their research in the digital environment, an approach which was also found to facilitate serendipity and trigger new ideas for existing and future projects. Several studies have looked into the role of serendipity in scholarly practice and examined whether it can be supported by information systems (one of the most recent is that by Martin and Quan-Haase, 2017). For instance, Foster and Ford (2003: 337) studied serendipity in the context of the information seeking behaviour of interdisciplinary scholars and suggested that further examination is needed in order to understand that phenomenon which, as they argued, is “[…] a difficult concept to research since it is by definition not particularly susceptible to systematic control and prediction.” Race and Makri (2016) examined the internal and external factors that facilitate serendipity, some of them often beyond the user’s control; these included aspects of the user’s personality, such as curiosity, and issues such as topical knowledge, time, communication or systemic characteristics. It should be noted that the authors highlighted the link that exists between creativity, serendipity and innovation, noting that ‘most of the same factors that encourage or discourage creativity and innovation encourage or discourage serendipity as well’ (Race and Makri, 2016: 16).  In this research, we discovered that serendipity was more likely to occur during the first stage of research, when scholars attempted to investigate a topic. At this stage, researchers tended to be more open to accidental information discoveries- a personal characteristic identified by Race and Makri (2016: 17) as necessary to experience serendipity- and the possibilities to find unexpected information that would significantly affect the research process were greater. On the other hand, during the later stages of research (e.g. writing stage), when creative behaviour was mostly related to the building of the research argument, information seeking behaviour was more goal oriented and the possibilities of experiencing a serendipitous discovery that would have a fundamental effect on the research process were fewer. However, the fact that some areas of research were found to benefit from a larger pool of online resources (e.g. 19th century European art compared to Non-Western art) cannot be overlooked when considering the possibilities of discovering information serendipitously. This issue, then, generates questions regarding the extent to which information resources available online - even when including secondary material - meet the needs of scholars in the various sub-disciplinary areas of art history, like non-Western art. Actually, the art period that a project was looking at, the geographical focus of its subject (e.g. non-Western art) or the fact that the topic under investigation may have not been researched before were often connected to issues of availability of resources, conveniently accessible to scholars. To conclude, as part of this paper we will report on the user requirements for designing systems that facilitate discovery, encourage creative use of information and trigger inspiration which will hopefully prove useful to information and other professionals supporting art historical scholarship. Thus, based on our participants’ reported experiences with digital libraries and resources, interfaces should be simple and the functionalities provided should encourage different types of searching. More specifically, given art historians frequent need to browse content in collections (e.g. when they are not sure what they are looking for) and to engage visually with information, digital resources targeted to this group of researchers should enable visual exploration of collections. This could be achieved through allowing users to get an overview of the material (or groups of information) in a collection, providing suggestions for similar content and offering services that facilitate intuitive interaction with information (e.g. zooming in-out, flicking through) (also see Shneiderman, 1996; Whitelaw, 2015). Apart from that, including related metadata alongside the digital objects in a collection as well as information on the decision-making process with regards to digitisation will enable scholars to make informed decisions when using digital content and gain necessary details for the purposes of their work. Finally, enabling access to digital collections through different means, including the ability to view and download material, is necessary in order to meet scholars’ evolving need to access and manage material across devices and tools. ",
        "article_title": "Embedding Creativity Into Digital Resources: Improving Information Discovery For Art History",
        "authors": [
            {
                "given": "Christina",
                "family": "Kamposiori",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Claire",
                "family": "Warwick",
                "affiliation": [
                    {
                        "original_name": "University of Durham",
                        "normalized_name": "Durham University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01v29qb04",
                            "GRID": "grid.8250.f"
                        }
                    }
                ]
            },
            {
                "given": "Simon",
                "family": "Mahony",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "art history and design studies",
            "user experience design",
            "interface",
            "gamification",
            "English",
            "library & information science",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  While digital archiving practices in the Netherlands in the past two decades have provided better access to oral history collections (Heeren et al. 2009, Kemman et al., 2013, Ordelman & de Jong 2011, de Jong et al. 2014), the effort has also demonstrated that the voices heard in those oral history projects are predominantly white. This paper argues that the composition of the Dutch oral history archive is in dire need of revision and seeks to generate a dialogue on how to remedy this silence. In a discipline that has traditionally prided itself on its emancipatory potential, ethnic minorities and formerly colonized peoples in particular have received relatively little attention. The reasons for this silence are manifold.    First, oral history projects in the Netherlands have mostly dealt with WWII memories, atrocities and trauma victims in particular (Karrouche 2018). Few of these WWII-related oral history collections and research projects have concentrated on events in the colonies and the participation of colonial troops on the battlefield (van den Berg et al. 2010). Others have investigated their interactions with Dutch citizens during the war (Hondius 2010) or the ways in which WWII and its aftermath were experienced in the era of decolonization. Some examples of oral history projects that have successfully honed in on these silences in the Netherlands and the former Dutch colonies are ‘Papua’s in diaspora’ and ‘Het Molukse perspectief in oorlogstijd’, two collections that deal exclusively with the Dutch Indies. Although oral histories of WWII have demonstrably become more sensitive to diversity and multiperspectivity, other historical events that may be relevant to the collective memory and mnemonic practices of minority groups with which the Netherlands have no immediate colonial tie (e.g. guest worker migrants and refugees from southern Europe, North Africa and the Middle East and their offspring), are underrepresented in oral history projects.      Second, oral historians in academic settings often struggle with the perception that their method and sub-discipline is highly subjective and thus, untrustworthy as a historical source (Dudley 2009). As a primary source, an oral testimony of a specific event or a life story is oftentimes attributed a lower status. Two, more recent developments deserve our attention in this regard. Oral history is increasingly being used as a method to gain insight into the ways in which individuals remember, how they selectively engage with the past, and give meaning to their present and future selves. Most oral historians will nowadays not so much regard their method as a means to reconstruct the past through memory. They are far more interested in the construction of identities, their representations and performances. Moreover, scholars acknowledge that the status of original recordings as more ‘truthful’ sources is changing rapidly (Scagliola & de Jong 2014, compare Dudley 2009).                              Third, individual researchers who  do  collect oral histories of post-colonial ethnic minorities in the Netherlands (as elsewhere, see Berger Gluck 2014) are often reluctant to deposit and share their archives in scientific repositories, for instance because they have particular ethical and legal concerns and feel unprepared to tackle these. Oral historians are often unaware of the vast array of choices they have within the ruling European privacy legislation (GDPR). Researchers are reluctant to deposit their oral history data.    But oral history’s roots are not only to be found in trauma studies and war history. Oral history also emerged in the 1960s in urban spaces where the voices of women, working classes and racial minorities had gone unheard (Ritchie 2014). Historians criticized the dominant top-down approaches to history, as histories that centered on the states and men who governed them. Oral history gave voice to the historically disenfranchised, and was practiced first and foremost at a local, not national level. While in the Netherlands these voices may not be heard through scholarly collecting practices, as listed and explained above, they are certainly available through community archives, which operate at a local level but have neither the means nor the expertise to store and disclose their collections. In many cases, oral history is the only way to gain insight into the specific historical experiences of ethnic minorities in the Netherlands, as these are groups that have been underrepresented in the traditional archive. Sources from less privileged groups in society should be added to our archive as a way to address topics and experiences that are underrepresented in national histories due to a lack of documentary sources. Community archives collect oral histories primarily among underrepresented groups, and first and foremost in order to share them with an audience.  In this short paper I therefore advocate a return to oral history’s urban roots and focus on identity construction and minority subjectivity. I will do so by elaborating my insights and preliminary analysis of an experimental cooperation between CLARIAH (Common Lab Research Infrastructure for the Arts and Humanities), a large-scale digital humanities research infrastructure project in the Netherlands, and the small community archive  Verhalenhuis Belvédère  in Rotterdam. This particular initiative aims at connecting local citizens through storytelling, including oral history. The one-year pilot is centered around a collection which voices a diverse group of local citizens and ethnic minorities’ memories of the era of reconstruction in the city of Rotterdam, and its aftermath. To this end, representatives of the community archive, digital repositories and the digital research infrastructure project closely collaborate. Can we design an oral history collection integration workflow that would help community collection owners to integrate their data in digital repositories and research infrastructures? How can ASR and annotation tools support this process? Which sustainable strategies can we develop in order for these collections to be used and re-used in research?    My aim is to generate a dialogue with digital humanities scholars and offer ideas on a template for the further inclusion of community oral history archives in scholarly repositories and digital research infrastructures. In a world rife with issues of diversity and representation, I propose this community-based strategy as a means to navigate these muddied waters, and achieve a more inclusive approach to oral history research in both academic and non-academic contexts through digital practices.  ",
        "article_title": "Still Waters Run Deep. Including Minority Voices in the Oral History Archive Through Digital Practice",
        "authors": [
            {
                "given": "Norah",
                "family": "Karrouche",
                "affiliation": [
                    {
                        "original_name": "Erasmus Universiteit Rotterdam, the Netherlands",
                        "normalized_name": "Erasmus University Rotterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/057w15z03",
                            "GRID": "grid.6906.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "anthropology",
            "interdisciplinary & community collaboration",
            "English",
            "public humanities and community engaged scholarship",
            "globalization & digital divides",
            "public and oral history",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Many stories have been told of the novel’s emergence: the majority string a narrative of development through a handful of texts (e.g. Watt, 1957). More recently, methods in corpus linguistics and topic modeling have determined lexical differences in emerging genres, treating genres as bags of words and counting words to determine large lexical trends (e.g. Biber et al., 1998; Underwood, 2019). This talk offers a different account of the novel’s emergence by focusing on format. A preliminary survey of the 18th century literary field locates two formatting features that emerge within the novel as the novel emerges, monopolizing the novel and differentiating it from other genres. These two features are the modern quotation mark and the chapter number unaccompanied by title or table of contents. Here I will focus on the quotation mark, interpreting it as a high-fidelity index of genre emergence in four particular respects. As a visible index, the quotation mark offers a way of intuiting internal changes within the emerging genre. As a recurrent index, the quotation mark offers a way of gauging the pace of the novel’s emergence. As an evolutionary index, the quotation mark offers a way of registering alternative paths. And as an English index, the quotation mark offers a geographic point of comparison with the emerging French novel (which adopts a different format—the indented dash—to address the same essential question: how to represent conversational dialogue in prose). In other words, the quotation mark contains high information about the  content,  tempo,  contingency, and  geography of the novel’s emergence. What’s more, the quotation mark is fully adopted by the 19th century novel, thereby setting, for an extended period of time, the conditions within which the novel continues to evolve. It is a testimony to the significance of the quotation mark that it continues to affect the evolution of the novel—most notably, I will show, by making possible the celebrated novelistic invention of free indirect style.    Methods A tendency among editors to modernize punctuation in editions of early modern texts has obscured the history of the modern quotation mark, which can now be reconstructed thanks to the wide-spread digitization of original editions. In order to attribute the modern quotation mark to the novel, I compiled my own corpus for each of the following genres—scientific articles (Philosophical Transactions Archive), trial transcripts (The Old Bailey Proceedings Online), literary reviews (Google Books), novels, poetry, drama, and history (ECCO)—then reviewed these pages looking for unique modes of quotation (as well as other genre-specific changes in format). Having attributed the modern quotation mark to the novel, I then reconstructed its emergence within the novel by recording methods of depicting dialogue in popular novels from each decade of the 18th century. As of now, these data are compiled by hand because computing within a larger dataset frequently encountered problems. In the case of the Chadwyck-Healey and HathiTrust databases, text-only transcriptions often remove marginal quotation marks and italics, and double quotation marks are sometimes replaced with single quotation marks (indistinguishable from apostrophes). This project thus raises questions about how best to encode OCR transcriptions to include formatting elements. Current practices have been shaped by the limitations of OCR and by the demands of corpus linguistics and topic modeling, both of which emphasize lexical differences above formatting ones. In what follows, I hope to demonstrate how quantitative analysis of the quotation mark results in a genuinely new account of the novel’s emergence, thereby demonstrating the value of developing new methods for preserving format in OCR transcriptions.   Plotting emergence As the modifier modern suggests, the creation of the modern quotation mark was not a sudden ex nihilo invention, but rather a gradual but substantial reworking of an earlier species. This earlier species of quotation mark ran along the  left-hand margin and was used to denote  the transcription of written text (but not speech). One finds it throughout the 17th and 18th century: in the margins of  Philosophical Transactions denoting passages excerpted from treatises and correspondence; in the margins of literary reviews, denoting sample passages from books reviewed; in the trial transcripts of the Old Baily, which reserve quotation marks solely for texts read aloud in court (leaving witness testimony, which quotes the words of others, unmarked); and even in some early novels, in  Moll Flanders and  Pamela, where dialogue is left alone and where quotation marks exclusively frame the margins of transcribed letters and notes. Based on my survey of the 18th century literary field, this earlier species would plausibly have remained the dominant model were it not for the novel: for it is in the novel that the quotation mark moved from the margin of the page into the gaps between words, and it is in the novel that the quotation mark was expanded beyond transcribed text to denote the larger grammatical category of direct discourse, written and spoken.  I have started reconstructing the emergence of the modern quotation mark within the novel by recording the methods for depicting dialogue in five of the most popular novels from each decade of the 18th century (excepting 1700-1710 and 1730-1740 in which novel production was minimal). Within these novels, dialogue could be left unmarked, italicized in various fluid forms, or denoted by one of four forms of quotation mark: what I’ve termed marginal-inclusive, marginal-exclusive, endpoint-inclusive, and endpoint-exclusive. Figure 1 plots each novel’s most frequent form of depicting dialogue. While collapsing much of the struggle and variation within individual texts (many of which incorporate multiple quotation strategies), Figure 1 nevertheless captures a deep structural shift: the gradual inauguration of the modern quotation mark, which itself advances through three prototypical versions before arriving at the modern format.    Figure 1: Evolution of methods for representing dialogue in the 18th century novel. Novels were selected from three bibliographies based on number of editions (McBurney, 1960; Hahn et al., 1985; Garside et al., 2000). The earliest available edition was downloaded from ECCO.   Tempo, contingency, content... Figure 1 captures a century-spanning trend: in this case, the gradual overlapping steps of an aggregating consensus. The gradual tempo of emergence suggests that a need is subconsciously intuited in the absence of a clear solution, that the quotation mark is being formed for an emerging content that is not yet fully understood. “During a paradigm shift,” writes Franco Moretti, “nobody knows what will work and what won’t” (Moretti, 2013: 74). And that is precisely what one sees here: expansive, almost blind, experimentation slowly whittled down to a single solution. Nor was experimentation limited to the  form of the quotation mark: novelists also fiddled with  purpose, most notably in the ultimately-failed experiment of printing indirect discourse within quotation marks:     Gale document number: CW3311810274    Gale document number: CW3311910127 Instances of this practice abound in the 18th century novel, and not without a certain logic: in each case there is an attempt to acknowledge the residue of direct discourse exuding through. Together with the many morphological possibilities, these instances remind us that the novel could have molded a very different type of quotation mark, both in form and function. These were alternatives, not mistakes. If the modern solution seems obvious, then it merely emphasizes how deeply the novel has defined our worldview. And yet one must ask: why did the novel develop the quotation mark in the way it did? The simple answer is because new forms emerge alongside new content, and in this case that emerging content was conversational dialogue...[abridged from abstract].   Free indirect style The quotation mark instituted a genuinely new matrix for structuring storytelling, one into which the oral stories of the past were slowly squeezed. Over the course of the century, narrators and characters begin to fit themselves more and more neatly into the confines of direct and indirect discourse. As a result, novels like  Moll Flanders, which seem more like witness testimony, are replaced by novels like  Evelina, which seem more like—novels. This is an exceptional ramification of the quotation mark, and nothing illustrates it better than the most celebrated of novelistic inventions: free indirect style.     (Austen, 1816) The introductory sentence is clearly a product of the narrator, and Emma clearly speaks the quoted section to herself; but who speaks those many dashed phrases that burst forth in frustration:  It was a wretched business indeed!—Such an overthrow of every thing she had been wishing for! The first linguists to recognize this style described it as a blending of what they considered two discrete categories: direct and indirect discourse. So Alfred Tobler noted a “peculiar mix of indirect and direct discourse” (Tobler, 1887). A decade later, Charles Bally brought the style to academic attention, defining “three possibilities of rendering the words or thoughts of a character”—direct discourse, indirect discourse, and discours indirect libre—“the first two being long known to grammarians,” the latter being some new combination of the two (Pascal, 1977: 8). As such definitions make clear, free indirect style depends on a clear distinction between direct and indirect discourse. Yet this very distinction comes into being, not with the ancient grammarians, but rather with the solidification of the modern quotation mark solely for direct discourse (Moore, 2011: 131). Novels that denote indirect discourse within quotation marks blur this distinction (see examples above) precluding the possibility of free indirect style. Only with the modern quotation mark, which created a functional and sustained binary between direct and indirect discourse, does free indirect style become possible. Which is not to say that Bally was technically wrong: the Greeks did make a distinction between direct and indirect discourse. Rather, it is a classic case of a theoretical distinction holding less influence than a functional one, and a powerful example of how a new format can allow a genre to evolve and differentiate in new, previously unthinkable ways.    ",
        "article_title": "The Novel And The Quotation Mark",
        "authors": [
            {
                "given": "Holst",
                "family": "Katsma",
                "affiliation": [
                    {
                        "original_name": "Harvard University, United States of America",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "bibliographic methods / textual studies",
            "English",
            "cultural evolution",
            "literary studies",
            "digital archives and digital libraries",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Introduction: Publishing Print Dictionaries as TEI-XML and RDF  The trend towards the retrodigization of print dictionaries -- especially those considered to have a historical as well as a scientific importance -- has been given a new impetus in recent years thanks to improvements in optical character recognition as well as to developments in the creation and promotion of standards and technologies (Khemakhem et al., 2017; Khemakhem et al., 2018) which enable machine actionable versions of such texts to be published and shared much more easily than before. Traditionally the Text Encoding Initiative (TEI) (Budin et al., 2012) has been the favoured standard for the encoding of retrodigitised print dictionaries, but there is now starting to be a move towards the publication of electronic editions of print dictionaries as Linked Data   A recent project, ELEXIS (  https://elex.is/ ), which aims to create a platform for accessing and working with dictionary data, and linking senses together across dictionaries in different languages, works with both TEI-XML and RDF versions of editions of dictionaries.    (LD).   The prior popularity of TEI for this task reflects the dual nature that digital editions of print dictionaries can often have: namely, both as representations of dictionaries as texts, that is as printed artifacts that follow particular typographical conventions and have specific styles of page layout, etc; and at the same time, as computational resources that serve to make the lexicographic and, more broadly speaking linguistic, information contained in the original texts more accessible for querying and further machine processing.  On the other hand one of the greatest strengths of Linked Data (LD) lies in its core emphasis on interoperability between datasets, heterogeneous as to subject area and type, through the use of a common semantic model, that of the Resource Description Framework (RDF), as well as the use of common vocabularies across datasets. Not only does LD encourage the mutual enrichment of individual datasets by facilitating the creation of extensive links between them, it also gives modellers access to a whole ecosystem of Semantic Web technologies and standards, including several out-of-the-box tools for manipulating and visualising structured data. In addition, formal languages such as RDF, RDFS and OWL, which make up part of the Semantic Web stack, allow us to specify and to elaborate on the semantics of the classes and properties used to structure dictionary data.  Modelling a dictionary using RDF requires us to represent the information contained within it as a series of subject-predicate-object statements, which taken together describe a formal graph structure. As a consequence it is much less successful -- which in this case means much less verbose -- than TEI at representing things like the layout and formatting of the original text, or properties relating to the status of the text as a series of tokens, as well as in encoding certain kinds of deeply nested structures. This might suggest that RDF is better suited to describing the more abstract linguistic content of retrodigitised dictionaries, e.g., describing grammatical and semantic information for each entry (along with dictionary metadata), and for embellishing this content through links to other salient datasets and vocabularies. However as we will see, there often exist aspects of print dictionaries that although strictly speaking they concern  how information is presented in the text -- and relate, for example, to the dictionary as a historical artifact -- and not the lexical information contained in the text itself, are still worth explicitly encoding in RDF. This is both because RDF allows us to make this extra-lexical information more accessible and usable and because it helps to ensure that each RDF version of a dictionary is a more self-contained resource.     Modelling Le Petit Larousse illustré In order to flesh out some of the issues outlined in the preceding section, especially from the point of view of elucidating the potential benefits of using RDF as a means of publishing historic dictionaries, we will focus on a particular case study which concerns the French national project, Nénufar   http://nenufar.huma-num.fr   (Bohbot et al., 2018). One of the main goals of Nénufar is to make different consecutive editions of the illustrated French language dictionary  Le Petit Larousse illustré (PLI), published during the first half of the 20th century, publically available both via an online interface as well as as downloadable TEI-XML digital editions and as a linked data dataset. So far all of the editions of PLI published between 1906 and 1924 have been digitised, encoded and made searchable. The native digitisation format is TEI, although the encoding adheres as much as possible to the TEI-Lex0 format (Bański et al., 2017; Romary and Tasovac, 2018); the conversion of entries into RDF is currently ongoing.   The PLI frequently includes encyclopedic information pertaining to its lexical entries along with the more typical kinds of lexicographic data which means that it possesses a strong socio-cultural and historic interest in addition to its significance as a legacy lexicographic resource; indeed, to some extent it can be considered a hybrid resource, dictionary-encyclopedic. Take, for instance, the entry for the word  aviation from three different editions of PLI, from 1906, 1912 and 1915   See the different versions of this entry at  http://nenufar.huma-num.fr/?article=3807  . It’s interesting to track how changes in the entry reflect contemporary discoveries that were taking place in the field of aeronautics at the time.  Here the three successive versions of the entry each contains slightly different encyclopedic glosses. Note also that in the last of the three versions of the entry a reference appears to the lexical entry for the word  aéroplane. In this case two aspects of the same entry have changed over the course of different editions: the textual content of the encyclopedic gloss and the appearance of a new reference to another entry.   As regards the linked data version of the PLI, we made the decision to include as much of the encyclopedic and bibliographic information from the original dataset as possible and to model the evolution of entries across editions since, as we mentioned in the last section, this helps to ensure that the dataset is relatively self-contained -- and prevents users of the RDF version of the PLI having to constantly refer back to the TEI encoding   Go tho the “resources” section of each Nénufar entry to inspect the TEI xml. , something which would go against the universalising spirit of the Semantic Web -- and also because some of this information is well suited to being represented in RDF. By explicitly modelling the editions in RDF using bibliographic and temporal vocabularies and associating each with a specific year, we can query the data for date-specific information.  Furthermore we used the well-known Ontolex-Lemon vocabulary (McCrae et al., 2017) for publishing lexicon-ontologies as linked data as the basis of our encoding in addition to making extensive use of other standards and vocabularies such as the  lexinfo registry, SKOS and DEO   http://www.sparontologies.net/ontologies/deo  . However these did not always provide the properties and classes we needed and so in several cases we decided to create our own. Note that although at the time of writing the Ontolex-Lemon lexicography module is still in the process of being finalised for publication (Bosque-Gil et al., 2016) we have tried our best to make sure that we don’t define any new classes or properties similar to those likely to be in the former. In order to model links between entries we utilised the already existing class  Reference from the DEO vocabulary, and defined a new class  DictionaryGloss to represent any written explanation of a lexical element in a dictionary.   To reiterate, our intention was to model the changes between PLI editions, and indeed in some cases between reprints of the same edition. We decided to model all the separate editions in one graph, since individual changes between entries in different editions usually weren’t comprehensive enough to warrant a separate graph per edition, and in addition there were also differences between reprintings of the same edition and we wanted to avoid creating too many different graphs. To this end we created a class  PLIDictionary to represent  separatePLI editions, along with the object properties,  appearsIn,  firstAppearsIn and  lastAppearsIn to allow us to associate elements with different editions. In our RDF encoding of the PLI, then, we model changes within entries by creating an individual entry component, whether this is a form, sense or gloss, etc, for every change and associating it with one or more PLI editions using  appearsIn,  firstAppearsIn and  lastAppearsIn.   We will explain the strategy which was followed using the example of the RDF encoding of  aviation. In Figure 1 we represent the entry for  aviation and its relationships with its senses. Note that we have added information to the entry regarding its first appearance in the PLI by associating it with the individual  1906_001, which represents the 1906 edition of the work, using the property  firstAppearsIn. Each of the two senses of  aviation has a gloss apiece neither of which changes over different editions in the example.   In Figure 2 we show the three notes associated with the  aviation entry each of which has been modelled as an individual of the class  DictionaryGloss and each of which is associated with a different edition of the dictionary. We are still looking into the best, read most efficient, way of adding information about what is contained in each edition. The simplest way would be to link each lexical element to each of the editions in which it appears, but this would obviously lead to an explosion of triples. Our provisional solution is to focus on adding version information to the elements that change between versions and linking them together using the Dublin Core relation  isVersionOf.     Figure 1. The PLI entry for aviation and its senses and their gloss.    Figure 2. The PLI entry for aviation and the various versions of the entry note associated with it.   Future Work At the time of writing we are carrying out the conversion of the dataset into RDF using the approach outlined above. By the middle of 2019 we plan to make whole of the dataset available both via a SPARQL endpoint and downloadable both in RDF and TEI-XML formats. In the final version we also plan to add links to external conceptual/ontological resources (such as DBpedia and WordNets) using the Ontolex  reference property.   ",
        "article_title": "Historical Dictionaries as Digital Editions and Connected Graphs: the Example of Le Petit Larousse Illustré",
        "authors": [
            {
                "given": "Anas Fahad",
                "family": "Khan",
                "affiliation": [
                    {
                        "original_name": "Istituto di Linguistica Computazionale \"A. Zampolli\", Italy",
                        "normalized_name": "Institute for Computational Linguistics “A. Zampolli”",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/028g3pe33",
                            "GRID": "grid.503055.6"
                        }
                    }
                ]
            },
            {
                "given": "Hervé",
                "family": "Bohbot",
                "affiliation": [
                    {
                        "original_name": "Praxiling UMR 5267 CNRS — Université Paul-Valéry — Montpellier 3",
                        "normalized_name": "Université Paul-Valéry Montpellier",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/00qhdy563",
                            "GRID": "grid.440910.8"
                        }
                    }
                ]
            },
            {
                "given": "Francesca",
                "family": "Frontini",
                "affiliation": [
                    {
                        "original_name": "Praxiling UMR 5267 CNRS — Université Paul-Valéry — Montpellier 3",
                        "normalized_name": "Université Paul-Valéry Montpellier",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/00qhdy563",
                            "GRID": "grid.440910.8"
                        }
                    }
                ]
            },
            {
                "given": "Mohamed",
                "family": "Khemakhem",
                "affiliation": [
                    {
                        "original_name": "Inria ALMAnaCH, Paris; Centre Marc Bloch, Berlin; Paris Diderot University, Paris",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Laurent",
                "family": "Romary",
                "affiliation": [
                    {
                        "original_name": "Inria ALMAnaCH, Paris; Centre Marc Bloch, Berlin; BBAW - Berlin-Brandenburgische Akademie der Wissenschaften, Berlin",
                        "normalized_name": "Centre Marc Bloch",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/043bqfp86",
                            "GRID": "grid.469511.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "linguistics",
            "lexicography",
            "English",
            "text encoding and markup languages",
            "computer science and informatics",
            "semantic web and linked data",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Retrodigitization of both printed and handwritten material is a common prerequisite for a diverse range of research questions in the humanities. While optical character recognition on printed texts is widely considered to be fundamentally solved in academia, with the most commonly used paradigm (Graves et al., 2006) dating back to 2006, this hasn't translated into increased availability of adaptable, libre-licensed OCR engines to the technically inclined humanities scholar. The nature of the material of interest commands a platform that can be altered with minimum effort to achieve optimal recognition accuracy; uncommon scripts, historical languages, complex or archaic page layout, and non-paper writing surfaces are rarily satisfactorily addressed by off-the-shelf commercial solutions. In addition, an open system ameliorates the severe resource constraints of humanities research by enabling sharing of artifacts, such as training data and recognition models, inaccessible with proprietary OCR technology.   Kraken The Kraken text recognition engine is an extensively rewritten fork of the OCRopus system. It can be used both for handwriting and printed text recognition, is easily (re-)trainable, and great care has been taken to eliminate implicit assumptions on content and layout that complicate the processing of non-Latin and non-modern works. Thus Kraken has been extended with features and interfaces enabling the processing of most scripts, among them full Unicode right-to-left, bidirectional, and vertical writing support, script detection, and multiscript recognition. Processing of scripts not included in Unicode is also possible through a simple JSON interface to the codec mapping numerical model outputs to characters. The same interface provides facilities for efficient recognition of large logographic scripts. Output includes fine-grained bounding boxes down to the character level that may be used to quickly acquire a large number of samples from a corpus to assist in paleographic research. Kraken implements a flexible output serialization scheme utilizing a simple templating language. Templates are available for the most commonly used formats ALTO, hOCR, TEI, and abbyyXML. While including implementations of all the subprocesses needed in a text recognition pipeline, most functional blocks can be accessed separately on the command line, allowing flexible substitution of specially optimized methods. A stable programming interface allows total customization and integration into other software packages.   Recognition    Network architecture (H: sequence height, W: sequence length, C: alphabet size) The recognition engine operates as a segmentation-less sequence classifier using an artificial neural network to map an image of a single line of text, the input sequence, into a sequence of characters, the output sequence. The artificial neural network employed is a hybrid convolutional and recurrent neural network trained with the CTC loss function (Graves et al., 2006) that reduces training data requirements to line-level transcriptions (Figure 3). Regularization is mainly provided by dropout (Hinton et al., 2012) after both convolutional and recurrent layers. User intervention in determining training duration and model selection is largely eliminated through early stopping.  Specialized networks, e.g. for particularly complex scripts, can be assembled from building blocks with a simple network specification language although the default architecture shown in Figure 1 is suitable for the vast majority of applications. Processing of dictionaries and library catalogues with extensive semantic markup such as italic, underlining, and bolding, is also possible through specially prepared training data.   Layout Analysis and Script Detection    Sample output of the trainable segmentation method. Kraken's layout analysis extracts text lines from an input image for later processing by the recognition engine. Apart from a basic segmenter taken from OCRopus a trainable line extractor is in the process of being implemented. Full trainability of layout analysis is of utmost importance to a truly universal OCR system, as text layout and its semantics varies widely across time and space, e.g. hand-crafted methods for printed Latin text are unlikely to work reliably on Arabic text or manuscripts with extensive interlinear annotation.  The trainable layout analysis module consists of a two-step instance segmentation method: an initial seed-labelling network operates on the whole page labelling the area between baseline and mean of each line. As the output of the network is a probability of each pixel belonging to a baseline it is binarized using hysteresis thresholding after smoothing with a gaussian filter. The binarized image is then skeletonized and end point are extracted with a discrete convolution. Finally, the vectorized baseline between the endpoints is rectified and a variable environment calculated based on the distance of connected components from the labelled area is extracted. The seed-labelling network is a modified U-net (Ronneberger et al., 2015) on the basis of a 34-layer residual network (He et al., 2016) pretrained on ImageNet.  Preliminary results on a page from a publicly available dataset of Arabic and Persian manuscripts (Kiessling et al., 2019) can be seen in Figure 2. Script detection, the basis for multi-script support in the recognizer, is implemented as a segmentation-less sequence classification problem, similar to text recognition. Instead of assigning a unique label to each code point or grapheme cluster we assign all code points of a particular script the same label. The network is then trained to output the correct sequence of script labels (Figure 3). The output sequence is then used to split the line into single-script runs that can be classified with monolingual recognition models (Figure 4).    Original and modified ground truth (top: original line, middle: transcription, bottom: assigned script classes)     Results    Mean character accuracy Standard deviation Maximum accuracy   Prints      Arabic (Kiessling et al., 2017) 99.5% 0.05 99.6%   Persian Mid-20th century printing  98.3% 0.33 98.7%   Syriac Late-19th century printing in Serṭā form  98.7% 0.38 99.2%   Polytonic Greek Late-19th century printing  99.2% 0.26 99.6%   Latin (Springmann et al., 2018) 98.8% 0.09 99.3%   Latin incunabula (Springmann et al., 2018) 99.0% 0.11 99.2%   Fraktur (Springmann et al., 2018) 99.0% 0.31 99.3%   Cyrillic 99.3% 0.15 99.6%   Manuscripts      Hebrew Midrash Tanhuma, BNF Héb 150  96.9% - -   Medieval Latin Josephus Latinus, Bamberg 78 with augmentation  98.2% - -      Sample output of the script detection on a bilingual French/Arabic page. Note that Eastern Arabic are always classified as Latin text Kraken has been used on a wide variety of writing systems, achieving uniformly high character accuracy (CER). Sample accuracies for a diverse set of scripts spanning across multiple centuries of printing are shown in Table 1.  As a special use case we evaluated recognition of text and emphasis in a mixed English and romanized Arabic library catalog on a training set of 350 lines (50 lines in the validation set) resulting in an averaged CER of 99.3% (σ=0.16) over 10 runs with 95.38% CER on cursive and text with increased spacing (σ=1.46). When using only emphasized text accuracy as the stopping criterium mean accuracy rises to 99.03% (σ=0.28).  ",
        "article_title": " Kraken - an Universal Text Recognizer for the Humanities  ",
        "authors": [
            {
                "given": "Benjamin",
                "family": "Kiessling",
                "affiliation": [
                    {
                        "original_name": "Université PSL, France; Leipzig University",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "artificial intelligence and machine learning",
            "theory and methodology)",
            "English",
            "computer science and informatics",
            "open/libre networks and software",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A self-reflexive turn in the digital humanities, also referred to as the third wave, prompts us to critically examine interlocking media and epistemic changes, seeking answers to the intriguing question, posed by Lorraine Daston, of “how humanists know what they know” (Daston, 2004: 363). Addressing this issue would entail identifying the ways in which humanist thinking depends (and has always depended) on technocultural infrastructures, as well as laying out remediations, affordances, literacies, and shared norms and values pertaining to making knowledge in the humanities. Adopting a media archaeological perspective – in which insights of past and often obsolete technologies inform our understanding of the contemporary media age and vice versa – enables us to analyze the epistemological implications of both the digital and the analog. This paper will explore a yet unexamined archive of paper index cards created and used by Stanisław Pietraszko, the pioneer of cultural studies in Poland, and propose a media-specific and infrastructure-oriented account of knowledge-making in the humanities, especially in the Polish postwar reflection on culture. In examining the artifact – dated back to the early 1950s – and revolving practices as revealed by interviews, narratives, and manuals, the following questions relating to technicality, instrumentality and creativity in the analog humanities come to light: How is knowledge crafted with a slip-box? What does this apparatus consist of? What is the genealogy of its format and standardization? Why were scholarly index cards resisted or criticized by some and, on the other hand, what made the cards so eagerly adopted by others (to an extent that they were later emulated by early-day computer programs, like Apple’s HyperCard, and still remain – in a somewhat skeuomorphic form – in a range of modern applications and interfaces)? What kinds of humanistic work was recognized as valuable thinking, as opposed to mundane, auxiliary labor? And, finally, can index cards affect the style of thinking, foster or even enforce any specific way of reasoning (structuralist, systematic, taxonomical, analogical, nomothetic)? Building upon Lisa Gitelman’s idea of paper knowledge (Gitelman, 2014) and Markus Krajewski’s study of Zettelkästen as a storage medium and a universal paper machine (Krajewski, 2011), I investigate index cards as a knowledge-making device. Apart from its more obvious applications in bibliography and libraries, index cards can also be understood as a part of academic infrastructures (instructively examined in the works by Shannon Mattern and Patrick Svensson, see for instance (Mattern, 2016) and (Svensson, 2016)) or “epistemic surroundings” (to put it in terms proposed by Mario Wimmer (Wimmer, 2017)) typical of the humanities, especially in the fields such as philology and history. I am interested in what can be exposed through examination of the cards about intellectual work in the humanities, especially in the context of postwar Poland. While seeking to grasp a local context (notably, praxiology – a philosophical theory of efficient action, developed by a renowned thinker Tadeusz Kotarbiński – as a specifically Polish background of card indexing), my analysis encompasses a broader view of the tool, referring to working habits and file cards of the luminaries such as Roland Barthes, Niklas Luhmann and Claude Lévi-Strauss. Framing my research in a media archaeology perspective, I propose to read the paper card index “alongside and against” digital media (Emerson, 2014: loc. 2127). A general trajectory of my thinking is threefold and goes from (1) the digital humanities and their critical insight into knowledge devices and epistemic infrastructures to (2) the analog humanities and their uses of paper technologies and (3) back to the digital again, looking into how the index cards have been repurposed by electronic media. This project is media archaeological in several respects. Firstly, it sets out to explore the materiality of scholarly production of Pietraszko, revisiting old and purportedly discarded technologies of intellectual labour. In a worm's eye view it concentrates on snippets, sketches and scribbles. Additionally, technological change is not viewed here in terms of imminent progress, i. e. boxes of paper slips, deemed an instance of cultural theorist’s mindware, are not simply the ancestors of the cutting-edge software employed nowadays by scholars. Last but not least, the nature of such an endeavour is in fact “an-archaeological” (as Siegfried Zielinski dubbed it (Zielinski, 2008)), since an interest in Pietraszko’s legacy is not particularly strong even in the Polish humanities and could be revived by this study. I adopt a notion of the analog humanities, aiming to use it in a more rigorous, theory-laden sense rather than simply in a metaphorical way, usually signaled by the quotation marks around the term. I would like to instill a kind of reflexivity or a retrospective, pre-posterous logic in it (I borrow a notion of “pre-posterous history” from the Dutch art historian Mieke Bal). As Jonathan Sterne argues, “the analog humanities refers to a nexus of methodological, technological, and institutional conditions across the humanities that have only come into clear focus in retrospect” and in this sense the term serves as “a rhetorical before” (Sterne, 2015: 19). Elsewhere, in his entry on the analog in the Digital Keywords he claims that we should “return some specificity to the analog as a particular technocultural sphere” (Sterne, 2016: 41) and not treat this category as a blurry term denoting everything being not digital. Therefore, my paper follows a similar logic seeing the analog humanities as a concept facilitated by the digital humanities and identifying a concrete technocultural condition of scholarship. At the same time, I understand index cards as a constitutive element of the analog humanities, which is not everything that preceded the digital age, but rather a media ecology of print, typewriters, sound recordings, transparencies, overhead projectors, copiers and microfiches, etc., used in the humanities, especially in the second half of the 20th century, to which my case study points to.  ",
        "article_title": "Index Cards and the Analog Humanities. A Media Archaeology of Cultural Studies in Poland",
        "authors": [
            {
                "given": "Aleksandra Maria",
                "family": "Kil",
                "affiliation": [
                    {
                        "original_name": "University of Wrocław, Poland",
                        "normalized_name": "University of Wrocław",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/00yae6e25",
                            "GRID": "grid.8505.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital ecologies",
            "communication and media studies",
            "digital communities and critical infrastructure studies",
            "English",
            "media archaeology",
            "cultural studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Large cultural heritage aggregators, such as Europeana and Google Arts and Culture (GA&C), collect metadata and images from cultural institutions. They provide a single portal that introduces cultural heritage from around the world to the public (Sood, 2016, Petras et al., 2017). Selecting images and artifacts for these aggregators is an outcome of curatorial decisions, enlarging an art canon (Earhart, 2012, Feldman, 2016), building a cultural capital (Bertrand and Kamenica, 2018), and providing an infrastructure for a corpus of art history images (Drucker, 2013) that is critically important for the research in Digital Humanities. However, are such portals indeed a representative and balanced collection, the foundation for objective humanistic study and judgement? In this paper we argue that diversity, although present in GA&C, is too narrow to support our hope that it can act as a corpus of digital art history images. Our evidence proves that the digital corpus amplifies biases within the arts world towards western culture.  Methodology  The source of GA&C collections Our analysis is based on the full collection of two-dimensional images with metadata available on GA&C web site (ca. 5,000,000 images), excluding videos and street view panoramas. All our numeric estimates below are based on  Collections (museum collections or collections of other holders, such as LIFE magazine or Opera national de Paris). We accessed collections via  Places option in the GA&C menu and identified the size of the collections submitted to GA&C and the country of the collection holders.    Russian collections in GA&C GA&C’s collections from Russia are relatively small (ca. 5000 images) supplied by 49 institutions. We have identified geographical location of all 49 GA&C contributors from the Russian Federation and for the 32 from them that are listed in the official museum registry of the Russian Ministry of Culture and are currently tabulating genres, periods and authorship of the 2,802 images that these museums supplied to GA&C. This is a limited share of the artifacts held by over 2,000 Russian museums that altogether hold about 60,000,000 objects in the main part of their collections.   French collections in GA&C We identified 87 French collections that we used as a control group and compared them with the list of over 1,200 French museums downloaded from the web site of the French Ministry of Culture. We found 19 museums from the list of the Ministry with 4,477 images of objects.  We understand that two countries can be hardly enough to find out how representative GAC is and further research is needed to cover a larger sample of countries.    Results  Is Google Arts and Culture a balanced corpus? Western museum collections have traditionally been biased in approaching art objects in the frames of western aesthetic/cultural concepts (Chalmers, 1996, Ang, 2005, Simpson, 2012) and the bias continues in new digital aggregators. Table 1 shows that the majority of images come from the top five countries at the time of writing this paper. It demonstrates that a vast majority of providing cultural institutions come from the United States, United Kingdom and the Netherlands. The largest collection (Table 2) is the LIFE Photo Collection, New York, with 4,5 million images (80% of all images). The fifteen countries to follow the top five are represented with a much smaller number of objects. However, all the countries from the list of the United Nations are represented in the aggregator either as countries with institutional collections or countries tagged as the places of origin of cultural objects ( Discover this place group). Our analysis shows that 123 countries out of 195 nations are not represented in the aggregator through their collections but can be referred to as the places of origin.  Table 1 The number of records with images for the institutional collections representing the places related to the names of countries in GAC. The images from the top five countries’ collections account for 93% of images   Country  No of images, institutional collections  % of total   USA 4,713,779 82 %   United Kingdom 334,558 5.8   Netherlands 170,201 3 %   Italy 98,225 1.7 %   South Korea 52,214 0.9 %   Other countries (190 countries from the UN list) 337,198 6.6 %   Table 2 The images from the top four collections account for 88% of images   Collections Country, city  Number of images, institutional collections   % of  total    LIFE Photo Collection USA, New York 4,403,372 79.3   The Natural History Museum UK, London 298,804 5.4   Rijksmuseum Netherlands, Amsterdam 164,510 3   The Strong National Museum of Play USA, Rochester 72,556 1.3   Other collections Other 615,113 11,1     Do smaller collections represent provincial museums to demonstrate diversity?  Our results demonstrate that although the aggregator can be considered a representative corpus at the scale of nations, it is by no means a balanced corpus. We show that provincial museums take a small portion of the museums represented by the aggregator for the two countries in the study.  The paper will provide the distribution of genres, periods, geographical regions and artists when the study is completed to be presented at Digital Humanities 2019.     Conclusion We demonstrate that GA&C is a representative collection of images as it includes at least two images from the cultural institutions of every country recognized by the United Nations. However, our results indicate that five countries (UK, USA, Netherlands, Italy, South Korea) contributed the largest share of images. Our hypothesis is that GA&C tends to work with the museums that are either easier to work with due to common legislation structures, or similar attitudes and languages, those museums that are more accessible in terms of opening their collections or with private museums that are interested in making their collections known. Further research is needed to provide evidence in support of this assumption. Introducing cultural heritage that was previously difficult to access seems to be the task that new platforms perform fairly well. However, the case of GA&C demonstrates that this digital corpus tends to reinforce traditional biases of western curatorship (Manovich, 2017). This results in important consequences for large scale research in Digital Humanities when obtained patterns are skewed towards western aesthetic and cultural concepts. It may also prevent the general public from building a cultural capital based on cultural diversity.  ",
        "article_title": "Measuring Bias in Aggregated Digitised Content: a Case Study on Google Arts and Culture",
        "authors": [
            {
                "given": "Inna",
                "family": "Kizhner",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University, Russian Federation",
                        "normalized_name": "Siberian Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/05fw97k56",
                            "GRID": "grid.412592.9"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "The University of Edinburgh",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Maxim",
                "family": "Rumyantsev",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University, Russian Federation",
                        "normalized_name": "Siberian Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/05fw97k56",
                            "GRID": "grid.412592.9"
                        }
                    }
                ]
            },
            {
                "given": "Valentina",
                "family": "Khokhlova",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University, Russian Federation",
                        "normalized_name": "Siberian Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/05fw97k56",
                            "GRID": "grid.412592.9"
                        }
                    }
                ]
            },
            {
                "given": "Elizaveta",
                "family": "Demeshkova",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University, Russian Federation",
                        "normalized_name": "Siberian Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/05fw97k56",
                            "GRID": "grid.412592.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "art history and design studies",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries",
            "cultural artifacts digitisation - theory",
            "diversity"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In the discussion about the status of Digital Humanities (DH) as a scientific discipline, the question of theory formation is often raised. Confronted with the accusation of a lack of theory (Bauer 2011) and the call for “more theory” (Lauer 2013, 112)   The text passages of the German research quoted above are translated by the author of this submission. , an excessive theoretical debate can be observed (Gold 2012). On the one hand, due to the heterogeneity of their objects as well as their tasks, DH are positioned somewhat beyond theory formation (Winko/Köppe 2013, 328). On the other hand, theory is being temporally separated from DH (Hall 2012). The proclamation of an ‘end of theory’ heralds a post-theoretical era (Scheinfeldt 2012), while at the same time affirming a pre-theoretical status of DH (Boellstorff 2014, 105). Either way, DH paradoxically appear to lie before, after, besides, or beyond theory formation. Instead DH seem to focus on the “activities” (Flanders/Jannidis 2015, 3) or “doing research” (Kath et al. 2015, 31; Cecire 2011).   In the course of this shift towards digital practice, the development of prototypical software is becoming increasingly important (Ramsey/Rockwell 2012). Transferred from informatic software development, a software prototype reduces a future system complexity to certain subcomponents and exhibits them assuming a descriptive or prospective potential (Houde/Hill 1997; Mogensen 1992). Software prototypes are understood as methodically and theoretically modelled “digital artefacts” (Rockwell/Ramsey 2012). In addition to the functions of exploration, explanation and understanding already established in computer science, software prototypes are regarded in other discursive contexts as arguments (Ruecker/Galey 2010), provocation (Boer/Donovan 2012), mediators of performance and criticism (Drucker 2012). Ruecker/Galey even speak of “prototypes as theory” (2010, 1).  The premise of this presentation is that software prototypes seem not only to gain epistemic status, but also to address the need for still outstanding, or arguably obsolete, theory formation. This presentation will show to what extent software prototypes can be understood as a ‘proto-theory’ of DH. Thereby, the presentation not only puts forward the theory-building effects of a software prototype, but also argues that the software prototype is a unique form of theory design. With ‘proto-theory’ I propose a theoretical form that is constituted not in text, but in the form of the software prototype. So, the presentation is intended as a contribution to the attempts to re-measure the “humanities as a place of advanced theory formation” (Grizelj/Jahraus 2011, 9) and to explore the scope and limits of theory formation based on software prototypes. First, an understanding of the concepts of prototype and theory will be presented. Afterwards, the theoretical merit of the software prototype will be determined.   On the View of Identity and Difference – The Concept of the Prototype  While the notion of a software prototype is highly prevalent in recent DH research discourse, an examination of the concept of the prototype beyond an informatic comprehension is overdue.   Derived from the Greek  prototypon 'Urbild', the composite consists of ' proto the foremost, first, most significant' (Kluge 2011, 728) and ' typos stroke, impact (of gr.  typtein ' strike, shape) 'formative form', outline, shape, pattern' (Ritter 1998, 1587).   Two determinations of the prototype can be distinguished: From the perspective of cognitive psychology (Rosch 1975), the term prototype is introduced in its standard use as an abstract entity that represents the typical characteristics of an extensional category as the best example (Kleiber 1993, 117). The affiliation to a category is determined in relation to the prototype qua analytical quantification of characteristics. To be distinguished from this is a morphological determination of the prototype, as it represents Ludwig Wittgenstein’s family resemblance among others. Using the example of the term “game” Wittgenstein explains that not all “members of a family” (Wittgenstein 1945, 278) are united by a single common defining characteristic, but by “a complicated network of similarities that overlap and cross” (278).   Wittgenstein defines “family resemblance” as followed: “I cannot characterize these similarities better than by the word ‘family similarities’, because in this way the different similarities that exist between the members of a family overlap and cross: Growth, facial features, eye colour, gait, temperament, etc. etc. - And I will say: the ‘games’ form a family” (1945, 278).  Belonging to a category is established by similarity relationships.    Theories as approaches: Negotiating contingency and complexity The concept of theory as a mental viewing or scientific consideration refers to “explicit, elaborate, ordered, and logically consistent category systems that serve to describe, research and explain the facts of their respective object areas” (Nünning/Nünning 2010, 6). Theories address an experience of deficiency that is expressed in the difference between theory and world: “Theories are an effect of the inaccessibility of the world in such a way that they substitute and thus compensate for accessibility as theory” (Jahraus 2011, 28). Providing an approach to the phenomena, theories pursue management of contingency. Given a certain degree of abstraction of the theoretical statement, which “applies to several, possibly even all, individual phenomena of a certain type or provides a model for the phenomena” (Winko/Köppe 2013, 8), theories have explanatory and prognostic functions. Furthermore, the theory formation is subject to the assumption that the complexity of the manifold phenomena can be traced back to superordinate, general principles.   The term complexity is under-defined (Koschorke 2015, 2). In the humanities, Niklas Luhmann’s determination (1976) became particularly relevant: Complexity is “number and diversity of the relations that are possible according to the structure of the system between the elements are.” Recently, James D. Proctor and Brendon M. H. Larson (2005) have described complexity as “a placeholder (in a variety of disciplines) for the unknown.”   Theories enable a (supposed) reduction of complexity by unifying the perspective and creating communicable observation effects.   The distinction between totality and complexity, which plays a role especially in the discussions of German idealism, is also linked to this: The more heterogeneous experiences a theory can unite among itself and bring to a coherent understanding, the greater is the range of the theory. But this also goes hand in hand with the idea that totality takes precedence over complexity (Sandkaulen 2002, 370).   When it comes to literary studies, theories not only constitute a specific approach to the object of literature by reflecting “conditions of production and reception [...] as well as [...] constitution and [...] functions” (Winko/Köppe 2013, 7), but they also generate the object as an epistemic object of their observation in the first place.   Methods differ from theories in that they rather describe the way of proceeding in a target-oriented way. Theories can therefore provide a set of general (reading, generating hypotheses) and specific (deductive, dialectical) methods. Exemplary for hermeneutic theory is the hermeneutic circle or the system-theoretical re-entry for systems theory.  Moreover, theories represent a specific literary text genre (Culler 2011, 3). Since theories appear primarily in the form of written texts and therefore follow a linguistic logic, they are subject to a specific textuality that is constitutive, but not reducible, for the shaping of a theoretical knowledge (Saar 2013, 47).     On the theoretical merits of the software prototype Against the background of the outlined prototype concepts and the function of (literary) theories, the question now arises to what extent software prototypes can meet theoretical requirements. The development of prototype software is first preceded by a process of data modelling and the production of an ontology (Jannidis/Flanders 2015, 7). In the classification procedures for obtaining classes, properties, and relations ([Gruber 2009] cited Jannidis/Flanders 2015, 9), theoretical knowledge already sediments itself, such as the idea of the text or meaning. Beyond the theoretical pre-configuration, however, the difference between the experience of observing and being observed that is constitutive for theory reveals itself in the software prototype. The software prototype represents an approach that is communicated and experienced interactively. As a reduced and compressed model of a set of mostly methodical operations, the software prototype often fulfils the “synthetic and integrating performance of a [...] view” (Wirth 2013, 138). Individual facts or activities are linked, which then appear to be prototypical, i.e. formative and exemplary, as part of a larger coherent explanatory context. More precisely, the software prototype represents theory as a functional structure and thus assumes an exemplary function. It stands “not merely for something else, but prototypically and materially for something from which it itself [promises] to be a part separated only for demonstrative purposes” (Schaub 2011, 12).   Contrary to the thesis of Rockwell/Ramsey (2012) on “Thing Theory”, it is not about the concrete materialization of theory as an objective object that substitutes the textual form of theory.  For example, the abstracts of the DH Conference 2018 contain not only numerous ideas of software prototypes   See DH 2018, Mexico City Abstracts: There are 25 mentions of software prototypes, understood as prototypical software, on pages 51, 55, 67, 115, 130, 137, 141, 178, 238, 250, 253, 397, 451, 470, 481, 483, 505, 531, 534, 537, 542, 559, 570, 584, 592. , but also formulations on explicative and prognostic performance as well as a form of validation.   Examples of mentions of software prototypes with reference to the scope of epistemic achievements: “typical episodes in the domain of a rudimentary experience” (p. 307), “networks are illustrated” (p. 451), “reflect upon the next steps in the project, considering the implications it may have on the data schema” (p. 451), “making it worth simply trying it out rather than lengthy discussions about the value of proceeding”, “establishes confidence and commitment”, “Gaps in desired functionality immediately become clear through interaction” (p. 470).  The precarious epistemic situation shows itself oscillating between the notions of the software prototype as a figuration of evidence and as a temporalized and spatialized form manifesting theory. For the software prototype dissolves the sharp “distinction between the world of things and the world of signs” (Rautzenberg/Strätling 2013, 11). Rather, the boundaries merge smoothly, so that at the same time the “difference-theoretical foundation of theory” (Jahraus 2011, 36) is questioned in favour of an interactive experience.   In contrast to the textual theoretical form, the subject, which wants to gain access through theory, is apparently 'co-constituted' by the interactive orientation of the software prototype. This is followed by the question of subject constitution in textual and technical theoretical forms.   In addition to proving the theory-generating effects, the question arises as to the form in which the theoretical knowledge appears in the software prototype. The visual order of the software prototype in the form of a graphical user interface is not committed to the textual aesthetics of the linear. The (textual) systematicity is replaced by a topological and temporary unfolding of the theory. The software prototype seems not only to represent theoretical knowledge, but visuality and interactivity generate a surplus in which new knowledge emerges. This interferes with empirically proven and hypothetically speculative knowledge. If we now consider the plurality of software prototypes in DH research from a distanced perspective, the individual software prototypes can be regarded as members of a family in Wittgenstein’s sense. In this way, no systematic theoretical structure is created, but rather the dynamics of an open and expanding network.   Conclusions The discussion about the epistemic status of the software prototype not only questions the self-conception of DH as (supporting) science and as research infrastructure   The aspect of research infrastructure includes considerations about interdisciplinary team structures, project constellations as well as research funding and financing. One of the questions under discussion is why resources are used for the production of software prototypes. , but locates its growing body work on theory formation firmly back into the centre of the humanities. What does a contemporary literary theory look like that situates the individual facts in the context of digitality? ‘Proto-theory’ promises to be an answer that implies a fruitful starting point for follow-up research. The growing family of software prototypes in the DH, if understood as a theoretical form, need to be thoroughly considered, contextualized, and critiqued. The tasks for the DH community are therefore to experiment with the legitimation of different forms of theory, to discuss the differences and barriers between textual and technical forms of design, and thus to contemplate this hybrid network of theories.   ",
        "article_title": "Prototypes as Proto-Theory? A Plea for digital theory formation",
        "authors": [
            {
                "given": "Rabea",
                "family": "Kleymann",
                "affiliation": []
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "digital ecologies",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "literary studies",
            "software studies",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   1.  Readers of literary texts have an impression of its complexity, and professional readers like literary critics often use if not the term, but the concept as a basis for their evaluation of literary texts. Recent attempts in literary studies to explore the notion had difficulties agreeing on a definition (Koschorke 2017), a fate which we try to avoid by relying on a very general understanding of complexity: the number of elements and the number and quality of their relations. When we are talking about texts these elements can be words, syllables, metaphors, intertextual relations, the syntax of sentences, themes and topics etc. In this perspective text complexity becomes a multi-dimensional phenomenon. We should emphasize that we are talking about a set of textual features and not about the difficulty for a reader to process the text, which is the domain of cognitive reader studies. Our attempt in this paper to describe a useful approach to thematic complexity in fiction is part of our ongoing research on the complexity of fiction. In Jannidis et. al. (2019) we looked at measures of vocabulary richness and syntactic complexity and applied them to the same collection of popular genre novels and the collection of literature of renowned German writers we are analyzing in this paper. Surprisingly with the exception of sentence length there is no single measure which allows to distinguish consistently between these two collections. This study now looks at thematic complexity. Obviously there is no limit to the themes and topics a novel can deal with, but an infinite amount is difficult to measure. So we use the mixture of genres in documents as a proxy for thematic complexity, and we measure this mixture using topic modeling and Zeta to describe the genres.   2. Cooperation with the German National Library The analysis is based on the digital texts delivered to the German National Library (DNB). The DNB collects, archives, records and makes available to the public the media works published in Germany since 1913 as well as the German-language media works published abroad. Since 2006, the collection of media works published online has also been one of DNB's tasks.  DNB's holdings currently comprise of more than 5 million digital objects, including some 900,000 e-books, around 1.5 million e-journal editions and about 2 million e-paper editions. In addition to the extensive physical inventory, DNB users have a growing pool of \"born digital\" objects at their disposal. The cooperation with the DH communities is a strategically important continuation of DNB's range of services. One aspect is the support of selected cooperation partners through the provision of even extensive corpora, primarily from born digital objects such as e-books, for carrying out automatic analyses in the DNB labs on site.   3. Corpus Our corpus holds 9000 novels from 13 pulp fiction genres and high brow literature novels from prize-winning authors (see Fig. 1).       Novels per genre in corpus       4. Methods   We aim to compute thematic complexity on the mixture of genres in documents. Since genre labels for documents are provided we will use these to identify word fields covering stereotypic themes, topoi or motives in genres. For this task, we use Zeta  (Schöch et al., 2018)  and LDA  (Blei et al., 2003) . In a second step, the words are used to measure a distribution of genres in each novel.    4.1 Zeta Originally, Zeta is used to determine which words are well suited for distinguishing two groups of texts, for example, texts by two authors. However, we are only interested in which words are characteristic of a group. This is why we compare a homogeneous group of texts of one genre with a heterogeneous group to represent the entire corpus.  The zeta scores of word  x   for each genre are multiplied by the frequency it appears within a document. To represent a document by a single vector, the average scores overall words are computed.  We will use variants of zeta as listed by Schöch et al. (2018), see Table 1.   Table 1: Variants of zeta from (Schöch et al. 2018)    document proportions relative frequencies    no transformation  log2- Transformation  no Transformation  log2- Transformation    Subtraction sd0 sd2 sr0 sr2   Division dd0 dd2 dr0 dr2      4.2 Topic Model   We use the LDA implementation Mallet  (McCallum, 2002)   to compute Topic Models with 50, 100, 150, 200 and 250 topics by 1000 iterations over our corpus. We use the same stopwords as in the zeta pipeline. For each topic a genre distribution is calculated from genre labels and this distribution is used to infer the distribution of genre in documents via their share of topics (see Fig.2). Because every topic contains all words and every document inherits all topics to some extent we set a threshold of 5% proportion for topics in documents to be taken into account.       From Topic Model to genre distribution       4.3 Complexity  Both approaches create a representation of documents as vectors with one dimension for each genre. To measure thematic complexity, we will use the Gini-Index (Giovanni Bellù and Liberati 2006). This measure of dispersion is commonly seen in economics to quantify inequality in income. The values for the Gini-Index lie between 0, which indicates that all incomes are equal or in our context that all genres contribute equally to this text, and 1 (for large numbers), which means that only one person has any income or in our context that only one genre contributes to the document. But in this form a higher value would actually mean lower complexity, so for better readability, we will use the inverse Gini-Index, so a lower value indicates usage of mainly one genre’s vocabulary and a higher one the occurrence of words from different genres.   4.4 Evaluation There is no way for us to evaluate thematic complexity scores directly because creating test data by humans would be quite time-intensive and of uncertain success. Instead, we will use genre classification as a downstream task with our document representation (see 4.1, 4.2) as input feature, to at least evaluate the genre distribution in documents, on which thematic complexity is based.    5. Results Evaluation results of genre classification (see table 2) show best results for LDA with 150 Topics. The following results, except the most distinctive words in table 3, are all based on this setup.    Table 2: f1 score (weighted) of genre classification with logistic regression   Zeta   Variant sd0 dd0 sd2 dd2 sr0 dr0 sr2 dr2   f1 .86 .62 .86 .83 .72 .68 .73 .77   LDA   Topics 50 100 150 200 250      f1 .86 .88 .89 .86 .86        Table 3: Most distinctive words per genre (Zeta, sd2)   Regional Western High Lit. Medical    tavern cows salute  mountain rescue   ride gun pasture bridle   so-called jewish earth (de: Erden) philosophy   surgery jealous eat sob       Figure  3  shows the transformation of document vectors consisting of the 14 genre proportions into a 2-dimensional representation. The calculation was done with Uniform Manifold Approximation  (McInnes et al. 2018).       Umap transformation of genre distribution     Figure 4 shows the distribution of genres in genres. Naturally the most prominent genre in each genre is its own class. But it shows that there are - as expected - strong intersections between royal, medical, family and love genres.       Genre proportions heatmap     Figure 5 shows the change in genre distribution of a mixed genre like western with erotic elements compared to its straight main genre, in this case western. Label for these crossover genres are provided by publishers.       Changes in genre distribution of mixed genres relative to main genre         Inverted Gini-Index of novels per genre     Figure 6 shows the inverted Gini-Index, which we propose as a measure of thematic complexity. High Literature, Science Fiction and Crime form a leading group, Love and Adventure novels show high diversity and the war genre seems to be the most monothematic group of texts.    6. Discussion and future work We have shown two approaches to calculating genre distributions, with LDA doing slightly better in downstream evaluation than Zeta. Based on this results, we were able to use the Gini index to calculate thematic complexity. Our assumption that high literature is constituted from a broader spectrum of subject areas than pulp fiction genres is confirmed. Because Zeta and LDA are so close to each other, it seems promising to us to combine both methods, for example, to initialize GuidedLDA with Zeta seed words.  An intermediate product of the calculation with Zeta are word vectors whose dimension represents the distinctiveness of a word for a genre. Considering this as a naive word embedding it seems almost obvious to redefine the whole workflow by learning word embeddings together with genre classification task.  ",
        "article_title": " Thematic complexity  ",
        "authors": [
            {
                "given": "Fotis",
                "family": "Jannidis",
                "affiliation": [
                    {
                        "original_name": "Würzburg University, Germany",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Leonard",
                "family": "Konle",
                "affiliation": [
                    {
                        "original_name": "Würzburg University, Germany",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Peter",
                "family": "Leinen",
                "affiliation": [
                    {
                        "original_name": "German National Library",
                        "normalized_name": "Deutsche Nationalbibliothek",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01n7gem85",
                            "GRID": "grid.424174.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "german studies",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "GLAM: galleries",
            "archives"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  We received funding from the American National Science Foundation to conduct automated formant extraction of stressed vowels, as they occur in sixty-four audio interviews conducted in the Southern US, about 370 hours of speech. The resulting data set contains about 2 million vowel tokens. After extraction, we processed the formants in order to reveal their distributional patterns, overall in the South and in a number of regional and social categories of speakers. Our Big Data phonetics thus represents a challenge to generalizations made on the basis of smaller data sets with few tokens per speaker. Now, with automated tools, we can record complete sets of formants from a data set instead of just a few examples, and our observation of how phonetic realizations actually occur for individuals and in populations can reform traditional understandings of phonetic systems.  We have found that the distributional patterns in our phonetic data follow the predictions of complexity science: realizations of each vowel in each group will occur in a nonlinear pattern where a few types of realizations are very common, some types are moderately common, and most types are rare (see Kretzschmar 2009, 2015). These distributions, call them A-curves, are scale-free, in that the same A-curve pattern will appear in every subset of the data, whether for the overall set, or for any regional of social subset, or for any individual. Our very large data set, Big Data in a humanities setting, shows that the complex system of human speech, generated by massive numbers of interactions between speakers, creates reliable distributional patterns that linguists can use to make better predictions about how language is produced and perceived in populations.  In this paper, we describe a new tool for visualization of all of our Big Data phonetic results, called the Gazetteer of Southern Vowels (GSV), available to the public at http://lap3.libs.uga.edu/u/jstanley/vowelcharts/. The site was built in Shiny (https://shiny.rstudio.com/) , a web application framework for R. With Shiny, users can utilize the computational power of the R programming language without having to learn R or install it to their computers. Shiny was designed to make interactive web apps. GSV exploits the framework to supply traditional F1/F2 plots of our phonetic data, and also to supply point-pattern F1/F2 plots that take advantage of a standard tool of spatial analysis (GIS). A key feature of GSV is a range of user-selected display features as applied to user-selected vowel types in specific environments, and used to display results from user-selected groups of speakers. GSV works with the 2 million vowel measurements from the forced alignment and automatic format extraction of our NSF grant. Our modifications of forced alignment and automatic formant extraction processes (see Evanini 2009, Rosenfelder et al 2011, Reddy and Stanford 2015) are described in Olson et al 2017, but our methods are not the subject of this paper.   The traditional F1/F2 plots of GSV show user-adjustable plots with associated counts and statistical information. Fig 1 shows a plot of the  fleece and  kit vowels from eight speakers in Georgia, with ellipses for the 95% confidence intervals and means marked with labels for the vowels. Fig 2 shows the same data, now with display of the individual tokens as points. The statistical information is shown in Fig 3.  Figure 1    Figure 2    Figure 3      These figures show the difference that Big Data can make for phonetics. Before automatic formant extraction, it was normal for phoneticians to make perhaps 10 measurements of a vowel, and then to take the mean of those measurements as the target location for realization of a vowel. Using Big Data we have hundreds of tokens instead of 10, and their distributional patterns are far more widely dispersed and the mean location, while we can still calculate it, does not appear to represent a very useful generalization about realization of a vowel.   Figure 4      Figure 5      With GSV, we can drill down into such distributions at will. Fig 4 shows the tokens from just one of the Georgia speakers. Fig 5 shows the tokens from the same speaker, now only in environments before a stop consonant. Unstressed tokens can be included at will. As Figs 4 and 5 illustrate, the means differ for the different environments for this speaker, and similar differences occur for each speaker and between all the speakers in the database. The flexibility of the visualization calls into question easy assumptions about targets and the amount of variation for phonetic segments. The point-pattern displays of GSV address the usefulness of statistical means and confidence intervals. Fig 6 shows the Georgia tokens for  fleece, Fig 7 the tokens for  kit.   Figure 6      Figure 7    The ellipses have been retained, but the new elements consist of a square grid applied to the F1/F2 space, and shading by density. The darkest grid units have the densest occurrence of tokens, down to clear units with lightest density (units with no tokens are not included in the display). Figs 6 and 7 show that there are several units with high density, not a single target unit, and that the means of the ellipses do a poor job of representing how people usually realize their vowels. As for the traditional plots, with GSV it is possible to drill down to single speakers and environments with the point-pattern plots.  In addition to the speaker and vowel summaries provided with the traditional plots, the point-pattern visualization provides charts of the frequency rankings of the grid units. Fig 8 shows the ranking for the Georgia  fleece tokens, and Fig 9 the  kit tokens. The rankings both clearly form A-curves, and this status is confirmed by the Gini Coefficient at top right in each figure. As Kretzschmar (2009) discussed, Gini Coefficients for normal distributions are always below 0.2, while A-curve, nonlinear distributions are always much higher. The coefficients here, near 0.5, match what we expect to find in our Big Data phonetics results. The point-pattern visualization thus has proven, with every chart at every level of scale in the dataset, that the phonetic realizations of human speech match the prediction of complexity science for nonlinear A-curves.  Figure 8    Figure 9    We look forward to more and better digital visualizations for our Big Data phonetics, such as a new visualization in 3D phonetic space over time now in development, as in Figure 10.   Figure 10      The visualization looks something like a sheaf (say, of wheat stems), which shows either a relatively straight or a curvilinear movement over time. We use five measurements for this purpose (20%, 35%, 50%, 65%, 80% of the duration of the vowel, as automatically extracted), and plot a line between the measurements. Here, the visualization shows 25 tokens of /i/ in the word \"three\" and 25 tokens of /ɪ/ in the word \"six\", in blue and green. The base of the visualization is a normal F1/F2 plot, so it shows that the origins of the tokens are relatively dispersed and the blue origins overlap with the green. Over time (the five measurements appear clearly in the leftmost green vowel) many of the tokens are straight, but some are quite bent, either diphthongal or perhaps measurement errors. When we develop the sheaf method we will be able to see and evaluate how duration affects the realization of vowels. The visualization of phonetic duration in 3D will be an essentially new way of looking at phonetic data. This challenge to traditional notions in phonetics would not be possible without effective digital methods. Part of the story is forced alignment and automatic formant extraction, which makes it possible to generate Big Data in phonetics. An equally important form of digital assistance is the visualization tool, GSV, as prepared with Shiny, which allows the analyst to see how Big Data is distributed in F1/F2 space. Traditional F1/F2 plots can begin the analysis, and innovative implementation of a spatial analysis technique in phonetic space offers a striking new interpretation of the way that vowels are realized.  ",
        "article_title": "Visualization of Big Data Phonetics",
        "authors": [
            {
                "given": "William",
                "family": "Kretzschmar",
                "affiliation": [
                    {
                        "original_name": "Department of English, University of Georgia, United States of America",
                        "normalized_name": "University of Georgia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00te3t702",
                            "GRID": "grid.213876.9"
                        }
                    }
                ]
            },
            {
                "given": "Joey",
                "family": "Stanley",
                "affiliation": [
                    {
                        "original_name": "Department of English, University of Georgia, United States of America",
                        "normalized_name": "University of Georgia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00te3t702",
                            "GRID": "grid.213876.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "modeling and visualization",
            "linguistics",
            "spatial & spatio-temporal analysis",
            "English",
            "3D/4D modeling",
            "computer science and informatics",
            "modeling",
            "simulation",
            "digital archives and digital libraries",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The region of Trentino-Alto Adige and South Tyrol in Italy is a multilingual border region which is home to a large number of linguistic varieties and languages. Here dialect varieties of the Romance type, like Venetian, Lombard, and Ladin, are spoken side-by-side with Germanic Tyrolean dialects. Additionally, there are two Germanic minority languages, Cimbrian and Mòcheno, and the national languages of the area are Standard German and Italian. The languages in the region exist in a constant state of language contact and language change, influencing and being influenced by neighboring varieties and the national languages. The grammatical systems of the Romance and Germanic languages families have profound differences, yet influence each other in environments of such long-term intensive language contact. Description and analysis of the languages and varieties of a complex linguistic environment will provide valuable insights into the processes of multilingualism and language contact. This project aims to collect data from this linguistically complex region with innovative data collection methods through a digital platform.  VinKo (Varieties in Contact) is an online platform for the collection of oral data from non-standard languages through crowd-sourcing. The platform is designed to collect oral data for the varieties spoken in the region of Trentino-Alto Adige. Speakers are asked to contribute data by filling out questionnaires on phonology, morphology, and syntax in their own local variety. One of the most original characteristics of VinKo is that it records oral responses, rather than written ones. This method of data collection has many advantages. Not only do speakers answer more spontaneously in a spoken register, but oral data also retains important features of spoken language, like prosodic properties. Additionally, it solves a myriad of problems which speakers of primarily oral varieties face when writing their local language in a digital environment. Many minority language and dialect varieties do not have a standard orthography in which to write their language or might need special characters not found on a standard keyboard. On the VinKo platform, data collection is done through a simple interface and in a quick manner, which facilitates easy collaboration with speakers and speech communities (Cordin et al., 2018).  The primary goal of the project is to provide further insight into the different aspects of multilingualism and microvariation. The contact between varieties of different language families, Romance and Germanic, makes this region particularly interesting for applications of the platform. For example, so far it has been used to acquire a complete inventory of obstruents (Alber, 2014) and to examine the syncretism of case on pronouns (Rabanus, 2018). Investigation into non-standard varieties can reveal a lot about the evolution of grammar, which research into standard languages with prescriptive traditions cannot. This type of microvariation research would be practically impossible using the traditional methods of field work, e.g. one-on-one interviews or paper questionnaires, since there is such a high level of variation and diversity in the area. The platform is still undergoing development and hopes to achieve the following things in the near future. First of all, the area for which VinKo is configured has to be enlarged beyond the borders of the Trentino/Alto Adige region, especially to the Veneto region, and data needs to be collected for underrepresented or missing varieties. Secondly, additional data collection methods, including but not limited to questionnaires, have to be developed and employed. Lastly, but perhaps most importantly, the data collected through the platform must be given back to the community in a meaningful way.  Collaboration with local communities and active engagement of speakers with the data will play a crucial role in the larger aim of the project. Minority languages and dialect varieties are experiencing increasing pressure from standard languages, and in many regions, speaker numbers are dwindling. Having an online presence increases the visibility and heightens the prestige of a variety, and it can play an important part in the maintenance of the mother tongue (e.g. Eisenlohr, 2004). Increasing the local identity and pride by increasing awareness of the own language can be an important contribution for improving life conditions and slowing down out-migration from remote rural areas. The platform can help to actively engage speakers with not only the language, but also with other speakers and the local cultural institutes. The digital nature of the data collection encourages collaboration between speakers of different generations, as young and old speakers can bring their own specialized knowledge of language and new technology together.  The platform straddles the balance between academic and community goals, combining them to the benefit of both parties. It sets itself apart from existing language atlases and data bases by collecting oral data and providing community resources. It is also distinctly different from community focused platforms aimed at language teaching and cultural revival, by adding an academic purpose. The platform is open-sourced, which makes it possible for other low-resourced regions to benefit from this development in the future. This talk aims to discuss and introduce the VinKo platform as a linguistics fieldwork tool and crowd-sourced data base, and hopes to inspire fruitful discussion with scholars with similar research aims, projects, or interests. ",
        "article_title": "VinKo: Language Documentation Through Digital Crowdsourcing",
        "authors": [
            {
                "given": "Anne",
                "family": "Kruijt",
                "affiliation": [
                    {
                        "original_name": "University of Verona, Italy",
                        "normalized_name": "University of Verona",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/039bp8j42",
                            "GRID": "grid.5611.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-17",
        "keywords": [
            "crowdsourcing",
            "linguistics",
            "interdisciplinary & community collaboration",
            "English",
            "databases & dbms",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Our research is concerned with the dissemination and transformation of scientific knowledge across Europe. The basis of our investigations forms a corpus of, to date, 343 books that have been printed between 1472 and 1650. We assembled the corpus around a specific text: the  Tractatus de Sphaera by Johannes de Sacrobosco. This 13th century treatise on cosmology describes the spheres of the universe according to the geocentric worldview. Up until the 17th century it has been repeatedly published as part of university textbooks. In these the treatise is included in original, commented or translated form, and accompanied by other texts that were seen as relevant for the study of cosmology from disciplines such as medicine, astronomy or mathematics. As many of these textbooks were part of the mandatory curriculum at European universities, we regard their contents as representative for the scientific knowledge that was being taught and seen as relevant at the time of publication of the books.  We extract several markers from the individual books that form the material evidence of our research. In addition to bibliographic data such as publishers, printers, date and place of publication, etc., we identified for every book the content structure: which texts it contains and, if applicable, wether the texts are commented or translated versions of existing texts. In doing so we can not only identify how the content of the books changed and – by extension – how certain disciplines gained and lost in importance, but also which publishers might be responsible for certain changes. The books also contain various types of visuals: diagrams, illustrations, decorative elements, etc. In the same way as texts, these visuals can offer insights into the kind of knowledge that is being distributed. By identifying and analyzing recurring images, we can evaluate the 'success' of certain imagery. If we find the same images being used by different printers, for example, that might be telling of one printer being influenced by another, or even indicate a physical exchange of wooden printing blocks. In this paper we present our approach for analyzing the more than 16.000 illustrations that we have annotated in our corpus. We employ an image hashing algorithm for identifying recurring images and existing visualization tools for analyzing the results. As the algorithm we use is independent of the visual material and – unlike machine learning algorithms – does not need to be trained, it can readily be used on arbitrary image collections. As part of this paper we will offer the entire analysis and visualization workflow for others to reuse.       The algorithm organises the images into groups of same or similar ones. While most of the groupings are correct, some diagrams may incorrectly be assigned the same group (e.g. third row from the top)   Process The images we analyses are being manually annotated by a team of student assistants using the Mirador IIIF viewer and classified as either Content Illustrations, Initials, Frontispieces, Printer's Marks, Title Page Illustrations or Decorations. They are stored in RDF as annotations on the digitised pages of the books, along with the remaining metadata that we gather in the project and store according to a CIDOC-CRM data model in a Blazegraph database. For processing, the respective regions of the original pages are downloaded to a local machine via a IIIF API. We want to identify which of the illustrations and diagrams appear several times in our corpus of books. In other words, we want to organise the total set of images into groups that are duplicates or near duplicates of each other. Duplicate detection in a set of digital images can be achieved through an Image Hashing algorithm, as proposed by Venkatesan et al.(2000). A hash function takes an arbitrary sized input and deterministically produces an output of a fixed size, the so-called  digest. For an introduction to hash functions see Knuth (1998). In order to identify images that are not duplicates but variations of each other, a particular type of image hashing algorithm is required. A perceptual image algorithm (Zauner, 2010) is designed to take an arbitrary image as input and produce a digest that bears a deterministic relationship to the input image. We use the difference hash, or dHash, algorithm (Kravetz, 2013) in an implementation for the Python programming language (Buchner, 2017). The algorithm works by scaling down and converting the input image to grayscale, and then produce a digest based on each pixel's difference in brightness to its neighboring pixels. The similarity between two images can then be expressed as the difference – the Hamming distance (Hamming, 1950) – between two digests.  For analyzing the output of the algorithm we make use of a tool initially developed to visualize a collection of coins (Gortana et al., 2018). The web app, which is freely available on GitHub, allows us to visually inspect the entire set of images and evaluate quality of the identified groupings.   Results After experimenting with different values for the threshold and rescaling resolution we found the ones that are optimal for our particular image collection, resulting in match found for 66% of the images in our set. We use a two-pass process in which the images that have not been matched in the first pass are processed again using adjusted settings. The resulting image groups have given us immediate new insights into the reuse of images within this particular corpus of publications. We were able to discriminate clear patterns in the use of certain images, such as, which images have been published throughout the print history of our corpus – there are two – as well as which images have changed context from one text to the other – only one. In contrast to more 'sophisticated' methods of image analysis using pre-trained neural networks, the method used works in a transparent fashion and independent of the material at hand. Only two parameters need to be adjusted – threshold and scaled image resolution – to optimise the output for a given image collection. The algorithm as well as the visualisation tool employed here can be applied on any given image collection.       Visualizing the identified image groups against date of publication reveals patterns in the use of certain visuals. The large groups of images at the top represents those that have not been assigned a group. Visualization tool: Coins by Flavio Gortana,     ",
        "article_title": " Calculating Sameness: Identifying Image Reuse In Early Modern books  ",
        "authors": [
            {
                "given": "Florian",
                "family": "Kräutli",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science, Germany",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            },
            {
                "given": "Matteo",
                "family": "Valleriani",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science, Germany",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            },
            {
                "given": "Daan",
                "family": "Lockhorst",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science, Germany",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-11",
        "keywords": [
            "digital humanities (history",
            "modeling and visualization",
            "bibliographic methods / textual studies",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "history of science",
            "cultural analytics",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  From qualitative work of Moretti (2013) on Shakespeare's plays and Chinese novels, through quantitative works on the 19th century English literary fiction by Elson et al. (2010) and Jayannavar et al. (2015) to the investigation of dynamic plots of German plays by Fischer et al. (2017), the analysis of social networks induced from literary works became a valuable tool in digital humanities research.  This paper presents a study on induction and quantitative analysis of character networks inferred from Polish novels. The corpus gathered for this study is an order of magnitude larger than the collection of novels used by Elson et al. (2010) and Jayannavar et al (2015). It contains primarily novels from the second half of the 19th century and the first half of the 20th century. The main goal of this paper is to present novel results on systematic differences between the 19th century and 20th century Polish prose with respect to the collected corpus. The two by-products of this research are:    The development of fully automatized, quantitative pipeline that leads from raw Polish text to the set of testable hypotheses.  The reproduction of the observations of Elson et al. (2010) and Jayannavar et al. (2015) on a larger, more demanding corpus of a different language origin  that contains both the 19th century and 20th century works.         Figure 1. Main characters of  Lalka by B. Prus    Data collection In order to build the corpus I utilized two digital libraries that offer texts in Polish. The Polona digital library (Polona, 2018) which is maintained by the National Library of Poland offers digitized copies of printed books. I have managed to fetch from Polona around 3000 volumes that are in public domain and are available online in a form of OCR-ed text. The multi-volume editions of novels fetched from Polona were merged resulting in ca. 2300 complete pieces of literary fiction. The second source of texts for the corpus is the Wolne Lektury library (Wolne Lektury, 2018) that focuses on school readings and offers carefully revised electronic editions of books that are in public domain. Around 230 novels were available for download from Wolne Lektury at the time of writing.  In order to make the corpus representative I decided to select exactly one (the most recent) edition of every novel that has authorship attributed, resulting in 1555 unique pieces of work. Due to the sparsity of available data I restricted my attention to novels created between 1800 and 1945, obtaining 1443 volumes in result. Since the focus of this research is on Polish novels only I have selected from the corpus the books that have Polish origin according to the catalogue of the National Library of Poland (Biblioteka Narodowa, 2018). Thus, the corpus used in this paper consists of 930 novels (392 from the 19th century and 538 from the first half of the 20th century).  Before feeding the texts from the corpus into the network induction procedure described in the following section some preliminary processing is required. In case of Polona texts word segmentation errors introduced by OCR have been fixed by a custom normalization script. Furthermore, since the part-of-speech (POS) and named entity recognition (NER) taggers used for the network induction are trained on a corpus of modern Polish language (Przepiórkowski et al., 2012), I have applied a diachronic normalizer (Graliński, 2018; Jassem et al., 2017) in order to contemporize the Polona texts for the purpose of improving the part-of-speech and named entity recognition accuracy. Finally, the novels are split into paragraphs by another script. The texts from Wolne Lektury are checked for errors and contemporized by library editors before publication, hence beside splitting them into paragraphs in accordance to the XML schema no further processing is required.     Figure  2 . Novels in corpus by decade     Network induction  Beforethe conversation network can be inferred from a novel, the text has to be passed to the annotation pipeline that appends additional data necessary for the network induction procedure. The annotation pipeline splits the paragraphs of text into sentences and tokens. Then, the text is lemmatized with help of the Polimorf dictionary (Woliński et al., 2012).Afterwards, the text is annotated by POS and NER modules that were trained using the manually annotated 1-million word subcorpus of the NKJP corpus (Przepiórkowski et al., 2012). The final step of the annotation pipeline is the detection of dialog boundaries. A script with hand-crafted rules that take into consideration possible shapes of beginnings, endings and internal paragraphs of dialogs is used for this purpose. The script failed to extract dialogs from 23 books, thus the networks have been inferred for 383 19th century novels and 524 20th century novels, respectively.  The network induction procedure iterates over dialogs identified in the novel. The dialog turns are surface parsed in order to detect speakers and distinguish them from other named entities that are referenced in the dialog, but do not talk. Since characters can be referred in text in different ways, E.g. by their names, surnames, first names, diminutives, honorifics. the detected speaker mentions are passed to the entity resolution module which is responsible for assigning a common identifier to all mentions of the same character on the basis of the dialog history and the plot. Finally, a (conversational) link is created in the network for every pair of identified speakers that participate in the dialog.      Figure 3. Annotation pipeline    Results Elson et al. (2010) made a distinction between characters and speakers (characters that took part in at least one conversation). I decided to focus the study on speakers only since the NER module identified many entities that are not relevant to the conversations such as historical figures. Plus some false positives that definitely are not named entities.   The conversation networks induced from the entire corpus have 34 (σ=25.42) characters on average. The mean number of dialogs is 191 and the average number of conversation links between characters is 80.   The average number of conversations that a character is involved into Average node degree according to graph terminology. is 2.28 . I have run the Walktrap community detection algorithm (Pons and Latapy, 2005) and found the mean number of communities to be 4.68 and the average size of a community to be 5.68. Excluding communities of size 1 that are found by the walktrap algorithm.    Network metric 19th century 20th century All   character count  36.87  ±27.48    31.48  ±23.56    33.75  ±25.42     dialog count  182.60  ±168.07    196.27  ±158.08    190.50  ±162.42     link count   89.56  ± 109.23     73.16  ±94.08     80.09  ±101.03     average degree  2.35  ±1.81    2.23  ±1.52    2.28  ±1.65     community count  5.13  ±3.49    4.35  ±2.83    4.68  ±3.14     community size  5.83  ±2.94    5.56  ±2.55    5.68  ±2.73      T able 1. Network properties   (Elson et al., 2010) and (Jayannavar et al., 2015) reported that the number of characters in the novel is correlated with the properties of the inferred network. As can be expected the same observation holds for the conversation networks induced from Polish novels. The number of characters is strongly correlated to the number of dialogs (r=0.73) and the number of conversational links (r=0.86). Furthermore, the number of communities is strongly correlated to the number of characters (r=0.83). In contrast to ( Jayannavar et al., 2015 ) I found the number of characters to be positively Instead of negatively. correlated to the average number of interlocutors that a character has (r=0.44) and the average community size (r=0.43). This result may be due to the definitional differencebetween conversational links used in this paper and interaction links used by Jayannavar et al.     Network metric 19th century 20th century All   dialog count 0.74 0.73 0.73   link count 0.84 0.88 0.86   average degree 0.35 0.54 0.44   community count 0.83 0.83 0.83   community size 0.40 0.46 0.43    Table 2. Network metrics correlated with character number  Having the corpus that contains comparable number of the 19th and 20th century novels, I decided to check if properties of networks change systematically between the centuries. Since network metrics are not normally distributed, Normality of all network metrics discussed in the paper is rejected according to Shapiro-Wilk test. I have used the Mann-Whitney test to verify the hypothesis that it is equally likely that a randomly selected novel from the 19th century subcorpus has a lower or higher value of the network metric being tested than a randomly selected novel from the 20th century subcorpus. This hypothesis has been rejected in case of the character number, dialog count, link count and community count metrics and maintained in case of the average degree and average community size (cf. Table 3). These results suggest that (at least with regard to the collected corpus) the prose of the first half of the 20th century became richer in dialogue, but at the same time focused on smaller sets of characters.    Hypo-thesis  19th century median   20th century median  p-value 0.95 conf. interval   character count  30.0  26.0 0.00098 [2.00, 6.00]   dialog count 132.0 161.5 0.00671 [-34.0, -6.0]   link count  54.0  43.0 0.00370  [3.00, 15.00]   average degree  1.9  1.9 0.68892 [-0.14, 0.21]   comm. count  4.0  4.0 0.00020 [0.000068, 1.00]   comm. size  5.2  5.0 0.26845 [-0.117, 0.49]    Table 3. Network metric change between centuries    Final Remarks The systematic differences between the 19th and 20th century Polish novels presented in this paper are interesting on their own, but they can also initiate further computational investigations of the character networks. One problem that is not covered by this study and should be examined in the future is the impact of indirect speech on the structure of character networks. Another issue that should be taken into consideration is the verification if the proposed hypotheses still hold in presence of the constantly growing corpus of digitized literary works of Polish origin. Polona claims to add up to 2000 digital objects such as books, photographs and postcards on the daily basis (Polona, 2018).   ",
        "article_title": " Quantitative Analysis of Character Networks in Polish XIX and XX Century Novels  ",
        "authors": [
            {
                "given": "Marek",
                "family": "Kubis",
                "affiliation": [
                    {
                        "original_name": "Adam Mickiewicz University, Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "corpus and text analysis",
            "natural language processing",
            "data mining / text mining",
            "English",
            "network analysis and graphs theory",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "     Library catalogues contain rich, albeit potentially incomplete information on historical trends and shifts in knowledge production. Their research potential has been debated for more than 50 years (Tanselle, 1974). Large-scale harmonization and analysis of these data collections has provided new ways to investigate classical research hypotheses in book history and intellectual history. Whereas the potential biases in the data, arising for instance from variations in data collection practices over time and place, have to be carefully considered in the analysis, a data-driven approach provides a uniquely large-scale view and a supportive role in research. Despite related earlier work (Buringh and van Zanden, 2009; Bell and Barnard, 1992), a systematic research use of bibliographic metadata has proven to be challenging. The lack of scalable solutions for improving and verifying data quality and completeness have formed major bottlenecks for large-scale analysis. Our team has recently proposed the concept of bibliographic data science in order to overcome some of these challenges. Here we provide an overview of the ongoing attempts to develop a broader research line that focuses on the development of targeted analysis methods in this research area, and specifically demonstrate the advantages of fully open bibliographic data science. A number of specific case studies and applications of this approach are included in the proceedings of this conference.     Data and methods   Bibliographic data science   (Lahti et al., 2019)   is an emerging paradigm in the digital humanities. It aims to improve the overall data reliability and completeness through systematic harmonization, error correction, and enrichment of missing information, thus greatly enhancing the research potential of bibliographic collections. It derives from the paradigms of open science and data science (see e.g. Lahti et al., 2015; Tolonen et al., 2016;  Lahti 2018b), and incorporates best practices from these fields, including reproducible analysis, open source code, and open collaboration models. We have recently integrated metadata across four large bibliographies and altogether 2.64 million harmonized entries in the period c. 1500–1800 (Tolonen et al., 2019) from the Finnish and Swedish National Bibliographies, the English Short-Title Catalogue, and the Heritage of the Printed Book database. Compared to the earlier efforts, our automated approach is uniquely scalable and comprehensive in terms of data integration and quality monitoring. Furthermore, it is combined with systematic expert curation and research use that allow us to detect shortcomings and inconsistencies that are historically relevant but challenging to observe by automated means. As such, our newly implemented methods exemplify the application of augmented intelligence in digital humanities data curation and analysis.    Case studies  Large-scale bibliographies are seldom available as open data. This has formed a bottleneck for the development of new data processing and analysis methods as their usability and value is very limited and restricted to only those research groups who have access to the same or similar data collections. This is in contrast to some other data-intensive fields such as bioinformatics, where considerable data resources are openly shared by national and international research organizations, and algorithmic tools to access and utilize them are being routinely developed and shared widely within the research community. The National Library of Finland has, however, released The Finnish National Bibliography in an openly licensed, machine-readable format (National Library of Finland, 2017). This combination of open data and open analysis workflows allows us to demonstrate the opportunities of fully open bibliographic data science. We have previously estimated the long-term development of book formats, which reflects shifts in reading habits and public communication over time (Lahti et al. 2015; Lahti et al. 2019; Tolonen et al. 2019). One example is the observed changes is the rise of the octavo format, which supersedes other printing formats during the eighteenth century, in parallel with a systematic decline in the use of Latin and a growing share of published books printed in vernacular languages. Here we complement such case studies by demonstrating the challenges and opportunities in opening the complete analysis workflows, and show how this establishes the overall methodological basis for the more specific case studies that are being presented in this conference.    Conclusion  Bibliographic data science is renewing research in digital humanities in general, and in book history in particular. Related data harmonization efforts include for instance the Collections as Data project (Padilla et al. 2019), which has promoted generic research use of data collections in digital humanities and related fields. In contrast, our work explicitly focuses on the specific research area of early-modern knowledge production and intellectual history. Our focus is therefore more specific than in generic data science projects. The scalability of the work over additional metadata fields and different time periods can pose remarkable further challenges which could be partially addressed by focusing on specific topics of interest, such as variations in authors, language use, publishing networks or document materiality, as is often the case in pragmatic research projects. Our vision includes combining the harmonized metadata with full-text collections such as the ECCO, and studying how the materiality of printing is related to developments in newspapers (Marjanen et al., 2017). When combined with a proper quality control, such data-driven approaches have potential for wider implementation in related studies in the digital humanities. Hence, the contribution of this work is not merely in the development or application of new algorithms or exploration techniques, but in demonstrating their wider potential in advancing the methodological basis of the field.   Funding  This work was supported by the Academy of Finland [grant number 293316].   ",
        "article_title": " The Emerging Paradigm of Bibliographic Data Science  ",
        "authors": [
            {
                "given": "Ville",
                "family": "Vaara",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Ali",
                "family": "Ijaz",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Iiro",
                "family": "Tiihonen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Antti",
                "family": "Kanner",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Tanja",
                "family": "Säily",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Leo",
                "family": "Lahti",
                "affiliation": [
                    {
                        "original_name": "University of Turku, Finland",
                        "normalized_name": "University of Turku",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/05vghhr25",
                            "GRID": "grid.1374.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-09",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "digital research infrastructures and virtual research environments",
            "bibliographic methods / textual studies",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As forms of digital textuality proliferate, genres such as the periodical and the scrapbook have come into view as productive lenses through which to understand the relationship between digital texts and print culture artefacts. Observing that the process of digitally remediating a periodical can lead to the emergence of “surprising affinities” between the two categories, Sean Latham suggests ways in which such remediation makes it possible to see periodicals as forming a bridge between print and digital media (33). Similarly, Ellen Gruber Garvey points to the connections between scrapbooks and contemporary information management practices, arguing that just as individuals in the present day “manage digital abundance with favorites lists, bookmarks, blogrolls, RSS feeds, and content aggregators,” so “nineteenth-century readers channelled the flood of information with scrapbooks” which allowed them to save, organise, and reprocess this information (4). These reflections on how the affordances of particular print culture genres produce particular reader behaviors chime with Johanna Drucker’s call to think about what a codex book  does rather than focussing on what it  is: to “understand it in terms of what is known in the architecture profession as a ‘program’ constituted by the activities that arise from a response to the formal structures” and, through this process of denaturalizing, come to better understand its specific structural and technological features (qtd. in Latham 37).  In this paper we describe our digital prototype,  Working from Scraps ( https://working-from-scraps.herokuapp.com/wfsscrapbooklevel), which remediates a 200-page subset from the scrapbooks of Scottish Poet Makar Edwin Morgan (1920-2010) held at the Glasgow University Library Special Collections. A sample two-page spread from one of the scrapbooks is provided in figure 1, and a screenshot of this prototype is given in figure 2. In considering how our prototype engages with some of the questions above, we seek to address the gap in the scholarly literature around the specific utility of  scrapbooks as objects to think through questions about the relationship of material texts to their digital analogues, to digital textuality more generally, and, we will argue, to other genres such as database and narrative. Remediating a set of scrapbooks as a relational database and a digital interface provides the opportunity not just to “denaturalize” a scrapbook and consider how its elements relate to one another, but also to better understand what is involved in ‘traversing the text’, to repurpose a phrase of Espen Aarseth’s from his discussion of ergodic literature: the effort required to tie image to text and carry out other meaning-making activities.  Constructing a database-backed digital prototype from a set of fragile scrapbooks which cannot be moved from the archive has the obvious benefit of bringing these artefacts, and the period of twentieth-century life which they meticulously capture, to a wider public than those individuals who are able to consult the artefacts in person. From the perspective of digital humanities, however, such a remediation has additional value in shedding light on the interplay between database form and more conventional book historical and literary critical categories. Drawing on art historian Ervin Panofsky’s characterisation of linear perspective as a key “symbolic form” of the modern age, Lev Manovich proposes that the database can be understood as “a new symbolic form of the computer age,” one which offers “a new way to structure our experience of ourselves and of the world” (81). If digital interfaces present us with an endless, unstructured array of images, texts, sounds, and other kinds of data records, Manovich maintains, it makes sense to model these records as a database, with the further consequence that we then need “to develop a poetics, aesthetics and ethics of this database” (81). The  Working from Scraps prototype seeks to advance this project of working through what database aesthetics, poetics, and ethics might look like, but with the additional imperative of engaging questions around materiality, ephemerality and copyright restrictions which are not ordinarily so prominent in such discussions.  When “deforming” a set of scrapbooks into a digital prototype, opportunities to learn arise from the process of thinking about the underlying data structures. Latham, borrowing terminology from Aarseth to describe the digital archive  The Modernist Journals Project ( http://modjourn.org), describes how creating a digital edition of a periodical involves first identifying its textons—the discrete parts which form its basic units—and then determining how those might be assembled into scriptons—the unbroken sequence of information that is delivered to the user on the screen (53). This identification was one of the core conceptual tasks involved in the construction of the  Working from Scraps prototype, and it included the following challenges:   As the scrapbooks were created between 1931 and 1967, and thereby fall squarely into the copyright “black hole” of the twentieth century, the time and financial costs of copyright clearance for the thousands of third-party materials prevented us from simply displaying digital facsimiles of pages and the clippings constituting them for our users. Information about each page needed therefore to be communicated legibly enough that a user who had never seen the page at all could understand it. This activity, borne of practical necessity, quickly became ekphrastic and raised specific identification questions: How much description should be provided? To what level should a scrapbook’s visual components be broken down: pages, clippings, or other units? How could we recuperate some of the visual presence of the clipping components through non-verbal means, so as to not rely solely on this ekphrastic practice?    As databases aim to provide machine-readable data which can be queried by users, they present an imperative for a degree of standardization. With scrapbooks, however, as with so many other real-world objects, such standardization can be difficult to produce, and is not always desirable to impose. Chronological data present one example of this problem. If a date is provided either by Morgan or as part of a clipping in his scrapbooks, the form in which it is given varies: as a year, a month and a year, or as a precise date. How should such dates be standardized, if at all? How would such standardization account for the (many) undated items? And does this standardization erase the “messiness” of the scrapbooks’ presence?    A similar challenge arose around the task of quantifying elements from the scrapbooks that were not inherently quantitative, for example finding ways to register how clippings are layered on the scrapbook pages. Our database uses a z-index format in which 0 denotes something drawn directly onto a page, 1 a clipping pasted onto a page, 2 a clipping pasted on top of another clipping, and so forth. However, an occasional “foldout” clipping with clippings pasted onto the back of the fold disrupted this schema, as it was unclear whether the attached clippings should be coded as 1, as 2 (given that they were additionally layered when the fold was closed), or as something different altogether. Difficulties such as these betokened an additional challenge: to avoid presenting the scrapbooks too thoroughly through quantitative lenses such that description, natural language, images, and so forth would be obscured.  In presenting  Working from Scraps as an exemplary prototypical object for working through these and other challenges arising from the remediation of print culture artefacts into data structures and digital textual forms, we also offer our interface as an apt illustration of Matthew Kirschenbaum’s point that platform studies and editorial studies share significant common ground, concerned as they both are with “the material substrates through which a text takes shape and acquires meaning” (Latham 57). In addition to revealing a wealth of detail about twentieth-century life, Scottish culture, queer identities, technological developments and a wide range of other topics, the Morgan scrapbooks and their digitally de/reformed versions seek to contribute to the conversations around how a reader’s “asynchronous, contingent, nonlinear encounter with the text” (Latham 57) may be remediated, and how the modelling of unruly print culture objects can be used to interrogate our assumptions around what constitutes a literary text.     Fig. 1. Double page spread from Edwin Morgan’s Scrapbook 1 (1931-53), pp. 227-8. Made available by the Glasgow University Library at  www.flickr.com/photos/uofglibrary/4545010659/in/album-72157623915217656/ .     Figure 2. Screenshot of the  Working from Scraps prototype, showing different views on the data contained in the clippings.  ",
        "article_title": "Database Aesthetics and Ergodic Ephemerality: Remediating the Scrapbooks of Edwin Morgan",
        "authors": [
            {
                "given": "Bridget",
                "family": "Moynihan",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Jonathan",
                "family": "Armoza",
                "affiliation": [
                    {
                        "original_name": "New York University, United States",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            },
            {
                "given": "Anouk",
                "family": "Lang",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "digital textualities and hypertext",
            "methods and technologies",
            "communication and media studies",
            "English",
            "cultural artifacts digitisation - theory",
            "literary studies",
            "digital archives and digital libraries",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Summary Objects are essential to artistic images as they reveal a person’s identity or profession; repetitions then testify to their popularity, reveal processes of reception and artistic relations (Fig.1). Thus it is crucial for art historians to find objects in images. Digitization has produced large image corpora, but manual methods proof to be insufficient to analyse these collections; the collaboration between art history and computer vision provides methods and tools, which enable a comprehensive evaluation of images. The paper presents a user-oriented search engine for object retrieval, thus assisting with art historical research. After presenting specific requirements for retrieval systems, the paper introduces the engine, exemplifies a search and shows qualitative results. We include critical remarks on existing tools and possible issues, which arise when working with artistic data.     Figure 1. The detection of objects is crucial for art historians to study reception processes or artistic relations. Here, the skull is shown in artworks of different genres, styles and techniques throughout time. Image by the Computer Vision Group, Heidelberg University  Object detection and retrieval have been core tasks in computer vision; results are obtained by using hand-crafted features (Gordo et al., 2016) or learning-based features, favorably used in recent years due to the rise of CNNs (convolutional neural networks) (Tzelepi and Tefas, 2018). Works used a template-based detector to find gestures in manuscripts (Schlecht et al., 2011) or additional curvature information of objects to improve detections (Monroy et al., 2011). A discriminative model based on parts and aggregated compositions was further utilized to propel object detection and scene classification (Eigenstetter et al., 2014). The success of CNNs has triggered research on how objects can be localized more precisely in images (Tolias et al., 2015), also using region-of-interest proposal networks (Ren et al., 2015). Networks were then used to detect objects in artworks (Crowley and Zisserman, 2016), establish visual links between paintings (Seguin et al., 2016) and find patterns in art collections by adapting a deep feature to this task (Shen et al., 2019). These works emphasize the community’s interest in using computational approaches for object detection; however, suggested methods have rarely been implemented in publicly accessible systems and thus cannot be used by art historians in practice.    Project description We developed an interface and underlying search engine for object detection based on the workflow and specific requirements imposed by art historical research. Requirements were observed first–hand and formulated by computer scientists and art historians and refer to the handling and functions of retrieval systems. The following aspects were identified as crucial: the interface must be intuitive to use, allow for an interactive experience, is accessible from the outside and provide the possibility to study large and diverse image collections. Systems must be applicable to diverse data, across various media or styles, enable a visual search – this is essential since most images have incomplete, false or missing metadata – and allow to search for entire images and object regions in images. The latter is of relevance to art history because objects provide more information about a depicted person or hint to a specific iconography: the lion as an attribute of Saint Jerome is just one example. The search process should be performed fast, enabling a free exploration of the data.     Figure 2. The figure shows the initial page of the search engine for object retrieval, where all available collections are displayed. To search for regions, the user can select an existing dataset or upload a new one. Image by the Computer Vision Group, Heidelberg University So far, search engines mostly allow a text or entire image search: ArtPI 1., developed by Ahmed Elgammal and team, uses deep learning methods to perform aforementioned tasks and a recognition of style, artist and genre. The Oxford Painting Search 2. by Oxford’s Visual Geometry Group, enables a text, color and structure-based or entire image search, but does not allow to search for image regions. Replica (Seguin, 2018), (Seguin, Replica, 2018) offers a text, entire image and region-based search, however, only in a given dataset of mostly Venetian art 3. While other systems fulfill some requirements, they are only partly sufficient for art historical research. It was our objective to develop a system, which considers all listed requirements, focusing explicitly on object retrieval to assist with a formal and semantic analysis.    Figure 3. The system enables an object search: here a region in a portrait of James Timmins Chance by Joseph Gibbs (c1902) is selected with a bounding box and defined as the search query. Image by the Computer Vision Group, Heidelberg University    Introducing the search engine The search engine and corresponding interface was developed in collaboration between computer scientists and art historians, thus considering technical possibilities and art historical requirements. The final engine offers to search for entire images and regions in large datasets. A learning-based approach is used, where an exhaustive search is performed using CNN features to find identical and most similar regions to a user-defined query. The process solely relies on visual input and can be described as follows: after uploading a new or selecting an existing collection (Fig.2), the user selects an image and marks, for example, an object with a rectangular bounding box (Fig.3). This allows to find images of a specific subject, which requires the presence of certain objects, or to study form developments over time and space. The search process is triggered; underlying algorithms operate on CNN features, which demonstrate enormous potential for processing and analyzing large datasets in an unsupervised manner. In contrast to HoG features (Histogram of oriented gradients), used for retrieval tasks in the past, features extracted with CNNs also contain high level information about semantically abstract concepts (i.e. nose or faces) and encode context information, hence are more suitable for object detection. After the search has terminated,    Figure 4. Retrieval results can be viewed from distance, showing more context information, and in close-up. Images by the Computer Vision Group, Heidelberg University   results are displayed in another window with decreasing similarity. The search engine not only detects identical but also similar regions; finding variances of a motif is relevant, when art historians aim to reconstruct reception processes of a particular object. Other functions add to the usability of the interface: the addition and access of metadata, storage of favorites and alternation between a close-up and distant view of images and retrieved regions (Fig.4). The layout of the interface supports an easy, intuitive navigation through the search process, where each function aims to simplify the workflow for art historians: the simultaneous view of selected favorites, for example, allows for a comparative analysis.    Figure 5. Shows the obtained results, based on the query, which is displayed in the top left corner. Results are arranged with decreasing similarity and show that the system was able to retrieve similar regions to the selected part. Image by the Computer Vision Group, Heidelberg University     Figure 6. Shows retrieval results for a query (shown top left) in a dataset of street art. The engine was able to retrieve similar regions to the selected part. Image by the Computer Vision Group, Heidelberg University   The system has proven its applicability to diverse datasets, such as medieval prints and pre-modern paintings, addressing different research questions. How conventional is the representation of specific objects? (Fig.5) shows that algorithms were able to retrieve the motif of a hand holding a letter in a challenging dataset of pre-modern paintings. Results indicate a great conventionality, mostly showing portraits of seated men, holding a letter in the right hand, while the left is put loosely on an armrest. Variations are shown in the second and third row, emphasizing that the system also detects variances of motifs; retrievals one and three of row two disregard the pose, the former also displaying a different subject matter. How popular are hats in street artworks? (Fig. 6) shows search results for the query ‘hat’ obtained from a dataset of street art: images highlight that Brazilian street artists OsGemeos often use hats in different shapes and colors for their yellow figures. Eventually, the tool enables a quantitative and qualitative analysis of the data: one might study the formal development of an object over time in a large dataset or fine-grained similarities between objects during a limited time period. Since computer technologies allow to study large image collections in a short amount of time, scholars can explore image sets first and formulate their research questions after they have assessed the structure and content of the data. This is not possible with traditional methods because it is too laborious. So far, a dataset for content-based retrieval in artworks does not exist; therefore we collected a dataset of 1101 historical paintings consisting of various media (i.e. oil, ink, drawing). We compare retrieval results obtained by our and a HoG-based model. Quantitative results are provided in Table 7 and a qualitative retrieval example is presented in Figure 8. Besides introducing the retrieval system, the paper includes (critical) remarks on existing tools and possible issues, which arise when working with art data; some of which have been mentioned, such as only allowing for a text or entire image search. Systems are then often challenged by unknown, often pre-modern object categories, such as medieval clothing, buildings, swords etc., because most networks are trained on ImageNet, a database which was collected without artistic consideration containing only modern object categories. Also deformations of objects and visible brushstrokes, due to the respective style, further challenge algorithms. (Fig.9) illustrates a failure case: the abstract style and the use of ImageNet features, which were used to retrieve a hand in a dataset of modern, abstract portraits, lower the performance of algorithms. Additional issues arise from missing or incomplete metadata or bad-quality reproductions.    Table 7. A comparison of precision accuracy for top k retrievals using our and a HoG-based model. We calculated the mean precision of 8 queries from different object categories (i.e. praying hands, cross or grape)     Conclusion The paper presents a system for object retrieval to analyze large image collections, thus assisting with art historical research. It enables a quantitative and qualitative evaluation, supports a form and semantic analysis, allows to study reception processes and artistic relations on large scale. The paper lists requirements for search engines, which were formulated by art historians and computer scientists, and illustrates how these were implemented. Eventually, we provide search examples and results and point to possible challenges when working with art images. The retrieval system is available from the outside: users do not need to install it but can access it online.     Figure 8. Qualitative example with our search engine. We show top 12 retrievals for one query from four annotated categories. Notice that our system was able to retrieve objects correctly (in blue). The search was performed in the dataset consisting of 1101 historical paintings. Image by the Computer Vision Group, Heidelberg University    Figure 9. Shows an example of an erroneous retrieval performance; the abstract style and ImageNet features lower the performance of retrieval systems. Image by the Computer Vision Group, Heidelberg University   Notes 1. Link to the search engine ArtPI, developed by Ahmed Elgammal and team, https://www.artpi.co/ (accessed 23 October 2018) 2. Link to the Oxford Painting Search by Oxford’s Visual Geometry Group, http://zeus.robots.ox.ac.uk/artsearch/ (accessed 20 November 2018)  3. Link to the Replica Search Engine by the École Polytechnique Fédérale de Lausanne, https://diamond.timemachine.eu/ (accessed 12 March 2019)   ",
        "article_title": "Finding Visual Patterns in Artworks: An Interactive Search Engine to Detect Objects in Artistic Images",
        "authors": [
            {
                "given": "Sabine",
                "family": "Lang",
                "affiliation": [
                    {
                        "original_name": "Heidelberg Collaboratory for Image Processing, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Nikolai",
                "family": "Ufer",
                "affiliation": [
                    {
                        "original_name": "Heidelberg Collaboratory for Image Processing, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Björn",
                "family": "Ommer",
                "affiliation": [
                    {
                        "original_name": "Heidelberg Collaboratory for Image Processing, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "art history and design studies",
            "user experience design",
            "semantic analysis",
            "content analysis",
            "interface",
            "gamification",
            "theory and methodology)",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Nowadays, as the use of digital data for research in Humanities has become the norm, researchers are dealing with a huge amount of data. As a consequence, the risk of data loss is increasing. Another difficulty is to provide full access to this flood of data to users often located in distant areas. These problems can no longer be addressed individually by researchers or even at a laboratory level: it is therefore necessary to use a technical infrastructure with specific skills to provide stable preservation services.  Huma-Num   https://www.huma-num.fr/about-us , the French national infrastructure dedicated to Digital Humanities, was looking for a way to address these challenges. The main goal was to deploy a technology that would be readily usable and transparent for average users. Scalability was mandatory considering the rapid evolution of the mass of data, and the system should be, ideally, distributed to ensure better security.  Besides these purely technological requirements, we also had some political and organizational concerns. We wanted to delegate the close relationship with users and local administration to an existing robust network of regional centres. By doing so, we expected a better appropriation of the proposed solution and also an enhanced capacity to respond to their specific needs. This paper will present the implementation of a preservation system in France, branded “Huma-Num-Box”, which aims to address all the above-mentioned goals. Then, we will give some feedback about this experiment and actions for the near future.  A technical choice We all know that researchers do not really take preservation into account during the research data life cycle, especially at the early stage, but are more accustomed to making copies on a local device. Accordingly, the first goal was to provide a device as simple to use as a local hard drive. Moreover, we operate in a classical server landscape and we need to be able to access these data on servers using different technologies. We had substantial experience with “IRods software   https://irods.org/ ” which was very efficient but not really user-friendly to say the least and not totally tailored to some servers’ technologies.  After some research, we decided to go for “Active circle   https://www.oodrive.com/products/save/active-circle-storage-archiving-solution/ ”, a software edited by a French company. Here are some reasons why we made this choice:  - It uses standard hardware which can be recycled and new hardware can be added with no re-replication - The file system is natively distributed and provides all the protocols we desired (CIFS, NFS, FTP) for users and servers - You can delegate user management to each node relying on a classical LDAP system - You can mount a share as local and you can share data between nodes - For each set of data, you can easily decide on the policy you want to apply (versioning, numbers and localization of the copies etc.) - All the steps of the data life cycle are integrated in a single tool (storage, replication, daily checks of integrity, hash verification etc.) - We knew that the support was very reactive. This software seems to be perfectly adequate and even offers more than we expected but the choice of a commercial software was not an easy one. One downside is the cost: the cost of the licence is far from being marginal and is related to the data size.       A Human mesh Huma-Num provides services at a national level, but relies on a network of 23 regional centres, called “Réseau des MSH (Maisons des Sciences de l’Homme)”   http://www.msh-reseau.fr/les-msh , to pass on information about its services and in return get some feedback. This network has been around for 20 years and each centre incubates research projects and provides local services.  It therefore seemed logical to set up our nodes in some MSH. Each node is associated with a technical correspondent who manages local accounts and shares under the supervision of Huma-Num. His/her role is also to ensure links with local system administrators and to perform the administrative tasks.   Feedback and the near future After two years, we have a mesh of nine nodes geographically distributed all over France used by 500 users with 100 data shares. We now host around 500 TBs of data, which was quite unexpected so we were forced to expand the system. We were able to save endangered archaeological data located in remote “Ecoles Françaises à l’Etranger” (mostly Greece and Egypt). We also discovered a set of data (60 TBs) of very important contemporary historical archives which did not have a single backup because of the lack of local resources. So, we can say that it’s a success. A nice side effect is that we built a logical network above the national network   https://www.renater.fr/?lang=en  to connect our nodes: this “private network for SSH” is ready to be used for future services. We now consider that this service is mature enough to make it available at a European level via the ERIC DARIAH   https://www.dariah.eu/  infrastructure.  However, the use of the mesh is very uneven. Some nodes are quite empty, and we decided to use them as backups for other nodes. This means that there is probably more work to do to convince users of the benefits of using it. In order to address this, we organize meetings, called “Huma-Num-Bar”, to inform communities about our services: these meetings are broadcasted and archived. We also do a “MSH Tour” to interact directly with potential users. The purpose of this paper is to demonstrate that providing a technology alone is useless: the key to success is user uptake. You absolutely need to rely on a network of expertise to explain the project and to make it work on a daily basis. We also learned that it takes time: from installing an “intrusive” machine inside an existing network until its use by researchers and engineers, every step comes with different difficulties. The main one is the real complexity of coordinating such different categories of actors involved in this project.  ",
        "article_title": "A Techno-Human Mesh for Humanities in France: Dealing with preservation complexity",
        "authors": [
            {
                "given": "Nicolas",
                "family": "Larrousse",
                "affiliation": [
                    {
                        "original_name": "Huma-Num CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            },
            {
                "given": "Joel",
                "family": "Marchand",
                "affiliation": [
                    {
                        "original_name": "Huma-Num CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "digital archives and digital libraries",
            "information architecture and usability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Manuscripts in a digital necropolis This story really began during the Middle Ages, with the creation of manuscripts by copyist monks   Un manuscrit naturellement (Foreword from The Name of the Rose / Umberto Eco 1980)  .  In 1930, Félix Grat a French archivist paleographer, experimented with a sophisticated camera to create microfilms of manuscripts in order to make them widely available and facilitate their study. This resulted in the creation of IRHT   Institut de Recherche et d’Histoire des Textes. See  https://www.irht.cnrs.fr/?q=en  , an institute devoted to fundamental research mostly on medieval manuscripts.  In 1979, an agreement was signed with the Ministry of Culture and IRHT to digitize all the manuscripts stored in French public libraries. This corpus is among the largest digitized medieval sources: the work is still in progress! The “original digital” copy was stored on Huma-Num’s   Huma-Num is the French national infrastructure for humanities which provides mostly digital services. See http://www.huma-num.fr/about-us   infrastructure: it was made of files for the manuscript pages encoded in TIFF format representing a huge volume of data, around 40 TBs distributed in 2 million files.  The fantasy of digital immortality is widely shared, but in reality, digital resources are highly fragile. Even if we are able to store them safely in a readable format, they prove to be totally unusable if we don’t provide related information to understand their content and their organization. In short, over many years, we have built a very safe digital necropolis progressively covered by layers of digital sand rather than a clean organized library.   The context As the original material of this set of data is clearly part of French cultural heritage, it is important to conserve both the manuscripts themselves, some of which are no longer physically available for consultation due to their poor condition, and the scientific work already done on them. It was becoming a matter of urgency to take action as the memory of the project began progressively to disappear mostly because of the numerous changes in human resources. We have an institution in France, the CINES   Centre Informatique National de l'Enseignement Supérieur. See http://www.cines.fr  , dedicated to the long-term preservation of research data. But the cost of preservation in our case was too high. In the meantime, the French National Library had successfully converted part of its resources from TIFF format into JPEG2000, thereby reducing the data to one-third of the original size, and prices at CINES had fallen dramatically.  It now became financially reasonable to consider the preservation with the CINES using the JPEG2000 format. We therefore decided to begin the preservation project.   Dealing with the data abundance and imperfection The first need was to sort and organize the huge number of files: deleting files is a serious decision to take. We made a copy of the corpus on the new storage system, and began to clean the data. We kept track of every single step in order to be able to go back over all the changes. Eventually, we succeeded in getting rid of one million files, mainly technical files and redundant images. The file tree also required some transformations as it was based on library names, and of course some of them had changed over this long stretch of time. We also needed to solve traditional encoding problems and special characters in both file and directory names. The analysis of the technical metadata also showed that some files were missing. A further check showed that some other files were empty or corrupted. All these files were manually checked, and some were regenerated from a copy. The next step was to transform all the files into JPG2000 format. The goal was to ensure that the transformation was technically correct but also that the image was still human-readable and of good quality. The result of this test workflow on a small sample of pictures showed that different TIFF encodings caused many errors and that it was again necessary to carry out a manual check on some files. It took us no less than two years just to do this part of the work.   Documenting data Then, it was time to retrieve the corresponding metadata from various technical and scientific databases. During this process, we discovered that for some manuscripts, the identification number was not correct: it was again necessary to re-adjust the file tree. To encode metadata, we chose a mix of different standards encapsulated in METS format to describe all the metadata available: TEI for scientific metadata and XMP for technical stuff. Lastly, we had to abide by French law on archives, which complicated matters even further. At last, we were able to create nice packages compliant with the needs of the CINES platform based on the OAIS   Open Archival Information System. See https://fr.wikipedia.org/wiki/Open_Archival_Information_System  model.    What we learned  To achieve this project, it was necessary to assemble a team of people with very different skills and backgrounds: it was not easy, to say the least, to make this team operate smoothly! We had on board Huma-Num’s system administrator and also people from IRHT, the database experts to take care of all the relevant metadata and the manuscript photographer who also happened to be the living memory of the project, and last but not least some archivists. It’s impossible do this kind of work if you don’t have access to a proper infrastructure: we had around 80 TBs to deal with, and we also needed computing power to proceed with the format migration. You can’t use the same approach with millions of files as you do with a standard corpus. The person who was the “memory of the project” was the key whenever decisions needed to be taken: this is due to the complexity of dealing with material created over a long period of time (nearly 40 years) by different persons.  ",
        "article_title": "“Un Manuscrit Naturellement ” Rescuing a library buried in digital sand",
        "authors": [
            {
                "given": "Nicolas",
                "family": "Larrousse",
                "affiliation": [
                    {
                        "original_name": "CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            },
            {
                "given": "Christophe",
                "family": "Jacobs",
                "affiliation": [
                    {
                        "original_name": "Agence Limonade & Co",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Michel",
                "family": "Jacobson",
                "affiliation": [
                    {
                        "original_name": "CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            },
            {
                "given": "Gilles",
                "family": "Kagan",
                "affiliation": [
                    {
                        "original_name": "CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            },
            {
                "given": "Joel",
                "family": "Marchand",
                "affiliation": [
                    {
                        "original_name": "CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            },
            {
                "given": "Cyril",
                "family": "Masset",
                "affiliation": [
                    {
                        "original_name": "CNRS (Centre National de la Recherche Sientifique), France",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "theory and methodology)",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Since it was first printed, the translation of Shakespeare's plays edited by August Wilhelm Schlegel and Ludwig Tieck has been re-edited many times (Reimer, 1832). A major reference in the first half of the 19th century, it is still regarded as a groundbreaking translation and referred to today. While there is little doubt that Schlegel translated the first edited plays, L. Tieck did not work out the edition of the final volumes by himself, but delegated the main translation work to his daughter Dorothea Tieck and Wolf Graf von Baudissin (Baillot, 2008; Paulin, 1998). This paper investigates the contribution of the three actors involved in this joint translation project. Machine Learning methods are used to analyse the plays and translations in order to gain quantitative insights into what may seem a peculiar authorship setting but was quite usual in the context of the 19th century. The method proposed here is hence likely to improve our understanding of co-creation conditions in the 19th century at large. Stylometric investigations of collaborative translations to identify translators has already been analyzed by Rybicki and Heydel (Rybicki and Heydel, 2013), who could show that Burrows’ delta features were able to distinguish between the different translators of novels by Virginia Woolf into Polish. Based on D. Tieck's statement of the repartition of the plays we start with the setting shown in Figure 1 (Uechtritz, Erinnerungen. Sybel, 1884). Since the manuscript of the raw translation is now lost, the sole material this paper can base its analysis on is the Shakespeare edition and the first German edition. We have no material traces allowing to easily discriminate between what D. Tieck translated, what W. Baudissin translated, and what L. Tieck corrected in the translations. We investigate two questions: firstly, the goal consists in defining the roles and tasks of the three translation partners, especially for scenes where D. Tieck and W. Baudissin collaborated. The second point of interest is to shed light on the cooperation mode between father and daughter Tieck (respective contributions and intervention scope, collaboration issues). In contrast to authorship attribution, translators are aiming at preserving the style of the original text – the traces of the translators should therefore be even harder to identify. This paper presents a novel approach to use methods such as Burrows’ delta in the  multilingual context, to compare translation styles and attribute translators.    Plays were written by Shakespeare and were translated by either D. Tieck or W. Baudissin. In some plays they collaborated. All translation drafts were then discussed in common, including L. Tieck.    Method The first two experiments deal with the question of the individual translation properties of D. Tieck and W. Baudissin, while the third experiment assesses the question of L. Tieck's contribution. The data layout and the analysis steps of all experiments are shown in Figure 2. The English corpus is retrieved from  First Folio (Shakespeare, 1623), for the German corpus, TextGrid (TextGrid, 2018) was used. Throughout the experiments, spacy (Honnibal and Montani, 2017) for preprocessing and pyphen for syllable counts are used.  In the first experiment, solely based on the German material, translation-stylistic characteristics are to be found that discriminate the translator. In addition to Nearest Neighbors on Burrows’ delta (Burrows, 2002; Argamon, 2008) that was used by Rybicki (Rybicki and Heydel, 2013), Bag-of-N-Gram features and also pre-trained word vectors using the Fasttext model (Grave et al., 2018) were used and classified by a Support Vector Machine with RBF kernels (Cortes and Vapnik, 1995; Müller et al., 2001). Cross validation was used to find good hyper parameters using sk-learn (Pedregosa et al., 2011). In the second experiment, we use the trained classifiers of Experiment 1 on the collaborative works of D. Tieck and W. Baudissin. We compute the predicted class of each scene individually and try to examine who the major translator of each part of the translation was. This explorative experiment enables us to concentrate on scenes for which the classifiers tend to agree, which we then manually evaluate. In Experiment 3, cross-language features are compared with respect to its translator. As shown in Figure 2, the first step for analysing the translation is to map the corresponding scenes, to be able to identify deviations on scene level. During the translation process, the scene boundaries were not always preserved and in order to compare intervals of the same contents, an automatic mapping of scenes is performed. Afterwards, two different features on scene level, namely the richness (a) and the number of syllables per line (b), and Burrows’ delta (c) on play level are compared.   Data layout for all three experiment settings. The first experiment evaluates the possibility of classifying translators based on textual features of the translation. Experiment 2 explores the unknown parts of the corpus with the trained classifiers of Experiment 1. Experiment 3 parallelizes the corpus of the English and the German version and investigates the influence of each of the collaborators.    Results  Experiment 1: Classify translator scenes in validation set As shown in Table 1, the individual classifiers on scene level show decent performance. Burrow's delta, however, does not show convincing results. For further improvement, we combined the classifiers by filtering scenes for which all scene-classifiers agree. This results in a smaller test set (57 scenes) but also in a dramatic performance boost. For this subset of the test set, our combined classifier is on average performing with a precision and recall of ≈.93. Overall, the classifiers perform better in identifying scenes by W. Baudissin.   Table 1: Scores on held-out test set for various features and groupings. For classification of N-Gram features and Word Vectors, an SVM with RBF Kernel has been used. The Support row denotes the number of scenes in the respective class. Parameters have been optimized using grid search and 5-fold cross validation. For Burrows’ delta, a Nearest Neighbors Classifier has been used. The optimal number of features for the delta has been cross validated.   Method  Burrows’ Delta Word N-Grams Char N-Grams Word Vectors Combined Classifiers   Grouping  Play Scene Scene Scene Scene   D. Tieck F1 .5000 0.6216 0.6486 0.7952 0.8947    Precision .5000 0.6765 0.7059 0.7674 0.9444    Recall .5000 0.5750 0.6000  0.8250 0.8500    Support 2 40 40 40 20   W. Baudissin F1 0.6667 0.7705 0.7869 0.8496 0.9474    Precision 0.6667 0.7344 0.7500  0.8727 0.9231    Recall 0.6667 0.8103 0.8276 0.8276 0.9730    Support 3 58 58 58 37   Weighted average F1 0.6000 0.7097 0.7305  0.8274 0.9289    Precision 0.6000  0.7105 0.7320 0.8298 0.9306    Recall 0.6000 0.7143 0.7347 0.8265 0.9298    Support 5 98 98 98 57     Experiment 2: Classify translator scenes in the collaboration set In Figure 3, the translator attribution for the collaborative scenes are shown. Additionally, we exploit the finding of Experiment 1 that our classifiers performance is boosted when they are combined. In  Viel Lärmen um nichts ( Much adoe about Nothing), fourth act, first scene the highest agreement for D. Tieck, in  Der Widerspenstigen Zähmung ( The Taming of the Shrew) first act, second scene the highest agreement for Baudissin is observed. As it turns out, the two scenes are exceptionally long scenes with 302 and 264 speeches respectively, although the mean number of speeches per scene over the whole German corpus is only ≈118.7. The length of the scene may give the classifiers more features to distinguish the translators.  The scene from  The Taming of the Shrew alternates between Verses and Prose which may have given the translator the chance to underline their characteristic style. The scene from  Much adoe about Nothing has a much more coherent rhythm which possibly fits D. Tieck's translation style better.    This figure shows the average score of all scene-level classifiers of Experiment 1 to attribute each scene to D. Tieck or W. Baudissin for the two plays in which they collaborated.    Experiment 3: Identify Contribution of Ludwig Tieck In Figure 4, the results of the cross-language comparison are shown. Points in all panels that are close to the diagonal do not deviate across language. The richness (a) of the scenes stay very close to the diagonal, however the majority of points is slightly below the diagonal. The original is slightly “richer” in the sense of our measurement than the translation, but there is no difference across translators. The median syllables per line (b) of the translation deviates quite significantly in that the German version often uses more syllables per line than the English version. D. Tieck stated in her letter that she also translated Sonnets even in a play that was otherwise translated by W. Baudissin. Because of this statement we originally expected D. Tieck to follow the number of syllables of the original more strictly. This expectation is also in line with the findings of Experiment 2 where most classifiers agree on D. Tieck as the translator in a scene with a coherent rhythm. However, the findings of (b) cannot verify this hypothesis, because the deviation exists for both translators. In (c), the points visualize Burrow's delta between the two plays in English, the vertical position is the Burrow's delta of the respective pair in German.  Each data point for which both plays are translated by the same person is color-coded accordingly (grey otherwise). Interestingly, the green points are almost exclusively below the diagonal, with only a few exceptions for plays that already exhibit a small delta in the English version. This indicates translations by D. Tieck move closer to each other and thus may incorporate a more consistent style.   Three different features that compare original texts and their translations across languages. For each panel, the horizontal axis corresponds to the original version in English, the vertical axis corresponds to the German translation. The richness feature (a) shows little deviation in both languages. The Syllables per line feature (b) shows deviation in the translation for both translators and the Burrow's feature (c) shows deviation especially for one translator: D. Tieck (Green). For (b) gaussian noise (with std. of .2) was added to the points to visualize overlapping points. Also, in (b), a few outliers are not visualized. The points in (c) are grey if both plays were not translated by the same person.     Conclusion We proposed an ensemble of translator attribution methods that result in a very high performance on scenes where they agree (Experiment 1). We show a significant improvement over state-of-the-art methods for translator attribution. This combination of classifiers is used to suggest translators for scenes where the true translator is unknown.  A close reading of the scenes revealed distinct characteristics that could explain the decision of the classifiers (Experiment 2). We thus argue that this method likely found scenes where the majority of translation work can be attributed to the proposed translator. A novel approach of comparing the material in the source language and the translations yield the result that D. Tieck has a more distinct style in her translations (Experiment 3, c). With regard to the daughter-father relationship this can be seen as a literary independence from her father. Also, it could be observed that there is a translation system on which the three collaborators agree (Experiment 3, a and b). In that, we identified candidate features that could signal a contribution of L. Tieck.  For further analysis we plan to include original plays by L. Tieck in order to identify distinct characteristics that further narrow down his contribution to the translation. We also plan to include additional cross-language features that characterize a distinct style of W. Baudissin.  ",
        "article_title": "Attributions Of Early German Shakespeare Translations",
        "authors": [
            {
                "given": "David",
                "family": "Lassner",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Berlin",
                        "normalized_name": "Technical University of Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03v4gjf40",
                            "GRID": "grid.6734.6"
                        }
                    }
                ]
            },
            {
                "given": "Anne",
                "family": "Baillot",
                "affiliation": [
                    {
                        "original_name": "Le Mans Université",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Julius",
                "family": "Coburger",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Berlin",
                        "normalized_name": "Technical University of Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03v4gjf40",
                            "GRID": "grid.6734.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "german studies",
            "stylistics and stylometry",
            "authorship attribution / authority",
            "translation studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Abstract  This paper explores the cultural representation of migration and the biopolitics of contagion and disease represented in a digital corpus of literary fiction from the British Library. This work is part of a project examining the shifting representation of migration, ethnicity and contagion in cultural memory. A curated subset of the British Library Digital Corpus was examined using techniques from artificial intelligence and text mining. Concept modelling with neural word embedding revealed complex relational dynamics between societal views of migration, ethnic identity and contagion that question prevailing theories. Thematic lexicons were generated with word embedding to mine the corpus for excerpts of text that capture these conceptual relationships and enable critical analysis. This bridging of digital analysis and close reading sets out a methodology whereby patterns identified in corpora with artificial intelligence techniques may be critically evaluated through close reading of the text.  Keywords: migration, contagion, biopolitics, word embedding, text mining, literary fiction   The complex relationship between societal views of migration, ethnicity and concepts of contagion and disease are explored in this paper through neural word embedding and text mining. This research is part of a project examining the representation of migration, ethnicity and contagion through the analysis of a collection of 45,000 digital texts from the British Library, primarily dating from the late 19th century. In order to explore the cultural representation of migrants, this paper focuses on their representation within literary fiction, which comprises 16,426 texts of the digital collection. Given the largest communities of migrants to Britain during the late 19th century were Irish and Jewish, this paper focuses on their portrayal in relation to prevailing concepts of contagion, disease and migration.   Lexicons of terms associated with the migration and the biopolitics of contagion were generated with neural word embedding. Thematic lexicons are learned associations between terms in the corpus and a set of seed terms corresponding to a concept (Lavelli et al., 2002). The dynamics of the relationship between concepts in the corpus were modelled and explored with t-SNE visualisation and measures of semantic distance. Through modelling how the concepts of migration and disease and contagion, real or imagined, were related within the corpus, patterns emerged that revealed a complex conceptualisation of contagion and the nature of its association with Irish and Jewish migrants. Excerpts of texts capturing the interaction between concepts of migration, ethnicity and contagion were extracted using text mining based on thematic lexicons, developed with word embedding. This bridging of neural word embedding methods with text mining demonstrates how complex conceptual relationships may be identified in text and critically examined through close reading.    Related Research  Migration and the Biopolitics of Contagion  Cultural attitudes towards migration have traditionally been associated with fear of contagious disease (Nelkin and Gilman, 1988, Kinealy, 2006). Poverty induced migration from Ireland to Britain during the famine has been cited as generating a fear of transmission of contagious disease (Morash, 2009). The conflation of issues of migration, ethnicity and contagion is evidenced by the fact that tuberculosis was identified as the “Jewish Disease” despite the fact that the mortality rates from the disease in London were lower for Jewish immigrants than their counterparts. Recent research, notably Samuel Kline Cohn ' s work on the history of epidemics, has produced a more complex and nuanced understanding of the relationship between fear of contagion and suspicion of migrants, based on a much broader historical and cultural archive than heretofore (Cohn Jr, 2018). This research addresses this by applying digital methods to support the systematic study of the relationship between concepts of migration, disease and contagion.    Concept Modelling and Text Mining  Machine learning has been used in digital humanities research to generate thematic lexicons for a range of purposes, including detecting language change over time (Hamilton et al., 2016), extracting social networks from literary texts (Wohlgenannt et al., 2016), sentiment analysis (Tang et al., 2014), and semantic annotation (Leavy et al., 2018). In developing domain-specific vocabularies, neural word embedding can be particularly effective (Chanen, 2016). The exploration of topics in text through visualisation of neural word embedding models has been applied in automated text analysis systems (Park et al., 2018). However, challenges have been identified in bridging patterns uncovered through visualisation and semantic similarity analysis with close reading of texts in digital humanities research (Janicke et al., 2015). This paper addresses this issue by using thematic lexicons developed through neural networks to explore the relationships between concepts in text and also as a basis for mining excerpts of text.     Methods  In this work, thematic lexicons are developed using neural word embeddings, and then visualized using the t-SNE algorithm (Maaten and Hinton, 2008). The word embedding algorithm used here is the popular word2Vec approach (Mikolov et al., 2013), which generates real-valued, low-dimensional representations of words based on lexical co-occurrences, as identified by sliding a window over documents in a corpus. Lexicons representing key thematic strands in the dynamics of bio-politics and migration were developed by using word embeddings to uncover terms that are semantically related to an initial set of seed terms (Table 1). The resulting expanded thematic lexicons were used to model concepts within the corpus, and also to uncover excerpts that capture relationships between key concepts in the text.    The dynamics of relationships between thematic lexicons and their positioning within the entire corpus were modelled using a t-SNE visualisation approach. The t-SNE method allows the visualisation of high-dimensional data, such as word embedding models. The conceptual structure was explored through an interactive embedding projector in TensorFlow platform (Abadi et al., 2015). Observed patterns proposed the nature of the relationship between ethnicity, migration and concepts relating to contagion. These patterns were evaluated through the analysis of the cosine similarity of word vectors in the embedding to quantify the semantic distance between concepts (see Fig. 3).     Table  : Seed terms for thematic lexicons    Texts relating to the key themes listed in Table 1 were uncovered based on the use of the lexicons described above. Specifically, excerpts of texts were extracted if they contained one or more words from a given lexicon. A sample of top words from the lexicon representing the concept of contagion is provided in Table 2.     Table  : Sample of top terms from thematic lexicon related to contagion     Findings and Conclusions  The findings of this research uncovered a dynamic between concepts of race and migration that challenge prevailing theories about the attribution of threats of contagion to Jewish and Irish immigrants. Contrary to expectations, analysis of the corpus with neural word embedding did not support a link between race and concepts pertaining to contagion. While Irish and to a lesser extent, Jewish communities were described as themselves being disease, a fear of transmission of disease to British people was not systematically evident in the corpus. Religion and new political ideologies, rather than ethnicity itself, show a stronger association with a threat of contagion and ultimately disease.     Figure : Visualisations of conceptual model of migration and biopolitics    A striking pattern evident within the clustering of concepts in the t-SNE visualisation was an absence of proximity between the lexicon of disease and those representing either Irish or Jewish identity (Fig. 1). However, the term “exterminate”, a term used in relation to the extermination of disease, was aligned with elements of the Irish lexicon. Cosine similarity analysis demonstrated a stronger association of “extermination” with ethnic identity, and particularly the religious aspect of that identity, than with disease itself (Fig. 2a). Excerpts of the texts which were identified as containing this term alongside elements from the Irish or Jewish thematic lexicons, suggested a conceptualization of some migrants as disease to be exterminated, rather than presenting a threat of contagion (Table. 4).     Figure : Semantic distance (similarity) of concepts in text    While most lexicons appeared clustered within the model, the concept of contagion was more dispersed within the t-SNE visualisation. Contrary to expectations, an overall close relation between concepts relating to contagion and Jewish or Irish identities was not evident (Fig. 3). However, the aspect of Irish identity pertaining to religion was aligned with elements of the concept of contagion. Cosine similarity analysis of contagion and Catholicism, the nearest neighbours from the Jewish identity, along with the key themes of poverty, migration and immorality, was used to evaluate the semantic distance between concepts. This indicated a stronger association between Catholicism and the concept of contagion than the Irish as an ethnic community or migrants in general (Fig. 2b). Similarly, the aspects of Jewish identity that pertained most strongly to religion were associated more closely with concepts of contagion, suggesting that religion rather than ethnic identity itself may have had a stronger association with concepts of contagion.      Table  : Excerpts capturing conceptual relationships between ethnic identity, religion, and contagion      Figure : Heatmaps indicating cosine similarities between thematic lexicons and ethnic identities   Excerpts of text capturing concepts associated with contagion along with Irish and Jewish identity were extracted to critically evaluate the patterns identified in the word embedding model. Close reading of these revealed a fear of contagion of religions and political ideology. Tracking back to these excerpts also demonstrates a belief that foreign religion could induce disease (see Table. 4). Future narrative analysis will examine the extent to which opposition to intermarriage with these groups used the imagery of infection.   Conclusion  This paper investigated key themes pertaining to migration and the biopolitics of contagion, and uncovered conceptual relationships and excerpts of texts from a collection literary fiction from the British library that question prevailing theories pertaining to the historical association of perceptions of migrants with fear of contagion. Insights regarding the association of the religious aspects of the ethnic identity of immigrants and concepts of contagion in Britain, particularly in the 19th century, were uncovered using neural word embedding. Critical analysis of these complex conceptual patterns was enabled though mining the corpus based on thematic lexicons derived from the embedding models. In bridging artificial intelligence and text mining approaches in this way, this research merges both digital and traditional forms of humanities research.   ",
        "article_title": "Migration and Biopolitics in Cultural Memory: Conceptual Modelling and Text Mining with Neural Word Embedding",
        "authors": [
            {
                "given": "Susan",
                "family": "Leavy",
                "affiliation": [
                    {
                        "original_name": "University College Dublin, Ireland",
                        "normalized_name": "RCSI & UCD Malaysia Campus",
                        "country": "Malaysia",
                        "identifiers": {
                            "ror": "https://ror.org/0474gs458",
                            "GRID": "grid.417196.c"
                        }
                    }
                ]
            },
            {
                "given": "Derek",
                "family": "Greene",
                "affiliation": [
                    {
                        "original_name": "University College Dublin, Ireland",
                        "normalized_name": "RCSI & UCD Malaysia Campus",
                        "country": "Malaysia",
                        "identifiers": {
                            "ror": "https://ror.org/0474gs458",
                            "GRID": "grid.417196.c"
                        }
                    }
                ]
            },
            {
                "given": "Karen",
                "family": "Wade",
                "affiliation": [
                    {
                        "original_name": "University College Dublin, Ireland",
                        "normalized_name": "RCSI & UCD Malaysia Campus",
                        "country": "Malaysia",
                        "identifiers": {
                            "ror": "https://ror.org/0474gs458",
                            "GRID": "grid.417196.c"
                        }
                    }
                ]
            },
            {
                "given": "Gerardine",
                "family": "Meaney",
                "affiliation": [
                    {
                        "original_name": "University College Dublin, Ireland",
                        "normalized_name": "RCSI & UCD Malaysia Campus",
                        "country": "Malaysia",
                        "identifiers": {
                            "ror": "https://ror.org/0474gs458",
                            "GRID": "grid.417196.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "artificial intelligence and machine learning",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "cultural analytics",
            "cultural studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "    Introduction:   Digital Cultural Ecosystems   The paradigm of ecosystems for digital cultural contents, the so called DCEs (Digital Cultural Ecosystems), can facilitate and foster the process of democratization of knowledge. This process started in the 1990’s with the first applications developed by means of ICT tools applied to Cultural Heritage (Felicori and Guidazzoli, 2003). Open digital frameworks allow the access and the enrichment of cultural digital resources enabling new researches and studies. Moreover, these systems can bring cultural content to different audiences in innovative ways and include citizens in the process of content enrichment (Apollonio et al., 2017) (Paneva-Marinova et al., 2017).  W ith respect to digital Cultural Heritage, citizens should be more than consumers   ViMM Manifesto full version, ViMM project, retrieved on 3 April 2019.   . The audience participation should be improved at an active level, implementing user-oriented perspectives.    The same principle was at the basis of the I-Media-Cities project, with its aim of involving both researchers and the general public, not only as users of the platform, as mere performers of queries, but, instead, as active contributors by tagging and further enriching the digital contents delivered by the platform.      The I-Media-Cities project   The  I-Media-Cities Horizon 2020 project is the initiative of 9 European Film Libraries, 5 research institutions, 2 technological providers and a specialist of digital business models to share, access to and valorise audiovisual (A/V) content about history and identity of European cities. A huge quantity of fictional and non-fictional AV works (from the end of the 19th century onwards)  ha s  been made available  for research and creative purposes, describing cities in all aspects, including physical transformation and social dynamics   (Table n. 1).  The platform, with its dynamic processing pipeline and automatic video analysis tools, enables the enrichment of the moving images meta-data (Fig. 1). By allowing also manual and automatic annotations of the A/V content the platform creates a new digital ecosystem (Caraceni et al., 2017) .       Fig. 1: IMC Movie Processing Pipeline   The innovative elements of the I-Media-Cities (IMC) platform are not limited to conveying the audio/visual contents of nine archives to a single collector and access point. Once collected, the A/V material is elaborated with a series of algorithms that automatically enrich it with a good set of meta-data. Nowadays it is a common activity for people to contribute to the enrichment of web contents with various kinds of data, in particular through social media. However, this manual activity is an extremely laborious process; especially when it regards, as in the case of the IMC project, the annotation of videos at shot or single frame level. Automatic meta-data, on the other hand, while not yet achieving the accuracy of a manual enrichment, is able to add a great deal of information. To this end, IMC has integrated a set of tools provided by Fraunhofer for the automatic analysis of video and images. These algorithms provide information on the quality of video, identify camera movements, such as zoom or pan, segment videos in various scenes (shot), and identify and recognize objects and various elements (object detection) that appear in the videos of IMC platform. The object detection tool, thanks to deep learning techniques, is able to identify the presence, for example, of people or means of transport or architectural or urban elements, differentiating between man or woman, adult or child; points out trams or carriages or bicycles; a square or a fountain, and so on (Fig. 2).       Figure 2: Visualisation of the object detection activity. Green detects objects with a confidence higher than 80%, yellow between 50 and 80% and red lower than 50%. This is a particularly interesting and complex procedure, which has to manage the analysis of sometimes scarred films digitized several years ago and dating back up to the end of the 19th century.        Figure 3: IMC platform: video content item page displaying manual and automatic meta-data. Tags are colour coded (automatic, manual from researchers, manual from general users).   Since the automatic tools still retain a certain margin of error, a sophisticated online “frame by frame” analysis tool has been set up for the manual correction of inaccuracies on the shot detection, particularly useful on older videos (Fig. 3).         Figure 4: IMC platform: geo-localised search results   The result of this process is the creation of data that are transformed into semantic data, directly understandable by people as well as by computers. The IMC meta-data model summarizes the descriptive meta-data from the archives, and then uploaded directly with the audiovisual materials, based primarily on the  CEN EN19507 standard; and the data generated once the material has been loaded onto the platform. As mentioned, these meta-data can be the result of an automatic or manual enrichment process (Baraldi et al., 2016), modeled using the W3C Web annotation Data Model (WADM).   In I-Media-Cities a semantic search engine was developed for processing the requests coming from the user interface and, by analysing all the available meta-data, it provides researchers and citizens with the search results.  The meta-data system includes different types of annotation associated with different details:  original meta-data of the video or image, providing information at the level of the whole content; automatic annotations at the level of a single segment of the video (scene or shot); manual annotations at the level of the single segment of the video (scene or shot), tags and geotag (geolocalised annotation) with which the content has been enriched.         Table n.1 - Contents delivered to IMC platform up to February 2019   In particular, the automatic and manual annotations, being linked to the single segment of the video (scene or shot), allow a collection of scenes belonging to different videos where the same element can be traced: for example the tramways and traffic of the early 1900s (Fig. 5).         Figure 5: IMC platform: search by term  The automatic annotations guarantee that all the audiovisual material is analysed and annotated homogeneously, at least on a common set of aspects, while the manual annotations provide an enrichment of detailed information, also in the form of textual notes, bibliographic references, links to other similar material, both internal and external to I-Media-Cities. At the moment, there are 59,457 manual annotations (with 1,708 different terms); 422,123 automatic annotations (with 78 different terms); 6,411 geotags for 1,091 different places.  The performance of the IMC platform rely upon the High Performance Computing resources of Cineca, which allow the use of the most suitable hardware architectures for the different analysis algorithms applied.  In order to adequately present the contents of the research, particular attention is paid to the development of the visual interface, which also allows to perform searches starting directly from the map on which all the AV contents are geolocated, delimiting the selection through a time bar (Fig. 4). In relation to these aspects, a user experience evaluation process is applied, in accordance with the Agile methodology, which pervades the project (Cohen et al., 2004). The Agile methodology implies an iterative approach, with several phases of development: check, correction, check and development (Tab. 2).       Table 2: Agile methodology flow chart  During the first phase  User and System requirements were gathered by breaking the project and its requirements down into little parts of user functionalities, called  user stories and  use cases, and prioritizing them. Each Film Heritage Institution (FHI) and research partner provided a list of vision items and user stories that were categorised and grouped in  use cases by the Coordinator of the project and Cineca staff. During this process, the technical partners detailed each  use case with more technical information, such as technological enablers.  A series of training sessions and a set of “living document” were curated in order to enable this Agile process among researchers and users and make them aware of what was already available and feasible. “Living documents” are continuously updated, following both the development of the project and the state of the art of the different areas on which the project insists. The final phase of the project foresees the opening of the platform to the wider public, and not only to researchers. The enrichment of contents can therefore take advantage of crowd-sourcing, obviously subject to a check by researchers and archives on all the information added   Among the metadata sent by the archives and associated to each content there is a “right status” value, selected from the following list: - In copyright  - EU Orphan Work  - In copyright - Educational use permitted  - In copyright - Non-commercial use permitted  - Public Domain  - No Copyright - Contractual Restrictions  - No Copyright - Non-Commercial Use Only  - No Copyright - Other Known Legal Restrictions  - No Copyright - United States  - Copyright Undetermined  All contents, whatever “right status” they have, can be visualised by everybody, even by the general users group. The archives, depending on the “right status”, can upload low resolution versions with watermarking. . The results of the research can be presented by archives and researchers as virtual exhibitions, set up in a 3D Web environment within the platform itself.     Conclusions   I-Media-Cities meets two main requirements of Cultural Heritage sector: gathering information of different types and from different sources and enriching the original information with annotations, automatic or manual ones. Each multimedia content, image or video, is inserted in a process in which the asset is associated with the original  meta-data  belonging to the archive and, then, it is enriched with automatic  meta-data  extracted by different algorithms and further enhanced by manual annotations. In the next phase of the project a particular attention will be devoted in improving the general public user experience (Krug, 2014). The citizens will be able to visualize the public domain content of the archives, search, browse, annotate A/V content and share their discoveries.         ",
        "article_title": " I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure Trove Of Audio Visual Assets  ",
        "authors": [
            {
                "given": "Gabriella",
                "family": "Scipione",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Antonella",
                "family": "Guidazzoli",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Silvano",
                "family": "Imboden",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Giuseppe",
                "family": "Trotta",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Margherita",
                "family": "Montanari",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Maria Chiara",
                "family": "Liguori",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Simona",
                "family": "Caraceni",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "linking and annotation",
            "crowdsourcing",
            "English",
            "film and performing arts studies",
            "computer science and informatics",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "    “Il piccolo Masaccio e le Terre Nuove” is a short animated Computer Graphics video explaining the origins and the history of San Giovanni Valdarno, a city of foundation (Cardini, 2009). The video was created for the  “Museo delle Terre Nuove”, which offers an overview of the phenomenon of the New Towns born in Tuscany, Italy and Europe during the Middle Ages. The main character of the video is a young version of the painter Masaccio (Tommaso), born in 1401 in Castel San Giovanni, the earlier name of San Giovanni Valdarno. Tommaso roams the city during a siege and, on top of the tower of Arnolfo’s palace, meets the Vicar, ruler of the town on behalf of the city of Florence, who explains him the rational rules underlying the creation of San Giovanni (Bertocci, 2003); (Bianchini, 2003); (Puma, 2003) (Fig. 4).  In order to communicate to the visitors a unique experience, combining education and entertainment, the video exploitsof a series of very different techniques, such as:  live shots, taken also by drone;  Computer Graphics, with photogrammetric, procedural and geometric 3D modeling, crowd (Fig. 3), cloth and particle simulations and a digital library for the vegetation of the time (Fig. 6); watercolours and 2D drawings, made also with a tablet device (Fig. 1); digital videos taken from Google Earth.        Figure 1 - Watercolours painted for the video Besides  CityEngine, used for the procedural modelling of San Giovanni (Fig. 2), and  PhotoScan, used for some materials and minor details of the buildings, the work was realised in  Blenderin an Open Source pipeline.     Figure 2 - Procedural reconstruction of San Giovanni     Fig 3 - Simulating a crowd of soldiers     Figure 4 - A view of the city  This contribution presents an overview of the solutions adopted in the development of a CG educational video in order to both attract more visitors to the museum and inspire them with the desire to deepen its topics.   Computer Graphics and videos for Museum communication The use of documentaries to present cultural contents in museums is not new. A short video allows a fairly rapid rotation of the spectators and is generally well compatible with pre-existing museum displays. Over time, real-life documentaries have been joined by 2D educational cartoons and, later, by Computer Graphics videos. The latter, however demanding, allows us to reconstruct, re-contextualize, explain and show different physical perspectives and make landscapes and events of the past much better understandable (Forte, 2017); (Ferdani and Pescarin, 2012); (Märker et al., 2012), enabling also the insertion of particular graphic elements into reality, as in the application of the Tate Britain Museum (Shi, 2016). Usually, Computer Graphics is used to show architectural reconstructions (Gabellone et al., 2017). However, the emotional narration is gaining ground, increasingly introducing storytelling (Perry et al., 2017)(Staus and Falk, 2017), also for interactive applications, such as  CrazyTalk, created for the   Thyssen-Bornemitzsa Museum.        Figure 5 - Tommaso on top of Arnolfo’s tower Since they are cheaper, actors are preferred over animated characters. In virtual sets (Pietroni, 2017) or alongside CG integrations, to show landscapes or reconstructions of the past (as in the video at  Roman Army Museum, Vindolanda). Storytelling with 3D animated characters is still limited because of its high costs and challenges (Guidazzoli et al., 2011), even if, thanks to tools such as  Cookie FlexRig it is possible to create them quicker than before (Guidazzoli et al., 2016). As stated by Vosinakis (Vosinakis, 2017), a key requirement for digital characters is credibility, that is more a “coherence of this character within the world it is situated in”, than a high degree of realism in its realisation. Believability involves the character’s behaviour as expressed by gaze, facial expression, gesture and posture, making the creation of a character even more complex and time consuming (Fig. 8). However, the use of characters inside a narration offers a significant contribution for capturing the audience’s attention and entertain it. Characters, atmospheres, dialogues and history, can all transform a passive activity into something more engaging (Gabellone et al., 2016), trigger the visitors’ interest, empathy and imagination, leading them towards a successfully entertaining experience (Roussou, 2008). Vosinakis remembers us to “...focus on the expressive side of the digital characters, even by incorporating non realistic elements, rather than striving for accuracy and realism” (Vosinakis, 2017). However, when working at a communicative product for a museum, it is very important to keep a good level of philological accuracy.  All these elements were central while developing “Il piccolo Masaccio”, implying a great commitment to philologization, for example in choosing and modeling clothes, and, at the same time, the adoption of something peculiar and cartoonish for attracting the viewer, such as the large eyes of Tommaso or its hair, stained with painting (Fig. 5).    Figure 6 - Distribution of the vegetation on the map    Directing a CG educational video “Il piccolo Masaccio” is a peculiar educational animated movie, both for the multiple techniques and for the different visual solutions adopted. The video explains how the “New Towns” of the fourteenth century, such as San Giovanni Valdarno, were born and on which principles and rules of political and architectural design were based. However, “teaching” through an animated video can risk of not reaching the goal because an excess of didacticism reduces its appeal (especially with a young audience). A varied visual poetic was used, therefore, to give more rhythm to the story and avoid a boredom effect. For example, the cold and precise lines of the digital houses were totally replaced by irregular and sweeter lines. Even the colours of the façades of the houses were made softer and warmer, inspired by the same frescoes by Masaccio and his contemporaries, such as Masolino (Spike, 1995). Surfaces and colours are never clean, but dirty, with darker patches to suggest the idea of real walls, scratched sometimes or affected by damp and mould (Fig. 7).    Figure 7 - Creating a credible town of the Renaissance  To give more emphasis to the film and to further soften the 3D model of the city, we resorted to a very blue sky with white clouds, almost wisps of whipped cream, reminiscent of Hayao Miyazaki’s skys (Arnaldi, 2014). Camera movements were also devised in order to be more independent from a classic navigation of a 3D model. In addition to 3D computer graphics, it was deemed appropriate to use 2D graphics (Keech et al., 2017). This choice enabled the explanation of geometric principles in a simpler way. Furthermore, we used hand-made drawings with ink and watercolours to lighten the narration and enrich the film with visual suggestions, which culminate when Tommaso, inspired by the Vicar’s narration, draws in an infinite and imaginary space a series of ideal cities. Live shots were adopted in order to link past and present and to highlight the continuity of history, where everything we now see is the result of what was conceived, designed and built in the past.       Figure 8 - Study of characters’ expressions   Conclusions Creating a 3D world requires a significant amount of time and resources, especially compared to a real-time shooting or, even, a 2D movie, which could lead to similar results in terms of communication. The 9 minute video “Il piccolo Masaccio e le Terre Nuove” required the effort of a team of six person for one full year; some small contributions of high-school students; the help of 2 video-makers, for live footages, and of a professional animator. In the end the repository hosted 15,3 GB of Blender files and textures and 11 GB of high resolution frames. The rendering is in Cycles, the Blender unbiased rendering engine, and for the low resolution version and its work in progress 72,000 core hours were used; 65,000 core hours more for the high-res version. Renderings were launched on the Cineca Blender Render Farm, scripted  ad hoc for this project with the new SLURM scheduler on the supercomputer Galileo  360 Compute nodes (Intel Broadwell), each with 2*18-core Intel Xeon E5-2697 v4 @ 2.30GHz - 128 GB RAM. The consistent effort became in time a co-production, with Cineca supporting the economic possibilities of the Museum. However, Computer Graphics also provided a higher level of philological accuracy, thanks to the multiplicity of solutions available, such as procedural modelling tools or material or vegetation libraries, plus a better support to the director’s creativity. Besides, once the 3D assets have been built, they are reusable in further applications. But, above all, CG and storytelling can be a powerful attractor towards a young audience, used to high quality productions.  The movie was premièred at the museum during the National Families at the Museum Day on October 2018. A questionnaire was submitted to the younger audience and it collected praises from everybody. Asked to draw the favourite character from the video, the kids privileged Tommaso, but also the vicar and Tommaso’s mother got their share of attention. From then on, the repeated submission of questionnaires to the audience will help in fully understand the capacity of “Il piccolo Masaccio e le Terre Nuove” in transmitting the desired contents and attracting more visitors to the museum.  ",
        "article_title": " More Than Just CG: Storytelling And Mixed Techniques In An Educational Short Movie  ",
        "authors": [
            {
                "given": "Antonella",
                "family": "Guidazzoli",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Giovanni",
                "family": "Bellavia",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Daniele",
                "family": "De Luca",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Francesca",
                "family": "Delli Ponti",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Federica",
                "family": "Farroni",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Maria Chiara",
                "family": "Liguori",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Beatrice",
                "family": "Chiavarini",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            },
            {
                "given": "Silvano",
                "family": "Imboden",
                "affiliation": [
                    {
                        "original_name": "Cineca, Italy",
                        "normalized_name": "Cineca",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/02f013h18",
                            "GRID": "grid.431603.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-08",
        "keywords": [
            "multimedia",
            "libraries",
            "museums",
            "English",
            "3D/4D modeling",
            "computer science and informatics",
            "GLAM: galleries",
            "archives",
            "modeling",
            "simulation",
            "audio",
            "video",
            "virtual and augmented reality",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The research community agrees on the influences and importance of the Tang   Tang is a Chinese dynasty that existed between 618CE and 907CE.  poems for the studies of Chinese literature and linguistics (e.g., Lo, 2005; Tharsen, 2015; Mazanec, 2017; Lee, Kong, and Luo, 2018), and we have seen discussions about the Tang poems in the community of digital humanities (e.g., Lee et al., 2018; Liu, Zhang, Geng, Lai, and Wang, 2017). Chinese is a language that does not place clear markers, e.g., spaces in alphabetic languages, between words in the texts. To study the Chinese contents with digital tools, words are more meaningful units than characters, although using characters is also possible. Unfortunately, it remains difficult to algorithmically segment words in classical Chinese (Huang and Wu, 2018).  Chinese word segmentation ( CWS) is a key step for computational processing of Chinese texts. CWS for modern vernacular Chinese has achieved good results in international competitions and in practical applications (Zhou, Yu, Zhang, Huang, and Chen, 2017). It is relatively easier to obtain data to train software for CWS of modern Chinese, but there are no known good sources of labelled data for CWS of classical Chinese yet. In fact, even segmenting classical Chinese texts into sentences is an ongoing research topic (Liu and Chang, 2018).  We report results of our attempt to segment words in the Complete Tang Poems ( CTP). CTP is a representative and arguably the most famous collection of Tang Poems (Peng et al., 2003). To study CWS for CTP, we must acquire poems that are segmented by domain experts. At this moment, we use only regulated pentametric octaves ( RPO) and regulated heptametric octaves ( RHO) of seven prominent poets.   Notes: (1) Regulated pentametric octaves and regulated heptametric octaves mean五言律詩and七言律詩, respectively. (2) An octave poem has eight lines. (3) The poets are Yuan Zhen(元稹), Li Shangyin(李商隱), Li Bai(李白), Du Mu(杜牧), Du Fu(杜甫), Bai Jyuyi(白居易), Wei Yingwu(韋應物).  The segmentation task was achieved by five assistants who have university-level domain knowledge in Chinese poetry. We have to ignore two poems because they include very rare characters that cannot be handled in our programming environment. We also dropped nine poems when our annotators believed that the poems could be segmented in multiple ways or when our annotators were not sure how to segment them. At this moment, we have 2433 segmented regulated octave ( RO) poems.   1427 regulated pentametric octaves and 1006 regulated heptametric octaves   There is a popular belief, among experienced researchers and readers of classical Chinese poems, about the word boundaries in the RPO and RHO poems (Lo, 2005). A sentence in an RPO poem contains five characters. They can be segmented into two patterns: 2-2-1 or 2-1-2, where 2-2-1 indicates that a sentence is segmented into a two-character word, a two-character word, and a one-character word. Similarly, a sentence in an RHO poem can be segmented into two patterns: 2-2-2-1 and 2-2-1-2. We have 19464 lines in the 2433 RO poems, and found that 96.5% of the lines followed the aforementioned expectation.   An octave poem has eight lines, so we have 2433×8=19464 lines.  Table 1 shows the details. The most common exceptions are due to place or person names in poems, and, in such cases, we observed 2-3 or 2-2-3 patterns. They represent 1.6% of the lines.     Table 1. Frequencies and percentages of the most frequent patterns in 19464 lines    Weighted Pointwise Mutual Information ( PMI)   The simplest version of a CWS problem is to determine whether we should segment a sequence of three characters, say XYZ, into XY-Z or X-YZ.   Here, an English letter represents a Chinese character. XYZ could represent “依山盡” in “白日依山盡”, which is a line in one of Li Bai’s poems. Our question is whether we should segment XYZ into XY-Z or X-YZ, i.e., should we choose 依山-盡 or依-山盡.   We compute the PMI for the competing candidates, and choose the bigram that has a larger PMI. Namely, we choose XY-Z if PMI(XY) is larger than PMI(YZ). The PMI of a bigram,  AB, is defined below (Manning and Schütze, 1999).     Normally, we use labelled data in machine learning research for training. However, we have only 2433 segmented poems, and will use them to evaluate our methods for segmentation. Hence, we cannot use 2433 poems for training, and have to train PMI values with other poems to segment the lines based on domain knowledge or heuristics.  Consider a line, JKLMNOP, in an RHO poem.   An RHO poem contains seven characters, and, again, we use an English character to represent a Chinese character.  Although we are not sure of the correct segmentation, we can assume that this line may follow either the 2-2-1-2 or 2-2-2-1 pattern as we explained above, and record the occurrences of JK, LM, NO, and P (if 2-2-2-1) or the occurrences of JK, LM, N, and OP (if 2-2-1-2). Since we are not really sure of the correct pattern for a line, we can only assign different weights to JK, LM, N, NO, OP, and P based on certain assumptions. We can then use the weights for unigrams and bigrams to estimate the probability values.   In this running example, we are more confident that we will see the occurrences of JK and LM than the occurrences of NO and OP, so it is reasonable to assign larger weights to JK and LM than to NO and OP. Under the current assumption, KL is unlikely to form a bigram, but we may choose to assign a small weight to its occurrences. This can be done in many different ways, and we will report technical details in DH2019.  We can compute the PMI for bigrams with 38580 CTP poems that contain only five or seven characters in their lines. Since there are less than 7500 distinct characters in CTP, we hope that having more than 2.22 million characters in 38580 poems will provide statistics about the PMI values with reasonable reliability.  We do not use the poems of the poets whose poems are in our segmented poems for training PMI values because we use the segmented poems as the test data. Hence, it is possible to encounter unseen unigrams and bigrams at test time. In these cases, we adopt a basic smoothing procedure to estimate the unseen instances.   Segmentation with Weighted PMI We can measure the quality of segmentation decisions in different ways. The most common measurement is the precision rate, recall rate, and F 1 measure. Denote the segmentation decisions for a line as an array of either N (for not segmenting) or P (for segmenting). Consider a seven-character line. There are six positions to place segmentation markers between the characters, and the correct decision for a line of 2-2-2-1 pattern is NPNPNP. An array of NPPNPP will produce a 2-1-2-1-1 pattern, and resulting in 2/4, 2/3, and 4/7 in precision, recall, and F 1, respectively.  Among our 2433 segmented poems, we can find 2009 poems that contain only lines that conform to the four patterns.   These four patterns are 2-2-1 and 2-1-2 for regulated pentametric octaves and 2-2-2-1 and 2-2-1-2 for regulated heptametric octaves.  For these patterns, the segmentation problem is boiled down to choosing either 2-1-2 or 2-2-1 for RPO poems and either 2-2-1-2 or 2-2-2-1 for RHO poems. Hence, we expect to achieve more favorable results when we use these 2009 poems as the test data.  Using this prior information in our segmenter, we produce only NPPN or NPNP decisions for RPO poems and NPNPPN or NPNPNP for RHO poems. Running our segmenter with the 2433 poems, we achieved  89.6% in F 1. When experimenting on the 2009 poems, we achieved  90.3% in F 1 , which is listed in line in row “ 4 patterns + PMI” in Table 2.     Table 2. Quality of Segmentation for different combinations of strategies and datasets (WR: percentage of word recovery; PSP: percentage of perfectly    More Demanding Measurements An F 1 of about 90% is an encouraging accomplishment, but it is somewhat clement. A correct segmentation decision does not guarantee the identification of a word. We need two correct decisions on both sides of a word to find the word. Hence, a more practical measure is to calculate the proportions of words recovered by our decisions. Running our segmenter with the 2433 poems and the 2009 poems, we recovered  85.2% and  86.3% of the words, respectively.  Is 90% in F 1 easy to achieve? Is recovering 86% of the words effortless? If we randomly choose one from NPPN and NPNP for RPO poems and one from NPNPPN and NPNPNP for RHO poems, we achieved only  78.3% in F 1 and recovered only  69.3% of the words for the 2433 poems.   Statistics for this method of segmentation is shown in row “ 4 patterns, but random” in Table 2.      Contributions of More Domain-Dependent Information Domain-dependent information is instrumental. If we do not employ the presumption about the four patterns and make a random guess for every segmentation decision, we would see  49.7% in F 1 and recovered  19.8% of the words for the 2433 poems.   Statistics for this method of segmentation is shown in row “ completely random” in Table 2.    Researchers found that it is more likely for the middle four lines in RO poems to parallel (Lee et al., 2018). Taking this factor into consideration, our segmenter can segment a pair of lines in the same pattern. We achieved  89.9% in F 1 and recovered  85.7% words for the 2433 poems.   Statistics for this method of segmentation is shown in row “ 4 patterns + PMI + parallelism” in Table 2.    The resulting improvements in F 1 and word recovery are not very impressive by adding parallelism into consideration. We have to consider an even stricter measurement: the percentage of perfectly segmented poems. Considering the number of correct decisions needed to perfectly segment a poem, this measurement is very challenging. After including the parallelism factor, we raised this percentage from  14.2% to  21.5% for the 2009 poems (see the last two rows of Table 2).    Concluding Remarks and Recent Progress with Word Vectors Table 2 summarizes the experimental results that we have discussed and observed in experiments that considered different combinations of segmentation methods and types of test data. Using the weighted PMI scores and adopting appropriate domain knowledge help the segmenter achieve better results. Our results are based on 2433 poems of seven famous poets. It is intriguing to replace the PMI scores with the cosine similarity that we can compute with the word vectors (Mikolov, Sutskever, Chen, Corrado and Dean, 2013), but we only observed some poor results in few preliminary explorations. More recently, we have increased the amount of labelled data significantly, and were able to apply deep learning, including the LSTM units (long short-term memory) in our classifiers for the CWS task. With the increased data, we have boosted the performance noticeably, and we shall discuss these latest results at DH 2019. Acknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-06, 107H121-08, 108-H121-06, and 108H121-08 of the National Chengchi University. The segmentation task was carried out by five assistants who major in Chinese Literature. We thank Yi-lin Dai, Nai-An Fu, Wei-Ting Huang, and Shuo-Feng Tsai of the University of Taipei and Yu-Ching Song of the National Taipei University.   ",
        "article_title": "Onto Word Segmentation of the Complete Tang Poems",
        "authors": [
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "corpus and text analysis",
            "oriental and asian studies",
            "content analysis",
            "natural language processing",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Overview Inspired by the increasing influence of China Biographical Database (CBDB   China Biographical Database: <https://projects.iq.harvard.edu/cbdb/home> ) to historical studies (Huang and Luo, 2018), researchers are building biographical databases for Japan (Born, 2018)   Japan Biographical Database: <https://www.network-studies.org/#!/>. , Singapore   Singapore Biographical Database on Chinese Pioneers: <http://www.nlb.gov.sg/resourceguides/category/singapore-bio-db-on-chinese-pioneers/> , and Taiwan   Taiwan Biographical Database: <http://tbdb.ntnu.edu.tw/>. . We extract information from various types of historical documents, and organize the extracted information for further queries and analyses. In CBDB and the Taiwan Biographical Database (Sie, Ke, and Chang, 2017), data are stored in relational databases.   We report experiences for producing a chronical listing for accounting an individual’s life.   In the Chinese community, this is called 年譜 /nian2 pu3/ or 年表 /nian2 biao3/.  The chronicles record main achievements of individuals in selected years. The statements summarize yearly achievements, and are usually brief and informative.   To this end, we locate descriptions about an individual’s yearly work from source biographies, and aim to produce shorter descriptions for the chronicles. One may accomplish the latter sub-goal with some different techniques including statistics, extractive summarization, abstractive summarization, and sentence compression (Knight and Marcu, 2000; Liu and Liu, 2009). We focus on our experience in sentence compression that utilize parsed structures (Filippova and Strube, 2008). Results of empirical evaluation suggest that we might produce reasonable assisting software for compiling chronicles, but fully automatic compilation of chronicles without large human annotated data can be challenging (Hasegawa, Kikuchi, Takamura, and Okumura, 2017).    Data Sources and Main Ideas We use the texts in two volumes of biographies which belong to the official gazetteers that were published by the Taipei city government (Chen, 2017), and we refer to this collection as STCG henceforth. The biography part of STCG includes more than 479 thousand of Chinese characters and punctuation for 319 persons. The biographies have between one and five pages. Although the contents of biographies differ, many of the statements in the biographies follow a typical style   We have opportunities to contact the editorial board of the gazetteers, and was confirmed that the authors did agree on a format that they should follow whenever reasonably possible. , so it is possible to extract statements about yearly accomplishments from those relatively long biographies.     Figure 1: a constituency tree    Figure 2: a dependency structure  We then process the statements with constituency and dependency parsers.   We use the Stanford package that includes several components, including constituency and dependency parsers. <https://nlp.stanford.edu/software/>  Figures 1 and 2 show the output of the parsers for the statement “ 張先生擔任臺灣電力公司的銷售經理”   An English translation for this Chinese statement is “ Mr. Zhang (張先生) served as (擔任) the sales manager (銷售經理) of the Taiwan Power Company (臺灣電力公司).” We chose not to show the constituency and dependency trees for this English translation mainly because the syntactic grammars (and word orders) of Chinese and English are different. Showing the constituency and dependency trees for the English translation might not really help reviewers and readers who do not read Chinese to appreciate the meaning and applicability of the trees.   , respectively.   These figures were produced by the Stanford CoreNLP 3.9.1 GUI (Updated 2018/04/05).  The constituency tree shows the syntactic relationships among the words, and the dependency tree shows the functional relationships among the words. In Figure 1, we can see that the constituency parser recognizes “ 張先生” as the main noun phrase ( NP) and “ 擔任” as the main verb of a verb phrase ( VP). In Figure 2, the dependency parser identifies “ 張先生” and “ 銷售經理”, respectively, as the subject ( nsubj) and the object ( dobj) of the main verb.  Given a dependency structure, we may design a computational procedure to shorten the original statement while attempting to keep the core information of the statement. For the above example, if we can identify the main verb in the dependency structure, keep the  nsubj and the  dobj nodes and the associative  compound:nn nodes, and drop all other nodes, we will produce “張先生擔任銷售經理”   The English translation of this compressed sentence is “ Mr. Zhang serves as the sales manager”.  , which is a good compressed expression of the original statement.     Empirical Evaluation To evaluate the effectiveness of the main ideas, we implemented a prototype for creating personal chronicles from a given biography.   Extracting Statements about Yearly Achievements The first step is to extract statements that begin at a time stamp and end at another time stamp. For instance, we can extract the statements between “In 1931” and “In 1932”, and assume that the statements are about the biographee’s status in 1931. Although this assumption might not work for all genres of texts, we actually extracted 4948 such statements from our corpora.  Our current method for identifying time stamps needs to be strengthened. Not all time expressions include numbers, e.g., “next year” and “two years later”. We need to build a full-fledged capability to identify all time stamps and extract the statements perfectly.   Figure 3: Distribution of sentence lengths    Segmenting and Sampling Chinese Sentences Although modern Chinese texts use a punctuation mark for ending a sentence, such a Chinese sentence may actually be separated by few Chinese commas, and these separated segments may correspond to multiple English sentences. Treating statements that are delimited by Chinese commas and/or Chinese periods as sentences is a common practice. We analyze the numbers of characters in the sentences, and Figure 3 shows the distribution of the sentence lengths. We chose to work on the sentences that have between 7 and 20 characters, and ignored those that have no verbs or have multiple verbs in the current study. Sentences that are very short do not need to be shorted, and, normally, parsers cannot correctly handle very long Chinese sentences yet.    Sentence Compression using Heuristics We can employ and evaluate heuristic rules for the compression task. An intuitive rule is to start from the main verb in a sentence. We reserve the  nsubj and  dobj nodes that connect to the main verb in a dependency structure. Keep the  amod,  compound:nn, and  tmod nodes that are with the  nsubj or  dobj nodes. Keep also the  auxpass,  case, and  neg nodes because they all carry crucial information, e.g.,  auxpass indicates the passive voice.   Definitions about the dependency labels are available at <https://nlp.stanford.edu/software/dependencies_manual.pdf>.   Alternatively, we compute the word vectors for the words in the corpora with Gensim (Mikolov, Sutskever, Chen, Corrado, and Dean, 2013).   Gensim: https://radimrehurek.com/gensim/: window_size=5, vector size = 250, CBOW  For typical simple sentences, we could find the top level NP and VP (Wang, Chen, and Liu, 2018), and identify the main verb and the main noun for the main VP in a constituency tree. We calculate the average vector for these core words, and select a next word to include to optimize the resulting cosine similarity between the average vector and the sentence vector of the original sentence until the similarity reach 0.95.    Evaluation Among the shortened sentences, we randomly chose ten samples for a specific length of sentences, and judged the appropriateness of the sampled sentences ourselves. We repeated the evaluation five times, and accepted 317 instances out of 700 test sentences when we tried the first heuristic. Figure 4 shows the average acceptance.   We list some pairs of the original sentence and the shortened sentence for reader’ reference. Accepted: (出任東方出版社董事長職務, 出任董事長職務), (大多演出詼諧逗趣的甘草角色, 演出甘草角色), (總統府頒授特種領綬景星勳章, 總統府頒授景星勳章), (於永和開設東方藝術舞蹈研究社, 開設舞蹈研究社), (中華民國營養學會頒發營養特殊貢獻獎, 營養學會頒發特殊貢獻獎), (板橋林本源家族在大稻埕開設林本源博愛醫院, 本源家族開設博愛醫院) Failed: (在台山東菁英籌組中華齊魯工商文教協會, 在菁英籌組文教協會), (于北投中華佛教文化館創辦中華佛學研究所, 佛教文化館創辦佛學研究所), (於該刊發表多篇關於中醫理論與實務的文章, 發表文章), (首度由雨傘節蛇毒分離出甲型雨傘節神經毒素, 分離出神經毒素), (病逝於美國洛杉磯, 病逝), (更進一步進行全國律師界之改革, 進行改革)  We repeated the same procedure to evaluate the effects of using the second heuristic. Figure 5 shows the average acceptance when we asked three different persons to judge the accpetance of the compressed sentences.    Figure 4. Average acceptance rates of the shortened sentences for the original sentences of varying lengths (The red dash line shows the trend.)    Figure 5. Average acceptance rates judged by three persons (using the second heuristic; the dash lines show the trends)  Acknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-08 and 108H121-08 of the National Chengchi University.   ",
        "article_title": "Toward Building Chronicles from Biographies in Local Gazetteers: An Application of Syntactic and Dependency Parsing",
        "authors": [
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Wei-Ting",
                "family": "Chang",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Ti-Yong",
                "family": "Zheng",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Po-Sen",
                "family": "Chiu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "corpus and text analysis",
            "oriental and asian studies",
            "linguistics",
            "natural language processing",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The term  restaurant, in Oxford English Dictionary, is defined as “a place where people pay to eat meals that are cooked and served on the premises” (Oxford, 2018). This word first appeared in 1806 (Merriam-Webster, 2018), and origins from Latin ( restaurare) and French ( restaurer). The term  menu, is defined as “a list of dishes available in a restaurant” (Oxford, 2018). This word first appeared in French in the 1830s, and origins from the Latin word  minutus (very small) (Merriam-Webster, 2018). Historical menus provide abundant information about changing regional tastes, the ingredients of popular dishes, the arrangements of different meals, and fascinating stories behind the menu, to name but a few. These menus constitute a significant aspect of culture of diet (Menus, 2018).   Recently network analysis has been applied in the fields of digital humanities, e.g., kinship analysis (Kindred, 2018) and correspondence network (Republic, 2018), to explore a variety of relations among different entities effectively and efficiently. However, research upon the modeling, measurement, and analysis of menus network is still at its very beginning. In this paper, we make particular efforts to answer the following questions: can we model the menus network in such a way that we can analyze interesting evolving patterns of cuisine? How can we integrate temporal, geographical, economic, and textual information into the network? In this paper, we aim to propose a menu network that closely resembles today’s social network based on the metadata and content of menus. It is our hypothesis that such a menu network provides a more holistic view of that period of time than what would be perceivable through single menu sheet. Literature Review Historic menus convey detailed information about the trends, styles, culture, and history of diet (Alejandro, 1988). An increasing number of libraries and institutes, e.g. New York Public Library (NY, 2018), Los Angeles Public Library (LA, 2018) and Cornell University Library (Cornell, 2018), devote to digitizing the menus collections and making them accessible to not only academic, but also the general public who are interested in the history of dining. Facilitated by digitization, historic menus embody emerging academic interests from fields such as history study, linguistics, literary study, and nutriology.  Researchers are curious to explore more beyond the words on the menu. For instance, Jurafsky et al. (2015) investigated the origin of Sushi and employed state-of-art statistical methods such as linear regression to compare the differences between the language used by low-cost restaurants to expensive ones on their menus. Turnwald et al. (2017) used log-likelihood ratio to reveal that restaurants described healthy dishes with significant less appealing words but more health-related words. However, these studies didn’t propose formal modeling of historic menus. Chahuneau et al. (2012) built a statistical model to predict price and sentiment for restaurants based on menus and customer reviews. However, they didn’t integrate temporal and geographical information in their model. Menu Networks In this section, we present the definition of a menu and propose our menu network model, respectively.  Definition 1. Menu . A menu is represented as a tuple m = { r, l, t, f, p} . r ∈  R denotes the restaurant name. l ∈  L denotes the restaurant location and t ∈  T specifies the date when the menu was used. f ∈  F specifies a dish on the menu. p ∈  P corresponds to the dish price.   Definition 2. Menu Network . A menu network is represented as an undirected graph G = ( V,E) , in which nodes V correspond to menus and labeled edges E correspond to the co-occurrence of dishes on the menus.  Each node has associated attributes {( r, l, t, fd) |  r ∈  R, l ∈  L, t ∈  T, fd ∈  F}. we define a “dish” function  fd :  V →  F as an attribute for each node to mark the dishes on each menu.  r ∈  R corresponds to the restaurant that created a menu.  l ∈  L corresponds to the location of the restaurant and  t ∈  T denotes the date that a menu was used.   Each edge has associated attributes {( d, f, p) |  d ∈ N , f ∈  F, p ∈  P}. Considering that there might be more than one dish appearing on both menus, we use an index number  d to differentiate each element in the edge attribute set. For instance, if there are  k dishes appearing on both menus  i and  j, the corresponding edge attribute is: {(1, f 1 ij, p 1 ij), (2, f 2 ij, p 2 ij) ,..., (k, f k ij, p k ij)}. The number of triplets in the set corresponds to the number of dishes occurring on both menus.  ",
        "article_title": " Dishes on the menu: Turning Historic Menu into Menu Network  ",
        "authors": [
            {
                "given": "Hui",
                "family": "Li",
                "affiliation": [
                    {
                        "original_name": "Heidelberg University",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-29",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "network analysis and graphs theory",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Existing studies have demonstrated the potential of network analysis in extracting and visualizing complex information from large historical datasets (Larson 2013; Sun 2016). Art history, however, still dwells in the early stages of exploring how network methodologies can benefit its scholarship. The only applications of network analysis in art historical research often equate network to social network and borrows concepts and theories from the social sciences (Kok 2013; Brosen et al. 2016; Lincoln 2016, 2017). Digital art history calls for new concepts tailored to artistic networks in order to account for the developments of art in all its complexity.  This research conceptualizes a novel art historical network of ideas inscribed in the iconography, connecting artists not through social ties but shared subject matters. Through visualizing this conceptual network, this study tries to answer the following questions: how are artists connected through their choice of subject matters? Have the patterns of such connections changed over time? And more importantly, is the choice of subject matter related to the choice of painting style, location of residence, and timing of entering the market? To answer these questions, this study draws on large digital collections of Netherlandish paintings to construct a dynamic network model of iconography. This research foregrounds history paintings produced in the Low Countries between 1575 and 1700, an established genre which witnessed dramatic iconographical changes during this period (Sluijter 1991). The results of this study demonstrate the spatial and temporal evolution of the artists’ network of ideas, revealing the changes in the structures of artistic interactions and the diffusion patterns of subject matters within the artist community in different cities.  Data  This research takes advantage of the large online collection of paintings of the Netherlands Institute for Art History ( RKDimages).   https://rkd.nl/en/explore/images   This database describes over 150,000 paintings within the scope of this study. In the absence of a mature image recognition technology that is able to discern a great variety of complicated subjects of the paintings, this research uses the iconographical notation system ICONCLASS, a hierarchically ordered system of unique alphanumeric classification codes with which most images in the RKD have been processed and tagged.   Iconclass is a classification system designed for art and iconography. It is the most widely accepted scientific tool for the description and retrieval of subjects represented in images (works of art, book illustrations, reproductions, photographs, etc.) and is used by museums and art institutions around the world, such as the Rijksmuseum. More details see:  http://www.iconclass.nl/   History paintings in the RKD database are distinctively labeled with ICONCLASS often referring to a specific scene or story, like  Adoration of shepherds (73B25) and  Diana discovers Callisto’s pregnancy (92C35211). I further link the  RKDImages to the comprehensive biographical database,  ECARTICO, to supplement the paintings with the biographical information of the artists.   ECARTICO is a comprehensive collection of structured biographical data concerning painters, engravers, printers, book sellers, gold- and silversmiths and others involved in the ‘cultural industries’ of the Low Countries in the sixteenth and seventeenth centuries:  http://www.vondel.humanities.uva.nl/ecartico/   The active period of the artists from  ECARTICO is used to introduce the dimension of time to examine the evolution of popular subjects and to evaluate the strength of connections among generations of artists.     Method The network of ideas draws a connection between two artists through a mutual subject matter; the strength of the tie between them is measured by the total number of iconographical overlaps in their oeuvre. The record of a painting in the  RKDImages identifies a link between the painter (if attributed) and the subject (labeled with ICONCLASS). One painter can link to many subjects as he painted various scenes and one subject matter can tie to numerous painters who depicted it. Putting all the works of art and artists together, a two-mode network is constructed, which consists of two types of nodes: artists and subject matters.  The two-mode network of persons and subjects are then collapsed into two one-mode networks by converting one of the node types to an edge or link between nodes of the other type (Watts 2004; Hanneman and Riddle 2005; Knappett 2011). For artists who picked a particular subject matter, the subject becomes the conceptual edge connecting the two artist nodes, while a single artist who depicted two stories would be the edge between those two subject matters. In other words, the two-mode, artist-subject network is projected into 1) an artist-artist network, in which the link is constructed by the subject matters artists mutually depicted; and 2) a subject-subject network in which the link is drawn by the artists who painted both themes. I resort to graph theory to examine the nature and the properties of the network of ideas. Among the measurements, modularity is particularly relevant to art history as it helps us to identify clusters of artists that are more likely to paint the similar subject matter or groups of subjects that are more often co-existent in a single painter’s oeuvre. Modularity can help understand whether or not the thematic choice is related to the choices of other aspects of their work and life.   Results and Discussion  Changes in the network of ideas over time Artists are grouped by the year they entered the market following Rasterhoff (2017)’s periodization of the art market in the early modern Netherlands and the network of ideas is constructed among artists in each period (Table 1).   Rasterhoff (2017) introduces the three periods of the painting industry and art market in the early modern Dutch Republic: 1580-1610 as a period of transition; 1610-1650 as an expanding market; and finally, 1650-1800 as mature market for paintings and other cultural goods.   A simplified version of the networks is visualized in Figure 1, showing the iconographies shared by at least three artists. Nodes in Figure 1 are sized by weighted degrees and are colored by the modularity classes.   Table 1. Metrics of the network of subject matter by time period      # of edges   #of Iconclasses   Avg. Degree   Degree centrality   Avg. Clustering coeff.   Avg. path length  Modularity    1575-1610   89   35   5.09   0.53   0.75   2.26   0.39     1610-1650   237   75   6.32   0.62   0.64   1.90   0.38     1650-1700   380   57   14.58   0.40   0.73   1.97   0.33    The structure of networks shown in Table 1 indicates an expansion in popular subject matters between 1610-1650, coinciding with higher interconnectivity seen in the network of artists. Astonishingly, although a greater variety of subject matters have been explored and popularized by the generation of masters who entered the market between 1610 and 1650, their successors fell back to a shrunken pool of subject matters and restrained their repertoire, which is reflected in the reduction of the number of nodes. This observation of the popularity of a limited number of subjects and the virtual absence of others in the evolution of the network of subject matters verifies the limited repertoire art historians suggested (Sluijter 2000; Bok 2001). Furthermore, a structural change is observed in the evolution of the network across time periods: the connections among the subjects depicted by painters entering the market after 1650 are more evenly distributed (Fig. 1c) and the network is less centralized, meaning the painter was likely to pick among the same pool of subjects instead of venturing into new iconographies. (a) 1575-1610   (b) 1610-1650   (c) 1650-1700   Figure 1: Network of subject matters by time periods Remarkably, the clusters of subjects (Fig.1, marked by colors) do not speak to the categories modern art historians use such as mythology, Old and New Testament. Rather, one can find a mix of all three in one cluster. The clusters in Figure 1 relate more closely to the visual resemblance of the scenes and the skills required to paint them. For instance, both the  Destruction of mankind (Old Testament) and  Marriage of Peleus and Thetis (Mythology) from the same cluster, depict scenes that are usually composed of a large group of nude figures in an outdoor setting.     Geographical variation of taste and connection The network of ideas infers a certain degree of locality, as observed in the size and structure of the network constructed by artists who mainly worked in the following artistic centers: Amsterdam, Haarlem, and Utrecht (Table 2). It means the concept of ‘local schools’ did exist before 1610 but gradually dissolved in the following decades when the market was more integrated and painters living in different cities could keep abreast of each other reflected in their choices of subject matters. Table 2. Metrics of the network of artists by location      # of edges   #of painters   Degree centrality   Avg. Clustering coeff.   Avg. path length  Modularity   Amsterdam 140 43 0.48 0.59 2.24 0.17   Haarlem 29 16 0.87 0.68 1.93 0.08   Utrecht 43 18 0.68 0.64 1.87 0.10    (a) Amsterdam    (b) Haarlem    (c) Utrecht   Figure 2: Network of subject matters by city The networks of ideas in Amsterdam, Haarlem, and Utrecht demonstrate unique characteristics: Amsterdam had higher modularity which loosely corresponds to different styles co-existing at the same time; Haarlem had a centralized network and a schism between generations, and Utrecht enjoyed the most densely woven networks of both artist and subject matters. The choice of subject matters shows certain preferences in each city, with Amsterdam venturing more into the new subjects popularized by innovators from Rembrandt’s teacher, Pieter Lastman, to Rembrandt, Haarlem demonstrating the legacy of Mannerism, and Utrecht embracing both old and new fashion in its oeuvre.  Amsterdam has the largest network among the three artistic centers. The high average path length and the relatively low clustering coefficient indicate a more hierarchical structure around key painters and the flow of artistic knowledge is likely to have trickled down from the great masters to their circle, then to the lower segments, instead of diffusing among the minor masters (Fig.2a). Amsterdam’s network is also the least centralized among the three (Table 2), suggesting that several masters formed their relatively independent circle of interaction.    ",
        "article_title": "Visualizing Networks of Artistic Ideas in History Paintings in the Seventeenth-Century Netherlands",
        "authors": [
            {
                "given": "Weixuan",
                "family": "Li",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam/Huygens ING",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "network analysis and graphs theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " While historical documents are common sources of digital humanities collections, many digital projects still do not use controlled terminologies to represent their content and aid search, discoverability, and use. There are often pragmatic reasons for this. The art of identifying appropriate descriptive terms is a valuable skill. Unfortunately, too few DH projects have access to information specialists who can index their documents for them. We are addressing these challenges with the Nineteenth-Century Knowledge Project, an ongoing initiative to create the first standards-compliant digital version of historical editions of the  Encyclopedia Britannica. The sheer scale of the project precludes human indexing, because it would take an estimated six-to-eight years to read through all of the entries. Instead, we use an innovative method to add automatically generated content metadata using linked open terminologies and the HIVE-approach. This method has allowed us to experiment on the optimal controlled vocabulary to use for indexing historical documents. Our presentation will focus on the results of this experiment.  HIVE stands for Helping Interdisciplinary Vocabulary Engineering, and allows for standardized, linked open vocabularies to be used for automatically indexing digitized text. The Knowledge Project demonstrates how large corpora can use automated indexing and still garner the benefits of controlled terminology. As part of the undertaking, we need to optimize the fit between the material and the controlled vocabulary we choose to do the indexing. What is the best vocabulary to use? We will compare different outputs generated from current and historical controlled vocabularies. The question we are trying to answer is whether a historical vocabulary that was current at the time of publication produces significantly different results than the present-day Library of Congress Subject Headings. The topic is critical for the Knowledge Project in particular. We are examining a 120-year span of historical editions, with an eye on identifying changes large and small in the construction of knowledge over time. Researchers know that knowledge changed dramatically between 1790 and 1910, and part of that change was a shift in attitudes about what counted as “official” knowledge and what did not. As social beliefs change, so too do the culture’s ideas about what matters and even about who has the authority to define knowledge. In the nineteenth century, that authority decisively shifted out of the hands of religious authorities and into those of scientists and professionals. The history of the earth changed, from a narrative based in scripture to an evolving narrative dependent of the discoveries of geologists and biologists, like Charles Darwin. Social beliefs at the time also define the selection of articles; there were few topics specific to women, as we would expect in a society that devalued their contributions. And social beliefs also shaped the content of the articles that were included: those on India and Africa reflect a colonizer’s perspective and represent indigenous people in ways we recognize as offensive stereotypes. Britannica was the most authoritative general reference source in the English language in the nineteenth century, largely because it faithfully represented the idea of knowledge at the time, in the most comprehensive fashion possible, and so it documents for us today the many problems inherent in the nineteenth-century concept of knowledge itself.  This historical collection is not what we would call “knowledge” today, and that is exactly the point. It serves as a viable stand in for what constituted knowledge in a previous century, and thus provides us with an important data set to explore the changing structure of knowledge over time. Within this corpus, we can trace the emergence of the twentieth-century concept of knowledge. But to do this accurately, we need to identify the older structure of knowledge and preserve its internal integrity as a historical object.  Example of knowledge organization in 1728.   Chambers, E. (1728), Preface.  Cyclopaedia, or, An universal dictionary of arts and sciences, 2 vols., London: Knapton, 1728, p. ii.    Like other comprehensive representations of knowledge, older vocabularies are also socially constituted, rather than neutral categorizations of knowledge topics. The Dewey Decimal Classification illustrates this principle well; in its early formation, the only two categories for African Americans were “slave” and “colonial subject.” This left no way to indicate books that had been authored by African American writers, and so it mirrored the same racial stereotyping present in the nineteenth-century Encyclopedia editions. Both are products of the same society and so both embody parallel issues in representing knowledge as a system.  In theory, by imposing a controlled vocabulary from the twenty-first century onto older historical materials, we are distorting the historical structure of knowledge and muddying the waters, precisely when we need to see most clearly the difference between current and historical concepts. In theory, using a historically-appropriate vocabulary will generate metadata that better captures the older structure of knowledge by not translating it into the terms of current, equally contingent vocabulary.   There is little evidence that this theory has ever been tested. We are running a study on these entries using the HIVE automated keyword generator with two controlled vocabularies. The first is the current Library of Congress Subject Headings. The second is the controlled vocabulary created by Ephraim Chambers for his  Cyclopaedia (2 vols., 1728). This was the ontology used by the  Britannica for its third edition of 1790, so it is the ideal vocabulary for the material. In this short presentation, we would like to review the experiment, compare the automated index terms output from the two vocabularies, and present our findings on the real-life consequences of indexing older materials with historical vocabularies.  ",
        "article_title": "Knowledge Representation: Old, New, and Automated Indexing",
        "authors": [
            {
                "given": "Peter M",
                "family": "Logan",
                "affiliation": [
                    {
                        "original_name": "Temple University, United States of America",
                        "normalized_name": "Temple University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00kx1jb78",
                            "GRID": "grid.264727.2"
                        }
                    }
                ]
            },
            {
                "given": "Jane",
                "family": "Greenberg",
                "affiliation": [
                    {
                        "original_name": "Drexel University, United States of America",
                        "normalized_name": "Drexel University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04bdffz58",
                            "GRID": "grid.166341.7"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-18",
        "keywords": [
            "concording and indexing",
            "content analysis",
            "history and historiography",
            "English",
            "library & information science",
            "ontologies and knowledge representation",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   The Late Gothic period (fourteenth-sixteenth century) was a phase of transition in Europe – with social, political, economic and cultural changes. Within this framework, Europe was the scene of a significant amount of mobility of artists that in some way materialised the production of architecture without borders: a \"Pan-European style\" (García Cuetos, 2011) capable of reproducing and adapting models in different places. This paper will present the project ArTNet “Analysing artistic transfer network. A social and spatiotemporal study of Late Gothic architectural production in the Iberian Peninsula” which was designed  to identify, record and analyse artistic transfer network transcending the building scale to better understand the process of Late Gothic architecture production in the Iberian Peninsula. An integral view bringing together several factors is being studied by multiscale models, combining HGIS and Graph model, and analysis (such as SNA, spatial statistics, map visualisation and spatiotemporal analysis).   The Iberian Peninsula was chosen for analysing the artistic transfer due to its spatial and historical characteristics: 1) it is a sufficiently extensive area, so it provides a suitable sample of data; 2) there was considerable communication and a great number of exchanges between Spain and Portugal during the Middle Ages; 3) during the transition to the Modern Era, the so-called “Reconquest” influenced the territorial consolidation and the architectural production; 4) Spain and Portugal were at this period conquering American and African territories, which increased their capital power and a new constellation of cities emerged providing more activities related to building construction.  The five  research objectives of the project are:  RO1. To digitally register the artistic transfer in the Iberian Peninsula with all the available resources, creating a new and open digital model (Gonzalez-Perez, 2018);  RO2. To analyse the movement of artists through space and time. These  track lines changed over time, reflecting also the political, economic, social and cultural evolution of the Iberian Peninsula. The study of the artists’ activities can show interesting patterns, especially when this took place between the main centres. In addition, we expect to discover the different level(s) at which these movements have occurred to also provide the analysis from the building scale;  RO3. To evaluate, from the heritage point of view, the systems and subsystems that make up the Late Gothic in relation to landscape, cities, infrastructure and the most significant architectures;  RO4. To formulate hypotheses through simulations to analyse transfers of artists due to incomplete or undocumented records. By doing so, the project will be able to deal with gaps in the historical data and offer new interpretations and insights;  RO5. Diffusion of the methodology and the results, increasing open access to the historical data created and gathered. In terms of scientific research in Architectural History, this project is providing new methodological concepts that will help to better understand the process of the Late Gothic architectural production.     Methodology  ArTNet uses a data-driven approach backed by techniques borrowed from GIS, Graph theory and data visualisation aimed at addressing the objectives outlined above. The methodology designed for this project is based on previous projects (Ferreira Lopes and Pinto Puerto, 2018) (Ferreira Lopes, 2018). The methodology is divided into four linked phases:  M1. Documentation of the artistic transfer . This is the main base of the database. All the information available (coming from a wide range of data sources, especially the most recent publications and studies) is being selected,   digitised and processed in order to convert analogue formats into digital ones.  M2. E-database Model. While doing  M1, a preliminary phase to structure the data as an “event-based model” was conducted and tested . Implicitly ArTNet therefore assumes that an “event” is the temporal entity that will interconnect artefacts, agents (artists, patrons, etc.), time, buildings, techniques, etc.  M3. Inter-sectorial data connection . This project also contemplates that the data should be open and interconnected with Open Scientific Data Repositories and Institutional Databases, such as the  Instituto del Patrimonio Cultural de España and the  Direção Geral do Patrimonio Cultural de Portugal.  M4 . The impact of artistic transfer in the architectural production. Two digital models will be considered: a spatiotemporal model capable of organising the information from the E-database in layers, a geographically-integrated historical GIS model; and a Graph model (represented by nodes and edges) based on an abstract model in which the main elements are the relationships, a relational-integrated historical model. In  M4 we were able to: i) create working models to simulate the movements of artists; ii) analyse the density of events and patterns (of techniques, materials, agents etc.) in space and time; iii) capture, analyse and visualise the influence of artists at the buildings where they were active; iv) run ego, centrality and clustering (SNA); v) provide models capable of testing hypotheses and also of generating new ones by simulation (to deal with historical data gaps,  RO4). These analyses offered new perspectives for understanding the phenomenon of the Late Gothic and re-evaluated its dynamism in light of the interaction between agents.     Discussion  Although in early stages, ArTNet has already provided new ways to research the artistic transfer by the simulation of new hypothesis and visualizations. The inputted data is still limited by the sources consulted and treated so far, making it difficult to achieve a more wide-ranging model. While ArTNet, as a project, is focused specifically on the artistic transfer in the Iberian Peninsula, collaborators have already identified cases that connects with other European countries (particularly France, Italy and Germany). One of our goals moving forward is to test and design a semantic web in order to adjust and expand the data structure for the research community.     Fig.1 Simulation of a flow hypothesis. Studying the route made by Juan del Castillo (master builder) between Seville and Lisbon in 1508.     Fig. 2 General of the Graph model.  ",
        "article_title": "Simulating Historical Flows And Connection. The Artistic Transfer During The 15th To 16th Century In The Iberian Peninsula.",
        "authors": [
            {
                "given": "Patricia",
                "family": "Ferreira-Lopes",
                "affiliation": [
                    {
                        "original_name": "University of Seville, Spain",
                        "normalized_name": "University of Seville",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/03yxnpp24",
                            "GRID": "grid.9224.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-30",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "network analysis and graphs theory",
            "databases & dbms",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   The objective of metadata curatorship is to ensure that users can effectively and efficiently access objects of interest from a repository, digital library, catalogue, etc. using well-assigned metadata values aligned with an appropriately chosen schema. However, we are often facing problems related to the low quality of metadata used for the description of digital resources, for example wrong definitions, inconsistencies, or resources with incomplete descriptions. There may be many reasons for that, all completely valid, e.g, in many cases those who host a digital repository have few human resources to work on improving metadata, and often data providers are not themselves the metadata creators.   In this paper we present our ongoing work aiming at defining computable metrics to assess metadata quality and automatize metadata quality check process.    State of the art  The curation framework developed by Bruce and Hillmann (2004) is considered as a benchmark in the pursuit of quality assessment of digital repository. This framework defines seven dimensions to measure the quality of the metadata:  Completeness ,  Accuracy ,  Conformance to Expectations ,  Logical Consistency and Coherence ,  Accessibility ,  Timeliness ,  Provenance . In the digital cultural heritage domain, these dimensions are fundamental for the evaluation of metadata quality. The evaluation helps various curators to systematically identify metadata problems, and could be straightforwardly applied to Europeana Digital Library   https://www.europeana.eu/portal/en    or the Ariadne   http://www.ariadne-infrastructure.eu    project as well. From the literature analysis it can be inferred that the problem of metadata curation is one of the most debated topic in the domain of humanities (Dangerfield et al 2013). Few attempts (e.g., Ochoa, X. and Duval, E. (2009)) were made to automatically compute quality metrics: however, they either focus on one dimension or are specific of some repository or metadata schema/profile.    Proposal for automatic quality check The solution we propose is a framework which aims at automatically checking metadata quality of a repository along the dimensions proposed by Bruce and Hillmann (2004). It would also enable reporting to metadata curators a detailed analysis of the metadata situation and suggestions for possible improvements. To develop such framework, two main activities have to be carried out:  To define metadata quality metrics, capturing the status of the metadata both at object level (i.e., how good are the metadata of a single entry in the repository) and, aggregated, at repository level; To define algorithms to compute the aforementioned quality metrics, and (possibly) return suggestions on how to fix low-quality metadata;  We already identified some strategies on how to perform these activities along some of the dimensions proposed by Bruce and Hillmann (2004), and in particular:   Completeness : computed by statistical approach; ratio of filled fields with respect to metadata profile.  The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object.   Accuracy : computed using Natural Language Processing applied to description field; several linguistic features (e.g., text length, syntactic complexity, conceptual density, entity mentions) are extracted from the description text, and, using part of the gold standard data as training set, a binary classifier will be trained to distinguish between “good” and “bad” descriptions based on such features. Each description is represented as a vector of features.   Logical consistency & Coherence : computed using semantic web technologies; leverage ontological background knowledge on artists, objects, dates, art periods, etc., to assess the coherence of metadata values (e.g., the dating of a metadata object should be compatible with the birthdate/deathdate of its author(s)); enforce use of reference ontologies / vocabularies on controlled metadata.   To validate the quality of the developed metrics and algorithms, we will rely on a “gold standard” dataset manually collected from the available repositories. Part of this dataset of good quality data (i.e., positive examples), will also be complemented with bad metadata quality objects (i.e., negative examples) to support the training of the envisioned supervised approaches.   Work done so far  We collected an initial “gold standard” dataset of 100 high-quality records, in terms of metadata description, from the repository of the italian digital library Cultura Italia   http://www.culturaitalia.it/   , spanning different object typologies (paintings, drawings, sculptures, etc.), to test the effectiveness of the metrics and methods to be developed.   We started implementing the metrics and methods for the  completeness  dimension. The idea is to count the available metadata, dividing this number over all expected metadata. More precisely, to capture that some metadata may be more important (e.g., compulsory fields) than others when estimating the completeness of an object description, we associate a weight to each metadata: e.g., compulsory fields may have weight 2, while non-compulsory ones weight 1. Hence, the completeness ratio results by dividing the sum of the weights of the available metadata over the sum of the weights of the expected metadata. The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object.  We computed the completeness ratio for all the ~2,500 records of the “Palazzo Pitti” dataset in  CulturaItalia. Fig. 1 shows a breakdown of the records in 5 interval categories according to their completeness ratio (e.g., 5% of the records have a completeness ration between 0.2 and 0.4), while Fig.2 plots the frequency of each specific metadata in the dataset.    Fig.1 Percentage of completeness ratio    Fig.2 Frequency of the metadata elements   Conclusion  Considering the amount of digital archives, problems related to metadata curation becomes evident. Reasons may be different: There is no curation task force, the metadata curation activity is delegated to the content providers or the metadata curation activity is made by hand. The development of an automatic process will enable the curators to not only obtain snapshots of the quality of a repository, but also to constantly monitor its evolution and how different events affect it without the need to run costly human effort. This could lead to the creation of innovative applications based on metadata quality that would improve the final user experience.   ",
        "article_title": "Computer Assisted Curation of Digital Cultural Heritage Repositories",
        "authors": [
            {
                "given": "Matteo",
                "family": "Lorenzini",
                "affiliation": [
                    {
                        "original_name": "Fondazione Bruno Kessler, Italy",
                        "normalized_name": "Fondazione Bruno Kessler",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01j33xk10",
                            "GRID": "grid.11469.3b"
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Rospocher",
                "affiliation": [
                    {
                        "original_name": "Fondazione Bruno Kessler, Italy",
                        "normalized_name": "Fondazione Bruno Kessler",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01j33xk10",
                            "GRID": "grid.11469.3b"
                        }
                    }
                ]
            },
            {
                "given": "Sara",
                "family": "Tonelli",
                "affiliation": [
                    {
                        "original_name": "Fondazione Bruno Kessler, Italy",
                        "normalized_name": "Fondazione Bruno Kessler",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01j33xk10",
                            "GRID": "grid.11469.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-02",
        "keywords": [
            "digital humanities (history",
            "libraries",
            "sustainability and preservation",
            "museums",
            "theory and methodology)",
            "English",
            "computer science and informatics",
            "GLAM: galleries",
            "archives",
            "ontologies and knowledge representation",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Recent work on digital scholarly editions by Elena Pierazzo (2011, 2016), Hugh Cayless (2010), and Joris van Zundert and Peter Boot (2012) describe the complex array of choices confronting editors of digital scholarly editions who wish to preserve their work beyond the life of current platforms, tools, and even standards. Here we describe a preservation model developed by the ARCScholar Digital Publishing Cooperative during tenure of a planning grant (February 2018-January 2019) awarded to Texas A&M by the Andrew W. Mellon Foundation.  Digital preservation is really two-pronged: libraries are working on the technological infrastructure, in the broadest sense, for maintaining assets (National Digital Stewardship Alliance, 2014), while creators and publishers of digital editions must use established standards in order to prepare the content so that it can be accessed and used via this infrastructure. ARCScholar is doing the latter. The Advanced Research Consortium or ARC (http://www.ar-c.org) consists of a community of scholars that began to approach digital aggregation in 2003. In that year, Jerome McGann organized a steering committee to launch his Mellon-funded project, NINES, or the Networked Infrastructure for Nineteenth-century Electronic Scholarship (nines.org). NINES served as a model for launching 18thConnect.org — focused on the preceding century — and the Mellon-funded MESA community, the Medieval Electronic Scholarly Alliance (mesa-medieval.org). This group of field-specific scholarly communities and online-finding aids needed to create an overarching organization that would make available for other groups the technical model spearheaded by NINES, provide infrastructure and programming support, and negotiate on behalf of all the groups with vendors that might contribute data. ARC is that organization.  ARCScholar, a subsidiary of ARC, is a digital edition publishing cooperative focused on helping publishers of digital editions, both large, institutionally backed publishers and scholar who have published their digital editions on the Internet without institutional support (peer-review, sustainability plans). Our goal is to make those digital editions interoperable and preservable in the most findable way possible, which is in keeping with the mission of ARC. During our planning grant, we created a process for preserving digital editions in university libraries’ digital asset management systems. The model has been developed at Texas A&M which has a Fedora 4 repository. We are creating a preservation model based upon on one edition, the Digital Donne Variorum.  The Variorum Edition of the Poetry of John Donne, edited by Gary Stringer, won a prize from the Modern Language Association in 2015-2016, and the press release explicitly mentioned the value of Digital Donne (Stringer, 1995). The  Variorum, both print and digital, was created using primarily assets held by the Cushing Memorial Library at Texas A&M University. The Digital Initiatives Group at TAMU-L created a pipeline for ingesting the TEI files of the edition that is stored in their instance of Fedora 4. Next, the metadata librarians are in the process of creating a catalog record of the digital edition that will appear not only in the local library catalog but in other library catalogs that use EBSCO’s Discovery System (EDS). The record will contain a) a link to the current, live digital edition; b) a link to the downloadable TEI file and its schema; and c) a IIIF Manifest of the manuscript page images available on a IIIF server (Figure 1):     Figure 1. We will be working with Cushing directly so that the IIIF Manifest references the page images digitized by TAMU-L which reside on their IIIF server. This would allow future researchers to access the page images along with TEI transcriptions (saved as “annotations” in the IIIF manifests) so that, once the Digital Donne website no longer exists, users can still access the edition through software such as Mirador (Figure 2):    Figure 2. Although enabling access to Digital Donne via EDS and eventually WorldCat does make the edition findable outside of its home institution, more is needed. Findability is critical to digital preservation: the more that digital content is accessed, used, and downloaded, the higher the probability of effective preservation. In order to maximize discoverability, access, and use, we also need to go beyond making metadata available. Future researchers may not know that the information they need is connected to authors, titles, dates of specific digital editions. In other words, the information needed in the future may be part of the content of a digital edition that is not exposed by its metadata. Fedora 4 natively exports RDF Triples: Resource Description Framework statements about the object which render the digital edition findable via the semantic web. However, the triples automatically generated by Fedora 4 reveal metadata only. We are setting up a SPARQL endpoint for ARCScholar that allows depositing much more detailed RDF, exposing the content of the edition as well as its metadata. For instance, in the case of  The Collected Letters of Robert Southey, published by Romantic Circles (Pratt, et al., 2009), we would like future generations of scholars to be able to find letters that British Poet Laureate Robert Southey wrote to Humphrey Davy a scientist, creator of the Pneumatic Institute, and developer of electricity. Beyond the life of any particular platform, the research embedded in scholarly editing would then be preserved for and discoverable by future generations.  ",
        "article_title": "Cutting the Gordian Knot: Sustaining Digital Scholarly Editions",
        "authors": [
            {
                "given": "Laura C",
                "family": "Mandell",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bryan",
                "family": "Tarpley",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Nicholas",
                "family": "Laiacona",
                "affiliation": [
                    {
                        "original_name": "Performant Software Solutions, LLC",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Shawn",
                "family": "Moore",
                "affiliation": [
                    {
                        "original_name": "Florida Southwestern State College, United States of America",
                        "normalized_name": "Florida SouthWestern State College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03g4gca53",
                            "GRID": "grid.434335.6"
                        }
                    }
                ]
            },
            {
                "given": "Lynda",
                "family": "Pratt",
                "affiliation": [
                    {
                        "original_name": "University of Notthingham, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "scholarly editing",
            "sustainability and preservation",
            "English",
            "semantic web and linked data",
            "literary studies",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Recent years have witnessed an impressive surge in the interest for Natural Language Generation. Advances in neural language modeling in particular have boosted the capacities of computational text generation systems, resulting in increased realism of generated text and the capability to generate text in a variety of genres or styles (Ficler et al., 2017). Aside from scholarship, text generation is currently triggering a significant deal of attention in the arts, for instance, with the emergence of artist communities such as botnik.org. While the limited semantic coherence of generated text remains a worry, the grammatical correctness and stylistic qualities of these artificial texts are remarkably convincing (Manjavacas et al., 2017).   Goal The current study set out to study how well human readers are able to distinguish between authentic and synthetic text fragments, with an interpretive focus on the linguistic cues that readers seem to rely on in their authenticity judgements. As a case study, we turned to the domain of Hip-Hop lyrics, known for its explicit content and typical rhythmical delivery (Adams, 2009; Potash et. al.,, 2015). Through a large-scale experiment in the form of a serious game, we crowdsourced authenticity judgments for original and generated rap lyrics. Through statistical modeling of the resulting database of authenticity judgments, the present study aims to (i) enhance our understanding of the cognitive processes at play in the perception of authentic and synthetic artifacts in cultural production and (ii) improve text generation systems.   Methods We have compiled a large corpus of Hip-Hop lyrics from the Original Hip-Hop Lyrics Archive (ohhla.com), an online archive of Hip-Hop songs. This corpus, amounting to approximately 38M tokens in about 64k songs, was preprocessed in a pipeline that included line and stanza detection, tokenization, syllabification, and grapheme to phoneme conversion (to detect word stresses and rhyme words). These data were used to train a neural language model (LM), which makes use of recurrent connections to model longer-term dependencies (Hochreiter et. al., 1997). With the help a LM, a sentence of n words, represented by words w_1 to w_n can easily be generated by following the generative process expressed in Equation 1:      During generation, we sample from the output distribution at each step inputting the token sampled at the previous step, which can also be a single character or syllable, depending on the sequences the model was trained on. We trained three kinds of LMs at different levels: a character-level LM, a syllable-level LM and a hierarchical LM (Chung et. al., 2016), the latter being similar to a word-level LM in that it decomposes the probability of a sentence into the conditional probabilities of its words but, additionally, it decomposes the probability of each word on the basis of its characters. Additionally, we also experiment with a conditional LM variant to each of the three models, which controls for sentence length and final sentence phonology (i.e. the phonological representation after the last stressed vowel). Figure 1 shows examples of generated text.    Figure 1: Generated samples from the experiment randomly extracted from different difficulty bins (e.g. 25%-50% refers to examples in the 25%-50% difficulty percentile according to a logistic classifier). Models correspond to character-level (C), syllable-level (S) and hierarchical (H). The trailing \"+\" indicates a conditional model.    Serious game At the heart of this study lies a crowdsourcing experiment carried out at a popular music festival. In this context, we collected authenticity judgments from participants in a serious game relating to how well participants could distinguish between authentic and artificial fragments generated by one of our models. In order to efficiently communicate the game’s purpose in the media, we publicized this experiment as a so-called ‘Turing test’, although the description below will make clear how the set-up is markedly different from the “imitation game” which Turing (1950) originally proposed. Each game took the form of a series of (independent) questions, each of which had to be solved within a time limit of 15 seconds. The questions randomly alternated between two kinds:  Type-A: presented with an authentic and an artificially generated fragment, the participant had to decide which fragment was authentic.   Type-B: presented with a single fragment the participant had to decide whether the fragment was authentic or generated.  Type-B questions involved less reading but only presented participants with a single fragment, meaning that participants were unable to compare two fragments. Each game allowed the player to answer at least 11 questions and the player was awarded one point per correct answer. After the first 10 questions, the game switched into a ‘sudden death’ mode, allowing the player to continue until a false answer was given. The length of fragments was randomly varied between 2-4 lines. We ranked pairs of generated and authentic texts in terms of difficulty (see below). Pairs were then collected into bins of increasing difficulty. After 5 questions, the questions presented would be sampled from the next, more difficult bin.    Figure 2: Example of a type-A question in the game's interface.    Modeling authenticity Each fragment for the game was enriched with a set of linguistic features that were deemed to be relevant in modeling the difference between authentic and synthetic Hip-Hop. These included morphological, lexical and syntactic characteristics (see below), which we refer to as “cues”. Most of these can be argued to capture some aspect of the linguistic complexity of the fragments. Using these features, we fit a Bayesian logistic regression model (see, e.g. Hoffman & Gelman 2014) with participants as random effect against the following two response variables (i) whether the text is authentic or generated (modeling objective authenticity) and (ii) the actual participant authenticity judgment (modeling perceived authenticity). Additionally, we also study and control for authenticity perception biases, learning effects and linguistic cues learnt to be exploited by participants to solve the game.    Results We restrict this discussion to our most salient results and refer the reader to the presentation for further details.  On average, the authenticity detection accuracy was above chance level (50%), with participants correctly answering 60.5% of the time. With 58% median accuracy, participants performed slightly worse on questions of type-B than of type-A, suggesting that the task becomes harder in the absence of a reference point. In addition, we observed marked differences in the authenticity detection accuracy for the three aforementioned Language Models. Hierarchically and word-level generated fragments were markedly harder to detect than character-level fragments. As shown in Figure 3a, there is considerable evidence of a learning effect in both question types, especially towards the beginning of the game. Importantly, the learning effect must be explained differently depending on question type. By design, any learning effect in type-A questions can only involve accuracy of the original fragment. For type-B questions, however, the learning effects seems to reflect a shift in bias towards “generated” to bias towards “original”, as can be seen in Figure 3b.    Figure 3: Marginal effects with 95% credible intervals of trial number for both type-A and type-B questions (a). Marginal effects plots showing the effect of trial number on guess accuracy for “Authentic” and “Generated” in type-B questions (b).  To estimate the objective feature importance, we perform a logistic regression analysis, with as dependent variable whether a text was original or generated, and as predictors the linguistic cues (Figure 4a); we performed the same procedure to model the perceived authenticity of text fragments (right panel). The odds ratios show interesting (dis)similarities in feature importance between the objective and the perceived authenticity. The average depth of syntax trees, for instance, suggests that generated text fragments have considerably less complex sentence constructions and this was clearly picked up by participants as well. Interestingly, Figure 4b shows that humans easily overestimated the positive weight of some feature types—e.g. the portion of politically incorrect words (pc words)—, indicating that humans underestimated a machine’s ability to produce foul language. These observations point to specific aspects for future text generation research to improve on.    Figure 4: Log-Odds ratios for objective (a) and subjetive (b) feature importance.   ",
        "article_title": " A Robot’s Street Credibility: Modeling authenticity judgments for artificially generated Hip-Hop lyrics  ",
        "authors": [
            {
                "given": "Enrique",
                "family": "Manjavacas Arévalo",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Mike",
                "family": "Kestemont",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Folgert",
                "family": "Karsdorp",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "artificial intelligence and machine learning",
            "natural language processing",
            "data mining / text mining",
            "English",
            "cognitive sciences and psychology",
            "computer science and informatics",
            "digital art"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The Digital Humanities (DH) community has made several attempts to foster serendipitous experiences through digital visualization tools (Ridge et al., 2014; Martin et al., 2017). Some tools (Bohemian Bookshelf (Thudt et al., 2012) and StackLife (Innovation Lab, 2012)) visualize bookshelves, hoping to produce the 'aha' moments that frequently occur in libraries and archives. Visualization tools developed specifically for exploring linked data, (De Vocht et al., 2015), (Heim et al., 2010), demonstrate the possibilities for serendipity that arise when information is semantically connected. Recently, the research team behind FERASAT, an interface for exploring linked datasets, identified design elements that enhance the potential for making unexpected connections (Khalili et al., 2018). These directed attempts at serendipity design reflect the importance of the accidental acquisition of information to humanities scholars. Sometimes, however, the aspects of visualization that lend themselves to serendipity are themselves discovered accidentally. This paper describes HuViz, the Humanities Visualizer, a digital tool for visualizing semantic relationships and ontologies represented using the Resource Description Framework (RDF). Though not originally conceived as an environment to foster serendipitous experiences, user studies conducted over the past two years indicate that it does just that. Moreover, unlike FERASAT, aimed at STEM and the social sciences researchers, HuViz was built with humanistic inquiry in mind. We here demonstrate how HuViz aligns with features identified in library and information science (LIS) literature as fostering serendipity. We conclude by introducing pending features designed to enhance its serendipitous potential.    HuViz: Background and use   Figure 1. The HuViz Shelf  OrlandoVision (OViz), the precursor to HuViz, was prototyped in 2010 as a tool to display extracts from The Orlando Project’s textbase (Brown, Clements, and Grundy, 2006-2018) as a series of interconnected nodes in a graph, allowing for a new method of exploring this large dataset (Holland and Elford, 2016). Initial experiments confirmed that such tools open up new hermeneutical pathways (Rockwell and Sinclair, 2016; Drucker 2014). The next generation of the tool, known as HuViz, grew out of the recognition of its potential for much wider applications beyond Orlando. In its current state, HuViz’s browser-based, interactive interface allows for the exploration of semantic relationships and ontologies represented in RDF. The shift positions HuViz to be compatible with Linked Open Data (LOD) from across the Web. HuViz users begin by selecting an RDF dataset (in Turtle format) and related ontology. The initial visualization is a circular 'shelf' containing all the nodes found in the dataset (Fig. 1). There are two ways to interact with the data in HuViz. The first method, for users unfamiliar with ontologies, is to simply drag a node into the center of the shelf. Once this original node is released, any nodes that are connected to it by one or more predicates will follow it into the graph, resulting in a small network of interconnected nodes (Fig. 2). Additional nodes can be dragged into the centre of the graph or “stage” to explore more connections, and unwanted nodes can be dragged back to the shelf. The Command Panel at the right of the shelf offers the second set of affordances (Fig. 3). This Panel allows users to select groups of nodes by class, edge/predicate, or set and perform operations on groups. Some familiarity with ontologies is desirable for using this second option, though either method of interaction lends itself to information exploration.    Figure 2. Network appears after one node is dragged into center.   User-testing User testing defamiliarizes a tool for tool makers, alerting the team to aspects of design and functionality that are only visible to an outsider, making this an important step in its development (see also: Terras et al., 2011; Ruecker et al., 2011). Testing brought our attention to the value researchers place on serendipity as well as to the ways in which HuViz facilitates unplanned discoveries. We conducted 23 user tests with graduate students, senior scholars, library professionals, and members of CWRC projects. The tests took the form of a combined survey-tutorial, beginning with the simple task of building a network using the drag-and-drop technique, gradually working up to more difficult, directed queries. Participants explored datasets generated from Orlando’s textbase of women writers, including a dataset centred on Ada Byron and another of famous cookbook authors. Participants could generate networks based on the node type, discovering, for instance, various jobs held by Jewish novelists. Finally, by clicking on links between nodes, users could see the texts from which the data was generated, learning for example about the lesser-known connection between Ada Byron and Charles Dickens. In both the survey responses and the post-test interviews, we found that users valued the ability to contextualize relationships, to draw upon their own expertise to customize the dataset, and to follow their own exploratory pathways through the data. This feedback helped to inform the next stage of the tool’s development.    Figure 3. The HuViz Command Box   Serendipitous elements of design Thudt et al. (2012) highlight several design features that they believe encourage serendipity, drawing on several LIS articles. These include: Multiple visual access points; Highlighting adjacencies; Flexible visual pathways; Enticing curiosity; and Playful exploration. Since 2012 a number of studies in LIS and DH have highlighted features of digital tools that encourage serendipity. Ridge et al (2014) used the concept of play to guide their development of Serendip-o-matic. Khalili et al (2018), in describing FERASAT, created a table of 12 design features related to serendipity. Two features relevant to our work are tools that support background knowledge and user contextualization and supporting convergent and divergent information behaviour.  As HuViz was not designed with serendipity as a primary goal, the research team did not have a similar list of features that support serendipity as objectives during development. However, user tests indicate that several design features actively support serendipitous information encountering as described in the literature. These design features are briefly described below:   Playful interaction and enticing curiosity HuViz is unusual in providing the ability to switch between two modes of inquiry, each of which lend themselves to serendipitous discovery in different ways and are also connected to the affordance of “having multiple visual access points” (Thudt et al., 2012). The hands-on mode encourages spontaneous, intuitive interaction, which in turn invites users to further explore both the dataset and the capabilities of the tool. Indeed, during testing, participants would frequently deviate from the set tasks and start experimenting on their own. Their tendency to go ‘off script’ suggests that most found working directly in the graph not only intuitive but enjoyable, moving from simple play to curiosity about other affordances of HuViz.   Convergent information seeking The Command Panel interface is less intuitive, requiring additional practice in order to familiarize oneself with the various functions. In this sense, it lends itself to more focused and purposeful, rather than exploratory investigation. At the same time, the ability to manipulate the dataset in more complex and sophisticated ways, which comes with higher levels of expertise, creates new opportunities for unexpected findings. Martin and Quan-Haase (2016) show how chance discoveries often incorporate “an element of intention.” Similarly, the HuViz Command Panel lets the user narrow their searches through related material, fostering serendipitous experiences through more directed forms of inquiry.   Divergent information seeking HuViz also allows for multiple ways to diverge from the task at hand, allowing users to come across related information that they were not expecting. Simply dragging a node onto the stage also moves in all nodes connected within one degree in as well - immediately opening up users to a series of relationships. Following the edge from one node to another, users can see the relationship between these two nodes, and can continue dragging in or activating more nodes to expand their inquiry. In addition to this, once they find an edge, or relationship between nodes, that catches their interest, they can click on the edge to display a pop-up box to provide context (see below) and a link to the source of the data.   Contextualization of information Martin and Quan-Haase (2016) found that historians were reluctant to use digital search strategies because of a lack of contextual information. HuViz provides context by providing 'snippets' of information to users when they click on the edge connecting two nodes, highlighting a portion of the original text from which the relationship was extracted (Fig 4). For further information, users can click through to the Orlando textbase (if they have institutional access) and expand the context further.    Figure 4. Once an edge is clicked, a \"snippet box\" appears to show context.    Conclusion and future steps Our user testing has had two main results: inspiring us to push forward with HuViz and motivating us to further foster serendipity, leading to the development of several new features, which we will briefly demonstrate. These include optimizing HuViz to work with the Web Annotation data model (further enhancing the ability to contextualize information), expanding the history function with a “path” option (highlighting adjacencies in the data, and creating a visual pathway for users), and responding to SPARQL queries (allowing users with background knowledge to traverse a web of data relevant to their interests).  ",
        "article_title": "Humanities Centered Design Features: Emergent Serendipity with HuViz",
        "authors": [
            {
                "given": "Kim",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Chelsea",
                "family": "Miya",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Shawn",
                "family": "Murphy",
                "affiliation": [
                    {
                        "original_name": "Nooron Collaboratory",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "user experience design",
            "interface",
            "gamification",
            "English",
            "library & information science",
            "semantic web and linked data",
            "literary studies",
            "ontologies and knowledge representation",
            "information architecture and usability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Scholars, researchers, and advocates of linked open data from the humanities and GLAM community are invited to participate in a half-day workshop, “Ontologies for Linked Data in the Humanities”, to take place prior to the start of Digital Humanities 2019, probably July 7th, in Utrecht, Netherlands. We are soliciting short papers (up to 2000 words) related to ontologies.   Those accepted will be circulated in an open-access online collection in advance of the workshop. (See collection for 2017 workshop here:   https://tinyurl.com/LOD4Hum  )   Selected contributors who are attending the workshop will be invited to give short presentations (5-10 mins) that will form the basis for discussion and debate. Please note: If you are unable to attend DH2019 but would still like to contribute a paper to this open source resource, please fill in the form at the link below (it will ask you if you are attending). This link will remain open after the workshop call is complete to collect further contributions.  Potential topics include:   Ontology re-use, including the choice of whether to import or cherry-pick terms Evaluations, applications, and implications of upper-level ontologies for humanities scholarship Application and extension of standards such as Web Annotation, IIIF, and CIDOC-CRM to solve challenges of digital representation or to support novel forms of analysis Strategies for combining ontologies Challenges of linking incommensurate vocabularies or ontologies  Modules, suites, and other strategies for promoting the uptake of ontologies in large or interconnected domains  Principles of ontology organization and architecture Tensions between simplicity and complexity or nuance Revision, management, versioning, and usage of dynamic ontologies Impacts of particular ontologies on discoverability, visualization, or analysis Reviews or overviews of generic tools for managing, versioning, or publishing ontologies   Paper submissions are due May 13th , after which the conference program committee will review them and notify potential participants, with a schedule of presentations, no later than June 7th. If you plan to participate in the workshop either as attendee or presenter, you will need to register for it through the DH2019 conference.   The 2017 workshop resulted in this collection:   Advancing Linked Open Data in the Humanities  . We intend to publish the 2019 collection of short papers in a similar format in advance of the workshop.   Short papers should be submitted using   this form  , which asks you to provide a link to your paper and confirm your willingness to make your submission part of an openly available online resource with a CC-BY-NC licence.   Looking forward to hearing about the exciting work everyone is doing with Linked Open Data! ",
        "article_title": "Ontologies for Linked Data in the Humanities Sponsored by the ADHO LOD SIG",
        "authors": [
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Kim",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Jasmine",
                "family": "Drudge-Willson",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Johanna",
                "family": "Drucker",
                "affiliation": [
                    {
                        "original_name": "University of California in Los Angeles",
                        "normalized_name": "University of California, Los Angeles",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/046rm7j60",
                            "GRID": "grid.19006.3e"
                        }
                    }
                ]
            },
            {
                "given": "Joseph T",
                "family": "Tennis",
                "affiliation": [
                    {
                        "original_name": "University of Washington",
                        "normalized_name": "University of Washington",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00cvxb145",
                            "GRID": "grid.34477.33"
                        }
                    }
                ]
            },
            {
                "given": "Ryan",
                "family": "Shaw",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "computer science and informatics",
            "semantic web and linked data",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " SIMSSA (Single Interface for Music Score Searching and Analysis) is an ambitious project that aims to unite, under a single framework, the ability to:   Transform images of musical scores into searchable digital symbolic representations using OMR (optical music recognition) Computationally extract meaningful statistical information (features) from symbolic music files Use machine learning and statistical analysis to conduct musicological research using this data Create a framework for searching symbolic scores based on both metadata and musical content Make the resulting information and tools easily accessible to other researchers  Much has been accomplished since SIMSSA was first presented at DH (Fujinaga and Hankinson, 2013), but we have also made missteps along the way. Both our successes and failures have provided insights applicable not only to the specialized fields of MIR (Schedl et al., 2014) and digital musicology, but also to the digital humanities in general. This paper is intended to share our experience with the DH community. The proper construction of datasets is one area of central importance. Too often, researchers simply combine digitized data as is, from whatever sources are available, or digitize data themselves without first developing a carefully considered workflow. This can lead to erroneous conclusions, as false patterns may be observed based on inconsistent dataset construction practices rather than on the underlying information itself. Alternatively, meaningful patterns may be obscured by datasets that fail to capture essential information. We encountered such problems when we carried out research on regional differences between Iberian and Franco-Flemish Renaissance music (McKay, 2018): individual transcribers had encoded note durations differently, so rhythm was correlated more with the transcriber than with the underlying music. Problems can also be introduced during the encoding process, as we observed when commercial score editing software confused the encoding of slurs and ties (Nápoles et al., 2018). We therefore developed a set of best practices to help avoid bias when constructing datasets from historical documents (Cumming et al., 2018). Selection of data is also essential. Results can be compromised if a dataset does not represent the full range of relevant instances (e.g., only an artist’s early works) or contains uneven class distributions (e.g., many more works by one artist than another). For example, we observed in machine learning-based research on composer attribution (McKay et al., 2017b) that, if we did not carefully prepare our data, trained models would sometimes perform classifications based on genre rather than compositional style, since the number of masses and motets were not evenly distributed across composers.  Judicious application of machine learning is another central area of interest. Most current research emphasizes deep learning, where models are trained on huge datasets, with relatively generic pre-processing. This contrasts with more traditional approaches where training is performed on hand-crafted statistical features that quantify specific qualities of domain interest, or where sub-systems sequentially process data in stages following a pre-defined workflow. Deep learning and more traditional alternatives each have different strengths and weaknesses, which one must understand before choosing which to utilize. The current emphasis on deep learning is understandable, given its widely documented success in many domains. We found, for example, that our OMR performance improved substantially when we switched from a traditional architecture to a deep learning framework that directly processes pixel windows (Calvo-Zaragoza et al., 2018).  Deep learning also has important limitations, however. Its need for huge training sets can be a serious limitation when dealing with historical data with limited extant instances, even when clever data augmentation techniques are used. This problem became apparent in our research on automatically harmonically analyzing chorales (Condit-Schultz et al., 2018), for example. Another important limitation is that deep learning, despite recent advances in model transparency, still often results in black-box classifiers. In contrast, feature-based systems (in conjunction with feature-selection algorithms) produce searchable data and directly accessible insights into how features differentiate classes in ways that are meaningful to domain experts. This can be even more important to DH research than class label outputs themselves. As an illustration of the potential advantages of a feature-based approach, we used 1497 features extracted by our jSymbolic software (McKay et al., 2018) not only to train models that could correctly attribute the music of Renaissance composers (McKay et al., 2017b) and identify Renaissance genres (Cumming and McKay, 2018) with high accuracy, but also to gain novel musicological insights into which musical characteristics statistically differentiate these classes. We also empirically tested expert predications about musical style in these studies, 63% of which were found to be inaccurate. There is a particular need for such testing in the humanities, as there are many generally accepted assertions that have never been validated empirically.  We also used the jSymbolic features to provide content-based support (McKay et al., 2017a) for composer attribution confidence levels proposed by Rodin and Sapp (2015) based only on historical evidence. This is a nice example of how computational and traditional humanities research can complement one another.  It is also essential to consider issues associated with making research data, software and results available, useable and attractive to other researchers in the humanities, including those not yet accustomed to computational approaches. As noted by Wiering (2017), one must consult domain experts about what they need, rather than imposing decisions on them. Other priorities include: clean and consistent interfaces; clear and extensive documentation, including tutorials; adoption of open accepted standards; compatibility with diverse data formats; facilitating extensibility for other researchers; and careful consideration of data and software in the context of intellectual property laws. The better DH researchers become at facilitating the sharing of data and software, the better we will be able to directly compare techniques and results across research groups. This will in turn permit experimental repeatability and validation, as well as encourage iterative refinements across groups. Researchers will be better able to explore data in new ways and subject long-standing assumptions to empirical validation, arguably the two greatest benefits computational approaches bring to the humanities. We believe such steps will further expand the already excellent research underway in the digital humanities. ",
        "article_title": "Lessons Learned in a Large-Scale Project to Digitize and Computationally Analyze Musical Scores",
        "authors": [
            {
                "given": "Cory",
                "family": "McKay",
                "affiliation": [
                    {
                        "original_name": "Marianopolis College, Canada",
                        "normalized_name": "Marianopolis College",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/004z6x951",
                            "GRID": "grid.439969.8"
                        }
                    }
                ]
            },
            {
                "given": "Julie E.",
                "family": "Cumming",
                "affiliation": [
                    {
                        "original_name": "McGill University, Canada",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            },
            {
                "given": "Ichiro",
                "family": "Fujinaga",
                "affiliation": [
                    {
                        "original_name": "McGill University, Canada",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "stylistics and stylometry",
            "musicology",
            "content analysis",
            "artificial intelligence and machine learning",
            "theory and methodology)",
            "English",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2014 the Andrew W. Mellon Foundation launched a new initiative to fund explorations of “long-form digital publishing in the humanities” (Straumsheim, 2015; Maxwell et al., 2017). That same year a small group of Emory University faculty began a conversation, with a planning grant from Mellon, about the state of humanistic scholarship, and in particular how the monograph might evolve or adapt alongside the digital humanities. Their findings (Elliott, 2015) serve as the foundation for the Digital Publishing in the Humanities (DPH) initiative, a five-year experiment (2016–2021) to support faculty authors engaged in digital monograph publishing. The DPH initiative shares its charge with a cohort of sister Mellon projects, including the Digital Publications Initiative at Brown University, Greenhouse Studios at the University of Connecticut (Ceglio et al., 2019), and Publishing Without Walls at the University of Illinois. Each project takes a unique approach in its offerings to faculty authors. At Emory, DPH provides support to individual faculty working on long-form humanistic scholarship that seems well suited to the digital environment, and it does so in real tenure/promotion clock time. While Emory’s T&P guidelines include a memorandum that “recognizes the significance of digital scholarship in the humanities, and affirms the importance of assessing this scholarship fairly and carefully in decisions of faculty tenure and promotion” (Emory College, 2013), many faculty and administrators continue to rely upon printed monographs as the safest route to advancement. The DPH initiative, then, consists not only in supporting individual publications but also in fostering conversations that promote wider acceptance of digital scholarship among humanities faculty. We also invite academic press editors to join these discussions, in hopes that the DH and publishing communities might collaborate to produce born-digital works of robust scholarship that stand alongside printed monographs in both quality and import. The DPH initiative is based at Emory’s Bill and Carol Fox Center for Humanistic Inquiry, an interdisciplinary space that has encouraged thoughtful and rigorous intellectual work for nearly two decades. During those years, as researchers developed the tools, theories, and methodologies that comprise today’s digital humanities, the open access movement forged digital pathways for the wider dissemination of scholarship. These developments, amid persistent critiques that the digital humanities lack true substance (Brennan, 2017; Weiskott, 2017), give rise to new and urgent questions about the future of monograph publishing. In partnership with the Center for Faculty Development and Excellence, Emory Center for Digital Scholarship, and Scholarly Communications Office of Emory Libraries, DPH extends the Fox Center’s mission into the conversation about evolving digital monographs and offers three pathways to their development. The first pathway offers a subsidy, directly to a university press, that supports the open access publication of a traditional monograph. The open access version replicates the content and form of its print edition and is typically disseminated as a PDF or an EPUB file. But as authors are increasingly eager to include digital enhancements, including audio and video clips, interactive maps, or data visualizations, alongside their written arguments, a second pathway offers support for an enhanced open access book, which integrates text with digital components. Here again, a print edition is typically available as well, and the essential form and structure of the monograph remains unchanged. The third pathway, toward an interactive open access monograph, raises fundamental questions of what humanities scholarship might look like in the digital environment. For authors committed to pushing the boundaries of the printed book, creating born-digital and interactive monographs not only allows them to showcase digital artifacts but also provides the opportunity to build an argument that doesn’t shy away from complexity or ambiguity. Such works seek to embrace an “enactment of digital rhetoric” (Eyman and Ball, 2015), perhaps inviting multiple conclusions and active participation from readers (Murray, 2017; Ryan, 2006). But how far can such a work deviate in form from the traditional book and still be recognized as a substantial contribution to its field? How is it reviewed? Distributed? Preserved? How might digital scholarship and/or humanities centers collaborate with publishers to share resources and expertise? Answers to these questions are complicated and dependent on multiple contingencies. In our first two years of the DPH initiative, we find that the composition process itself, including research, for a digital publication must be relearned, or seen with fresh eyes. In some cases, DH methodologies alter the research and even the research question in innovative ways that demand new publication strategies. In others, the numerous options for assembling digital material into a compelling argument might overwhelm an author. Moreover, the printed monograph as a genre is not static, nor is it entirely consistent across disciplines. Digital variants of this traditional form, then, will also resist monolithic expression. We have found that one promising starting point is to guide authors through the essential attributes of a monograph using genre analysis (Ball et al., 2018). Other tactics include introducing authors to best practices in data management (including metadata creation) and multimodal composition, and hosting workshops that guide project teams through visualization exercises or allow them to share works in progress with other researchers. Another critical aspect of our approach is the commitment to inviting collaboration with publishers early in a digital monograph’s development. While traditional publishers are increasingly willing to explore open access, including best practices for distribution and preservation, most have yet to test the waters of digital/multimodal composition beyond an enhanced e-book. Editorial and production staff at most publishing houses are not often trained in DH tools and methodologies, and so we offer expertise of our team at the Emory Center for Digital Scholarship to help offset that lack. On the other hand, DH practitioners of all stripes might benefit from the editorial eye and discipline that publishing professionals bring to the table, as well as their expertise in marketing and distribution to ensure that the work is discoverable and cited. My presentation will elaborate on these guiding principles behind the DPH initiative and offer case studies of digital monographs in development by Emory faculty. ",
        "article_title": "DH And The Evolving Monograph",
        "authors": [
            {
                "given": "Sarah E.",
                "family": "McKee",
                "affiliation": [
                    {
                        "original_name": "Emory University, United States of America",
                        "normalized_name": "Emory University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03czfpz43",
                            "GRID": "grid.189967.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-04",
        "keywords": [
            "copyright",
            "open access",
            "digital textualities and hypertext",
            "digital humanities (history",
            "digital ecologies",
            "open content and open science",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "library & information science",
            "licensing",
            "scholarly publishing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In his forward to the first edition of the  Companion to Digital Humanities Roberto Busa (2004) declares:  “Humanities computing is precisely the automation of every possible analysis of human expression (…) in the widest sense of the word, from music to the theater, from design and painting to phonetics, but whose nucleus remains the discourse of written texts.”  The revered pioneer’s claim was at the same time modest, and universal: universal in terms of attesting ‘automation’ relevance across the entire spectrum of the humanities; modest in terms of reserving its application to analytical procedures. Although Humanities Computing has meanwhile morphed into DH, similar programmatic pragmatism still characterizes current text-book style introductions to the field (see e.g. Thaller, 2017). DH’s internal methodological self-reflection however has long identified an exclusively pragmatic self-definition as reductionist (see among other McCarty, 2005; Beynon, Russ and McCarty, 2006; Vanhouttee, 2013; Krämer and Huber, 2018; Deck, 2018).  This is friendly fire though: nuances and methodological arguments within the DH are of little concern to hard core humanists, as Fish (2018) demonstrates. He labels the digital humanities ‘an anti-humanistic project, for the hope of the project is that a machine, unaided by anything but its immense computational powers, can decode texts produced by human beings.’ The substance of the argument is absurd; the fact that it continues to find an audience among traditional humanists warrants attention though. One cause of irritation is the focus of current DH research on methods such as stylometry, topic modeling, sentiment analysis, NER, SNA, visual pattern analysis, word2vec etc. All of these share an epistemological and, perhaps ontological premise (Capurro, 2010) which the traditional humanities cannot readily accommodate: in distant reading approaches human culture and symbolic practice are primarily conceptualized as a phenomenon of data patterns evolving over time, not as experiential and meaning-making artefacts.   To many digital humanists the empirical focus of Humanities Computing and DH initially provided a welcome antidote to the plethora of changes-in-paradigm, grand visions and anti-realistic ideologies that have emerged since post-modernism became  en vogue  (Nida-Rümelin, 2018). Then, from the early 2000s onward, DH began to acknowledge that the mere identification of data patterns equals  ἐμπειρίᾱ in the Platonian sense of: experience without knowledge of reasons and causes for the experienced phenomena. And so the empirical paradigm in DH was gradually counter balanced by the hermeneutic again; a process that is ongoing. But the outside perspective onto DH differs distinctly. From the traditionalists point of view DH is at best a ‘Hilfswissenschaft’, an ancillary science whose purpose is to support the old humanities disciplines–a problematic label (Sahle, 2015) for a field of practice which itself has finally begun to engage in discourse on the  Critique of Digital Reason  (the Kantian motto of DHd 2018 at Cologne University; see Sperberg-McQueen, 2018).       Moretti (Hackler and Kristen, 2016) attributes the difficulty of supplying a plausible  raison   d'être  to the vagueness of the label itself:  “ Digital humanities is simply a formula that has come to identify a large field. I use it only because everybody uses it and everybody will use it. But, frankly, I don’t like it. I think it means nothing, whereas ‘quantitative’ and ‘computational’ mean something.”   But there is more at stake than terminology: it is the  Computationality of Hermeneutics and a hermeneutic ethos which DH has to deliberate and communicate more prominently, as  van Zundert (2016) points out. And it is precisely the topos of a fundamental lack of such ethos which skeptical and polemic interjections questioning the humanistic legitimacy and legacy of DH raise time and again. Take Fish (2018): ‘It is true, as digital humanists claim, that a corpus that has been digitized can then be searched for patterns the naked eye could never discern–frequency patterns, contiguity patterns, collocation patterns (…) The problem is that once such patterns have been uncovered, there is no legitimate route from them to the interpretation of texts.’ For Fish the only ‘legitimate route’ from data to interpretation is the one whose point of departure is a concrete author subject, or as he puts it: ‘Interpretation can’t get started without the prior identification of an intentional agent.’   May this critic saunter down the yellow brick road of intentional fallacy; we will take the philosophical high way: The search for a plausible ‘route’, a genuinely humanistic method to connect ‘data’ with meaning motivated among other Dilthey’s 1883 distinction between the objective explanation of external natural phenomena in the sciences ( Erklären) and the subjective, re-enacting comprehension of inner phenomena of the mind and soul ( Verstehen) in the humanities. Dilthey highlights  reflexivity as the key characteristic of the workings of the human mind, and as that of humanistic methods for the study of its artefacts and practices. But Dilthey (1923) also points out the impetus to interpret even our ‘Erklären’-motivated findings in terms of a ‘Verstehen’-motivated reflexivity:  “(…) the same human will then turn back from nature to life, toward himself. This return of man to the experience by which nature exists for him in the first place, into life within which alone meaning, value and purpose appear, is the other great tendency which determines scientific work.” (own transl.) The application of hermeneutics is thus not restricted to works of art and symbolic practices. Moreover, our 21 st century reality includes phenomena which we observe in the (conceptually and instrumentally mediated) form of quantified, discrete data. Yet is DH willing to ‘turn back to life’ and risk the return from digital data analysis, from the mathematical and statistical modelling of symbolic objects and practices, back into the domain of a self-reflexive practice of ‘Verstehen’? Individually: certainly; see e.g. Moretti (2013) who demonstrates how digital operationalization can feed back into a reappraisal even of Hegelian aesthetic theory. Programmatically though: on what grounds? How could a  humanistic, self-reflective legitimation of the digital humanities be argued?   I propose a philosophical appraisal which situates DH provocatively and in the most unlikely of traditions: Romantic theory of science and aesthetics. For the ‘point’–that is: the core epistemological and philosophical added value–of the digital for the humanities stems from three characteristics that seem to resonate with Romantic ideals:   The digital representational principle of analytical segmentation of an object into discrete points of observation can represent as  capta (Drucker, 2011) three types of semiotic phenomena (sensu Frege and Peirce respectively): (1) symbols, i.e. phenomena which we process as having a  meaning function, (2) signs, i.e. phenomena which we process as having a  referential function, and (3) phenomena interpreted either as random effects, or as  self-referential, intra-systemic data patterns. The only other semiotic system capable of this threefold function is: natural language.   Unlike natural language the digital is however a lingua franca that comes without ontological, existential, conceptual, cultural or historical bias. Rather, because of its radical abstraction from context it provides a medium and conceptual space for the exploration and bottom-up modelling of dynamic structures, relations and effects within and across the realms of sensory modes, as well as across the cultural, the natural and the mental domains under investigation. This I term the conceptual affordance of  digital synesthesia. (For the conceptual equivalent in a cognitive science perspective see Ramachandran and Hubbard, 2001).   The digital is a particularly ‘un-natural’ language when it comes to representing symbolic meaning in the Fregian sense of: phenomenological-existential relevance. It enforces abstraction from context–but more importantly, eventual explication of context once we begin to realize the deficiency of a digital model. Taking the detour toward meaning via formalization is the quintessential ethical principle of DH hermeneutics. However, in order to progress from the calculated alienation from and suspension of synthetic interpretation via abstraction and formalization to a new level of content-oriented epistemology DH will eventually need to engage in methodological self-reflection: the new mode of modeling and understanding meaning requires us to turn it onto itself.   This scientific ideal and vision of a knowledge generation process based on a form as well as a content-oriented methodology, on a synesthetic epistemology and, at the same time, on an overarching self-reflective ethos was central to Friedrich Schlegel’s late 18 th / early 19 th Century philosophic and aesthetic theory. In this context the synesthetic epitomized the Romantic ideal of a universal, inter-modal and inter-operable poetic and scientific practice, an ideal taken to the extreme in Kleist’s (1810) self- agrandissement as an individual ‘capable of formula and metaphor’. In Schlegel’s theory such risk of declaring a new intellectual absolutism had, however, already been anticipated and counter-balanced by the equally important postulate of  parekbasis, that is: the commitment to constant critical self-reflection, encapsulated in Schlegel’s dictum ‘Ironie ist Pflicht’ (‘irony is an obligation’; Schlegel, 1963: 85).   I argue that it is precisely these two commitments and necessities which (ought to) characterize the methodological ethos of the digital humanities; something similar to the tendency toward an aesthetics of synesthetic, yet cognitively paradoxical inter-modality which Scarlett (2015) attests contemporary digital media art (he terms this inter-modality a ‘catachrestic synesthesia’). DH does not require a poetics though: it is not concerned with producing symbolic artefacts, but with modeling, exploring, analyzing and interpreting them–and it does so by using the digital as a representational (more precisely: meta-representational) system of unrivalled flexibility and applicability. But rather than mere breadth of scope its greatest affordance is that, unlike natural language, this is a system and ‘language’ not already invested with meaning and value per se–a characteristic which, rather than rendering the digital hermeneutically dysfunctional, turns it into a universally relevant heuristic tool which in the end aids, rather than delimits hermeneutic reflection.  And somewhere, over the rainbow, this prospect in combination with an appraisal of DH as an offspring of Romantic theory might catch our harshest critics unaware. For if the humanistic goal of DH is to arrive at ‘meaning’ in its fullest phenomenological sense, albeit via a different route, then where exactly is the problem? “There’s no place like home!” ",
        "article_title": "Digital Synesthesia or the Point Of the Digital for the Humanities",
        "authors": [
            {
                "given": "Jan Christoph",
                "family": "Meister",
                "affiliation": [
                    {
                        "original_name": "University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-03",
        "keywords": [
            "digital humanities (history",
            "linking and annotation",
            "methods and technologies",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Vitali Rosati, Marcello. 2018.  On Editorialization: Structuring Space and Authority in the Digital Age. Institute of Network Cultures. Amsterdam.  ",
        "article_title": "Palatine Anthology. Complexity for a digital research project",
        "authors": [
            {
                "given": "Margot Lise",
                "family": "Mellet",
                "affiliation": [
                    {
                        "original_name": "Canada Research Chair in digital textualities, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marcello",
                "family": "Vitali-Rosati",
                "affiliation": [
                    {
                        "original_name": "Canada Research Chair in digital textualities, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Elsa",
                "family": "Bouchard",
                "affiliation": [
                    {
                        "original_name": "CRIHN : Centre de Recherche Interuniversitaire sur les Humanités Numériques",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Servanne",
                "family": "Monjour",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            },
            {
                "given": "Enrico",
                "family": "Agostini-Marchese",
                "affiliation": [
                    {
                        "original_name": "Canada Research Chair in digital textualities, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Beth",
                "family": "Kearney",
                "affiliation": [
                    {
                        "original_name": "University of Montreal",
                        "normalized_name": "University of Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0161xgx34",
                            "GRID": "grid.14848.31"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "digital textualities and hypertext",
            "classical studies",
            "theory and methodology)",
            "English",
            "semantic web and linked data",
            "information architecture and usability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  As researchers in the digital humanities we have been successful in building online components for our work. However, we have failed in making it a priority to devise a plan to gracefully discard our online components once we no longer need them. Thus, many of the online projects in the digital humanities have an implied planned obsolesce —which means that they will degrade over time. Previous work presented in Digital Humanities 2017 and 2018 has explored the abandonment, and the average lifespan, of online projects in the digital humanities using metadata from HTTP response headers  (Meneses and Furuta, 2017) and contrasted how things have changed over the course of a year  (Meneses et al., 2018). We believe that managing and characterizing the degradation of online digital humanities projects is a complex problem that demands further analysis because the methods for identifying change in the Web do not fully apply; and the end of life for a digital humanities project may or may not be indicated by updates in its content and tools.   In this sense “abandonment” is not necessarily a sufficient designation —as there are different nuances involved. We have seen many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.tamu.edu and http://botany.csdl.tamu.edu/). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. In this case, the lack of updates and new content is not a signal of abandonment. These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities. In this abstract, we go one step further into exploring the collectively shared distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. For this purpose, we have created a framework that collectively quantifies their signs of abandonment. This study aims to answer three questions. First, can we systematically identify the signals of abandoned projects using computational methods? Second, can the degree of abandonment be quantified? And third, what features are more relevant than others when identifying instances of abandonment?   Methodology A complete listing of research projects in the Digital Humanities does not exist. However, the Alliance of Digital Humanities Organizations publishes a Book of Abstracts after each Digital Humanities conference as a PDF. Each one of these volumes can be treated as a compendium of the research that is carried out in the field. To create a dataset, we downloaded the Books of Abstracts from 2006 to 2018. Then we proceeded to extract the text from these documents using Apache Tika  (Apache Software Foundation, 2018) and parse the unique URLs for each Web resource using regular expressions.   Then we periodically created a set of WARC files  (International Organization for Standardization, 2017) for each resource using Wget  (Free Software Foundation, 2018). The WARC files are systematically processed and analyzed using Python  (van Rossum, 1995) and Apache Spark  (Apache Software Foundation, 2017) to create a hash that represents their contents —pinpointing changes over time— and to extract the analytics that we used in our statistical analysis. More specifically, our analysis has two parts that incorporate the retrieved HTTP response codes, number of redirects, a detailed examination of the contents and links returned by traversing the base node, external resources, HTTP headers and linked files. Figure 1 shows the workflow that we used in our framework to quantify the sings of abandonment.     Figure 1: Workflow to quantify sings of abandonment in Online Digital Humanities Projects First, we carried out a preliminary classification of the websites into two groups depending on their correctness according to their HTTP response codes: valid (responses in the 200 and 300 range) and decayed (all other response codes). If a Web resource reports more than one redirect, we placed it in the decayed category. This is a preliminary classification because a Web resource could return an HTTP response code implying correctness while showing erroneous content  (Meneses et al., 2012: 404) —justifying the second part of our analysis where we cluster the contents of each Web resource in the valid category. We perform the clustering using topic modeling and Term frequency–Inverse document frequency (Tf-Idf).   The textual contents and the links associated with shared resources are the most obvious feature for clustering. Previous work has shown that shared resources are the first to disappear from the Web  (SalahEldeen and Nelson, 2012) —which we interpret as premature indications of degradation.  To detect these early signs, we generated topic and term frequency models to examine the similarity among the documents in a given project (the contents of the base node and the metadata and the contents of the child nodes). We used Latent Dirichlet Allocation (LDA) to model the content of the text  (Blei et al., 2003)  and a simple Tf-Idf ranking function to measure and compare them. This ranking function is based on adding the Tf-Idf values for the documents linked to a Web resource, which were calculated using the terms from the topic modelling as a vocabulary. This combination metrics and techniques allow us to compare and assess the degree of change of online digital humanities projects over time.    Conclusions In this this study we aim to computationally identify the indicators of the abandonment of digital humanities projects —a very specific domain— as well as quantify their degrees of neglect. It is important to highlight that not all projects are equal and thus require different levels of attention. Previous work in this area was based on the metadata from HTTP headers —emphasizing the need for a framework that utilizes robust metrics to identify the collectively shared indicators of degradation. We intend this study to be a step forward towards better preservation mechanisms and for adopting strategies for the planned obsolesce of digital humanities projects.  ",
        "article_title": "A Framework to Quantify the Signs of Abandonment in Online Digital Humanities Projects",
        "authors": [
            {
                "given": "Luis",
                "family": "Meneses",
                "affiliation": [
                    {
                        "original_name": "Electronic Textual Cultures Laboratory - University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Jonathan",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "King’s College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Richard",
                "family": "Furuta",
                "affiliation": [
                    {
                        "original_name": "Center for the Study of Digital Libraries, Texas A&M University",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ray",
                "family": "Siemens",
                "affiliation": [
                    {
                        "original_name": "Electronic Textual Cultures Laboratory - University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "sustainability and preservation",
            "data mining / text mining",
            "English",
            "computer science and informatics",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The Web can constitute a natural medium for the publication and discovery of historical evidence, their related resources, and descriptive metadata. Increasingly, but steadily, more and more historical artifacts make their way from archaeological sites to musea, libraries, digital archives, and also the Web of data (Meroño-Peñuela et al., 2015), besides their traditional proliferation to the wider public through books (Kilmer and Mirelman, 2013). The discovery and retrieval of historical digital objects and their descriptive metadata on the open data space of the Web are challenging tasks (Meroño-Peñuela et al., 2015).  For example,  “What is the oldest music score known?” is a relevant question of interest to art historians, music historians, musicologists, and the wider public. Web search can lead users to at least a partial answer to this question; as it turns out, the oldest song known left to present in written form is a Sumerian Hymn written 3,400 years ago. Part of the Hurrian songs, the Hurrian Hymn to Nikkal (also known as h.6) is the oldest substantially complete work of notated music in the world, inscribed in cuneiform on clay tablets and excavated from the ancient city of Ugarit (today northern Syria) and dated 1400 BC (Kilmer, 1971). The work of a number of historians has allowed for these inscriptions to be transcribed in modern Western notation (Duchesne-Guillemin, 1984), ultimately leading to the possibility of being played in a modern lyre, which resembles ancient Sumerian instruments.   The provenance trail of this unprecedented discovery of musical culture is complex, and this complexity manifests on the Web in the form of variety and heterogeneity. Necessary pieces of knowledge to reconstruct this breakthrough are scarce, hard to find, and, most importantly, spread through a number of heterogeneous and semantically incompatible information representation formats: HTML hypertext, PDF documents, scanned images, digital score transcriptions, MIDI files, MP3 files, etc. Therefore, the complexity of all human available Web knowledge on h.6 comes in the form of multimodality. In the broader context of digital data access and processing, we must therefore ask: how can these relevant, multimodal pieces of knowledge be queried together for further computation in a consistent and reproducible way? In this paper, and following established practice (Meroño-Peñuela et al., 2015), we propose to use the Resource Description Framework (RDF), the Linked Data paradigm, and the SPARQL query language (Harris et al., 2013) to answer fundamental questions in music history about the Hurrian Hymn to Nikkal. In order to do so, we follow a three-step approach in which, first, we convert MIDI representations of the hymn into RDF, effectively transcribing the notation of the oldest known music written score into a modern, well understood, and machine processable knowledge representation language; second, we enrich this RDF representation with all provenance metadata facts we could collect from the Web about the hymn, its discovery, and its interpretation; and, third, we store data retrieving SPARQL queries to provide unique API links that reproduce the retrieval of relevant h.6 data without knowledge of these technologies. We use the results of this methodology to study the relevance and idiosyncrasy of the hymn when we compare its notation and metadata to a large dataset of modern MIDI music of 10 billion RDF facts.  All relevant resources resulting from this approach are published online at   https://github.com/midi-ld/h.6     Background The Hurrian songs are a collection of music inscribed in cuneiform on clay tablets. These tables were originally excavated in the 1950s from the ancient city of Ugarit, a headland in northern Syria, and date to approximately 1400 BC. The Hurrian hymn to Nikkal (also known as the Hurrian cult hymn or A Zaluzi to the Gods, or simply h.6) is encoded in one of these clay tablets, and it was first transcribed into modern Western notation in 1972 (Kilmer, 1971) (see Figure 1). After this, a variety of alternative interpretations have been suggested (Duchesne-Guillemin, 1984). The proliferation of various plausible interpretations and theories on the correspondence of h.6’s content and modern music notation has led to different modern recordings. Many of these are scarcely available (Kilmer and Crocker, 1976), while some others are openly available on the Web. The use of the lyre is generally accepted as a proxy to recreate the timbre of original Sumerian instruments. Other transcriptions use the popular synthesizer language MIDI.     Figure 1. Excerpt of the Hurrian hymn to Nikkal and its transcription into modern Western music notation (Kilmer, 1971).  The Musical Instrument Digital Interface (MIDI) (MIDI Manufacturers Association, 1996) standard allows electronic musical devices to communicate by exchanging messages that can be interpreted as music notation. MIDI encodes so-called “events” into typically 3-byte messages that describe some event relevant for the production of musical sound. For example, the action of pressing the middle C key in the piano quickly can be expressed with the MIDI message <144, 60, 100> (144 is 90 in hexadecimal; 9 stands for the type of event, “note on” or “start sounding a note”; 0 for the first MIDI channel; 60 is the 60th key in the piano counting from left to right; and 100 is a measure of how hard that key is hit).  The midi2rdf algorithm (Meroño-Peñuela and Hoekstra, 2016) represents information originally encoded as MIDI in the open, standard and machine-processable knowledge representation language of the Web: the Resource Description Framework (RDF) (Cyganiak et al., 2014). RDF expresses knowledge as subject-predicate-object sentences (or “triples”). RDF triples use URIs identifiers (the same identifiers used to uniquely identify HTML pages on the Web) to indicate these subjects, predicates and objects. For example, the fact that “Tim Berners-Lee is a person” can be expressed in RDF with the triple <  https://www.w3.org/People/Berners-Lee/ > <  http://www.w3.org/1999/02/22-rdf-syntax-ns#type > <  http://xmlns.com/foaf/0.1/Person >. Therefore, the midi2rdf algorithm can be used to transform any MIDI file into a collection of such RDF triples. This has been done on a large collection of 500K MIDI files gathered from the Web, leading to the creation of the MIDI Linked Data Cloud, the largest dataset of machine-readable music notation, containing more than 10B RDF triples about MIDI songs and all their events and notes (Meroño-Peñuela et al., 2017). More recent work proposes methods that leverage musicians’ performances, MIDI similarity algorithms, and entity recognition techniques to establish links between music notation content (e.g., the MIDI notes of “Hey Jude” by the Beatles) and their descriptive Web metadata (e.g. the Wikipedia page of that song) (Meroño-Peñuela et al., 2018).    Approach and Findings Here, we propose to follow a three-step method in order to represent historical symbolic music notation content and descriptive metadata of the MIDIs of the Hurrian Hymn to Nikkal as Linked Data through RDF, and incorporate those into the MIDI Linked Data Cloud:  Symbolic music notation content. As a first step, we encode the MIDI files of the Hurrian Hymn to Nikkal in RDF, using the midi2rdf algorithm. This produces various graphs of RDF data encoding the different musical events of the digital notation. Descriptive metadata. In this step, we add additional contextual provenance and metadata triples to the RDF graphs produced in the previous step. These are harder to integrate in the historical case than in modern music, since generic approaches linking only large datasets is ineffective in this case. To address this, we investigate the resources and workflows around the creation of modern music notations of the h.6, and compile all these using newly minted URIs, shared vocabularies, and reusable modeling practices.  Reproducible queries. Since the output of the previous steps is RDF data that can only be queried through the (complex) SPARQL query language, we compile a list of SPARQL queries encoding relevant competency questions over these new RDF graphs, interrogating them using combinations of musical knowledge (derived from the MIDIs) and contextual provenance (derived from the links compiled in step 2). By doing so next to automatic API generation tools (Meroño-Peñuela and Hoekstra, 2017), we generate stable and reproducible links to execute these queries, retrieving predictable results without the need of knowledge in RDF or SPARQL.  Our findings reveal both shortcomings and advantages to this approach. The first limitation is the manual, unprincipled, and scarcely understood nature of collecting Web-based relevant provenance and contextual resources, links and workflows over these music-historical objects. Secondly, this process is prone to be biased towards the selection of resources that are only available on the Web, ignoring those published elsewhere; this aims at the general problem of reachability of offline archives and repositories. These shortcomings are balanced by a number of advantages. First, the end result supports a thorough documentation process that generates a semantic RDF graph of historically relevant and connected Web resources, posing the added value of machine-readability over more traditional text-based documentations. Second, the result supports a more principled retrieval mechanism, based on mixing musical (through the h.6 notes represented in RDF) and historical (through the supporting metadata RDF graph) knowledge under the same querying paradigm (reproducible SPARQL links). Third, the availability of both MIDI and their history enables a more exploratory approach, in which besides providing query results, the serendipitous discovery of information is incentivated through similarity links. Ultimately, we hope to enable comparative studies through graph metrics pointing differences between historical and contemporary music.  ",
        "article_title": "The Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data",
        "authors": [
            {
                "given": "Albert",
                "family": "Meroño Peñuela",
                "affiliation": [
                    {
                        "original_name": "Vrije Universiteit Amsterdam, The Netherlands",
                        "normalized_name": "VU Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/008xxew50",
                            "GRID": "grid.12380.38"
                        }
                    }
                ]
            },
            {
                "given": "Marnix",
                "family": "van Berchum",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, The Netherlands",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Bram",
                "family": "van den Hout",
                "affiliation": [
                    {
                        "original_name": "International Institute of Social History, The Netherlands",
                        "normalized_name": "International Institute of Social History",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/05dq4pp56",
                            "GRID": "grid.450142.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-19",
        "keywords": [
            "musicology",
            "digital art",
            "English",
            "computer science and informatics",
            "semantic web and linked data",
            "ontologies and knowledge representation",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This work offers a method for detecting the degree of fluency and disfluency used by a poet when reading his/her poems. Such a determination of (dis-)fluency plays a particularly important role in the evaluation of poetry translations. Lawrence Venuti showed that the notion of fluency became a dominating principle by which to judge English translations: a translation reads “fluently, when it gives the appearance that it is not translated” [1, p. 4]. This was meant critically, because such translations often transform an elliptic and fragmentary style within a source poem into a tangible, concrete and fluent target language. Facing current machine-translation systems, there can be no doubt that Venuti's critique is more on topic than ever: The fluency of the text in the target language became today's predominant translational ideal, due to so-called “speech disfluency removal systems” used in conversational speech translation [2]. To judge a good or bad translation thus means to estimate the degree of fluency within the original poem and its translation. Following Venuti's critical approach, our paper will offer a new technique to estimate this degree of (dis-)fluency with regards to poetry. In a first step, we will offer a precise framework to use it for estimating a spectrum of (dis-)fluency by using two important theories for analyzing poetry: The grammetrical ranking and the rhythmic phrasing. The idea of grammetrical ranking was developed by Donald Wesling, whose neologism “grammetrics” is a hybridization of grammar and metrics, based on the key hypothesis that in poetry as a kind of versified language, the grammatical units (sentence, clause, group, word, morpheme) and the metrical units (syllable, foot, part-line, line, rhymated pair, stanza, whole poem) interact in a way for which Wesling finds ‘scissoring’ an apt metaphor. The grammetrical raking assumes that meter and grammar can be scissored across each other [3, p. 67]. The second important approach to detect (dis-)fluencies in poems is Richard Cureton's theory on rhythmic phrasing. Cureton divided the poetic rhythm into three components: meter, grouping and prolongation [4, p. 125]. Meter is about the perception of beats in regular patterns, grouping refers to the linguistic units gathered around a single climax or peak of prominence, quite similar to Weslings ranking. Cureton's new idea, basically, is that of prolongation which refers to the anticipation and overshooting of a goal, the experience of anticipation and arrival. Rhythmic prolongation is a matter of connected, goal-oriented motion, based on three levels: anticipation (a), arrival (r), and extension (e) [4, p. 146]. For example: an extension occurs in the prosodic phrasing of an enjambment, where the line break is felt as a linear extension of the sentence before the end of the sentence is reached in the next line. Using this theoretical framework, we will establish a gradual one-dimensional continuum, whose two poles are denoted by the terms “fluent” and “dis-fluent”. We illustrate this prosodic spectrum by ranking nine different poetic styles within the free verse spectrum, starting with the most fluent one, the (1 = cadence). The basic idea of the cadence is the “breath-controlled line” as an isochronous principle. Ezra Pound, who invented this idea of the cadence, was influenced by Chinese poetry, which lacks any enjambments. This explains the so-called line-sentence as the fundamental principle of the cadence. In difference to this class, more dis-fluent poems use “weak enjambments” separating the nominal phrase and the verbal phrase of a sentence. Such “weak enjambments” can be divided furthermore into those not emphasizing the enjambments (2 = parlando), and those which do emphasize them (3 = variable foot). These two classes are also rather fluent ones, compared to those poems using “strong enjambments”. A strong enjambment separates articles or adjectives from their nouns or even splits a word across a line, like in Paul Celans poems. Poems using “strong enjambments” can also be divided into those not emphasizing the enjambments (4 = strong enjambment), and those emphasizing them (5 = gestic rhythm). Moving forward towards to the more dis-fluent pole, the next pattern is the (6 = permutation). A permutation is a conversion or exchange of words or parts of sentences or a progressive combination and rearrangement of linguistic-semantic elements of a poem, a principle that was very popular in German \"concrete poetry\". The next pattern is the (7 = ellipsis), the omission of one or more grammatically necessary phrases. This rhetorical figure can also affect the prosody of a poem, which has been observed for example in poems of Paul Celan. Even more radical kinds of poetic disfluency have been developed in modern “sound poetry” by dadaistic poets like Hugo Ball and Schwitters or concrete poets like Ernst Jandl. Within the genre of sound poetry, there are two main patterns: the (8 = syllabic decomposition), dividing the words into syllables; and the (9 = lettristic decomposition), the last and most disfluent pattern, which can be found for example in Ernst Jandl's famous poem schtzngrmm. Using this spectrum, we can very accurately mark whether a translation is more fluent than the source text. Therefor we collected German poems available on the website of our partner (www.lyrikline.org). The philologist and literary scholar of the project (first author) classified 268 of a total of  ∼ 2,400 German poems into the nine prosodic classes defined above. We also collected the corresponding audio recording of each poem as spoken by the original author, yielding a total of 52 hours of audio for all German poems. We perform forced-alignment of text and speech for the poems using the text-speech aligner published by [5] which uses a variation of the SailAlign algorithm [6] implemented via Sphinx-4 [7]. This process in spoken poetry is non-trivial (in particular for decompositions in more abstract poetry). Therefore, the alignment data are corrected on the line level (start of first and end of last word for each line) as well as checked and corrected again by an expert (second author). We present a model for the automatic classification of rhythmical patterns in the free verse poetry by using deep hierarchical attention networks. We do not use the processing on word level. Instead we used character-by-character encoding of lines in the poem and used character embeddings, sine we have a small amount of data. While processing on the word level might allow our model to build a better higher-level understanding of the poem's meaning, this semantic information would likely not help in style differentiation. In addition, word representations would not capture the usage of whitespace, for example, in indentation to create justified paragraphs or other uses, nor special characters. We use a bidirectional recurrent neural network (RNN, using gated recurrent unit (GRU) cells) which encodes the sequence of characters into a multi-dimensional representation. As for the text, we use speech line-by-line via additional encoders. We extract Mel-frequency cepstral coefficients (MFCC) for every 10 milliseconds of the audio signal as well as fundamental frequency variation (FFV) vectors, which are a continuous representation of the speaker's pitch. We z-normalize all feature dimensions. We compute the mean and standard deviation of 10 consecutive frames for every feature. To satisfy the requirement of inspectability of the decision making process, we implement a notion of inner attention that is to learn how to combine the sequential states of each line encodings (text, audio, and pause between lines) to a representation that is best suited towards our training objective. We combine the line-by-line representations using a poem-level encoder which is fed to a decision layer and a final softmax to determine the poem's class. Our model is implemented in dyNet and python. Since there are a broad variety and relatively a small number of poems. We implement the pre-training with additional data from German Text Archive [8]. We used the pre-trained models in the training procedure. We first leave out the poem-level encoding and directly pass each line representation to a line-by-line decision layer. Afterwards, we replace the line-by-line decision layer with the poem-level encoder and final decision layer and train towards the per-poem decisions based on the parameters estimated before. Thus, the final model is able to steer its attention mechanism towards the important lines and can learn to sacrifice the initially trained per-line optimization for the overall per-poem optimization. Each encoder is two layers deep and has a 20-dimensional state. We train a classifier to distinguish the nine classes of poetic styles with all features (text, speech, and pause) using pre-processing and pre-training; given the little available data, we use 10-fold cross-validation (090% training and 10% test data). The best result, calculated by the average F-measure (weighted by class size), for the classification of the nine rhythmical patterns is 0.62. This indicated that it is indeed possible to check Venuti's critique of fluid translations automatically by distinguishing prosodic classes based on text, speech, and pauses using a deep neural model.  ",
        "article_title": "From Fluency To Disfluency: Ranking Prosodic Features Of Poetry By Using Neural Networks",
        "authors": [
            {
                "given": "Burkhard",
                "family": "Meyer-Sickendiek",
                "affiliation": [
                    {
                        "original_name": "Freie Universität Berlin, Germany",
                        "normalized_name": "Freie Universität Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/046ak2485",
                            "GRID": "grid.14095.39"
                        }
                    }
                ]
            },
            {
                "given": "Hussein",
                "family": "Hussein",
                "affiliation": [
                    {
                        "original_name": "Freie Universität Berlin, Germany",
                        "normalized_name": "Freie Universität Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/046ak2485",
                            "GRID": "grid.14095.39"
                        }
                    }
                ]
            },
            {
                "given": "Timo",
                "family": "Baumann",
                "affiliation": [
                    {
                        "original_name": "Carnegie Mellon University, Pittsburgh, USA",
                        "normalized_name": "Carnegie Mellon University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05x2bcf33",
                            "GRID": "grid.147455.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-28",
        "keywords": [
            "digital humanities (history",
            "multimedia",
            "artificial intelligence and machine learning",
            "natural language processing",
            "theory and methodology)",
            "English",
            "literary studies",
            "audio",
            "video",
            "speech processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We often think about the placement of a particular word in its context in a poem. Constraints of syntax, meter, and rhyme interact with choices about emphasis. It is more difficult to compare an instance of a word to other instances of the same word across a collection. Which words appear early in poems and which late? Which words occur at the beginning of lines, and which at the end? This work presents a method for uncovering the average positions of words. We consider a collection of 10,000 16th to 20th century English-language sonnets from the Chadwyck Healey  English Poetry  collections, whose primary bibliographic source is the  New Cambridge Bibliography of English Literature (1969-72). Although limited to published poems (not manuscript poems or those published in newspapers, broadsides or journals), and skewing towards the canonical, the texts are accurate and offer a sizeable sample of poetry published during this period.   We create a series of two-dimensional views, one for each distinct word. Imagine that each poem has been stretched to a fixed, square shape, with each line fully justified left-to-right. The first word of the first line sits at the top left of the square, and the last word of the last line sits at the bottom right corner. The following figure illustrates this process using Shakespeare Sonnet 91.  Figure 1: For each word, we find the location of all instances of the word in all poems, after stretching all poems to a standard, idealized square.  In the first panel, we highlight all seven instances of the word  some. The second panel shows a reduced view, showing only a symbolic representation of those words. The third panel shows the final aggregate view, with all instances of the word  some from all 10,000 poems, with a slight random jitter to show density. This corpus-level view surfaces patterns:  some appears frequently at the beginning of lines and rather often about two thirds of the way through a line, but rarely at the end. The word occurs slightly more frequently in the second half of poems, but the primary pattern is left-to-right, not top-to-bottom.   The layered visualization reveals surprisingly deep relationships between metrics and meaning. The pattern of  some across the sonnet is both conceptual and functional: as a single-syllable word without a strong set of phonetic or graphic rhymes, the word is less valuable at the end of lines, while its conceptual flexibility allows it to be placed in either a stressed or unstressed position depending on its emphasis. As such, its utility in either establishing iambs or disrupting them (in, for example, trochaic inversions) is equally as likely to place it near the beginning of the line as its measured indeterminancy as either a pronoun or adjective.   The Sonnet Stretcher provides a comprehensive, abstracted view, yet requires few theoretical commitments. Though scholars such as Natalie Houston have performed distant reading on a corpus of poems from the Victorian period, focusing in particular on Elizabeth Barrett Browning in her recent paper “a distant reading of EBB’s rhymes,” her work focuses only on end-rhyme and not the way that certain words might appear more frequently in internal rhymes, at the beginning of poems, or throughout the poem (Houston, 2014). Poemage attempts to show all sonic elements of a single poem in a visual form, and presents a tremendous amount of information that many scholars disagree on, including assumptions about rhymes and metrical “perfection” (as in “a perfect iamb”) (McCurdy et al. 2016). Programs modeling poetic meter are now quite common, from Ryan Heuser’s Prosodic to programs for Russian poetics (Thorsen and Birnbaum, 2017), Medieval High German poetry (Hench and Estes, 2018), English and Italian (Delmonte 2014), Bengali, Sanskrit (N. and Lakhshmanan, 2010).    Figure 2: A sample of selected words illustrating typical patterns. Scholars have long wrestled with whether, how, and why algorithmic tools and visualizations might aid in the reading of poetry. After all, scansion itself (the series of diacritical marks that appear above a word to indicate a stressed or unstressed syllable in traditional accentual-syllabic scansion) is a form of visualization. But visualization tools can only do so much with prosody; prosody is pronunciation and versification, and though there are rules to both pronunciation and versification there are also hundreds of variants and exceptions to those, not to mention disagreements. It is for this reason that the carefully rendered visualizations of these word positions in The Sonnet Stretcher are so evocative. The Sonnet Stretcher returns a crucial visual aspect of poetry frequently lost in the digitized reproduction of the text: the position of the words on the page. From Herbert’s “Easter Wings” to Plath’s “Bell Jar,” the poem’s spatial arrangement – what words can appear where – remains a crucial axis of meaning. Not only do these visualizations appear as conglomerate poetic texts in their own right, creating a new poetic object for observation and critical commentary, but they both confirm and complicate several commonplaces about rhyme and metricality by the very fact of this accumulative effect. That is, scholars of poetry take for granted the ways that their reading practices, shaped by education and the memorization of the very rules we mention above, impact or bias their approaches to a poetic text. With this tool, scholars are evocatively challenged to think through these assumptions.  The visualization reveals both presence and absence. We are not surprised to see that  apart,  start, and  heart commonly appear as end-rhymes, with  heart appearing more often throughout the line than the other two. But what of  summer, which unlike  winter, begins in the second and penultimate positions most often (whereas  winter,  spring, and  fall are resolutely at the end);  summer seems to have overtaken  spring as having both a beginning and an end. The evocative emptiness of  farther begs to be read as a text on its own right, showing a constellation of positions with no adherence to one or the other, much like  floating, which hovers across the cumulative sonnet image. Similarly,  rich texturally imitates the word’s meaning in its stretched form. The interpretive possibilities for reading the visualizations are myriad, and this paper will seek to think through what we might learn from position outside of the realm of rhyme (though of course rhyming-end words are impossible to ignore).  ",
        "article_title": "The Sonnet Stretcher",
        "authors": [
            {
                "given": "David",
                "family": "Mimno",
                "affiliation": [
                    {
                        "original_name": "Cornell University",
                        "normalized_name": "Cornell University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05bnh6r87",
                            "GRID": "grid.5386.8"
                        }
                    }
                ]
            },
            {
                "given": "Meredith",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "Princeton University",
                        "normalized_name": "Princeton University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hx57361",
                            "GRID": "grid.16750.35"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-11",
        "keywords": [
            "data mining / text mining",
            "literary studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Political terror in the USSR has remained a painful theme for Russian society. In academic and public history different waves of terror and controversial official statistics of victims are being widely discussed. Many documents that accompanied terror operations and even some investigative cases remain classified in archives, and almost 30 years after collapse of the USSR, we still know only 20% of names of political terror victims, i.e. about 3 million real names. These data are mentioned in different “Books of Memory”, created in almost every region of the former USSR and consist of short biographical cards on every victim known. All data of “Books of Memory” are collected in unified database made by International society “Memorial” (http://base.memo.ru/). It’s a SQL-based large dataset of victims, which updates every few years and support only Russian language. We initiated the creation of another database – “Open list” – to help “Memorial” collect names and eradicate mistakes from their data.   \"Open list\" wiki-like open dataset (https://openlist.wiki) is created on the basis of very heterogeneous and diverse dataset by International “Memorial”. That is the only unified dataset on political terror victims in Russia from 1917 to 1991 containing more than 3.1 million records and which has four pages on national languages: Russian, Ukrainian, Belorussian and Georgian. A data card of each wiki-page is unified for any of national subsections of the project. Some historical sources were not included in “Memorial” database, names and bios from these sources are mentioned only in “Open list”. The main advantage of “Open list” is that people can add new names and correct already existing information online. “Open list” updates daily. Users can edit pages via special form with user-friendly interface with 31 fields for personal data and description of arrests, or use wiki-markup to add fields mentioned above manually and upload files on pages. Editors are to verify all crowd-sourced data manually using documents and files that people provide when making their corrections. Most useful documents are digital copies from investigative cases or rehabilitation certificates. If a user cannot provide any file, page remains unapproved with special disclaimer on it.    Crowdsourcing is a very important part of the project. There are several possible activities for our users: they can add new people to the list, parse data from biographies to fields in biographical form or add templates such as \"repressed relatives\" using inter-wiki to link pages of persons from a kinship family. The special algorithm automatically defines potential relatives according to similar surnames, patronymics and rehabilitation dates (some kind of record linkage approach). We also have tools for our users to identify and unite duplicate pages. It usually requires much historical knowledge to identify description of two different arrests correctly, and the result of such research is almost always manually checked by editors. There are only few people who work with duplicate pages while we have about 100 thousand duplicates, so we need ideally automatic algorithms to merge them. These algorithms vary depending on the quantity of data on the particular page and mentioned type of repression. The simplest method is to compare a full name, a birth year and a birth place, but here additional parameters are required. If we need to find one person in two different historical sources, we add dates of arrest and conviction. In case of possible mistakes in full names and dates of birth we use full coincidence of biographical data in primary sources. Historians provide these algorithms and IT specialist realize them on Python.   Advanced search of “Open list” contains 21 search fields; all text fields allow using logical operators. Users can gain a long list of personal pages by request. The search is not strict and sometimes it shows more pages than were requested. Data visualization is now possible only on certain types of information like dates of birth, or arrest, or conviction, so text fields should be normalized in close future. This is also one of the project goals to make analysis of these data easier and persistent. \"Occupation\" is one of the most difficult fields for normalization. We use classification made by historians on materials of All-Union 1937 and 1939 censuses and NKVD internal instructions for classification of occupations. HISCO is not in use on this step of work because it seems not suitable for linking occupations to Soviet social stratification in the 1930s, as this linking is a necessary step for analyzing social portrait of terror victims. Also such kind of work with HISCO has never been done on materials of early Soviet period. Now we are only preparing for normalizing data and will do most part of work automatically.   As the database grows, we can use it for academic needs and deepen knowledge of political repression in the USSR through different ways. “Open list” provides an opportunity to make samples of data and construct a social portrait of terror victims. Pages with templates could be objects of network analysis as well as investigative cases published on some pages. Massive of biographies could be a source to study family history. We can also analyze geography of terror using field “place of living”, which in some sources contains concrete addresses. “Open list” itself conducts an archival work in cooperation with Russian state archive on compiling the united electronic “Book of memory” of Moscow and Moscow region. This work consist of two parts. Firstly, digitizing the materials of archival investigative cases is taking place. Secondly, the crowdsourcing takes the floor: our volunteers decrypts information from archival documents and create new pages in a list or contribute to existed. They use the instruction provided by professional historians based on principles of source-study of investigative cases. Editors verify all new records in electronic books. Thus, new historical source with names that had never been mentioned before is emerging online on “Open list” web site. This project helps not only to supplement the list of names, but also correct mistakes in “Books of Memory”.  ",
        "article_title": "\"Open List\": How to Collect Primary Data on Soviet Terror",
        "authors": [
            {
                "given": "Ekaterina",
                "family": "Mishina",
                "affiliation": [
                    {
                        "original_name": "Lomonosov Moscow State University, OpenList.wiki, Russian Federation",
                        "normalized_name": "Lomonosov Moscow State University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/010pmpe69",
                            "GRID": "grid.14476.30"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-12",
        "keywords": [
            "digital humanities (history",
            "crowdsourcing",
            "theory and methodology)",
            "English",
            "public and oral history",
            "databases & dbms",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  1.   A number of lexical diversity measures have been proposed and applied in stylometric studies (Tweedie & Baayen,1998). Covington et al. (2015) introduced three stylometric measures including moving-average type-token ratio, or MATTR (Covington and MacFall, 2010). They applied the measures to classify ten English translations of the Gospel of Mark.   The study focuses on decision tree models based on several measures of lexical diversity, aiming at classifying genres of authorship attribution and critical types in various editions of the Greek New Testament. We extract measures of lexical diversity that do not significantly correlate with tokens and investigate specific indices that highly contribute to the performance of discriminant models. After creating training and test subsets from ten editions, we apply two classification algorithms such as Classification and Regression Tree (Breiman et al., 1984) and Random Forest (Breiman, 2001). We then figure out the classification accuracy with the token-independent measures.   For all that the aim of the study is to classify genres of authorship attribution and critical types in various New Testament editions, we do not simply pursue higher accuracy of classification per se, especially in the edition types. We are rather focusing on the characteristics of misclassified texts and edition types. Before comparing contents among the editions, we try to identify the measures of lexical diversity contributing to classification according to purposes such as authorship and edition types in this case.   2. Methods   2.1. Data   In this study, we focus on the top ten longest books over 4000 tokens in the New Testament: the four Gospels, Acts, Romans, the first and second Epistles to the Corinthians, Epistle to the Hebrew and Revelation.   Table 1 shows the list of ten books with its abbreviated name, genres and authors. The author’s names followed the general consensus in the biblical studies. We distinguish the authors between the Gospel of John and the Revelation and we do not specify the name of the author the Epistle to the Hebrew.    Book Genre Author Stephanus Nestle-Aland      Tokens Types Tokens Types   Matthew Gospel Matthew the Evangelist 18769 4281 18348 4190   Mark Gospel Mark the Evangelist 11656 3128 11306 3005   Luke Gospel Luke the Evangelist 19949 4977 19490 4858   John Gospel John the Evangelist 15942 2878 15641 2809   Acts Acts Luke the Evangelist 18814 4927 18455 4846   Romans Epistle Paul 7220 2121 7111 2086   1 Corinthians Epistle Paul 6941 2084 6832 2055   2 Corinthians Epistle Paul 4499 1499 4478 1484   Hebrews Epistle Unknown 5016 1922 4955 1896   Revelation Revelation John of Patmos 9975 2301 9857 2218    Table 1: List of top 10 longest books in the New Testament   The ten editions we selected can be divided into three types, such as the so-called Received Text (\"Textus Receptus\"), critical edition and Byzantine Textform.   Table 2 shows the list of ten editions of the Greek New Testament that are used in the study. Most of the texts are obtained from the electric texts in two biblical software such as Bible Works 9.0.12 and Accordance 11.2.5. The last column of the list represents the electric version of the text.   We apply 100 samples in total (10 books × 10 editions) to discriminant analyses. The samples of these editions are randomly distributed into training and test subsets.    Edition Date Type  Electric Text Vers.    Stephanus (R. Etienne) 1550 Textus Receptus 4.8   Tregelles 1857-1879 Critical Text 1.0   Tischendorf 8th ed. 1869-1872 Critical Text 2.7    Westcott-Hort  1881 Critical Text 2.7   Scrivener 1894 Textus Receptus -    Von Soden  1902-1910 Critical Text 1.0   Robinson-Pierpont 2005 Byzantine Textform 2.8   Nestle-Aland 28th ed. 2012 Critical Text 2.0   BGNT 2014 Byzantine Textform -   Tyndale House 2014 Critical Text -    Table 2: List of the Greek New Testament editions   2.2. Measures of Lexical Diversity   We use measures of lexical diversity as categorical variables for classification. First of all, we calculated 16 measures including basic indices such as types, tokens and punctuation using the koRpus package (Michalke, 2018) in R version 3.5.1.   Fig. 1 shows the Correlation Coefficients of each measure of lexical diversity with Tokens. We extract measures with no significant correlations with tokens at the 0.05 level of p-value. In this way, the following the eight measures are going to use the classification: Punctuation Ratio, Yule’s K, MATTR, Dugast’s U, Maas, Somers, MTLD, and HD-D.      Fig. 1: Correlation Coefficients with Tokens   2.3. Classification   We apply two classification algorithms to classify authors and edition types in the Greek New Testament. One algorithm is Classification and Regression Tree, or CART and the other one is Random Forest, or RF. Discriminant analyses are performed in R 3.5.1 using the rpart package (Therneau, 2018) for the CART algorithm and the randomForest package (Liaw, A.& Wiener, M., 2002) for RF.   For CART tree models, trees are split based on the Gini index and the values of variable importance are recalculated so that the sum total becomes 100. The minimum number of observations is set to 3. We prune trees based on the cost-complexity parameter and cross validated error results, if necessary. Meanwhile, the minimum size of terminal nodes in RF is varied to optimize the classification accuracy.   The samples of the New Testament editions are randomly distributed into 50 training and 50 test subsets. Generating decision tree models from training samples, we apply the trees to test datasets and then examine the classification results. We also observe the discriminant measures of lexical diversity.   The breakdowns of datasets are shown in Table 3 for authorship and in Table 4 for classification of edition types. The names of each authors referred to the third column in Table 1. The letter \"E\" of \"E_(Mark/Matthew/Luke/John)\" stands for the Evangelist.     E_Mark E_Matthew E_Luke E_John Paul Unknown John of Patmos   Training 4 5 10 4 16 6 5   Test 6 5 10 6 14 4 5   Table 3: Breakdowns for authorship classification     Byzantine Textform   Textus Receptus  Critical Text   Training 9 11 30   Test 11 9 30   Table 4: Breakdowns for classification of edition types  3. Classification Results   3.1. Authorship   First, we apply CART tree models to classify authors. Fig. 2 shows the variable importance in CART classification. Although MATTR indicates the highest score among eight measures, Dugast’s U can be also regarded as important index for discrimination. There are many other relatively important measures such as Maas, Yule’s K and MTLD. Pruning trees was not needed in this model.       Fig. 2: CART's Variable Importance     Fig. 3: RF’s Variable Importance    E_Mark E_Matthew E_Luke E_John Paul Unknown John of Patmos   E_Mark 6 0 0 0 0 0 0   E_Matthew 0 5 0 0 0 0 0   E_Luke 0 0 10 0 0 0 0   E_John 0 0 0 6 0 0 0   Paul 0 0 0 0 12 2 0   Unknown 0 0 0 0 0 4 0   John of Patmos 0 0 0 0 0 0 5    Table 5: CART Classification (Accuracy: 96.0%)   As shown in Table 5, two Pauline Epistles were misclassified as the author of Epistle to the Hebrews and the accuracy is 96.0%. Meanwhile, the RF algorithm classified with 100% accuracy. Fig. 2 plots Mean Decrease in Gini coefficient equivalent to variable importance in CART. Maas and Yule’s K highly are considered to contribute to the authorship classification.   3.2. Edition Types   In the CART classification, we pruned the tree where the value of complexity parameter is 0.17. Fig. 4 shows the variable importance in CART. Punctuation Rate indicates exclusively the highest score and can be regarded as the most important index for discrimination. As shown in Table 6, all editions of Byzantine textform were misclassified into both Textus Receptus and Critical Text. The classification accuracy in CART is 60.0%, while that in RF is slightly higher: 62.0% shown in Table 7, where minimum size of terminal nodes is set to 10. As shown in Table 5, the constituent ratio of Mean Decrease in Gini coefficient is very similar to that in CART in Fig. 4: distinctive importance of punctuation rate.      Fig. 4: CART's Variable Importance      Byzantine Textform   Textus Receptus  Critical Text    Byzantine Textform  0 3 8    Textus Receptus  0 5 4   Critical Text 0 5 25    Table 6: CART Classification (Accuracy: 60.0%)     Fig. 5: RF’s Variable Importance     Byzantine Textform   Textus Receptus  Critical Text    Byzantine Textform  0 3 8    Textus Receptus  0 4 5   Critical Text 0 3 27    Table 7: RF Classification (Accuracy: 62.0%)   4. Discussion   The token-independent measures of lexical diversity can be distinguished by Pearson correlation coefficients of each measure with tokens. Punctuation rate is exclusively crucial when classifying edition types, while the others are effective in the authorship classification. In both authorship and edition types, the classification accuracy in RF is higher than that in CART. For all that edition types were poorly discriminated against, that does not indicate the limitations of the techniques. We will rather focus on the misclassified texts, especially the editions of the Byzantine textform, to work out these peculiar characteristics.  ",
        "article_title": "Applying Measures of Lexical Diversity to Classification of the Greek New Testament Editions",
        "authors": [
            {
                "given": "Maki",
                "family": "Miyake",
                "affiliation": [
                    {
                        "original_name": "Osaka university, Japan",
                        "normalized_name": "Osaka University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/035t8zc32",
                            "GRID": "grid.136593.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "theology and religious studies",
            "stylistics and stylometry",
            "authorship attribution / authority",
            "theory and methodology)",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  1 Historical linguistics and the Americas Minority, non-European languages — such as indigenous American ones — are critically under- represented in the literature on language change. This not only narrows our view of the historical interaction of peoples and languages predating European expansion, but also limits our under- standing of language change as a whole. In the absence of the hundreds of years of philological study available for Old World languages, digital methods emerge as ideal means for systematically compiling and exploring the available data for historical languages in the New World. This said, carefully tagged, historically-oriented corpora for Native American languages are still few and far between, despite the general growth in Digital Humanities scholarship throughout the region.   2 Mapudungun, a prime candidate Mapudungun (arn, ISO:639-3), the ancestral language of the Mapuche people of present-day Chile and Argentina, has been fairly well documented for over 400 years. While the types of available historical material for the language are fairly typical — including missionary, military and ethnographic works (see Villena 2017: for an overview) — the historical depth, as well as the number of texts available for Mapudungun is better-than-average for the region. Current Mapudungun varieties are mostly well described and remain in use, nevertheless, there is very little explicit work on their history. This availability of historical and contemporary data, coupled with a gap in historical research, makes Mapudungun an excellent candidate for a first corpus-based historical account of a language of the Americas. The building of such a resource, The Corpus of Historical Mapudungun (CHM), is the focus of an ongoing research project at the Angus McIntosh Centre for Historical Linguistics, Edinburgh, and the topic of this talk. Beyond the empirical and methodological advantages to working with historical data for Mapudungun, there are also theoretically interesting reasons to do so. Features such as nominal incorporation, verb serialisation, agglutination and polysynthesys raise interesting questions about the diachrony of units of sound and meaning, which cannot be probed by better-studied, yet typologically distinct languages. Overall, the rich word-internal morphology of Mapudungun challenges traditional domains for sound change. In particular, the fact that morphological transparency is paramount to the language’s system, means that there is little indication of word-level reduction and neutralisation processes, which are key to the study of sound change in Indo-European languages (Molineaux 2017, 2018). These facts of Mapudungun will be key both to our analysis of change in the language and to the practicalities of corpus-design.   3 Building the corpus The majority of texts included in the CHM are printed material dated between 1606 and 1930, making up some 400k words (see  https://benmolineaux.github.io/bookshelf/). Using both archival images and newly digitised materials from the  Biblioteca Nacional de Chile and the  Archivo Rodolfo Lenz in Santiago, Chile, the relevant texts have undergone optical character recognition. This was done via the  Digital Humanities Dashboard (Tarpley 2018) and followed up by hand-checking. The result is a collection of digital text covering the first 324 years of written Mapudungun. While the texts are gathered from all major language areas, they inevitably remain imbalanced in nature — both temporally and spatially —, as tends to be the case for historical corpora.  As the objective of the corpus is to provide a view into the synchrony and diachrony of lexical, morphological and phonological features, texts are being parsed at all three of these levels. The first stage of this process — lemmatisation — identifies the key root-elements, as well as the part-of-speech (POS) category for each word, providing a single identifiable label and reducing both morphological and spelling heterogeneity (see 1). (1)    Form Lemma Translation POS   a.  ⟨kude-kefuingu ⟩   kuden  ‘to play’  V   b.  ⟨kuthe-kalape ⟩   kuden  ‘to play’  V   XML  <w xml:lang=“arn” lemma=“kuden” pos=“V” corresp=“play”>kudekefuingu</w>  <w xml:lang=“arn” lemma=“kuden” pos=“V” corresp=“play ”>kuthekalape</w>  The second stage is morphological parsing, which identifies individual morphemes beyond the root and labels them according to function (as in 2). The result of both these processes is a TEI-standard XML text with the relevant tags embedded. A full 10% of the total word-types in the corpus texts is soon to be completed, after which an AI algorithm, developed at the University of Edinburgh’s NLP Group, will be trained to tag the remainder of the material both at the level of the lemma and the morpheme. Additional hand corrections will be necessary in order to complete the process. (2)    Form Morphemes   a.  ⟨kude-ke-fu-ingu ⟩   root(play)- habit-broken.implicature-ind.3.dual    b.  ⟨kuthe-ka-la-pe ⟩   root(play)- cont-neg-imp.3.sg    XML  <w><m baseForm=“kude” type=“root” corresp=“play”>kude</m><m baseForm=“ke” type=“habit”>ke</m><m baseForm=“fu” type=“BI”>fu</m><m baseForm=“ingu” type=“ind.3.d”>ingu</m></w> <w><m baseForm=“kude” type=“root” corresp=“play”>kude</m><m baseForm=“ka” type=“cont”>ke</m><m baseForm=“la” type=“neg”>la</m><m baseForm=“pe” type=“imp.3.sg”>pe</m></w>  The final stage of the tagging will be grapho-phonological parsing (Kopaczyk et al. 2018), which entails providing sound values for each word (as in 3), following a list of spelling-based rules for each text. The results should effectively reconstruct the phonic structure of each text, such that it can be compared with others from different periods and locations, helping to map phonological change from the bottom up. The front end of the corpus — soon to be available in beta form — will provide search options (in both English and Spanish) at all three levels of tagging (word, morpheme and sound), as well as allowing users to correlate these features across texts and with relevant non-linguistic metadata such as date, location, author, genre, etc. A simpler browser version will also be available for non-linguists, allowing for texts to be browsed and downloaded with parallel translations.  (3)     Form Sound Lemma Translation Source Dialect   a.  ⟨vúta ⟩  [vɨta] fücha  ‘old/big’ Valdivia 1606 North   b.  ⟨fücha ⟩  [fɨʧa] fücha ‘old/big’ Augusta 1916 Central/Coast   XML  <m><c ipa=“v”>v</c><c ipa=“ɨ”>ú</c><c ipa=“t”>t</c><c ipa=“a”>a</c></m>  <m><c ipa=“f ”>f</c><c ipa=“ɨ”>ü</c><c ipa=“ʧ”>ch</c><c ipa=“a”>a</c></m>    4 Applications In this talk I will give example applications of the CHM to language change, looking into (a) the spread of Quechua lexical borrowings and (b) the evolution of morpheme-boundary epenthesis. More generally, however, the CHM paves the way for the application of digital methods to the history of minority, non-standard languages, creating transferable tools, and foregrounding under-studied typological features. Such outcomes will broaden our understanding of language change overall, by allowing a detailed view of the interaction of the lexicon, morphological structure, and sound systems over time and space. Locally, the project will provide teachers, learners and advocates of Mapudungun with a repository of words in their historical usage and forms, which may be used to enhance word-building strategies, revitalise dialectal vocabulary and more generally improve transmission, both through written and spoken medium.  ",
        "article_title": " The Corpus of Historical Mapudungun: Digital Tools for New-World Language Change ",
        "authors": [
            {
                "given": "Benjamin Joseph",
                "family": "Molineaux",
                "affiliation": [
                    {
                        "original_name": "The University of Edinburg, United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "linguistics",
            "English",
            "indigenous studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "     The categorization of games into genres is one of the more complex issues in game studies (Apperley 2007; Clarke et al. 2017; Clearwater 2011; Doherty et al. 2018; Wolf 2001). Some of the most widely recognized genres, such as “Platformer”, “Beat ‘em up”, “Shooter” or “Role Playing Games” (RPG), have their roots in the renaissance of the medium in the late 80ies (Newman 2004). However, the explosive growth of the last decades in the video game industry and the community surrounding it, is also mirrored by a wild growth of video game forms and styles. As a creative as well as billion-dollar industry, developers end up copying or emulating elements of each other’s works for artistic as well as financial reasons.     As is the case in the production and consumption of other forms of (entertainment) media (e.g. Giltrow and Stein 2009; Grant 2007), a video game’s classification is a topic of much debate among creators, critics, and consumers — see for example how and why (not) Assassin’s Creed: Odyssey is an RPG from the perspective of a   Reddit thread   (Anonymous 2018), a   critical appraisal   (Grayson 2018), and the game’s developer   Ubisoft   (2018). Perhaps the underlying reason for this is how games come to be defined through the experiences of its players (Anthropy and Clark 2014).   In short, a video game genre is an example of a personal, social, cultural, and technological construct that cannot be captured by strict boundaries, but arises from the complex interplay of interactive, multi-media elements in sets of games and their appraisal by the community at large.     Language Games with Steam Games  This in itself is not a new idea.   The complex classification of games was already used as a discussion of language games and generalities in language by Wittgenstein in his  Philosophical Investigations  (1953) .  In Statement 67, he examines the commonality of (analog) games through a comparison of the elements of individual examples and concludes: ‚[W]e see a complicated network of similarities overlapping and criss-crossing: sometimes overall similarities, sometimes similarities of detail. I can think of no better expression to characterize these similarities than „family resemblances.“ ‘    This paper will provide the results of an ongoing project that puts Wittgenstein’s concept of game families into practice as a way to explore the complexity of genre in this medium. To this end a similarity network has been created with data drawn from the industry’s leading digital distribution platform, Steam (Valve 2003). Steam uses Steam Tags as a crowd-sourced recommender system. The system allows Steam 125 million users to tag games describing an element of a game they played. Examples of such tags include overall terms, such as Indie, RPG, or Action, but can also be relatively specific, such as “Story Rich”, “Historic”, or “Choices Matter”. It bears pointing out that, like any database created in and through public discourse, these tags should not be considered to be an objective description of the game’s contents. Furthermore, which tags can be used to apply to games is curated by Valve — after a brief time in which people could create any tags they wished, a trial predictable outcomes. These tags are a first step to a computational and network scientific-driven understanding of the idea of game families. Any classification system arising from this can then be cross-referenced versus existing ideas or potential other computational genre approaches.   Using the public SteamSpy API (Galyonkin 2018), 342 different tags applied to 23985 games on Steam (at the time of collection on 23 and 24 July of 2018) were collected in a SQL database, including not only if they were applied to a game but also how many times.     Tag Networks as Game Families  This data provides the basis for a network science approach to genres in the form of a two-mode (game-to-tag) network (Borgatti and Everett 1997; Brandes et al. 2013). Such two-mode networks can be transformed to either game-to-game or tag-to-tag affiliation networks and have been used in a wide range of humanity contexts, including the study of stylistic diversity (Mol 2014). Game-to-game networks would themselves be interesting for comparing overall similarities of games — and are used by Steam to suggest new games to consider for potential buyers based on their previous purchases— but will fall outside the scope of this paper.     The tag-to-tag network can then be used to explore game families and genres using network community detection algorithms, such as Louvain Modularity (Blondel et al. 2008). To finalize this short paper and illustrate ongoing work, two case-studies can be visually explored (Figures 1 and 2): one with all tags of games that are also tagged as “historical” and another highlighting Steam’s 100 best-reviewed games.    While families of games can be detected in this tag-database, they may only make sense in combination with an advanced understanding of a video game corpus and community. This is relatively straightforward in the case of historical games, but is less useful if the subset of the corpus itself is fuzzily defined. For example, in the case of the best-reviewed games network, it does not necessarily present the ingredients for making a universally good game — unless one is interested in making Japanese romantic visual novels or other games with small but supportive communities. Preliminary results indicate that, as Wittgenstein predicted, game families as networks are better at capturing the multi-stranded nature of this media form than the more monolithic genre classification, especially when working with subsets of games. Next steps would be to check the robustness of these models by including similarity indices or votes as link weights. Still, the notion of game genre is unlikely to be abandoned soon, as anyone familiar with the internet’s collective fascination with language games will surely agree on.     Figure 1: The tags in this network all occur in games that have also been tagged as ‘historical’. Width of links in this network indicates how often tags co-occur together (e.g. are found in the same game). The color of nodes is based on a Louvain modularity measure and show what family of historical games they belong to. These can be broadly characterized as Strategy (image from  Sid Meier’s Civilization 6), Action-Adventure (image from  Assassin’s Creed: Odyssey), and Shooters (image from  Battlefield 1)     Figure 2: The tags in this network all occur in games that have the top 1% user reviews on Steam at the time of data collection (with more than 100 reviews). Width of links in this network indicates how often tags co-occur together (e.g. are found in the same game). The color of nodes is based on a Louvain modularity measure and show what types of families . There is a large amount of variability in this family, which includes (from topleft, clockwise) e.g. puzzle games (image from  7 Wonders: Magical Mystery Tour), retro platformers (image from  After Death), cooperative survival games (image from  The Eden Project) and Japanese visual novels (image from  The Eden of Grisaia).    ",
        "article_title": "Gaming Genres: Using Crowd-Sourced Tags to Explore Family Resemblances in Steam Games.",
        "authors": [
            {
                "given": "Angus A. A.",
                "family": "Mol",
                "affiliation": [
                    {
                        "original_name": "Leiden University, Netherlands, The",
                        "normalized_name": "Leiden University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/027bh9e22",
                            "GRID": "grid.5132.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-09",
        "keywords": [
            "crowdsourcing",
            "games studies",
            "English",
            "network analysis and graphs theory",
            "philosophy",
            "software studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In this paper, we present a strategy for the integration of existing heterogeneous language resources such as texts and dictionaries by connecting these resources and making them available for internal projects and third-party applications through (Web) APIs. We describe our approach in the context of the C-SALT initiative ( Cologne South Asian Languages and Texts    http://c-salt.uni-koeln.de   ), which gathers projects and resources hosted at the University of Cologne covering South Asian languages. To illustrate the potential use of our approach, we first introduce VedaWeb, a web-based platform that provides access to ancient Indian texts composed in Vedic Sanskrit, the oldest form of ancient Indo-Aryan. Then we describe the C-SALT APIs for dictionaries    https://api.c-salt.uni-koeln.de   . These APIs make several large Pāli and Sanskrit dictionaries available online. Building on that, we present the architecture behind these APIs, and finally we summarize by analyzing the potential role of APIs in Digital Humanities (DH) projects.    About VedaWeb The cornerstone of VedaWeb is a digital edition of the Rigveda, one of the oldest and most important texts of the Indo-European language family, which comprises approx. 160,000 words. VedaWeb can be accessed either via a web application    https://vedaweb.uni-koeln.de/rigveda    or directly via an API    https://dh-cologne.github.io/vedaweb/#description-of-the-api   .  VedaWeb provides several layers of linguistic and philological information, alongside various editions of the text of the Rigveda. A search function with multiple linguistic parameters is available (including lemma, word form, morphological and metric information), which allows to execute queries across different levels of annotation by means of complex, combined search criteria. Besides the annotated version of the text, further layers include the display of translations (including Geldner, 2003; Grassmann, 1876; Griffith, 1896, Renou, 1956-1969) as well as commentaries to the Rigveda (Oldenberg, 1909/1912, Renou, 1956-1969). Parallel to the morphological annotations, all of these additional information layers can be accessed via full-text search as well as a more structured search function. The possibility to combine these multiple layers is crucial for enabling novel perspectives on the data, e.g. by means of quantifying feature combinations or by identifying context-dependent phenomena such as different types of constructions. VedaWeb is meant to advance research in all areas of Vedic studies, for example in syntax (e.g. referential null objects (Keydana & Luraghi 2012), non-configurationality (Reinöhl, 2016)), morphology (e.g. the Vedic  vr̥kī-type (Widmer, 2007),  ya-presents (Kulikov, 2012)) or word formation (e.g. compounds (Scarlata & Widmer, 2015)).     A Screenshot of the VedaWeb Application, with two layers selected. Rigveda data and the Dictionaries are proportioned via the C-SALT APIs  An important feature of VedaWeb is the enrichment of the Rigveda text by linking each word with entries from the standard dictionary for the Rigveda by Hermann Grassmann (Grassmann, 1873). Instead of encapsulating the data in the application, our approach is to leave the resource ‘in place’ and obtain the data via the C-SALT APIs for Sanskrit Dictionaries    https://cceh.github.io/c-salt_sanskrit_api   .    C-SALT APIs for Dictionaries The C-SALT APIs for Dictionaries    https://api.c-salt.uni-koeln.de    have been developed to provide access to existing lexicographic resources in Pāli and Sanskrit without doubling work or hosting efforts. The dictionaries available via these APIs are also accessible through traditional monolithic web applications, like the Critical Pāli Dictionary Online    http://cpd.uni-koeln.de   , and the Cologne Digital Sanskrit Dictionaries    http://www.sanskrit-lexicon.uni-koeln.de   , which are a product of a major Sanskrit digitization project (Kapp & Malten, 1997).    C-SALT APIs Overview    API Architecture The basis of the APIs and of the VedaWeb application are versions of the texts and dictionaries encoded in TEI    Text Encoding Initiative:  https://tei-c.org/  -XML    Extensible Markup Language  . We employ a TEI schema   https://github.com/cceh/c-salt_dicts_schema  developed initially for the three most complex Sanskrit dictionaries (Apte,1920; Böhtlingk & Roth, 1855-1875; Monier-Williams 1899). By using one TEI schema, we not only achieve data persistence, but we also achieve a consistent structure for all dictionaries. While software such as frontend applications or APIs change over time, TEI offers the DH community the safest way to assure data persistence. For this reason, all the data accessed through APIs is ultimately based on TEI files. The different C-SALT projects use different technologies as ‘middleware’ between TEI and endpoints and also different Web API technologies: REST (Fielding, 2009) and GraphQL    https://graphql.org/   . Independently of the technology employed, our APIs focus on performance and on providing well-documented access to curated linguistic data.    Summary and Outlook Developing APIs means the separation of concerns. In the specific case of APIs: Well-curated data that should be efficiently accessed, through a clearly defined structure. For web applications this means : Focusing on a specific user target, employing, if required, multiple APIs. We have described the potential use of APIs for lexicographic resources. There are several advantages to making the data accessible through APIs instead of encapsulating the data within the application. Instead of forcibly homogenizing diverse data sets into a general data model, it is more efficient to provide a common interface for accessing them. This also opens up opportunities to employ the different resources in the context of other applications. The main goal in developing C-SALT is to keep all resources as modular as possible, so that they can be used and reused in different research scenarios. In the case of VedaWeb, this currently applies to the dictionaries involved, but we see the potential to transfer the concept onto the other information layers as well, in particular the Rigveda text and its translations. In general, we believe that an API based approach to digital resources and data in the Digital Humanities provides efficient access to data and encourages the reuse of available resources. It thus facilitates novel uses by other researchers while avoiding repetition of work and unnecessary redundancy of resource instances. Applications are transient, but the knowledge, represented by the data, may stay and be reused.   ",
        "article_title": "C-SALT APIs - Connecting and Exposing Heterogeneous Language Resources",
        "authors": [
            {
                "given": "Francisco",
                "family": "Mondaca",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Felix",
                "family": "Rau",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Claes",
                "family": "Neuefeind",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Börge",
                "family": "Kiss",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Kölligan",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Uta",
                "family": "Reinöhl",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Patrick",
                "family": "Sahle",
                "affiliation": [
                    {
                        "original_name": "Universität zu Köln, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-02",
        "keywords": [
            "oriental and asian studies",
            "software design and development",
            "sustainability and preservation",
            "lexicography",
            "English",
            "computer science and informatics",
            "information retrieval and query languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Exploration Full Fund  Sacred Sound (University of Tübingen, 2018 –2020) investigates the interacting of architecture and furnishings of sacred spaces with sound and the relations between concepts of sacred spaces and religious experience as well as the shaping of liturgical forms.  Such complex systems of relations are particularly demanding if sacred buildings and their acoustics don't exist anymore or at least not in their original form. New approaches of research are provided by recently refined methods of virtual reconstruction of historical acoustics based on reconstructed 3D-models of the architecture (Vorländer, 2008; Suárez et al., 2016). This research project will explore the contextualization of liturgical singing in its original sound space. We expect that chant as a sacred sound was embedded in a complex system of relations between movement within sacred space during the liturgy and the acoustical characteristics of the space. As a result we are looking for new insights into the shaping parameters for transmission, notation and performance practice of liturgical music.  Since the 1990s computer-aided methods are well established in acoustic research as well as the rendering of data into audible sound, which is called \"auralization\" (Vorländer, 2008). For sacred spaces however there is still lacking reliable in-depth data, although research into acoustics of churches Meyer, 2015; Girón et al., 2017) and the intelligibility of speech is well documented. Still lacking are investigations into the relations between acoustics of space and liturgical rituals performed within them. The Institut für Technische Akustik (ITA) at RWTH Aachen provides technical means to set up a simulator with auralization, as complete simulation of room acoustics, auralization and 3D-audio reproduction. The ITA is one of the leading centers for auralization technology and will collaborate with the research project. A case study on the acoustical reconstruction of a 11th century church in Spain has been already carried out (Pedrero et al., 2013). The innovative character of the research project consists in the combination of musicological, liturgical and ritual studies with techniques of Digital Humanities.  The project will start with the UNESCO World Heritage Cistercian monastery church of Maulbronn/Germany, one of the best-preserved churches of its kind. A 3D-laserscan of the church is performed in spring 2019 by the Hochschule Karlsruhe. In August 2019 we will record the office for St. Bernhard of Clairvaux (12th century) with ensemble Ordo Virtutum for medieval music (dir.: Stefan Morent, 6 singers a capella) in the church in collaboration with the Südwest Rundfunk (SWR).  In parallel the real acoustic will be measured. The 3D-model will be used as a basis to render the virtual acoustic of the church, taking into account its material and interior surfaces. Selected pieces of the music will then be recorded by the singers in an enechoic chamber at Aachen with a 32 microphones array, giving the singers the auralization of the sound of the church in real time. Both recordings will be compared in order to callibrate our results. The singers will also perform in the AixCave-space at Aachen, the biggest 5-side virtual acoustic immersion room of Europe. There it will be possible to test various positions of singing in the virtual model and its results on the acoustics.  We hope to be able to describe the connection between Cistercian chant (reform), theology and acoustics. There exist several papers on the acoustics of Cistercian churches but none of them used the amount of advanced techniques and music we intend to use.  If the experiment is succesful we plan in 2020 a recording of the offices for St. Gall and St. Ottmar in a virtual reconstructed model of the former monastery church of St. Gall with its virtual acoustic.  In the future we also intend to do recordings for Cluny and St. Peter and Paul of Hirsau, which was influenced by Cluny, but presents a different concept of architecture and acoustics at the beginning of the Hirsau reform. Virtual reconstructions of these churches (Fritsch and Klein, 2012; Morosan, 2013) shall help to reveal the character of their acoustics. We hope to be able to describe the difference in sound concepts of a Benedictine church of the 9th century and a Cistercian church of the 12th century.  Though this paper can't present results (which will be able only in 2020), it will describe what we want to do, which techniques will be used, what will be the challenges and questions and what impact on academic research and performance practice we expect. This short paper will present the current status of the project and its challenges and goals as a work-in-progress. ",
        "article_title": "Sacred Sound – Sacred Space: In Search Of Lost Sound",
        "authors": [
            {
                "given": "Stefan",
                "family": "Morent",
                "affiliation": [
                    {
                        "original_name": "University of Tübingen, Germany",
                        "normalized_name": "University of Tübingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03a1kwz48",
                            "GRID": "grid.10392.39"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "multimedia",
            "musicology",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "modeling",
            "simulation",
            "audio",
            "cultural artifacts digitisation - theory",
            "video",
            "virtual and augmented reality"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The present day transformation of newspapers from print to digital is not the first time they have evolved drastically. Instead, this change of format reminds of similar transformations when the newspaper first appeared as a distinct material genre. One influential definition separating a newspaper from a newsbook or pamphlet in its early days was that a newspaper was a \"sheet of two or four pages, made up in two or more columns\" (Hutt 1973: 9). The Dutch had two-column news at the time, while civil war in Britain saw both the rebels and the crown printing their propaganda. It took, nevertheless, centuries before journalism became a profession of its own and newspapers took their particular shape in the mid-nineteenth century (Morison 1932; Allen 1940; Baldasy 1992; Høyer 2005; Kutsch 2008; Pettegree 2014).  In the context of digital humanities, newspapers have become an iconic example of “big data” research (cf. Buntinx et al., 2017a; Lansdall-Welfare et al., 2017; Cordell and Smith, 2017;  ). While in localised research (Cristianini et al., 2018; Tilles, 2016) the material can be thought uniform, in the big data approaches it is striking how little attention is paid to what the data consists of. A telling example of waking up to this is the Oceanic Exchanges project ( ) where M.H. Beals and Ryan Cordell quickly concluded that mapping metadata across its many datasets is to be one of its most important contributions ( ).  Framed against this background, the idea of this paper is to outline how we developed a tool to uncover and explore the varied materiality of newspapers. As part of the large-scale digitisation, the accessibility of historical newspapers has improved drastically, but at the same time much of the information about the size, shape and feel of the newspapers, that was so central to past readers in understanding what kind of documents they were perusing, has been hidden from view. Interestingly, the digitised versions of the newspapers also allow for large-scale study of their material dimensions – an opportunity that has so far been paid very little attention to. Our analysis of the data shows that the digitised newspapers are by no means uniform in their materiality, and further that changes and aberrations in the material aspects correlate with changes and aberrations in content. Thus, besides allowing to study the material development of newspapers in itself, the information is useful for both the discovery of anomalies as well as partitioning the data into more uniform subsets for content analysis. In short, our argument is that content and form interact, and one cannot be analysed without the other. In this, the paper continues on a path previously charted by for example Moreux, 2016, Tolonen et al. 2018 and Lahti et al. 2019, while providing an orthogonal axis to those expanding study from text to visual elements (Smits, 2017; Wevers et al., 2018). In the following, we describe first in concrete terms how we extract and derive materiality-related information the ALTO XML format commonly used for newspaper data. Then, we evaluate the implications of charting the material development of newspapers.   Extracting and deriving material aspects from ALTO XML ALTO (Analyzed Layout and Text Object,  ) files contain a description of the visual organisation of content on a page, at the core of which are the individual words and their page coordinates. At the same time, the words are also grouped into blocks, often corresponding to paragraphs or columns. The format also contains general layout information, such as the sizes of margins and main printed area.  The usefulness of ALTO for analysing materiality crucially depends on the choice of the measurement unit in which all coordinate and size information is given. Unfortunately, many collections such as the Dutch Delpher ( https://delpher.nl) and French Gallica ( https://gallica.bnf.fr) provide their data using pixel coordinates. Luckily, the ALTO files of the National Library of Finland ( http://digi.kansalliskirjasto.fi) had a MeasurementUnit of mm10 (tenth of a millimeter, directly relating the measurements to physical dimensions).  Given this, we easily extract page size, printed area and character and words counts for each page. Directly given are also the fonts used. What proved problematic however was that the National Library of Finland had used altogether 22(!) versions of scanning software, some of which did not differentiate between Fraktur and Antiqua. By using metadata to analyse which newspapers were scanned with which version, we determined that trustable font identification could only be had up to the year 1910. For each page, we also extract all text box coordinates. While these are primarily meant to locate text visually on the page in reader interfaces, they can be processed to yield layout information. First, we extract column counts using a lighter-weight process than the computer vision approach used in Buntinx et al., 2017b. We scan the page from top to bottom, for each Y coordinate counting the number of text boxes present there. This yields a distribution associating all column counts with the area they control on the page.    ALTO text blocks overlaid on a newspaper page   One problem here (shown in Figure 1) is that the text boxes only cover areas with text. This would diminish the number of columns counted for coordinates laying between for example a heading and body text. To counteract this, we expand boxes to meet their neighbours. Another problem is that for tables, usually each column gets counted, sometimes yielding counts of over 40. For this, we have no automated correction. However, in the aggregate, such outliers quickly disappear, while in anomaly detection, they usually are interesting in themselves.    Different layouts calculated for the Hufvudstadsbladet newspaper based on paper size and text box coverage. A2 and A1 sizes show two different layouts, necessitating further partitioning.   We also keep the text boxes, associated with the number of characters each contains. From these we calculate information density distributions for each page, highlighting the role different parts have in holding headlines, advertisements or body text. This information can also be used to visualise how page layouts have changed in time (see Figure 2). Finally, for each issue, we gather page count and date information, from which we derive publication frequency. For analysis, we combine all this with newspaper-level metadata, such as name, publisher, language and place of publication.   Materiality explorer interface    The materiality explorer After gathering the data, we developed an interface that allows it to be explored. Shown in Figure 3, a first view provides overview graphs that allow tracking material changes on long time scales, both for individual papers as well as for larger groups. Here, the materiality information is contrasted with a baseline measure of text per month. By counting the number of characters each newspaper produces in a month without regard to how they are divided between issues or pages, this measure shows how much content needs to be transmitted. As this quantity rises, newspapers must respond with material innovations, whether by increasing page count, page size or publication frequency, or by cramming more material into available space by decreasing font size, line breaks or margins. A second view allows grouping the data by a combination of material dimensions, thereby allowing exploration of archetypal materiality categories. Finally, two views allow the user to explore page and issue-level material anomalies: for example pages which have more text than usual or pages with abnormal layout, or issues with appendices or which appear on the same day as another issue. Common to all views are selectors allowing limiting the newspapers under comparison.    Discussion For the Finnish newspapers, the data shows a general order in how they expanded: first, layout was changed to include more words per page; second, page size was increased; third, publication frequency was increased and only after that was the amount of pages increased. This last step often coincides with the introduction of rotary presses, which allowed newspapers to more easily be composed of more than four pages, and also allowed them to move back from large page sizes to more easily handled formats. Simultaneously, the data shows also high variability, where papers not only frequently printed supplements, but could switch back and forth between formats inside a single week, or cram text into a special issue through diminished line breaks. Similar shifts took place also with regard to fonts. Newspapers explored different Fraktur and Antiqua fonts to try out readability, but also because fonts were oftentimes used to signal that the contents was aimed for a particular audience. While there are plenty of exceptions to this, it seems that Fraktur was more often used when dealing with economy and religion, whereas Antiqua was reserved to politics, philosophy and the high arts. To test such hypotheses, we still need more robust statistical information. We also aim to compare fonts used with other factors, such as language frequency and size of newspapers. (For the history of newspaper layout and design, see Broesma 2007; Moen 1989; Olson 1940; Prebey 1929; Sutton 1948; Swansson 2000; Kapr 1993). Compared to earlier studies, our data driven approach gives us a great opportunity to evaluate the main findings of earlier historical studies of newspaper materiality (Moran 1973; Tommila 1998; Myllyntaus 1991; Gustafsson & Rydén 2010; McReynolds 1991; Wilke 2007). What we also aim to do with these patterns is to develop evidence-based archetypal categories of newspapers across history. We are then able to trace and compare these through time and place, but also use them to study the evolution of individual newspapers. These categories will also help us understand the newspapers as objects of intellectual activity, creating a theory of different historical maturity levels of newspapers. This in turn will help us chart the development of public discourse over time. Besides using the data to study the material development of newspapers as a genre in itself, we argue that content and form interact, and thus big data approaches to newspaper analyses also need to pay attention to material differences in order to accurately understand the subdivisions in large corpora.. For example, using the metadata we can create meaningful subsets of the data that are balanced by paper type for for example topic modelling or teaching automated transcription algorithms. Here, Finland makes an intriguing case for digital history because its public sphere is bilingual, with newspapers in both Swedish and Finnish. One interesting phenomena that arises from this are publishers publishing newspapers in both languages. For example, in Kotka there are both Finnish and Swedish newspapers by the same publisher with identical layouts and advertisements. Such could be used to create parallel corpora, interesting for the study of commonalities and differences between the different language public spheres, but also perhaps as material for machine translation.  ",
        "article_title": " Charting the Material Development of Newspapers  ",
        "authors": [
            {
                "given": "Eetu",
                "family": "Mäkelä",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Mikko",
                "family": "Tolonen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Antti",
                "family": "Kanner",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "content analysis",
            "communication and media studies",
            "history and historiography",
            "data mining / text mining",
            "English",
            "digital archives and digital libraries",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Within the Digital Humanities (DH), research applications such as databases, digital editions, interactive visualizations, and virtual research environments play a central role in securing and presenting research results [16]. Often, such  living systems [15] are the actual bearers of information content, thus representing the added value of the scientific output [16]. However, within the DH a great number of smaller, highly heterogeneous software solutions are produced, which all are subject to the problem of  software aging [14]. Against this background, institutions like the Data Center for the Humanities at the University of Cologne (DCH,   http://dch.uni-koeln.de ) face the challenge of preserving an unknown, potentially unlimited number of research software systems to assure their availability on a permanent basis. While there are well-established methods of preserving primary research data, e.g. in existing data repositories and archives, living systems are part of a constantly changing digital ecosystem and must regularly adapt to it, e. g. they need (security) updates. However, due to their steadily increasing number and their heterogeneity (both technologically and methodologically), permanent maintenance, support and provisioning of such living systems is a major technical, organizational, and therefore ultimately financial challenge.   This contribution presents an approach to the preservation of web-based research applications in the DH, based on the  Topology and Orchestration Specification for Cloud Applications (TOSCA) [11, 12, 13]. TOSCA is an OASIS standard for modeling, provisioning, and managing cloud applications in a standardized and provider-independent way. The TOSCA standard aims at providing a superset of service modeling and orchestration features and can thus be seen as a meta-framework that includes vendor and domain specific solutions like e. g. Docker, OpenStack or VSphere. In the following, we focus on an exemplary use case to describe the main concepts of our approach.     The Musical Competitions Database The DFG-funded project  Musical Competitions between 1820 and 1870 is conducted by the Department of Musicology at the University of Cologne in cooperation with the Cologne Center for eHumanities (CCeH). The aim of the project is to gather comprehensive information about music related competitions from 1820 to 1870 [6]. Data is extracted by musicologists from music-related journals and stored as JSON files in a document-oriented database (CouchDB). Access to the data is given through a web application written in React (  http://musical-competitions.uni-koeln.de ). Further, Elasticsearch is used to provide advanced options for querying/filtering and analysis of the data. At the time of writing, the database features information on approximately 1300 musical competitions, 1000 corporations and 3100 persons related to those competitions. The  Musical Competitions Database contains and presents a unique data set relevant to the musicology community. To allow for reproducibility in the sense of good scientific practice, a sustainability strategy to keep this data accessible on a permanent basis must include the web application itself, because the separation and archiving of the primary data alone would inevitably lead to a loss of functionality (and thus information).    TOSCA and OpenTOSCA Technological basis of our approach is the OASIS standard TOSCA [11, 12, 13]. TOSCA allows for a portable description of IT systems to automate their provisioning and management. In TOSCA, a cloud application or service [9] is modeled as a  Service  Template. Inside a Service Template, the  Topology Template describes the service’s topology as a directed multigraph, consisting of  Node Templates and  Relationship Templates that specify the edges between the nodes. Thus, this enables to describe arbitrary deployments in the form of  declarative deployment models [5]. Underneath, TOSCA employs a type system defining common properties and attributes in  Node Types and  Relationship Types, respectively. To automatically deploy, provision and manage the modeled service, TOSCA defines an archive format called  Cloud Service Archive (CSAR) which contains the Service Template, including all Node Types and Relationship Types, as well as all required software artifacts, scripts, and binaries needed for provisioning. Moreover, imperative management plans can be added to CSARs, which enables the implementation of arbitrary kinds of management functionality in an automatically executable manner. These plans can be implemented using standardized workflow languages such as BPEL or BPMN, or domain-specific modeling extensions such as BPMN4TOSCA [7]. Any TOSCA runtime environment can consume such a CSAR to automatically deploy and instantiate the enclosed application [2].   In a series of projects, the Institute for Architecture of Application Systems (IAAS,   http://iaas.uni-stuttgart.de ) at the University of Stuttgart has developed the OpenTOSCA ecosystem, an open source implementation for the TOSCA standard. OpenTOSCA includes (i) the graphical modeling tool  Winery for the creation of TOSCA-based application models [8], (ii) the runtime environment  OpenTOSCA container for automated provisioning and management of the modeled applications [1], and (iii) the self-service portal  Vinothek [4], which lists all applications installed in the OpenTOSCA container and serves as a graphical user interface.   We believe that the TOSCA standard is generally suitable for assuring the digital sustainability of research results, as research applications, which are packaged in CSARs, can be executed years later by a TOSCA-compliant runtime environment [3].   A TOSCA Model for the Musical Competitions Database In the following, we describe an application model for the Musical Competitions Database to exemplify some of the basic concepts of (Open)TOSCA. The application is composed of a CouchDB and Elasticsearch instance and accessed through a React frontend. The resulting TOSCA-compliant topology model is depicted in figure 1.    Screenshot of OpenTOSCA’s modeling tool Winery, showing the topology of the use case application. On the left side, the available components are listed.   The topology consists of a React web application hosted on an Apache web server, which itself is hosted on a Docker container, where the container operation system can be passed as a Docker image identifier (e.g.  ubuntu:latest). Additionally, the React application connects to a CouchDB database and to Elasticsearch. To accommodate Elasticsearch's dependency on Java, an OpenJDK has to be available. Therefore, seven different node types, namely  React,  Apache,  CouchDB,  Elasticsearch,  OpenJDK,  DockerContainer and  DockerEngine, as well as the  HostedOn, the  ConnectsTo and the  DependsOn relationship types must be available. A TOSCA Service Template describing this application will contain those seven node templates and three relationship templates – where each template is an instance of the respective type definition. The resulting service template can then be packed in a CSAR which may be instantiated by any TOSCA runtime or to be archived in a repository. As the TOSCA standard (and therefore OpenTOSCA) thrives on being vendor-independent, the topology root depicted in figure 1, namely  DockerEngine and  DockerContainer, may be substituted for e.g. OpenStack or VSphere and their respective VM/container representations.    Summary and Outlook The concepts described above emerged from  SustainLife [10], a DFG-funded joint project of the DCH Cologne and the IAAS Stuttgart. The overall objective is to develop generic solutions for standards-based operation and maintenance of DH-applications and to implement them in a way that they find practical application, e.g. in humanities data centers like the DCH.   As TOSCA depends on a generic type system enabling the reuse of recurring components, we work towards providing a set of typical system components. Examples for components, which were identified in further use cases [10] and modeled in TOSCA are Java runtime environments, the Spring framework and several types of databases like MySQL, mongoDB and eXist-db. In addition, reusable Service Templates reflecting typical software stacks are under development. For example, a common pattern for web applications is the so-called LAMP-stack, composed of a Linux operating system, an Apache web server, a MySQL/MariaDB database and a PHP/Perl/Python interpreter. These components can be reused are intended to simplify future application modelling, development and maintenance using TOSCA and the OpenTOSCA ecosystem.  Beyond that, a number of further extensions of the OpenTOSCA ecosystem are in the scope of the SustainLife project. For example, applications that are archived in CSARs need to be deployable several years after their development. Therefore, approaches to  freeze and  defrost whole applications and their respective execution states are also part of our research. This includes the possibility to version TOSCA models, to reflect the fact that living systems are subject to constant changes. Another desideratum is to add the possibility to update a service’s components. If a component must be exchanged because of security issues or deprecation, the CSAR may no longer be deployable. We therefore work on additional management functionalities which provide standardized operating and maintenance solutions, e.g. applying updates or software patches.   With our approach we expect to reduce maintenance costs significantly and will evaluate this expectation on the basis of selected use cases. Findings and best practices are prepared in a way that they can be transferred to partners and are communicated to the scientific community through workshops and publications. Thus, with this contribution, we want to trigger a discussion about the applicability of methods and technologies of professional cloud deployment and provisioning strategies to problems of long-term availability of research software in the DH-community.   Acknowledgements This work is partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation). Project title: “SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften” (see  ).   ",
        "article_title": " Sustaining the Musical Competitions Database: a TOSCA-based Approach to Application Preservation in the Digital Humanities  ",
        "authors": [
            {
                "given": "Claes",
                "family": "Neuefeind",
                "affiliation": [
                    {
                        "original_name": "Data Center for the Humanities, University of Cologne, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Philip",
                "family": "Schildkamp",
                "affiliation": [
                    {
                        "original_name": "Data Center for the Humanities, University of Cologne, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Brigitte",
                "family": "Mathiak",
                "affiliation": [
                    {
                        "original_name": "Data Center for the Humanities, University of Cologne, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Aleksander",
                "family": "Marčić",
                "affiliation": [
                    {
                        "original_name": "Department of Musicology, University of Cologne, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Hentschel",
                "affiliation": [
                    {
                        "original_name": "Department of Musicology, University of Cologne, Germany",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Harzenetter",
                "affiliation": [
                    {
                        "original_name": "Institute of Architecture of Application Systems, University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Uwe",
                "family": "Breitenbücher",
                "affiliation": [
                    {
                        "original_name": "Institute of Architecture of Application Systems, University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Johanna",
                "family": "Barzen",
                "affiliation": [
                    {
                        "original_name": "Institute of Architecture of Application Systems, University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Leymann",
                "affiliation": [
                    {
                        "original_name": "Institute of Architecture of Application Systems, University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "software design and development",
            "musicology",
            "sustainability and preservation",
            "English",
            "standards and interoperability",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Despite the thousands of digital projects launched during the past 20 years, experts warn of a new “digital dark age” (Cerf, 2015; Davis, 2016) as our ability to produce digital information continues to outpace our capacity to preserve and access that knowledge for the long term, even (or especially) when using content management systems (Montoya, 2016).  Project Endings is a collaboration between the Humanities Faculty and the University Library which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications.  Project Endings endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work.  The project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for all of these factors, using practice-based methods (Holmes, 2017; Arneil and Holmes, 2017; Holmes and Takeda, 2018), diagnostic tools (Holmes and Takeda, 2017), and scholarly research as listed in the project bibliography at  .  The project conducted a survey on the  LimeSurvey platform consisting of 30 questions   to discover how project leaders dealt with the issues of long-term sustainability for each of the five primary components. We promoted the survey to Canadian and international professional communities and received 128 responses. 25 detailed interviews were run with a sample of the respondents to get more information on the issues raised by the survey results.  Results of the survey show that concerns about longevity for digital humanities projects are not exaggerated. 57% of survey respondents did not consider an endpoint for their project, despite the fact that project management principles include declarations of goals, timelines, and milestones (Zanduis and Stellingwerf, 2013). In the light of this, perhaps it is not surprising that 54% did not have long-term preservation plans. These findings suggest that many researchers do not distinguish between products generated to exploit the features of the processing environment and products generated to survive after active work on the project ends or independent of development work in the project. Furthermore, only 32% considered “benchmarks for assessing progress” and 41% included precise timelines in their plans. In a group of projects that were for the most part (74%) less than 10 years old and 58% still in progress, 22% reported that project outputs stopped working due to software obsolescence. This is in a field of projects in which 74% started with born-digital data. If a failure occurs during the active life of the project it might be repairable, but repair is much less likely if the project has ended. The value of using a standardized data model is not universally recognized, with 14% of survey respondents not using one at all and 26% making up their own. Although a home-made data model is by definition not standardized, it may still be viable for a long time if well documented. 60% claimed to have a clearly documented data model, but 90% of those that had documentation considered it to be partial or inadequate, so it appears that a project’s data model is well documented in only about 50% of cases. HTML is the most popular standard output for DH projects (68% of respondents used it), despite the continued popularity of PDF (45%), XML (38%), and various binary media formats (>65%). Javascript is considered by many (30%) to be a major technology in their project. HTML and Javascript are robust long-term (Holmes, 2017), but if they are produced in a project only on-the-fly by a content management system (CMS) or database, then the longevity of the output is dependent on that of the CMS or database. 34% of the respondents used WordPress or Drupal, 31% used PHP/SQL databases, 38% used XML/XSLT/XQuery systems, and 41% used “other” software services and libraries. Some projects used more than one of these. Lack of ongoing funding was cited by 38% of respondents as the main obstacle to long term preservation. Perhaps more surprisingly, 33% of respondents rated either lack of expertise or bad technology choices as their main obstacle, which may explain the results reported above regarding software obsolescence. Early results from the interviews suggest that CMS and other software libraries and services are the likeliest sources of software failure over time. We hope that further analysis of the interviews will tell us whether a more expert assessment of software and output choices would have mitigated the issue of lack of ongoing funding. While a reassuringly high 42% of respondents reported that university services were responsible for long-term maintenance of the project’s work, an alarming 45% reported that this responsibility fell to the Principal Investigator or nobody, demonstrating either significant vulnerability or great confidence. Our survey results suggest that there is a limited use of project management (“What is PRINCE2?”, 2018; Sedlmayer et al., 2015) and software lifespan principles in DH projects. Results further suggest that there is a need for an improved understanding by researchers of specific attributes of a project which are likely to facilitate long-term viability of the project data, outputs and documentation at minimal cost for those charged with preservation. Blurring the lines between data, processing, outputs and the management of those components over time can result in vulnerabilities for long term preservability which may not be apparent until it is too late. With all of this in mind  Project Endings is working on a suite of recommendations that will provide guidance on project structure and management with long term viability as the goal. We are offering an online interactive questionnaire that assesses the long-term viability of each component in a project and provides recommendations for improving the prospects for long-term survival. Behind each question is the empirical evidence provided by survey/interview participants as well as the combined experience of the  Project Endings team. The questionnaire is intended primarily to be a thought-provoking activity for project leaders and principal investigators. An early draft of the questionnaire is available at  .  ",
        "article_title": " Project Endings: Early Impressions From Our Recent Survey On Project Longevity In DH  ",
        "authors": [
            {
                "given": "Stewart",
                "family": "Arneil",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Greg",
                "family": "Newton",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "management",
            "sustainability and preservation",
            "theory and methodology)",
            "standards and interoperability",
            "English",
            "organization",
            "digital archives and digital libraries",
            "project design"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Netherlandish art from the seventeenth century is closely associated with a few famous painters from that era: Rubens, Rembrandt, Van Dyck, Vermeer and a few others. This is not only true for the general public, but to a large degree for the field of art history as well. A great many books and articles have been written on Rembrandt and Rubens alone. Worldcat, for instance, lists 20 984 publications on Rembrandt and 11 589 publications on Rubens. The enormity of these figures becomes even more striking when we compare them to those of lesser known seventeenth century Netherlandish painters like Hercules Seghers (303 publications), Cornelis de Vos (62 publications) and Daniel Vertangen (1 publication). And when something is written about these lesser known painters it is often in relation or in contrast to Rembrandt or Rubens. To illustrate this: for his recent book (2015) on all history painters from mid seventeenth century Amsterdam, Eric Jan Sluijter could think of no better title than  Rembrandt's Rivals.  The omnipresence of Rembrandt and Rubens in our thinking about seventeenth century Netherlandish art has without doubt biased art historical documentation on this period. Data on these two famous painters are readily available, even in digital formats. In an effort to counter this bias towards the famous the authors of this paper have collected and structured biographical data on all (known) painters from seventeenth century Antwerp and Amsterdam in the ECARTICO prosopographical database. If available, we have also collected biographical data on their parents, spouses and often other direct relatives. Person in the data set have been extensively linked to external resources like VIAF, Getty ULAN, Wikidata and more. As a result ECARTICO provides a (partial) virtual model of the social networks that constituted the Antwerp and Amsterdam art worlds in the seventeenth century. In this paper we present the outcomes of a network analysis on the data derived from ECARTICO. The analysis shows that Rubens and Rembrandt – maybe as one might expect – were indeed key actors in the social networks of their artistic communities. We could be satisfied by this outcome because it once and for all shows that Rembrandt was not a “lone genius.” But we also have to raise the question whether this outcome is not affected by the fact that Rembrandt and Rubens are somewhat over-documented. Furthermore, we should ask the question to what extend the central positions of Rubens and Rembrandt shaped the structure of their artistic communities as a whole. To address the latter and related questions we have to deal with an important feature of (virtual) models that are not well understood by many historians, namely that models allow for experimentation. Network analysis packages allow users to delete nodes in the network and to rerun the analysis. For this paper we have done such an experiment and deleted Rubens and Rembrandt from their respective social networks. This experiment shows that despite the centrality of Rubens and Rembrandt in the art worlds of respectively Antwerp and Amsterdam, the social structure of these milieus was not entirely centered around them. Furthermore it will highlight the independent centrality of some painters that operated outside the sphere of influence of Rubens and Rembrandt. We are aware that such an experiment will meet skepticism by many historians (art historians included) or – even worse – will be qualified as “iffy history.” However, we argue that such a counterfactual analysis is not intended to get an impression of a world without Rembrandt and Rubens. On the contrary, the whole experiment is intended to get a better understanding of the impact of Rubens and Rembrandt on their artistic communities. Furthermore, it might provide us some assistance in assessing potential bias in the underlying data. Last but not least there is an epistomological argument. Sunstein (2016) recently argued that all historical explanation implicitly involves counterfactual reasoning. Due to the ongoing digitization of sources and data, (art) historians will increasingly have to deal with models of the past. It is a great opportunity to make our counterfactual reasoning explicit. ",
        "article_title": " Unthinking Rubens and Rembrandt: Counterfactual Analysis and Digital Art History  ",
        "authors": [
            {
                "given": "Harm",
                "family": "Nijboer",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Judith",
                "family": "Brouwer",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Marten Jan",
                "family": "Bok",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "theory and methodology)",
            "English",
            "network analysis and graphs theory",
            "cultural analytics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  It is important for libraries and museums to analyze and understand how digital collections and their contents have been used for many reasons, e.g., accountability for stakeholders. Results of analysis can be used to improve digital collections (Hughes, 2011). The usage analysis is conducted along with two steps: (1) Selection of a measurement: A measurement that suits for the purpose of the usage analysis is chosen. Then, the measurement is calculated based on data such as server logs. So far, measurements such as the number of accesses to materials (e.g., manuscripts) and images have been widely employed (Jones et al., 2000). (2) Visualization of the result: The result of the usage analysis is visualized to facilitate users to understand. Charts (e.g., line charts) have been used.  In these years, a lot of libraries and museums have adopted IIIF (International Image Interoperability Framework) (Snydman et al., 2015), which promotes mutual use of images. IIIF defines a couple of APIs to enable interoperable use. In IIIF-compatible digital collections, images are fetched via IIIF Image API, whose syntax is defined as:  {scheme}://{server}{/prefix}/{identifier}/{region}/{size}/{rotation}/{quality}.{format} Every time an image is zoomed and panned on an image viewer, the image is called via IIIF Image APIs with varying values of the region. Thus, it is possible to investigate the detailed image usage by examining which regions have been requested. In this paper, we show a method to analyze image usage and to visualize the analysis result. Specifically, we employ the number of accesses to each pixel as a measurement and visualize by heat maps. Since a pixel is the smallest unit of an image, we enable a fine-grained analysis different from previous studies (Warwick et al., 2008; Jones et al., 2000).   Method This section describes how to measure and visualize the detailed usage of images on IIIF-compatible digital collections (Section 2.1) and how to display the visualized result (Section 2.2).  Measurement and visualization The method is comprised of following two steps:   Measure the number of accesses to each pixel: For each image, an H×W matrix, where all elements are 0, is generated. H and W are height and width of the image in pixel. Each element of a matrix corresponds to each pixel. The height and width of images are retrieved by info.json provided by IIIF Image API. Subsequently, requested images and regions are acquired by parsing logs of IIIF Image API. Based on requested regions, the number of accesses to each pixel is counted and recorded to the matrices.  Generate heat maps: After counting the number of accesses to each pixel, the result is outputted as a heat map. The RGB value of each pixel is calculated considering the minimum and maximum values of the number of accesses to pixel in an image.   In order to save the memory for the analysis, we count the number of accesses in the unit of N×N pixels, instead of counting the number of accesses to each pixel. In addition, we output heat maps in (H/N)×(W/N) pixels, instead of the same size with the target image, in order to save the storage.    Display of heat maps Generated heat maps are displayed over corresponding target images, in order to enable users (e.g., administrator of a digital collection) to understand image usage. IIIF Presentation API enables overlay images by specifying two images on a page. Mirador, a popular viewer among IIIF community, implements a function of overlay display, as shown in Figure 1. One can manipulate visibility and opacity for each image in the left side panel.    Figure 1. Overlay display of a heat map on its target image using Mirador. Photograph courtesy of the Main Library, Kyoto University -   Konjaku monogatarishuu      Example and Improvement This section illustrates examples of analysis results and improvements for the analysis method.   Analysis considering probabilities to be accessed Figure 2 illustrates a typical heat map that represents image usage. The number of accesses close to the center is higher than others. This tendency has been observed in other images. The reason is that pixels close to the center have a higher probability to be accessed. In order to treat each pixel equitably, it is necessary to adjust the number of accesses according to the probability.    Figure 2. Typical heat map. Photograph courtesy of the Main Library, Kyoto University –   Yashiki-zu (design drawing of a mansion) from Nakai Collection   We compute the probability to be accessed for a pixel that is a and b pixels from the midpoint of each side of the image as:     Then, let c(w,h,a,b) be the number of accesses to a pixel that is a and b pixels from the midpoint of each side of the image. Then, the number of accesses can be adjusted as:       is the probability to be accessed for the center of the image divided by the probability to be accessed for the point, which is a and b pixels from the midpoint of each side. We take logarithm in order to mitigate influence from the adjustment. As a result, the number of accesses for Figure 2 is adjusted as shown in Figure 3.       Figure 3. Heat map where probabilities to be accessed are considered for Figure 2    Referrer of images As exemplified in Figure 4, we observe images, in which accesses are concentrated in specific regions. Looking into referrers of access logs, it turns out that these regions are referenced by  IIIF Curation Platform. Since IIIF enables mutual use, regions and images have more opportunities to be referenced from other platforms. Referrers show motivation and background behind accesses. If the web site that the referrer indicates is completely disclosed, it is possible to present a link to the web site on a viewer as annotations. So that users can discover regions and images that are highly relevant.     Figure 4. Example where specific regions get many accesses. Photograph courtesy of the Main Library, Kyoto University -   The story of Benkei, a tragic warrior      Applications This section lists possible applications of the result of the usage analysis.   Collaborative research platform: The data model used in IIIF follows  Web Annotation Data Model. Therefore, IIIF facilitates to share not only images but also information accompanying images (e.g., annotations such as transcriptions). For this reason, IIIF-compatible collaborative research platforms have been developed (Sato and Ota, 2017). By presenting heat maps, researchers can understand which regions have not been examined by collaborators and work for these regions.   Transcription Platform: In the decades, a lot of transcription projects have been launched (Terras, 2016). Transcribers zoom and pan images during generating transcriptions. If a platform is compatible with IIIF, it is possible to verify a pattern such as whether there is a difference in transcription performance (e.g., accuracy) between regions being zoomed and those being not zoomed. If there is a pattern, we can use it to facilitate verification process for transcriptions.    Selection of thumbnails: In many cases, images on the first page of materials are used as thumbnails. However, the first image does not necessarily represent the material. We may select the most-viewed region of images in the material as a thumbnail, which can be revealed by the analysis method.    Challenges Visualization of access logs is not a problem, if anonymization is conducted. However, anonymization does not make sense in some cases. For instance, for images that are only accessed by researchers in a specific field, colleagues can easily guess who accessed images and regions. Even if anonymization is complete, a series of activities of a researcher on images might reveal his/her viewpoint that would be a key issue of his/her academic outcome. In this case, his/her priority right of discovery may be infringed. Therefore, careful consideration will be necessary to exploit the analysis result as a service. In the future, we would like to consider some guidelines based on the above-mentioned challenges as well as existing policies such as privacy protection.  ",
        "article_title": "Log Analysis Method towards Understanding Detailed IIIF Image Usage",
        "authors": [
            {
                "given": "Chifumi",
                "family": "Nishioka",
                "affiliation": [
                    {
                        "original_name": "Kyoto University, Japan",
                        "normalized_name": "Kyoto University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/02kpeqv85",
                            "GRID": "grid.258799.8"
                        }
                    }
                ]
            },
            {
                "given": "Kiyonori",
                "family": "Nagasaki",
                "affiliation": [
                    {
                        "original_name": "The University of Tokyo, Japan",
                        "normalized_name": "University of Tokyo",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/057zh3y96",
                            "GRID": "grid.26999.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "user experience design",
            "interface",
            "gamification",
            "English",
            "library & information science",
            "computer science and informatics",
            "digital archives and digital libraries",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Existing Linked Data (LD) literature contains several examples (such as those based on Propp’s  Morphology of the Folktale (Peinado et al., 2004) or on ancient Sumerian mythologies (Nurmikko-Fuller, 2014)) where the fabula and syuzhet of Western folktales have been represented, and information regarding the stories themselves have been published in machine-readable formats such as RDF. However, similar (linked) datasets and analyses are largely non-existent for equivalent stories from Chinese mythology. This paper seeks to bridge that gap by creating, analyzing and publishing a case study example—the classic  Shanhaijing ( the Classic of Mountains and Seas 山海经). We recount the complexities of representing ancient Chinese literary narratives, captured in unstructured data, using tools developed from Western perspectives and for complete and largely homogeneous, highly-structured data.    Shanhaijing is an ancient encyclopedia. Its origins can be traced back to the pre-Qin period of China (4th century BC), its development continuing through to the early Han Dynasty (1st century AD). It covers broad areas such as ancient mythology, geography, witchcraft, religion, medicine, and other aspects (Hu, 2003).  Shanhaijing occupies a significant position in the literary and mythological corpora of the East, and is representative of a wider spectrum of Eastern mythologies. Over thousands of years, numerous Chinese novels, literary fictions and dramas have been derived from the book, such as  Zhuangzi ( 庄子) (Zhuangzi and Palmer, 1996) and  Strange Tales of Liaozhai ( 聊斋志异) (Pu et al., 1995). Mythologies from other Asian countries were influenced by it, e.g.  Kaiki Choju Zukan (Wang, 2018) and  Hyakki Yagyō (Tanaka, 2007), both examples of Japanese folklore.   In this paper, we report on the state of existing work combining LD methodologies and approaches with literary compositions (Section II), and summarize the narrative of the  Shanhaijing (Section III). We outline our chosen methodology in Section IV. The custom-built user-interface (UI) is demonstrated in Section V, and we conclude the paper with a discussion of the complexities of the process in Section VI.     Related work In the domain of literature, several publicly available datasets have been published as LD. For example, the Book-Sampo project (Mäkelä et al., 2011), which provides information on fiction literature published in Finland going back to the 15th century, alongside rich descriptions of both content and context (Mäkelä et al., 2013), or the Perseids Project (Almas and Beaulieu, 2013), which provides a platform for creating, publishing, and sharing research data, in the form of textual transcriptions, annotations and analysis (Almas, 2017). Other essential work in this space include the Brothers Grimm project (Franzini et al., 2016), an ontology for Greek mythology (Syamili and Rekha, 2018), and the Aarne-Thompson’s Motif-Index project (Declerck et al., 2017).    Classic of mountains and seas, 山海经  The present version of  Shanhaijing contains 18 chapters, approximately 31,000 words in total. It records ancient Chinese mythologies, where numerous monsters with fanciful descriptions are portrayed as possessing magical powers or as related to ancestor (totem) worship (Li and Chan, 2012), such as the monster Lushu ( 鹿蜀) , which looks like a horse with a white head, a scarlet tail and tiger’s markings, and lives on Mount Niuyang (纽阳之山). Whoever wears its fur will have a greater number of descendants.  We focused exclusively on the capture of the data for the monsters in  Shanhaijing. The reason for this is that the fascinating and detailed accounts of these creatures overwhelm the other aspects of the story; and these descriptions account for a notable proportion of instances of literary borrowings and inspirations in other cultures, increasing the likelihood of reuse and inter-linking with other ontologies.     Methodology The first stage of the project focused exclusively on the cornucopia of monsters (a total of 277). Through a close reading in both English and classical Chinese, we extracted structured data from the unstructured narrative.  In the second stage, we designed an ontological structure to model the domain (Pan et al., 2017). After considering several pre-existing ontology software libraries, we concluded that the suitability of these vocabularies and resources for the representation of  Shanhaijing was limited. This necessitated the building of a new ontology, which captures data types and represents the relationships between them as .TTL file.  The ontology represents the characteristics of the monsters and the complex relationships between them. It contains a taxonomy of body parts, the characteristics, and the habitats of all monsters. We used a combination of top-down and bottom-up ontologies and schemas to identify Class and Property hierarchies, captured through  rdfs:subClassOf and  rdfs:subPropertyOf and reused existing vocabularies such as DBPedia Ontology, BioTopOntology (Whetzel et al., 2011), Mahabarata Ontology, RDFS, and XML Schema. This approach enabled us to capture the specifics of the data (bottom-up approach), but also maximize the benefit from other well-developed and rich ontologies.   All concepts related to monsters were collected, then split into terms. A term is considered as a Class if it has attributes pointing to other classes or literals, or it is a Superclass of other classes. Otherwise, it is defined as a property. For example, the term  Monster is defined as a Class because it has attributes linking it to other Classes, such as  Mountain, through the property  livesIn. However, term  Noise is not a Class because it is not the domain of any attribute. Hence, it is considered as a property  hasSameNoiseAs with  Monster as its domain, and a literal as its range. The class  Monster is defined as a subclass of  Character in FRBRoo, allowing the use of FRBRoo to represent the relations of works. Figure 1 demonstrates a graphic version of the Shanhaijing Ontology.     Figure 1: The Shanhaijing Ontology  Instance-level data was normalized by mapping it to the ontology, and the Silk-Link Discovery Framework (Volz et al., 2009) was used to automatically link appropriately matched resources to external datasets (DBpedia, Wikidata and Schema.org) using  owl:sameAs,  owl:equivalentClass and  owl:equivalentProperty.   An interactive data explorer software tool ( iSHJ) was built for visualizing, querying (through a SPARQL interface) and analyzing the data. The dataset, ontology, and source code are available via GitHub (https://github.com/aaasteria/chinesemonster).    User interface  iSHJ was built as a domain specific application for  Shanhaijing. This tool has “Browse”, “Search” and “Visualization” interaction modes. Users are provided with quick search functions to explore the data by clicking buttons and inputting keywords rather than writing SPARQL queries directly, although they can be when more flexible and variable searches are needed (see Figure 2). The results are displayed both in plain text and charts for visualization (see Figure 3). We also provide a graphical version of  Shanhaijing, where mountains are placed to represent the locations described in the book, the monsters correspondingly placed on the mountains where they live. A video of the UI is available at YouTube (https://youtu.be/oyZGIoTb78k).    Figure 2: Results of Quick Search Example \"monster\" in \"Browse\" Section in  iSHJ     Figure 3: SPARQL Query Sample of Monster's Tail Number with Visualization Results in  iSHJ     Discussion Despite the work on the narrative of separate regions of some prominent Western myths, projects focused on Chinese literature within this interdisciplinary field are rare. Existing LD methods have been developed almost exclusively in the context of Western culture, and predominantly for highly structured data. When facing ancient Chinese mythologies, there are two main unsolved challenges: non-existent structured datasets and the unavailability of reusable ontologies.  Before LD methods can be applied to the narrative of Chinese myths, a structured dataset capturing in-depth knowledge of Chinese mythologies must be constructed. However, the full potential of this pioneering project can only be tapped into once a greater number of external, disparate, but complementary datasets are published using the LD paradigm. That is to say, until other projects focusing on the analysis of Chinese literature engage in LD, there are limited opportunities for outward linkage.  The protagonists of Eastern and Western mythologies are not entirely similar. For example, in many ancient Chinese mythologies (such as  Shanhaijing), numerous gods and creatures are described as a monstrous combination of different animals, falling somewhere between, for example, the human-like (both physically and emotionally) gods and heroes of Greek myths and the anthropomorphized animals of Aesop’s tales. Although there are some complementary aspects – e.g. in the Aarne-Thompson’s Motif-Index, the Nine-tailed Fox ( 九 尾狐 ) (Kiyoshi, 1961; Lee, 2011; Chen, 1995), is recorded as B15.7.7.1; other motifs are the four-eyed tiger (B15.4.1.2.) and serpent with a jewel in its mouth (B103.4.2.) – These ontologies neither contain the narrative of Chinese myths nor are they created for Chinese folktales. Ultimately, the existing overlaps are insufficient.   Based on the differences in the narratives, most ontologies created for Western folktales are not completely suitable for the representation of ancient Chinese mythic classics and could not adequately demonstrate the characteristics of these gods and monsters in the  Shanhaijing.     Conclusion  We used LD methods for textual analyses and visualization of a book of Chinese mythology,  Shanhaijing. We created and published a structured dataset, relevant LD, and an interactive explorer to represent the monsters within the text. An extensive review of existing ontologies for literary motifs and mythological creatures revealed that there was insufficient overlap between them and the needs of the dataset, necessitating the development of a new ontology.   Future work will see us expand this analysis to all the contents of  Shanhaijing. New ontologies will be generated from the one in this paper, and structures will be   redetermined and improved to adapt to other mythologies. Other ontologies could be reused or interlinked to, increasing the number of linked elements.   We will also test the suitability of our ontology on other mythologies, ranging from Chinese mythologies appearing before and after  Shanhaijing to other Asian mythologies such as Japanese tales. We will also apply our ontology to Western mythologies to assess the similarities and differences between Eastern and Western folk tales.    ",
        "article_title": "Analysis and Visualization of Narrative in Shanhaijing Using Linked Data",
        "authors": [
            {
                "given": "Qian",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities Research, Australian National University, Australia",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            },
            {
                "given": "Terhi",
                "family": "Nurmikko-Fuller",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities Research, Australian National University, Australia",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            },
            {
                "given": "Ben",
                "family": "Swift",
                "affiliation": [
                    {
                        "original_name": "College of Engineering and Computer Science, Australian National University, Australia",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "oriental and asian studies",
            "multilingual / multicultural approaches",
            "etnography and folklore",
            "English",
            "semantic web and linked data",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Research context  Handwritten Text Recognition (HTR) is “the ability of a computer to transform handwritten input represented in its spatial form of graphical marks into an equivalent symbolic representation as ASCII text.” (Romero et al., 2012: 5)  What is the state of the art of the application of HTR to early modern manuscripts? With what level of accuracy   can HTR models automate their transcription? What is known about how HTR currently accommodates manuscript text that shows changing writing styles, hands and text in multiple languages? We will explore these questions with reference to the wider literature and a case study of the first HTR model to be created for the hand of   Sir Hans Sloane (1660-1753).    Optical Character Recognition on documents with perfectly machine-printed characters can reach an accuracy level of more than 99% (Cao and Natarajan, 2014: 336–37). However OCR is often problematic for historical documents (Smith and Cordell, 2019: 5) . It also cannot be used for handwritten documents, since the space between characters and words is inconsistent.  Holistic segmentation-free off-line  HTR technology works at a line level and can deal with cursive characters, slanted words and irregular calligraphy, but it must be trained for a specific handwriting  (Alabau and Leiva, 2012: 2274; Sánchez et al., 2014: 111–12) . HTR is not accurate enough to replace human expertise, however it holds the potential to bolster the transcription process (Toselli et al., 2018: 174;176) .     ‘  Enlightenment Architectures: Sir Hans Sloane’s Catalogues of his Collections is a Leverhulme-funded collaboration between the British Museum and UCL.  It studies 5 of the manuscript catalogues of  Sloane,   The catalogue of Miscellanies, two of his Natural History catalogues (Fossils vol. I and vol. V) and two of his library catalogues (Sloane MS 3972C vol. VI and Sloane MS 3972B).   and is encoding them in TEI to understand the information architectures they use. In 2017, selected catalogues were transcribed to a high level of accuracy by the company AEL Data Service in Chennai, India. We thus had high quality transcriptions of Sloane’s manuscript materials, in addition to images of his catalogues, available for use in the training of an HTR model for Sloane’s hand.    The HTR model discussed here was trained using the software Transkribus. The aim of the e-Infrastructure project READ (Recognition and Enrichment of Archival Documents) is to make archival sources more accessible through technological development. The centrepiece of READ is the service platform and application Transkribus, which enables the automatic recognition and transcription of handwritten documents and the ability to search within them (READ project, 2018a; READ project, 2018b) .   Methodology   To train an HTR model with Transkribus, one has to provide it with training data (digital surrogates of the original folios and their transcriptions). This is known as  ground truth  or  reference data.  The segmentation of the document into its elements, in particular the baselines, and the actual transcription is crucial for creating an adequate HTR model. The ground truth data must consist of a representative sample of a collection’s documents and also respect the original appearance of the script, e.g. special characters, as closely as possible. With Transkribus, this serves the purpose of training the HTR model, and also the evaluation of its accuracy. Between 75 and 100 pages (around 15,000 to 20,000 words) of training data are necessary for an effective HTR model.   A randomized selection of documents is recommended ( READ project, 2018c: 3–4; READ project, 2017: 10) .   We determined that the   first sub-section of the Miscellanies catalogue (folio 2-152v) would give enough training and test data to evaluate the model because it contains important characteristics of the whole collection of catalogues, such as annotations and a complex layout.   We wish to thank the members of the Enlightenment Architectures team for their assistance in making this selection and for their wider advice about this case study.      For this research, five different HTR models were created to allow a comparison between their changing accuracy. This includes one pre-test model. Training started with 75 folios and was then increased to 100 and 125 folios. For the last model, in addition to the 125 folios of training data, a base model was added. Results   The quality of an HTR transcription can be evaluated according to a Word Error Rate (WER) and Character Error Rate (CER ) (Romero et al., 2012: 93). Transkribus allows both measures (READ project, 2018c: 5).  WER is […] “the minimum number of words that need to be substituted, deleted or inserted to convert a sentence recognized by the system into the corresponding reference transcription, divided by the total number of words in the reference transcription […]”  (Romero et al., 2012: 55). C ER is the minimum number of single characters which need to be corrected, divided by the total number of characters in the reference text  (Romero et al., 2012: 55) . Transkribus also allows the evaluation of the general accuracy of a model with a learning curve visualisation and the accuracy of a model on the page level to be specified via the compute accuracy function (READ project, 2018d: 9–12). According to READ  (2018d: 10),  a model with an accuracy rate of 90% can be regarded as an effective automated transcription.    The evaluation showed that our current model of 20,803 words reached a CER of 12.73% without the base model. The transcription has not reached a level of accuracy that is sufficient for academic research without further human input. The model has problems transcribing names (persons and places), abbreviations, double letters (e.g. ee), punctuation, Latin text and the numbers in the margins correctly.  Conclusion   In the paper we will reflect on how our methodology and model might be refined in order to improve the CER, in line with the experiences of other projects (for example Hodel, 2017 or  Prell, 2018). We will give particular attention to questions like ‘  Where in particular does recognition fail?’. ‘ How much training data is necessary to create a model with an accuracy of at least 90%?’ and ‘how might external resources like gazetteers and name authority lists be integrated into Transkribus and used in conjunction with the HTR model in order to increase the accuracy of the transcription of named entities?   Our responses to questions like this are likely to be transferable to other projects who seek to build HTR models for the transcription of early-modern manuscript materials.      Although our model reached a relative high level of accuracy, is it not good enough to be used for scholarly work. We will therefore also reflect on scenarios where the model could still be used, such as Authorship Attribution (Franzini et al.,     or Named Entity Recognition (Carbonell et al., 2018; Toledo et al., 2019).   ",
        "article_title": "The Application of HTR to Early-modern Museum Collections: a Case Study of Sir Hans Sloane's Miscellanies Catalogue",
        "authors": [
            {
                "given": "Marco",
                "family": "Humbel",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Julianne",
                "family": "Nyhan",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-04",
        "keywords": [
            "libraries",
            "museums",
            "English",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The thirteenth century in the Czech lands is undoubtedly the most interesting period for the nobility. After the prince period (until 1198), the throne is surrounded by “magnate” families with precisely defined family relations, and the lower nobility rise in numbers. It is the last period when a staggering social rise is possible for a broader number of aristocrats and warriors. Over the century, the Estates are relatively precisely established (Novotný, 1937; Vaníček, 2002; Žemlička, 2002). When the economic and social gap between the lower and higher nobility widens, the political development becomes dynamic. After an almost invariable group of families around the monarchs has been established, the impossibility of political upheaval led to the uprising of part of the nobility and the civil war in 1248-1249. Using social network analysis (Wasserman and Faust, 1994) we attempt to describe polarization within the nobility, identify who joined the uprising in the ranks of Přemysl Otakar II (Ottokar II), and how it influenced their chances to be appointed to high-ranking positions within the kingdom. Whereas social network analysis has been qualitatively used by some medieval historians (e.g., Ruffini-Ronzani, 2016), current scholarship on the Czech civil war (e.g., Jan 2008) focuses on individuals. It hypothesises about cliques around Václav I and Ottokar II based only on the holders of offices during their reigns. Our analysis relies on more detailed data (co-occurrences in the charters) and more advanced method (centrality measures coupled with clustering).   Data The data concerning relations between Václav I and Ottokar II and the Bohemian nobility were collected manually from the charters released between year 1198 and 1283. In total, we collected data on approximately 2300 noblemen from 568 charters. Identification of individuals was at times ambiguous – for example, Jan, the son of George, and Jan of Brno appearing within a few years, may be one, two or three men – leading to arbitrary choices. A cross-check with other sources was not possible: a) there are only few charters common to  Regesta Imperii or  Monumenta Germaniae Historica and Czech sources; b) in the narrative sources, very brief  Annales of 1198-1278 and longer  The Stories of Wenceslaus I, there are only five person mentioned, two of them unknown to the charters.  At this time, we cannot follow social relations in its own sense, we can only determine the agnatic kinship (women in charters are exception), the kinship between individual generations can only be thought of to discover names typical of another genus.   The diagonal provides the number of noblemen included in a network for a given period. The off-diagonal tiles show how many noblemen appear in two given periods    Methods The primary concern of this paper lies within the relations between noblemen. From the charters, we extracted weighted networks of noblemen (as network nodes) and their co-occurrence in charters (as links; the weight of a link is proportional to the number of co-occurrences) in four periods 1240-47, 1248-49, 1250-1253, 1254-1257. The length of the first and last period was chosen so as to build networks of comparable sizes. We analysed only the largest connected components of such networks, in order to meaningfully define quantities such as shortest paths connecting any two people or their centrality indices (which are measures of nodes’ importance in the network or simply: their ranking).   Changes in centrality of nodes in the network.  Left: cluster of people thriving under Václav I, whose position declined under Ottokar II.  Right: cluster of people who gained their position during rebellion; two example noblemen are indicated   We used node strength (sum of weights of its links) as a proxy for its centrality. Next, to each person appearing in the networks we attributed a vector of four values: their network strength in the four consecutive periods. These vectors were agglomeratively clustered with the use of so called “chessboard distance” into groups of noblemen whose centrality underwent similar changes due to the changes of reign. Analysis was performed in: Wolfram Mathematica 11.3; network visualisation in: Gephi 9.2.   Results In Figure 1, we show that the networks of decision-makers in the consecutive periods overlap at most in one third, indicating considerable rotation of posts. Next, as shown in Figure 2, we automatically found two groups of people: benefitting or losing from the uprising. Using that information we extracted the names of noblemen hypothetically loyal to Václav I or opposing him. In Figure 3, we show one of the analysed networks, notably with future rebels in vicinity of Ottokar II, and some filial and brotherly kinships within the highest-ranking noblemen.   Conclusions and outlook The results show that almost 800 years later we can still identify the people involved in coup d’état and how it influenced their power in an important period of Czech history. We aim at extending this study by incorporating other information from the charters, e.g., their geographical location, the posts held by the noblemen or family membership. Methodologically, we plan to explore other centrality measures as well as community detection to corroborate the results with different techniques, and use bootstrap approach by generating ensembles of random networks with realistic properties to further assess statistical significance of the results. In terms of historical sociology, an interesting task would be to compare characteristics of the above networks with other known – contemporary or historical – social networks, and obtain a complementary insight into the (power) relations in medieval societies.   Network constructed from charters issued in 1240-1247. The size of nodes is proportional to their strength. The kings ( purple) and 10 noblemen with highest centrality are labelled.  Red nodes correspond to the cluster that thrived during rebellion.  Light green nodes correspond to the cluster of people thriving under Václav I. Insets A and B show Přemysl II’s and Václav I’s subnetworks, respectively    ",
        "article_title": "How To Detect Coup d’État 800 Years Later",
        "authors": [
            {
                "given": "Jan",
                "family": "Škvrňák",
                "affiliation": [
                    {
                        "original_name": "Department of History, Masaryk University in Brno, Czech Republic",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Škvrňák",
                "affiliation": [
                    {
                        "original_name": "Department of Sociology, Charles University in Prague, Czech Republic",
                        "normalized_name": "Charles University",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/024d6js02",
                            "GRID": "grid.4491.8"
                        }
                    }
                ]
            },
            {
                "given": "Jeremi K.",
                "family": "Ochab",
                "affiliation": [
                    {
                        "original_name": "Institute of Physics, Jagiellonian University, Kraków, Poland",
                        "normalized_name": "Jagiellonian University",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/03bqmcz70",
                            "GRID": "grid.5522.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-22",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "data mining / text mining",
            "English",
            "network analysis and graphs theory",
            "medieval studies",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Why do we need a metamodel for metadata?  The Text Encoding Initiative (TEI; TEI-Consortium 2019) environment provides a generic document model. TEI customizations typically follow an explicit or implicit model for text representation. The TEI document model provides modules for encoding text via mark up as well as modules for the metadata referring to the TEI document and the encoded text. The specialized (“odd”) customization presented here follows an explicit Metamodel for Corpus Metadata (MCM) representation and expands the range of applications of the TEI metadata modules to non-TEI corpora. Metadata are defined as “structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information resource.” (NISO 2004: 1). The application scenarios for metadata are diverse, which in turn generates requirements for metadata classifications (c.f. for example Zeng and Qin 2016). Corpus documentation is a typical example of the application of metadata. In order to (re-)use historical corpora, a researcher needs to be able to gain intellectual access to a heterogeneous set of properties of a corpus. Moreover, the reusability of (historical) corpora is an important indicator for their sustainability, following Simons and Bird (2008: 90) who stated that a “[...] resource will be used if it still exists, if it is usable, and if a user finds it relevant.” Historical corpora attest diverse methods and approaches towards corpus creation, exemplified by a variety of existing resources such as  Referenzkorpus Althochdeutsch (Donhauser, Gippert and Lühr, 2018, Version 1.1),  Deutsches Textarchiv (Berlin-Brandenburgische Akademie der Wissenschaften, 2019), and  Kasseler Junktionskorpus (Ágel and Hennig, 2008, Version 1.1). Thus, corpus documentation must cover a complex and heterogeneous field of corpus data. MCM provides a UML-based extensive and extensible metamodel for describing corpora, including their documents, annotations and preparation workflows, to address this complexity and enable intellectual access to corpus data.   In this paper, I will discuss how MCM can serve as a content model for the TEI document model, which in turn is used for a realization of MCM. This way, information is passed from model to model. Following this approach, I will also show how flexibly the TEI framework can be used and how two types of models can interact with each other.   The TEI document model  The TEI Guidelines provide an extensive and easily adaptable framework for text encoding of various text types and genres, and for the encoding of metadata (cf. Stührenberg 2012). Especially the digitization and encoding of historical documents are often achieved with the help of the TEI (cf. for example Haaf, Geyken, and Wiegand 2014; Schroeder and Zeldes 2016; Durrell, Ensslin, and Bennett 2007). Figure 1 illustrates the TEI document model, and particularly the relation of data and metadata.   TEI document model with a basic structure containing teiHeader and body. The metadata in the teiHeader refer to the body part as well as to the external source and the TEI document itself. The body part refers as well to source. The source itself can be more complex than indicated here.  A TEI-compliant document   http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-TEI.html Accessed 2018-11-25.   contains a body, which ‘contains the whole body of a single text’   http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-body.html Accessed 2018-11-25.  , and a teiHeader   http://www.tei-c.org/release/doc/tei-p5-doc/en/html/HD.html Accessed 2018-11-25.   for the metadata records and for the documentation of a TEI document. The metadata in the teiHeader refer to the TEI document itself, the body and the external source (in our case historical texts). The data in the body of the TEI document refer to the source as being the digital surrogate of the source. In this way, metadata and data are integrated in a single document, are defined by the TEI content model, and refer to the same source. The annotation (tag set) of the document is typically not documented in the teiHeader. Instead, it is extracted from, and documented in, the TEI Guidelines, and modeled within the TEI environment.    A model-to-model-based approach for developing corpus metadata  The TEI Guidelines can be seen as a content standard, the TEI XML as a file exchange standard which is defined by the TEI modeling tool ‘One Document Does it all’ (ODD, Burnard and Rahtz 2004).   This is an advantage compared to other metadata schemes (e.g. ISO 2015) which do not have such a modeling environment. For a discussion of other approaches see Odebrecht (2018, chapter 5)  By modeling the TEI with the help of the TEI, the TEI is validated within TEI as well. This way, the data creation and documentation is exclusively dependent on the TEI environment. While using the TEI framework enables a high degree of interoperability across disciplines, other formats also mix TEI annotations with other annotations, or do not use the TEI at all. The complexity for corpus data documentation increases when dealing with digital surrogates of different encoding standards, which in turn are based on diverging text, data models, or formats. To handle this complexity, the MCM, by using the Unified Modeling Language (UML, cf. Rumpe 2011)   https://www.omg.org/spec/UML/ Accessed 2018-11-25.  , identifies common features across diverse corpus data and defines which corpus metadata need to be included in a corpus documentation to enable intellectual access and corpus data reuse (Odebrecht 2018). This kind of documentation needs a stronger focus on information about corpus preparation, because the preparation steps for a historical corpus are not always included in a single environment such as the TEI. Adopting the TEI approach and focusing on the metadata framework, the MCM provides an additional conceptual layer (cf. Figure 2).    The model-to-model approach: The MCM provides extended metadata for corpus documentation which are handed over to the TEI modeling environment ODD. Each TEI customization refers to a central corpus documentation part: source (text model), annotation (data model) and creation (workflow model).  The MCM serves as a blue print for metadata modeling and realization in TEI (Odebrecht 2017). It is a format-independent and extensive content model for the application of the TEI to corpus documentation. The realization of the model-to-model approach can explicitly add additional statements and information relationships which go beyond the original TEI model. From a TEI perspective, the document model with a focus on metadata is extended by including non-TEI corpora but looses the close relationship between the source and the TEI document. The customizations adapt the TEI for an originally unintended and novel use case which allows for the documentation of historical corpora of any format and any architecture. This is done via three customizations: One customization describes corpus-internal information such as the text representation which is close to the typical functionality of the TEI. The second customization contains explicit information about all kinds of annotations in the corpus. A third customization describes corpus-external information of the workflow. A concrete application scenario for historical corpora is as follows: TEI or non-TEI corpora which shall be archived in a repository (LAUDATIO   http://www.laudatio-repository.org/ Accessed 2019-04-24.  ) can be documented comprehensively and deeply structured. The resulting TEI-XML metadata are then used for metadata search and display in this repository.  The TEI customizations represent an innovative perspective on the application of the TEI. The TEI already provides a full metadata structure which is integrated in a document-centric model. With the help of another model – the MCM – the application of the teiHeader can be extended. The MCM can be defined as a concrete and extensive content model for metadata information. The application of the MCM benefits from the TEI environment and its interoperability, and can make use of the modeling tool ODD and its validation mechanism. This approach enables use cases for metadata that were previously separated from the TEI universe and proves that the adaptability and flexibility of the TEI allows reuse scenarios, in this case TEI XML for corpus metadata, which have not been initially intended.  ",
        "article_title": "A Model-to-model Approach for Developing Corpus Metadata. An “Odd” TEI Customization for Encoding Metadata",
        "authors": [
            {
                "given": "Carolin",
                "family": "Odebrecht",
                "affiliation": [
                    {
                        "original_name": "Humboldt-Universität zu Berlin, Germany",
                        "normalized_name": "Humboldt University of Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01hcx6992",
                            "GRID": "grid.7468.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "linguistics",
            "English",
            "text encoding and markup languages",
            "library & information science",
            "data models and formal languages",
            "information architecture and usability",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Enabling the easy discovery of resources is an important service for Digital Humanities scholars. In CLARIN, the Virtual Language Observatory (VLO) serves this purpose, but it is currently mostly suited for the discovery of data. Discovering software is not so easy in the current VLO. In order to address this issue, we present a proposal for faceted search in metadata for software, which is based on a CLARIN Component Metadata Infrastructure (CMDI) profile for the description of software that enables discovery of the software and formal documentation of aspects of the software. We have tested the profile and the faceted search based on this profile by making metadata for over 80 pieces of software, and by creating an implementation of the faceted search. It is available on the web.    http://portal.clarin.nl/clariah-tools-fs     We propose   to add this faceted search to the VLO, and show how metadata curation software, combined with provided metadata curation files, can curate existing metadata descriptions for software using other profiles to make them suited for such faceted search (implemented for 280 other pieces of software, also available on the web    http://portal.clarin.nl/clariah-tools-fs-global    ).    Metadata Profile ClarinSoftwareDescription  The ClarinSoftwareDescription (CSD) profile    http://catalog.clarin.eu/ds/ComponentRegistry/rest/registry/profiles/clarin.eu:cr1:p_1342181139640/xsd    enables one to describe information about software in accordance with the CMDI metadata framework   (Broeder et al., 2012).   The profile has been set up in such a way that it enables (1) the description of properties that support discovery of the resource, and (2) the description of properties for documenting the resource, in as formal a manner as possible.  Since the focus of this paper is on the faceted search for software, we only briefly describe some aspects of the profile.   The profile consists of the CMDI components Generalinfo, SoftwareFunction, SoftwareImplementation, Access, ResourceDocumentation, SoftwareDevelopment, TechnicalInfo, Service and LRS.   The  SoftwareFunction  component enables one to describe the function of the software in terms of the closed vocabulary elements  tool category ,  tool tasks ,  research phase(s)  (for which it is most relevant),  research domains  and, for the linguistics domain, relevant  linguistic subdisciplines  for which it was originally developed.    The  SoftwareImplementation  component enables one to describe information for users on the implementation and installation of the software. The most important components are for the description of the  interface , the  input  and the  output  of the software.   The  Service  component (CLARIN-NL Web Service description) is intended for describing properties    of web services. It is compatible with the CLARIN CMDI core model for Web Service description version 1.0.2.    The  LRS  component is intended for the description of the properties of a particular task for the CLARIN Language Resource SwitchBoard (CLRS,   (Zinn,   2016)). It is our viewpoint that specifications for an application for inclusion in the CLRS registry should be derivable from the metadata for this application. We devised a script to turn a CSD-compatible metadata record that contains an LRS component into the format required for the CLRS and tested it successfully with the Frog web service and application (Van den   Bosch et al., 2007).    Semantics  Many of the profile’s components, elements and their possible values have a semantic definition by a link to an entry in the CLARIN Concept Registry (CCR,   (Schuurman et al.,   2016)). For the ones that were lacking we created definitions and provided other relevant information required for inclusion into   the CCR. These are currently being evaluated by the CCR coordinators for inclusion in the CCR.  After our submission to the CCR, we made some new modifications to the profile, so there are new elements and values for which the semantics does not yet exist.    Metadata Descriptions using the CSD profile  We have described more than 80 software resources with the CSD profile. Describing these software resources resulted in various improvements of earlier versions of the profile. Most descriptions started from the information contained in an unformalized software overview. The information from this overview was semi-automatically converted to CMDI metadata in accordance with the CSD profile. The resulting descriptions were further extended and then submitted to the original developers and CLARIN Centres that host the resources for corrections and/or additions.      Faceted Search   A major purpose of metadata is to facilitate the discovery of resources. An important instrument for this purpose in CLARIN is the Virtual Language Observatory (VLO, (  Van Uytvanck,   2014)). The VLO offers faceted search for resources through their metadata, but its faceted search is fully tuned to the discovery    of  data . For this reason, we defined a new faceted search, specifically tuned to discovery of  software . This faceted search offers  search  facets and  display facets:   Search Facets  LifeCycleStatus, ResearchPhase, toolTask, researchDomain, linguisticsSubject, inputLanguage, applicationType, NationalProject, CLARINCentre, input modality, licence   Display Facets  name, title, version, inputMimetype, outputMimetype, outputLanguage, Country, Description, ResourceProxy, AccessContact, ProjectContact, CreatorContact, Documentation, Publication, sourcecodeURI, Project, MDSelfLink, OriginalLocation   Furthermore, all metadata profiles for the description of software must be able to provide the values for the facets. That is the case to a large extent, though some metadata curation is needed in some cases and existing values must be mapped to a restricted vocabulary for use in the faceted search.    Curation of existing metadata for software  The basic idea is as follows: we create a new standardised metadata record for all software descriptions, in principle each time a record is harvested. This metadata record contains the components and elements that are required for the faceted search as defined above. The record is constructed from the original CMDI record for the resource, combined with the data for this resource contained in a curation file, by a script. The curation file can be used to add information that was lacking or only present in an unformalised way, and it can be used to map existing values to other values, e.g. to restrict them to a specific closed vocabulary. We report on our experiments with such a curation file for the WebLichtWebService profile, since it was most needed and most complex for this profile. Over 280 WebLicht Web Services can now be found with the faceted search.    Concluding Remarks  The faceted search is publicly available and in use by digital humanities researchers. We already received feedback from users for further improvements, which we hope to make in the course of 2019. We will also describe some problems we encountered, which we only briefly mention here: (1) definition of closed vocabularies (2) several technical problems related to the CLARIN Component Registry (3) lack of good CMDI metadata editors. Finally, we will identify some future work, in particular on deriving CLRS registry entries for CLAM-based applications and web services,    https://proycon.github.io/clam/      for extracting metadata information from independent initiatives such as codemeta.    https://codemeta.github.io/       References  Daan Broeder, Menzo Windhouwer, Dieter Van Uytvanck, Twan Goosen, and Thorsten Trippel . (2012). CMDI: A component metadata infrastructure. In  Proceedings of the LREC workshop ‘Describing LRs with Metadata: Towards Flexibility and Interoperability in the Documentation of LR’. , pages 1–4, Istanbul, Turkey. European Language Resources Association (ELRA).   Davor Ostojic, Go Sugimoto, and Matej Ďurčo . (2017). The curation module and statistical analysis on VLO metadata quality. In Selected papers from the CLARIN Annual Conference 2016, Aix-en-Provence, 2628 October 2016, number 136 in Linköping Electronic Conference Proceedings, pages 90–101. Linköping University Electronic Press, Linköpings Universitet.    Ineke Schuurman, Menzo Windhouwer, Oddrun Ohren, and Daniel Zeman . (2016). CLARIN Concept Registry: The New Semantic Registry. In Koenraad De Smedt, editor, Selected Papers from the CLARIN Annual Conference 2015, October 1416, 2015, Wroclaw, Poland, number 123 in Linköping Electronic Conference Proceedings, pages 62–70, Linköping, Sweden. CLARIN, Linköping University Electronic Press.   http://www.ep.liu.se/ecp/article.asp?issue=123&article=004   .    A. van den Bosch, G.J. Busser, W. Daelemans, and S. Canisius . (2007). An efficient memory-based morphosyntactic tagger and parser for Dutch.   In F. Van Eynde, P.  Dirix, I. Schuurman, and V. Vandeghinste, editors,  Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting , pages 99–114. Leuven, Belgium.    Dieter Van Uytvanck.   (2014).  How can I find resources using CLARIN?  Presentation held at the  Using CLARIN for Digital Research  tutorial workshop at the  2014 Digital Humanities Conference , Lausanne, Switzerland.   https://www.clarin.eu/sites/default/files/CLARIN-dvu-dh2014_   VLO.pdf  , July.    Claus Zinn . (2016). The CLARIN language resource switchboard.   https://www.clarin.eu/   sites/default/files/08%20-%20ZINN-Lg-Sw-Board.pdf  . Presentation at the CLARIN 2016 Annual Conference.   ",
        "article_title": "Faceted Search for Discovering Software",
        "authors": [
            {
                "given": "Jan",
                "family": "Odijk",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, Netherlands, The",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "linguistics",
            "natural language processing",
            "speech processing",
            "history and historiography",
            "English",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Visionary Cross Project is medium-sized international digital scholarly edition and archive of texts and objects from Anglo-Saxon England (O’Donnell et al., 2012; Rosselli Del Turco, 2014). The objects it studies include two stone crosses, an altar cross/reliquary, and several poems from the Vercelli Book, one of the four main manuscripts of Old English verse. The objects range in size from approximately 5 m to little more than 30 cm. They date from the 8th through the 11th centuries. They include several of the best known and most studied artifacts from the period (see among many others Ó Carragáin, 2005; Orton, 2004). The Visionary Cross Project is both a Digital Library and an edition (O’Donnell, 2013; Leoni et al., 2015; O’Donnell, 2016; Anderson, 2017). Its goals include providing a navigable library of digital replicas of these unique and important artifacts, and a scholarly mediation and assessment of their importance, key features, and relationships. This means that each object must be represented in an appropriate digital format: 3D models, 2D photography, and XML transcriptions. At the same time, the collection as a whole must be overseeable and navigable: users must be able to find their way from 3D object to XML transcription to 2D photograph and back, understanding the relationship of the parts to the whole. Finally, a design goal since the very beginning has been that the project must be easily extensible without negotiation. That is to say that other projects, researchers, and users must be able to discover, access, and reuse independently individual elements of our edition in their own projects without requiring either write-access to our infrastructure or additional permissions beyond the standard licences. In several cases, capturing our data required interventions with the environment that the owners of these objects are unlikely to allow again soon. In such circumstances, others need to be able to discover and access all our material and be able to reuse it for their own purposes. In this paper, we discuss the approach we are taking to the publication of this collection. In particular, we show how Zenodo and Github can be used together in an interrelated set of workflows to disseminate content as both a (machine and human-readable) Linked Open Data Digital Library and a (human readable) Digital Scholarly Edition.  GitHub is a free commercial software development platform, recently purchased by Microsoft, that is widely used by developers to host and review code, manage projects, and build software; it provides a free integrated Jekyll-based web-server that can be used to serve out webpages built within the repository (Wikipedia contributors, 2018; see Spiro, 2016 for a discussion of the use of GitHub in DH). Zenodo is the data repository of the European Community OpenAIRE project. It is hosted by CERN and provides ready and free access to and storage and tools more commonly used for “Big Science” projects (Wikipedia contributors, 2017; Zenodo). In addition to allowing uploads of data from scientific projects, Zenodo can also be used to archive snapshots of projects built in Github, providing long-term preservation and discovery aides such as DOIs to individual GitHub releases (Potter and Smith, 2015; GitHub, 2016). Taken together, these services provide researchers with a complete, free, and archivally responsible way of storing and disseminating data and longer form analyses in formats that work well for both machines and human readers. We are aware that GitHub is a commercial repository (now owned by Microsoft) that does not guarantee long-term preservation. As we demonstrate in the paper, however, the use of GitHub in conjunction with Zenodo does provide archival quality protection for the codebase, each snapshot of which is preserved as a Zenodo record. Zenodo guarantees preservation for the life of CERN + 20 years.  Although we discuss these issues and our solution in the context of the Visionary Cross project, our paper is very much not a project report. The approach we present in this paper simply and easily addresses a number of long-standing issues surrounding the longevity, extensibility, and reusability of data and results in the Digital Humanities:  It ensures the easy discovery and long-term survival of published data and results with no requirement for future maintenance (Publish-and-Forget); It conforms to archival standards and principles for longevity, machine-readability, and linking and is easily scaled (up or down); It is fully available for future extension, addition, excerption, reuse, repurposing, or reanalysis by others without negotiation; It ensures that data and contextual analysis are linked bi-directionally, meaning that users are always able to access the discrete data points from which a Humanities-focused analysis and commentary is built and understand each data point in the context of these larger synthetic research products.   Just as importantly, the techniques we exemplify using objects and analyses from our collection are easily generalised and extended to any small or medium-sized scholarly edition or cultural heritage collection, especially if these objects and analysis are processed by hand or partial automation. These are projects that, in particular, may otherwise find it difficult to justify devoting resources to developing or implementing the workflows and infrastructure necessary to publish datasets or individual pieces of data in a linkable fashion, especially if synthetic, humanities-focussed research and results are the project’s main interest. Although our focus in this paper is on small and medium-sized humanities and cultural heritage projects, the systems we are discussing were developed originally to support “big” science, which commonly works with data on a scale far beyond even the largest Digital Humanities or Cultural Heritage project. In our discussion, we indicate how our approach can be adjusted for use with fully or largely automated processing and publication systems typical of larger Humanities and Cultural Heritage projects. In conclusion, the main value of this work is that it shows how Linked Open Data standards and techniques more typically thought of in the context of large corpora and libraries can be used productively in small and medium scale projects, especially those in which more traditional, synthetic humanities commentary and research are understood to represent a major or even predominant part of the intellectual value of the edition or collection.  At the same time, our paper also shows how data (including collections containing heterogeneous and “unusual” types such as 3D) and analysis can be published in an easily discoverable, archivally sound fashion that requires little to no subsequent maintenance while remaining open to non-negotiated reused and extension.  And finally it demonstrates how data publication can complement rather than compete with traditional longer-form humanities research, ensuring that users can access bidirectionally the discrete data upon which an argument is built and the traditionally more synthetic humanities analysis and commentary that typically provides an essential context in small and medium sized collections and editions.  ",
        "article_title": " Publishing (and Forgetting) the Small or Medium-sized Scholarly Edition or Cultural Heritage Collection as Linked Open Data: Using Zenodo and Github to Publish the Visionary Cross Project  ",
        "authors": [
            {
                "given": "Daniel Paul",
                "family": "O'Donnell",
                "affiliation": [
                    {
                        "original_name": "University of Lethbridge, Canada",
                        "normalized_name": "University of Lethbridge",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/044j76961",
                            "GRID": "grid.47609.3c"
                        }
                    }
                ]
            },
            {
                "given": "Gurpreet",
                "family": "Singh",
                "affiliation": [
                    {
                        "original_name": "University of Lethbridge, Canada",
                        "normalized_name": "University of Lethbridge",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/044j76961",
                            "GRID": "grid.47609.3c"
                        }
                    }
                ]
            },
            {
                "given": "Dot",
                "family": "Porter",
                "affiliation": [
                    {
                        "original_name": "University of Pennsylvania",
                        "normalized_name": "University of Pennsylvania",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00b30xv10",
                            "GRID": "grid.25879.31"
                        }
                    }
                ]
            },
            {
                "given": "Roberto",
                "family": "Rosselli Del Turco",
                "affiliation": [
                    {
                        "original_name": "Università degli Studi di Torino",
                        "normalized_name": "University of Turin",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/048tbm396",
                            "GRID": "grid.7605.4"
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Callieri",
                "affiliation": [
                    {
                        "original_name": "ISTI-CNR",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Matteo",
                "family": "Dellepiane",
                "affiliation": [
                    {
                        "original_name": "ISTI-CNR",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Roberto",
                "family": "Scopigno",
                "affiliation": [
                    {
                        "original_name": "ISTI-CNR",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "digital humanities (history",
            "open content and open science",
            "scholarly editing",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries",
            "scholarly publishing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this paper we report our experiences developing and applying a set of digital infrastructure elements which, in combination, realise a layered digital library (Page et al 2017) for the investigation of manuscript provenance. We describe several related technical contributions: encoding of manuscript catalogue and local authority records as TEI; using Github for version control, issue tracking, and collaboration; automated production of catalogue user interfaces derived from the TEI; an XML processing workflow identifying, extracting, and processing TEI elements for reuse in research; mapping workflow output into a CIDOC-CRM RDF export; reconciliation of RDF entities with external authorities enabling the creation and use of Linked Data bridging multiple datasets. We contextualise the co-evolution of these components and exemplify their use in studies of the provenance of medieval manuscripts. We reflect on the flexibility and extensibility provided by our layered approach, and the independent benefits for catalogers and scholars.   Catalogue implementation and Linked Data workflow  The foundation layer of the approach described herein is the TEI encoding of manuscript metadata undertaken by the University of Oxford Bodleian Libraries. TEI has previously been used to encode text-based catalogues of manuscripts     , and we briefly reference the particular problems and solutions posed for the Bodleian Libraries previously described elsewhere (Holford, Hankinson, and Morrison, 2018).  The digital catalogue records are mostly derived from earlier printed catalogues, though in many cases they have been enhanced and updated for the digital catalogue. More than 9,200 Western medieval manuscripts are described. TEI XML was chosen for this detailed cataloguing of manuscripts because of its rich and flexible syntax: it can encode a complete retrospective conversion of existing catalogue description texts, adding structured markup of specific concepts and identifiers adapted to the various formats of historical catalogues, while allowing a variable degree of comprehensive or selective markup as required or desired. Catalogue records are implemented using a customisation of the TEI P5 manuscript description module     , with minor variations for Western, Islamic and Oriental manuscripts. Significant effort has been invested in the creation of local authority files for works, people and places, also using TEI. These have been, in turn, manually reconciled with URIs of records in external authorities such as VIAF, Library of Congress, Bibliothèque nationale de France, Système Universitaire de Documentation, Gemeinsame Normdatei, and WikiData.  TEI records are created and edited in the Oxygen editor, and stored in repositories      using the Git version control system. GitHub provides for issue tracking and collaboration - requests for modifications or additional markup in support of researcher investigations, such as that described in this paper, can be added, trialled, reverted, or otherwise properly and consistently managed without negatively impacting on the traditional library functions of the catalogue.  This TEI layer is, therefore, focussed purely on the creation and maintenance of the XML record files, with appropriate support tools and functionality, and which could easily be transferred to an alternative repository systems should the need arise. While the TEI records are freely and openly available via GitHub, the primary interface for library users is a Medieval Manuscripts collections website     , where the full gamut of traditional searching, browsing and viewing functionalities are provided. The website is built using open source technologies including XSLT, XQuery, Solr and Blacklight     . Since the website is generated atop a version-controlled check out of the TEI catalogue layer, it too can be developed independently of the other layers.  A further benefit of this separation of concerns is the ability to create parallel specialised data layers targeted towards distinct areas of research, supplementing rather than supplanting the canonical TEI catalogue and core digital library functions. Here the flexibility and adaptability offered by TEI is excessive, perhaps counterproductive, for computational processing of the catalogue metadata. Answering specific research questions we instead desire reasoning over logically consistent relationships, such as those in the CIDOC Conceptual Reference Model   The CIDOC-CRM     , and an ability to cross-reference multiple corpora and authorities using Linked Data: we create a selective RDF layer derived from the catalogue metadata. While theoretically conversion could be include all available TEI elements and attributes, in practice undertaking the mapping is detailed and complex, and limiting scope according to the investigation engenders progress. RDF complements this approach through data structures well suited to future extensions retaining consistency.  The first stage of processing simplifies the TEI records, extracting pertinent information (manuscripts, parts, works, authors and other people, places) using XQuery      into a more rigid XML structure. This transforms records into a single file, conforming to desired metadata fields, normalising some data (e.g. languages), referencing authority files, and building URIs.  The second stage ingests our simplified XML into the 3M mapping tool (Oldman, Theodoridou and Samaritakis, 2010) for transformation to a data model combining entities and relationships from CIDOC-CRM and FRBRoo  An object-oriented version of FRBR harmonized with CIDOC-CRM . Entities are also reconciled with local and external authorities, and RDF is exported ready for querying against research questions.  In creating these two alternate layers atop the TEI encoded catalogue, we serve several distinct but complementary motivations: a robust, maintainable and consistent record system for cataloguing; a visible and discoverable interface for browsing and searching the catalogue; and a malleable data structure for detailed scholarly investigation. These parallel the affordances offered by the layers’ encodings, deploying TEI and RDF (and, similarly Solr/Blacklight) by their strengths. In the remainder of the paper we focus on the last of these motivations.    Application to manuscript provenance research  Having described the infrastructural components and overall workflow, we demonstrate the use of this novel digital library for research into the provenance of medieval manuscripts: their origins and movements, and the collectors and owners involved in their history. As the result of changes in ownership over centuries, European manuscripts are spread across the world in diverse library, museum and gallery collections. Information relating their often-complicated histories is dispersed and fragmented across numerous sources, compelling historians and other researchers to make painstaking and time-consuming searches of printed and online catalogues. Digital tools which can assist in these searches, recording their outcomes, are of great benefit; cross-referencing and reconciliation across catalogues even more so. As such, our ultimate aim is to search across multiple distributed catalogues      ,  Furthermore, the RDF described here for our manuscript catalogues has already formed part of Linked Data network combining records from the gardens, libraries and museums of the University of Oxford as part of the OXLOD project, including the Ashmolean, Museum of Natural History, and Pitt Rivers Museum. , using ontologies to resolve conceptual equivalencies and indirect relationships, overcoming differences in underlying catalogue structures, and so enabling unified searching and interrogation. Here, however, we constrain discussion to our completed implementation at the University of Oxford, noting it provides a template for equivalent layer creation over other catalogue systems, and that the queries below will be equally applicable to a cross-corpora search  Indeed this reusability and extensibility is one of our primary motivations for using Linked Data. .  There has been little previous work transforming TEI manuscript catalogues into RDF suitable for the combined data explorations described here. The Medieval Electronic Scholarly Alliance (MESA) published samples of transformations from the Walters Art Museum into the Dublin Core based schema      of the Advanced Research Consortium (ARC); while Compton and Schwartz (2019) outline the general motivations and benefits of TEI to RDF conversion.  For our modelling, we began by considering TEI markup for the manuscripts records themselves, which can be complex and hierarchical, often describing a manuscript divided into several parts each with its own history and containing works-within-works (e.g. a collection of poetry and individual poems). Information about the provenance of a manuscript is sometimes encoded with a single XML element describing the entire history of the manuscript, and sometimes as multiple elements each recounting one event in that history. Dates might be encoded with ‘date’ tags or attributes on the ‘provenance’ element, and so on. Given this inherent complexity within the data, we identified a reduced set of ‘frames of reference’ to practically scope our RDF conversion. Consulting with other manuscript scholars identified archetypal questions required of any data investigation. We include an illustrative selection of these queries here, which we reference to their realisation in TEI, simplified XML, 3M mapping, RDF. and SPARQL:  How many manuscripts were produced in Northern Italy and/or Lombardy? How many manuscripts were produced in London in the 15th century? How many manuscripts did French collectors acquire from dissolved English monasteries?  Our focus on manuscript provenance and associated research questions scoped our choice of elements and attributes to include in the simplified XML: selecting necessary entities, cross-referencing information from local authority files, and creating URIs required for the next stage of processing. The authority files themselves, being essentially flat lists, could be mapped in the 3M tool directly. Within 3M, we show examples of mapping from the TEI customisation to CIDOC-CRM and FRBRoo  For example, from the catalogue work item (TEI bibl) to F1 Work; msItem to F22 Self-contained Expression; and so on. , taking care to separate evidence derived directly from the text from that which embeds institutional knowledge (i.e. inscriptions require interpretation).  Finally, we provide examples of SPARQL resolving the research questions, above, paying attention to how data semantics can overcome complexities not immediately apparent in their natural language form. For example, breaking down a query to retrieve “manuscripts from 1550-1600 produced in European countries” entails reasoning over variable temporal constructs, mapping and resolution of external spatial definitions (Getty, Wikidata).  Acknowledgements: We are grateful for the support and insights provided by our colleagues participating in the wider projects surrounding this work: Prof. Donna Kurtz and Gabriel Hanganu of the Oxford Linked Open Data project (OXLOD); and Mapping Manuscript Migrations project team members at the the University of Pennsylvania Libraries, the Institut de recherche et d’histoire des textes, and the Semantic Computing Group at Aalto University.   ",
        "article_title": " A Layered Digital Library for Cataloguing and Research: Practical Experiences with Medieval Manuscripts, from TEI to Linked Data  ",
        "authors": [
            {
                "given": "Kevin",
                "family": "Page",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Toby",
                "family": "Burrows",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Hankinson",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Holford",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Morrison",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Lewis",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Athanasios",
                "family": "Velios",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom; University of the Arts London, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "libraries",
            "museums",
            "English",
            "library & information science",
            "ontologies and knowledge representation",
            "GLAM: galleries",
            "archives",
            "semantic web and linked data",
            "medieval studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  While there is increasing research about scholarly publication workflows through Linked Open Data in Archaeology (even from the viewpoint of non-specialists [Seifried, 2014; May et al., 2012]), little has been done to take advantage of its potential for teaching. However, LOD is going to be the main method of publication and outreach in the discipline: therefore, it is becoming increasingly important for students and teachers to become familiar with its structure. Linked Open Data is a powerful tool for navigating through the complexity of the inherently multifaceted reality of archaeological sites, which results from the intersections of space, materiality, language, visual culture, history, text, and so on. However, LOD also poses the challenge of how to manage such complexity in a meaningful way. In this paper, we report on an experimental project developed during a Classical Archaeology course in 2018, during which we researched four different Graeco-Roman sites, with the goal of reconstructing the main aspects of their material history through exclusively LOD-based resources.   The experiment We explored four sites: Ostia Antica, Herculaneum, Alexandria, and Eleusis. Because the designed ground of the exploration was geographical, we started by using resources that focused on location as the main interlinking structure. We started with   Peripleo  (Simon et al., 2016), the LOD-based search engine of Pelagios, a project whose main purpose is to connect together several partnered datasets, by using places as the common ground (Isaksen et al., 2014).  Preliminary choosing the sites to explore enabled to establish a certain hierarchy in the information: first, we gathered the resulting links to gazetteers (especially  Pleiades [Bagnall et al., 2018]), to collect information about the site, including relevant geographical connections and attested chronologies. We then used Peripleo’s internal vocabulary to isolate specific periods of use of our sites, and classified the resulting links according to the database of provenance.  Initially, we managed the richness of the results by focusing on specific typologies of information: so, we harvested links to   Fasti Online  (2004-) to collect data about excavations in our sites; we looked for artifacts through a large variety of databases, including   Ariadne  (2012),   Arachne ,   WikiData ,   Vici  (Voorburg, 2014), the   Perseus  (Crane, 2018), and   Flickr ; we used these resources to find pictures and basic information, and as a starting point towards other datasets from museums and archives.  Coinages and hoards were a prominent feature, which was explored through the resources of the American Numismatics Society (2016),   Nomisma , and the   Münzkabinett Online Catalogue  of Berlin. Textual sources were mainly collected through   ToposText , which was privileged for its focus on locations. Inscriptions were found through the   EAGLE Network  and the   University of Graz Online Portal .     From each of these resources, we were further directed in our exploration according to the emerging patterns in our findings: for example, the information collected through   Arachne  was extremely useful to access bibliographical data on religious artifacts in Eleusis; we used   Europeana  to further research Herculaneum, and from there we accessed the Bodleian Library collection of the Herculaneum Papyri and the related information about their context of finding, the Villa of Papyri, on  Pleiades (Bagnall et al., 2018). Researching Coinages of rulers in Alexandria provided a significant amount of information about the Roman emperors that controlled the city, through links to biographical databases (e.g.   VIAF ). EAGLE enabled us to even access  not LOD-based resources, such as the   Epigraphic Database of Rome , to collect alternative transcriptions.       Results We created object cards, specifying the essential information, the online identifier, and the further connections to other findings or contexts. Collectively, we were able to assemble approximately 50 cards, which, depending on the specificities of the site, included excavations, inscriptions, coinage, textual sources, monuments, mosaics, houses, papyri, sculptures (in bronze, marble, terracotta), pottery, and even furniture remains. We then wrote reports about each site, focusing on the best attested chronological periods (Herculaneum) or on prominent aspects of material history: in the case of Alexandria, for example, we propose an exploration of material evidence documenting cultural syncretism in the Graeco-Roman era.   Pedagogical and scholarly outcomes  Discoverability: through Linked Open Data, students are able to discover a variety of archaeological information with scientific reliability. Provided that the tools used are adequate to the purpose, this kind of exploration has a strong potential for non-experts, which is extended to the discovery of less known museums and archives around the world. Complexity: Linked Open Data enables to navigate through an immense set of records, discovering unexpected and continuously new types of findings. This contributes to create a rich picture of the complexity of an archaeological site, which is the result of several interconnected pieces of evidence. Contextualization: LOD makes it possible to explore contextualized information. Students are not only able to look for a specific artifact, but they can dig into the data that provide information on the context where it was found, the people and organizations related to it, the events connected to its chronology (Meadows and Gruber, 2014). Effective scholarly digital research: students can perform complex research across a variety of different resources, and learned how to navigate through the information of specialized datasets with very different structures, vocabularies and functions.    Limitations   There is still too much volatility in the adoption of shared vocabularies across LOD resources: this limits the range of searchable items, specifically types of archaeological findings. This is a well-known problem in the world of Archaeological Linked Open data, now further reinforced by user experience (Bechhofer et al., 2013; May et al., 2015).  While there has been much investment in LOD infrastructures, the actual availability of data and metadata is still questionable for some archaeological sites. Whereas sites like Ostia and Herculaneum display a considerable amount of semantically diversified and interlinked material, places like Alexandria are more of a challenge in researching accurate and rich information, for the scarce availability of digitized archaeological records.   ",
        "article_title": "Using Linked Open Data to Navigate the Past: An Experiment in Teaching Archaeology",
        "authors": [
            {
                "given": "Chiara",
                "family": "Palladino",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            },
            {
                "given": "James",
                "family": "Bergman",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            },
            {
                "given": "Caroline",
                "family": "Trammell",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            },
            {
                "given": "Eleanor",
                "family": "Mixon",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            },
            {
                "given": "Rebecca",
                "family": "Fulford",
                "affiliation": [
                    {
                        "original_name": "Furman University, United States of America",
                        "normalized_name": "Furman University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04ytb9n23",
                            "GRID": "grid.256130.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-10",
        "keywords": [
            "digital humanities (history",
            "archaeology",
            "theory and methodology)",
            "English",
            "pedagogy",
            "semantic web and linked data",
            "teaching",
            "and curriculum",
            "information architecture and usability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The collection of Complete Taiwan Poems (CTP) includes traditional Chinese poems that were authored between 1661 CE and 1945 CE in Taiwan.   This time period covers the Kingdom of Tungning in Taiwan (1662 - 1683), the Qing Dynasty in Taiwan (1683 – 1895), the Japanese occupation period (1895 - 1945) and the post-war period (1945 - )  We combine natural language processing and spatial information technology to analyze the literary trajectory and geographical distribution of poets and their poems. Assisted by digital tools such as geographic information systems (GIS) which can manage and present spatial data, we can describe the connection and evolution between poets, poems and space.    Spatialization of literary works In this study, we extract spatial and temporal information about the poets and their poems, and present the information via a GIS interface. Figure 1 shows the main steps of our work.   Figure 1. Main steps of our work   Identification of place names At this moment, we focus on poems that consist of five or seven-character lines because they are the majority in CTP. Table 1 shows the statistics for the poems in CTP. In order to spatialize the literary materials, we extract the words that carry spatial attributes by marking words as place names at the step of part-of-speech (POS) tagging. Before that, we have to segment Chinese strings into words. Chinese word segmentation is a very difficult task for traditional poems, and the segmentation results have a considerable impact on the subsequent analysis. In addition to employing the CKIP segmenter (Ma and Chen, 2003), we also rely on some heuristics for Chinese segmentation that are applicable to traditional Chinese poems.   Table 1. Statistics for the poems in CTP    Table 2. Statistics about the Nc words. The numbers in parentheses show the amount of the unique Nc words    Figure 2. The distributions of lengths of Nc words  Relying on a heuristic (Lo, 2005), we may segment a line of five characters in two ways: 212 and 221. For example, we can divide “奉命籌軍國” into “奉命-籌-軍國” (212) or “奉命-籌軍-國” (221). Similarly, we may segment a line with seven characters into two ways: 2212 or 2221, e.g., producing “昔日-東寧-今-豫章” (2212) or “昔日-東寧-今豫-章”(2221) from “昔日東寧今豫章”. Applying these heuristics with the CKIP segmenter, we segment the CTP poems. The CKIP segmenter also does POS tagging, and Nc is of the code for place names. We show some statistics about the place names in Table 2.   Filtering and geocoding After the POS step, we filter the list of Nc words for unique place names. Figure 2 shows the distribution of lengths of these unique words. After filtering the place names, we attempt to geocode the unique place names. The process of geocoding is shown in Figure 3. We do geocoding in two steps. First, we use the API of Chinese Civilization in Time and Space (CCTS API) provided by the Academia Sinica.   This is a service created by the Center of GIS, RCHSS, Academia Sinica. The place names are geocoded by specialists based on many historical documents. Available from: http://ccts.sinica.edu.tw/api [cited 2019 April 29]  CCTS reports reliable spatial information for places in Taiwan and China. If we cannot find a location for a name via CCTS, we continue geocoding via the Google Map API.  Google. Developer Guide | Geocoding API [cited 2019 April 29]. Available from: https://developers.google.com/maps/documentation/geocoding/intro.  We ignore names that cannot be geocoded by both services.     Figure 3. Flowchart of the place name geocoding  Recall that the results of the word segmentation are not completely correct, so there may be some errors with geocoding. For instance, some place names are geocoded to other countries. To avoid these problems at this moment, we use a spatial processing method to eliminate the points outside Taiwan and China.    Application examples We can present and analyze the data with GIS, and offer three applications to demonstrate the spatial visualization of literary data.  Temporal and spatial distribution of poets' birthplaces After analyzing the personal data of 844 poets, we can show the poets' birthplaces on maps. They lived in different dynasties. With these two variables, we generated four maps for different time periods, shown in Figure 4.   Figure 4. Temporal and spatial distributions of poets' birthplaces  If scholars have more ideas about the distribution of data, the data can be further analyzed through spatial methods. Figure 5 shows a further example. It analyzes the distribution of poets in parts of Taiwan. It shows that the changing distribution densities of poets in Tainan (in the south), Taipei (in the north), and Changhua (in the middle) in these time periods. This may provide hints for studies of the socioeconomic developments and cultural activities of those years.   Figure 5. Kernel density estimation maps of poets in Taiwan    Temporal and spatial distribution of place names in poems When literary experts can confirm the creation times of poems, we can analyze the places that were mentioned in poems of different time periods. Figure 6 shows some possible outcomes.   There are 12 points (place names) in Figure 6(a), 1674 points in Figure 6(b), 3210 points in Figure 6(c), and 221 points in Figure 6(d), respectively.     Figure 6. Temporal and spatial distributions of place names in CTP  Splitting the 200 years of the Qing administration in Taiwan into four sub-periods, Figure 7 helps us see that some place names are more popular in different periods. Place names of the eastern part of Taiwan were mentioned less in the early Qing Dynasty. Scholars can discover more phenomena based on the spatial-temporal results, and interested scholars can certainly investigate the implications of the changing trends.   Figure 7. Distributions of place names in Taiwan in CTP poems in different Qing periods    A poet's trail We analyze the contents of the poems and personal data of Lian Heng,   Lian Heng (連橫) was a very famous and influential person in the history of Taiwan.  and show the results in Figure 8. When he was young (before 36 years old), most of the places he mentioned were in Taiwan and China. During his middle age (37-46), he mentioned many places in Taiwan and Japan. In his older period (after 47), he mainly focused on Taiwan, occasionally mentioning the place names of China and Japan.     Figure 8. The trail of Lian Heng’s place names in different periods of life  We can find that poets may have some preferences for the place names in their works in different periods. Through the spatiotemporal analysis of the poets' trails, we may pursue deeper issues about the observations. Wang et al. have done similar analysis for Yu Yonghe (郁永河). (Wang et al., 2011)    Conclusion We analyzed the poets and poems in the Complete Taiwan Poems via GIS viewpoints. By combining the techniques of natural language processing and the methods of spatiotemporal analysis, we present information about poets and poems geographically. We demonstrate our ideas with three examples: a poet's trail, the distributions of poets’ birthplaces, and the distributions of place names in the poems. We hope that these cross-disciplinary explorative results may inspire fruitful ideas for literary research. Acknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-06, 107H121-08, 108-H121-06, and 108H121-08 of the National Chengchi University. We are grateful to the National Museum of Taiwan Literature for their sharing the Complete Taiwan Poems and their comments on our work.  ",
        "article_title": "Some GIS-Based Analysis of the Complete Taiwan Poems",
        "authors": [
            {
                "given": "Yi-Fan",
                "family": "Peng",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan; Research Center for Humanities and Social Sciences, Academia Sinica, Taipei, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-15",
        "keywords": [
            "modeling and visualization",
            "content analysis",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Purpose A  bibliographic citation is a conceptual directional link from a citing entity to a cited entity, for the purpose of acknowledging or ascribing credit for the contribution made by the author(s) of the cited entity, as stated by Newton in his motto – “If I have seen further it is by standing on the shoulders of Giants” (Newton, 1675). The citing and cited entities may be scholarly publications, online documents, blog posts, datasets, or any other authored entity capable of giving or receiving citations. While the act of citing by the author may be the work of a moment, the citation itself, once the citing work is published, becomes an enduring component of the academic ecosystem.  This abstract introduces the uptakes and the benefits of releasing a huge set of  open citation data as public domain material. In particular, we introduce the main events that have characterised this movement towards opening citations by discussing what has happened with the establishment of the Initiative for Open Citations. We accompany the discussion with examples and projects that were born as a consequence of this movement, including some contributions from the Digital Humanities domain.    Definition As stated by Peroni and Shotton (Peroni and Shotton, 2018), a bibliographic citation is an  open citation when the data needed to define the citation are FAIR (Wilkinson et al., 2016) and, in particular, are:   Structured – expressed in one or more machine-readable formats, such as XML, JSON, RDF; Separate – available without the need to access the source bibliographic entity (e.g. the article or book) in which the citation is defined; Open – freely accessible and reusable without restrictions, for example by publication under the CC0 1.0 Universal waiver/license or generally released as public domain material; Identifiable – the citing and cited entities described by an open citation must be clearly identified by using a specific persistent identifier scheme (e.g. a DOI) or a URL; Available – there must exist a mechanism to resolve the identifiers of the citing and cited entity to obtain the  basic metadata of both the entities, i.e. sufficient information to create or retrieve textual bibliographic references for each of the entities.     Findings The first project that introduced the open availability of open bibliographic and citation data by means of Semantic Web (Linked Data) technologies was the OpenCitations Corpus, in 2010, which was one of the outputs of the Open Citations Project funded by Jisc (Shotton, 2013). However, the availability of open citation data recently changed drastically with the establishment of the  Initiative for Open Citations (I4OC,  https://i4oc.org), in April 2017.   The Initiative was born with the idea of promoting the release of open citation data and has explicitly asked the main scholarly publishers, who deposited their citations on Crossref (https://crossref.org), to release them in the public domain. As a result, now we have several millions of citation data openly available on the Web, a list of important stakeholders – such as libraries, consortiums, projects, organizations, companies, and, in particular, founders (Shotton, 2018) – supporting the movement, and several international events (e.g. the  Workshop on Open Citations and  WikiCite 2018) organised for promoting the open availability of citation data. Several projects and datasets have been released so far so as to leverage the open citation data available online. As a result, there is a growing list of publishers that release their citation data in Crossref, and these citation data also come from important journals in the Digital Humanities field such as  Digital Scholarship in the Humanities published by the Oxford University Press.    Research implications Several citation indexes, such as Clarivate Analytics's Web of Science and Elsevier's Scopus, make available citation data in a structured form. However, their access is possible only by paying expensive fees of several hundreds of dollars (Chadegani, 2017). In addition, the license associated with these data is quite restrictive and does not allow one to reuse them for any purpose. The current coverage of the open citation data available is still far from being competitive with the aforementioned well-known proprietary citation indexes (van Eck et al., 2018). However, open citation data already allows researchers – particularly those ones working in institutions that cannot pay exaggerate fees to access the aforementioned commercial indexes – to work on citation data and perform interesting and important discoveries.   Practical implications Collecting all the citation data from the whole scientific knowledge in just one centralised repository is practically unfeasible. The only long-term solution is to set up a federation of decentralised citation databases that can co-operate with each other by means of Web technologies, as suggested in (COAR Next Generation Repositories WG, 2017) – see, for instance, the RDF datasets made available by  Wikidata and  OpenCitations.    Social implications Citations have been one of the core parts of the scholarly system since the beginning. However, they are not currently seen only as an acknowledgement medium, but rather have recently been used according to additional functions. For instance, citation networks can be characterised (a)  topologically by defining the connected graph between citing and cited entities during time (Chawla, 2017), (b)  sociologically such as for identifying odd conducts in or elitist access paths to scientific research (Sugimoto et al., 2017), (c) according to  quantitative rationales by creating citation-based metrics for evaluating the impact of an idea or a person (Schiermeier, 2017), and (d) defining a sort of  economic value, i.e. the currency with which a researcher addresses his/her own academic sustenance (Molteni, 2017).  Having open citation data available is fair, since it enables anyone, from scholars to citizen scientists, to study and follow the evolution of science during time according to all the aforementioned perspectives.   Value Open citation data makes a positive disruption in the world of scholarly communication since they change entirely how we face to science, its evolution, and all the related context, such as research assessment evaluations, science of science, bibliometrics, and future scientific discoveries in all the domains – including the Digital Humanities domain.  ",
        "article_title": " The Open Citations Movement  ",
        "authors": [
            {
                "given": "Silvio",
                "family": "Peroni",
                "affiliation": [
                    {
                        "original_name": "Department of Classical Philology and Italian Studies, University of Bologna, Bologna, Italy",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-04",
        "keywords": [
            "copyright",
            "open access",
            "open content and open science",
            "English",
            "library & information science",
            "licensing",
            "databases & dbms",
            "scholarly publishing",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The aim of this paper is to introduce to stylometry the methods allowing for evaluation of classification results obtained with (i) hierarchical clustering methods, with the distinction of performance of individual linkage methods, and (ii) network clustering, with the comparison of community detection techniques. We compare three recognized evaluation measures: AMI, ARI and NMI using 6 model datasets of known clustering, of which three constitute binary problems and three – corpora with a large number (25) of expected internal groups, which were designed for authorship attribution (or similar multiclass problems). Our results show (i) superiority of Ward linkage method as compared to 5 other, (ii) greater performance and stability of Cosine Delta for both hierarchical and network clustering, (iii) Louvain as the most reliable method of community detection, and (iv) usefulness of AMI method for hierarchical clustering, which we propose for general use making our scripts available.   For visualizing the stylometric structure of a text corpus, many studies and popular tools like ‘stylo’ (Eder et al., 2016) rely on explanatory methods and intuitively-interpretable visualizations, usually belonging to either tree charts as produced by hierarchical clustering, or network representations. The intuition behind it is that the graphs will group texts of high stylistic similarity close together, thereby allowing the spectator to visually identify multiple levels of groupings instead of simply singling out one nearest neighbour for each individual text. There were even some methodological studies on stylometric distance measures based on evaluating the clusterings they produce (Jannidis et al., 2015). Despite the popularity dendrograms and networks have in stylometry, there is to this date no systematic investigation on how these methods behave when applied to problems in computational stylistics – research usually focuses on method selection from general pool of classification approaches. In clustering, both fine-grained clusters (e.g. groupings by author) and larger groups containing several texts (e.g. higher level genre-wise groupings of multiple authors) are generated by the method itself – a very reproducible process  – but particular clustering method used is a matter of choice. Most cases rely on the ‘Ward’ linkage method (Ward, 1963) which is the default in ‘stylo’, but its advantages and disadvantages for the research field have not been investigated systematically. E.g., Hoover’s choice of Ward linkage is based on a concise comparison of Ward, complete, and average linkages (Hoover, 2003), but the methodological design of the comparison has not been outlined in the paper.  The case of network analysis is even more problematic, as low and/or high level groups are often/usually only identified visually. The best approach to operationalize  how people perceive group formation in networks is to use community detection.    This study aims to constitute an initial step to the systematic investigation of these issues, which we do by analyzing the quality of grouping achieved by different clustering and community detection algorithms in comparison to known properties of different test corpora.   Corpora We conducted our study on 6 text corpora, each assembled with an inherent internal structure with groups expected to be stylistically distinct. We used both corpora with a large number (25) of internal groups, and corpora that can be separated into two major stylistic groups.  As examples with a large number of low-level groups, we used three reference corpora for authorship attribution, each composed of 75 novels by 25 authors. These corpora, from English, French and German literature were previously used in various studies on text distance measures (e.g. Jannidis et al. 2015) and are freely available (  ).   As examples for problems featuring two major stylistic groups, we took (1) a corpus of 17th century French drama published by Christof Schöch (  ), where we removed all texts not clearly labeled comedies or tragedies; (2) a corpus containing Latin verse and prose from the so-called Golden Age; (3) a corpus of Latin historiography containing texts from the very same Golden Age (late first century BCE) on the one hand, and the “Silver Age” on the other. The Latin materials were assembled from the Latin Library (  ).    Methods: Clustering quality measures There are many similarity indices for clustering evaluation (see Albatineh et al. 2006; Wagner and Wagner 2007; Vinh et al. 2010 for their comparison). In this pilot study we focus on three best understood: ARI (Adjusted Rand Index; Hubert and Arabie 1985), AMI (Adjusted Mutual Information) and NMI (Normalized Mutual Information). The first two are adjusted for baseline value in case of random clustering (Vinh et al. 2009, 2010) but not fully for selection bias (i.e. usually bias to select a clustering having higher number of clusters than ground truth; Romano et al. 2014, 2016), while the third one is not adjusted. Standardized Mutual Information (SMI;  Romano et al. 2014) might offer a further improvement in future studies.    Methods: Community detection methods in networks Community detection methods allow for automatized search for natural division of network into smaller clusters, that is communities. They can be applied to bare distance tables (resulting in complete weighted graphs), or on pruned or otherwise sparse networks (such as consensus networks in Eder 2017). We use “A not so small collection of clustering methods” facilitating consensus clustering (Lancichinetti and Fortunato 2012), which includes such algorithms as  OSLOM (Lancichinetti et al. 2011), Infomap (Rosvall and Bergstrom 2007), label propagation method (Raghavan et al. 2007), and modularity optimization by simulated annealing (Guimera et al. 2004) and by Louvain method (Blondel et al. 2008).   Methods: Experiments We first conducted a large series of stylometric tests that were later used for evaluation. For each dataset we conducted myriads of tests for a total of 160 basic experimental scenarios, being a combination of the following changing factors of: (i) number of MFWs (100 to 1000, iterated by 100), (ii) distance measure (Classic and Cosine Deltas) and (iii) linkage method (single, complete, average, McQuitty's, Ward in two implementations: “ward.D”, “ward.D2”, \"median\"  – k-median, and \"centroid” – k-means). We then evaluated quality of the clusterings in the results, considering the number of clusters ranging from 2 to the number of texts in the corpus using a script implementing ARI, AMI and NMI indices. E.g.: for any of the 75-book corpora, we would obtain 160 scenarios and test 74 clustering options for each of them, obtaining a total of 11,840 clustering scenarios up for evaluation.  We performed similar experiments utilising networks: in one, we used all the mentioned community detection methods together with consensus networks; in the second experiment, we used distance tables instead. Let us note that modularity optimization is well defined in the case of directed and weighted networks (Arenas et al. 2007) and indeed it is the only one that produces non-trivial clustering of distance tables (since distances are inversely proportional to network weights we chose a transformation – from among the infinitely many  – from one to the other  w=exp(-d) , yielding weights in the range (0,1). Owing to that we could use consensus clustering over distance tables that were generated by a range of the selected most frequent words (as one does with consensus bootstrap trees).    Results: hierarchical clustering In all of our tests, Ward’s linkage outperformed other algorithms, both for Classic and Cosine Deltas. Notably, this particular linkage method was designed for large-scale tests of more than 100 samples: the speed, not the optimal clustering was a priority (Ward 1963: 236). Cosine Delta proved superior of the two distances – it systematically scored higher and gave maximal agreement with ground truth at the same number of clusters, even for a small MFW number.      Fig. 1 Clustering quality for the French corpus, Cosine (Wurzburg) Delta distance measure and Ward linkage using different MFW ranges. An example of clustering assessment, here for the French corpus, is shown in Fig. 1. Vertical dashed line marks the expected number of clusters (here, 25 authorial classes). Each curve represents values of one quality measure for a given number of MFWs, and dots mark their maxima. AMI maximum falls around the expected and intuitive number of authorial clusters. NMI is heavily biased towards a larger number of smaller clusters, in line with its characteristics as described in the method section, proving that it should not be recommended as a quality measure for this type of problems. The conclusions for all considered corpora are qualitatively similar.      Fig. 2 Clustering quality for different linkage methods – Ward performing best, and centroid  worst – at 1000 MFWs. Layout as in Fig. 1.   Results: networks The results of Louvain method on distance tables indicate that this scenario should be avoided: neither Classic nor Cosine Delta could reach AMI=0.2 on any corpus, which is worse than most of the hierarchical clustering methods (cf Fig. 2). However, a more sophisticated approach involving undirected consensus networks (Eder 2017) offers much better results, see Fig. 3–4: the community detection methods rarely score below AMI=0.4 (on the exemplary French authorial corpus), and the best one scores above AMI=0.6 which contends for the first place with Ward linkage clustering. While hierarchical clustering typically provides a number of clusters higher than expected, community detection tends to produce a smaller number of larger clusters. This might be caused by the less detailed (and less noisy) information contained in the consensus networks. One should also take into account that hierarchical clustering did not take advantage of the consensus scheme and hence is computationally less demanding.        Fig. 3 Layout analogous to Fig. 1–2. Results of community detection on consensus networks based on Classic Delta. Each community detection method provided one clustering for the consensus network. All methods predict smaller number of clusters than expected. Louvain method typically scores highest.          Fig. 4 Layout as in Fig. 3. Results of community detection on consensus networks based on Cosine Delta.    Conclusion Our study provided empirical proof for the choice of Ward linkage method in clustering applications on literary text, and further strengthened the argument for the use of Cosine Delta as a method more resistant to changes in the number of used MFWs and factors such as language or size of the corpus. The best community detection methods, which again make use of Cosine Delta, can contend with hierarchical clustering, however, they clearly require previous data filtering, e.g., by means of constructing consensus networks. Interestingly, they offer a complementary more coarse-grained view. Importantly, we also propose introduction of clustering evaluation step into the analysis, in particular AMI method which worked best for all model cases with expected divisions. We will make the evaluation script available in our github repository, encouraging other scholars to use this or similar techniques. Acknowledgements  JB was partially funded for the research by Poland's National Science Centre (grant number 2017/26/HS2/01019), SP contributed to this research as part of a Short Term Scientific Mission financed by the EU COST Action “Distant Reading” (CA16204), ME was partially funded by the National Science Centre (grant number 2014/12/W/ST5/00592).   ",
        "article_title": " Identifying Similarities in Text Analysis: Hierarchical Clustering (Linkage) versus Network Clustering (Community Detection)  ",
        "authors": [
            {
                "given": "Ochab",
                "family": "Jeremi",
                "affiliation": [
                    {
                        "original_name": "Institute of Physics, Jagiellonian University, Kraków, Poland",
                        "normalized_name": "Jagiellonian University",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/03bqmcz70",
                            "GRID": "grid.5522.0"
                        }
                    }
                ]
            },
            {
                "given": "Byszuk",
                "family": "Joanna",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language, Polish Academy of Sciences, Kraków, Poland",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            },
            {
                "given": "Steffen",
                "family": "Pielström",
                "affiliation": [
                    {
                        "original_name": "Department of Literary Computing, University of Würzburg, Germany",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Eder",
                "family": "Maciej",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language, Polish Academy of Sciences, Kraków, Poland",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "digital humanities (history",
            "stylistics and stylometry",
            "theory and methodology)",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The history of the composition of French saint’s Lives collections in prose is still a mystery. At the beginning of the XIII th century, legendaries were already constituted and intermediary steps are missing to understand how the Lives have been gathered. One of the existing hypotheses is that small collections of saint’s Lives might have circulated as small independent units about one saint or a series of saints (Philippart, 1977), sometimes traceable to a single author or translator, under the material form of libelli.  The first milestone in the study of the composition of legendaries was laid by Paul Meyer (1906). His studies led him to discover that some of these legendaries were derived from successive compilations. Using their macrostructure, Meyer dispatched them into families. He also had the intuition that the families were constituted of smaller components, groups or rather sequences of texts. He proposed the existence of primitive series based on authorship and the repetitive grouping of selected lives, such as the hagiographic collection of Wauchier de Denain’s Seint Confessor of the family C or the consecutive and recurrent series in the family B and C of the following three lives: Sixte, Laurent and Hippolyte. However, because most of the French saint’s lives are anonymous and because the collections were rearranged by multiple editors over time, it is extremely difficult to find what could have been the primitive series and Meyer couldn’t go further. This serial composition of the Lives of Saints is a datum also noted by other specialists of Latin hagiography such as Perrot (1992) and Philippart (1977), who even points out that these hagiographic series must be studied in their entirety in the same way as a literary work. But it is still very rare today that hagiographic text editing concerns a complete author’s legendary, mostly because of a lacking certainty about these groupings.  From there, with an exploratory stylometric analysis, we first want to find if Meyer’s hypotheses are wrong, can be nuanced or completed. In a second time, we would like to discover if the observed proximity between saint’s lives can reveal series from the same author.      We based our study on a legendary from family C, namely the manuscript fr. 412 of the Bibliothèque nationale de France. The task is complex, because textual variants and the absence of a standardised spelling can affect the stylometric analysis (Kestemont et al., 2015). While the lemmatization of the texts could have nullified the problem of spelling variation, in our case, the preparation of the corpus would have been extremely time-consuming. We decided to work from witnesses written by a single hand in the same manuscript in order to limit biases that could have been induced by spelling variations linked to the scribes and not to the author.   The analysis of the complete legendary is made possible by the use of the OCR software Kraken (Kiessling, 2018), trained on about 8890 transcribed lines of ground truth and tested on 897 lines, which results in up to 95.2% success in Character Recognition Score. Such results allow us to hope that the stylometric analysis is not corrupted by the margin of error (Franzini et al., 2018).           We work from the text thus obtained. Because most of the texts of the legendary are anonymous, we follow an unsupervised approach to the analysis of the texts (Camps & Cafiero, 2012), using agglomerative hierarchical clustering with Ward’s criterion (Ward, 1963), guided by its ability to form coherent clusters.    The texts are, on average, quite short, a known difficulty for stylometry (Eder, 2015), with a median value of 16,863 characters (space excluded), but with extreme values of  1,364 and  85,378. Texts that are too short create a problem of reliability, as the observed frequencies may not accurately represent the actual probability of a given variable’s appearance (Moisl, 2011). To limit this issue, we removed texts below 5,000 characters. Because of the errors regarding segmentation in the OCR, we extracted character n-grams, ignoring spaces. We experimented with different lengths, but, following existing benchmarks (Stamatatos, 2013), we retained the 4000 most frequent 3-grams. The metric and choices of normalisation are also an important parameter, one to which much attention has been devoted (Evert et al., 2017; Jannidis et al., 2015).   Following the benchmark by Evert et al. 2017, we chose to use Manhattan distance with z‑transformation (Burrows’ Delta) and vector-length Euclidean normalisation. The results are partially presented in figures 1 & 2.     Dendogram of agglomerative hierarchical clustering using Manhattan distance, z-transformation and vector length normalisation over 4000 most frequent 3-grams        Dendogram of agglomerative hierarchical clustering using Kohonen SOM coordinates over 4000 most frequent 3-grams    Because, at the same time, the corpus is homogeneous, the texts can be quite short, and the data is noisy, separating them in stable clusters can prove quite hard. We tried to improve the quality of the signal by applying, first, a Kohonen self-organising map (Kohonen, 1988:59–69), and then using the coordinates of the points in the SOM for hierarchical clustering (Camps and Cafiero, 2012).   In addition, the specificity of composition of the legendary C by successive additions (lives of A, then lives of B, and finally addition of new lives) allows us to ensure a quick control of the likelihood of some proposed groupings. The presence of the hagiographic collection of  Seint Confessors of Wauchier de Denain where the author identifies itself twice (both in  Dialogues de Sulpice Sévère and in  Vie de saint Martial de Limoges) also serves as an indicator of validity.    The study has already shown interesting connections between the legendary of Wauchier de Denain and the  Vie de Saint Lambert de Liège and some collections have been revealed. Two of them are quite certain, one of the first five texts of C, all about saint apostles and hypothetically from A, and another one of six virgin saints’ lives, all from B including the Lives of saint Agathe, Lucie, Agnès, Felicité, Christine and Cécile.   To conclude, our analysis attempts to evaluate the best parameters for our study and to overcome certain difficulties inherent to our corpus. Two major obstacles have to be overcome: the lack of spelling standardisation and the lack of homogeneity in the separation of words. At the end of this prospective study, we hope to be able to reveal new hagiographic series prior to the composition of legendaries that were transmitted to us and that could have escaped us so far. ",
        "article_title": " Stylometry for Noisy Medieval Data: Evaluating Paul Meyer's Hagiographic Hypothesis  ",
        "authors": [
            {
                "given": "Ariane",
                "family": "Pinche",
                "affiliation": [
                    {
                        "original_name": "Ecole nationale des chartes, France; Université Lyon 3",
                        "normalized_name": "Claude Bernard University Lyon 1",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/029brtt94",
                            "GRID": "grid.7849.2"
                        }
                    }
                ]
            },
            {
                "given": "Jean-Baptiste",
                "family": "Camps",
                "affiliation": [
                    {
                        "original_name": "Ecole nationale des chartes, France",
                        "normalized_name": "École Nationale des Chartes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/013xvg556",
                            "GRID": "grid.462175.3"
                        }
                    }
                ]
            },
            {
                "given": "Thibault",
                "family": "Clérice",
                "affiliation": [
                    {
                        "original_name": "Ecole nationale des chartes, France",
                        "normalized_name": "École Nationale des Chartes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/013xvg556",
                            "GRID": "grid.462175.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "English",
            "philology",
            "medieval studies",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " From Bakhtin’s heteroglossia to Genette’s transextuality, theories of the novel describe its form as a mixture of diverse elements or styles. While many of these mixtures are contiguous with the text as a whole (a novel, for example, can operate in both the genres of history and the gothic), in this project, we are interested in overt stylistic changes within the text of a novel that mimic the discourse of non-novel genres like psychology, philosophy, or natural science—what we call here shifts in  microgenre. For example, in Edward Bellamy’s 1888 novel,  Looking Backward, the first-person narration of the time-travel story transforms key moments into third-person voices that explain to the narrator (and the reader) how Boston’s future society operates. In Stoker’s  Dracula, the same kind of shift takes place through the overt inclusion of multiple media forms, such as newspaper articles and excerpts from the  Demeter’s log. More subtle is the sermon embedded in the third chapter of Joyce’s  Portrait of the Artist, delivered in the voice of Father Arnell. In each of these cases, there is a strong shift in the style of the text towards the discourse of other disciplines (history, in the case of Bellamy, journalism in Stoker and theology in Joyce). In this project, we use computational methods to identify the stylistic differences between disciplinary writing that enables this phenomenon and to reveal when and how these microgeneric shifts take place within the framework of a novel.   Corpus and Methods While the phenomenon that we describe above is present throughout the history of the novel, and in many different novelistic traditions, we restrict our inquiry in this paper to the period between 1880 and 1914. Not only does this enable our investigation to control for the historical change in both disciplinary discourse and fiction, but the late nineteenth century is also a critical period in the history of modern disciplinarity. In both America (with the second Land Grant act for universities in 1890) and in the United Kingdom (with the rise of Civic Universities beginning in 1900), this period saw a massive growth in university education and, with it, in the number of disciplines concretized through their inclusion in the academy. For example, the subjects offered at Mason Science College (later, the University of Birmingham) in 1880 included only Math, Physics, Chemistry and Biology. By 1898, this had grown to 21 subjects, including Literature, Physiology, Philosophy and Engineering. By concentrating our research on this period, we are able not only to find evidence of disciplinary styles within the novels written at the time, but also to recover the stylistic differences that accompanied this growth and division of disciplinary writing.  To explore both areas, we hand-assembled a corpus of 97 texts written between 1880 and 1914. This corpus included 10 texts each from a range of disciplines, including Anthropology, Economics, Philosophy, Psychology, History, Politics and Natural Science. These texts were chosen by the project participants to be representative of the range of possible discursive styles of each discipline and to be of a length similar to the novels of the period. The corpus also includes 27 novels (again written between 1880 and 1914), which both served as an example of the narrative discursive style for our model, and also allowed for further classification of the various disciplinary styles as they are embedded in these texts.  The theory of microgeneric shifts that we propose here is similar to the multi-authorial problem addressed in stylometric work by rolling delta. Like rolling delta, our concern is also with the linear analysis of sequential phenomenon; however rather than intra-textual changes in authorship, our project seeks to identify stylistic shifts within a narrative written by a single author, but which reveal the presence of extra-disciplinary discourses. For this reason, our approach builds on the work of stylometry, but departs in two important ways. First, because we are interested in shifts in discourse rather than content, we have opted to take a non-lexical approach. Rather than a feature set built from a subset of token frequencies (like most frequent words), we use grammatical features in our model, including the frequency of the Penn Treebank POS tags, the average sentence length and the average number of clauses per sentence. In a lexical approach, a model built on the most frequent words, as in the calculation of delta, is too tied to authorial signals, while a model built on distinctive words would be too indicative of the subject matter, rather than the style, of the individual disciplines. Secondly rather than a cluster-based distance approach, we adopt a classification model that builds upon research into literary genres using computational methods. By training a model on 200-sentence subsections of our corpus of disciplinary texts and then classifying similarly sized passages of contemporaneous novels, we can use the posterior probabilities of the classification results to identify the mixture of disciplinary writing in each part of each novel.  Analysis The project seeks to answer two related questions. First, are there differences in the discourse of disciplines that are stylistic rather than semantic? And, second, if these differences in style can be identified, is it possible to detect evidence of these styles embedded within novelistic prose? To answer the first question, we created a classification model using a discriminate function analysis. Our choice of a method spoke to both our hypothesis that the classification of disciplinary style would be found in a linear combination of grammatical features, as well our subsequent use of the posterior probabilities calculated by the model (rather than hard classifications) as proxies for the disciplinary mixtures in each of the passages that we classified. The results of our initial classification (cross validated through a withheld test sample) reveal that our model was able to classify the texts from each discipline at a far greater percentage than chance (Figure 1). Even the most ambiguous discipline, economics, was classified correctly more than 50% of the time, far greater than chance (12.5%—equal for all fields, since we down-sampled during training). The misclassifications in each genre speak both to the complexities of the model and to the same microgeneric phenomena that occur in novels. In a book on natural science, for example, the straightforward discourse of history includes aspects of natural science in its attention to the details of historical events.      Figure 1: Discriminate Function Analysis of corpus using grammatical features  For the second question, we used the model we had developed to classify individual parts of novels. For example, in the resulting graph of Sir Arthur Conan Doyle’s  A Study in Scarlet (Figure 2), each bar represents a 200-sentence slice of the text, and the mixture of colors indicate the posterior probabilities of each disciplinary discourse in each slice as assigned by the model. Although the majority of slices are, as one might expect, dominated by novelistic discourse, the eighth slice is divided between anthropology and natural science. In fact, that portion of the novel is given over to a description of the geography of the southwest United States, followed by details about the appearance and customs of Mormon migrants—in other words, something like natural science and anthropology. This is an intuitive confirmation that disciplines have discursive styles (remember that the classifier has captured this mixture with no semantic information), and that novels can deploy them in strategic ways. Examples like this suggest not just the technical accuracy but also the literary critical use of microgenres, especially for more densely interdisciplinary works, like Bellamy’s  Looking Backward ( Figure 3).      Figure 2: Classification of 200 sentence slices of  A Study in Scarlet      Figure 3: Classification of 200 sentence slices of  Looking Backward  The results of our project reveal that the disciplinary discourses of the late nineteenth century did indeed have unique formal stylistic markers that extend beyond their subject matters or word choices. Moreover, by classifying parts of novels using a model built on these features we can trace how different disciplinary styles were embedded as microgenres within the narratives of nineteenth century prose writing. ",
        "article_title": " Microgenres A computational model of disciplinarity and the novel ",
        "authors": [
            {
                "given": "J.D.",
                "family": "Porter",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Fredner",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Michaela",
                "family": "Bronstein",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Alexander",
                "family": "Manshel",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Nichole",
                "family": "Nomura",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Abigail",
                "family": "Droge",
                "affiliation": [
                    {
                        "original_name": "University of California, Santa Barbara, United States of America",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "corpus and text analysis",
            "data mining / text mining",
            "English",
            "cultural analytics",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Libraries and other cultural heritage organisations (CHOs) and their staff have a central role in digital humanities, as collection builders, project and service managers, and experts on how to use their collections to the fullest by digital means. Given the importance many of these organisations attach to their role as supporters of research, a case can be made for the opportunity and need for libraries and their staff to position and assert themselves as partners in research. We will address the issues raised when talking about libraries as digital humanities research partners. This pre-conference will present opportunities and examples to showcase library-based digital humanities work that makes the case for the partner model. This will be approached from different angles; discussing why this model is mutually beneficial to both library and partner organisations, what policies and infrastructure are needed for the organisation to be able to become a sustainable partner, which steps are needed for staff to obtain relevant skills, and finally how to choose who to partner with and then build those relationships. Two keynote speakers will set the theme of the event. The rest of the programme will be filled through a call for participation for short papers, lightning talks, panels, and workshops. This full-day pre-conference will be held at the National Library of the Netherlands in the Hague. Transportation will be provided from Utrecht. Please see our website for up-to-date information, including a CFP:  https://adholibdh.github.io/dh2019-preconference/   Programme committee   Hege Hosoien, National Library of Norway Sarah Potvin, Texas A&M University Isabel Galina, Universidad Nacional Autónoma de México Silvia Gutiérrez, El Colegio de México Andreas Degkwitz, Humboldt University of Berlin Tamara Butigan, National Library of Serbia Dawn Childress, University of California, Los Angeles Steven Claeyssens, KB National Library of the Netherlands Marian Lefferts, Consortium of European Research Libraries Lotte Wilms, KB National Library of the Netherlands  Local organizers  Lotte Wilms, KB National Library of the Netherlands Martine Klaassen, KB National Library of the Netherlands Steven Claeyssens, KB National Library of the Netherlands Marian Lefferts, Consortium of European Research Libraries  Additional SIG conveners  Glen Worthey, Stanford University Angela Courtney, Indiana University Zoe Borovsky, University of California, Los Angeles  External links    Preconference website    ",
        "article_title": "ADHO Libraries and Digital Humanities SIG Pre-conference: Libraries as Research Partners in Digital Humanities",
        "authors": [
            {
                "given": "Lotte",
                "family": "Wilms",
                "affiliation": [
                    {
                        "original_name": "Koninklijke Bibliotheek, National Library of the Netherlands, Netherlands, The",
                        "normalized_name": "National Library of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/02w4jbg70",
                            "GRID": "grid.425631.7"
                        }
                    }
                ]
            },
            {
                "given": "Sarah",
                "family": "Potvin",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Isabel",
                "family": "Galina",
                "affiliation": [
                    {
                        "original_name": "Instituto de Investigaciones Bibliográficas at the Universidad Nacional Autónoma de México",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Silvia",
                "family": "Gutiérrez",
                "affiliation": [
                    {
                        "original_name": "Daniel Cosío Villegas Library - El Colegio de México",
                        "normalized_name": "College of Mexico",
                        "country": "Mexico",
                        "identifiers": {
                            "ror": "https://ror.org/01vp99c97",
                            "GRID": "grid.462201.3"
                        }
                    }
                ]
            },
            {
                "given": "Andreas",
                "family": "Degkwitz",
                "affiliation": [
                    {
                        "original_name": "Humboldt University of Berlin",
                        "normalized_name": "Humboldt University of Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01hcx6992",
                            "GRID": "grid.7468.d"
                        }
                    }
                ]
            },
            {
                "given": "Steven",
                "family": "Claeyssens",
                "affiliation": [
                    {
                        "original_name": "Koninklijke Bibliotheek, National Library of the Netherlands, Netherlands, The",
                        "normalized_name": "National Library of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/02w4jbg70",
                            "GRID": "grid.425631.7"
                        }
                    }
                ]
            },
            {
                "given": "Marian",
                "family": "Lefferts",
                "affiliation": [
                    {
                        "original_name": "Consortium of European Research Libraries",
                        "normalized_name": "Consortium of European Research Libraries",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01fwca479",
                            "GRID": "grid.470540.2"
                        }
                    }
                ]
            },
            {
                "given": "Hege",
                "family": "Hosoien",
                "affiliation": [
                    {
                        "original_name": "National Library of Norway",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-08",
        "keywords": [
            "libraries",
            "museums",
            "English",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper discusses the latest approaches to developing information systems for digital cultural heritage on a global scale, including the creation of catalogs and infrastructure for resource documentation. Digital cultural heritage resources are diverse in content, origin, purpose, scale, technology and user audience. They may take the form of digital collections of institutions in the GLAM sector, such as virtual museums [11], electronic archives and libraries [10], or national infrastructures and international aggregators, systems and services for visualization and analysis of sources, and for representing virtual reconstructions. Documentation systems are essential to facilitate advanced digital humanities research and to provide greater user access.  Whereas object-oriented documentation may be used effectively to describe individual digital cultural heritage objects (including digitized objects of written and textual heritage, objects of art, 3D-models from objects to architectural complexes, etc.) [5], information resources for digital cultural heritage require additional description of their creation process and the technologies used, details of content, and other features. The V-Must project (Virtual Museum Transnational Network) [12], contains a set of virtual museums and a catalog including 54 information resources of predominantly European museums containing digital collections of exhibits. V-Must has a meta-description system for organizing resources, which includes 8 description fields, including for content, duration, audience engagement, interaction technology, level of immersion, format, scope and sustainability. There are also lists of information resources organized thematically [13], by use (as in education [1], for example) or by type of information resource [3]. However, such lists are not catalogs in the full sense, as they do not allow sophisticated search or sorting functions.  It is important to note that national infrastructures for digital cultural heritage [for example, 7, 9] and international aggregators [2, 4] integrate digital collections of GLAM institutions at the object level, providing cross-collection search capabilities. However, they are not primarily intended for documenting the information resources (such as websites, web-projects, platforms and their features) of these institutions. Effective systems for documenting such resources require a different approach because their purpose is to organize data by categorizing a very wide range of types of information resources. In the Center for Digital Humanities at Perm State University, Russia, a documentation system has been developed – a project known as “Historically-oriented information systems” or the DigitalHistory.ru platform [6]. Initially, this information system was designed to solve a specific scientific problem related to the study of historical-oriented information systems [8]. The platform was then upgraded and the database of the system was expanded to include a much wider range of characteristics related to describing information resources for digital cultural heritage. The current system is designed to document digital historical and cultural heritage information resources of various types, for wider use in research and education. The resource meta-description structure includes 39 fields that represent 3 groups of data: 1) Data on the creators of the information resource (authors, organizations, cities and countries); 2) General information about the information resource (title of the resource, website, abstract of content, languages ​​of the interface and collected sources, year of creation of the resource, and sources of maintaining); 3) Content description metadata (subject, geography, period, discipline and subdiscipline, type of electronic resource, purpose, target audience, types of cultural heritage objects presented, presence of virtual tour, interactive ways of communicating with users, and existing users’ personal accounts).  The platform currently contains 1397 documented information resources, and it continues to be expanded, pre-moderated by a specialist. In the structure of the documentation system, there are several ways to present created catalog data: 1) by country; and 2) by the full list of all resources. The extended search form allows the user to select one or more desired characteristics for a list of results. The documentation system is freely available online and is widely used as a reference and information retrieval system. In addition, the system is actively used in the training of Bachelor and Master’s students in courses related to Digital Humanities. The Center for Digital Humanities has developed a Master’s degree program “Digital Sociocultural and Art Practices”, which includes the course “Virtual Museums, Archives and Libraries”, which includes practical tasks related to the search and analysis of information resources for digital cultural heritage.  In summary, documentation systems play an important role in the modern world of information infrastructure for digital cultural heritage. The diversity of information resources requires further study and classification, which is also necessary for more detailed documentation and cataloging of these resources. The method and solutions proposed expand possibilities for finding thematically similar information resources, and provide a global model to make such resources more accessible for research and education. ",
        "article_title": "Documentation of Digital Heritage Information Resources: Expanding Access for Research and Education",
        "authors": [
            {
                "given": "Nadezhda Georgievna",
                "family": "Povroznik",
                "affiliation": [
                    {
                        "original_name": "Perm State National Research University, Russian Federation",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-12",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "libraries",
            "sustainability and preservation",
            "museums",
            "theory and methodology)",
            "English",
            "GLAM: galleries",
            "archives",
            "cultural studies",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Silk played an important role in European history, mostly along the Western Silk Road’s network of production and market centres. Silk, however, has become a seriously endangered heritage. Although many European specialized museums are devoted to its preservation, they usually lack size and resources to establish networks or connections with other collections. The H2020 SILKNOW project (Silk heritage in the Knowledge Society: from punched card to Big Data, Deep Learning and visual/tangible simulations)   aims to produce an intelligent computational system in order to improve our understanding of European silk heritage. The SILKNOW platform will form a coherently integrated system to give access to a wide variety of data describing silk-related objects to researchers, museums curators or general public with a single interface.   This computational system is modelized and trained thanks to these datasets, mapped according to the SILKNOW ontology. In this paper, we will present how we have defined this data model, and how we have specified the entities to be represented by the ontology and the existing relationships between these entities. The design and implementation of the SILKNOW ontology representing the model is based on CIDOC-CRM.  SILKNOW has crawled datasets from websites or online databases of Cultural Heritage Institutions (CHIs) preserving silk-related artefacts - such as the Musée des Tissus de Lyon (via the Joconde   database), the Victoria and Albert Museum   or the Museos estatales del MEC. The SILKNOW crawler is made in Node.js and its source code is available at:   . We have then analyzed the structure of the records on silk-related items from these different institutions.   In order to give access to these various datasets via a unique point of entry, it is necessary to harmonize them by designing and implementing a unique and complete data model, well adapted to Cultural Heritage data describing textile-related artefacts and more precisely silk-related artefacts.This data model is based on the CIDOC Conceptual Reference Model (CIDOC-CRM) which provides definitions and a formal structure for describing the underlying semantic of the structure of documentation on Cultural Heritage. The CRM is an object-oriented model independent from any technical implementation framework. It defines a limited set of objects with which it is possible to describe complex realities. More precisely, the CRM is a core ontology – that is to say a formal representation of knowledge – with more specialist extensions (for instance the FRBRoo, an ontology designed to represent the underlying semantic of bibliographic information). It is an empirical ontology, elaborated from the analysis of the data produced by the cultural heritage experts. Moreover, the CRM data model is flexible and extensible. In other words, given that it is based on a double hierarchy of classes and properties, if needed, it is possible to add new subclasses and sub-properties, in order to express more specific relationships and properties, without modifying the basic structure of the model. There is yet no CRM extension for dealing with the production of textile artefacts; something similar to FRBRoo, for the creation, production and expression process in literature and the performing arts. Therefore, some of the free-text fields, especially the complementary fields on the technical and material information, are still in need of a more thorough reflection. The more complex modeling of the semantics included in data about the creative and productive process of silk textiles requires elaborating new classes and properties. SILKNOW takes these digital silk textiles’ data to analyze and and processed them with advanced text analytics. A text analytic module is currently designed and developed, in order to analyse the text content from the data collection. The ontology is used to structure the analysed information and map this information to populate the SILKNOW ontology. The text semantic meaning is based on English and will be translated from/to the other languages (French, Italian, and Spanish) in order to be processed. Thus, when text content is analysed, many different natural language processing techniques are applied to splitting sentences, tokenization, and entity extraction. The result of these methods is used to enrich the SILKNOW ontology by employing matching algorithms to determine the correct corresponding semantic concept of a named entity. The SILKNOW ontology will thus allow the project to elaborate new CRM classes and properties well adapted to describe silk textiles data. These results will be freely available, and will help to describe more precisely silk-related objects, and improve the way we analyze and understand these Cultural Heritage items. They can also be used as a basis to elaborate new CRM classes and properties for textiles’ data and not only silk textiles’ data. Moreover using the SILKNOW ontology will also allow to a wide variety of users to have access to an endangered heritage, and encourage new research on this heritage. ",
        "article_title": " Improving the understanding and preservation of European Silk Heritage. Producing accessible and reusable Cultural Heritage data with the SILKNOW ontology in CIDOC-CRM  ",
        "authors": [
            {
                "given": "Marie",
                "family": "Puren",
                "affiliation": [
                    {
                        "original_name": "LARHRA (CNRS), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Pierre",
                "family": "Vernus",
                "affiliation": [
                    {
                        "original_name": "LARHRA (CNRS), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "GLAM: galleries",
            "archives",
            "modeling",
            "simulation",
            "cultural artifacts digitisation - theory",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This presentation is the first report on the project “Hindi Lexicography and the Cosmopolitan Cultural Encounter between Europe and India around 1700” from Uppsala University (UU). The primary goal of the project is to produce an online dictionary (Latin-Hindustani-French) on the basis of the unpublished  Thesaurus Linguae Indianæ by François-Marie de Tours (de Tours, 1704). The shortcomings of the Uppsala project will guide the design of an extended cross-linked online dictionary of early modern Hindustani based on little known wordlists and vocabularies compiled by European merchants and missionaries in the 17th c. India. The novelty of the approach resides in combining multilingual sources describing a foreign language to create a ‘pan-European perspective’, which may offer new comparative insights for the historical linguistics of target languages. If successful, this approach can be applied to other early modern vocabularies containing unique descriptions of non-European languages.  Preparing a digital edition of de Tours’  Thesaurus faced many challenges from its inception. The tool chosen for this project was Fieldworks Language Explorer (FLEx) created by SIL (https://software.sil.org/fieldworks/). This decision was motivated by the ease of access to infrastructure and know-how, as the same software is used across many linguistic projects at UU. FLEx is a tool developed for field linguists allowing them to create a semantically categorized glossary that can later be elaborated into a dictionary. However, using it for a multilingual historical text had many disadvantages that had to be mitigated, with a different level of success.  The original text consists of four columns across two pages: Latin, French, and Hindustani in both Devanagari script and its Romanisation. The nature of FLEx required a decision which language to prioritise for headwords. This was not a straightforward choice, as it had lexicographic implications going further (ultimately, Latin was chosen). The most challenging part was the transcription of the quite unusual form of Devanagari script. The only readily available option–normalising it to modern Hindi–however practical, meant information loss and reduced historical linguistic value of the resulting dictionary. Similarly, the complicated diacritics invented by the author to render the foreign sounds were arbitrarily simplified. Since FLEx proved to be not particularly well suited for a historical dictionary, and especially, going forward with extending the project by including other early modern Hindustani wordlists and vocabularies (specified below), more sustainable and scalable solutions are required. The main innovation of the proposed project lies in the ambition to create an integrated historical dictionary of Hindustani, in which multilingual unpublished sources are edited and linked. The majority of existing historical lexicographic projects mimetically retro-digitise printed dictionaries (e.g. Cologne Digital Sanskrit Dictionaries, http://www.sanskrit-lexicon.uni-koeln.de/), not taking full advantage of the possibilities the new digital environment offers. By contrast, the information from the Hindustani manuscripts will be linked in all possible ways: between corresponding headwords in the respective works, but possibly also by external links to existing 19th-century dictionaries of Hindustani online (such as Digital Dictionaries of South Asia: http://dsal.uchicago.edu/dictionaries/). The user will, therefore, be able to see all the meanings of a Hindustani word from a ‘pan-European’ perspective. Not only this extended project will offer new functionalities, but it will also deal with methodological issues the Uppsala project had to put away. Nevertheless, the challenges are still manifold.  This project tackles a few issues at the same time: 1)    Includes data from unpublished manuscripts  2)   Multilingual sources (Latin and French, Dutch/Flemish and Portuguese, Persian)  3)    Three different scripts (Latin, Perso-Arabic, and Devanagari)  4)    Special characters  5) Various arrangements (onomasiological, semasiological, alphabetical, by the grammatical category) 6)    Cross-linking of entries between vocabularies  The primary task will be to prepare the transcription of the works. All languages will require normalisation of spelling variants next to the added modern form. The biggest challenge of these early modern texts is that the words in the target language were often written down from hearing, using the orthography of the writer’s native language and employing many diacritic innovations. And so, a Dutch author would write down the sound /u/ as <oe>,  use <g> for /x/, and <oo> for /o:/. At the same time, A French author would probably mark the sound /u/ as <ou>, use <g> for /ʒ/ or /ɡ/; an English author would use <oo> for /u/, but a Dutch person would read it as long /o:/. Understandably, it can be quite confusing for non-native speakers. To further complicate the matter, many values change over time. To deal with this issue, a common solution needs to be found for entities, which then will be converted to Unicode, thus creating a set of special characters for the project. In the second step, the normalised entries will be analysed by phoneticians of individual languages and ‘translated’ into IPA, revealing the metalanguage-independent form of a Hindustani word. This will offer a solid ground for comparing and contrasting the Hindustani sounds as recorded by French, Dutch and Portuguese speakers. Since Hindustani is the focus of the project, the main task will be the linking of the Hindustani glosses from all the dictionaries in one integrated database. To utilise the thematic arrangement of two works, establishing the ontology for the whole project will allow adding an additional layer of enrichment in the database with semantic categorisation. This will enable researchers to categorise, select and study certain types of vocabulary – a development not available in FLEx. If successful, this approach can be applied to other early modern vocabularies, which were often created by both trained and amateur philologists in different source languages, and which today are unique and valuable descriptions of Asian languages. ",
        "article_title": "A European-Hindustani Dictionary? Reflections on Methods",
        "authors": [
            {
                "given": "Anna",
                "family": "Pytlowany",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Ireland",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "oriental and asian studies",
            "multilingual / multicultural approaches",
            "scholarly editing",
            "linguistics",
            "lexicography",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Application of state-of-the-art digital methods in the fields of Digital Paleography and Manuscript Studies has long been a challenging task, even with the proliferation of techniques within the field of Document Image Analysis (DIA) (Kasturi et al., 2002). Several reasons can be attributed to this. From a methodological perspective, many of these techniques are black boxes (Hassner et al., 2015), whose results cannot be completely scrutinized and understood. From a development perspective, there is a distinct lack of accessibility to the various published and open-source services and methods. Furthermore, many require technically complicated configurations to execute them. Projects like DIVA (Würsch et al., 2016) attempt to overcome the latter by making them available as web services. However, one still requires programming experience to use the methods and create computational solutions for scholarly research questions.    Motivation Recently, Visual Language-based applications like AppInventor (Wolber, 2011) and Blockly (Trower and Gray, 2015) have gained a lot of attention. By using an intuitive visual syntax, instead of textual syntax, they let non-programmers to create computational solutions without the overhead of learning a traditional programming language (Narayanan and Hübscher, 1998). However, such Visual Language (VL) environments do not exist for Manuscript Studies. Although there are tools like VMR CRE (Mahony and Bodard, 2016) and DigiPal (Stokes et al., 2014) (along with its latest incarnation archetype.ink) that are interesting in their own setting, as yet, they do not provide a programming-like environment to implement DIA methods. DIVA comes very close to this idea, but mainly focuses on providing a seamless backend for other programs to take advantage of. However, very recently, Würsch et al. (2019) have come up with a new workflow design tool DIVA-DIP that involves users in the design and execution of workflows.  In this context, we introduce the Advanced Manuscript Analysis Portal (AMAP). The aim of AMAP is twofold. On the one hand, it offers a largely self-usable toolbox that humanists can use to build solutions themselves. On the other hand, it facilitates communication between experts from Computer Scientists and Humanists (Rajan and Stiehl, 2018b). Allowing users to jointly develop solutions minimizes the black box problem as they better understand their final system of interacting DIA modules. AMAP also strives to support reproducibility and data provenance. It further intends to encourage exploration of various relevant tools and algorithms.   AMAP Design and Implementation AMAP has been designed to be platform independent and, thereby, allows the utilization of mobile devices. The architecture is entirely based on web-based technologies to facilitate this. It was ideally designed to be used on large-touch based devices and encourages touch-based interaction and collaboration (Rajan and Stiehl (2018a)).    Figure 1: AMAP Workspace (Screenshot) AMAP in its current version consists of a central workspace that serves as the main canvas for interaction. Images (Digitized Manuscripts) are the central focus of this design space. They can be imported individually or in multiples. If the latter, they are imported as a virtual pile that can be stacked and unstacked as necessary. All other kinds of operations and methods that act upon images are visualized as virtual objects that can be attached or detached to a specific image or image pile. The images can also be subsetted and the derived subsets can be further processed independently. Any subsetting will maintain a visible connection to the main image that it is derived from and, hence, preserve provenance. Various DIA methods are available as objects called action chips, which get attached to the right-side of an image. They can either return a completely processed image or just image segmentations with specific Regions of Interest (ROI). If an action chip modifies the images itself, the source image is directly changed to reflect the new modification. This is to maintain the focus on the image itself. In case of action chips for image segmentation, the ROIs are shown as selection boxes that can be used to create image subsets. The parameters of DIA methods are visualized as knobs. Thus, abstracting the type and the range of the parameter space. Experimenting by various parameters is now simply a matter of selecting the value from a given range as provided by the knob. Filters that will not directly affect the image but only change the visual appearance temporarily are made available as plugs. These are attached to the bottom part of an image and can affect image characteristics such as transparency and brightness. The chips can also be connected to each other to form an experiment processing chain or a workflow. Any changes in parameters of the methods in the chain are instantly propagated to the next elements. The intermediate results for the source images are kept for viewing and inspection. If two chips are compatible in terms of chainability, they intuitively click together, whereas, incompatible objects either do not click or repel each other. In this way, users with little or no DIA experience are guided towards configuring mutually compatible chips. It is also possible to create loops for large-scale experimenting and, hence, find the optimal parameter range for a given problem setting.  All DIA operations performed in the workspace are logged in terms of timestamps, I/O and the various processes (with the associated parameters) that the source image has gone through. Logging is essential to perform well-documented scientific experiments and saves effort by avoiding experiment replication and improving on previous experiments. AMAP also provides various virtual tools that can be used for paleographic purposes. For instance, tools such as scales and protractors can be used to measure individual features of characters or images.  The workspace is live and gives immediate feedback by being always reflective of the current status of overall processing in a chain. The entire workspace can be saved, allowing work continuity. It also enables sharing experiments among the community and will foster greater transparency and reproducibility.  By providing an exploratory environment, scholars will have better access to the latest DIA techniques and will be empowered to create computational solutions themselves. It also provides them a hands-on experience to communicate with external developers. AMAP currently supports image processing, image segmentation, basic keyword spotting, keypoints visualization, OCR and writer identification.   Conclusion In our short paper, we have briefly introduced Advanced Manuscript Analysis Portal (AMAP) for programming with open source DIA methods. We initially outlined the need and motivations for developing AMAP and its potential applications, and finally, elaborated the design and implementation of AMAP. As of now, AMAP is a proof-of-concept tailored to joint experimentation/workflow design for use cases from pilot projects in SFB 950 (with a focus on word spotting and writing style analysis). A first larger-scale use case analysis along with user studies from a variety of domains is in the making to solicit user feedback and improve the interface design. Acknowledgement We gratefully acknowledge support by Sonderforschungsbereich (SFB) 950 - Manuscript Cultures in Asia, Africa and Europe - (Faculty of Humanities of Universität Hamburg) through Deutsche Forschungsgemeinschaft (DFG).  ",
        "article_title": "Advanced Manuscript Analysis Portal (AMAP): An Interactive Visual Language Environment for Manuscript Studies",
        "authors": [
            {
                "given": "Vinodh",
                "family": "Rajan",
                "affiliation": [
                    {
                        "original_name": "Research Group Image Processing, iXMan Lab, Department of Informatics, University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "H. Siegfried",
                "family": "Stiehl",
                "affiliation": [
                    {
                        "original_name": "Research Group Image Processing, iXMan Lab, Department of Informatics, University of Hamburg, Germany; SFB 950 Manuscript Cultures in Asia, Africa and Europe, University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "software design and development",
            "user experience design",
            "interface",
            "gamification",
            "mobile applications and mobile design",
            "English",
            "computer science and informatics",
            "epigraphy and paleography",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  A common practice in spatial humanities is georeferencing historical maps to generate rasters for use in Geographic Information Systems (GIS) (Clifford et al., 2013). This method is typically carried out with different levels of precision and with different goals in mind; for example, spatial humanities practitioners create base maps from historical maps, they extract vector data or they compare spatial representations over time. If we consider both the historical map and the base map as imperfect representations--rather than reflections--of reality, the georeferencing process creates a relation which needs to be critically analyzed (Presner, et al., 2014). This paper aims to expand the modes of visualization available to scholars of cartography interested in exploratory spatial analysis of medieval maps in a way that the complex spatiality of the historical map is not overtaken by that of normative base maps. In the case of historical maps whose coordinates and scales are roughly comparable to modern maps, valuable historical information can be extracted from the georeferencing process (Baiocchi and Lelo, 2005). Furthermore, algorithmic analysis, such as differential distortion, can help identify geometric inaccuracies of so-called “old” maps (Claeys Boùùaert et al, 2016). Visualizing such divergence allows one to analyze historical cartographic technique. In this paper we turn instead to  very old maps--examples of so-called “complex” medieval maps that blend conventional T-O structure with pseudo-geographic detail. Historical cartographic studies have emphasized the encyclopedic quality of such maps, their textual sources and their blend of geography and sacred, cosmological detail (Paul, 2018; Edson, 1997). Rather than a modern coordinate system, we find in some of them notions of spatial orientation suggested by the abundance of detail and named entities on the surface of the map. Instead of consistently stretching the map, georeferencing can lead to problems of occultation, that is, to hidden “folds” in the distorted map. Another way of expressing this is that the distances or orientations of places or features on one map representation may not be consistent with another map. Furthermore, a map predating modern coordinate systems may contain non-geographic information. Stretching such a pre-modern map to a contemporary map projection, therefore, might seem counterintuitive. We argue that visualization of purposeful distortion can allow us to understand better the pre-modern organizational structure of maps. In our research we have found that this practice allows us to situate such pre-modern maps on a spectrum between the more topological and the more symbolic.    Map Projections and Georeferencing The method of drawing a map of the almost spherical earth on a planar surface is called a map projection, a process only possible with distortion of some important geographic properties. One such projection known as the Mercator (Chamberlin et al, 1950), distorts areal scaling inconsistently, so that the areal scaling factor increases as one moves north. In the process of georeferencing, control points on the historic map are manually referenced on the modern map. For each control point in the coordinate system of a scanned historic map image, there is a control point in the coordinate system of the modern map image, which represents the same entity. Several algorithms exist to use this information to project the whole historic image onto the modern coordinate system. In this paper we use the Thin Plate Spline (TPS) method (Duchon, 1977), which maps the control points accurately and interpolates the locations in between, such that the bending energy of an imaginary surface is minimized.   Existing Visualization Techniques for Showing Distortion The computation and visualization of modern map projection distortion of a globe-to-map setting has been extensively studied. A common approach is to show ellipses on the map, indicating the distortion of infinitesimal circles on the globe. This method is named after French mathematician Tissot (Tissot, 1881) and has been extended to visualize flexion and skewness simultaneously (Goldberg et al., 2007). Further visualization techniques for the globe-to-map scenario have been elaborated (Tobler, 1966; Mulcahy et al, 2001), including the use of color maps and contour lines. Methods to compute and visualize distortion in a map-to-map scenario also exist (Jenny et al, 2007; Claeys Boùùaert et al, 2016). The latter study presents a methodology to visualize areal and angular distortion using differential analysis for every point of the modern map as projected onto the historic map or vice versa, given an arbitrary, differentiable projection function.   Visualizing Areal Distortion in  Mappae Mundi  It goes without saying that because medieval maps do not follow the projections used in modern cartography, by georeferencing them as described in section 2 we do not expect a seamless, intuitive result. Instead, when projecting  mappae mundi onto modern maps or vice versa, topological inconsistencies are likely to occur. As shown by the control points in Figure 1a, an imaginary place A might be left to a place B on the historic map, while their order is reversed on the modern map. Further, the places C and D remain their order. To resolve this, the TPS method creates a “folded” projection of the historic map on the modern map as shown in Figure 1b.      Topological inconsistent projection. In order to show the pre-modern organizational structure of historical maps, possibly demonstrated by those regions of topological inconsistency, we visualize how the historic map will be scaled and where it will be folded by the TPS method. We base our calculations on an established areal scaling factor formula (Claeys Boùùaert et al., 2016) shown in Eq(1) in Figure 2 which can be simplified to Eq(2). This factor can also be derived by calculating the area A of the parallelogram the partial derivatives span using the shoelace formula (Meister, 1769), shown in Eq(3). The term Â will be negative only if the points of the parallelogram are found in a clockwise direction; in which case the TPS projection is flipped. As such, this property helps us to identify folded regions. The resulting visualization for the historic map of the example outlined above is shown in Figure 1c.  Our visualization design can be read like a traditional contour map showing a mountain area: the higher the mountain, the larger the scaling factor of the image. Each contour line in the visualization represents a region where the scaling is identical. The wider such contour line is drawn, the smaller the gradient is, i.e., the smaller the change of scaling is at that location. To remain independent from units, we normalize that factor between the minimal and maximal occuring scales. Black contour lines represent mean scaling. The more saturated the lines are, the larger the magnification or shrinkage is with respect to the mean. The color blue indicates a factor smaller, respectively the color red a factor greater than the mean. Turquoise indicates shrinkage, and yellow, magnification in a folded region, that is, where the orientation of features of the historic map is flipped. A white line represents the crease of the fold. In addition to the contour lines, we identify the regions which will overlap in the projected image and draw those darker, resulting in “shadows” around folded areas. The result of applying the projection on the visualization itself can be seen in Figure 1d.    Areal scaling factor calculation.     Discussion At this stage of our research, we have used three medieval  mappae mundi: the Ebstorf map made in Northern Germany around 1235, the Cotton Anglo-Saxon map created at Canterbury between 1025 and 1050 and the Hanss Rüst woodcut map made in Augsburg around 1480. The abovementioned distortion techniques for medieval maps provide us with important insight into the particular blend of pseudo-geographic and non-geographic features found in each document. Results of this visualization are shown in Figure 3.  The Ebstorf map is the most detailed of our three examples. The key distortion, located around the map center, is due to the seal with Christ indicating Jerusalem, a symbolic, rather than geographic, placement. Other distortion is visible around Sicily, Crete, Cologne and cities of the Alexander legends. In the Cotton Anglo-Saxon map, no folds were produced given the chosen control points. On the other hand, we observed significant scalar expansion relative to the mean in the regions of Barbary and North Africa as well as the Western Mediterranean and the Black seas. Shrinkage of scale was observed particularly in two directions: along the axis Jerusalem-Babilonia-India as well as over Scotland and the Hebrides. The Hanss Rüst map exhibited the least geographic realism of all three.    Visualization results. From top to bottom: The historic map image, the historic map including our visualization and the historic map projected onto a mercator map. The folds created by map distortion, visualized in our design, provide strong visual stimuli for understanding the continuum of pseudo-geographic features of old maps, that is, for the critical evaluation of the spatiality of such maps on their own terms.  Whereas for the TPS method used in this paper we chose control points based on written geographic information (names of provinces, cities, images of walled cities), other choices for control points could lead to contradictory results. In future work, we intend not only to expand the set of maps that we can annotate, but also expand and develop our visualization methods. Finally, we intend to alter our mode of annotation, by choosing, as in classic modes of georeferencing, features on the map, such as peninsulas, mountains or rivers, instead of text or in combination with text to assess what different results we might obtain.  ",
        "article_title": " Using Visualization to Understand the Complex Spatiality of Mappae Mundi  ",
        "authors": [
            {
                "given": "Martin",
                "family": "Reckziegel",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "David Joseph",
                "family": "Wrisley",
                "affiliation": [
                    {
                        "original_name": "New York University Abu Dhabi, Abu Dhabi, United Arab Emirates",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Taylor Wright",
                "family": "Hixson",
                "affiliation": [
                    {
                        "original_name": "New York University Abu Dhabi, Abu Dhabi, United Arab Emirates",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Stefan",
                "family": "Jänicke",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "medieval studies",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The proposed paper will scope the complexity of born-digital archives from a digital forensic, historical and philological perspective. Personal digital archives, institutional repositories, web archives, email archives and social media archives create(d) digital primary records that the historical humanities struggle to fully recognize as documents in their own right (Winters 2018, 2019, Kirschenbaum 2016b, Baker 2018). The historicity of the forensic materiality and structure of the born-digital record is a concept still to be methodologically and theoretically understood in the humanities and in archival science (Ries 2017, 2018). Digital forensic features of primary records shape digital evidence by virtue of historically specific undocumented or unintended software and operating system behaviour, hard- or software bugs, physical damage, system crashes, malware or deliberate manipulation that leave born-digital traces or historically specific lacunae in the primary record that call for a historical understanding of distributed digital forensic materiality (Ries 2018, Ries, Palko 2019, Drucker 2013, Blanchette 2011, Kirschenbaum 2008).  Selected examples from forensic investigations into Friedrich Kittler‘s (Deutsches Literaturarchiv Marbach am Neckar), Michael Speier‘s (private archive), Marcel Beyer‘s (private archive), Hanif Kureishi‘s (British Library), Craig Taylor‘s (British Library, private archive) and Glyn Moody‘s (private, Science Museum) personal digital archives, born-digital records in the Mass Observation Archive (The Keep, University of Sussex) and digital art and web archives – which are all part of the ongoing research project based on Ries 2010, 2017, 2018 - will serve as examples of irreducible forensic complexity of born-digital archives that needs to be preserved in appropriate forensic formats, not only to ensure authenticity and chain of custody, but also to preserve historically specific computing artefacts and traces. These include recoverable drafts of writing projects, file and file system structure artefacts, error correction reports, recoverable temporary data, i/o-driver data and metdata, operating system traces, revealing fragmentation and lacunae that trace the history of digital events, based on historically specific forensic features and mechanisms. Born-digital forensic methods and tools themselves are subject to digital history, with their bugs and limitations that have to be methodologically and historically reflected. The purpose of this paper is to argue that forensic materiality and analysis is methodologically relevant for critical appraisal and understanding of production processes of born-digital sources in the humanities as a whole, including history, social history, political and culture studies (including literature, art history etc).  ",
        "article_title": " Born-Digital Archives A Digital Forensic Perspective on the Historicity of Born-digital Primary Records  ",
        "authors": [
            {
                "given": "Thorsten",
                "family": "Ries",
                "affiliation": [
                    {
                        "original_name": "University of Sussex, United Kingdom",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital humanities (history",
            "libraries",
            "sustainability and preservation",
            "museums",
            "theory and methodology)",
            "English",
            "media archaeology",
            "GLAM: galleries",
            "archives",
            "digital archives and digital libraries",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "     This long paper reports the development of 1) a new theoretical approach, with accompanying methodology, to the representation of place in historical digital humanities projects arising out of 2) a new digital resource that will be used in fall 2019 in secondary schools in Nova Scotia, Canada to teach restorative justice practices as a means of addressing the impacts of systemic historical racisms that are still active in participating school communities.    There is a robust tradition of virtual reconstructions of historical places in the digital humanities, with a strong thread of applications in theatre and performance history (Roberts-Smith 2017; a survey in chronological order might include Friedlander 1991; KVL 2002; Best 2002; Saltz 2004; Tompkins and Delbridge 2009; Roberts-Smith, DeSouza-Coelho and Stoesser 2016; Wall 2016; and note Franklin J. Hildy’s career-long leadership in the area of historical theatre architecture and the digital humanities). Digital theatre history projects, like cognate projects in other domains (such as architecture, archaeology, library science, even gaming), have tended to approach the virtual reconstruction of historical place as a process of synthesizing and visualizing surviving documentary and archaeological evidence of material places as accurately as possible. Normally, we have better evidence of the venue in which a past performance took place than we do of the performance that took place there, so DH researchers working in this area have consistently been careful to make explicit the relative stability of the architectural models their projects have produced, in comparison to their much more hypothetical virtual reconstructions of performance. While this is a rigorous and productive approach to historical performance research in the digital humanities, its emphasis on documentary and material evidence is limiting in at least two ways: first, it implicitly privileges place over event, which can result in an exaggerated emphasis on the semiotic impact of the venue for a past performance or of the location for a past historical event (a similar imbalance occurs in non-digital research including influential studies such as Fitzpatrick 2011); and second, it cannot accommodate some of the most important threads of longstanding theatre-historiographical theory, namely those arising out of emphases on discourse (Postlewait 1991), memory (Phelan 1993), and repertoire (Taylor 2003) as repositories of performance history (as well as of the histories of events and cultural practices).   These limitations have been especially problematic in the  Digital Oral Histories for Reconciliation  project (DOHR), which is charged with the responsibility of creating a virtual reality experience, based on oral histories, to teach 17-year-old high school students about the harms suffered by former residents of the Nova Scotia Home for Coloured Children (NSHCC; see DOHR). The DOHR VR experience, titled  The Home , is the centrepiece of a two-week curricular unit that fulfils the educational mandate of the Restorative Inquiry currently underway in the province of Nova Scotia, Canada (NSHCC Restorative Inquiry). The Inquiry, building on the work of the Victims of Institutional Childhood Exploitation Society (VOICES), an association of former residents of the NSHCC, has collected the oral histories of more than three hundred former residents, which bring to light harmful events and experiences that were systematically excluded from the public documentary record. As a result, although the oral histories that we are rendering in the VR environment of the  The Home  are set in the historical NSHCC building, it is essential that our virtual reconstruction of the fabric of the building  not  be represented as more stable than accounts of the events that took place there: after decades of protest against a false and harmful documentary record, the voices of the project’s first person storytellers must not be overshadowed by it.    In response to this challenge, the DOHR project’s virtual reality development team (made up of theatre and DH scholars, theatre artists, games studies scholars, and game industry professionals) has drawn on discourse-, memory-, and repertoire-based theatre-historiographical theory to attempt to render the historical NSHCC as a place that is performed by (brought into being, and hence ontologically subsequent and secondary to) the speech acts (Austin 1966; 1975) of oral historians, rather than occupied by them (pre-existing and hence ontologically prior to their voices). The concrete implications of this approach include some unusual engagements with the conventions of VR, especially with regard to character representation (we emphasize multi-modal  voice  over analogues for embodiment); configuration of the VR participant’s role (resisting the conventions of  immersive  design practices in both VR and theatre); and affectivity (privileging affective dissonance [Zembylas 2016]) over the widely-celebrated VR simulation-induced  empathy ). In combination, these practices argue (we hope) a definition of  past place  that supersedes reconstruction and construes it instead as a complex form of  presence  (Slater and Wilbur 1997; Sas and O’Hare 2003)- a sense of  being there  that in our VR experience is generated by a participant’s growing ability to reflect on their own perspective  in relation to  (i.e. importantly distinct from, although hopefully increasingly sympathetic to and willing to ally with) the perspectives of the storytellers whose histories they are witnessing. The goal of this sense of “relationality” (Llewellyn 2011) is to equip Nova Scotian young people with tools to help them identify - and begin to mobilize their communities to work together to address - the systemic causes of racism that are still actively causing harm in Nova Scotia and elsewhere in Canada.     The Home  will be tested systematically with youth during our in-school pilot study in fall 2019. Team-internal assessment has been an integral part of the iterative, participatory design process (Björgvinsson, Ehn, and Per-Anders Hillgren 2010) undertaken by our community-based research partnership, which includes former residents of the NSHCC, the NSHCC Restorative Inquiry, VOICES, Nova Scotian education administrators, teachers, and school resource workers, historians, and legal experts, as well as theatre artists and games studies scholars (dohr.ca). This paper will be illustrated by screen capture video of excerpts from the Beta build of  The Home ; the VR experience will also be available.   ",
        "article_title": "Performing Historical Place: Leveraging Theatre Historiography to Generate Presence in Virtual Reality Design for Restorative Justice",
        "authors": [
            {
                "given": "Jennifer",
                "family": "Roberts-Smith",
                "affiliation": [
                    {
                        "original_name": "University of Waterloo, Canada",
                        "normalized_name": "University of Waterloo",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01aff2v68",
                            "GRID": "grid.46078.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-07",
        "keywords": [
            "digital humanities (history",
            "digital art",
            "theory and methodology)",
            "English",
            "film and performing arts studies",
            "public and oral history",
            "virtual and augmented reality",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Created in 2012, the Mémoires des Archéologues et des Sites Archéologiques (MASA) Consortium has been certified by the Very Large Research Infrastructure Huma-Num. MASA was born from the experience acquired by and within several Maisons des Sciences de l'Homme in the field of processing the documentation produced by archaeologists. MASA's partners have pooled their skills to meet the needs of the archaeological community. The issues identified are multiple and involve several levels of complexity intertwined. The first level of complexity is related to the nature of the discipline itself: by digging, the archaeologist irreparably destroys his own object of study, even with a rigorous protocol for recording data during their production, the experiment is not reproducible. This places a particular responsibility on the archaeologist and gives his records the status of primary data. The return to this data is often necessary for comparative purposes and reinterpretation. The second issue is related to the nature of the documentation and its supports, which are very diversified and sometimes very fragile (field notebooks, surveys, plans, plans, photographs, pencil surveys on layers, etc.) and whose digitization should facilitate consultation and storage. The excavation archives are made up of these various recordings and the artifacts collected, which represent a considerable mass of material elements. As the two are inseparable, information systems must ensure that the link between them is maintained. The third challenge stems from the habit of archaeologists to work with heterogeneous databases, often designed without a methodological and technical choice protocol. Ex-post work is therefore necessary to make these databases standardised and interoperable (RDF, SKOS, DC), to use common repositories that make it possible to consider linking these data on the web of data, to document them with new metadata where appropriate, to enrich them and to facilitate access while ensuring their sustainability. To meet these objectives, the MASA consortium proposes to the archaeological community a process of data manipulation from acquisition to publication according to a systemic approach that respects FAIR principles. The MASA ecosystem is composed of bricks for archiving and sharing archaeological data sets. Once processed, documented and standardized, the archaeological data sets are put online according to the standards in force (XML, TEI, EAD...). Standardised repositories are used for spatial (GeoNames), temporal (PeriodO) and descriptive (PACTOLS via the OpenTheso thesaurus manager) information. The OpenArchaeo platform ensures their interoperability in a MASA triplestore and allows their interrogation via a simplified HMI that translates requests into SPARQL according to a generic model for mapping archaeological data with the CIDOC CRM ontology. The data are accessible from the publications that mobilize them. The LogicistWriter logicist writing tool offers the matching of inferences with the CIDOC CRMinf extension on reasoning. Each step of the process is documented by a good practice guide in the OpenGuide platform developed for this purpose. The whole process will be illustrated by the example of the excavation in Rigny (France), which follows all these recommendations from the field recording put online to the logicist publication of the results (Marlet et al., 2019). With this digital ecosystem, the MASA consortium relies on the data culture of archaeologists and their long experience in computerization to bring the community to respect the FAIR principles and to open these corpus in the Linked Open Data. ",
        "article_title": "Digital Ecosystem For The French Archaeological Community",
        "authors": [
            {
                "given": "Xavier",
                "family": "Rodier",
                "affiliation": [
                    {
                        "original_name": "CITERES-LAT CNRS/Université de Tours, France; consortium MASA (TGIR Huma-Num)",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Olivier",
                "family": "Marlet",
                "affiliation": [
                    {
                        "original_name": "CITERES-LAT CNRS/Université de Tours, France; consortium MASA (TGIR Huma-Num)",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "archaeology",
            "theory and methodology)",
            "standards and interoperability",
            "English",
            "semantic web and linked data",
            "open/libre networks and software"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The recent appearance of Steven Pinker’s  Enlightenment Now is a topical reminder of the enduring importance of 18th-century legacies in contemporary thought, culture, and politics. Considered pernicious or positive, French intellectuals began assessing the legacy of  les Lumières almost from the outset of the events of 1789 and continued this polemic throughout the 19th century. While the study of pro- and anti-Enlightenment or Revolution writers active during the 19th century is certainly of great value, our work in this project aims to examine the complexities of Enlightenment legacies using new “distant reading” approaches (Underwood, 2017). To this end, and in conjunction with the Observatoire de la Vie Littéraire (OBVIL) at Sorbonne Université, we have developed a new generation of sequence alignment software that detects reused passages in very large corpora; we use this software to compare several important collections of 18th-century texts to the Très Grande Bibliothèque (TGB) corpus of 19th-century printed materials made available by the Bibliothèque Nationale de France (BNF). Following an overview of the datasets and software developed for this effort, we will sketch some preliminary results arising from this project and conclude with an outline of further work we will carry out based on this database.   We used three different text collections for this project. The 18th-century sample is drawn from the holdings of the ARTFL Project and includes the  Encyclopédie of Diderot and d’Alembert as well as 1,367 17th-18th century texts from the ARTFL Frantext database.   See http://encyclopedie.uchicago.edu/ and http://artfl-project.uchicago.edu/content/artfl-frantext/.  Both are well-curated collections and provide solid samples of Enlightenment discourse. For the 19th century, we were able to employ selections of the TGB collection released by the BNF in conjunction with OBVIL. The collection consists of 128,441 documents by more than 58,000 authors almost all of which have metadata drawn from the BNF catalogue. The vast majority of the collection was published during the 19th century, though this includes a significant number of reprints of older texts. The collection contains a broad selection of themes and subjects with 35,710 documents listed as  Littérature, 28,885 as  Histoire de la France and 23,776  Droit. As expected, the quality of the raw data – based on uncorrected Optical Character Recognition – varies widely depending on a range of factors, including age, preservation status and print quality. In order to identify those texts that were originally published before 1800, we used a series of heuristics based on the metadata provided by the BNF to eliminate duplicates and near-duplicate texts. This left 112,907 documents in our working TGB sample.  While the ARTFL Project has developed text alignment packages in the past (Horton et al., 2010; Roe, 2012), this system is less-suited for very large-scale comparisons, e.g., those in 100,000+ document range. Detecting identical or similar passages requires a one-to-one document comparison of every text in the dataset (Gladstone and Cooney, 2020). Given the scale of the TGB dataset, we developed the TextPAIR system to address limitations of the previous model using new technologies. Installed as a Python package, it includes a text preprocessing component written in Python, a sequence aligner written in Go to maximize speed and scalability, and a single-page web application written with the VueJS framework to guarantee maximum interactivity when text alignments are deployed in the browser. The package is available as open-source code on Github, with accompanying documentation meant to assist other research groups in installing and running their own text-reuse experiments.   See https://github.com/ARTFL-Project/text-pair.   The sequence alignment of the pre 19th-century sample of Frantext and the  Encyclopédie against the 112,000 documents of the TGB produced a large number of resulting  passage pairs, our basic unit of analysis. Figure One shows a typical alignment pair, in this case a passage from the famous  Discours Préliminare reused with some indication of the source in Peignot’s 1801  Dictionnaire raisonné de bibliologie. It is important to note that TextPAIR can detect similar passages with considerable variations which can arise from textual insertions, deletions or modifications along with data capture errors, differences in spellings and word order changes. Figure 1 uses the “show differences” feature to highlight the variations between the passage pair.      Figure 1  Each record of the result database stores metadata for each document of the pair from the TEI headers, byte locations and offsets in the corresponding text data files, the passages in question, the size of the alignments, and whether or not the alignment is considered “banal” or uninteresting. The databases are loaded into a PostgreSQL relational database with a dedicated interface to allow users to query the document pairs, get summary results and navigate to the original documents at will. Figure 2 shows the query form of the  Encyclopédie to TGB alignment database, which supports metadata queries to allow the user to focus on specific questions, in this case a search for all aligned passages from articles written by Rousseau.      Figure 2. Searching for similar passages from articles by Rousseau in the Encyclopédie  The query returns 611 passages, as shown in Figure 3, where the first reused passage in this query is his article “Accolade”, which is found almost verbatim in a dictionary of music from 1825. The query interface makes extensive use of facets, allowing the user to consult frequencies broken down by different criteria. Looking at the reuses of Rousseau’s contributions to the  Encyclopédie, it is interesting to note that while most of Rousseau’s entries in the  Encyclopédie were about music, it is his political philosophy article “ECONOMIE” that is most reused in the 19th century.     Figure 3 The interface also supports the generation of time series graphs of the results. Figure 4 shows that reuses of the article “ECONOMIE” was fairly consistent through the 19th century.     Figure 4 The Baron d’Holbach presents another interesting case. As one of the  philosophes with the most notorious reputation as a free-thinking materialist he contributed some of the most controversial articles to the  Encyclopédie, such as “Représentants” and “Prêtres”. As shown in Figure 5, it was however his work on chemistry, mineralogy, and German history that is most reused in the 19th century. Instead of his scandalous article on “Prêtres” being cited, as one would expect, you find resonances of the rather orthodox article “EVÊQUE” which outlines the historical background of elector Bishops under the Holy Roman Empire. In fact, not one reuse of d’Holbach’s controversial material was found in the TGB, which sheds new light on our vision of d’Holbach as not simply an atheist propagandist, but as a man of science whose articles in various domains continued to be cited and used well into the 19th century. This is an image of d’Holbach that rarely, if ever, occurs in modern intellectual and literary histories.     Figure 5 The 19th-century reuses of passages from the 17 texts by Rousseau found in ARTFL Frantext, show a similar combination of expected and unexpected avenues of influence. It is not particularly surprising to find the nearly 1,500 instances of passages from his  Contrat social in works dealing with political theory even if they are used in a negative fashion. As shown in Figure 6, the most frequent reuse is in Pierre Landes’ attack on the  philosophe in his  Principes du droit politique, mis en opposition avec ceux de J.-J. Rousseau sur le contrat social followed by numerous expositions of Rousseau’s political thought.     Figure 6 By contrast, as shown in Figure 7, the over 10,000 reuses of Rousseau’s work more generally seem to focus on his reputation as a prose stylist, with the most frequent reuses found in various dictionaries and grammatical works. It is important to note the various vectors through which particular texts or authors can exert influence, even if it is indirect.    Figure 7  We believe that we can begin to use these techniques and these sorts of large-scale databases to refashion literary history, to give a more expansive vision of literary culture by identifying various forms of intertextual activity, from reuse to referencing, in a broadened set of 18th-century corpora and to eventually make use of various visualisation tools to navigate the output. While our interpretive work on this set of reuses is still in its initial phases, we have already been able to identify significant findings that challenge our understanding of the impact of the  Lumières in the 19th century.   Our full paper will expand on our observations above and begin the systematic exposition of the various complexities of identifying text reuse at such an unprecedented scale. We are aware, however, that these larger questions are well beyond the ability of any small group of researchers to explore, and thus invite interested parties to consult the alignment databases themselves.   See http://artfl-project.uchicago.edu/legacy_eighteenth.   ",
        "article_title": "Enlightenment Legacies: Sequence Alignment and Text-Reuse at Scale",
        "authors": [
            {
                "given": "Glenn H",
                "family": "Roe",
                "affiliation": [
                    {
                        "original_name": "Sorbonne University, France",
                        "normalized_name": "Sorbonne University",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/02en5vm52",
                            "GRID": "grid.462844.8"
                        }
                    }
                ]
            },
            {
                "given": "Clovis",
                "family": "Gladstone",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, USA",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Olsen",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, USA",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Morrissey",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, USA",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-03",
        "keywords": [
            "corpus and text analysis",
            "linking and annotation",
            "french studies",
            "data mining / text mining",
            "English",
            "cultural analytics",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " PROJECT ELITE-LOEP The National Library of Spain ( BNE) and the research group on Spanish Silver Age Literature ( LOEP) at the Complutense University of Madrid present \"La Edad de Plata interactiva\" (\" The interactive Spanish Silver Age”), a collaborative project to enrich the Library's digital collections and their use in teaching and research, exploring the resources that new digital technologies offer to the edition of texts in the field of cultural dissemination. The results of this research are inserted within the framework of eLITE-CM’s Project ( H2015/HUM-3426).  THEORETICAL FRAMEWORK This Project started with the question of whether today's philology, supported by new digital technologies, could propose rereadings of certain authors and works that, for reasons which are not often strictly literary (such as gender, ideology, aesthetic rarity...), have fallen into oblivion or are not currently occupying the space that would correspond to them within literary historiography if the quality of their writings is taking into account. After the official canon of the Silver Age there is, indeed, another dimension of Spanish literature that must be discovered or rediscovered (Romero, 2014: 16).  In the context of this debate, lines of research have emerged precisely around the concept of rereading to propose innovative interpretations of the literary past. Among them, we want to highlight those referring to the critical review wich has been developed by Digital Humanities. The application of new technologies to humanistic analysis has undoubtedly revolutionized the way in which we read, interpret and interact with literary works. Over the last years, the concept of rereading has served to designate a process of critical textual revision that could eventually influence the formation of a new extended canon. Nevertheless, nowadays it serves to provide today's readers and researchers a brand new form of access to authors and texts which were no longer read and now have been recovered. Complutense’s research group  La Otra Edad de Plata (LOEP) has witnessed all these methodological changes that are taking place in the humanities sphere as a result of the technological revolution. Furthermore, the aforementioned group intends to participate in the digital rereading of the Silver Age through the project eLITE-CM, with the aim of constructing textualities that can satisfy the digital natives. In addition, it allows to recover the voices of forgotten authors of this literary period. With this project we want to demonstrate our commitment to Research, Development and Innovation (R + D + i), including the Information and Communication Technologies for Development as one of the most significant challenges of our working group.   OBJECTIVE  In response to these new reading habits that the Digital Revolution has brought to the current panorama, the LOEP group of research has had as objective within the project eLITE-CM the development of three collections of forgotten texts of the Silver Age, rescued through digitization and enriched by computer programs, with the intention of exploring new resources that hypertext offers to philologycal studies in the Digital Age (Sanz and Romero, 2007).  To achieve our goals, it has been taken into account that dematerialization of cultural heritage linked to digitalization leads, necessarily, to new representations of cultural objects (Vinck, 2018: 73), which now overcome their physical limitations to become universally accessible through the Internet. This transformation, within the scope of text editing, implies a reconsideration of the book as a knowledge disseminator (Lucía Megías, 2012: 18) that now assumes characteristics such as transmediality or interactivity, which should be reinterpreted from an hermeneutic and phenomenological point of view, as part of a new perspective on the Reception theory linked to the field of Digital Humanities. Indeed, in terms of cultural dissemination, the enrichment of texts with images, sounds, concept maps, geolocators, hyperlinks, thematic transversality with other arts, etc. modifies the traditional concept of personal reading (understood as a unidirectional, silent and intimate situation), which now becomes interactive. If Bajtin (1975) considered that the literary work already constitutes a polyphony of voices, now the text enters into dialogue with the voices of the world that speak to us from the network. With this regard, it has also been an objective of the project to expose and evaluate the research results of our interactive collections. Therefore, this new dialogue established between the readers and the literary work has been analyzed as well. For this, data has been collected, not only from the users of the National Library of Spain’s website, but also from a wide number of students that have used our editions within a context of meaningful teaching in the classroom. As a result of this practical experience, some theoretical reflections will be offered. The main one is that the benefits that digital reading brings to the user on a cognitive level follow the five principles of what can be called a  smart reading: Simplicity, Motivation, Accessibility, Recycling and Transference to the global community. In this sense the  smart reading entails an innovative way of learning within a close-knit community that reaches high levels of thinking and emocional engagement to access knowledge, according to Edward D. Hess and Katherine Ludwig (2017).    OUR COLLECTIONS   Taking this into consideration, \" La Edad de Plata interactiva\" has carried out three collections of interactive books from the Silver Age linked to different topics, which will allow readers to access knowledge about this period through points of view not so widely considered by canonical historiography: 1) The Modern Woman in the Silver Age Literature; 2) Children's Literature in the Silver Age; 3) Madrid in the Silver Age Literature.   The “ Modern Woman in the Silver Age” collection offers interactive editions of short stories, novels and essays written by modern Spanish women authors that, in most cases, were no longer read (except in the context of a very specialized criticism) and has been rescued and critically annotated for a general public thanks to technological resources. This collection is complemented with two monographs related to the topic of modern women, which allows users to access relevant information for the interpretation of the literary works and the historical period itself, carried out by specialists.   The “ Children's Literature” collection presents two stories that has been updated for today’s readers –most of them digital natives– in an interactive way. Both tales were originally published in Spain anonymously in 1923 by the publisher Calleja:  Plague of dragons and  Spoiled Summer. As a result of this research, our project discovered that the real author of these stories was the writer Edith Nesbit, one of the best English-language authors considered a background of current fantasy literature. In this collection we have implemented an audiobook system so that users can listen to the stories in a dramatized way. Readers will also be able to consult two magazines of additional information in which the historical context of the works have been studied and the evolution of dragon’s myth in the fantastic literature has been traced (Reina, 2018).   In “ Madrid in the Silver Age Literature” collection, we have selected several works from this period in which the city of Madrid presents itself as a space of modernity. For this purpose we propound a new approach to the works of the novelist and journalist Andrés Carranque de Ríos (1902-1936), who represented through his writings the complexities of a world in transformation towards modernity. This collection is complemented with a geolocator through which readers can access to several maps where the itineraries of the characters that appear in the texts have been traced, so that the different corners of Madrid during this historical period can be explored in a virtual way.    INSTITUTIONS AND FINANCING   This work has been funded by the Biblioreca Nacional de España and by the project itself, within the Program of Research and Development Activities among research groups of the Comunidad de Madrid in Social Sciences and Humanities, co-financed at 50% with the European Social Fund. The result of the research has resulted in an interactive kiosk of the Silver Age of Spanish Literature hosted on the website of the Biblioteca Nacional de España.    LINKS OF INTEREST  BNE interactive collections   http://www.bne.es/es/Colecciones/LibrosInteractivos/index.html   Silver Age Literature kiosk:    http://cloud.madgazine.com/46f185c3185976675/?quiosco=46f185c3185976675&t=1542576418   ",
        "article_title": "Interactive Reading of the Silver Age: teaching and research promoted by the National Library of Spain",
        "authors": [
            {
                "given": "Alicia",
                "family": "Reina-Navarro",
                "affiliation": [
                    {
                        "original_name": "Universidad Complutense de Madrid, Spain",
                        "normalized_name": "Complutense University of Madrid",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02p0gd045",
                            "GRID": "grid.4795.f"
                        }
                    }
                ]
            },
            {
                "given": "Dolores",
                "family": "Romero-López",
                "affiliation": [
                    {
                        "original_name": "Universidad Complutense de Madrid, Spain",
                        "normalized_name": "Complutense University of Madrid",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02p0gd045",
                            "GRID": "grid.4795.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-12",
        "keywords": [
            "open content and open science",
            "libraries",
            "museums",
            "English",
            "pedagogy",
            "GLAM: galleries",
            "archives",
            "teaching",
            "digital archives and digital libraries",
            "literacy and creative writing",
            "and curriculum",
            "scholarly publishing",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "    This paper fleshes out the relations between the conceptual trinity of modernity, civilization and Europe (MCE) using digital history techniques. The idea of Europe as it emerged during the early modern period and developed over the nineteenth and twentieth centuries is often said to coincide with both ‘civilization’ and ‘modernity’ (Murray-Miller, 2018; Eisenstadt 2001). In the latest contribution to this topic, Murray-Miller argues that ‘the intertwining of these concepts is so extensive that, historically, one has typically served as a metonym for the other’ (Murray-Miller, 2018: 418-422). In this research we elaborate on this conceptual entanglement and evaluate the semantic boundaries that are said to define the MCE trinity. Based on a computational analysis of four Dutch newspapers spanning the period 1840-1990 we conclude that, in contrast to what the literature claims, semantic relations among the MCE elements are hardly visible and when they do, they are far from a ‘trinity’.    Histories of concepts such as civilization and modernity are often based empirically on a selection of semantically dense works written by a limited number of intellectuals. While we do not claim that newspapers fully represent the historical ‘Zeitgeist’ we do argue that they reflect a broader swath of periodically iterated public opinion, and thus help us understand the development of concepts in a broader segment of society. This research shows how the ‘streetlight effect’ in intellectual history can lead to what in effect is a eurocentric fallacy in the study of modernity and civilization.   By investigating conceptual interrelations we add to the emergent field of digital conceptual history. Earlier historians of concepts already recognized the value of tracing patterns in word use to understand conceptual change (Reichardt, 1985). Digital datasets and computational methods now enable more advanced enquiries into frequencies and distributions that reveal important aspects of conceptual change (Kenter et al., 2015; Recchia et al., 2016).    Corpora  We restrict ourselves to four Dutch-language newspapers issued between 1800 and 1990: Algemeen Handelsblad (AH, 1828-1970), Leeuwarder Courant (LC, 1800-1990), Nieuwe Rotterdamsche Courant (NRC, 1844-1869, 1909-1929, 1970-1990; AH and NRC merged in 1970 as NRC Handelsblad), and De Telegraaf (TEL, 1893-1990). These newspapers are fairly representative of the Dutch newspaper landscape and cover a large part of the nineteenth and twentieth centuries (Wijfjes, 2004). In this period, the form and content of the newspapers in questions changed significantly: the size and regularity increased, political affiliations became more explicit and commercialization contributed to a focus on local, regional and national matters (De Graaf, 2010). Although we find these changes not to ‘affect’ our concepts, we are aware that our selection consists of newspapers distributed nationally. Comparisons with regional newspapers, or newspapers more strongly attached to political ideologies would be fruitful for further research.    Methodology  Considering the variable and imprecise meaning and usage of ‘Europe’, we approach the ‘trinity’ from the perspective of modernity and civilization. We first aggregate bigrams that contain modern * , beschaafd *  (‘civilized’) and beschaving *  (‘civilization’) and subsequently analyze bigram-frequency, -productivity (number of different bigrams used) and -creativity (number of ‘new’ bigrams introduced) to identify change and stability in the word usage. We complement these methods with PMI-collocations. Subsequently, we look into our concept’s meanings and interrelationships by employing word embeddings (Mikolov et al., 2013). Since the introduction of this method, much work has been undertaken to employ word embeddings in the study of diachronic word evolution (Kutuzov et al., 2018; Tang, 2018)). We analyze the concepts in five periods (1840-1869, 1870-1899, 1900-1939, 1950-1969, 1970-1990), a periodization based on the trends observed in the bigram frequencies and the availability of newspaper data (Hamilton, Leskovec and Jurafsky, 2016). Because the corpus size increases considerably over time we use random sampling to obtain equally sized input data for our vector space models. After aligning the models, we extract the most similar terms that appear in all the models in a given period and rank them based on the average cosine distance. Entanglement is investigated using bigrams that can be seen as combinations of two concepts (i.e. ‘modern europa’ or ‘moderne beschaving’), collocations and a network-based approach to word embeddings. We extract the thirty words most similar those thirty words most similar words to the adjectives ‘modern’, ‘civilized’ and ‘european’ across five periods. Using the open-source visualization software Gephi we visualized the results, using the resulting 13,500 words as nodes in a network, and the relations between them as edges. The entanglement thus becomes visible through the degree of connectedness of the nodes, as well as the overall network density.    Findings We show how during the nineteenth century the concept of modernity experienced an interpretative shift from modernity as ‘the present’ to modernity as a stage in history. Bigrams show that the adjective ‘modern’ was also increasingly combined with abstract phenomena such as state, time and freedom. Similarly, the concept of civilization shifted from being associated with enlightenment and prosperity to a geographically locatable counterpart of barbarism. Elements of purity and superiority followed this localization of civilization in the decades surrounding 1900, only to be combined with notions of global values, science and the history of humankind in the second half of the twentieth century.   While these conceptual developments hint at a tightly woven idea of a modern European civilization, this idea is hardly corroborated by our data. If we assume that to be modern meant to be European, and to be European meant to be civilized, we would to find overlap in semantically similar words. Semantically similar words to ‘modern’, ‘civilized’ and ‘european’, however, seldom overlap. Signs of an MCE trinity are only spotted in the period 1870-1899, when colonialism and a rapidly changing industrial society produced new ideas about a superior and civilized European modernity. Outside this period, conceptual connections are present, but seldom between all three components of the trinity. Even if the trinity is assumed to be present, it was far from an equally sized triangle: Europe only marginally features in network representations of semantically similar words (Figure 1). Instead, it appears as a relatively insignificant word. Civilization and modernity hardly figure as spatial categories in newspapers, and in so far as they do, the most important concept is not ‘Europe’ but ‘the West’ (Geulen, 2011; Mishkova and Trencsényi, 2017; Bavaj and Steber, 2011, Bonnet, 2004).    Conclusion  Instead of an extensive and constant entanglement of modernity, civilization and Europe our research shows intermittent and alternating connections. Given that these results differ from research based on elite discourse, this paper demonstrates the need for digital research into conceptual interrelationships. In the case of the MCE trinity, this leads to a misreading of ‘eurocentrism’. Digital methods counter this fallacy and show how conceptual configurations assume different forms in different sources and social groups.    ",
        "article_title": "Disentangling a Trinity: A Digital Approach to Modernity, Civilization and Europe in Dutch Newspapers (1840-1990)",
        "authors": [
            {
                "given": "Ruben",
                "family": "Ros",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Joris",
                "family": "van Eijnatten",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-19",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "semantic analysis",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  1.   This paper presents an attempt to help understand the complex relationships between writers, their texts and their translators. A text by a writer may be translated by a single translator, or rival translations of the same text may appear; or a writer’s one text may be translated by one translator, and another of his texts by another translator; all this creates a complex mesh of connections that begs to be analyzed as a social network. The fact that some translators limit themselves to just one source language and one target language – the usual situation – does not mean that exceptions do not exist: Polish-Jewish anarchist poet Joseph Bovshover translated Polish into English (Sienkiewicz) and English into Yiddish (Shakespeare); Polish translator Ireneusz Kania translates (directly!) from 20 different languages (including Sanksrit and Tibetan). One of the main aims of this reconnaissance was to establish the degree of clustering by language, i.e. whether single-source-language communities are also interconnected.   2. Material and methods   Data on writers and translators was obtained from UNESCO’s Index Translationum, a large database of existing translations of texts from numerous domains (Bosser 2000). The data for this study was limited to  literary translations into Polish , a total of almost 18,000 individual editions of novels or collections of short stories by 8290 authors and 6582 translators from 155 languages. Editions of the same translations by the same translators were treated as separate entities, since this allows to discover authors, texts, translators and translations of particular cultural significance (or merely popularity). The network, prepared in Gephi (Bastian et al. 2009), used the number of editions of texts of an author translated by a translator as weights.    3. Simple measures   It is not surprising that a lion’s share of translations into Polish as recorded in the Index Translationum was from English-language literature: a total of 12190 editions, or 67.73%; this domination occurs in most cultures nowadays; in Poland, English has been the most-translated language since the 1930s (Krajewska 1972, 14). This is also visible in the most-translated (or, more precisely, most-published) authors: of the top 10, only Hedwig Courts-Mahler and Jules Verne wrote in a language other than English (respectively German and French); of the top 5, only Jack Higgins was male, preceded by Barbara Cartland (the top scorer with 75 published translations), Nora Roberts, Agatha Christie and Danielle Steel; the top 10 also included Lucy Maud Montgomery, Graham Masterton, André Norton and various Disney products (treated jointly here). Authors deemed as “classical”, “canonical” or “artistic” only began to appear at the bottom of the second ten: Philip K. Dick came as 19 th , followed by Mark Twain, Jack London and Hans Christian Andersen; Dickens was 27 th ; Tolkien, 41 st ; Conrad, 46 th ; Ursula K. Le Guin, 54 th , beat Shakespeare, 56 th . German and French literature came far behind English and close to each other, with, respectively, 6.91% and 6.77% of the entire database, followed by Russian (4.22%), Spanish (2.16%) and Italian (1.90%). Apparently, Poland’s most prolific and ubiquitous translator is one Małgorzata Fabianowska, the translator of 73 editions by 71 authors (Disney products and books based on films), followed by Jacek Manicki, with 66 translators of such authors as Stephen King, Robert Ludlum and John Grisham. The fact than none of Poland’s celebrity translators appeared in any top positions shows the extent to which quantity does not equal quality in literary translation.   3. Social network analysis   Figure 1 presents a three-dimensional network visualization of the data, produced with the Fruchterman-Reingold force-directed algorithm (1991) and edited with UCSF Chimera (Pettersen et al. 2004).    Figure 1. Writer-translator network. Nodes are writers and translators; edges are translations.   As is evident from the general description of the content of the database, the large community in the middle of the sphere shows a strongly-interconnected web of translations from the English. By comparison, other major literatures (German, French, Russian, Spanish and Italian) are much less represented. There is precious little connection between the source languages (despite the above-mentioned existence of multilingual translators). The debris at the external limits of the sphere are instances of one-time translators of texts by authors translated only once, and thus unconnected with the community of a given source language; instances of a single author translated just twice or thrice and/or by just one or two translators, etc. Figure 2 presents an example of this:     Figure 2. A small community of two French authors, Peyremaure and Reznikoff sharing a translator, Olędzka; Reznikoff was also translated by Wasitowa.   Whenever larger communities are less interconnected, it is feasible to follow the connections within. Figure 3 does this for a subset of the French community. Jean-Paul Sartre is thus connected via his translators to such writers as Raymond Queneau and Jacques Prévert (both Satrapes of the Pataphysical Club), indicating that sharing translators might follow a chronological if not a generational or an ideological logic. But a somewhat more versatile translator, Jacek Trznadel, also links Sartre with Marquis de Sade. Numerous other interesting connections could be observed.      4 . Conclusions   Social network analysis thus seems a good tool to examine relationships within a large database such as the Index Translationum, relationships which are not accessible through direct retrieval of its information. The examples of connections mentioned above obviously do not even begin to describe the extent to which visualizing these networks can be helpful to write, perhaps, an entirely new history of literary translation. Once similar network are drawn for other target languages (and perhaps for more reliable databases of translations than the Index Translationum, which, for all its size, has some issues of reliability and representativeness), mathematical social network values (such as homophily, density, or clustering coefficient) could be compared between the literary cultures of these languages to produce quite a new type of distant reading of the phenomenon of literary translation.  ",
        "article_title": "Analysis of Writer-Text-Translator Social Networks",
        "authors": [
            {
                "given": "Jan",
                "family": "Rybicki",
                "affiliation": [
                    {
                        "original_name": "Jagiellonian University, Krakow, Poland",
                        "normalized_name": "Jagiellonian University",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/03bqmcz70",
                            "GRID": "grid.5522.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-31",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "translation studies",
            "network analysis and graphs theory",
            "English",
            "cultural analytics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In humanistic research, Named Entity Recognition is highly useful, but it mines surface data, rather than revealing the complex nature of relationships between these entities. Named Entity Recognition (NER) extracts the names of people, locations, organizations, and, depending on the model, may also extract references to money, percentages, dates, and times, in addition to a miscellaneous class. Although this is certainly useful, NER does not represent the richness of the documents with which we work. For example, consider this fragment from a nineteenth-century French chronicle of Ottoman Algerian history: ‘To further attach himself [to his ally], Pasha Hassan married his [ally’s] daughter, then he launched troops against the rebel…’ (Mercier, 1903: 200) In just this short passage, which is not even a full sentence, we find several people referenced who are unnamed. If we look back at the text, we see that Pasha Hassan’s ally is Ben-El-Kadi of Kuku, Algeria, and the rebel is Abd-El-Aziz, but Ben-El-Kadi’s daughter is never named. This occurs frequently in historical source material. Those who remain unnamed are most often women, servants, slaves, and Indigenous people – the very people about whom scholars are most anxious to know more. This short paper presents a work-in-progress: a digital workflow and Python script to mine and model the relationships between extracted entities from French-language documents in order to grapple with the complexity of human relationships and cultures, as well as the perspectives of authors and their informants.   This short presentation will share the complete information extraction code, its accuracy, the resulting visualizations, and a brief analysis from the case study. The method presented has applications far beyond French language and the history of the Middle East and North Africa. For instance, with some adjustments for language, this method would be highly useful in the analysis of The Twenty-Four Histories of China, the official history of the Chinese dynasties between 3000 B.C.E. and the seventeenth century. More broadly, this approach will be of use to scholars interested in identifying and studying relational data, social positions, and networks of both known and previously unknown actors, particularly those who remain unnamed in the source material.  As a test corpus, this project uses four digitized, OCRed, and hand-cleaned nineteenth-century French chronicles of Ottoman Algerian history. The volumes range between 41,341 words and 170,737 words and cover the period 1567 to 1837 with a focus on Constantine, the easternmost province in Algeria. The challenge is to extract not only named entities and their relations to one another, but to extract unnamed persons and their relationships as well. In simple NER, the names Moustafa and Namoun, would be the only extracted data in the following sentence: ‘Moustafa avait épousé une des filles de Namoun,’ but the daughter of Namoun who married Moustafa would not appear. (‘Moustafa married one of Namoun’s daughters.’ Vayssettes, 2003: 52. Author’s translation.) The goal of this project is to uncover the positions and roles of women in Algerian society, so it is essential to locate and retrieve data about unnamed people.  The built-in language models and extensibility of the spaCy natural language processing (NLP) library for Python makes it most suitable for this project (Honnibal and Montani, 2017). Specifically, spaCy enables researchers to define entities and build custom information extraction systems. Additionally, spaCy’s library features a French language model that has a built-in tagger, parser, and NER, unlike the Natural Language Toolkit or Stanford’s CoreNLP Open Information Extraction system.  To build an information extraction system with spaCy that pulls the desired relational data, we must first identify an extractable pattern by parsing and tracing the dependencies of a sample sentence, as follows: SpaCy’s visualizer also allows us to view the dependency parse tree using the following code and sample sentence.  An examination of the parse tree above yields a pattern of parts-of-speech around the keyword ‘épousé’ that we can use to extract the desired information about this relationship. Since we are interested in identifying the relationships between both named and unnamed people, we will look for specific patterns in parts of speech and syntax, as well as the location of proper nouns in relation to keywords. Based on an examination using the concordance method with the sample texts, the following keywords generated the best data: fils, fille, mariage, épous*, gendre, and beau pére (son, daughter, marriage, spouse/to marry, son-in-law, father-in-law). For example, the word ‘fils,’ or ‘son,’ yielded more consistent results for father-son pairs than the word ‘pére,’ or ‘father.’  From an examination of the word ‘fils’ in context, as shown above, general patterns emerged. The patterns for ‘fils’ and the proper output format for each pattern are shown below. These outlines then inform the Python script that uses spaCy’s library to extract the relational data. This script will be made freely available on GitHub following the DH 2019 Conference. Based on the examples and patterns above, the information extraction system derives relational data that easily translates into node and edge lists. In this case study, network analysis of the extracted data highlights how women, marriage, and kinship connections legitimated Ottoman rule. Initial findings suggest that Algerian women were key links in the chain that bound Algeria to the Ottoman Empire.  ",
        "article_title": "From the Margins to the Center: A Method to Mine and Model Complex Relational Data from French Language Historical Texts",
        "authors": [
            {
                "given": "Ashley",
                "family": "Sanders Garcia",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles, United States of America",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "natural language processing",
            "data mining / text mining",
            "English",
            "network analysis and graphs theory",
            "near eastern studies",
            "indigenous studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  When working with literary texts, a problem for linguists, literary scholars and for machine-based text understanding is the classification of text-types. The term “text-type” refers to a variety of different phenomena reaching from a superordinate view of genre (Chatman 1990) to functionally motivated text-types as aggregations of structural or linguistic features (Biber 1988, 1989). While the taxonomy, textual layer and functionality of different theories behind text-types may differ widely, a common concept behind these theories is the understanding of textual surface structures varying in their respective text-type (Fludernik 2000). The text-types “descriptive”, “narrative” and “argumentative” emerge very frequently in many theories (Werlich 1975, Adam 1985, Chatman 1990). Being able to automatically assign sentences to these text-types is therefore highly desirable when aiming to support quantitative literary studies. In this work we present our text-type dataset, a feature based machine-learning model and a deep-learning based model and show that both are able to classify text-types.    Annotation Scheme  Functionalizing the existing theories to a respective abstraction of surface phenomena, we came to our annotation guidelines:  descriptive: the description of a physical object in its dimensions, parts, and/or properties  narrative: the representation of a chain of events, actions or activities in its temporal progress driven by persons or other “actors”  argumentative: the presentation, explanation and justification of an abstract idea in its logical context   To consider possible shortcomings of these guidelines or the underlying model and to give the annotators an opportunity to indicate their uncertainty, we introduced additionally the label “unknown”. In our annotation tool (see figure 1), sentences were presented within a small contextual framing.       Figure 1: Contextual frame for the sentence (bold) to be annotated in our annotation tool    Dataset  We chose a random subset of 30 novels of the DROC corpus (Krug et al. 2017) for our experiments. From each novel we extracted a continuous segment of about 1% of the length. Each of the 1773 sentence obtained in this way was annotated by 3 annotators in order to judge the complexity of the annotation task and to identify a subset of sentences annotated with highly reliable text-type labels. Especially the argumentative text-type seemed to be difficult to annotate since the annotators often disagreed or showed uncertainty on the text-type e.g. for exclamations or other speech acts. When demanding full agreement among the annotators, the number of instances is reduced from 1773 to 830 sentences denoted as D S (218 descriptive, 352 narrative, 260 argumentative). A majority vote (i.e. the consent of at least two of three annotators) leads to 1503 labeled instances denoted as D M (366 descriptive, 540 narrative, 597 argumentative).     Experimental Setup    Feature Construction  For the task of classifying text-types, we modeled several features targeting different surface levels:  Bag-of-Words. As a baseline, we computed a simple count-based feature vector, representing how often which words occur per instance.  Indicator-Words. Especially the  argumentative text-type has frequent discourse-related indicator words. We used discourse particles, modal particles, interjections and punctuation marks.   Word-Vectors. In contrast to sparse bag-of-words vectors, dense word-vectors like Word2Vec (Mikolov et al. 2013) comprise semantic information. We used FastText (Mikolov et al. 2017) since the character-based model is able to model compound words implicitly, which are very common in German. After unsupervised training of 100 dimensional  Using 100 dimensions proved suitable in a initial experiment.  word-vectors on the complete DROC corpus, a TSNE-based visualization of the vectors (Maaten & Hinton 2008) revealed a noticeable cluster of words of the  descriptive text-type ranked via SD2-Zeta (Schöch et al. 2018) (see figure 2). Therefore, we modeled FastText based vector-semantics in two ways: averaging the word-vectors over all words within an instance and counting cluster-membership for words using a previously trained k-means (k=5) model.   Germa-Net. We used GermaNet (German WordNet) to generalize words in two ways: First, we followed each path towards to the most general hypernym and selected different hypernym levels as degrees of generalization. Second, we used GermaNets categorical structure for abstraction. Besides its taxonomic structure, GermaNet classifies words into 54 categories, e.g. the verb ‘sagen’ (engl. to say) belongs to the category ‘verb / communication’. We use these categories as generalizations for each word to reflect e.g. descriptions of places (‘adjective / place’) or people (‘adjective / body’).      Figure 2: TSNE-Visualization of FastText word-vectors. Blue color indicates that a word is more prominent for the descriptive text-type. The indicativeness was judged opposing the descriptive and other text-types via SD2 Zeta. Other text-types show similar results.     Classification Task  To examine the contribution of each feature, we conducted an extensive feature analysis using a Support Vector Machine (SVM)  Random Forest classifier achieved slightly worse to similar results.  as a classifier and compared its performance to a deep-learning based Recurrent Neural Network (RNN) Model.  The SVM was used with linear and RBF kernels, varying the hyperparameter C  C ∈{1,10,100,1000} . Additionally we also evaluated the influence of the degree of generalization using GermaNet on the classification performance.  For the RNN, we adopted the BiGRU model from (Song et al. 2017) but introduced additional loss functions and model parameters as follows: We varied the number of GRU layers between 1 and 4 and the dimensions of the hidden layers between 100 and 400. We used the pretrained FastText model as initial embedding weights.  We conducted a grid-search based parameter study for both classifiers to find the best model-parameter configuration. The model architecture is depicted in figure 3.       Figure 3: BiRNN Model. Number of GRU Layers were varied between one and four in a parameter-study. Categorical-cross-entropy as loss and softmax as activation achieved best results. Table 1: Precision, recall, f1-score and accuracy for the SVM classifier on a random hold-out dataset from D S and D M            Results  We evaluate the models on sampled hold-out dataset fractions of 10% and report accuracy for both datasets, D S and D M. The majority baseline (always predicting the most frequent class) yields an accuracy of 0. 457 for D M  and 0.398 for D S .  The best performance of 0.864 mean  For 20 repetitions on different hold out-sets  accuracy ± 0.047 standard deviation for D s resp. 0.776 ± 0.022 for D M was achieved by the linear SVM classifier, C = 1, only using the average FastText word vectors as feature (see table 1 for a label-wise evaluation). This result supports our finding of rather distinct text-type word-vector clusters in our explorative analysis. However, we surprisingly find that additional features do not improve the result. The best feature combinations without averaged FastText vectors yield significantly  For 20 repetitions on different hold out-sets, α = 0.05 using Pitman’s permutation test as suggested by (Dror et al. 2018)  lower accuracies (0.813 resp. 0.735) and use the FastText cluster features. The best features without any FastText use bag-of-words and part-of-speech features for D M (0.696) and additionally GermaNet hypernym and category features for D S (0.751).  The generalization-depth study aimed at finding the optimal degree of lexical generalization between the word itself and a very abstract and non-indicative root-hypernym. We therefore followed the hypernym path from each word to each root-hypernym and selected the highest and the lowest three hypernym-levels in a classification setup only using this feature. Our results indicate that the second hypernym (i.e. the hypernym of the hypernym of the word) is the most promising level of abstraction for our task. However, we are aware of the problem that hypernym relations for different words might each represent different degrees of abstraction, depending on the level of detail at which the taxonomy is modeled. The RNN model achieved significantly  For 10 repetitions on different hold out-sets, α = 0.05 using Pitman’s permutation test.  lower accuracies for all model variations in comparison to the SVM. We believe that this is mainly a problem of too few training data since in theory, the RNN should be able to model the best performing feature, the averaged FastText vectors. In contrast to (Song et al. 2017), our best performing model used categorical-cross-entropy as loss function and one GRU layer with a dimensionality of 400 on both datasets reaching a mean  For 10 repetitions on different hold out-sets.  accuracy of 0.801 ± 0.049 standard deviation for D S and 0.702 ± 0.029 for D M.  All detailed results, including the results for our feature study are also available on GitHub  https://github.com/cligs/projects2019/DH_TextTypes .     Discussion and Future Work  Our results show that an SVM as well as a deep-learning approach are able to classify text-types with an accuracy far beyond the baseline. To some extent, handcrafted features are able to compensate the small amount of training data for the D S dataset with an SVM and outperformed the best deep-learning based model. Since the performance of deep-learning based models heavily relies on a sufficient amount of training data, this outcome isn’t very surprising and might be revised if more data becomes available. In comparison, the feature driven linear SVM classifier might also have an advantage when it comes to interpretability: The coefficient-weights of an SVM classifier can be interpreted as a whitebox model (Zehe et al. 2017) and reveal interpretable insights into the decision process, whereas the decisions made by a neural network cannot easily be inspected, which is crucial for theory construction and deconstruction in the (digital) humanities.  For future work, we plan to examine in detail why the handcrafted features such as indicator-words or WordNet abstractions don’t seem to be as useful as expected. We also plan to incorporate data augmentation methods and finally do a consolidating annotation run to have a bigger and cleaner text-type dataset.  ",
        "article_title": " Classification of Text-Types in German Novels  ",
        "authors": [
            {
                "given": "Daniel",
                "family": "Schlör",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Christof",
                "family": "Schöch",
                "affiliation": [
                    {
                        "original_name": "University of Trier, Germany",
                        "normalized_name": "University of Trier",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02778hg05",
                            "GRID": "grid.12391.38"
                        }
                    }
                ]
            },
            {
                "given": "Andreas",
                "family": "Hotho",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "artificial intelligence and machine learning",
            "natural language processing",
            "English",
            "computer science and informatics",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  With his idea of 'Distant Reading', Moretti (2000) introduced an important  leitmotif in the Digital Humanities that has led to an ongoing discussion about quantitative methods in literary and cultural studies (Clement et al., 2008; Crane, 2006). We believe that the literary genre of drama is particularly well suited for quantitative analyses and hence adapt the concept of \"Drametrics\" (as proposed by Romanska, 2015) as a term for the distant reading of dramatic texts. In addition to the actual dialogs, dramatic texts contain other structural elements that can be easily quantified, such as the characters of the play as well as an explicit act and scene structure. Keeping these features in mind, it is hardly surprising that we find a number of recent studies dedicated to the quantitative analysis of drama (e.g. Ilsemann, 2013; Wilhelm et al., 2013; Nalisnick and Baird, 2013; Trilcke et al., 2015; Dennerlein, 2015; Xanthos et al., 2016; Willand and Reiter, 2017; Krautter, 2018). At the same time, there have been quantitative approaches to the analysis of drama that date far back into the pre-digital age. As an example for early approaches to quantitative analyses of drama, we would like to refer to the ideas of Marcus’ (1973) mathematical poetics, which also contains interesting approaches for quantitative drama analysis.    Solomon Marcus’ Mathematical Poetics Marcus suggests the scenic presence of characters as a basic computable measure of a play, which, for each dramatic text, can be visualized by means of a  configuration matrix (Marcus, 1973). The matrix (cf. figure 1) contains one row for each character of the play, and one column for each scene. Whenever a character appears on stage, the value 1 is entered into the corresponding cell; if a character is not present in a scene 0 is entered as a value.      Figure 1: An example configuration matrix visualizes the appearances of characters (A-G) throughout the 15 scenes of the play.  Configuration matrices can be used to compute various quantitative aspects of a drama, for instance: the  scenic distance and  proximity of characters and even specific relationships between characters (e.g.  dominance, alternation, independence or  concomitance) as well as the overall  configuration density of plays (Marcus, 1973). The  configuration density is calculated by dividing the number of cells holding a 1 by the total number of cells. In other words, the configuration density indicates how many of the potential character appearances have actually been realized. It can be understood as a measure of a play’s 'population density'. When every character appears on the stage in every scene, the play has a theoretical maximum configuration density value of 1.  During the 1970s and early 1980s, several studies applied Marcus’ mathematical approach for the analysis of texts, always dealing with very few samples of text (cf. Marcus, 1974; Marcus, 1977; Marcus, 1984). In these studies, configuration matrices proved to be useful in text analysis, as they fasten and simplify the overview of a character’s first or last appearance, co-presence or avoidance with other characters. Some years later, Ilsemann (1998) took on the ideas of Solomon Marcus to explore Shakespeare’s plays in a quantitative way. Ilsemann (1998) used the frequency and lengths of characters’ speeches as further parameters and found that the configuration density is an important aspect of genre-distinct quantitative patterns for comedies, romances, tragedies and history plays. In 2005 and 2008, Ilsemann used the frequencies and distributions of speech lengths to discuss authorship attribution in Shakespeare’s plays.    The  Katharsis Tool  In order to be able to automatically analyze quantitative aspects of dramatic texts according to Marcus’ character configurations and Ilsemann’s analysis of speech lengths and frequencies, we have created  Katharsis, a tool for computational  drametrics. The  Katharsis tool comprises a  parsing component that extracts and calculates various quantitative parameters as suggested by Marcus (1973) and an analysis component that searches for dramatic texts of a certain author, genre, timeframe, etc. Currently, a test corpus of approx. 100 German drama texts from the  TextGrid Repository  Available online via https://textgridrep.org/ (Note: all URLs mentioned in this article were last checked April 29, 2019)  is available for analysis. The texts are available as TEI-XML, allowing for the extraction of metadata (title, author, year etc.) and speeches with the corresponding speaker and structural information. Note that the tool can be extended with further plays from other authors and genres if the texts are encoded in TEI-XML. Furthermore, the quantitative metrics are independent of the language. Figure 2 shows the  Katharsis results for a search for dramatic texts by Friedrich Schiller. Users can download any quantitative information displayed in the screenshot in JSON format for individual analysis.      Figure 2: Summary of quantitative information calculated by Katharsis for dramatic texts by Friedrich Schiller.  With the help of  Katharsis researchers are able to examine a specific drama in more detail. The tool provides an interactive configuration matrix to explore character appearances and speech statistics for each configuration (figure 3).      Figure 3: Katharsis snippet of the interactive configuration matrix for the play Maria Stuart, by Friedrich Schiller.  Furthermore,  Katharsis produces a table and several interactive bar charts to analyze the distribution of speakers and speech statistics on the structural levels (act and scene) and the progression of these metrics throughout the course of a play (for an example see figure 4).      Figure 4: Average length of speeches (measured in number of words) throughout all acts of the play  Maria Stuart by Friedrich Schiller.  Another segment of the tool shows statistics concerning the comparison of speakers like speech statistics and the distributions of scenic presence. Furthermore, following Marcus’ (1973) approach, specific character relations derived from the configuration matrix can be explored. For each character of the play, the tool displays relations to other characters which may be of the type  dominate/dominated, alternative, independent or concomitant.   The last component concerning the analysis of individual dramatic texts follows Ilsemann’s (2005; 2008) idea to examine the distribution of speech lengths in the play. We calculated the speech length by counting the number of words. Users can analyze an interactive histogram and a curve chart. Different speech lengths can be included in the visualization dynamically to narrow down the range of speech lengths for more in-depth analysis (see figure 5 for an example with a comparison).   Finally,  Katharsis can be used to analyze and compare self-created collections of plays by means of various quantitative aspects. The comparison of different genres and authors is a pre-configured comparison. Figure 5 illustrates a comparison of speech lengths for Goethe and Schiller showing that Goethe’s most frequent speech length is seven while Schiller’s is rather low with only four words. This might be one reason why the plays of Goethe never were that successful on stage like those of Schiller.      Figure 5: Comparison of the relative distribution of speech lengths for the plays of Goethe and Schiller.  The  Katharsis tool is available online and can be tested as a live demo in any current web browser:  http://lauchblatt.github.io/Katharsis/index.html    Case Studies on Quantitative Drama Analysis In this section, we illustrate the usefulness of  Katharsis by means of short case studies: An important computable aspect of dramatic texts are the encounters of characters on stage in different configurations. A case study that used  Katharsis on 13 tragedies, 17 comedies, one tragicomedy and one  Schauspiel of the German authors Andreas Gryphius, Christian Weise, and Gotthold Ephraim Lessing verified the hypothesis that there is a trend for comedies to have higher configuration densities than tragedies (Dennerlein, 2015). For dramatic German texts from 1600 to 1800 the mean length of speeches in comedies (as compared to tragedies) is lower (see figure 6), whereas the total number of speeches is higher (see figure 7), which means characters in comedies seem to interact in a more dialogic manner.      Figure 6: Average length of speeches in comedies and tragedies of the corpus.      Figure 7: Average number of speeches in comedies and tragedies of the corpus.  This seems plausible with regard to some characteristics of tragedies and comedies already known: Tragedies more often feature monologues because they provide the ideal occasion to reflect on jealousy, hatred, guilt, plans of murder, or suicide. A general lack of communication, or communication difficulties, may be associated with the fact that generally fewer characters share the stage. In comedy, however, protagonists more often encounter each other. Typical comic effects such as confusions between characters or characters exchanging roles as well as speeches delivered at spectators, are staged in the presence of several characters and may result in a rather high configuration density.   Future Work: Sentiment Analysis for Drama To enhance the applicability of  Katharsis as a tool for computational drametrics, we are currently preparing to include basic sentiment analysis techniques (Liu, 2016) as an addition to mere structural parameters. While sentiment analysis has been particularly popular in the field of computational linguistics, the approach is also gaining popularity in literary studies (Alm and Sproat, 2005; Nalisnick and Baird, 2013; Mohammad, 2011). So far, we have evaluated different sentiment analysis techniques for the context of historic, German language plays (Schmidt and Burghard, 2018a; Schmidt and Burghardt, 2018b; Schmidt, Burghardt and Dennerlein, 2018a; Schmidt, Burghardt and Dennerlein, 2018b). A first  Katharsis prototype (Schmidt and Burghardt, 2018b; Schmidt, Burghardt and Dennerlein, 2018b) that implements sentiment analysis for 12 German plays by Gotthold Ephraim Lessing is available online:  http://lauchblatt.github.io/Katharsis/sa_selection.html  In the long term, we plan to combine character-to-character sentiment analysis (cf. Nalisnick and Baird, 2013) with the existing configuration matrices, thus not only transferring Marcus’ approach of mathematical drama analysis to a digital tool, but rather enhancing it by using additional parameters such as character sentiment.  ",
        "article_title": "Katharsis – A Tool for Computational Drametrics",
        "authors": [
            {
                "given": "Thomas",
                "family": "Schmidt",
                "affiliation": [
                    {
                        "original_name": "University of Regensburg, Germany",
                        "normalized_name": "University of Regensburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01eezs655",
                            "GRID": "grid.7727.5"
                        }
                    }
                ]
            },
            {
                "given": "Manuel",
                "family": "Burghardt",
                "affiliation": [
                    {
                        "original_name": "University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Katrin",
                "family": "Dennerlein",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg, Germany",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Christian",
                "family": "Wolff",
                "affiliation": [
                    {
                        "original_name": "University of Regensburg, Germany",
                        "normalized_name": "University of Regensburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01eezs655",
                            "GRID": "grid.7727.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "german studies",
            "content analysis",
            "natural language processing",
            "data mining / text mining",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Abstract This paper presents a fully automatic approach to the scansion of Ancient Greek hexameter verse. In particular, we describe how finite-state automata can be used to discriminate between the 32 variants of Ancient Greek hexameter. We evaluate the performance of our annotation algorithm against hand-annotated scansion data. The project code is available online   https://github.com/anetschka/greek_scansion. .    Greek literature has, for centuries, served as a paradigm and model for literary writing all over Europe. The epitomes of Ancient Greek literature, the Odyssey and the Iliad, are epic poems that share the same metre: hexameter. Hexameter annotation is crucial for large-scale and data-driven investigations dedicated to these poems, and automatic annotation algorithms open up new opportunities for research in this field. Ancient Greek hexameter verses can be described as regular sequences of long and short syllables, with the length of each syllable being determined by the length of the syllable’s vowel. Long and short syllables are organised in  feet of the following form:    Dactyl: The foot is composed of three syllables, the first of which is long, while the others are short.   Spondee: The foot is composed of two long syllables.   Six feet make a complete hexameter. Feet 1-5 can be either spondees or dactyls, only the last foot is restricted with respect to its metric form: It is composed of a long syllable and the so-called anceps the length of which is variable. Figure 1 is a generic depiction of the resulting 32 variants of Ancient Greek hexameter. Due to the free flow of either dactyls or spondees, hexameter can accommodate varying syllable counts (from 12 to 17 syllables) and produce a broad range of rhythmic effects. Section 2 of this paper provides an overview of related work. Section 3 describes the annotation algorithm. Section 4 gives the evaluation results and section 5 concludes this paper.    Figure 1. Generic hexameter scheme. Vertical bars separate feet. Horizontal bars indicate long syllables, bows indicate short syllables. Vertical stacks of symbols indicate that both variants are possible. X marks the  anceps.     Related Work An early, rule-based approach to the semi-automatic scansion of Greek hexameter has been developed by Höflmeier (1982). Höflmeier combines two different kinds of knowledge to resolve hexameter verses:  Local linguistic rules that establish which vowels are short, and which vowels are long. Knowledge about the overall structure of the verse for the resolution of partially annotated verses.  The approach is semi-automatic since the resolution of verses that possibly exhibit complex linguistic phenomena such as  synizesis (the metric contraction of normally distinct vowels) is delegated to the user. A similar approach has later been proposed by Pavese and Boschetti (2003).  An advanced study in the automatic scansion of metric poetry is the work by Greene, Bodrumlu, and Knight (2010) who use weighted finite-state transducers, trained on a small corpus of manually annotated data, to analyse Shakespearean sonnets. The authors report accuracy values of up to 81.4 %. An interesting approach to the problem of Greek hexameter scansion is presented by Papakitsos (2011). Papakitsos performs syllabification and then employs a search strategy to identify dactyls, that is, the verses are not analysed left-to-right. Rather, the search starts in the fifth foot where dactyls are particularly likely. Once the appropriate – for the established number of syllables in the verse – number of dactyls has been identified, the search terminates. Dactyls are, again, identified by means of local linguistic rules. The search, however, is strongly dependent on the correctness of the syllabification. For instance, if the verse under analysis has been found to consist of 13 syllables, the search algorithm will look for exactly one dactyl. Papakitsos reports a recall of 0.98 and a precision of 0.80.  A rule-based implementation of a fully automatic Greek hexameter scansion algorithm has been published by Hope Ranker   https://github.com/epilanthanomai/hexameter. . This algorithm uses an ensemble of weighted finite-state transducers to resolve the feet one by one.  Alternative approaches to the automatic analysis of metric poetry employ machine learning. In these studies, the problem is usually modelled as syllable-wise classification. For instance, Estes and Hench (2016) employ a Conditional Random Fields classifier to analyse Middle High German epic texts, reaching an f-measure of 0.90. Zabaletak (2017) reports on a very wide range of experiments, but achieves the best results with a combination of a sequential model and deep learning for the classification of English, Spanish, and Basque verses. N-grams, positional and length features as well as linguistic markers are used to train the models.   Finite-State Approach to Hexameter Analysis Our approach to the scansion of Ancient Greek hexameter is based on the same two types of knowledge that were already used by Höflmeier (1982):   Local search: We use 5 local linguistic rules to determine whether a pair of syllables can safely be considered long (that is, it forms a spondeus).   Global analysis: We exploit knowledge about the overall structure of Greek hexameter to complete partially annotated verses, that is, verses that could not fully be resolved with the help of the linguistic rules.   Moreover, the local search step follows the strategy of Papakitsos (2011) in that it searches for a fixed number of spondees that result from the syllable count established during syllabification. Figure 2 shows a visual representation of our scansion algorithm. The algorithm scans epic Greek text verse by verse:   Pre-processing consists mainly of lower-casing and the removal of diacritics.  Moreover, we have implemented a  syllabification algorithm that uses regular expressions to identify syllables and to establish the syllable count of the verse.  The  local search and all following steps are then handled by dedicated deterministic finite-state automata (FSAs). There are specialised FSAs for verses of 13, 14, 15, and 16 syllables and a simpler FSA for all remaining cases. In the local search step, the active FSA performs a targeted search for a given number of spondees, using 5 simple linguistic rules. If enough spondees are found, the plausibility of the solution is checked. Otherwise, the verse is passed to the global analysis step. The FSAs were implemented using an existing Python library   https://github.com/pytransitions/transitions .  .  For  global analysis, we use a non-deterministic finite-state transducer (FST). In this transducer, each syllable corresponds to a state, and alternative solutions are modelled by means of alternative paths. The FST is weighted, but since we did not have access to an appropriate training corpus, we were not able to learn transition weights from data. Instead, they were set manually following the description provided by Papakitsos (2011). The FST was implemented using the Helsinki Finite-State Tools   https://hfst.github.io/python/3.12.1/index.html. .  If the  plausibility check fails, the verse is passed over to  error handling to compensate for potentially erroneous syllabification. Global analysis then completes the verse. The plausibility of the result is checked again. Depending on this result, the FSA will transition to its final state, that is, either  success or  failure .  If the verse, however, passes the plausibility check immediately after the local search step, the FSA transitions directly to the  success state.      Figure 2. Visual representation of the scansion algorithm.    Evaluation We have evaluated the performance of both our syllabification and our scansion module against hand-annotated verse data. The annotations were carried out by two advanced students of Greek philology, discrepancies and errors were clarified by means of group discussions. For syllabification evaluation, we randomly chose a set of 171 verses (2695 syllables) from both the Odyssey and the Iliad. For scansion evaluation, we randomly selected 346 verses from a broader range of Ancient Greek texts. Table 1 provides an overview of this data set. For syllabification, we achieved a syllable-wise accuracy of 0.98. Verse-wise accuracy reached 0.82. Scansion correctness was evaluated by means of precision, recall, and f-measure with the following results:   Precision: 0.95   Recall: 1.00   F-measure: 0.98   The evaluation scripts are included in the open-source code package of our software.   Table . Evaluation data.    Conclusion In this paper, we have presented a fully automatic approach to the analysis of Ancient Greek hexameter text. Automatic annotation tools are crucial for data-driven investigations in Greek philology. Our algorithm integrates various kinds of linguistic knowledge into a set of finite-state automata and thus makes use of well-defined concepts in the field of computational linguistics, while remaining transparent to philologists. Our evaluation results are competitive. Future work will be dedicated to the exploitation of the resulting annotations for research in Greek philology.  ",
        "article_title": "A Finite-State Approach to Automatic Greek Hexameter Analysis",
        "authors": [
            {
                "given": "Anne-Kathrin",
                "family": "Schumann",
                "affiliation": [
                    {
                        "original_name": "Fernuniversität Hagen, Germany",
                        "normalized_name": "University of Hagen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04tkkr536",
                            "GRID": "grid.31730.36"
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Beierle",
                "affiliation": [
                    {
                        "original_name": "Fernuniversität Hagen, Germany",
                        "normalized_name": "University of Hagen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04tkkr536",
                            "GRID": "grid.31730.36"
                        }
                    }
                ]
            },
            {
                "given": "Norbert",
                "family": "Blößner",
                "affiliation": [
                    {
                        "original_name": "Freie Universität Berlin, Germany",
                        "normalized_name": "Freie Universität Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/046ak2485",
                            "GRID": "grid.14095.39"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "corpus and text analysis",
            "linking and annotation",
            "artificial intelligence and machine learning",
            "English",
            "computer science and informatics",
            "philology",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper explores the development of a prosopographical database for the field of Syriac studies called SPEAR: Syriac Persons, Event, and Relations. Syriac is a dialect of Aramaic used in the Near East and South and Central Asia between the 3rd and 8th centuries and continues to be used liturgically by Christians in the Middle East and India as well as expatriate communities in Europe and North America. This project employs a factoid-based approach to prosopography. Where most factoid-based prosopographies organize data in a relational database, SPEAR encodes prosopographical data from primary source texts in Text Encoding Initiative (TEI) XML using a customized schema designed to facilitate linking this propopographical data to other linked data resources and for serialization into RDF (Resource Description Framework). The  Srophé Application employing eXist DB ingests the TEI documents and allows for visualization and querying of data.    From text to factoids SPEAR has looked for inspiration to the Prosopography of Anglo-Saxon England ( PASE) and other factoid prosopographies coming out of the Department of Digital Humanities at King’s College London. Where traditional print prosopographies distill short narrative summaries of known information about historical individuals, factoid-based prosopographies collect and tag discrete pieces of information asserted in primary source texts (Bradley and Short, 2005; Tinti, 2007). The result is a text-based persons database. Encoders work text-by-text to encode relevant prosopographical material that can be sourced to that text such as name variants, genders, occupations, physical characteristics, personal relationships, and historical events. Each factoid is encoded in a unique <div> element with a TEI customization to add a @uri attribute, thus assigning a Uniform Resource Identifier (URI) to each factoid <div>. Every factoid also includes one or more <bibl> elements pointing to a Syriaca.org bibliography URI for a specific edition of the work. TEI encoding provides an XML structure and the shared semantic content of elements and attributes.     Sample SPEAR factoid   Linked open data framework The SPEAR data model integrate the TEI encoding of prosopographical data with the Linked Open Data (LOD) framework of Syriaca.org. Syriaca.org provides URIs and disambiguating information for entities in the field of Syriac studies:  persons,  places,  works, and  bibliography. This system of URIs facilitates the production of a data graph out of the discrete pieces of prosopographical data contained in each factoid; the data about a particular person, the references to a particular location, or the persons with a particular occupation. SPEAR uses the relationship  ontology produced by Standard for Networking Ancient Prosopographies ( SNAP) when possible. Modern ontologies generally do not accommodate important ancient social relationships, such as those constituted by slaveholding. Employing SNAP relationships enables SPEAR to encode the relationships present in the source base according to a standard used by related projects likely to be interested in the data produced by SPEAR.  Transforming personal information gleaned from ancient textual historical sources into machine-readable and queryable data has required the development of a robust controlled vocabulary for systematically describing personal information and events. Out of a desire to use a vocabulary familiar to scholars in the field, SPEAR modified and expanded the keyword list used by the Comprehensive Bibliography on Syriac Christianity ( CBSC), a list that has evolved over almost fifty years to describe scholarly work in the field of Syriac studies (Simpson and Brown, 2013). Though in an early stage of development, Syriaca.org has encoded the  Syriac Taxonomy in TEI, assigned URIs to each term, and provided minimal hierarchical structuring. This rich encoding and RDF serialization will facilitate faceted browse and search by multiple fields. It will also allow the display of links between individual factoids and scholarly bibliography described with the same controlled vocabulary.    From factoids to text SPEAR’s use of LOD facilitates another important aspect of the project, the ability for users to return to the primary sources from which factoids have been derived. SPEAR bibliographic references use the system of  DTS:URNs developed by the Homer Multitext Project to cite specific portions of a text. A shared URN standard is also employed by the  Digital Syriac Corpus, a partner project offering TEI encoded Syriac texts. Each factoid page containing a URN in the bibliography displays the portion of text corresponding to the reference. The page also includes a link to take users to the full Corpus text as well.     A factoid viewed in HTML   Conclusion SPEAR shows how a prosopography project can employ TEI, field-specific scholarly standards, and Linked Open Data to produce a highly structured and semantically rich database that maintains close ties to the texts from which it is derived. The work speaks to recent discussions on the need for TEI to engage more fully with semantic web technology (Ciotti and Tomasi, 2016). Future developments for this project include: encoding data from additional texts, developing the current taxonomy into a structured domain ontology for use by the field of Syriac studies more broadly, exploring different approaches to data visualization, and developing protocols for serializing this data into the RDF standards of SNAP, the Advanced Research Consortium ( ARC), and  CIDOC-CRM (Pasin and Bradley, 2015).   ",
        "article_title": "Syriac Persons, Events, and Relations: A Linked Open Factoid-based Prosopography",
        "authors": [
            {
                "given": "Daniel L.",
                "family": "Schwartz",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "near eastern studies",
            "standards and interoperability",
            "English",
            "text encoding and markup languages",
            "prosopography",
            "semantic web and linked data",
            "medieval studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As cultural historians, should our responsibility be to the people in our historical data set or to our methodology? Is it possible to, as they say, have it both ways, and if so, what do Digital Humanities methods offer us as we seek to responsibly represent political history? In digitizing and digitally remixing a primary source data, should we value data collection consistency or value recovering information that the original methodology could not capture? We plan to report on the data collecting practices of Lesbian and Gay Liberation in Canada, a Canadian history project. As Stan Ruecker and Stéfan Sinclair have argued, the work of the humanities consists of adding value to cultural artifacts through interpretation and analysis (Sinclair et al.), but what is the most sound way to gather together the materials that will permit the creation of this added value? The best practices may lie in our collective past. It has been 10 years since Tom Scheinfeldt posted his pivotal blog post “Sunset for Ideology, Sunrise for Methodology?,” reminding us that that “late 19th and early 20th century scholarship was dominated...by methodological refinement and disciplinary consolidation...Serious scholarship was concerned as much with organizing knowledge as it was with framing knowledge in an ideological construct” (Scheinfeldt). Digital humanities scholars have answered Scheinfeldt’s call: the field has revived the great 19th century work in bibliography, prosopography, chronology, philology and lexicography. We want to move further and respond to Elijah Meeks’ call to expand our explicit discussion of the methods and methodologies we use to collect and organize the data we augment through interpretation and analysis (Meeks). The Lesbian and Gay Liberation in Canada (LGLC) project is comprised of 34,000 records about the people, places, events, and organizations of the gay liberation movement from the formation of the first homophile group at the country’s largest university in 1964 to the start of the AIDS crisis in 1981. The project builds on Donald McLeod’s monographs , Lesbian and Gay Liberation In Canada: A Selected Annotated Chronology (1964-1975 and 1976-1981). The base text consists of event records organized by date and then by location, with each summarizing a moment in liberation history. We have augmented the text both through digitization and the addition of data from archival sources and oral history The chronology text was encoded in TEI, to identify (and occasionally disambiguate) events, people, places, and dates. The TEI was then converted via XSLT into Cypher, the language that underpins Neo4j, a popular graph database. In addition to facilitating our own analysis of the data, the database underpins our node-based public history web app at lglc.ca. We are starting the second of the project: direct data augmentation.  The base text that underpins our project is a chronology that was amassed primarily through archival research conducted in the periodical studies tradition. This method left gaps in the data, since periodicals tend to favour those groups newsworthy enough to be covered in the periodical press; wealthy enough to advertise in the periodical press; and situated close enough to the press to benefit from print advertisements. Sensing that there were voices on the margins, racially, socio-economically, and geographically, missing from the periodicals-only collection method, we have devised an experiment to decide on which method will best help us fill these gaps. We know that oral history has fruitfully been used in the field (Nyhan et al.; Chenier), so we have assigned one of three potential methods to three research assistants: The first assistant is using library authorities paired with secondary sources to create a prosopography, or collective biography, of the over 3,500 people mentioned in our chronology in order to find statistical norms within the political movement and to map activist relationships. This will help us ascertain where the likely gaps are in our periodicals-only based representation of the political movement. The second assistant is continuing with the periodicals and archives-only method, but turning to periodicals that were dedicated to documenting the concerns and activities of groups currently marginalized in our base text;  The third assistant is conducting oral testimony interviews to capture events not covered in our chronology (and for the moment is verifying them via secondary print sources). In this short-presentation, we report back on the findings of this research into methodological best practice and will demonstrate the affordances of the alpha version of our public history site.  As we are entering the augmentation phase of the project, we are keen to get feedback and best practice recommendations from the ADHO membership.  ",
        "article_title": "Where Our Responsibilities Lie: People, Method, and Digital Cultural History",
        "authors": [
            {
                "given": "Michelle",
                "family": "Schwartz",
                "affiliation": [
                    {
                        "original_name": "Ryerson University, Canada",
                        "normalized_name": "Ryerson University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05g13zd79",
                            "GRID": "grid.68312.3e"
                        }
                    }
                ]
            },
            {
                "given": "Constance",
                "family": "Crompton",
                "affiliation": [
                    {
                        "original_name": "University of Ottawa, Canada",
                        "normalized_name": "University of Ottawa",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03c4mmv16",
                            "GRID": "grid.28046.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "digital humanities (history",
            "theory and methodology)",
            "English",
            "prosopography",
            "text encoding and markup languages",
            "ontologies and knowledge representation",
            "diversity",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The   Meditationes  is the scientific notebook of the mathematician Jacob I Bernoulli (1654–1705), a member of the Bernoulli dynasty from Basel. The notebook consists of 367 pages; more than 90% of the 287 entries deal with issues in mathematics and physics. Parts of the  Meditationes have been published in six volumes over the past decades according to rather varying standards.  Our project will provide a complete edition of the manuscript for the first time, providing facsimiles, transcriptions (from diplomatic to normalized texts), translations, and comments. The edition is part of   Bernoulli-Euler Online  (BEOL), a platform for early modern mathematical texts. BEOL makes texts available to historians of science and will provide tools for working with the resources available on the platform.    Transcription of the Meditationes  Regions and normalized text The transcription of the  Meditationes is based on the digital facsimiles of the manuscript. For each entry – a Meditatio –, regions are defined on the manuscript page and transcribed individually. Regions are categorized as title regions containing the entry’s number, text regions (mainly philological text), mathematical notation regions (mainly equations), and mathematical figure regions. Regions can be connected by cross references.    Figure 1: Part of Meditatio 165  Figure 1 shows a part of Meditatio 165. Two regions are shown (marked in green). The region at the bottom of the page (M165-07-T) contains a cross reference sign “#” (marked in red) that reoccurs in the region in the page’s margin (M165-08-TN). This means that the text in the margin belongs to a specific place in “M165-07-T”. When presenting the transcriptions, we offer a multi-layer graphical user interface (GUI). The diplomatic layer shows the single regions aligned with their transcriptions. When specifying a region in the GUI, the associated transcription is shown to the user. The next layer shows a text that is easier to read with the margin region directly integrated into the text of region “M165-07-T”. The normalized text reflects the Meditatio’s logical structure: it is a sequence of paragraphs that can span regions and pages. The example below shows the normalized text that combines the transcription of “M165-07-T” and “M165-08-TN” (indicated by curly braces): [...] hoc est, { quia    d d x     .   d x   : :   d   y   3       .   I n t .   d   y   3    , erit [...] proportional‹es› ‹?› }    d d x     .   d x   : :   d   y   3       .   I n t .   d   y   3     & quia etiam [...]  Regions are geometrically defined and shown as overlays. The regions can be converted to IIIF image URLs. The International Image Interoperability Framework (IIIF) defines a URL-based syntax for accessing images, allowing for interoperability among image repositories. (Appleby et al.) With IIIF, regions can be easily viewed as image files, defining their format, resolution, and even rotation. Rotation is useful when text has been added vertically in a margin, as in “M165-08-TN”, which is easier to read when it is rotated 270 degrees clockwise.   Philological and mathematical text The transcription of the Meditationes is based on two markup systems: XML for the philological aspects of the text, and LaTeX for mathematical notation. LaTeX is the input format preferred by the specialists working on the project, and can be automatically converted into MathML if necessary. To represent both the mathematical and philological aspects of the text, LaTeX mathematical notation is embedded in the XML-based transcription.   The XML transcriptions can be transformed to TEI/XML by means of XSL transformations.  In some cases, the surrounding context has to be taken into account when rendering the LaTeX, e.g., if text is underlined or struck through. This can be achieved by dynamic macro insertion to avoid redundancy in the transcription source files.    Figure 2: Title Text of Meditatio 54  Figure 2 shows the title text of Meditatio 54. In the manuscript, philological text and mathematical notation are not differentiated: both kinds of text are underlined. The excerpt below shows a part of the transcription source. When transforming the XML to HTML using an XSL transformation, macros are inserted dynamically if needed. In case of “abc”, the macro “\\underline{abc}” is inserted dynamically. <hi rend=\"underline\">In <choice><abbr>ang<am>.</am></abbr><expan>ang<ex>ulo</ex></expan></choice> <formula notation=\"tex\">abc</formula>. ductâ obliquè <lb/> ... </hi> Philological text can be converted to HTML for rendering in a web browser. The   MathJax  library is used to render mathematical notation. MathJax generates HTML and CSS from LaTeX or MathML, and can be instructed to render mathematical notation as underlined or struck through.  When using LaTeX macros directly in the source, custom macros are used that are replaced when transforming the XML source code to HTML. This way, the usage of MathJax-specific macros can be avoided, making it possible to use other rendering environments, such as a LaTeX-based typesetting system for print output. We have found   Pandoc  to be a useful tool for changing the implementation of a macro. The XML transcription files are preprocessed: for each formula, the LaTeX is passed to Pandoc, which replaces custom macros with the implementation to be used in the production environment.   https://pandoc.org/MANUAL.html#latex-macros      Translations For each Meditatio, translations in English, German, and French are available. Translations mainly refer to the normalized text, and can be aligned with the regions by their IDs. To avoid redundancy, complex formulae are referenced rather than re-encoded. In some cases, additional macros are needed for language support. The excerpt above shows LaTeX code for a formula referenced from the translation file. Depending on the target language, the macro “\\language{...}{...}{...}{...}” has to be replaced with the correct argument. In case of the English translations, the first argument is taken into account, in case of German the second and so forth. This can be accomplished by using Pandoc to preprocess the XML.    Integration into Bernoulli-Euler Online The transcriptions, facsimiles, translations, and comments of the  Meditationes will be imported into  Bernoulli-Euler Online (BEOL), a web-based platform for studying early modern mathematics. The platform already contains correspondence (currently about 2000 letters) relating to members of the Bernoulli dynasty and Leonhard Euler. The material in BEOL has been imported from existing editions, both print and digital. For BEOL, a comprehensive index of persons and bibliographical items had to be created. This index makes it possible to search for persons and bibliographical items in a global manner, overcoming the previous divisions into different parts of the edition. (Schweizer et al., 2017; Alassi et al., 2018) BEOL will serve as a single entry point for historians of science interested in the works of members of the Bernoulli dynasty and Leonhard Euler.  The BEOL platform is based on  Knora, a generic framework for storing, sharing, and working on primary sources and data in the humanities. All operations (reading, creating, changing of data) are performed through calls to the Knora API over HTTP. This API will enable clients to integrate the material available on BEOL into other platforms and vice versa. BEOL contributes to the development of Knora. Project-specific requirements are generalized so they can be implemented as generic components. These components can then be used by projects other than BEOL.   ",
        "article_title": "An Interactive, Multi-layer Edition Of Jacob Bernoulli's Scientific Notebook Meditationes As Part Of Bernoulli-Euler Online",
        "authors": [
            {
                "given": "Tobias Julius",
                "family": "Schweizer",
                "affiliation": [
                    {
                        "original_name": "University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Sepideh",
                "family": "Alassi",
                "affiliation": [
                    {
                        "original_name": "University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Mattmüller",
                "affiliation": [
                    {
                        "original_name": "University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Rosenthaler",
                "affiliation": [
                    {
                        "original_name": "University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Helmut",
                "family": "Harbrecht",
                "affiliation": [
                    {
                        "original_name": "University of Basel, Switzerland",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-10",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "linking and annotation",
            "manuscripts description and representation",
            "theory and methodology)",
            "English",
            "history of science"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The discipline of genetic criticism regards text as a dynamic rather than static object and tries to “put the text back into motion, opening it to the moving constellations that presided its genesis” (Contat et al. 1996, 2). Consequently, such a dynamic perspective implies an equally dynamic model of representation.  A traditional danger of manuscript research is that the researcher gets lost in the details of the archival material. Over the last decades, numerous editing projects explored the potential to capture text-genetic processes digitally. Most of these digital editions and archives, such as WoolfOnline, the Jane Austen Fiction Manuscripts, the Shelley-Godwin Archive, present the textual genesis in two ways. First by arranging the extant source documents in a stemmatologically established order, giving the reader an insight into the chronological document succession from a first note or draft to a first edition. Secondly, the individual documents are transcribed and critically annotated in rich detail, giving the user an insight into what each extant version looked like on paper. In addition to these features, some editions also integrate extra tools, such as a writer’s correspondence or an author’s own reading in the form of a digital or virtually reconstructed library.  What all these projects have in common is a hierarchical data model. Documents are stored in collections (folders and subfolders) which could well be visualized as directed rooted tree-graphs. In such a tree the edition as the overarching structure represents the root element, (sub-)collections represent its child elements and the individual documents are descendants. Below the document level, the tree continues in strictly hierarchical TEI encodings that lead us down to the smallest annotated unit of a document – a phrase, a word, sometimes a letter – nested in XML brackets. Up to this point, in terms of data structures, the edition can be visualized as a single coherent tree structure. There are three common practices to describe the textual genesis against the background of this hierarchical representation of the material.   Stemmatological metadata describe relationships that break this strict tree structure. The documents are arranged in a chronological sequence that might well vary from the documents’ physical order in the collections as represented by the tree. Yet, these genetic relationships always link elements on the same level of the tree’s hierarchy – the document level. They neither allow to  zoom in on deeper levels of the tree nor to derive information on how text units on these finer levels of granularity are genetically interconnected.    Annotating the textual genesis within individual documents, e.g. with text-genetic TEI encodings (TEI Consortium 2011 §11), allows us to link nodes of the tree (here XML elements) across the hierarchical tree structure. Additions, deletions and substitutions are assigned to groups (tei:change elements) which are put into sequences (ordered and unordered tei:listChange elements) in the metadata. Just like the stemmatological metadata, these structures represent genetic paths that break the hierarchy, yet in this case they do not allow us to  zoom out along the tree or the stemmatological document relations. We cannot draw conclusions about how the sequential making of an individual version is connected to the textual genesis across multiple versions.    Collation software such as CollateX (Dekker and Middell 2011) and the upcoming HyperCollate (Bleeker et al. 2018) detects the textual variance between different text versions and models these differences in so-called variantGraphs. Collation software allows us to capture paths that represent the textual variance between the documents on the granularity level of the token –  across the tree hierarchy of an XML encoding. Without stipulating any genetic interpretation, these graphs raise questions such as “how was a sentence/phrase altered syntactically (semantically) between draft A and draft B?”. The graph does not give explicit answers. Instead, it neutrally visualizes the variant and invariant text tokens between a selection of versions. Collation is limited to capturing connections between the documents, yet not on the hierarchical level of the document, but on the level of the token, which may well be smaller than any TEI annotation (on the level of XML text nodes). Again, this approach does not allow us to  zoom out. We cannot derive information from the stemmatological order (1), nor from the witness-specific genesis (2).   In all three cases, additional graph-structures are annotated across to the underlying hierarchical tree, which is itself a graph. Each one of these structures provides an alternative navigation for a particular level or subtree of the work and thus each of these structures represents a different aspect of the work’s textual genesis. Only very few projects, such as the Faust Edition and the Beckett Digital Manuscript Project, incorporate all three structures and even those projects have not managed to merge them in a way that allows the user to seamlessly navigate over all genetic information.  What is missing is a comprehensive model that allows to navigate seamlessly between the different types of genetic paths, to zoom in and out on writing processes (between the  macrogenetic and  microgenetic levels) and to connect external source texts to their use in the drafts (linking ‘ exogenesis’ and ‘ endogenesis’). Ideally, we should be able to implement this model in an easily accessible and extensible research environment, allowing the scholar to capture, organize, visualize and analyze genetic paths of all described types.  Building on the system of genetic paths as developed in HyperLearn (D’Iorio 2003, Barbera 2005) the proposed paper presents a digital way of modelling text-genetic relationships in an eXist-db based research environment for genetic criticism, henceforth referred to as a Manuscript Web (MW). Such an MW in the form of a customizable web application allows textual scholars to organize their document-collections consisting of facsimiles, TEI transcripts and bibliographical metadata, in four different module types: (1) virtual libraries, (2) collections of notes, (3) drafts and (4) published editions. A Manuscript Web also starts from a project tree, but unlike the projects described above it enables users not only to capture genetic paths across the given tree hierarchy, but to search the respective modules to which they belong and to store relationships (source-to-target vectors) between all identifiable elements/hierarchical levels of the project tree (that is modules, collection-folders, document-entities, XML elements and text nodes). The model thus enables users  to zoom in and out between  macro- and micro-genetic levels, as well as between  exo- and endogenesis.  For example, to link endo- with exogenesis, a scholar may connect an entire section of a TEI-encoded notebook to an identified source in the author’s virtual library to indicate that this notebook section contains reading notes from the related source. On a microgenetic level, individual notes from this section may be linked to paragraphs, phrases or interlinear additions in a manuscript draft. On a ‘higher’ level in the hierarchy level (macrogenesis), this draft may be linked to the following draft in the stemmatological sequence. Such one-to-one relations can be captured individually and regardless of the granularity level. Where the source- and target-references of independent relations overlap, they form paths and genetic graphs across the corpus. Since all these graphs refer to elements of the underlying project tree, this tree provides a navigational backbone that allows the user to zoom in and out on the genetic information. From any document within the environment the user can access all genetically related entities to answer questions such as “what does this particular paragraph look like in the next draft?” or “which literary sources inspired this paragraph?”. The aim of the proposed model is to enable users to connect what is usually merely juxtaposed. Most digital archives and scholarly editions offer the traces of a work’s genesis as digitized items, side by side. What this paper proposes is a way to enable not only scholarly editors, but also users to discover and record the connections between these textual traces. The ability to record these connections facilitates a more comprehensive understanding of a work’s genesis. “Put[ting] the text back into motion,” as Contat et al. described it (1996, 2), implies a dynamic model that allows users to turn the different genetic traces or “stills” – so to speak – into the “motion picture” of the genesis. With the proposed model, zooming in on the smallest level of textual change no longer entails the danger of getting lost in the labyrinth of the digital archive thanks to the possibility to zoom out again at every stage in the enquiry and see the bigger picture.  Finally, the paper shows how this dynamic model facilitates not only research into one single work’s genesis, but also comparative genetic criticism of several authors’ works. Up till now, comparative studies have been relatively rare in the field of genetic criticism, because every author’s writing method is characterized by idiosyncrasies. By modelling the text-genetic data in such a way that they become more comparable, the proposed model will contribute to the development of comparative genetic criticism. ",
        "article_title": "Modelling Text-Genetic Relationships",
        "authors": [
            {
                "given": "Dirk",
                "family": "Van Hulle",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Joshua",
                "family": "Schäuble",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "corpus and text analysis",
            "digital textualities and hypertext",
            "manuscripts description and representation",
            "scholarly editing",
            "English",
            "philology",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Information about places whose locations are not easy to identify with certainty and whose names may vary because of cultural and historical contexts are of great importance to historians. Places of cultural meaning or administrative units meet the needs of historians, rather than physiographic landforms on which many existing digital gazetteers and data models focus (Southall et al., 2011). Al-Ṯurayyā  The name al-Ṯurayyā, “Pleiades” in Arabic, is a tribute to Pleiades Gazetteer ( ), which was the main source of inspiration at the early stage of development of this project. The previous version is developed at Tufts University ( ).    provides an extensive gazetteer of the early Islamic Empire with over 2,000 toponyms and almost as many route sections from Georgette Cornu’s  Atlas (Cornu, 1985)  It covers from western provinces in Spain and North Africa to Eastern province, Sind (modern Pakistan). —where the primary attribute of collected objects are their geographical coordinates and their place in the Empire’s administrative hierarchy. Beyond the gazetteer, al-Ṯurayyā implements a spatial model that visualizes settlements, routes, itineraries, regions, and networks; additionally, it can perform specific queries that are meant to help to analyze specific historical events and phenomena through resulting visualizations. Following the idea of  Linked Data (Bizer et al., 2009), al-Ṯurayyā is designed to be connected to the primary and external sources. Its data model takes into account connectivity, spatial relations, and the additional evidence provided by the historical context from the primary sources. Al-Ṯurayyā can be used with other data sets of the same structure thus acting as a gazetteer and a geospatial model for different historical contexts.   The minimum components of the gazetteer entry is defined by: (1) settlements—currently with toponyms in English transliteration and Arabic; (2) geographic location—latitude, longitude; (3) administrative classification of settlements— metropol,  town,  village, etc., which present the administrative organization of the Empire.    Places: The gazetteer provides a search panel to find and visualize a place and access the relevant information that the data model provides. As in Figure   1 , searching for Baghdad in both Arabic and transliterated Latin characters yields a list of matches and highlights the position of a selected match in red on the map. The type of a settlement is represented in the visualization of places by different sizes of circles, which are assigned according to their type.  According to al-Ṯurayyā data model, the toponymic data is linked to contextual information including the records from primary sources  The current version provides the descriptions only from: Abū ʿAbd Allãh al-Ḥimyarī.  Rawḍ al-miʿṭār fī ḫabar al-aqṭār. Ed. by Iḥsān ʿAbbās. 2nd edition. Bayrūt, 1980 .    and links to secondary sources, which is shown together with the technical details when a toponym is selected (Figures   2 ,   3 ,   4 ,   5 ).  The descriptions from the primary sources are matched automatically and the percentage value in parenthesis next to each record indicates the match certainty (Figures   3 ,   4 ).        Figure 1. Toponym search      Figure 2. Technical information on a selected place—coordinates, URI, region, sources, type, names, etc.      Figure 3. List of records from primary sources relevant to a selected toponym       Figure 4. Detailed description of a selected toponym from primary sources       Figure 5. References to external sources  Routes and Itineraries: The spatial model of al-Ṯurayyā currently offers two main modules that compute and visualize routes and itineraries of various complexity, using pathfinding algorithms, and networks of reachability from the selected center(s), using the network on of settlements places and routes (al-Ṯurayyā also provides information of individual route sections—Figure   6 ).       Figure 6 - Selected route section information Relevant features of the geospatial model are as follows:   Pathfinding: The model computes paths between a source and a destination, the shortest and “optimal” routes. The shortest path implements Dijkstra’s algorithm (Dijkstra, 1959) while the “optimal” path computes the next shortest path with the higher number of waystations along the way (under the assumption that such routes are safer to travel). For example, in the shortest path (red) in Figure   7 , the sparsity of waystations makes the path dangerous across the Syrian desert while the optimal one (green)—including higher number of stations—leads around the desert through the populated regions.   Itineraries:  One can plot an itinerary by selecting stops (maximum ten) along the way to map more specific routes. This feature customizes the pathfinding computation model by considering the places that should be included in the itinerary. Figure   8  plots  Nāṣer-e ’s Khosraw’s itinerary from Nishapur/Naysābūr to Cairo/Fusṭāṭ, as described in h is own travelogue titled The Book of Travel (Thackston, 1986). This model suggests not mentioned locations that he might have visited.    Network Flood:  To represent reachability, al-Ṯurayyā introduces a method to model the network of settlements that are reachable from a starting point within a certain number of days of travel (one day equals about 30 km). Network flooding shows the reachability as well as limitations of the reach from a selected center, which in historical contexts are of great value to find answer for questions related to spread of power, explore the viable geographical limits of a state with the seat of power in a given center, and visually measure the prominence of specific urban centers. In the context of this model, we represent network flood by coloring the places on the map based on their distance from one/multiple center(s) that the user dynamically chooses. Each color represent a network of places reachable within the same day(s) of travel. For example, Figure  9  depicts  Marw al-Šāhiǧān’s network within ten days in which locations in red, orange, yellow, and green are  reachable within ten, twenty, thirty, and fifty days respectively and places in pale colors are unreachable according to the underlying route network and criteria. Network of multiple centers represent the same reachability concept from multiple centers, which can be used to represent itinerary courts (Figure   10  ).       Figure 7. Shortest (red) and optimal (green) paths from Baġdād to Dimašq (Damascus)      Figure 8. Plotting the itinerary from Naysābūr to Fusṭāṭ from Nāṣer-e Khosraw’s The Book of Travel  Regions: Al-Ṯurayyā models and visualizes regions, using the underlying network. The initial view makes the complete view of the whole area that the data covers and the overlap of the colored points properly shows the shape, density, and the extent of each province in the period in question, avoiding the modern idea of “borderlines”. In the “Regions” panel, the visualization highlights the selected region depicting its geographical position and administrative extent with all its settlements and routes.        Figure 9 - Network flood wit h Marw al-Šāhiǧān as a center       Figure 10. Network flood of two centers      Figure 11. Modeling a region  Al-Ṯurayyā is designed to serve as a starting point for the visual analysis of spatial data in written documents, and as a tool for answering meaningful, complex research questions about how the geography of premodern empires was shaped and conceptualized. For future developments we are planning to provide tools for data verification and contextualization. ",
        "article_title": " Al-Ṯurayyā, the Gazetteer and the Geospatial Model of the Early Islamic World  ",
        "authors": [
            {
                "given": "Masoumeh",
                "family": "Seydi",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Maxim",
                "family": "Romanov",
                "affiliation": [
                    {
                        "original_name": "University of Vienna, Austria",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "software design and development",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "English",
            "network analysis and graphs theory",
            "computer science and informatics",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Observing poetic similarity is fundamental for identifying interrelationships and poetic influences among authors. When investigating poetic similarity, intertextuality is always regarded as the most significant factor (Fowler, 1997), and it is also an index that can be calculated computationally (Coffee et al, 2018; Büchler et al., 2018 forthcoming). In terms of verse, homogeneous formal elements like rhyme (Hollander, 1981), the basic rhythmic structure, and meter (Boomsliter et al., 1973) among poems can also act as crucial indicators of poetic similarity. The goal of this research is to design a framework to quantitatively measure poetic similarity with digital methods, which can dig into a vast number of data and suggest the interrelationship of different authors’ works. This research focuses on the world-renounced Irish poet William Butler Yeats (1865-1939). The complex poetic influence he received deserves scrutinized investigations and the possible influence of the English Romantic poets is examined here. In this study, “influence” refers to the shaping power of a precursor poet on a later poet’s poetic style and poetic genre, which can be traced and observed. Specifically, a model is constructed to compare Yeats and the English Romantic poets, exploring their similarities in three aspects: (1) intertextuality; (2) formal elements, including rhyme, meter, and enjambment; (3) sentiment.   Methods     Fig. 1. Framework of model  As Fig. 1 shows, the model includes a preprocessing stage and four parallel quantifications: intertextuality calculation, rhyme and meter detection, enjambment calculation and sentiment analysis. The raw data are crawled from the Bartleby collection (https://www.bartleby.com/verse), including 130 works of Yeats from three collections, 216 works of Blake, 119 works of Byron, 53 works of Keats and 469 works of Wordsworth.  For intertextuality, after tokenization, lemmatization, and filtering stop words (words are lemmatized since it is unnecessary to distinguish different forms of the same word, and stop words are filtered out so as prevent function words that have little lexical meaning from being chosen), considering the remaining lists of words generated from Yeats’s works as target and those generated from the English Romantic poets as source, we compare them in turn in the unit of phrases, a segment of text demarcated by a semicolon or a colon. All source-target phrase-pairs that share at least two distinct words are recorded. Next, each recorded phrase-pair is weighted according to the following formula (Forstall et al., 2014):     Here, f(t) and f(s) are the frequency of each matching word in its target and source phrase divided by the length of the phrase respectively, and dt and ds are the distance of the farthest matching word (the number of words between two matching words with the largest distance among all matching words) in their target and source phrase. Phrase-pairs with words of lower frequency and those with closer distance are privileged because these indicate stronger possibility of intertextuality, which is set as the summation of every phrase-pair’s score within them divided by the product of their lengths (number of phrases).    Finally, the rate of intertextuality of Yeats and each English Romantic poet is defined as the average value of each verse-pair with non-zero value. For formal elements, CMU Pronouncing Dictionary is exploited to identify syllables, stresses and rhyme words (http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=C+M+U+Dictionary). The results of the identification are recorded as strings and are compared to every standard rhyme type and meter style to calculate their Levenshtein distances (Levenshtein, 1966), and the rhyme and meter types of the target verse are guessed and defined accordingly. For enjambment, after line segmentation, the proportion of enjambments is calculated as the number of the lines divided by the number of the lines that contain “,”/”.”/“!”/“?” immediately before the line breaks. For sentiment analysis, Python library TextBlob (Loria, 2018) is used to define the emotional tendency of the verses. Each verse is inputted into the system, and a parameter ranging from -1 (totally negative sentiment) to 1 (totally positive sentiment) is outputted. The emotional tendency of each poet is calculated as the average of that parameter in all of his works.   Results  Table 1. Rates of intertextuality of Yeats and the English Romantic poets     Table 1 shows that the rate of intertextuality between Yeats and Blake is the highest. The results of significance test show that the difference between the rate of intertextuality of Yeats-Blake and Yeats-any other Romantic poet is statistically significant at a significance level of 10-6, which shows that the intertextuality between Yeats and Blake is remarkably higher than those between Yeats and the other English Romantic poets. Since a higher rate of intertextuality shows a stylistic rather than generic imitation (Conte, 1986), the results indicate that Blake may have exerted a stronger influence on Yeats’s poetry than the other poets studied.  Table 2. Formal elements of Yeats and the English Romantic poets (bold character is used to distinguish the closest percentage with that of Yeats)     Table 2 shows that the distribution of rhyme types in Wordsworth’s verses is relatively the most similar to that of Yeats, and he also has the closest proportion of enjambment with Yeats (at a significant level of 0.05, the difference between their enjambment proportion is not statistically significant). In terms of meter style, Yeats has a very similar distribution with Blake.  Table 3. Sentiment of Yeats (and his collections) and the English Romantic poets     *The abbreviations stand for names of the collections: The Wind among the Reeds, Responsibilities and Other Poems, The Wild Swans at Coole, respectively. After the sentiment analysis, two major findings are observed from Table 3: (1) Yeats has the smallest value, while Blake has the second smallest (at a significant level of 0.05, the difference between their sentiment value is not statistically significant, which is unique contrasting the value between Yeats and the other English Romantic poets); (2) Sentiment parameters of Yeats’s different collections ascend in a chronological order.   Conclusion This research successfully builds a model to quantitatively measure poetic similarity. The results show that Blake, among the English Romantic poets, is the most similar to Yeats both in terms of intertextuality and sentiment. With regard to formal elements, Yeats resembles both Blake and Wordsworth. This study’s possible contribution to Yeats scholarship is to quantitatively measure and prove the prominent influence of Blake on Yeats’ poetry, and concretely shows Yeats’ relationship with such movements as Romanticism. Furthermore, the framework designed by this research can be applied to investigate poetic similarity or intertextuality among other poets or poems, thus making contribution to literary studies in general. We believe that by the means of investigating massive data of poetic similarity, the influence of chanciness in literary interpretation can be substantially weakened. Digital methods can serve as powerful tools to detect latent literary attributes, raising significant topics that can inspire further studies.  ",
        "article_title": "Modelling Poetic Similarity: A Comparative Study of W. B. Yeats and the English Romantic Poets",
        "authors": [
            {
                "given": "Wenyi",
                "family": "Shang",
                "affiliation": [
                    {
                        "original_name": "Department of Information Management, Peking University, China",
                        "normalized_name": "Peking University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/02v51f717",
                            "GRID": "grid.11135.37"
                        }
                    }
                ]
            },
            {
                "given": "Jingzhou",
                "family": "Zhang",
                "affiliation": [
                    {
                        "original_name": "Deparment of Chinese Language and Literature, Peking University, China",
                        "normalized_name": "Peking University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/02v51f717",
                            "GRID": "grid.11135.37"
                        }
                    }
                ]
            },
            {
                "given": "Win-bin",
                "family": "Huang",
                "affiliation": [
                    {
                        "original_name": "Department of Information Management, Peking University, China",
                        "normalized_name": "Peking University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/02v51f717",
                            "GRID": "grid.11135.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "corpus and text analysis",
            "stylistics and stylometry",
            "data mining / text mining",
            "English",
            "literary studies",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper shows how the systematic and quantitative study of genre in large digitized newspaper collections sheds light on the development of journalism discourse. We argue that genre conventions that can be discerned in a newspaper text signal the underlying discursive norms and practices of journalism as a profession (Broersma, 2010). However, digital newspaper archives do not contain fine-grained genre information on the article level. As such, this paper adopts a machine learning approach to add genre labels to newspaper articles. Classifying genre in a standardized and reliable manner is challenging, though, because genre is a typical example of a latent content category, which needs considerable interpretation. To ensure the reliability of the results and to evaluate the machine learning approach, it is crucial to make the methodological impact of various machine learning pipelines transparent. This paper therefore discusses how the distribution of news genres developed over time and how transparent classification empowers journalism history researchers to benefit from the computational methods for doing large-scale content analysis.    Genre and journalistic discourse The grand narrative of journalism history, prevalent in journalism historiography, claims that journalism from the end of the 19 th century has moved away from partisan journalism and opinion-oriented reporting practices, towards critical and autonomous journalism that is event-centred and fact-based. For the Netherlands specifically, this journalistic development has taken place between the 1950s and the 1990s as part of the depillarization of society (Harbers, 2014). Since genre connects textual features to journalism’s underlying discursive norms and routines (Broersma, 2010), a historical analysis of newspaper genres can elucidate how journalism’s professional practice has developed over time. We focus particularly on modal genres, which are defined based according to their formal features, e.g. an interview. This definition has been dominant in how journalists and academics have defined journalistic genres in the Dutch context. We have formulated 16 classes of journalistic genres (see figure 1).      Figure 1 : The distribution of 16 news genres used in this study in a selection of newspaper articles of the Dutch newspaper NRC. Genres of interest to our study are represented by bright colors while other genres are colored grey. The large difference in genre frequencies caused the machine learner to focus on predicting frequent genres correctly while ignoring the others. In order to avoid this problem, we use a balanced training set in which all genres occur equally often    Transparent automatic genre classification Classifying the genres of newspaper articles retrieved from digital newspaper archives raises multiple conceptual and methodological challenges. Firstly, genres are prototypical constructs: some articles do not necessarily match the characteristics of the genre perfectly, nor can they always be neatly delineated from other genres. Furthermore, the distribution of genres over specific topics is skewed, e.g. reports are often about sport matches and interviews about human interest. However, topic is not the defining feature of modal genres and it may introduce unwanted bias in the automatic classification process. To make an informed decision on the right pipeline and understand its inherent biases, we have developed a dashboard that allows the scholar to explore the underlying decision-making process of the machine learning pipeline. On this platform, transparency is achieved through data visualisations that show the performance of the classifier per genre, by offering article-level and classifier-level explanations for the performance, as well as by providing comparison between various machine learning pipelines.   The workflow The platform supports data preprocessing and feature extraction, pipeline configuration, training and testing of the pipeline, pipeline evaluation and comparison, and hypothesis testing (Bilgin et al., 2018). Regarding the data pre-processing, we remove quotes from the documents using regular expressions, to avoid confusion between practices of sources and journalists (the latter being relevant to the genre distinctions). For the feature extraction in order to represent the data, we consider three methods: Bag-of-words (TF-IDF) features, manually curated linguistic features (extracted using a natural language processing toolkit named FROG), such as the number of sentences and the number of first person pronouns, and pooled features combining word features with linguistic features. As for pipeline configuration and training, the platform currently supports 7 machine learning algorithms: GradientBoost, LightGBM, Multi-Layer Perceptron, Naïve Bayes, Random Forest, Support Vector Machines and XGBoost.   Pipeline evaluation The machine learning pipelines are trained on a balanced data set containing 960 annotated articles from 9 different newspapers evenly spread over time, and across topic domains. Through an iterative process, which involves comparing pipelines that use different pre-processing settings in addition to various machine learning algorithms and assessing their underlying decision-making criteria given by interpretability tools, the best pipeline is chosen to perform the genre classification. We measure the performance by commonly used classification metrics such as precision, recall, and accuracy. The accuracy scores vary between 0.41 and 0.70 (compared to 77% inter-annotator agreement). This paper shows, however, that choosing a pipeline based on accuracy score alone does not necessarily result in choosing the best pipeline for our research purpose. Not all genres are equally difficult for a machine learning algorithm to distinguish. Service messages, such as television guides or stock market information, are particularly easy, for example. Yet, these are not the genres we are most interested in for our research questions, since these do not reflect the developing journalistic practices we are interested in. Pipelines that score higher accuracy scores because they are outperforming others on classifying these less relevant genres, are thus not the best choice for our purpose. More relevant are pipelines that are performing well on the genres we are most interested in. We thus need to evaluate the pipelines beyond their accuracy scores. Furthermore, looking into the explanations, we noticed that some pipelines tend to do topic classification instead of genre classification, as can be observed by the recurrence of topic-related words in the feature importance rankings for some genres. The predictive features for the decisions of other pipelines, however, do line up more closely with how genres are defined in journalism studies: first personal pronouns (“ik”, “me” and “mij” in Dutch) being predictive for an article to be a column, for example (see Figure 2). The pipelines that combine a bag-of-words approach with manually curated linguistic features give the most promising results. They are able to identify both distinctive words for certain genres - such as “to be continued” (“wordt vervolgd” in Dutch) for literary fiction - and the relevance of linguistic features - such as the number of adjectives which can be predictive for the reportage. Furthermore, these pipelines can recognize words that point at the direction of relevant (linguistic) features that had not been considered yet, contributing to the understanding of genre in journalism history by uncovering patterns that were not known before. Based on the pipeline evaluation and comparison, the preferred pipeline for our studies is chosen.      Figure 2 : Local feature explanation by LIME. On the left it shows the probability that the article is a particular genre. On the right it shows the bag of word and curated linguistic features of this article that have been most predictive in classifying it as column, and which have been suggesting that it is not a column. The features, especially the first personal pronouns (“ik” (I), “me” (me) and “mij” (me), line up with the journalism studies understanding of column as an article that is focused on the personal impressions of the author      Application of transparent automatic genre classification to journalism history In order to gain more insight into the development of genre in journalism history, we aim to gain insight in how the machine learning pipeline’s output compares to the distribution of the manually coded golden standard data. In other words, the goal is to observe if the machine learner can reproduce the genre distribution for the years for which we have gold standard data. From previous research, we have inherited a data set with approximately 10.000 manually annotated articles for two constructed weeks in 1965 and in 1985 for four newspapers:  Algemeen Handelsblad/ NRC Handelsblad,  De Telegraaf and  De Volkskrant. This data set is used as a golden standard for the distribution of genres in 1965 and 1985. The study compares the distribution of genres in this data set to the distribution of machine-labelled data. In this way, we test the trustworthiness of the results: does the distribution of the machine-labelled data correspond to the distribution noted in the gold standard data?  The future work for this study is to apply the most trusted machine learning pipeline to large-scale unlabelled data to comprehend the development of genre distribution between 1950 and 1995. The results of such work will shed light on the shift from opinion-oriented to fact-centred news in Dutch journalism, marking advances in Dutch media history.  ",
        "article_title": "Grounding Paradigmatic Shifts In Newspaper Reporting In Big Data. Analysing Journalism History By Using Transparent Automatic Genre Classification.",
        "authors": [
            {
                "given": "Kim",
                "family": "Smeenk",
                "affiliation": [
                    {
                        "original_name": "University of Groningen",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            },
            {
                "given": "Aysenur",
                "family": "Bilgin",
                "affiliation": [
                    {
                        "original_name": "CWI, Amsterdam",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tom",
                "family": "Klaver",
                "affiliation": [
                    {
                        "original_name": "Netherlands eScience Center",
                        "normalized_name": "Netherlands eScience Center",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/00rbjv475",
                            "GRID": "grid.454309.f"
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Tjong Kim Sang",
                "affiliation": [
                    {
                        "original_name": "Netherlands eScience Center",
                        "normalized_name": "Netherlands eScience Center",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/00rbjv475",
                            "GRID": "grid.454309.f"
                        }
                    }
                ]
            },
            {
                "given": "Laura",
                "family": "Hollink",
                "affiliation": [
                    {
                        "original_name": "CWI, Amsterdam",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jacco",
                "family": "van Ossenbruggen",
                "affiliation": [
                    {
                        "original_name": "CWI, Amsterdam",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Harbers",
                "affiliation": [
                    {
                        "original_name": "University of Groningen",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            },
            {
                "given": "Marcel",
                "family": "Broersma",
                "affiliation": [
                    {
                        "original_name": "University of Groningen",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "corpus and text analysis",
            "artificial intelligence and machine learning",
            "communication and media studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Critiques of literary representation  Literary studies has a long tradition of analysing texts from an ideological perspective. Inspired by feminist (Butler 1990), postcolonial (Said 1978) and Marxist (Eagleton 1976) strands of thinking, these so called critiques of literary representation have been focusing on hierarchies between genders, ethnicities, and classes in literary texts. One way in which these hierarchies can be traced is through comparatively analysing representations of characters with different demographic backgrounds. For the field of Dutch literature, a diverse range of detailed close readings have been conducted analysing the relative importance of certain represented identities as opposed to others (Pattynama 1994, Meijer 1996a, Meijer 1996b, Pattynama 1998, Minnaard 2010, Meijer 2011).   In recent years, quantitative methods such as social network analysis have made it possible to study character representation on a larger scale (Alberich e.a. 2002, Stiller et al 2003, Elson et al 2010, Lee & Yeung 2012, Karsdorp et al 2012, Agarwal et al 2013, Jayannavar et al 2015, Karsdorp et al. 2015, Lee & Wong 2016, Van der Deijl & Smeets 2018). Insights from e.g. network theory can lead to a broader understanding of the power dynamics between characters. Important aspects of these dynamics are positive (friends) and negative (enemies) relations between characters, as bonds and conflicts in networks are indicative of hierarchical oppositions between represented identities.   In order to gain an empirically informed understanding of character hierarchies in present-day Dutch literary fiction, the present paper models conflicts for all 2137 characters in a corpus of 170 novels that were submitted to one year (2012) of the Libris Literatuurprijs, one of the most prestigious literary prizes in the Dutch language area. It draws on extensive metadata from earlier research in which gender, descent, age, education and profession of all these characters were gathered (Van der Deijl, Pieterse, Prinse, Smeets 2016), as well on more recent research in which relational information (family, lover, colleague, friend, enemy) between these characters was collected (Volker & Smeets 2018).    Methodological design Social networks for each of the 170 novels are semi-automatically extracted using the co-occurrence approach described in Smeets & Sanders 2018. These networks are used to model conflicts in two ways, the first of which focuses on conflicts between two characters (dyads), the second on conflicts between three characters (triads).    Conflict scores  In earlier research (Smeets et al 2018), all characters were ranked with Python’s NetworkX library (Hagberg et al 2008) for five basic network centrality metrics: degree, betweenness, closeness, eigenvector, and Katz. Each of these rankings are an indication of a certain aspect of a character’s relative importance in the story. For every dyad of enemies in the corpus, it is detected who the higher ranked character is. For each of the five centrality metrics, a character’s   conflict score  is incremented by 1 in case he/she is higher ranked than his enemy.    Finally, a multiple linear regression analysis is carried out to test the extent to which a character’s gender, descent, age or education is a predictor of his/her conflict score. The outcome of the regression analysis serves as an indicator of which represented identities are the more powerful ones in the conflict.      Social (im)balance  The social balance theory (Heider 1946) postulates that there is  social balance  in a triad when either all three nodes are friends, or when two friends share the same enemy. Conversely, it postulates that there is  social imbalance  when all three nodes are enemies, or when two enemies share the same friend. This is used as a theoretical framework for modelling conflict dynamics between subnetworks of three characters in the corpus.   For every enemy/friend triad, it is automatically established whether it is socially balanced or imbalanced according to the theory. It turns out that the majority of triads, 69%, is socially balanced as opposed to 31% of socially imbalanced triads. Among these two general categories of social balance and imbalance, fully positive and fully negative triads are most present (see Figure 1 for the absolute distributions per type). In light of authoritative narratological theories (Propp 1928, Greimas 1966), the prevalence of social balance is remarkable, as conflict is commonly esteemed to be one of the driving forces behind narrative action.      For the analysis of conflicts in individual novels, this observed pattern can be used as a general framework to contextualise and evaluate the particularity of (im)balanced triadic subnetworks. One such a contextualisation will be demonstrated by evaluating a single triad in light of the overall pattern.         Figure 1. Absolute distribution of social (im)balance for all enemy/friend-triads in the corpus divided by type (N =1059)   Contribution to the field  In this paper the two models of conflict will be used to disentangle the complexities of power dynamics in character representation. We will assess the possibilities and challenges of our approach for critiques of literary representation that mainly use qualitative close reading methods. It will be argued that conflict situations co-shape the ideological representation of characters in literature, and the importance of a data-driven and empirically informed approach to character representation will be highlighted. .   Keywords:  conflict,  social network analysis, Digital Literary Studies, Dutch literature   ",
        "article_title": "Modelling Conflicts Between Characters in Present-Day Dutch Literary Fiction.",
        "authors": [
            {
                "given": "Roel",
                "family": "Smeets",
                "affiliation": [
                    {
                        "original_name": "Radboud University Nijmegen, Netherlands, The",
                        "normalized_name": "Radboud University Nijmegen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/016xsfp80",
                            "GRID": "grid.5590.9"
                        }
                    }
                ]
            },
            {
                "given": "Eric",
                "family": "Sanders",
                "affiliation": [
                    {
                        "original_name": "Radboud University Nijmegen, Netherlands, The",
                        "normalized_name": "Radboud University Nijmegen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/016xsfp80",
                            "GRID": "grid.5590.9"
                        }
                    }
                ]
            },
            {
                "given": "Antal",
                "family": "van den Bosch",
                "affiliation": [
                    {
                        "original_name": "Radboud University Nijmegen, Netherlands, The; KNAW Meertens Institute",
                        "normalized_name": "Radboud University Nijmegen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/016xsfp80",
                            "GRID": "grid.5590.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "English",
            "network analysis and graphs theory",
            "cultural analytics",
            "cultural studies",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The study of a historical language like Latin requires a corpus-linguistic perspective. Since we cannot appeal to native speakers of Ciceronian Latin, medieval Latin or pre-classical dialects known from early inscriptions, our knowledge of the language depends on the surviving documents we choose to study. Several excellent Latin morphological parsers already exist. (In addition to the list at  , note also LatMor   and Parsley  .) All of them are designed as comprehensive systems applicable to \"Latin\" generally, however. Some support adding new vocabulary items, but scholars and teachers studying texts with \"non-standard\" morphology or orthography have few options. \"Normalizing\" the text directly contradicts the corpus-linguistic mandate to understand the language as it is attested. Forking one of the existing open-source systems (whether Python, C, Java or a finite state transducer notation like LatMor's SFST-PL) will appeal to few Latinists.  This paper describes an alternate methodology. It differs from current approaches in two ways: by automating the building of parsers tailored to particular corpora, and by identifying all components of a parser's output with canonically citable URN values. While it would be possible to build a parser covering all known digital Latin texts, parsers targeted at the language and orthography of specific corpora can reduce the ambiguity of analyses to instances of true morphological ambiguity. In a corpus of Plautus, for example, the surface form \"anime\" can only be the vocative singular of \"animus\" (urn:cite2:hmt:ls:n2636 in the citable version of the Lewis-Short dictionary from Furman University). In a diplomatic edition of manuscripts of the Latin Psalms, \"e\" might represent the orthographic equivalent of classical \"ae\" so that \"anime\" could be genitive singular, dative singular, nominative plural or vocative plural of \"anima\" (urn:cite2:hmt:ls:n2612). A comprehensive morphological parser would have to accept all these possibilities for analyses of \"anime\". A classical Latin parser, on the other hand, could accept only \"ae\" as valid first-declension endings; the lexicon for a Latin parser of the Psalms does not need an entry for \"animus\", since that word does not appear in the Psalms, so the only ambiguity it would identify is the identical form of four case-number combinations of the first-declension noun \"anima\". By using URNs to identify all components of an analysis, we can readily combine analyses from multiple parsers. CITE2 URNs identify collections of discrete objects. (See   At the time of this writing, the CITE2 URN scheme is in expert review with the IETF's URN working group). Since we do not use string values to identify forms or lexical entities, when we analyze Plautus with a classical Latin parser and a diplomatic edition of the Psalms with a parser specific to that corpus, the analyses of each parser can identify the lexical item \"anima\" with the URN urn:cite2:hmt:ls:n2612. Our parsers can thus recognize that \"anime\" in the Psalms and \"animae\" in Plautus are equivalent forms of the same lexical entity, but the token \"anime\" represents different lexical entities in the two corpora.  This approach opens up new possibilities for research and pedagogy. For editors of diplomatic editions, automated morphological analysis is invaluable for validating manually edited texts, but it is only possible when the orthography, lexica of stems and inflectional rules can all be defined for the corpus being edited. Morphological data can enrich familiar analytical models such as social networks. Parsing of named entities is often limited because they are precisely the kind of vocabulary missing from standard lexica. If we construct a social network of persons appearing in the same passage and associate with each name its grammatical case, the resulting graph not only distinguishes clusters of co-occuring figures, but indicates what syntactic role they fulfill in relation to each other. The approach described here invites a beginning-language pedagogy preparing students to read a particular corpus. It is customary to analyze a digital text in order to determine what vocabulary should be stressed in introductory language courses. In deciding how best to sequence topics, we can also analyze the frequencies of forms and of specific inflectional rules. If supine forms are rare in our target corpus, we might choose not to emphasize them. But we can go further, and evaluate which particular inflectional classes should be emphasized. Does every variation of third-declensions i-stems appear in the corpus we're preparing students to read, or should we devote more time to other topics? In this approach, the central technological component is not a Latin parser, but an open-source system for building corpus-specific parsers. It is modelled in part on the Kanónes system for building Greek morphological parsers (described in  Bulletin of the Institute for Classical Studies 59-2, 2016, 89-109), but extends and generalizes some of its ideas. As with Kanónes, a digital humanist manages a set of structured text files. A build process managed with sbt ( ) reads the text files, translates them into source code in SFST-PL, and applies the Stuttgart Finite State Transducer tools to compile binary finite state transducers for working with Latin morphology. The package includes a suite of utility scripts in Scala that can be run from the sbt console and parse the SFST output into an object model. They include scripts to parse a wordlist, summarize a corpus morphologically, and export the morphological analyses to a tabular format suitable for use with tools such as a RDBMS.  Three data sets define the parser for a corpus. First, a plain text file defines the orthography by enumerating all Unicode codepoints allowed in parseable tokens. Second, a set of delimited-text tables defines a lexicon of \"stems,\" recorded in the defined orthography. Each stem is uniquely identified, and associated with a URN for a lexical entity, as well as an inflectional class (roughly corresponding to traditional inflectional classes such as \"2nd declension noun stem\"). Third, a further set of delimited-text tables defines the inflectional rules that apply to the corpus. The rule is uniquely identified, includes an \"ending\" recorded in the defined orthography, and is associated with one of the same inflectional classes used in the tables of stems. Sample data sets illustrating how to organize the data for a complete parser include diplomatic editions of Latin manuscripts, Latin texts digitized from print editions from the Tesserae Project, and a \"corpus\" comprising all paradigms in Allen and Greenough's  Latin Grammar.  Other than a text editor to create or modify the data files, the system has only two technical requirements (plus their dependencies): sbt (and therefore a Java SDK), and the SFST toolkit (and its required GNU \"make\" toolchain). Directly using the included scripts is the simplest way to analyze or export results, but the scripts in turn rely on a JVM code library imported by sbt that can be used with any JVM language (including Java, Groovy, Clojure, and Kotlin). DH projects that want to use the parser's output differently can use the code library to ingest the parser's output and have direct access to the data through high-level abstractions (such as a \"NounForm\", which includes a \"Gender\" property, which has an enumerated value of \"Masculine,\" \"Feminine\" or \"Neuter\"). While it is less likely that digital humanists will choose to expand on the set of included transducers, the organization of the SFST system supports this, too. The included transducers are chained in a standard design pattern: data transducer -> acceptor transducer -> analysis transducers The data transducer is the Cartesian product of all stems with all rules. The acceptor transducer filters these so that only combinations of rules and stems belonging to the same inflectional class remain. Analysis transducers suppress some categories of information to provide a mapping from an incomplete set of data to a full analysis. A final transducer that suppresses all analytical information and keeps only stem+ending strings therefore maps surface forms to a full analysis (i.e., it provides mappings like \"jacio -> first singular present indicative active of urn:cite2:hmt:ls:n25153\"). Alternatively, a transducer that suppresses all data except a URN for a lexical entity and symbols for person, number, tense, mood and voice provides mappings like \"first singular present indicative active of urn:cite2:hmt:ls:n25153 -> jacio\". The theme of DH2019 is \"complexities.\" The approach to morphological analysis presented here respects the complexity of Latin as it is attested in millenia of surviving documents. Managing simple text files to build corpus-specific parsers with analytical output identified by URNs, we can bring a more nuanced corpus-linguistic perspective to research and teaching with digital corpora of Latin. ",
        "article_title": " A Corpus-linguistic Approach to the Analysis of Latin Morphology  ",
        "authors": [
            {
                "given": "David Neel",
                "family": "Smith",
                "affiliation": [
                    {
                        "original_name": "College of the Holy Cross, United States of America",
                        "normalized_name": "College of the Holy Cross",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05dwp6855",
                            "GRID": "grid.254514.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-28",
        "keywords": [
            "corpus and text analysis",
            "scholarly editing",
            "linguistics",
            "natural language processing",
            "English",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The purpose of the GB1900 project was to transcribe all text strings appearing on the second edition of the Ordnance Survey’s County Edition six inch-to-one mile maps (1:10,650 scale) published between 1888 and 1914 and covering the whole of Great Britain. The only exclusions were primarily numerical data easily obtained from modern digital mapping, i.e. spot heights, the depths of lakes and distances on road signs. The scale of this task, and the limited effectiveness of optical character recognition when applied to text on maps, made crowd-sourcing the most appropriate methodology.   Figure 1: Excerpt from County Edition Six Inch map (blue dots mark GB1900 transcriptions)  The project grew out of Cymru1900, a collaboration between the National Library of Wales, the University of Wales Centre for Advanced Welsh and Celtic Studies, the Royal Commission on the Ancient and Historical Monuments of Wales and People’s Collection Wales, with funding from the Welsh Assembly. Cymru1900 launched in October 2013 and remained live until it was replaced by GB1900, but was much more successful at obtaining initial transcriptions than the matching confirmatory transcriptions required to finalize each string. GB1900 involved additional partners at the National Library of Scotland, supplying a digital map mosaic covering the whole of Great Britain, and the University of Portsmouth, providing additional development time to revise the software to encourage confirmations, then hosting the revised system. GB1900 launched in September 2016, incorporating all existing transcriptions and user registrations from Cymru1900, and was closed down at the end of January 2018, by when it was very hard to find new text to transcribe. The overall project history, sources and software is described in Southall et al (2017), while the crowd-sourcing process and the motivations of volunteers are explored in Aucott et al (2019).   The GB1900 Datasets Following some manual work to resolve c. 30,000 problematic transcriptions, final datasets were made available for download in July 2018, from Portsmouth’s web site  A Vision of Britain through Time:   http://www.visionofbritain.org.uk/data/#tabgb1900  Three distinct datasets were created. Firstly, a “raw dump” consisting of all the uncleaned tables from the MongoDB database behind GB1900, excluding only the table holding user registration details and including all the different transcriptions of each string; this is offered under a CC0 license, enabling anyone to do what they like with it. Secondly, the main “cleaned” dataset, containing just one “agreed” transcription of each of 2.55m. strings, together with OSGB and WGS84 geographical coordinates. Thirdly, an “abridged” version from which the commonest strings judged not be place names have been removed. The latter two datasets are offered under CC-BY-SA licenses. The abridged dataset is presented as a gazetteer, meaning an inventory of the names by which people identified particular places: towns, villages, hamlets, woods and so on. As such, it is possibly the most detailed gazetteer of Britain ever created, and certainly the most detailed specifically historical gazetteer. It is further described in Aucott and Southall (forthcoming). However, more than half the transcriptions are not place names but are still of very considerable interest, and our main focus here. The three most commonly occurring names in the abridged dataset are “Manor House” (1,617 occurrences), “Manor Farm” (1,496) and “Mount Pleasant” (838). Conversely, the most common unabridged terms are “F.P.”, meaning a footpath (306,583 occurrences), “W”, meaning a well (190,979) and “P”, meaning a pump (115,877). The original justification for including these in the transcription process was the difficulty of defining “place names” with enough precision and clarity to be really consistently applied by the volunteers.   Figure 2: Locations and types of windmill, from GB1900  The complete and abridged datasets are currently made available simply as CSV files, for easy uploading into databases or viewing in spreadsheets. Although we have written elsewhere about the importance of presenting gazetteers as linked data (Southall et al, 2011), the datasets consist primarily just of strings and an associated coordinate, plus the names of containing modern local authorities and parishes added via point in polygon look-ups, so these data are not linked. However, we are exploring how the GB1900 data may be linked to the DEEP gazetteer created from the Survey of English Place Names (Ell et al, 2016), which has a complex semantic structure but currently contains locations for only 4.4 per cent of the included places.   Capturing geography in words In some senses, the County Series maps are not large scale maps but small scale plans: they contain no symbology or key, just the outlines of buildings and other features; and a great deal of text. As a result, transcribing the text, and recording the locations where it appears, captures most of the maps’ meaning, their semantic content. Capturing all those detailed outlines from the maps would not greatly extend our geographical knowledge, while greatly complicating both the transcription software and the volunteers’ task. It would however have been desirable to capture the fonts and sizes used for each text string, as that does incorporate some additional meaning.   Figure 3: Chalk Pits in south east England, from GB1900  We can learn much about the detailed geography of Britain’s people over the last two centuries from the census. However, sources for the broader study of the evolving cultural landscape are more limited. We have worked extensively with the Land Utilisation Survey of Great Britain from the 1930s, but this provides only a very broad brush overview of land use, and little about what makes particular places special (Baily et al, 2011). Figures 2 and 3 provide two different examples from GB1900 data of how humans both used and shaped the landscape, making particular places distinct. Figure 2 shows how windmills were a common feature of the flat landscapes of eastern England, and give the lie to those who see modern windfarms as a new intrusion. Figure 3 of course partly just shows the distribution of chalk in the underlying geology, but the concentrations, especially into the Hampshire-Surrey borders, also tell us something of agricultural improvement.   How GB1900 is being used The data are already being used by project partners. The National Library of Scotland have implemented a search facility using it on their open access online map collection for their six inch to the mile maps. Searching by place names from GB1900 finds the location on the six inch map and the visitor can then choose to view another map for the same location:  https://maps.nls.uk/geo/explore/  The Royal Commission on the Ancient and Historical Monuments of Wales have a  List of Historic Place Names in Wales which must be consulted for all new developments in Wales. Currently using data from Cymru1900, it will be updated to include GB1900:   https://historicplacenames.rcahmw.gov.uk/  The authors are incorporating a GB1900 search facility into the Vision of Britain system. This links GB1900 data into the existing historical information structure and for the first time will offer greater detail for places within parishes:  http://www.visionofbritain.org.uk/expertsearch#gb1900/  Researchers independent of the project have also been experimenting with the dataset, including Jim Clifford, a historian at the University of Saskatchewan who looked at breweries, distilleries and pubs:  GB1900 Works Website  Joe Rose (B.C. Canada) has been working on industrial sites and has produced a 33 volume listing of Scottish mills, linking them to other locational information. His dataset is being used by a current Glasgow PhD student. He is also working on a desktop application. This link shows an early extraction of quarries:  https://geo.nls.uk/maps/gb1900quarry/    Conclusion The County Series maps are the largest scale at which the Ordnance Survey mapped the whole of Great Britain, larger scales existing for towns and for more densely populated rural areas. Most of their meaning is in the text that GB1900 has extracted, and while it would be hard to claim this is a literary work, it is a remarkably detailed record of both physical and cultural landscapes, which we can now present in summary form.  ",
        "article_title": "Capturing the Geography of 1900s Britain as Text: Findings from the GB1900 Crowd-Sourced Gazetteer Project",
        "authors": [
            {
                "given": "Humphrey",
                "family": "Southall",
                "affiliation": [
                    {
                        "original_name": "University of Portsmouth, United Kingdom",
                        "normalized_name": "University of Portsmouth",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03ykbk197",
                            "GRID": "grid.4701.2"
                        }
                    }
                ]
            },
            {
                "given": "Paula",
                "family": "Aucott",
                "affiliation": [
                    {
                        "original_name": "University of Portsmouth, United Kingdom",
                        "normalized_name": "University of Portsmouth",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03ykbk197",
                            "GRID": "grid.4701.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "crowdsourcing",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  <DH-Heading1>1. Introduction</  DH-Heading1>  This paper presents a project to encode and analyze ancient Greek music. Notated music of Ancient Greece survives only in about 40, often quite fragmentary, sources spanning a chronological range of 500-600 years. Only one song exists that can be considered complete. The information preserved in these documents, however, can be quite rich. It informs us about lyric, pitch, rhythm, meter, section breaks, dynamics, and instrumentation. Documents containing notated ancient Greek music survive primarily in the form of papyrus fragments. A smaller but significant number appears in inscriptions, while a limited amount of the music of Hadrian's court musician, Mesomedes of Crete, survives in a manuscript tradition. Due to the accidents of survival, the majority of these compositions date from the Roman period, although a significant number of Classical and Hellenistic fragments, such as a fragment from the Orestes of Euripides, survives as well. They range geographically from Greece to Anatolia to Egypt, and include tragedies, paeans, hymns, comedies, instrumental works, theoretical exercises, and one early Christian hymn. Huge challenges for the study of this corpus are its fragmentary nature and its geographical, chronological, and topical extent. The goal of this project is to implement a design permitting digital study of this corpus, and to provide a framework for research and pedagogy. The digital corpus must be able to encode the same information across a variety of source types but also take the different aspects of the documents' histories into account so that understanding can be made in context. 2. The Digital Corpus Our digital editions should capture the semantics of ancient Greek musical documents. They should support computational analysis and presentation directly from the information recorded in the ancient documents. To do this, they must address two fundamental challenges: recording ancient Greek musical notation, and coordinating musical notation, lyrics, and other analytical data sets. It is not possible to record ancient Greek musical notation using existing systems for the digital encoding of music such as MusicXML and the Music Encoding Initiative (MEI). The Music Encoding Initiative (https://music-encoding.org/), for example, is excellently suited to encoding modern western music notation. Its XML scheme can be extended with modules for particular kinds of notation. However, MEI assumes that music can be encoded in terms of pitches as notated in familiar staffed systems. The limits of this assumption are most evident in MEI's guidelines for encoding neumes, which are forced to avoid encoding unstaffed neumes entirely. For this reason, Waller et al. (\"Encoding the Oldest Western Music,\" https://dh2018.adho.org/en/encoding-the-oldest-western-music/) developed the Virgapes notation for encoding medieval plainchant, which they presented at DH2018. We need instead an encoding scheme that expresses the semantics of ancient Greek documents. The Unicode block for Ancient Greek Musical Notation, for example, does not satisfy this requirement, since its codepoints focus on graphemes rather than sememes, with the result that visually identical but semantically distinct symbols are encoded to a single codepoint. We therefore define an original encoding establishing a one-to-one correspondence between the symbols used in Greek musical texts and their digital encoding. Most surviving Greek musical documents are vocal. Our editions of the musical notation must therefore be coordinated with editions of the lyrics. We adopt the model mentioned above, by Waller et al. Musical notation and lyrics are transcribed in separate TEI XML documents aligned by canonical citation using Canonical Text Services (CTS) URNs. We extend the model of Waller et al. by adding further editions. Any aspect of analysis which is not able to be easily captured in an existing edition is simply captured in an aligned edition. For example, Greek texts had a pitch accent that is explicitly written in modern printed editions, and for a source document can be reconstructed by the editor. Similarly, meter can be inferred from the length of syllables in the text. Both accent and meter are recorded in separate documents and aligned by CTS URN with the diplomatic editions of the lyrics. 3. Systematic Data Extraction, Computational Analysis, and Presentation Along with its digital editions, the project includes a suite of programs to extract and analyze data from the editions. This endeavor operates by the principle that the corpus and the manipulation of the data therein are to be entirely separate. Data within the corpus can be systematically extracted for computational analysis or presentation. Since all editions of a single document are aligned, correlated patterns in different types of editions can be identified and analyzed. For example, accent information and pitch information can be compared, to study the known phenomenon in ancient Greek music by which melodic pattern tends to correlate with accent. Instances of agreement and of conflict can be separately analyzed in relation to the other editions to achieve an understanding of how and why agreement or conflict might occur. For presentation, for example, composite information from the aligned editions can be exported as a text file which can then be read by a custom-coded application created with the music program Max MSP. This aural analysis provides the choice to realize the music according to a variety of tuning systems championed by a variety of competing ancient theories, such as those of Aristoxenus and Ptolemy, for which the program is specially equipped. 4. Open Source All material from the project will be available in a public repository on Github. New editions and code from contributors can expand the project, creating an ongoing initiative to advance the study of this topic. This framework's explicit modeling and coordination of a large and complex corpus of information underscores the theme of \"Complexities.\" The automated generation of aural analysis with Max MSP takes a novel approach of audio synthesis by systematic data extraction, and introduces a range of options for presentation and teaching. ",
        "article_title": "Encoding Ancient Greek Music",
        "authors": [
            {
                "given": "Zachary David",
                "family": "Sowerby",
                "affiliation": [
                    {
                        "original_name": "College of the Holy Cross, United States of America",
                        "normalized_name": "College of the Holy Cross",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05dwp6855",
                            "GRID": "grid.254514.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-04",
        "keywords": [
            "corpus and text analysis",
            "scholarly editing",
            "musicology",
            "English",
            "text encoding and markup languages",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Aligning different versions of the same work is both a computational and a philological challenge. In particular, the collation of witnesses of an ancient or medieval text poses specific difficulties due to the coexistence of macro-structural and localised variants, including a large number of formal variants. We present an experimental computer-assisted workflow for aligning several witnesses and classifyingvariants. Formal and substantive variants are examples of categories especially relevant for languages which are unstable in their graphic system, as are medieval languages. The case studies are in Old French, and, marginally, Old Spanish.  The distinction between formal and substantive variants enables to treat them separately. Stemmatology, for instance, will be mostly interested in the former (even if this has been challenged in Andrews, 2016), while, for linguistic analysis the latter are needed. In automatic collation, based on full transcription of the texts to be compared, the formal variation is generally preserved, but temporarily nullified by means of normalisation or fuzzy match: this enables an accurate alignment of the texts and at the same time the preservation of the original forms.   How to handle variation Medieval texts, especially in vernacular, often exhibit important variation. At the phrases or words levels, syntactic or graphic variations account for diachronic and diatopic differences, varying scribal practices and the plurality of graphematic standards. This makes it difficult to align sequences between texts, when they have very few letters in common, e.g.,  Cait del fuere |  Chiet dou fuerre |  Kiet du feurre (‘[The sword] falls of the scabbard’).  Difficulties due to spelling or flexional variation only add up to already existing variations in word order or substance. Consider the following example taken from Chrétien de Troye's  Chevalier au lion (Meyer, 2006, v. 3701):   H Li frans li dolz ou ert il donques   P Li frans li dous ou estoit donques   V Li franz li doz ou ert il donques   F Li frans li dols ou ert il donques   G Li biaux li preuz ou estoit donques   A Li preus li frans u est il donques   S Li preus li frans u ert il donques   R Li frans li dols u ert il donques   M Li frans li preus ou est il donques  Spelling (e.g.,  dolz,  dous,  doz,  dols) and flexional variants ( est,  ert,  estoit) go along with substitutions ( dous |  preus or  biaux |  frans), additions/deletions ( il), or permutations ( preuz). In such a case, clearing out spelling and flexional variation might help in resolving the other difficulties.   This paper offers a new approach to the normalisation task made possible by the developments in the field of NLP and the resources now available for medieval languages, following the steps described in fig. 1.    Processing workflow   The initial step is the acquisition of the text, from the digital image, done by a combination of manual transcription (for producing ground truth), automated handwritten text recognition, and post-correction. The raw text thus obtained is then structured and stored in an XML/TEI based format. All these tasks are performed before the normalisation step, here represented by lemmatization and linguistic annotation, done with the help of neural network-based taggers/lemmatizers. Traditionally, normalisation consists of the preparation of the texts for alignment and might imply lowercasing, removing punctuation or editorial markup, as well as the temporary removal of formal features (Silva and Love, 1969 ; Robinson, 1989). Our proposal is to move to an automatic normalisation performed using NLP tools. Each token (i.e. word) is annotated with linguistic information such as part of speech, lemma and detailed morphological information. This kind of normalisation is only possible when suitable resources are available. For Old Spanish, Freeling (Padró Stanilovsky, 2012) provides a specific module (Boleda, 2011; Porta et al., 2013). For Old French, we used the data provided by the  Geste corpus (Camps et al., 2016), annotated with lemmas, as well as POS and morph tags according to the Cattex scheme (Prevost et al., 2013). With this data, we trained a neural tagger/lemmatizer suitable for variation-rich languages (Kestemont et al., 2017 ; Manjavacas et al., 2019). On the test set, accuracy reached 94.5 and 95% for lemmatization and POS-tagging, and was in the range 94-98.5% for different morphological features.  After normalisation, the texts enriched with linguistic information can be used to perform the alignment. Variation in structure, order or content in medieval texts is favoured by the existence of ‘active textual transmission’ (Vàrvaro, 1970) and by processes of rewriting, prosification/versification, etc. Changes in the order of the structural entities (verses, paragraphs, etc.) are also common. In order to collate these displaced entities, a phase of macro-structural alignment might be needed. This process can be done by a combination of direct expertise and tools conceived for detecting paraphrase, text reuse or computing similarities (Büchler et al., 2014; Jänicke and Wrisley, 2018). The very collation is then made by using the collation program CollateX (Dekker et al., 2011 and 2015) in its Python version. CollateX uses multiple alignment algorithms, suitable for the comparison of more than two witnesses (Spadini, 2017); its modular structure, based on the Gothenburg model, enables the user to intervene on each module separately and to add new ones.   Automatic categorization of variants All these software bricks can be integrated in a more complex pipeline up to the the final output. The modular structure of CollateX enables us to adjust the alignment and the visualization phases, in order to take into account the linguistic annotations added to each token. The alignment is performed directly on the annotation, used as a normalised form. In the creation of the output, some rules are added to compare the original forms with the annotation and to assign a category to the variant. For example, the category 'formal variant' is assigned to aligned tokens which have the same annotations but different original forms, such as:  mielz (pos: adverb; lemma:  mieus),   miels (pos: adverb; lemma:  mieus),   miaus (pos: adverb; lemma:  mieus).  Additional rules can be used for classifying variants into finer-grained categories, using linguistic annotation (fig. 2).    Possible classification of variants using linguistic annotation, with examples of possible subcategories and cases. The broad paradigmatic subcategory encompasses synonyms, cohyponyms, hypero-/hyponymes or holo-/meronyms; the semantic subcategory is reserved for lexemes who do not hold this type of relation between them.     Conclusions and Further research This paper presents some early results of an ongoing research on automatic collation and categorization of variants. Performing normalization using NLP tools not only speeds up the task, but also makes the identification of fine-grained categories possible. The case studies show the strong and weak points of this proposal and of the technical solutions for its implementation. Eventually, this research forces us to reflect upon the importance of having software components which are open and modular, in order to improve them and to include them in computational pipelines.  ",
        "article_title": " Collating Medieval Vernacular Texts: Aligning Witnesses, Classifying Variants  ",
        "authors": [
            {
                "given": "Jean-Baptiste",
                "family": "Camps",
                "affiliation": [
                    {
                        "original_name": "École nationale des chartes (PSL)",
                        "normalized_name": "École Nationale des Chartes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/013xvg556",
                            "GRID": "grid.462175.3"
                        }
                    }
                ]
            },
            {
                "given": "Lucence",
                "family": "Ing",
                "affiliation": [
                    {
                        "original_name": "École nationale des chartes (PSL)",
                        "normalized_name": "École Nationale des Chartes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/013xvg556",
                            "GRID": "grid.462175.3"
                        }
                    }
                ]
            },
            {
                "given": "Elena",
                "family": "Spadini",
                "affiliation": [
                    {
                        "original_name": "Université de Lausanne, Switzerland",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-21",
        "keywords": [
            "corpus and text analysis",
            "scholarly editing",
            "artificial intelligence and machine learning",
            "bibliographic methods / textual studies",
            "English",
            "philology",
            "medieval studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Modern (foreign) languages have been under pressure in a number of countries for some time now, with reduced provision in many English-speaking countries, and options increasingly focusing on English elsewhere. In the English-speaking world, a long series of reports has catalogued these challenges at all levels of education, often using words like ‘crisis’ and ‘urgency’ (Tinsley 2013) and it has been suggested that the field of Modern Languages needs to re-configure itself to meet new cultural/media landscapes (Worton 2009). Meanwhile, as the digital humanities have expanded and increasingly entered mainstream debates about the future of the humanities, there has been a growing focus on its terms of representation (Spence 2018) and commitment to diversity (Galina 2014). On a geocultural level, various initiatives have sought to address the immense Anglophone linguistic and cultural disparities (e.g. https://programminghistorian.org/ or http://www.globaloutlookdh.org/) which digital culture has amplified — and which DH has often, albeit unwittingly, reinforced within its own areas of influence — but there has been relatively little consideration given to possible interplay with ML, which seems strange given ML expertise in studying multilingual diversity, translation and transcultural perspectives. This paper starts by tracing interactions between ML and DH based on detailed empirical research, and argues that there is much to gain from connecting current debates about the future of the Modern Languages (ML) with concerns about the linguistic diversity deficit in the digital humanities.  Topography So far there has been little systematic analysis of interactions between ML and digital humanities (or digital culture more generally), something the Digital Mediations strand on the AHRC-funded ‘Language Acts & Worldmaking’ project aims to address (https://languageacts.org/digital-mediations/). Through a series of landscape surveys – questionnaires, interview surveys, literature reviews, resource reviews and curricular studies - we have mapped the topography of digital ML research and found that the engagement of Modern Foreign Languages with digital is both wider and deeper than is widely understood by DH or indeed by ML itself, although recently it has started to surface more prominently through initiatives such as the ‘digital hispanisms’ panel at the Association of Hispanists of Great Britain and Ireland conference in 2018 (http://community.dur.ac.uk/hispanists/wp-content/uploads/2018/03/AHGBI-2018-Abstracts-Booklet.pdf) or the 2019 MLA forum on ‘Digital Humanities & Modern Languages’ (https://mla.confex.com/mla/2019/forum/extra/index.cgi?EntryType=Session&username=4494&password=340557). Digital mediation in ML is particularly strong around language learning/teaching, where researchers and teachers have an elevated understanding of the potential for authentic, multimodal, learner-centred, transcultural and polylingual interaction (Guikema and Williams 2014), but the focus of our research is principally on research into (foreign) languages and their related cultures, rather than language learning. Our research first sought to establish the topography of ML-DH interactions through an open survey (158 respondents), interview study with ML respondents (35) across a range of different roles, course review (analysing 12 ML modules at 9 institutions) and resource review (analysing 1,327 digital ML resources across 8 portals/platforms). We have found that ML researchers have an elevated sense of digital affordances (and risks/limitations), but, generally speaking, low engagement with the kind of advanced digital methods represented by DH, beyond areas such as digital editing, linguistics, electronic literature and web archive studies of language communities (Spence and Brandao 2019).  There were some interesting trends in both survey and interview studies: while transcultural and plurilinguistic expertise is valued, national and disciplinary boundaries are deemed less and less relevant to ML research (making future DH-ML collaboration all the more necessary); large scale data approaches are judged to be increasingly important in facilitating interesting comparisons of cultural reception and influence across many languages; digital is becoming increasingly important as a research object in itself; and there is support for the idea that ‘digital’ needs to (and will) be integrated more closely into ML practice, rather than being something different in future.   Towards a DHML agenda? How does this connect to DH specifically? How can we start to theorise the connection between the digital humanities and ML, and what opportunities are there for deeper, bidirectional and equitably conceived/constructed collaboration? In November 2015 a writing sprint on the theme of ‘Modern Language and the digital’, later published as an open access multi-author collaborative article on ‘The Shape of the Discipline’ edited by Claire Taylor and Niamh Thornton (2017), opened up debate about this. Conversation ranged from themes such as data-driven approaches to modern languages or the ML research process, to users/interfaces in ML and digital ethnography and while some contributors argued that DH often has too narrow a conception of ‘digital’ (which doesn’t always engage well with the digital cultural studies focus which is more common in ML), the article set the scene for greater possible collaboration between the field of ML and DH. In an article for Digital Humanities Quarterly, Thea Pitman and Claire Taylor expanded on this theme, arguing that ML bridge a gap between traditional DH praxis and “linguistically- and culturally-specific cultural studies approaches to digital materials” (2017). They went on to argue that more collaboration is needed “to optimise the potential of both disciplines” through what they call a “‘critical DHML’ approach.”   Mutual disruption Debates about DH’s own communication practices have rightly received a fair amount attention in recent years, with various attempts to address linguistic and cultural diversity (http://go-dh.github.io/translation-toolkit/about/), but it has been much less common for linguistic diversity to be explicitly harnessed as part of DH’s research agenda. There is a substantial body of DH research involving language, but the overriding dynamic has historically been digital humanities disrupting ML-based research practices, rather than the other way around, and this naturally shapes discourse around the ML-DH relationship. DH2018, in many ways an exemplary conference in terms of representation, with ample discussion of cultural diversity and reappropriation of digital spaces – such as the keynote ‘Weaving the Word’, by Janet Chávez Santiago – addressed this balance to some extent, but even here the emphasis was largely on digital transformation of ML rather than vice versa, and where ML perspectives surfaced explicitly, it was generally in relation to pedagogy (e.g. ‘Global Perspectives On Decolonizing Digital Pedagogy’ panel). What if we were to change the direction of flow, or to look for challenges which are mutually transformative?  As a result of our research we propose some areas where DH might benefit from a greater, and more explicit, adoption of ‘ML’ sensibilities, which can be summarised as follows:    The international classroom. Greater attention to the concept of the international and plurilingual classroom in DH pedagogy - including diversification of the DH curriculum, and the use of virtual communications to explore multilingual interactions between DH student cohorts, potentially in liaison with ML programmes.   Linguistic diversity as research topic. The growing consolidation of ‘supercentral’ languages and English as the ‘hypercentral’ language has major implications for language diversity (de Swaan 2001), a phenomenon which will increasingly become a major challenge for DH and scholarly communications as a whole (Ortega 2014). Digital humanists are already active in some great projects to protect endangered, minority or heritage languages, but there is a need for greater awareness of, and commitment to, linguistic diversity in the actual construction of DH resources and this is an area ML where can make an important contribution.    Linguistic/cultural interfaces. Early digital experiments like the work of Jiménez, Underberg and Zorn adopted key Andean concepts such as “complementary duality and the tripartite division of time and space” into web design (Underberg and Zorn 2013: 29). Closer attention from DH to how resources are constructed linguistically and culturally would help foster greater awareness about how resources work transculturally within the field.   Greater recognition and organisation of multilingual resources. With the exception of corpus-based resources such as CLARIN (https://www.clarin.eu/portal), it is not easy to find language-based DH research online, even on otherwise excellent resources such as the EADH project list (https://eadh.org/projects).   Multilingual methods and tools. We know digital methods and tools are not culturally neutral, and yet much of the discussion about them still tacitly assumes that they operate globally and monolingually (in English) – we need more formal work in DH to explore how they operate differently in and across different linguistic and cultural contexts.     Conclusions Languages (and their associated cultures) have been a key area of focus for the Digital Humanities from its origins as a field, and yet DH has rarely explored what this means in any real depth for its own research and pedagogical practices. In this paper, we have outlined a rationale for greater focus on ML-DH collaboration and have proposed some key elements of a possible future common research agenda.",
        "article_title": "Topographies of Digital Modern (Foreign) Languages research",
        "authors": [
            {
                "given": "Paul Joseph",
                "family": "Spence",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Humanities, King's College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Renata",
                "family": "Brandao",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Humanities, King's College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "multilingual / multicultural approaches",
            "theory and methodology)",
            "English",
            "spanish and spanish american studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Spell-checking software is well established in consumer applications but often unexploited by data-creation projects in the digital humanities. We argue that spell-checking provides a relatively straightforward way to find (some) transcription errors even in texts written in idiosyncratic or inconstant spelling.   Working hypotheses We believe that spell checking is feasible, useful, and underused in DH projects. More specifically:  Fewer than half of DH projects transcribing existing materials use spell-checking technology. Standard word-in-isolation spell checking can find transcription errors. Project-specific spelling dictionaries can do better than off-the-shelf dictionaries for  writing by idiosyncratic or inconstant spellers older language forms no longer supported in off-the-shelf dictionaries non-standard and minority languages which lack off-the-shelf dictionaries   Project-specific filters may be necessary to create a checkable alpha text ([Huitfeldt 2006]) but are feasible.    Modeling spell-checking, modeling languages In conventional word-in-isolation spell-checking ([Earnest 2011], [Damerau 1964], [McIlroy 1982], [Bentley 1986], [Peterson 1980], [Kuenning 2018], [Atkinson 2017], [Németh 2018]), the language model is trivial: all acceptable forms are equiprobable, a form is acceptable if and only if listed in the dictionary, unknown forms have probability zero, and any token with probability zero is a probable error. To find alternative spellings, a Levenshtein distance of one ([Norvig 2007]) or more ([Garbe 2012], [Garbe 2015]) is sometimes used. A combination of phonetic encoding and Levenshtein distance can sometimes be helpful ([Atkinson 2017]). Recent work on spelling correction (e.g. [Choudhury et al. 2018], [Dashti et al. 2018]) uses more elaborate models to detect ‘real-word’ errors (e.g. “be” for “he”). In this paper, however, we assume the simple model of text as a sequence of equiprobable known forms.   Challenges in using spell-checking For spell-checking in DH projects, some complications arise.  Transcribers normally seek to reproduce the spelling of the exemplar, not to correct it. When standard spelling dictionaries are used to check material which consistently violates orthographic norms (idiosyncratic spelling), they will erroneously flag some correctly transcribed misspellings and miss unconscious corrections by the transcriber. Off-the-shelf dictionaries reflect current norms for widely spoken languages. Older language varieties and under-resourced languages often lack dictionaries. The language transcribed may have no standardized orthography; spelling may vary by scribe or line-by-line (inconstant spelling). XML documents may contain material not to be spell-checked (markup, project-internal comments, etc.), or material in different languages or varieties (e.g. 21st-century English in the header and 17th-century English in the body).  We believe these complications can be addressed. For idiosyncratic spelling, the solution is to use a project-specific dictionary, not a standard dictionary, so that correctly transcribed misspellings will be accepted and inadvertent corrections flagged. Inconstant spelling makes spell-checkers miss transcription errors that substitute one accepted form for another. But spell-checking can still catch other errors. (Similarly, an English spelling dictionary with both British and American spellings won't catch \"colour\" in American texts, but it will catch the typo \"teh\".) Producing project-specific dictionaries from scratch requires some work, but our experiments suggest that even modest effort can produce spelling dictionaries that will detect existing transcription errors without excessive noise. For dealing with XML, it's helpful to write filters to extract the desired word forms. Fortunately, this is normally straightforward.   A small pilot study Several practical questions arise:  How can project-specific dictionaries be constructed? What should they contain (and exclude)? How much work is involved? How big must the dictionary be:  to catch as many actual errors as possible? not to flag correctly transcribed words erroneously?   How much project data is necessary to obtain a dictionary of that size?  We have explored these questions using material from the Wittgenstein Archives at the University of Bergen and from Liam Quin's digital version of Alexander Chalmers's General Biographical Dictionary ([Quin 2017]).  For each project, we selected test material: for Wittgenstein, two small texts taken from non-final versions of the corpus; for Chalmers, 10,000 words from volume 25. For Wittgenstein, we checked the normalized-spelling text, identifying word forms which violate German orthographic norms. The Wittgenstein project defined standard orthography as that of Duden's  Rechtschreibung, but admitted some idiosyncratic spellings consistently used by Wittgenstein.   For Chalmers, we proofread the sample against the page scans. With programmatic filters we extracted an alpha text (a list of words for spell-checking). We constructed dictionaries of various sizes by compiling lists of correct forms from different subsets of the project corpora. In principle, project-specific dictionaries should be built by proofreading texts one-by-one; to streamline the pilot project, we took the shortcut of checking wordlists against off-the-shelf dictionaries. This does not visibly affect the statistical results shown later, but it does mean that for Chalmers some mistranscriptions were missed and some bad flags thrown.  We checked the test samples against those dictionaries. For each test, we counted the number of correct and incorrect tokens in the sample flagged or left unflagged by the spell checker.    Results of the pilot study  Constructing project-specific dictionaries The simplest (not fastest) method is to start with an empty dictionary and spell-check texts from the project's corpus one by one. For each word flagged by the spell checker, either add it to the dictionary or correct it in the text. (More on this below.) With an empty dictionary, the spell-checker will at first flag every form in the text. To avoid the tedium of dealing with so many bad flags, it may be worthwhile to list the most frequent forms in whatever part of the corpus is available for consultation, check them manually, and seed the dictionary with them. For Wittgenstein, a dictionary of 1300 forms covers about 90% of the running tokens in the text, flagging only one token in ten.   What to include and exclude Ideally, the dictionary should include all forms which actually occur correctly in the corpus and no forms which are transcription errors. This ideal is unattainable for two reasons. First, the same form may occur both as a correct transcription and as a mistranscription (real-word errors); it cannot be both included and excluded. Second, as the corpus grows, there will always be some correct forms not yet found in the dictionary, and thus always some erroneous flags. The optimal solution is to balance the relative inconvenience of undetected errors and false flags against the relative frequency with which each form is correct or mistranscribed. If undetected errors and bad flags have equal weight, then a form should be included in the dictionary if any occurrence is more likely correct than not. If we would rather see ten bad flags than miss one mistranscription, then a form should be included only if it is ten times more likely to be correct than incorrect. The project's preferences determine the threshold to be met. If spelling habits vary from document to document, it can be useful to make both a project-wide dictionary and document-specific dictionaries for texts with distinctive usage. When forms intentionally excluded from the dictionary do occur correctly transcribed, they can be marked with sic or similar markup and excluded from the alpha text, to avoid throwing bad flags for them. With these complications, the rule for forms flagged by the spell-checker becomes:  If the form is correctly transcribed and meets the project's correctness threshold, add it to the project dictionary. If the form is correctly transcribed and meets the threshold in the current document but not elsewhere, then add it to the document-specific dictionary. If the form is correctly transcribed but does not meet the threshold, then tag it with sic or the equivalent. If the form is incorrectly transcribed, correct it.    Dictionary size Error detection does not require a big dictionary. Indeed, because of real-word errors, bigger dictionaries often find fewer actual errors than smaller dictionaries, as shown below for Chalmers.      Small dictionaries, however, throw too many bad flags. Fortunately, the number of bad flags falls dramatically as dictionary size rises, as shown below for Wittgenstein (red, two samples) and Chalmers (blue).      Whether spell-checking feels useful or pointless depends (we think) on its signal:noise ratio; in our data dictionaries of about 15,000 forms reach a (bearable?) ratio of 1:10 (ten erroneous flags for each detected error).      How big a corpus must be processed to produce a dictionary of suitable size? It varies, but as the plots below show, something more than 200,000 tokens are needed for a dictionary of 15,000 forms.            Conclusions and future work Spell checking can find transcription errors in real-world data, even though transcription errors are logically distinct from misspellings and even when the spelling is idiosyncratic or inconstant. Developing a project-specific dictionary takes little time and can be expected to improve the results of proofreading. Even very small project-specific dictionaries can be useful. Work remains to be done to extend the pilot study to more materials, to support interactive correction of texts, to improve on the XML support offered by existing spell-checkers, and to explore the application of more sophisticated models of text to support word-in-context spell-checking in lieu of word-in-isolation spell-checking.  ",
        "article_title": " Bootstrapping Project-specific Spell-checkers  ",
        "authors": [
            {
                "given": "C. M.",
                "family": "Sperberg-McQueen",
                "affiliation": [
                    {
                        "original_name": "Black Mesa Technologies LLC, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Claus",
                "family": "Huitfeldt",
                "affiliation": [
                    {
                        "original_name": "Universitetet i Bergen",
                        "normalized_name": "University of Bergen",
                        "country": "Norway",
                        "identifiers": {
                            "ror": "https://ror.org/03zga2b32",
                            "GRID": "grid.7914.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-02",
        "keywords": [
            "methods and technologies",
            "management",
            "scholarly editing",
            "English",
            "computer science and informatics",
            "project design",
            "philology",
            "cultural artifacts digitisation - theory",
            "organization",
            "data models and formal languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Cooking traditions, whether they are regional or in a larger context, are one of the most distinguishable items of European culture and an important part of European identities. But how did they become to what we know them now? How did they develop and what were their influences? During the last decades, research arrived at two important conclusions on these questions. Firstly, there are no quantitative studies on the origin and formation of regional cuisines in Europe. Secondly, manuscripts containing thousands of cookery recipes first appeared in the Middle Ages, which can be consequently regarded as the birth of modern European cuisines. On the European continent Latin, Middle French and Early New High German recipes provide the majority of culinary transmission. The project is preparing the cooking recipe transmission of France and the German speaking countries, which sums up to more than 80 manuscripts and about 8000 recipes, for the analysis of their origin, their relation, and their migration through Europe. The comparison of French and German food history is especially suited for this task as France always had a culturally formative influence on German speaking peoples.  Cooking recipes are culturally charged transient texts, which are best diachronically and spatially analyzed by strongly relying on digital humanities methods. However, understanding these recipes, their context and their transmission is not an easy process. The technical terms that describe ingredients, utensils, procedures, and customs of the time are a challenge even for history scholars who specialize on the topic. Thus, the texts need not only be transcribed and edited but also semantically enriched so that further analysis like machine aided comparison of ingredients or cooking processes can add to standardized philological research like the collation. The base of our data will be customized TEI/XML documents with a schema aiming at facilitating the semantic annotation of cooking recipes in general.  The core of our digital research strategy is the Semantic Web and the idea of Linked Open Data. We are in the comfortable position within the Humanities that we are mainly dealing with food ingredients i.e. animals, plants and fungi, all fields of research that are provided with a sophisticated amount of already established ontologies and Linked Data   For an overview of ontologies covering these topics see  http://www.ontobee.org/,  http://aims.fao.org/,  https://ndb.nal.usda.gov/ndb/,  https://agclass.nal.usda.gov/about.shtml,  http://zbw.eu/stw/version/latest/thsys/70498/about.de.html all of which are connected to the Linked Open Date Cloud by proving its data in one or more serializations of OWL and/or RDF(S).    including the general knowledge bases of Wikidata and DBPedia. Ontologies are also already being successfully used to represent (Dooley et al., 2018, Sam et al., 2014, Ribeiro et al., 2006) and analyze (Hammad and Hassouna, 2011, Vadivu and Waheeta Hopper, 2010) cooking recipes, albeit with different focuses and granularity of data. In the digital research-approach, we partly rely on text similarities but largely on the occurrence of ingredients, preparation instructions and time, tools, serving suggestions and medicinal, cultural as well as religious implications in the texts. The sheer quantity of semantics within one historic recipe shows how complex a semantic annotation of these texts is, leaving apart all philological observations that can be found in the material. Besides the ability to find links between resources that were not known before, a main argument for the decision to put Semantic Web technologies at the core of the project, was to ability to work outside of language barriers. Through the use of concepts in the sense of a notion, an idea rather than a term we are trying to overcome historical and language constraints i.e. the German word for potato ‘Erdapfel’ (‘erdaphel’ in Early New high German, literally ‘apple from the ground’) appears in a manuscript from about 1488) long before the potato was imported from South America to Europe, giving us proof that the concept of ‘Erdapfel’ must have been a very different one (probably ‘any kind of beet’) from today’s notion. For this reason, we needed to introduce a workflow that reflects the recipes’ philological and semantic complexity. A precompiled list of medieval plant names and their translations to modern English and German as well as their medieval variant dictions   http://medieval-plants.org  collected by Helmut W. Klug, the principal researcher of the Austrian part of CoReMa, gave us the opportunity to start with a (semi)automated alignment of Wikidata concepts with the Reconciliation Service API   https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API  provided by OpenRefine   http://openrefine.org/ . Once each term has a concept connected with it, these concepts are used to enrich the ingredients within the actual recipe texts in the TEI documents. However, as stated above, a crucial point of this process is the human interpretation of the enriched entities and the decision for a concept. Therefore, the auto alignment procedure can only be viewed as a means to an end.   Once the entities of each recipe are equipped with concepts, the project’s analysis can reveal concurring or deviating eating habits, text migration as well as the influence of neighboring countries on their respective cuisine. The vast implementation of ontologies in the natural sciences allows us to establish connections from historical eating habits to modern concepts of food and generate new knowledge for the domain of food history. The research data will also be the basis for spatial and temporal visualization and statistical evaluation. The storage, analysis and dissemination of the project’s data is handled by the data repository GAMS (Geisteswissenschaftliches Asset Management) developed by the Austrian Centre for Information Modelling in Graz.   http://gams.uni-graz.at/  Within this infrastructure aimed at long-term preservation, the triplestore Blazegraph is accessed through a web service for the storage and retrieval of RDF triples which allows us to query our project’s databases of medieval recipes in France and Austria as well as all connected concepts in the Linked Open Data Cloud.  ",
        "article_title": "Cooking Recipes of the Middle Ages: Corpus, Analysis, Visualization",
        "authors": [
            {
                "given": "Christian",
                "family": "Steiner",
                "affiliation": [
                    {
                        "original_name": "University of Graz, Austria",
                        "normalized_name": "University of Graz",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/01faaaf77",
                            "GRID": "grid.5110.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-12",
        "keywords": [
            "corpus and text analysis",
            "semantic analysis",
            "English",
            "ontologies and knowledge representation",
            "semantic web and linked data",
            "medieval studies",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Writing often takes place or is displayed on a two dimensional surface, but many of the digital techniques for the representation of language reduce two dimensions to one. This reduction leads to many powerful tools in corpus linguistics, which depends heavily on patterns involving words appearing in linear sequences. However, it is widely acknowledged that language is not wholly captured by a purely sequential representation. As (Gross, 1997, p16) notes in the context of poetry: ‘Texts that are presented as spatial (rather than linear) arrangements in order to activate the visual composition on the page as an element of signification have a long tradition’.  More widely than poetry, layout in the two dimensional space of a single page can carry meaning – alignments both horizontal and vertical of lines of text, and placement of blocks of text in relation to images and diagrams are a key feature of multimodal documents (Bateman, 2008). Frameworks for such documents have been proposed by Bateman and others. Further areas where layout is essential, including comics and graphic novels (Bateman et al., 2017), have also been studied.  It is striking that although representing two dimensional structure in documents of many kinds is clearly relevant to the digital humanities, existing work has made very little use of the techniques of qualitative spatial representation that have been applied in artificial intelligence over more than the past two decades. This paper describes work currently in progress to apply these particular techniques as a means of representing some aspects of the two dimensional structure of poetry layout.   Qualitative Relations in Poetry Layout The idea of qualitative spatial representation (QSR) is that spatial relationships, such as  next to, alongside, bordering, overlapping, and many other topological notions, can be represented computationally using logic. An introduction to the technical foundations of QSR is provided by (Cohn & Renz, 2008). The motivation in artificial intelligence is that humans use common sense spatial concepts in everyday situations rather than the numerical coordinates which predominate in most computational representations of space. The origins of QSR go back to philosophical interests in an account of space fitting human experience, such as the theory of extensive connection proposed by (Whitehead, 1929).   To explain how QSR can be applied to the space of poetry in its printed form we will consider some examples. The following is the result of some standard image processing techniques applied an image of Southey’s  Cataract of Lodore printed in 1823. The text here occupies four pages but initially we only consider relationships between successive lines of text.    Layout in Southey’s Cataract of Lodore  This poem is a well-known example of visual arrangement as the form follows the cascade of water: initially having a variety of directions before a long descent ending in progessively widening course as it reaches the bottom. Many related examples are known and described in (Gross, 1997) and (Hollander, 1975). Two further examples are given in Figure 2.   Layout in poems by Eavan Boland (left) and by Alasdair Gray (right).  On the left of Figure 2 is Eavan Boland’s poem “CODE. An Ode to Grace Murray Hopper 1906-88 maker of a computer compiler and verifier of COBOL.” from (Boland, 2001) consists of four blocks in which lines start successively further to the right, interleaved with three blocks in which all lines are justified on the left. On the right of Figure 2 is Alasdair Gray’s poem  First of March 1990 (Gray, 2010, p128) consisting of six blocks of text sharing a roughly similar shape. In each block the lines end successively further to the left down the page, although the right hand ends do not display any easily identifiable regularity.  Although the examples have been presented visually, the reduction of poetry to simplified images does not give a computational means of comparing one poem to another. Given a corpus of tens of thousands of poems, how can the space of different shapes be understood? How can one query such a corpus for poems of a particular kind of shape? How could shapes be described, and how might one map changes to favoured shapes over time? These questions suggest a visual kind of distant reading (Moretti, 2005) but one we have begun to investigate using QSR rather than geometrical methods.    Application of Allen’s Interval Relations One basic QSR technique comes from the work of Allen (1983) in qualitative relationships between intervals of time. If we consider possible ways two intervals may relate, one answer is the 13 relationships indicated on the left in Figure 3. This shows 7 ways the uppermost interval of each pair relates to the lower one. The last 6 relations have inverse relations. Although the inverse of before might be called after, we use the initial letter of each of the 6 followed by “i” to indicate the inverse. The application of this scheme to a sequence of lines appears on the right in Figure 3.   The Allen relations applied to layout  This means the spatial arrangement can be represented by: di, di, di, e, e, e, d, d, d. Of course, this loses many features, such as the left-right and up-down symmetry, in this case. However it does capture the structure of: lines becoming shorter on each side, then continuing down the page at a fixed width and finally expanding on both sides.  We have developed prototype software by coding in Python which determines qualitative relationships between lines and between blocks of text. This uses standard image processing techniques to segment images of pages into text lines as rectangular regions. Qualitative relationships are then calculated from the positions of these regions. In continuing work we are exploring other systems of spatial relationships, since the ones provided by Allen are only one example of many that are mentioned in (Cohn & Renz, 2008). We note that (Dubba et al., 2015) uses spatial relations between bounding boxes in video frames to detect patterns making up events. It is an exciting possibility that QSR can describe patterns making up visual poetic structure in an analogous way.  ",
        "article_title": "Qualitative Space of Poetry",
        "authors": [
            {
                "given": "John",
                "family": "Stell",
                "affiliation": [
                    {
                        "original_name": "University of Leeds, United Kingdom",
                        "normalized_name": "University of Leeds",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/024mrxd33",
                            "GRID": "grid.9909.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "artificial intelligence and machine learning",
            "English",
            "computer science and informatics",
            "literary studies",
            "ontologies and knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The quality of Optical Character Recognition (OCR) is a decisive factor for the application of text mining techniques on historical newspapers (Chiron et al., 2017; Walker et al., 2010; Strange et al., 2014). OCR for texts published in black letter is particularly challenging due to several factors: the low distinctiveness of characters, the change over time regarding vocabulary and spelling, the use of small font sizes, and the oftentimes poor paper quality. Holley (2009) argued that in light of the poor OCR quality in newspapers, a focus on manual crowd-correction is more promising than investments in software development. Although automatic OCR post-correction can improve the quality of the text, the methods often lack precision, are not robust enough, or require a lot of in-domain training data (Alex et al., 2012; Chiron et al., 2017). The problems are manifold and complex, but recent progress in neural OCR techniques promises significant improvements (Springmann and Lüdeling, 2016). These OCR models often outperform commercial systems like  ABBYY FineReader     . However, the training of a neural system using open-source software (e.g.,  Tesseract     ) is demanding. Integrated handwritten text recognition and annotation platforms like  Transkribus      facilitate the creation of a ground truth, as well as the training and application of neural and corpus-specific models for OCR.  Transkribus was initially designed to decipher manuscripts see   . It allows the manual transcription of uploaded documents so that they can be used as training material for  Handwritten Text Recognition (HTR) models (Weidemann et al., 2017). A useful feature of Transkribus’ HTR models is that the recognition of printed texts works just as well as that of manuscripts. A few dozen of corrected pages are sufficient for high-quality OCR results.  In this study we illustrate how to drastically improve OCR quality for black letter in newspapers with a modest amount of manual work for ground truth creation. The integration of HTR model training into the Transkribus platform enables Digital Humanists to leverage the performance of neural OCR without having to tackle unnecessary technicalities. In our experiments we additionally address the following questions. Robustness: Are HTR models reusable for material that varies in digitisation quality (medium-resolution scans from microfilm vs. high-resolution scans from paper). Transferability: How well does a model perform on another newspaper than the one it was trained on?    Data and Experiments  We use PDFs with medium-resolution images produced in 2005 from scanned microfilms of the German-language  Neue Zürcher Zeitung (NZZ) for our experiments. The OCRed text stems from  ABBYY FineReader XIX     , which was ABBYY’s product for 19th century black letter recognition at that time.  The first experiment evaluates the differences between three OCR systems: (a) FineReader XIX (FRXIX) results from 2005, (b) ABBYY FineReader Server 11 (FRS11) results see  , available within Transkribus , (c) Transkribus’ HTR model. Figure 1 shows example output from our three OCR systems.     Example excerpts with low-quality OCR from two pages of the NZZ (1819 left, 1859 right, red: FRXIX, blue: FRS11, green: Transkribus HTR)   In our second experiment we apply the HTR model trained on medium-resolution images to high-resolution images (400dpi) from 1899 digitised anew from paper in order to test the transferability of the model. We also analyse the performance of the HTR model in two other publications.   Creation of a ground truth and HTR model training  The NZZ had been published in black letter from 1780 until 1947. We chose one title page per year at random from this period and loaded the image extracted from the PDF into Transkribus. We used the Transkribus internal FRS11 to recognise the text in the images and manually corrected words and baselines. The resulting ground truth of 167 pages contains 304,286 words and 43,151 lines. Depending on the amount of text on a page, the correction of a page including the baselines (needed to train the HTR model) takes between 1 and 2.5 hours. We used 90/10 split for training and testing the model.    Evaluation  We use the bag-of-words F1-measure metrics of PRImA TextEval 1.4    for evaluation. The F1-measure is the harmonic mean of precision and recall. Precision gives the percentage of OCRed words that are part of the ground truth, while recall measures the percentage of ground truth words that were found by the OCR system. By applying a bag-of-words approach, possible differences in layout recognition cannot distort the results.      Results  Figure 2 shows the evaluation on all pages from the test set. The FRS11 (mean F1-measure 81.1%, SD 7.3%) beats the FRXIX (mean 67.8%, SD 11.1%) throughout. Our HTR model scores 97.0% (SD, 1.8%) on average and achieves significant improvements over both ABBYY products.    Comparison between original FRXIX, FRS11, and Transkribus HTR.   The application of our HTR model to five high-resolution images of newspaper pages from the NZZ shows accuracies of at least 98% and an average improvement of 4.24% over FRS11 (see Figure 3).    Comparison between FRS11 and Transkribus HTR model on five high-resolution images from 1899.   In terms of the transferability of our HTR model the average F1-measures of 98.6% (SD 1.9%) for the  Bundesblatt  and 98.9% (SD 0.6%) for the  Neue Zuger Zeitung over five pages each show that although the model has been trained on the NZZ, it is able to score equally high on different publications. The FRS11 reaches 92.4% (SD 2.7%) for the Bundesblatt and 88.4% (SD 3.7%) for the Neue Zuger Zeitung, showing the superiority of our HTR model.     Conclusion  We have shown that Transkribus is an excellent tool for creating HTR models for the OCR of newspapers typeset in black letter. Even with a limited amount of training data (150 pages), our HTR model consistently outperforms state-of-the-art commercial software. Our HTR model trained on medium-resolution images digitised from microfilm still performs better than commercial software when applied to high-resolution images derived from paper originals. Given the availability and abundance of digitised historical material in the form of PDF files with poorly OCRed text, our findings showcase how digital humanists can improve their source material for text mining with a reasonable effort.   Acknowledgments We would like to express our gratitude to Günter Mühlberger and the Transkribus team for their support in training HTR models and partially correcting baselines of our ground truth. Moreover, we thank Camille Watter and Isabel Meraner for their help in the transcription process. This research is supported by the Swiss National Science Foundation under grant CR-SII5_173719.  ",
        "article_title": " Improving OCR of Black Letter in Historical Newspapers: The Unreasonable Effectiveness of HTR Models on Low-Resolution Images  ",
        "authors": [
            {
                "given": "Phillip Benjamin",
                "family": "Ströbel",
                "affiliation": [
                    {
                        "original_name": "University of Zurich, Switzerland",
                        "normalized_name": "University of Zurich",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02crff812",
                            "GRID": "grid.7400.3"
                        }
                    }
                ]
            },
            {
                "given": "Simon",
                "family": "Clematide",
                "affiliation": [
                    {
                        "original_name": "University of Zurich, Switzerland",
                        "normalized_name": "University of Zurich",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02crff812",
                            "GRID": "grid.7400.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-02",
        "keywords": [
            "digital humanities (history",
            "artificial intelligence and machine learning",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "library & information science",
            "software studies",
            "OCR and hand-written recognition"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This short paper will trace the roots of the digital humanities certificate option as it is now most commonly conceived, beginning with Lisa Spiro’s 2010 post \"Opening Up Digital Humanities Education\" and then summarizing how her ideas were developed by scholars such as Lynne Siemens and Kara Kennedy in journal papers. It will then move on to examining how these ideas were put into different kinds of practice at institutions like Texas A & M University, the University of Nebraska-Lincoln, and the University of Virginia, and finally it will turn to the author's own experiences as the developer of a digital humanities workshop series who then participated in an attempt to standardize it for university accreditation as part of a certificate program. Issues explored along the way will include faculty collaboration and labor equity, fair intellectual representation, the complexity that conversations about these assume, and conditions for librarians and campus partners to agree to when developing curricula together. ",
        "article_title": "The Digital Humanities Certificate Option: What's At Stake?",
        "authors": [
            {
                "given": "Stephen",
                "family": "Sturgeon",
                "affiliation": [
                    {
                        "original_name": "Boston College, United States of America",
                        "normalized_name": "Boston College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02n2fzt79",
                            "GRID": "grid.208226.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "digital humanities (history",
            "digital ecologies",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "pedagogy",
            "digital communities and critical infrastructure studies",
            "cultural evolution",
            "teaching",
            "literary studies",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Abstract – HORAE (Hours, Recognition, Analysis, Editions) is a cross-disciplinary research project studying religious practices and experiences in the late Middle Ages and Renaissance as evidenced by the medieval bestseller, Books of Hours. Developing tools in artificial intelligence, computer vision and image analysis to “read” a very large and diverse corpus of medieval manuscripts, in Natural Language Processing (NLP) to “identify” textual units and structures, and in book history and religious practices, HORAE also implements diverse tools developed in the DH community (network analysis, data visualization, text mining). This paper presents HORAE as a research  Gesamtkunstwerk that tackles a common challenge of complexity, uncertainty and granularity with different tools from different fields. By broadening the perspective in a genuinely cross-disciplinary research, we discuss the position of DH within the humanities.   HORAE (Hours - Recognition, Analysis, Editions) is a cross-disciplinary research project studying religious practices and experiences in the late Middle Ages and Renaissance through Books of Hours, the number one medieval best seller. It involves three partners in humanities and computer science: the Institut de Recherche et d’Histoire des Textes (CNRS), the private company TEKLIA and the Laboratoire des Sciences du Numérique de Nantes (LS2N). More than 10,000 of these books survive and the production of such a large number of manuscripts is a major phenomenon and witness to profound changes in late medieval society, on cultural, religious, and industrial levels: speculative book production rather than on commission for specific clients, devotio moderna and imitation of clerical practices by lay people, customization of devotional objects, etc. Books of Hours are at once deluxe items of social display and intimate objects of devotional intensity, used for one’s salvation (Wieck et al., 1988; Reinburg, 2012; Hindman and Marrow, 2013). Their textual content is immense (300+ folios per volume; only 1.7% miniatures), yet scarcely studied. HORAE combines the research and expertise of all three partners: in artificial intelligence applied to computer vision and image analysis to automatically “read” a very large and diverse corpus of medieval manuscripts; in natural language processing (NLP) to parse and “identify” textual units and structures; and in book history and religious practices. The project aims to create an integrated chain from image treatment to producing new knowledge by placing the end user at the center of developments and focusing on ergonomics and data visualization.    Aims, corpus, relevance The aims encompass: (1) re-using the many digitized manuscripts which are online but underused; (2) new open-source software for Handwritten Text Recognition (HTR); (3) tools for segmentation and plagiarism detection, adapted to the transcription of medieval manuscripts produced by the machine, in order to identify the texts in Books of Hours; (4) identifying and editing unpublished texts; (5) visualization of manuscript clusters which share the same textual characteristics, either in the order of the different parts (Hours of the Virgin, votive offices, suffrages, prayers), or in the order of their smaller textual units; (6) studying the diffusion and circulation of devotional and liturgical texts at the end of the Middle Ages in order to better understand the cultures and faith in the 13th c.-16th c.   Big, linked, open, usable data. With its aims and methods, HORAE addresses the largest corpus of manuscripts in medieval studies so far. It changes research methods in auxiliary sciences and tackles the challenges of big data. Books of Hours have been little studied until new because they are too numerous, too complex and too standardized: now, their very number, repetition and complexity allow this project to develop new and efficient technologies and methodologies, and to gain new knowledge about the Middle Ages. The suitability of liturgical data for fruitful historical analysis was recently illustrated by data from calendars (Heikkilä and Roos, 2018).   Libraries, infrastructures, and digital humanities. Several hundred thousand medieval manuscripts and millions of medieval archival documents are preserved worldwide. The textual wealth of these, mostly unpublished, resources is far from exhausted. It is for this reason that HTR and NLP are needed. Books of Hours have been massively digitized because they are often heavily illuminated and of interest to art historians; in spite of this, these books, as texts, are massively underused. By making new use of documents first digitized for other audiences and other purposes, HORAE demonstrates the opportunities created by open-access digitization using the IIIF (International Image Interoperability Framework) protocol   https://iiif.io/ .   Community relevance. The late Middle Ages and Renaissance were times of extreme religious tension within communities. Direct comparison with modern times is of course impossible, and history does not solve current problems. However, studying how religious practices contributed to shape communities and served as common identifiers, and how texts were circulated as a basis for specific religious tendencies, can help produce relevant comparisons. This can inform sociology of religion and is of clear contemporary relevance.    Methods and realizations Based on previous and preliminary works (Stutzmann, 2019; Plummer and Clark, 2016; Baroffio, 2015; Cavet, 2004), we have built a first reference corpus of Books of Hours worldwide, including more than 7,000 items, which will be enhanced during the course of the project (cf. fig. 1). In accordance with previous observations, they mostly originate in Italy, France, and the Low Countries (fig. 2), but their liturgical use shows the extent of the influence of Rome (fig. 3). Fig. 1: Map of 7,000+ preserved Books of Hours (visualized in Dariah-DE GeoBrowser version 2.2.1 using PLATIN ©2018 DARIAH-DE. Open source code)    Fig. 2: Place and date of production of 2,400+ Books of Hours (visualized in Dariah-DE GeoBrowser)    Fig. 3: Liturgical destination of 2,300+ Books of Hours (visualized in Dariah-DE GeoBrowser)    In this corpus, we have selected 500 manuscripts that are available through the IIIF protocol (manifest URLs were collected manually and images accessed for analysis, but not stored in our system). We have successfully applied a “page classifier” to distinguish bindings, blank pages, calendars, full page or half page miniatures, historiated initials, and plain text pages and published the results as IIIF manifests at  https://github.com/oriflamms/HORAE (fig. 4). We then segment the page into lines of text. Next, we “read” or “recognize” the text with the same techniques which were applied successfully in the HIMANIS project and now allow full text search to 199 volumes, the largest corpus of medieval manuscripts yet indexed (Stutzmann et al., 2018). They rely on deep learning (based on the open-source Kaldi library) and a dedicated ground truth (fig. 5).  NLP techniques are then applied to match the automatically-generated text with editions, in order to identify the different texts and parts of texts that are present in the manuscripts. Our automated transcription is sufficiently accurate to create a “mid-level” approach at “text” level and to correctly identify texts despite the remaining errors. The authors develop new techniques, but can also rely on existing software such as Tracer, Collatex and ITEAL (Haentjens Dekker and Middell, 2010; Büchler, 2015; Jänicke and Wrisley, 2018) (fig. 6). In future steps, we will compare the contents of manuscripts in order to understand historical patterns and identify and edit hitherto unknown texts. Fig. 4: Page classifier: ¾ page miniatures and historiated initials from Books of Hours preserved at the Houghton Library, Harvard University (visualized in Mirador IIIF viewer)    Fig. 5: Automated reading of a medieval manuscript as IIIF annotations (visualized in Mirador IIIF viewer)    Fig. 6: Visualization of correspondence between the biblical text of the Psalms and the content of a given Book of Hours (elaborated and visualized in ITEAL, developed by David J. Wrisley and Stefan Jänicke)      Challenges: uncertainty, sequence, and granularity The following methodological challenges should be explained.  Uncertainty. HTR produces an error-prone text. Text identification based on methods of text reuse detection should be able to ascertain the textual source and thus create correct data from fuzzy, incorrect and automatically-generated data.    Sequential text A+B ≠ B+A. Books of Hours present sections whose ordering may represent only a social practice (e.g. “mixed hours”) or may distinguish their “liturgical use”, i.e. their geographical validity and applicability. Most techniques of distant reading tend to erase the sequential order of sentences and words. We thus need accurate sequential tables of contents and to identify which variations are relevant for research purposes.   Granularity. Books of Hours are built as a sequence of many sections with subsections in a highly hierarchical manner (fig. 7), with conflicting logical (text) and physical (page) structures. Any sequence of a coherent section of several texts can be subsumed in the first sentence of the first text, and words can be represented by their initial letters (fig. 8). This circumstance creates numerous ambiguities, and it is also extremely difficult to ascertain if some parts of the text were omitted in writing but supposed to be said or if they are not meant to be there at all. Therefore, in order to establish a correct table of contents, we have to record what is there and what is not, but also to interpret what  should be there.  Fig. 7: Simplified structure of a Book of Hours    Fig. 8: Granularity of textual content       Integrating DH in the humanities Gesamtkunstwerk There are many definitions of DH (Terras et al., 2016). We stress the parallel between DH and “auxiliary” sciences (also called “ancillary” or “fundamental”, in German “Hilfs-”, or “Grund-wissenschaften”). They are diverse, have a large focus on epistemology (source criticism and history of science and scholarly practices), and need specific training, curricula, and data infrastructures. They are also dispersed and taught in several faculties. Equally, they strive for autonomy when their results and tools are used by other disciplines, sometimes without any interest in their construction, relations and inner coherence. HORAE  integrates several fields of research in “digital humanities” with a common goal (image analysis, linguistic computing, text mining, data modelling and visualization). It makes an extensive use of DH-specific methods and tools. Having a strong focus on manuscript studies and text transmission, it combines digital research methods and historical study in a mutually-enriching process. It is therefore connected to both traditional auxiliary/fundamental sciences and DH.   Beyond that, it implements technologies developed without connection to the humanities. The alignment of research questions from different fields makes it possible to have non-humanities and non-DH partners engaged in mutually-beneficial research, in which no partner is only a service provider to the others. As a consequence, it also  disintegrates DH as a specific field by extracting a set of questions, research, and tools, as the new normal in the humanities. With this perspective, we suggest that DH could benefit from analyzing experiences in fundamental sciences to reflect on their fluid position within the humanities.   ",
        "article_title": "Integrated DH. Rationale of the HORAE Research Project",
        "authors": [
            {
                "given": "Dominique",
                "family": "Stutzmann",
                "affiliation": [
                    {
                        "original_name": "Institut de Recherche et d'Histoire des Textes (CNRS), France",
                        "normalized_name": "Institut de Recherche et d'Histoire des Textes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/05evznf71",
                            "GRID": "grid.450127.5"
                        }
                    }
                ]
            },
            {
                "given": "Jacob",
                "family": "Currie",
                "affiliation": [
                    {
                        "original_name": "Institut de Recherche et d'Histoire des Textes (CNRS), France",
                        "normalized_name": "Institut de Recherche et d'Histoire des Textes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/05evznf71",
                            "GRID": "grid.450127.5"
                        }
                    }
                ]
            },
            {
                "given": "Béatrice",
                "family": "Daille",
                "affiliation": [
                    {
                        "original_name": "Laboratoire des Sciences du Numérique de Nantes (Université de Nantes), France",
                        "normalized_name": "University of Nantes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/03gnr7b55",
                            "GRID": "grid.4817.a"
                        }
                    }
                ]
            },
            {
                "given": "Amir",
                "family": "Hazem",
                "affiliation": [
                    {
                        "original_name": "Laboratoire des Sciences du Numérique de Nantes (Université de Nantes), France",
                        "normalized_name": "University of Nantes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/03gnr7b55",
                            "GRID": "grid.4817.a"
                        }
                    }
                ]
            },
            {
                "given": "Christopher",
                "family": "Kermorvant",
                "affiliation": [
                    {
                        "original_name": "Teklia",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-08",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "theory and methodology)",
            "English",
            "OCR and hand-written recognition",
            "medieval studies",
            "digital archives and digital libraries",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Though the Text Encoding Initiative (TEI) has long supported the use of the XML fragment linking mechanism known as XPointers (Grosso et al. 2003; TEI 16), they are seldom implemented or recommended as a method of linking TEI documents. Hugh Cayless has argued that TEI pointers are an underappreciated mechanism, the “victim of a Catch-22”: the specifications are “difficult to grasp, and as a result, most people never bother to try using [them]” (Cayless 2013). Though Cayless’ proposals for new definitions of TEI pointers are compelling, five years later, his diagnosis holds true: the definitions of the pointing schemes have not been updated and there is still a paucity of examples of XPointers in TEI projects. This short paper responds to Cayless by outlining the use of TEI pointers in the Waterloo-based Stratford Festival Online (SFO) project, which aims to encode the Festival’s world-class collection of theatrical promptbooks (Malone 2013; 2018) in TEI. Through this real-world example of the necessity of XPointers in the SFO project, we make the case that they may also be a viable option for other TEI projects.  Promptbooks are difficult objects to encode because they are simultaneously both textual artifacts and also instructions for actualizing the aesthetic decisions that define a particular production of a play (Bauman 2001; Kaethler, Roberts-Smith, and Malone 2017). Over the course of a rehearsal process, the stage manager creates a book to remind themselves what they will need to do during each performance (calling technical cues, monitoring actors’ performances, etc.) to ensure the performance is consistent (Shattuck 1965). The stage manager records the “what” of the book (what the stage manager needs to do) along a timeline showing “when” each action needs to happen; the “when” recorded in the book takes the form of words spoken by actors. The text of a promptbook is a performance timeline along which the events of a performance are sequenced for the stage manager’s reference. Promptbooks therefore contain three ontological categories of content: 1) a timeline (words spoken by actors in a play); 2) descriptions of performance events; and 3) annotations made by the stage manager, which link events to the moments on the timeline when each should occur (Roberts-Smith, Kaethler, Malone et al. 2017; Roberts-Smith, Kaethler, Malone et al. forthcoming). Current TEI Guidelines for “Drama” cannot accommodate this ontological complexity (TEI 7; Lavaigno and Mylonas 1995; Bauman 2001; Kaethler, Roberts-Smith and Malone 2017; Roberts-Smith, Kaethler, Malone et al. forthcoming). To complicate matters further, stage managers use a range of idiosyncratic marks to link performance events to moments in the textual timeline, and these “moments” may in turn be literally moments, as brief as the space between two syllables pronounced by an actor, or may conversely be periods as long as several lines of spoken text (Kaethler, Roberts-Smith and Malone 2017; Roberts-Smith, Kaethler, Malone et al. 2017; Roberts-Smith, Kaethler, Malone et al. forthcoming; see figure 1).    Figure 1. Some systems for linking events to moments in the timeline of spoken text in the promptbook from the Stratford Festival’s King Lear (1988).  In response to these challenges, our research team is developing an approach to promptbook encoding that uses two data files (one for the performance text and the other for non-verbal performance events [Roberts-Smith, Kaethler, Malone et al. forthcoming]) that are linked by stand-off markup (the advantages and disadvantages of which we do not summarize here; see TEI 16.9; Bański 2010) and XPointers. Our choice to use XPointers derives from earlier experiments with standard techniques of standoff markup. We initially attempted to use <anchor> elements, repurposing the TEI’s double end-point-attachment method (TEI 12.2.2). However, this approach makes the event document unusable without the text document. Since one of the project’s goals is to map events to other electronic versions of the same base text, we rejected <anchor> elements. We also considered tokenizing each character of the text document so that an encoder could specify a range of characters (encoded using the <c> element). This method does not require an encoder to add elements manually to the performance text document and could enable the creation of user-friendly markup interfaces. However, linking to individual <c> elements in the text document requires that the performance text be edited completely before running a tokenizing algorithm, whereas the project’s workflow requires encoding the text and event documents simultaneously; and adding <c> elements significantly increases the size of the document, makes it difficult for human encoders to read and seriously inhibits any further editing of the document.  By contrast, the TEI pointer framework provides a non-intrusive and event-centric method of addressing segments of the text document, because it does not rely on the encoding of the text document. There are multiple TEI pointer schemes that can handle stable element ID references, character position references, and string matching. To be sure, using TEI pointers in a standoff document is not a mutually exclusive practice with encoding <anchor>s or tokenizing the source text; instead, TEI pointers offer the project further options for addressing segments of text beyond ID references, including XPath, string matching, range selection, and left and right point selection. For instance, TEI Pointers can take advantage of the match() feature, which allows encoders to specify a string to address based off a canonical reference (for example, canonical line numbers) that two projects share as an interoperable data-point, like a canonical line number. This is not to say that TEI Pointers are inherently more interoperable than other methods,but, instead, provide another mechanism for addressing contiguous and adjacent nodes, which is difficult to do purely with IDREFS.  Consequently, we are developing a method, using Apache Ant and an XSLT function library, of parsing TEI pointers and associating the text document with the standoff markup. Since our approach to resolving XPointers requires a two stage process, this paper does not describe how to resolve XPointers in standard “just-in-time” setups; instead, we demonstrate a method of handling XPointers that can be integrated as an intermediate process in a static build pipeline, the infrastructural benefits for creating sustainable and archivable digital projects will be dealt with in further detail elsewhere. Since XPointers address segments that are not necessarily stable or canonical and links between the standoff document and the source document can easily break, we have also developed a suite of diagnostic tests to ensure that the XPointers used in the project are, at minimum, resolvable (Holmes and Takeda 2019). Both the sample implementation and the diagnostic code, along with documentation discussing the development of and differences between XPointers and TEI Pointers, will be available via Github as a proof-of-concept for the feasibility of using XPointers within TEI projects. We hope that other projects—particularly those dealing with standoff annotation structure or other situations where the source text is not available for editing—can make use and, ultimately, improve the TEI Pointer system for use across various domains. ",
        "article_title": "One More Time With Feeling: Revisiting XPointers to Address the Complexities of Promptbook Encoding",
        "authors": [
            {
                "given": "Joey",
                "family": "Takeda",
                "affiliation": [
                    {
                        "original_name": "University of British Columbia, Canada",
                        "normalized_name": "University of British Columbia",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03rmrcq20",
                            "GRID": "grid.17091.3e"
                        }
                    }
                ]
            },
            {
                "given": "Jennifer",
                "family": "Roberts-Smith",
                "affiliation": [
                    {
                        "original_name": "University of Waterloo, Canada",
                        "normalized_name": "University of Waterloo",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01aff2v68",
                            "GRID": "grid.46078.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "digital humanities (history",
            "linking and annotation",
            "scholarly editing",
            "theory and methodology)",
            "English",
            "film and performing arts studies",
            "text encoding and markup languages",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The goal of this project is the preservation and dissemination of Guatemala’s colonial musical heritage by applying music information retrieval tools to a group of Guatemalan manuscript sources while maintaining the original sources in their homeland. This group of sources is written in mensural notation, a music notation style used in Europe throughout the Late Middle Ages and the Renaissance. The only known Guatemalan mensural repertoire is contained in two extant manuscript sources: (i) the Santa Eulalia manuscripts (US-BLI), copied in the late sixteenth century; and (ii) the Metropolitan Cathedral choirbooks (GCA-Gc), copied during the sixteenth to eighteenth centuries. The Santa Eulalia manuscripts, a collection of fifteen books originally found in a north-west region of Guatemala, are no longer in the country. Therefore, the set of choirbooks kept in the Metropolitan Cathedral of Guatemala City contains the only mensural repertoire that remains in the country, and so it is of utmost importance to treasure and preserve them for future access. In this paper, we will explain the different steps that will be taken to guarantee the preservation and accessibility of the Guatemalan Cathedral choirbooks. The musical heritage from the colonial period of Latin American countries is not well known around the world. Even though colonial music is rooted in the early Western music traditions (i.e., Medieval, Renaissance, and Baroque music), little is known about how these traditions and repertoires evolved in the Spanish colonies of the Americas. The lack of high-quality digitization of music documents from these countries and era limits access to people able to visit the site that has the original sources. Furthermore, the use of old music notation restricts performance of the music to the knowledgeable experts. In this paper, we will use recent music encoding technologies to address the accessibility issues of the mensural colonial repertoire of the Americas. The barriers to accessibility in the body of Guatemalan mensural music can be summarized into: (i) the lack of high-quality digital images, (ii) the notation style, and (iii) its layout. Firstly, high-quality images are necessary for the music to be readable. While low-quality images can still be read by humans, high-quality images are necessary to encode the music in an automated way using techniques such as Optical Music Recognition (OMR). Furthermore, high-quality images allow for digital restoration, enhancing details nearly invisible in the original sources. Secondly, mensural notation differs significantly from our current music notation system in that the duration of notes is not absolute, but rather context-dependent. This characteristic makes it impossible for non-experts to accurately read this music. Lastly, the parts corresponding to each voice (i.e., soprano, alto, tenor, bass) are written in separate areas of the page instead of being vertically aligned as in a modern score format. Together the context-dependent nature of the notation and the separate-parts layout hinder the appreciation of the polyphonic texture of the music. It is only until musicians acquainted with the notation sing the various parts together or until an expert transcribes the music into a modern score that these textures can be really perceived and enjoyed. The layout of the original music deters its study even for experts, because it is hard to visualize the vertical relationships between notes sung simultaneously in two different voices, something that only becomes clear when the music is presented in score format.  In this paper, we will present the step-by-step process that will unravel the accessibility barriers of this repertoire, resulting in the digitization and encoding of the repertoire as musical scores in a machine-readable file format. This procedure involves the use of the following tools:  A Do-It-Yourself (DIY) book scanner. Given their antiquity, these books cannot be digitized face-down using a common flatbed scanner. On the contrary, they need to be digitized from the top using a “book scanner” and, ideally, using a v-shaped book cradle to lower the stress on the book spine. Currently, there is no book scanner in Guatemala that has the appropriate dimensions to digitize these books. Since buying a large book scanner is out of our possibilities, we will build our own by emulating their book cradle, lights, and camera configuration.  Optical Music Recognition (OMR) software applications. Just as Optical Character Recognition (OCR) allows the machine to read the characters written in the page, OMR systems read the music symbols from a music document. OMR software that will be used to read and encode the sources include:  Pixel.js (Saleh et al., 2017),  Gamera (Droettboom et al., 2003) and a machine learning model developed by Pacha and Calvo-Zaragoza (2018) that was trained with Spanish mensural music. The resulting data from the OMR process will be encoded in the MEI (Music Encoding Initiative) format (Roland, 2002).  The mensural notation  Scoring-Up Tool (Thomae, 2017). This set of scripts convert the part representation of the score into a modern rendering, providing a more conventional and modern representation to musicians and lay people.    We hope that the digitization and encoding of the music documents with the aforementioned techniques will allow for the preservation and dissemination of Guatemalan mensural music—a historic musical heritage that otherwise may be lost, forgotten, or damaged. The result should facilitate its dissemination, study by musicologists, and appreciation by the general public, especially for repertoires that are virtually unknown abroad. Furthermore, we expect this research to be used as a model for the digitization of the mensural repertoire of other countries that were once part of the colonial past, such as the Spanish Colonial Empire.  Music Manuscripts  US-BLI: Bloomington, Indiana University, Lilly Library MS 1–15. GCA-Gc: Guatemala City, Catedral, Archivo Capitular 1–4. ",
        "article_title": "Taking Digital Humanities to Guatemala, a Case Study in the Preservation of Colonial Musical Heritage",
        "authors": [
            {
                "given": "Martha E.",
                "family": "Thomae",
                "affiliation": [
                    {
                        "original_name": "CIRMMT, McGilll University, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Julie E.",
                "family": "Cumming",
                "affiliation": [
                    {
                        "original_name": "CIRMMT, McGilll University, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ichiro",
                "family": "Fujinaga",
                "affiliation": [
                    {
                        "original_name": "CIRMMT, McGilll University, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-14",
        "keywords": [
            "musicology",
            "sustainability and preservation",
            "OCR and hand-written recognition",
            "English",
            "text encoding and markup languages",
            "library & information science",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The complexities of the relationship between Europe and America—historical, commercial, ideological, cultural—are so manifold that digitally rendering them may seem quixotic. Yet our mobile software application Amerigo sets out to do just that: explore meaningful, multi-layered connections through space and time between the Netherlands, America, and the broader Atlantic World. In its current phase, Amerigo uses a single location as its jumping-off point—the city of Groningen in the north of the Netherlands—and maps its role in Atlantic colonial networks during the Dutch “Golden Age.” It charts these relationships through an interactive digital narrative interwoven into the space of the city itself. The lead character in Amerigo’s first storyline is Rock Brasiliano, a Groninger who went to Dutch Brazil in the mid-seventeenth century and then became one of the archetypal “pirates of the Caribbean” (Exquemelin, 1678). As users of Amerigo walk through the city, they will also encounter other memorable characters from the period, including a crypto-Copernican scientist who helped to found the University of Groningen; his mother, a Mennonite martyr; rival botanists arguing over West Indian specimens; shopkeepers vending tobacco from the English Chesapeake; and a Tupi Indian from Brazil training to become a Reformed minister. These characters—all based on real historical individuals—vividly illustrate Groningen’s place in a multi-nodal Atlantic network. Indeed, this complex set of relations, seemingly too vast for one person’s grasp, becomes tangible and comprehensible as Amerigo’s users choose their way through the story-space of the city. Why is early modern Groningen an especially promising locale for such an experiment? In the last twenty-five years, study of the Atlantic World as a complex, interconnected space has drawn together once disparate histories of early modern Europe, the Americas, the Caribbean, and West Africa (Greene and Morgan, 2009). At the same time, scholars and activists in Europe have brought increased attention to the colonial past embedded within their own countries and cultures, especially with regard to slavery and the slave trade (Oostindie, 2011). Meanwhile, heritage institutions have begun to highlight dimensions of this past, as well—often sparking controversy as triumphal accounts are challenged or replaced by complex, critical, multi-centered narratives. Finally, individuals and institutions are developing new forms of digital heritage through websites, games, and tours that shed light on these historical connections (Hondius et al., 2014; Fokken and Henkes, 2016). History and heritage, always closely linked, have only become more tightly connected as they move into the digital sphere. How these producers, consumers, and “pro-sumers” of digital history, culture, and heritage interact, however, is an open question (cf. Poole, 2018; Hagedoorn and Sauer, 2019). A key location for all of these trends and processes is the contemporary city, which serves at once as a site for the production of modernity and a monument to its lost past (Benjamin, 1999). Human geographers, urban planners, and cultural theorists have long employed qualitative and quantitative methods to examine how people use, understand, and construct urban space in time (Lynch, 1960, 1972, 1990). In more recent years, media scholars have brought new attention to mobile narrative and locative media as subjects of study (Farman, 2014), and heritage research has focused on the significance of contemporary cities as locations for the production, consumption, and study of heritage (Ashworth et al., 2007; Sen and Johung, 2013). Meanwhile, the efforts of cycling and pedestrian advocates have converged with cities’ attempts to promote urban tourism and led to more walkable, bikeable urban centers that lend themselves to new (and old) forms of historical exploration and cultural experience (Middleton, 2016). Groningen, a busy, dense university town interlaced with shopping streets and bike paths, turns out to be an excellent location for the study of all of these domains. In 2019 our Groningen-based consortium—consisting of academic researchers, multiple heritage institutions, and a private app developer—began building Amerigo. The application will serve different groups in different ways. As a mobile tour app produced by a commercial software company, Amerigo will enable a broad audience of users to explore the city-space of Groningen through its complex historical relationships with the Americas and the Atlantic World and to see its present and past in a new light. As a platform for the production and distribution of historical and cultural research, Amerigo will provide scholars and students with a central portal for connecting to archival collections and resources, conducting collaborative research and teaching, and sharing their complex results with the public through interactive narratives. And as a tool for the study of user behavior in relation to digital heritage, Amerigo will allow the members of the consortium to study the complex problems of how different audiences move through the space of the city, how they engage with (and contribute to) the curated content, and how they connect with one another through the application. It is here in this last area—measuring and testing how users interact with a digital heritage platform linked to a real urban space—that we foresee having the most significant impact in the digital humanities. Amerigo offers a new way of exploring these complexities together. In our short paper we discuss the preliminary findings from our project up to the present. This discussion will include a brief overview of the paths we have taken (including some dead-ends) up to the current state of the application as of July 2019. We will also discuss our efforts to integrate development of the app with an experimental undergraduate course that was led by one of the researchers in the spring of 2019. By the time of the conference, we aim to have an early version of the application available for testing, and we are eager to receive feedback from attendees. Ultimately, our paper is intended not only to offer a portrait of a digital heritage project in progress but also to foster creative and critical thinking about how we might use digital methods to engage with our complex past. ",
        "article_title": " Atlantic Journeys through a Dutch City's Past: Building a Mobile Platform for Urban Heritage  ",
        "authors": [
            {
                "given": "Mark L.",
                "family": "Thompson",
                "affiliation": [
                    {
                        "original_name": "University of Groningen, Netherlands, The",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            },
            {
                "given": "Joanne",
                "family": "van der Woude",
                "affiliation": [
                    {
                        "original_name": "University of Groningen, Netherlands, The",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-29",
        "keywords": [
            "English",
            "pedagogy",
            "cultural studies",
            "teaching",
            "literary studies",
            "mobile applications and mobile design",
            "and curriculum",
            "virtual and augmented reality",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  From ‘Bitchcoin’ and ‘CryptoKitties’ to distributed ledgers for nuclear non-proliferation, feverish adoption and experimentation with blockchain technology is matched only by the promissory hype that accompanies it (Gloerich, Lovnik, et. al., 2018). This paper presents the aims and background of  Blocumenta - an experimental blockchain-based contemporary arts project that deploys creative methodologies to investigate the poetic potential of blockchain, while exploring alternate models for funding, archiving and historicising the creative practices of our era.  Since the Satoshi Nakamoto Bitcoin White Paper in 2008, blockchain has ignited public debate, carving out a distinctive place in the cultural imaginary (Catlow, 2018) . However, current research recognises that the celebrationist discourse surrounding ‘the Blockchain Revolution’ ( Radziwill, 2018) must be matched with rigorous critical interrogation to ensure the positive development of this technology for human societies. The  Blocumenta project thus distances itself from the crypto-libertarian blockchain evangelism that is commonly associated with this field of tech development (Golumbia, 2016). Instead it builds upon research emphasising the cooperative properties of this technology, and its potential to enable decentralized governance of community assets (Scott, 2016).  Departing from an initial stage of creative engagement and aesthetic experimentation across local arts and tech communities in Sydney and the Yirrkala (Australia), São Paulo (Brazil) and Guangzhou (China),  Blocumenta interrogates whether a distributed, autonomous and trustless contemporary art archive can in fact overcome the limitations of centralised and often exclusionary art historical narratives, or whether the cultural and historical biases simply find new iterations and determinations that can be expressed through this emergent technology.  Blocumenta does this through the design and implementation of a distributed autonomous arts organisation (DAAO), that prototypes:  [1] A new technical and economic model for cultural exchange addressing material inequalities that pervade artistic production, using a cryptocurrency crowdfunding structure that builds upon existing financial, social and cultural applications of blockchain, including Bail Bloc (The New Inquiry, 2017) and Game Chaingers (UNICEF, 2018); [2] A distributed application (DApp) and blockchain architecture for artists to document, present and archive their cultural practices, cooperatively governing this cultural asset that serves as an alternate resource to circumvent the digital clutter and corporate biases of the Web 2.0. A future-oriented cultural heritage project,  Blocumenta distinguishes itself from current explorations into blockchain’s promise as a tool to revolutionize artistic practices and communities, as seen in programs delivered by EU funded initiatives such as DAOWO (Myers 2015) and State Machines (De Vries, 2018). Its distinction is found in a commitment to decentralizing the historic eurocentrism of ‘global contemporary art’, and in arguing that conversations regarding the cultural development of Blockchain as a decentred technology should equally strive to realise a globally decentred human approach to cultural heritage.  Likewise,  Blocumenta serves as a counterpoint to long standing approaches to the digital museum, which are underpinned by an ethos of open access to data, as exemplified by institutions such as SFMOMA (Winesmith & Carey, 2014) . Rather than providing a window into the content and organisation of traditional collecting bodies,  Blocumenta intervenes in the composition of an immutable archive governed by artist collectives based across the Global North/South divide, experimenting with archival protocols and formats that challenge the stasis and rigidity long associated with the museum as mausoleum (Adorno, 1967). Through its decentralized governance structures, it raises questions such as: What kind of work merits archiving? How could the immutability of blockchain archiving provoke a different kind of digital activity, through its ‘trustless’ accountability? How might such technical architectures create distinctive exhibition, performance and reception spaces for contemporary art practices, from new media to performance to literature?   Finally, in this paper we will consider how  Blocumenta exemplifies a shift away from a digital age characterised by an ethos of free and open access to content that is, in reality, underpinned by hidden centralised informational systems mediated by corporate interlocutors. Building upon Birchall’s discussions of ‘shareveillance’ (2016) and Glissant’s concept of ‘opacity’ (2009), we will question whether digital systems in which archival participation and access must be ‘earned’ through the cooperative labour of a decentralized community of stakeholders, may be an appropriate antidote to the implicit power dynamics of traditional archives, as well as the chaotic transience of Web 2.0 digital culture.   Subverting the original aims of the quinquennial survey of contemporary art  Documenta (to document significant examples of modern and contemporary art in order to overcome Germany’s cultural isolation after World War II),  Blocumenta challenges the homogenising and colonial processes associated with the encyclopaedic museum, archive and exhibition, envisaging new forms of cultural heritage grounded in dynamic collaboration and equitable exchange between members of a globally distributed community.   ",
        "article_title": "Blocumenta: An Experimental Art Project on the Blockchain",
        "authors": [
            {
                "given": "Denise",
                "family": "Thwaites",
                "affiliation": [
                    {
                        "original_name": "University of Canberra, Australia",
                        "normalized_name": "University of Canberra",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04s1nv328",
                            "GRID": "grid.1039.b"
                        }
                    }
                ]
            },
            {
                "given": "Baden",
                "family": "Pailthorpe",
                "affiliation": [
                    {
                        "original_name": "Australian National University, Canberra",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-22",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "interdisciplinary & community collaboration",
            "theory and methodology)",
            "English",
            "globalization & digital divides",
            "digital archives and digital libraries",
            "digital art"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " DH2019’s theme Complexities asks the field “to focus on DH as the humanist way of building complex models of complex realities, analysing them with computational methods and communicating the results to a broader public.” Understanding the diversity of the human experience means turning to diverse sources, which includes the different forms in which people produce knowledge and communicate ways of knowing. One way that people make and communicate meaning is through audio and visuals. Forms such as drawing, film, and radio construct, convey, and circulate our complexities including beliefs, values, and histories. Analyzing these forms has become an increased object of study within DH; a field which has traditionally focused on text. With this move from the periphery to the center of audio and visual (AV), scholars are turning to computational methods to increase the discovery, analysis, and accessibility of AV materials.  The AVinDH SIG Workshop will bring together DH scholars interested in learning how to analyze AV materials. The day-long workshop will include: 1. Hands-on Sessions that share approaches and methods for computational image and audio analysis. The workshops will cover approaches to computational analysis of audio and visual. The workshops are designed to offer strategies across the analysis pipeline from collecting data (Tutorial: Collecting YouTube Data and Tracking Reuse of Public Broadcasters), assessing the quality of AV data (Tutorial: CLARIAH Media Suite and Data Transparency), video annotation (Tutorial: Semantic Annotation Tool) and image analysis (Tutorial: Distant Viewing with the DV Toolkit). Two sessions will be running simultaneously. The AVinDH SIG decided to include two tutorials with tools developed by CLARIAH since the conference is located in the Netherlands. This will allow multiple members of the teams who have built these tools to attend, which would be cost prohibitive if all of the members had to travel, and support DH members who are interested in learning about these tools. The other two sessions are from scholars based in the United States.  2. Lightning Shorts where participants can share briefly share projects, with a focus on works in progress. The Lightning Shorts session will focus on current research in AV DH. Each participant will have three-minutes to introduce their work. The goal of the session is to share a range of work in the field in the same space so that participants can quickly become aware of each other’s work as well as identify those their scholarship is in conversation with. It also allows more participants than traditional models like paper sessions. Participants will be invited to register ahead of time and also will be able to sign-up the day of the workshop if there are time slots remaining.  3. Closing Session that provides a recap of the day. The AVinDH SIG Workshop builds off of several years of effort to center AV in the field. In Lausanne (2014), Krakow (2016) , and Montreal (2017), the AVinDH SIG led an all-day mini-conference. The workshops were primarily a conference format in which participants submitted proposals, were reviewed, and then selected to present their research. Attendance has steadily increased. The AVinDH SIG is now shifting toward hands-on workshops. DH scholars have been forging new computational approaches and building software as well as platforms to facilitate computational analysis of AV due to the rapid advancement in deep learning and computer vision. Due to this shift, the AVinDH SIG offered a half-day workshop on computer vision with images in Mexico City (2018). The sold-out workshop indicated significant interest in hands-on workshops. For 2019, AVinDH SIG is looking to blend the two models. We are proposing a day-long workshop for 2019 because workshops require significant time to be effective; there has been an exciting increase in DH tools and software in this space; and, the SIG would like to support work in audio and visual analysis. The lightning shorts still offer a space for people to present their research and share projects with the community. The closing session will be a recap of the day and closing remarks.  Schedule: https://avindhsig.wordpress.com/workshop-2019-utrecht/ ",
        "article_title": "AVinDH SIG Workshop",
        "authors": [
            {
                "given": "Lauren",
                "family": "Tilton",
                "affiliation": [
                    {
                        "original_name": "University of Richmond, United States of America",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            },
            {
                "given": "Jasmijn",
                "family": "van Gorp",
                "affiliation": [
                    {
                        "original_name": "Utrecht University",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "multimedia",
            "communication and media studies",
            "English",
            "film and performing arts studies",
            "cultural analytics",
            "audio",
            "image processing",
            "video"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Eververse is a project which synthesises perspectives from disciplines in the humanities and sciences to develop critical and creative explorations of poetry and poetic identity in the digital age. Deploying tools and methods from poetic theory, data analysis, and Natural Language Generation (NLG): the automatic production of natural language output from a non-linguistic data source (Reiter and Dale, 2000).  Eververse uses data from quantified self (QS) devices to automatically generate and publish poetry which correlates to the wearer/poet’s varying physical states.    Context  One of the more common ideas to be found in different statements and theories of poetry presents the poet as a creative vessel or conduit, admitting the sensory input of the world into their bodies and minds, and producing poetic output in turn. For Walt Whitman, every atom of being and sensation was an appropriate inspiration for poetry, and his works exhaustively catalogued the varieties of personal and public experience in nineteenth-century America. In poetics such as this, art increasingly collapses into the being and identity of the artist. Poet W. B. Yeats famously articulated this conundrum, asking “How can we know the dancer from the dance?” The production processes of print, which intervened between the poet and their desire for immediate and spontaneous expression, are circumvented in the digital age, as poets use the internet and social media to create a poetry that is “vast, instantaneous, horizontal, globally distributed, paper thin, and, ultimately, disposable” (Goldsmith, 2016: 195). The internet is shaping contemporary poetry with these characteristic forms, and by incorporating the modalities of images, gifs, video, and sound.  Eververse seeks to explore networked technologies and their affordances to articulate new and novel means of being a poet in the digital age.     Project description   Eververse sends biometric data from a Fitbit fitness tracking device worn by the project PI/poet to its custom-built poetry generator. This generator utilises NLG techniques to output poetic text published in real time, and 24/7, on the  Eververse website.  The form and content of the poetic output is designed to change according to different physical sensations and experiences in the poet’s waking and sleeping life. Following Charles Olson’s injunction that “the line comes...from the breath, from the breathing of the man who writes” (Olson, 2008),  Eververse’s poetic lines decrease in length as the poet’s heart rate increases and breath contracts. Similarly, the response to the randomness of the dream sleep (REM) state is an increased irregularity in the poetic form. Content, too, reflects these variations, as heightened-sentiment vocabulary is produced to reflect the emotional intensification of an increased heart rate, while the dream sleep (REM) state generates  surreal images and vocabulary.     Technical description  The  Eververse application consists of three main modules. The first module interfaces with the Fitbit device and its data through its Application Programming Interface (API). The activity data of the poet wearing the device is then sent, in JSON form to the NLG module referred to as the ‘generator.’ This generator carries out a number of steps in order to generate and return a poetic couplet based on a conceptual model of states based on the activity information contained within the passed JSON data. The number of words and the frequency of the generated couplets correlate with the heart rate of the poet, whereas the textual content of the couplet is generated from the input corpus which is fed to the generator. Thematic links exist between the content of the input poetic corpora and the conceptual topics of physiology (broadly), and sleep and the body (specifically) corpus comprises poems on the topic of the body; all poems are previously published and none is composed by the  Eververse poet. In order to disassemble and reassemble the corpora for publication, they are arranged in a reverse ngram matrix and further shaped into a frequency lookup table by Poesy, a Markov Model-based Natural Language Poetry Generator. The lookup table is used to create verse lines and a python library,  Pronouncing, is deployed to rhyme the verses. In short, our method takes a language model approach similar to (Barbieri, G., et al., 2012) although we do exploit some semantics, specifically alignment of couplets with Fitbit activity states.   The generator is written mainly in the Python programming language using the micro web framework, Flask. It consists of a web interface to display the generated poetry and an administrator interface that is used to define heart rate parameters for different zones and to determine the form and content of the verse that corresponds to these zones. The extensible approach we use to build the poetry generator means the project can easily incorporate additional biometric data types along with their associated corpora in future. The public user interface created to display the generated poetry relies on a number of Open Source JavaScript libraries. These libraries enable display of generated text (Handlebars.js, Textillate.js) and retrieval of data from the web application’s API and user interface animations (jQuery). The dynamic background images are created in realtime, and utilise the activity data as an input to affect their appearance, representing a visual correlate to the generated poetry. Multiple versions of this interface were created for deployment on the web, in a live performance environment, and for display in a standalone exhibition setting. Each interface was adapted to take into account the context in which it would be experienced, for example, differences in how, or if, user interaction was required, and addressing the differing requirements for text size, line spacing, and overall page layouts.    Conclusion  The presentation of this paper will report on the technical work completed to develop Eververse, while reflecting on the implications of the project for issues in poetics, authorship, and automated literature generation. In addition, the presentation will describe deployments of the project in web, exhibition, and live contexts, concluding with a brief live demonstration. Accessible supporting materials for the conference presentation will be made available in English, Irish, Italian, and Urdu .   ",
        "article_title": "Poetry In Motion: Quantified Self Data And Automated Poetry Generation",
        "authors": [
            {
                "given": "Justin",
                "family": "Tonra",
                "affiliation": [
                    {
                        "original_name": "National University of Ireland Galway",
                        "normalized_name": "National University of Ireland, Galway",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/03bea9k73",
                            "GRID": "grid.6142.1"
                        }
                    }
                ]
            },
            {
                "given": "Brian",
                "family": "Davis",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Kelly",
                "affiliation": [
                    {
                        "original_name": "National University of Ireland Galway",
                        "normalized_name": "National University of Ireland, Galway",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/03bea9k73",
                            "GRID": "grid.6142.1"
                        }
                    }
                ]
            },
            {
                "given": "Waqas",
                "family": "Khawaja",
                "affiliation": [
                    {
                        "original_name": "Insight Centre for Data Analytics, Galway",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "natural language processing",
            "English",
            "electronic literature",
            "literary studies",
            "english studies",
            "embodied & haptic technologies; wearable computing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The impact of climate change has become more and more obvious. How to understand its cause and effect to find a way to deal with it has become an important research topic. To trace the occurrence and impact of climate disasters, many clues can be found in the rich records left by historical materials. \"China's Three Thousand Years of Meteorological Records\" (2004) extracts meteorological descriptions from 8,228 historical sources and organizes these descriptions by regions and dates. This important work contributes to the analysis of the temporal and spatial characteristics of meteorological phenomena. The \"East Asian Historical Climate Database\"(P.-K. Wang et al., 2018) is compiled based on chorographies and official histories. This study develops an event classification method based on the meteorological records in the early Qing Dynasty (1644-1795) in this database. By representing classical Chinese texts into word embedding vectors and the k-means algorithm, we overcome the difficulty of analyzing classical Chinese and not having enough training data. We then integrate the classification results with the map and timeline to develop a Spatio-Temporal search interface, which facilitates climatologist to access and analyze data according to the three dimensions of time, area and event categories.   Methodology The main task of this study is the meteorological text classification, which is composed of three steps: preprocessing, text representation generation, and k-means clustering. We use 36,123 historical meteorological descriptions in the \"East Asian Historical Climate Database\" as the input data. As shown in Table 1, each record contains six fields.  To make meteorological data more suitable for machine learning semantics and classification of meteorological events, we first pre-process meteorological data, including (1) replacing GanZhi, year, month and number with  G,  Y,  M, and  N, respectively (2) removing place names and punctuation symbols because they have nothing to do with classification.  Then, we use the word2vec algorithm to convert each meteorological record written in classical Chinese into 200-dimensional embedding vectors and then use the k-means algorithm to divide all embedded vectors into k groups. We use the validation set to find the most suitable k value of 45, and evaluate the clustering results with the labeled 9,530 records. The overall accuracy is 82%. Table 2 details the 45 clusters. From Table 1, we can see that many clusters contain the same climatic events, but after careful examination, these groups are still slightly different. Taking clusters 2 (Table 3), 29 (Table 4) and 37 (Table 5) as examples, although all three clusters can be roughly classified as flood hazards, it is found that most of the texts of cluster 37 refer to seasons. The records of cluster 29 mostly refer to building damage, while the records of the other two groups do not. Table 1. The table of the East Asian Historical Climate Database   ID year province  county/ city   meteorological description source   1795-11 1663 Anhui Province Guichi County  秋，池州大水，城井行舟。【鄉父老云：與明萬曆三十六年水相似。】 Flood in Chi-zhou in autumn. Villagers said, \"The flood is similar to the one happened in Wanli period in Ming Dynasty (1608).\"   Volume twenty-nine of \"Chi-zhou Local Gazetteers\", published in Kang-xi period (1711) in Qing Dynasty   1795-12 1663 Anhui Province Huangshan City   七月望，邑令陳恭備建學官，是日雷雨大作，千山如注，田間水數尺，忽浮飛木於縣東門。  In mid of July when Chen Gong, the county magistrate, was going to build a school, a thunderstorm happened. Flood was a few feets high, and floodwoods flew at the east gate of the county.   Volume eight of “Tai-ping County record”, published in Chia-ching period in Qing Dynasty    1795-13 1663 Anhui Province Shitai County  秋大水，父老云：與萬曆三十六年水勢相似。 Old man commented about the autumn flood: “It was said to be similar to the one happened in the Wanli period in Ming Dynasty (1608).”   Volume two of \"Shi-di Local Gazetteers\", published in Kang-xi period in Qing Dynasty   1795-14 1663 Anhui Province Dongzhi County  秋大水，十一月始退。 The autumn flood began to recede in November.  Volume seven of \"Dong-Liu Local Gazetteers\", published in Qian-long period in Qing Dynasty   1795-15 1663 Jiangxi Province Jiujiang  大水，潰堤數處，禾黍盡沒。 Flood broke levees and crops were all be drowned.   Volume fifty-three of \"De-Hwa Local Gazetteers\", published in Tong-Zhi period in Qing Dynasty   1795-16 1663 Jiangxi Province Ruichang County  秋八月，大水入城。 Flood crashed the city in August t  Volume one of \"Rui-Chang Local Gazetteers\", published in Kang-xi period in Qing Dynasty   Table 2. The 45 clusters and their semantics    Cluster_0: Crop Failure   Cluster_15: Abnormal Animal Behavior   Cluster_30: Social Problem     Cluster_1: Wind   Cluster_16: Drought   Cluster_31: Rain     Cluster_2: Flood   Cluster_17: Social Problem   Cluster_32: Famine     Cluster_3: Crop Failure   Cluster_18: Crop Failure   Cluster_33: Flood     Cluster_4: Insect   Cluster_19: Crop Failure   Cluster_34: Crop Failure     Cluster_5: Rain   Cluster_20: Drought   Cluster_35: Crop Failure     Cluster_6: Flood   Cluster_21: Undetermined   Cluster_36: Rain     Cluster_7: Famine   Cluster_22: Social Problem   Cluster_37: Flood     Cluster_8: Social Problem   Cluster_23: Famine   Cluster_38: Undetermined     Cluster_9: Social Problem   Cluster_24: Rain   Cluster_39: Flood     Cluster_10: flood Cluster   Cluster_25: Disease   Cluster_40: Drought     Cluster_11: Flood   Cluster_26: Crop Failure   Cluster_41: Rain     Cluster_12: Flood   Cluster_27: Insect   Cluster_42: Crop Failure     Cluster_13: Crop Failure   Cluster_28: Wind   Cluster_43: Rain     Cluster_14: Social Problem   Cluster_29: Flood   Cluster_44: Crop Failure    Table 3. Examples of records in Cluster 2.    廣東省，大水 Guangdong Province, Great Flood.     安徽省，大水 Anhui Province, Great Flood.     河北省，大水 Hebei Province, Great Flood.     福建省，六月，大水 Fujian Province, June, Great Flood.     江西省，七月，大水 Jiangxi Province, July, Great Flood.     江西省，七月十五，大水 Jiangxi Province, 15th July, Great Flood.     江西省，七月，大水 Jiangxi Province, July, Great Flood.     湖南省，五月，大水 Hunan Province, May, Great Flood.     湖南省，五月，大水 Hunan Province, May, Great Flood.    Table 4. Examples of records in Cluster 29.    貴州省，四月十二日，大水，壞心心橋欄杆。 Guizhou Province, 12th April, Great Flood, The railing of the bridge was broken.     福建省，大水，東橋折水至西門橋頭十字街。 Fujian Province, Great flood, The folding water was from the east bridge to Ximen Bridge Crossroad.     福建省，五月初十日，南鄉大水入城。 Fujian Province, 10th May, Nanxiang water flooded into the city.      廣東省,夏五月，大水，城垣崩陷八處，共六十餘丈。  Guangdong Province, in summer and May, Great flood, The eight places of the city, total 60 feet, collapsed.     江西省，秋八月，大水，蘆溪市宗濂橋圮。 Jiangxi Province, in autumn and August, Great flood, the Zonglian Bridge collapsed in Luxi City.      甘肅省,霪雨，塌傾城垣。  Gansu Province, the city walls collapsed after a long period of rain.      江蘇省,泗州城陷沒，漕堤決。  Jiangsu Province, the city of Suzhou collapsed, the dike was broken.      廣東省,夏五月，大水，秋溪鯉魚溝堤潰。  Guangdong Province, in summer and May, Great flood, Qiuxi carp ditch dike burst.      江西省,秋七月，大水連漲七次，西溪橋鷹咀石俱沖圮。  Jiangxi Province, in autumn and July, floods rose seven times, and Yingzui Stone of Xixi Bridge collapsed.      廣東省,大水，城北平地可通舟。  Guangdong Province, Great floods, the city flat land was navigable.      河北省,堤下口決。  Hebei Province, Breach of the embankment.    Table 5. Examples of records in Cluster 37.     浙江省,秋大水。  Zhejiang Province, Autumn, Great flood.      安徽省,夏秋水。  Anhui Province, Summer and Autumn, Great flood.      湖南省,秋大水。  Hunan Province, Autumn, Great flood.      安徽省,秋大水。  Anhui Province, Autumn, Great flood.      江蘇省,秋大水。  Jiangsu Province, Autumn, Great flood.      湖南省,夏水。  Hunan Province, Summer, Great flood      浙江省,夏秋大水。  Zhejiang Province, Summer and Autumn, Great flood.      浙江省,秋大水。  Zhejiang Province, Autumn, Great flood.      江西省,春大水。  Jiangxi Province, Spring, Great flood.      Front-end interface Then, we use the \"Time and Space Infrastructure of Chinese Civilization\" (CCTS) to present meteorological events on the map interface based on the year and location of the climate events in the historical meteorological records, providing an interface for researchers (see Figure 1). Our system is located at  http://iisrserv.csie.ncu.edu.tw:5000/English. The main features of the interface include a scrolling timeline, a pop-up condition selection window, and an instant response map. When the user selects the conditions, the map will immediately display the records satisfying the conditions. If the cursor is moved over the location on the map, the page below the map will show the meteorological records of the location in the timeline interval.     Figure 1. System interface.   Case Study To show the usage of our system, we look into the meteorological records during 1650 to 1700, which is the late stage of the Little Ice Age, to investigate the phenomenon of climate change in Qing Dynasty of China. First, we choose the “temperature” event category. Among these “temperature” records, there are 68 extreme cold climate records. We can see that this kind of phenomenon was located from tropical zone to tepid zone in China (see Figure 2), therefore, we can conclude that extreme cold records appear not only in middle- to high-latitude areas, but also in lower latitude area.    Figure 2. Areas with Low-Temperature Record from 1650 to 1700. Extreme cold weather usually comes with disasters. We choose “rain” as our variables as step two. Among the records, there are 379 records related to “snow.” To deeply look into the records, besides directly meteorological phenomena of “snow,” we further collect the indirectly meteorological phenomena data, such as “three days in a row,” “frost-damaged trees,” ...etc (see Table 6). From Table 6, we can see that during the Little Ice Age (1650-1700), the extremely cold climate led to disasters is more frequent than that during Non-Little Ice Age (1745-1795). Table 6. Numbers of Data related to “snow” during Little Ice Age and Non-Little Ice Age in Cluster “rain.”   Meteorological Phenomena Data  Little Ice Age  （1650-1700）   Non-Little Ice Age （ 1745-1795 ）    Rain 3,014 1,618   Snow 379 119   Snow, more than 10 days /month /ten days in a row three days in a row or more 46 5   Snow, frost damaged trees 8 2   Snow, birds, animals and human beings freeze to death 17 6   When investigating the meteorological phenomena during Little Ice Age, we find out rare snow records in Taiwan. As shown in Table 7, these records are in terrain area, such as Chia-Yi county and Tainan City, which could be seen as a strong evidence of the extreme climate during Little Ice Age. Table 7. Rare snow records in Taiwan.   year province  county/ city  cluster meteorological description data sources   1683 Taiwan Province --  Temperature (including frost and dew), and rain   冬十一月，雨雪。是夜冰堅厚寸餘。從來臺灣無雪無冰，此異事也。 In November, winter, it snows and rains. The ice is more than an inch thick that night. It's strange because there has not snowed since ancient times.  Volume ten of \"Taiwan Local Gazetteers\", published in Kang-xi period (1685) in Qing Dynasty   1683 Taiwan Province -- Rain, crop failure, temperature (including frost and dew)   五月，大雨。霪雨連月，鄭氏之土田阡陌多被沖陷，有“高岸為谷”之歎。冬始雨雪，冰堅厚寸餘。臺土氣熱，從無霜雪。  In May, it rained heavily. After a long period of rain, the fields of Zheng's family were crashed, and the high bank became valley. It began to snow in winter, and the ice was more than an inch thick. Normally the field was hot, and there was no frost and snow in Taiwan.  Volume nine of \"Rebuilt Taiwan Local Gazetteers\", published in Kang-xi period in Qing Dynasty   1683 Taiwan Province Chia-yi County Flood, rain, crop failure, temperature (including frost and dew)   夏五月，大雨水，時霪雨連月，鄭氏土田多沖陷，有“高岸為谷”之歎。冬十一月，始雨雪，冰堅厚寸餘。諸羅有霜無雪，是歲甫入版圖，地氣自北而南，信有矣。  In May, it rained heavily. After a long period of rain, the fields of Zheng's family were crashed, and the high bank became valley. In November, Winter, it snowed and iced over. Normally the field was hot, and there was no frost and snow in Tsu-lo (ancient name of Taiwan). It was believed that the climate was from Northern area, starting from the year Tso-lo became Qing’s territory.  Volume twelve of \"Tsu-lo Local Gazetteers\", published in Kang-xi period in Qing Dynasty   1683 Taiwan Province Tainan City   Flood, rain, flood, rain, crop failure, temperature (frost and dew), temperature (frost and dew), temperature (frost and dew), drought, rain     春，鯽魚潭涸。夏五月，大雨水，田園多沖陷。六月，澎湖潮水漲四尺。秋八月壬子，鹿耳門潮水漲。冬十有一月，雨雪冰。是臺地氣暖，從無霜雪，是歲八月甫入版圖，冬遂雨雪，冰堅寸許，地氣自北而南，運屬一統故也。  In Spring, crucian pond dried up. In May, Summer, it rained a lot, and fields are mostly crashed. In June, the tide at Penghu rose four feet high. On August 18th, the tide at Luermen rose. In November, Winter, it snowed and iced over. Normally the field was hot, and there was no frost and snow in Taiwan. However, started from August when Taiwan was under Qing’s authority, it rained and snowed in Winter, and the ice was more than an inch thick. The climate was from the Northern Area. It is because of territorial unity.    \"Rebuilt Taiwan Local Gazetteers\", published in Qian-long period in Qing Dynasty        Conclusion Although technology has improved nowadays, it is still hard to predict the weather. In order to understand the impact of climate disasters and find a way to deal with it, tracing the historical climate event could be a solution. This study establishes a principle to classify meteorological phenomena based on \"China's Three Thousand Years of Meteorological Records\", develops a Spatio-Temporal research platform, and build an instant response front-end interface. By the climate case study, we collect the users’ feedback and improve the front-end interface, as well as enhance the precision of a mass of meteorological data analytics. Although the research platform only contains the meteorological data from 1647 to 1795, we hope to expand the capacity of the database and establish a mature Spatio-Temporal research platform in the future.  ",
        "article_title": "Climate Event Classification Based on Historical Meteorological Records and Its Presentation on A Spatio-Temporal Research Platform",
        "authors": [
            {
                "given": "Shang-Yun",
                "family": "Wu",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science and Information Engineering, National Central University; Department of Atmospheric Sciences, National Central University.",
                        "normalized_name": "National Central University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00944ve71",
                            "GRID": "grid.37589.30"
                        }
                    }
                ]
            },
            {
                "given": "Cheng-Han",
                "family": "Wu",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science and Information Engineering, National Central University",
                        "normalized_name": "National Central University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00944ve71",
                            "GRID": "grid.37589.30"
                        }
                    }
                ]
            },
            {
                "given": "Pi-Ling",
                "family": "Pai",
                "affiliation": [
                    {
                        "original_name": "Research Center for Humanities and Social Sciences, Academia Sinica",
                        "normalized_name": "Centre Français de Recherche en Sciences Sociales",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/04j8fde02",
                            "GRID": "grid.503111.7"
                        }
                    }
                ]
            },
            {
                "given": "Yu-Chun",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            },
            {
                "given": "Richard Tzong-Han",
                "family": "Tsai",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science and Information Engineering, National Central University; Research Center for Humanities and Social Sciences, Academia Sinica",
                        "normalized_name": "National Central University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00944ve71",
                            "GRID": "grid.37589.30"
                        }
                    }
                ]
            },
            {
                "given": "I-Chun",
                "family": "Fan",
                "affiliation": [
                    {
                        "original_name": "Research Center for Humanities and Social Sciences, Academia Sinica; Institute of History and Philology, Academia Sinica",
                        "normalized_name": "Institute of History",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/003sdh081",
                            "GRID": "grid.460364.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "geography and geohumanities",
            "English",
            "theory and methodology)"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Events are important historical research targets. Major events are composed of a series of smaller atomic events that involves several people and their interactions at a specific time and location. For investigating Chinese historical events, imperial historical records are one of the most reliable sources. There are two main challenges of studying imperial historical records. The first is that the records are usually quite lengthy. To systematically trace the evolution of an event or a subject such as “Qing conquest of the Ming,” historians need to mark up all mentions in relevant paragraphs in the  Ming Shilu,   The Ming Shilu (traditional Chinese: 明實錄; simplified Chinese: 明实录; literally: \"Veritable Records of the Ming\") contains the imperial annals of the Ming emperors (1368–1644).  which is a tedious, time-consuming job. The second obstacle is that there is no publicly available text-mining tool that can extract person, location, time, and event mentions from Ming dynasty historical records. The aim of this work is to begin development of text-mining tools that tackle these two challenges.  One common way to represent an atomic event is as an event frame containing an event trigger and several event arguments. An event trigger, or “predicate”, is a key action that evokes the event. An event argument is a word/phrase in the sentence related to the predicate. Event frames are therefore called predicate-argument structures (PAS) in computational linguistics. Event arguments are distinguished from one another by their relations to the event trigger. Such relations are referred to as semantic roles, including main arguments such as  agent and  patient, as well as adjunct arguments, such as  time,  manner, and  location. Every predicate has its own frameset, that is, the set of sematic roles. The sentence “大寶法王 差 喃哈鎖南” can be represented by a PAS in which 差 (dispatch) is the predicate, 大寶法王(Karmapa) is the sender, and 喃哈鎖南(Nanhasuonan) is the person sent.   The task of marking up atomic events in texts can be treated as a PAS recognition problem. PAS recognition comprises two steps: (1) locating the event trigger; (2) recognizing arguments and labeling them with semantic roles. Since the second step is the key, PAS recognition is referred to as semantic role labeling (SRL). In this paper, we aim to identify tributary events in the  Ming Shilu automatically by using SRL techniques. We first compile a set of tributary verbs and define framesets for each verb. Then, we formulate SRL as a sequence labeling task and employ the state-of-the-art algorithm, conditional random fields (CRF)  (Lafferty), to tackle this task. This work represents the first machine-learning-based attempt to extract atomic historical events from Classical Chinese as well as the first effort to solve this problem using SRL.    Predicate Selection and Frameset Construction Selecting predicates and defining their framesets are the prerequisites of SRL. To reduce human effort, we first employ the clustering-based classification method  (Tsai) to identify paragraphs related to tributary events. Next, to facilitate the selection of predicates and their corresponding framesets, we label these paragraphs with NEs. We select 12 tributary verbs as predicate candidates. Manually defining the frameset for a verb requires linguistic expertise and is time consuming. We base our framesets on those in PropBank,   http://verbs.colorado.edu/chinese/cpb/  the most widely used frameset database, revising them according to verbs’ appearance in the  Ming Shilu. For each verb  v, we revise its PropBank frameset by checking on average 100 tributary paragraphs containing  v and NEs.     Semantic Role Labeling  After compiling the frameset database for tributary verbs, we perform SRL for such verbs. First, we search all tributary paragraphs for these verbs to locate event triggers. Our SRL system then recognizes arguments with semantic roles from such paragraphs. The SRL task here is formulated as a sequence labeling problem in which the system should output the most probable sequence of labels when given a sequence of tokens (each Han character is regarded as a token). Although deep learning methods such as  (Qian et al., 2017) have demonstrated their effectiveness in the SRL task for modern Chinese, they are still impractical for classical Chinese because there are no word and sentence segmentation tools available for the Ming-Shilu. Without segmented sentences, we cannot train a word embedding model, making deep-SRL impossible. Therefore, we employ the CRF model, which achieves the competitive performance in the SRL task for modern Chinese  (Tan et al., 2009) and does not require segmentation information  We use the relative position in a chunk and that in an NE as features because these types of information are very informative for predicting semantic role tags. For this step, we use Tsai et al.’s NER system  (Tsai). Since there is no chunking tool or annotated corpora for classical Chinese, we design a chunking algorithm that requires no training data:   NER is used to mark NEs in all paragraphs. NEs and strings between NEs are treated as chunks. All chunks with their counts are recorded in a list  l.   For each chunk  c i and its superstring with the same prefix or suffix  c j, if  c i’s count is larger than  c j’s over a threshold  N,  c i’s count is increased by 1 and  c i is cut off from  c j.  Step 2 is repeated until there is no new chunk generated. For each paragraph, all strings between NEs are segmented by the maximum matching algorithm with the final  l as the reference dictionary.     Experiments We select 500 paragraphs from the 2,874 paragraphs containing tributary events in the  Ming Shilu for our experimental dataset. 12 verbs are selected as event triggers. All paragraphs are labeled with PAS’s by two experts (inner annotator agreement  (Cohen, 1960)    κ  =.89). All experiments are conducted using 10-fold cross validation.   The data set is divided into 10 subsets, and the holdout method is repeated 10 times. Each time, one of the 10 subsets is used as the test set and the other 9 subsets are put together to form a training set. Then the average error across all 10 trials is computed.  The parameters are set to the default of CRF++.  Table 1 shows the argument-recognition performance, which achieves an F1-measure of 82.90%. Table 2 shows the PAS-recognition performance. Our system achieves an F1-measure of 48.45%. Although the argument-recognition performance is satisfactory, correctly identifying every argument in a given PAS is very challenging. Therefore, PAS-recognition performance is somewhat less than ideal, though it is still comparable to the performance of other systems on modern Chinese  (Xue, 2008).  Table 1: Argument-recognition Performance   Precision Recall F1-measure   81.08% 84.79% 82.90%   Table 2: PAS- recognition Performance   Precision Recall F1-measure   48.45% 48.45% 48.45%    Analysis and Application In this section, we present a detailed breakdown of PAS recognition results for the major predicates to show the practical performance of our system to historians. In the “Analysis—Extracted Tributes and Rewards Items” subsection, we categorize the most common tributary/rewarding items extracted by our system. In the “Analysis—Extracted Envoys” subsection, we show some top-frequent envoys extracted by our system. At last, we briefly illustrate the application of our system to historical studies. Table 3: Tribute Categories   Category Frequency Examples   Animals(Slaves  724 駝(camel), 羊(sheep) ,馬(horse) ,象(elephant) ,人(human)   Local Products 178 方物(local products)   Luxury Goods 58 金(gold) ,珊瑚(coral) ,文綺(silk) ,玉石(jade)   Buddhist Items 40 香(incense) ,香爐(censer) ,佛像(Buddha statue) ,舍利(sarira)   Weapons 29 盔(helmet) ,甲冑(armor) ,劍(sword) ,刀(knife)   For rewards, the extracted 401 terms can be divided into 7 categories, shown in Table 4. Table 4: Reward Categories   Category Frequency Examples   Money 667 銀兩(silver currency) ,鈔(banknote)   Cloth 580 布(cloth) ,紗(yarn) ,素叚(white satin) ,綵緞(colorful satin)   Banquets 479 宴(banquet)   Clothing 318 衣服(clothes) ,蟒衣(embroidered robe)   Buddhist Items 53 佛像(Buddha statue) ,袈裟(kasaya)   Imperial Ceremonial Objects  41 冠帶(coronary band) ,誥印(imperial stamp) ,誥命(imperial mandate)   Animals 24 馬(horse) ,駝(camel) ,羊(sheep) ,牛(oxen)     Analysis—Extracted Tributes and Rewards Items We have analyzed the atomic events whose predicates are related to tributes, such as “朝貢/chao gong、貢/gong、進/jin、獻/xian,” and rewards, such as “賜/ci、賞/shang、給賞/gei shang”. For tributes, the extracted 175 items are classified into five categories, shown in Table 3. According to Tables 3 and 4, we can see that the tributes are quite different from the rewards. Animals and slaves are the most popular form of tribute, while money is the most common reward. The tributes and rewards reflect the diplomatic and trade relationships between Ming and its tribute states. Interestingly, we found that rewards sometimes included oxen, which was unexpected for tribute states that did not extensively practice agriculture. For example, we examined the paragraphs containing the reward oxen and found that the reward target was泰寧衛/Tai-Ning-Wei, a Ming-allied Mongolian tribe that lived a nomadic lifestyle. This reward of oxen is evidence suggesting that Tai-Ning-Wei may have practiced agriculture due to Ming influence  (Yingtai).    Analysis—Extracted Envoys There are 4,002 tributary events which contain 3,064 envoys. 塔卜歹/Tabudai, a Tai-Ning-Wei chief, visited Ming officials 33 times between 1500 and 1555. The secondly most frequent envoy is升合兒/Shengheer, another Tai-Ning-Wei chief. He paid tribute to the Ming authorities 25 times between 1537 and 1611. This shows that Shengheer lived more than 74 years, and may suggest that he was actually several different men. In addition, we can also observe chiefs who sent envoys to the Ming in extracted tributary events. There are 488 events in which envoys were sent and 313 distinct senders. The leader who sent envoys the most is孛來罕/Bolaihan, the governor of朵顏三衛/Duo-Yan-Three-Wei. He sent envoys 20 times to the Ming between 1502 and 1537, such as塔卜歹/Tabuda (8 times), 納哈出/Nahachu (3 times), 納挨/Naa (3 times).   Application With a well-developed SRL tool, historians can quickly identify historical events and terms labeled with roles in events from Classical Chinese literature. The extracted PAS information can be further used to support large-scale analysis of historical events, such as the distribution of events overall or for some person or some place.   ",
        "article_title": "Event Extraction on Classical Chinese Historical Texts: A Case Study of Extracting Tributary Events from the Ming Shilu",
        "authors": [
            {
                "given": "Richard Tzong-Han",
                "family": "Tsai",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science and Information Engineering, National Central University; Research Center for Humanities and Social Sciences, Academia Sinica",
                        "normalized_name": "National Central University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00944ve71",
                            "GRID": "grid.37589.30"
                        }
                    }
                ]
            },
            {
                "given": "Yi-Hsuan",
                "family": "Lu",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science and Information Engineering, National Taiwan University",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            },
            {
                "given": "Yu-Chun",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            },
            {
                "given": "I-Chun",
                "family": "Fan",
                "affiliation": [
                    {
                        "original_name": "Institute of History and Philology, Academia Sinica",
                        "normalized_name": "Institute of History",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/003sdh081",
                            "GRID": "grid.460364.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-17",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "natural language processing",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the global context, no single unified definition of digital humanities (DH) is possible. The scholarly context that DH was defined and debated in the Greater China region is starkly different from that in Western academia, owing to the unique features of humanities data in Chinese, especially for texts (Mahony, 2019). With special focus on the context and cultural politics of the conditions in which DH emerged and the contestations it encountered, our paper unravels the complex issue of DH emerging as a scholarly field in China from a historical standpoint. In this state-of-the-field investigation, we draw from our experience in running major DH projects in Chinese studies, as well as in building DH communities in China by founding and editing a popular academic blog on DH and moderating online discussion groups on the Chinese social media app WeChat. The Classical and modern Chinese language presents technical challenges in digitizing, organizing, and mining Chinese texts, for instance, in word segmentation. The amount and heterogeneity of Chinese characters increase the difficulty of mining Chinese texts (Tsui & Wang, 2019). Despite this, DH is hardly a novel approach to Chinese academia. During the 1980-2000s Chinese humanities experts have begun producing computational and corpus linguistic, statistical, and GIS analyses in history, literary, and geographical studies. Examples include stylistic research on the 18 th century novel  The Story of the Stone. Such projects have employed cutting-edge research methods, but did not gain enough currency to be accepted by mainstream humanities scholars. During this “prehistory” of DH for Chinese humanities, there were also large-scale digitization projects on Chinese materials, especially the historical compendium that are the most basic:  Siku quanshu [Complete library of the four treasuries] and the Twenty-five Standard Histories. Database companies, libraries, university departments, and governments have made substantial investments in digitization in the form of R&D for commercial databases, cultural heritage projects, funding for libraries, research grants, etc.  After pioneering scholars Jieh HSIANG, JIN Guantao, and WANG Xiaoguang introduced the concept to China around 2009 (Wang, 2009; Hsiang & Weng, 2011; Jin, 2014), most mainstream humanities experts have been skeptical or have even rejected the academic practices that DH entails. One common misunderstanding about it among Chinese scholars concerns how research questions can directly yield different answers from employing DH tools and methods. Some historians assume that DH research is solely about using quantitative methods (i.e. cliometrics or quantitative history). The value and potential of qualitative inquiry in DH, especially in media studies or visualization-related studies, is rarely noticed. For these researchers, the use of digital tools is merely for acquiring research materials; the further utilization of data that is conventionally recognized as DH research is seriously lacking. Such (mis-)conceptions and practices have prevented Chinese scholars from moving from digitization to deeper-level knowledge discovery and methodological renewal, until the 2010s. In the field of Chinese history, quantitative historians (mostly historical demographers and economic historians), historical geographers, and researchers in prosopography and social networks have gained most from digital advances. All in all, the paradigm shift in China is slowly taking place, but it is a delayed one given the amount of preparation from the “prehistory” phase. Taking into consideration that many of the elements of DH already existed in Chinese academia, DH should not be taken as a completely novel paradigm for the humanities in China. Since the late 2000s, DH, “Electronic evidential scholarship ( e-kaoju)”, and quantitative history have all become academic buzzwords. Among these digital-related concepts, junior scholars and students in China are increasingly exposed to DH. Several major Chinese universities have set up DH centers that involve the study of digital humanities, or at least its promotion. More institutions have secured funding for research projects that include the building of academic databases. Proprietary databases are already plenty and are dominating the field in China—most scholarly data is owned by commercial companies and made available to scholars through institutional subscribed databases (Tsui, 2016).  Given this amount of investment and input, DH is underdeveloped for the Chinese-speaking world, if we try to trace DH as an emerging trend. According to our analysis of the main theoretical and manifesto-like texts on the DH paradigm in China and the main ongoing projects, we contend that DH has become a canopy term for Chinese scholars to reconceptualize, recatergorize, and repackage old projects and academic practices from the “prehistory” phase examined earlier. Several longstanding text and biographical databases have recently gained widespread attention, partly thanks to online communities on social media. These include the Database for the Study of Modern Chinese Thought and Literature (since 1997) and the China Biographical Database (CBDB) (since 2005) (Bol, 2018). Disciplines are highly institutionalized and centrally governed in China; academic boundaries are rigid. Incentive systems discourage researchers from engaging in DH research, which often disrupts traditional forms of academic publishing. As a result, young DH scholars in China tend to collaborate with like-minded peers that are outside, rather than within their own disciplines. Mainstream senior scholars have only begun reflecting on digital history and the use of databases in history journals since 2016. To tackle these challenges, several major Chinese universities have set up DH centers and their libraries are beginning to develop DH librarianship. There are also international efforts to construct an infrastructure for connecting digital resources and systems for Chinese history. Since 2015 there have been acclaimed conferences to discuss the scholarly trend of DH, but capacity building events such as workshops that cover vital DH skills are still rare, given the growing demand. Humanities curricula in China usually do not provide the basic skill set that is useful for budding DH work, forcing interested learners to be proactive and daring. Weak support for digital literacy among humanities scholars in China also account for the dearth of DH expertise. Institutions within universities that possess the capacity to provide digital training and research support for such humanities scholars are rare, although libraries have begun to recognize the need to strengthen such efforts (He & Chen, 2018). ",
        "article_title": "Defining and Debating Digital Humanities in China: New or Old?",
        "authors": [
            {
                "given": "Lik Hang",
                "family": "Tsui",
                "affiliation": [
                    {
                        "original_name": "Harvard University, United States of America",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            },
            {
                "given": "Jing",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "Nanjing University, China",
                        "normalized_name": "Nanjing University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/01rxvg760",
                            "GRID": "grid.41156.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "oriental and asian studies",
            "multilingual / multicultural approaches",
            "digital ecologies",
            "theory and methodology)",
            "English",
            "digital communities and critical infrastructure studies",
            "diversity"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In response to the conference’s theme of ‘Complexities’ and the “humanist way of building complex models of complex realities,” this paper will report the findings of my doctoral research into how different structural forces “meet, reroute and disrupt” one another (Levine, 2005) in Alison Bechdel’s graphic memoir,  Fun Home (2006). By encoding multiple understandings of the same source material at the data creation stage of the project, and by creating tags that reflected a “plural, partial and situated” (Haraway, 1988) conceptualisation of ‘Truth’, it was possible to “operationalise the resulting parallax” (Turton, 2017) and build a richer model of the comic’s rhizomatic rhetoric. This made it possible to better understand the critically under-theorised issue of how different structuring ‘forms’ operate on the text and one another. In addition to reporting these specific findings, this paper will communicate the investigative method by which I reached them in the belief that it has a broader application for Digital Humanities projects that must work with non-indexical or otherwise ambiguous sources.    Background and method The narrative of a comic is organised by structural, spatial, temporal, and thematic logics, but analysing how each of these ‘forms’ interact with, and organise, one another is challenging not only for non-digital scholars but also for digital ones. This is because each of these categories can be understood, defined, and therefore represented in a database, in multiple, equally plausible and valid, ways; it is unclear which particular understanding(s) of which particular ‘form(s)’ might prove to be the most productive in explaining the patterns and relationships that emerge from the data.  The recent emergence of digital and empirical approach to comics (Dunst et al., 2016; Tufis, 2016; Walsh, 2012) reflects an understanding that the volume of information found in a comic’s panels exceeds the capacity of human memory and, therefore, requires prostheticising into digital memory if scholars are to progress from, and in some cases even apply, non-digital criticism and theory of the system (Groensteen, 2007) or language (Cohn, 2013) of comics. A key facet for digital approaches to comics, and an almost unique opportunity in terms of distantly reading literature, is the medium’s mode of articulating time, the panel. Panels assemble signifiers at a discrete site, giving creators minute control over what – both in terms of content (words; characters; objects; themes) and container (headers; speech; inset captions) – is present for, and therefore co-locates with, each disclosure of narrative information, each event, each phase of a scene. This means that themes and references can be rhetorically woven into the diegesis. In  Fun Home, for example, Bechdel’s father was killed by a Sunbeam bread truck, and loaves of this particular brand are scattered through the text at otherwise seemingly innocuous moments, inextricably linking them to that pivotal scene.  This rhetorical affordance, however, isn’t limited to the content of panels. It extends to the structures that organise that content, too, the ‘forms’ that execute the parsing of narrative information, the unravelling of a text’s arguments. Not only does each signifier occur at a discrete site, but that site is also placed in varying relations with other sites. Some of these relations are more literally structural and include linearity (they come after one panel and before another), as well as being part of a particular row and page, and row layout and page layout. Others of these relations are commonalities they have in terms of  diegetic chronology, event, scene, thematic concerns, location or source of information.  Each of these different kinds of organising logics, or lines along which the text can be folded, operate on a comic’s content, its words and its images; they are in relationship with them. Further, the combination of words and images found in comics means that one narrative track, often the words, can be used to anchor the narrative, allowing the other track to move around freely, and without extra-diegetic explanation, in time, space, or subject (Bechdel and Farley, 2012); the medium exists in a state of almost perpetual montage and this produces a richer and more networked rhetorical texture. As well as choosing what is present for the disclosure of information, the author has precise control over the pacing of their narrative and can rhetorically interweave different chronologies and different sources of information. By ‘sources of information’ I particularly mean archival documentation. Whilst drawing on archival “evidence” (Cvetkovich, 2008) is a feature of most graphic memoirs (El Refaie, 2012; Chute, 2010), particular attention and acclaim has been directed towards its usage in the scholarly and popular reception of Bechdel’s memoir (Chute, 2010; Rohy, 2010; Tison, 2015). Indeed, over the course of the memoir, Bechdel draws on photos, maps, diaries, letters, dictionaries, tape recordings, annotated copies of family books, and newspapers. But where these existing scholarly analyses stop is when it comes to linking this usage to, or interrogating its relationships with, the other structuring ‘forms’ at play in the memoir. Most strikingly, given it is the very mode of articulating narrative, this intersects with another gap in the critical context – and one by no means unique to  Fun Home – a sustained study of the rhetorical effect of page layouts. Critical attention to page layouts tends to be reserved for unique and remarkable layouts, even though the signification of the majority of panels is operated on by more standardised, and reproduced reproducible layouts. It is these gaps in isolation,  and in their relationships to one another, which my doctoral work, and this paper, addresses.  These critical gaps can be understood partly as a result of the analogue mode of scholarship used to investigate such an information-rich medium, and partly as a result of the idiolectical nature of usage, meaning these features cannot be precisely formalised across texts by theorists. However, another significant cause of this gap – and one which persists in larger-scale digital approaches to comics due to their need for a single cross-corpus markup scheme – is that these organising logics cannot be formalised into a single classification scheme without doing significant violence to the source. This is where my method differs from other, larger, digital approaches to comics. Not only are the contents of panels ambiguous signifiers, but so are the structuring ‘forms’ that operate on and organise these contents; both can be understood in multiple and equally valid ways, and by restricting my project to a single text I was able to encode several of these interpretations and play them off against one another.  Not only do singular definitions do violence to the source in their positivism, but they forego the potential of leveraging different conceptualisations against one another to see which definitions of which features interact with the data in meaningful ways, create rhythms and harmonies, and furnish us with consistent interactions. By encoding different interpretations of these organising ‘forms’, and comparing the strengths of the resulting correlations, relationships, and collisions, it is possible not only to enrich our understanding of how they organise the book, but also which definitions of them are more appropriate in the book’s system. Rather than getting caught in a “feedback loop of fore-projection,” (Sculley and Pasanek, 2008) this method offers a mode of investigating sources when not only is it unclear which features will prove significant or interesting, but also what conceptualisations of those features will, and leveraging this ambiguity for investigation.   Conclusion By building “a complex model of [a] complex realit[y],” and by asking how different structuring ‘forms’ are arranged, arrange, and arrange one another in Alison Bechdel’s graphic memoir,  Fun Home, this paper will demonstrate the investigative power of multiply tagging ambiguous data outlined, and advocated for, in ‘Towards Feminist Data Creation,’ and will report the specific findings of this doctoral research as a counterpoint to the larger projects undertaking digital comics research.   ",
        "article_title": "Operationalising Ambiguity; Mapping the Structural Forms of Comics",
        "authors": [
            {
                "given": "Alexander Robert",
                "family": "Turton",
                "affiliation": [
                    {
                        "original_name": "University of East Anglia, United Kingdom",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-05",
        "keywords": [
            "concording and indexing",
            "corpus and text analysis",
            "semantic analysis",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper discusses the value of  indigenous knowledge and practices providing sustainable food security. Indigenous knowledge and practices play a crucial role in maintaining and enhancing genetic diversity, reduce environmental damage and boost resilience to climate change. Innovations based on traditional knowledge provide climate smart alternatives that can significantly increase food productivity and incomes of local communities, while ensuring sustainability and maintaining genetic diversity, as required by SDG2  . The Food and Agriculture Organization (FAO) acknowledges the breakdown of traditional systems and how this has affected environmental degradation, undermining the long-term viability of pastoral livelihoods.  Access to adequate food is a basic human right. It is a catalyst to the realization of all other human rights, such as the right to education and good health, to mention only two  (FAO, IFAD, UNICEF, WFP, and WHO, 2017) . Therefore, attaining national and household food security is of fundamental importance. However, achieving food security and making it sustainable, remains a global challenge.     Although Uganda is well known as an agricultural-based economy, sustaining food security has become challenging, owing to the poor economic status of the country. The current situation is that of low productivity, persistent hunger and malnutrition. This leads to much human suffering and substantial low productivity  ;  USAID, 2016 ;  (Tugume, 2017 ;  .     argues that, while indigenous knowledge is gaining recognition, the need for ICT must also go high to capture, manage and disseminate indigenous knowledge. It is also needed to enable, control and share indigenous knowledge within the local context and according to unique and specific local needs. Peoples’ experiences, competences, talents, ideas, practices, intuitions, skills, wisdom and capabilities need to documented and codified  . He argues that a number of countries are using IT to develop digital libraries of indigenous knowledge in various local languages to prevent it from misuse through commercial patents. Managing and digitalizing indigenous knowledge is meant to develop cost effective and sustainable survival strategies for food security, poverty alleviation and income generation for poor rural communities. Digitizing indigenous knowledge makes it more resilient in the face of new threats such as those posed by climate change and genetically modified organisms (GMOs)  . ICT can play major roles in improving the availability of indigenous knowledge systems and enhancing its blending with the “modern/ scientific and technical knowledge”  .    Digitizing indigenous knowledge bridges the geographical and perceptual gap between communities  . Mark and Rensselaer (no date) argue that ICT is vital in sustaining and stimulating communities’ traditional ways of knowing. According to  , ICT could indeed act as a source of empowerment and knowledge exchange by enabling young people, old, employed and unemployed to exchange traditional and modern knowledge. It creates a platform for interaction among members of the community in form of collaborative processes   .    Due to the complexity of decision making processes, collaborative decision-making plays an essential role in the design and communication at all levels in problem solving processes. Collaborative decision-making refers to a situation where different people working together toward achieving a common goal come together to exchange ideas on how to achieve the stated goal. It is defined by   as a joint effort toward a common goal; a process in which stakeholders with different perspectives of a problem, can constructively explore the differences and can search for solutions that go beyond their own limited visions. It ideally involves a free exchange of ideas to allow creation of most innovative and strategic decisions  . Within collaborative decision making, there are many processes and best practices that can be employed and shared to ensure the best outcomes. A collaboration process provides a mechanism for engaging stakeholders in an effort to identify and address food security problems  . In the case of rural food insecurity, relevant stakeholders need to be involved to appropriately brainstorm, i.e. share their experiences on how they apply indigenous knowledge  .     Following a design science research philosophy in an engaged scholarship research paradigm,   a digital platform for managing and sharing indigenous knowledge that enhance food security is developed as an intervention base. This study was conducted in the selected areas of rural Uganda where agriculture is predominantly the major source of livelihood. A literature review was carried out to gain a generic understanding from different stakeholders’ perspectives and to get deeper insights of the challenges rural farmers face and decisions they take to overcome the challenges. In the exploration phase, it was observed that rural farmers operate in complex circumstances characterised by poor coordination and collaboration, lack of information and knowledge sharing to learn about what other farmers do in order to resolve food security decision making challenges. The exploratory study findings also revealed that the context in which rural farmers make food security decisions was complex and unstructured.    The platform enables farmers and stakeholders to share knowledge and experiences regarding household food security. The platform provides a collaboration suite that offers an interactive environment for knowledge sharing and decision-making.     It provides steps for engaging farmers and stakeholders in food security decision-making. The collaboration suite describes steps that are crucial for collaborative decision-making. The artifact offers an environment for collaborative decision-making among farmers and stakeholders. It is a platform where farmers and stakeholders freely exchange ideas on food security improvement and come up with innovative and strategic decisions. Collaborative decision-making enables farmers and stakeholders to share knowledge and information on best and worst practices as experienced by farmers. The initiator of the topic for discussion can invite people of his choice to the chartroom to discuss about the proposed topic by giving their views in the brainstorming session.    As noted by  , collaboration promotes sharing of experiences and practices in a specific context. During collaboration decision-making, farmers are encouraged to brainstorm ideas, tell their stories on indigenous knowledge experiences from which the best ideas are agreed upon by consensus and shared understanding. The purpose of sharing indigenous knowledge experiences by farmers is to learn from each other the best practices. Domain experts like extension workers and agriculture officers can provide technical advice by joining discussions on the platform. Collaboration stimulates comprehensive thinking and is a means of identifying best practices for addressing food security problems  . In the digital platform, CDWs take the role of intervention facilitators. The role of intervention facilitator is to provide support to rural farmers with no or low education background on how to use the platform. A facilitator gives assistance to farmers by using the intervention schemata to provide technical assistance to farmers and stakeholders who may want to participate in the discussions.     A screen shot of the collaborative decision-making   The digital interface provides collaborative platform for farmers and stakeholders engaged in a discussion about food security issues in a brainstorming manner. Every registered farmer can post his/her opinions suggesting ways in which food security could be made better using indigenous knowledge. The link allows any registered user to initiate a topic of discussion and to invite the views of other farmers and stakeholders. It represents a decision-making style in a way of brainstorming by instant messaging. It offers an opportunity for engaging farmers as domain practitioners in food security and key stakeholders in a group discussion. Collaboration builds teamwork and enables knowledge and experience sharing as a means of identifying alternative ways for addressing food security problems, see  ;  . The digital interface has a provision for farmers to give their views regarding issues of their concern and also to participate in a discussion by chatting using the chat room that is provided on the dropdown menu bar. Upon clicking the farmers’ views icon, one is able to see the views of others and can join the discussion by scrolling down and writing his/her views in “give your comments” space provided at the bottom of the page. In the chartroom, any registered user can initiate a topic for discussion and can invite people of his/her choice to chat with in a way of sharing knowledge. In the interface, CDWs take the role of facilitator. Their role as intervention facilitators is to guide farmers and stakeholders on how to collaborate in decision-making process  . They give instructions on how to brainstorm, generating alternative ways for improving food security.  The platform was instantiated and evaluated by the users and domain experts in the field of food security and information systems. It was perceived as a usable and useful ICT interface for sharing indigenous knowledge among farmers. This study provides both descriptive and prescriptive solutions to the problem of food insecurity in rural communities. Furthermore, it contributes to digitized and contextualized interventions to solving food insecurity in developing countries.  ",
        "article_title": "Collaborative Decision Making and Food Security: Digitizing Indigenous Knowledge of Rural Farmers in Uganda.",
        "authors": [
            {
                "given": "Robert",
                "family": "Tweheyo",
                "affiliation": [
                    {
                        "original_name": "Kyambogo University, Uganda, Uganda",
                        "normalized_name": "Kyambogo University",
                        "country": "Uganda",
                        "identifiers": {
                            "ror": "https://ror.org/01wb6tr49",
                            "GRID": "grid.442642.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "methods and technologies",
            "sustainability and preservation",
            "interdisciplinary & community collaboration",
            "sociology",
            "English",
            "indigenous studies",
            "cultural artifacts digitisation - theory"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  / Importance  The sonnet is a prodigious poetic form. Since its invention in the 13th century by Giacomo da Lentino, hundreds of poets have written many thousands of sonnets in European literary languages. It was popularized by Petrarch in the 14th century, translated by Wyatt and Camões in the 16th, and reformulated by poets from Shakespeare to Rilke to Frost. The experimental poet Raymond Queneau has even written a machine-generated sequence whose lines can be recombined in a hundred trillion different ways.  This project has begun to compile every extant sonnet into a database < acriticismlab.org >, in order to quantify their features through time. Those features include dates, languages, authors, diction (word choices), sentiments, named entities, and form.  My research question is straightforward: just what is a sonnet? Definitions have tended to focus on its form: 14 lines of rhymed ten-syllable (pentameter) verse. Subtypes, including the Petrarchan and the Shakespearean sonnet, are often based on rhyme scheme. But another definition is based on generic rather than formal features: a first-person reflection or “dialectical self-confrontation,” often with a volta (or turn) from problem to resolution (Oppenheimer: 1989). To what degree, then, is the sonnet a form or a genre? What subtypes will a comprehensive, quantified taxonomy reveal?  I am pursuing these inquiries by gathering as many known specimens of sonnets as possible, and then quantifying my analysis of their metadata. This includes metadata at the level of tokens and lines; of clauses and sentences; of rhyme-units (couplets/quatrains/sestets/octets) and complete sonnets; and of their published sequences. There are many features of these units that can be encoded, largely through automated natural-language processing. Tokens can be lemmatized and tagged with their parts of speech; their order and frequencies can be modelled as topics; their syllables per line can be counted; their rhyme with other tokens can be represented. The only human-dependent encoding the database includes at present leverages the expertise of anthology editors: orthography, punctuation, authors, dates, and copyright.  The sonnet genre must be localized in its diction. Some words appear more frequently than others, particularly in the sonnet’s early centuries of first-person lovelorn reflections: words like ‘love’ and ‘she’ and ‘suffer’ and so on. So, too, do words describing the sonnet’s own composition: words like ‘ink’ and ’lines’ and (simply) ‘this’. But genre can be quantified at the level of the sentence, as other scholars have discovered by analyzing topics and principal components in Shakespeare’s sentences (Estill and Meneses: 2018; Hope and Witmore: 2010). This project will determine what generic features the sonnet’s words and sentences reveal.    Methods  The ACL Sonnet Database has standardized its texts according to the TEI guidelines, making them available to basic query functions and JSON object serialization. Thus far it contains 1880 Englishlanguage sonnets, including 445 transcribed from a single print anthology (Hirsch and Boland: 2008). My students and I have populated this repository first with English-language sonnets because they are numerous enough to offer a test case for machine-enabled research in any natural language.  The database also maintains a Python class for connecting to its data via the RESTful API, automating much of the data parsing for analysis with software like the Natural Language Toolkit (NLTK). Initial student-driven inquiries began with close readings of ten sonnets from the anthology to identify quantifiable features. Students have charted the frequency distributions of the sonnets’ rhyme schemes; enjambment; rhetorical figures (anaphora and epistrophe); and topics, including rhetorical questions and references to celestial objects and classical muses.    Results  At this proof-of-concept stage, the database offers results only in these limited domains, and on this limited dataset. At the time of the DH2019 conference, it will have many more thousands of sonnets. I will report on their quantifiable formal characteristics, including rhyme schemes, meter, line lengths, sentence lengths, word frequencies, part-of-speech distributions, and ngrams. I will also report on topic models and the sonnets’ principal components distributed through time, author nationality and gender, and other salient subdivisions.    Discussion  Anthologies of sonnets are sufficient for preliminary student-driven inquiries, but to generate insights into the sonnet writ large, a wider net is necessary. I have begun conversations with machine-learning specialists to train a neural network to recognize sonnets in undifferentiated text files, based on the formal and generic characteristics of sonnets isolated by anthology editors. To prepare for this phase my approach will be two-pronged: to give students another dozen anthologies for further transcription; and to use that expanding repository of sonnets as a training set for a machine-learning process that will identify similar poems in a corpus of 70,000 English texts printed before 1700, the Early English Books Online - Text Creation Partnership (EEBO-TCP) corpus. Early sonnets establish conventions to which later English sonnets respond, so they are a valid place to begin this inquiry. That process has already begun with a subset of 18,000 XML files from the EEBO-TCP corpus containing the <l> element, denoting lines of poetry. My lead programmer, who built the database, will write an algorithm that parses these undifferentiated elements into clusters of 14-line sequences, on the provisional assumption that all 14-line stanzas or poems bear a family resemblance to the sonnet. (This, too, is a provisional assumption; there are sonnets, including one by Shakespeare, of irregular lengths.) I will begin with 14-line sequences in order to identify the extra-formal characteristics that are twinned with this form; only then can I unshackle the detection algorithm from the constraints of form, to see which other poetic units bear the nearest affinity.  ",
        "article_title": "The Augmented Criticism Lab’s Sonnet Database",
        "authors": [
            {
                "given": "Michael",
                "family": "Ullyot",
                "affiliation": [
                    {
                        "original_name": "University of Calgary, Canada",
                        "normalized_name": "University of Calgary",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03yjb2x39",
                            "GRID": "grid.22072.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "corpus and text analysis",
            "natural language processing",
            "renaissance studies",
            "data mining / text mining",
            "English",
            "text encoding and markup languages",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Rise in data-driven studies  Data-driven research on bibliographical data in the field of literary studies is on the rise from the beginning of the 21st century with researchers such as Franco Moretti (Moretti, 2005; 2013), Matthew Jockers (Jockers, 2013), Katherine Bode (Bode 2012; 2017), or Hoyt Long and Richard So (Hoyt and So, 2013) leading the way of empirical and theoretical research and discussions. Notable and established methods of data-driven research – like “distant reading” (Moretti), or “macroanalysis” (Jockers) – have gained considerable recognition in the scholarly field. However, recently the quality and representativeness of datasets for performing such research have come under scrutiny (Bode, 2017). Bode calls for data-driven research that is based on datasets that are richer, better documented, curated, and more systematic in their scope. While Bode addresses the scholarly community in the first place, this paper, written from the perspective of data producers, recognises the challenges facing existing literary bibliographic resources in the age of data-driven research. Basing on the experience with two large bibliographic databases (Czech Literary Bibliography [CLB] and Polish Literary Bibliography [PBL]), this paper assesses how literary bibliographies adapt to the need for advanced data uses, and how application of data-driven methods in literary research revolutionises the way the bibliographies are prepared, standardised and published. In conclusion, the paper identifies the gap between data studies and data production, which should be bridged through the interdisciplinary cooperation within digital humanities.   Literary bibliographies: from auxiliary subdiscipline to producers of research data Literary bibliography (subject bibliography of literary matters) has traditionally been an auxiliary subdiscipline in the humanities, and its research goals were relatively limited. The place of bibliography within the larger scientific context – and the very meaning of the term – has shifted in the last decades. Until mid-20th century that meaning was broad and covered a variety of “problems of organizing and selecting documents” (Buckland, 2011: 36), while more recently these problems have been attributed mainly to disciplines connected to library and information sciences (LIS). And the term “bibliography” narrowed its scope to “detailed examination of printed books as physical objects” (Ibidem).  As a result of this shift, bibliographers came to play a rather passive role in the development of the research potential of data gathered by subject bibliographies in the humanities. Such development could only be driven by the expectations of researchers in the field, and these, until recently have been rather modest, rarely demanding more than reference provision.  The rise of data-driven research literary had a far reaching impact on literary bibliographies, redefining the role of bibliographers as data specialists, their core operations, the setup of bibliographical departments, and the modes of gathering and publishing metadata. The way data-driven research uses bibliographic data amounts in fact to redefinition of a bibliographic record itself. While its main purpose used to be to provide reference, i.e. an accurate representation of the described document (book, article, etc.), now this aura of “raw data” (Gitelman, 2013) have dissipated in favor of a more nuanced understanding, which presents the bibliographic record as already a statement of both cultural meaning and certain research assumptions, which need to be examined in their own right. This is especially relevant in case of literary bibliographies. As texts constitute the quintessential primary source of literary studies, the advances of data-driven research in this field depend on the acute awareness of the possible implications of decisions, methods, tools used by literary bibliographers. And conversely, it depends on whether literary bibliography can open to and assimilate the insights coming from e.g. the sociological and cultural developments within historical bibliography (McKenzie, 1986; Gupta, 2015), or the new wave of documentation studies (Lund, 2009; Buckland, 2015), which examines e.g. theoretical issues of “documenting” and its implications in contemporary culture (Day, 2014; Ferraris, 2012).   CLB and PBL as scholarly data infrastructures CLB and PBL are examples of scholarly infrastructures that gather rich and large datasets that aim at reconstructing the “literary system” in a most comprehensive way possible (Bode, 2017: 88). They are unique resources of open bibliographic metadata, covering respectively 250 and 70 years, each containing app. 0,7 mln records in database form, and app. 1,5 mln records in form of scanned pages/cards of bibliographic information. Departments responsible for their creation have been continuously running for 70 years, consisting of teams of app. 15 bibliographers working full-time. In the last decades they transformed their bibliographic output from the original printed form (books) to the database versions with proprietary system and public interfaces that present large, deeply structured datasets.  The information they gather represents an augmented perspective on the complexities of literary culture as a whole. It deals with literary, artistic and scientific,  publications (books, journals, plays), their  reception, literary  events (competitions, prizes), and the organization of literary  scientific life. The special attention is paid to the  actors of the literary life (authors, publishers, journalists, scholars etc.; with emphasis on the attribution of the authorship, codenames, pseudonyms etc.), and the  cultural and scientific institutions and their activities.  For each year CLB and PBL process thousands of literary books, and hundreds of journals in order to register information on a broad range of literary subjects outlined above. The idea behind literary bibliographies is an idea of  recreating the complexities of literary system through bibliographic metadata:  - they aim for completeness of information, processing relevant sources (nearly) in totality – books, parts of the books, proceedings, articles from the journals and newspapers, in the print and electronic (CLB) form; - they deal with the broad cultural environment: register daily journals, presence of literary issues on radio, TV, the Internet (CLB); - they aim for methodological stability through decades of processing sources of information, and they rely on stable teams of bibliographers (specialists in literary studies); - they use the same infrastructure for broad range of topics – information about the literary figures, works, organisation, events etc. are interconnected; - they are a long-running projects with stable financial foundation.   The “missing link” between data producers and data researchers While certain researchers – as Bode argues – fail to recognize or describe the limitations of datasets they work on, more thorough and sustained discussion between the researchers in the field of data research and data producers,could lead to both better research and better preparation of bibliographic data.  Cooperation in bridging empirical data studies in literary research and regular data production will lead to a better understanding of how certain literary issues (sociological, theoretical, thematic, etc.) can and should be represented in the form of bibliographic datasets, not only from technological point of view (which is less of an issue), but from a literary point of view.  The paper will expand on the complexities of 5 crucial issues from CLB and PBL databases that should be the subject of such analysis: 1) very basic yet underappreciated issue of readiness of bibliographic databases for quantitative analysis and statistical representation; the bibliographies have not traditionally taken this kind of usage of their data into account (the very small or relatively unimportant information can produce many records); this can be taken into account by the researcher, but this issue is so prevalent, that it needs to be addressed; 2) balancing the stability of metadata (especially subject headings) with the sensitivity to changes in terminology; in large databases the subject headings could be changed without the “memory” of the previous ones which is sometimes needed – in case of mistake on inherent prejudices (Knowlton, 2005) – but sometimes it might induce ahistoricity; 3) how to account for the incompleteness and heterogeneity in data and the assessment of its impact on data research? Not all incomplete information is retrospectively traceable, as the literary bibliographic databases have not valued possible statistical uses; for example – for different years there might be journals that have not been processed due to very practical reasons (lack of access to the issue, etc.), or materials can be processed using different methods (from autopsy, from data extraction etc.); 4) the introduction of “new” or “atypical” forms of documents into the literary bibliographic databases – like Internet documents, or grey literature, samizdat, performance, stand-up, video games etc. – with the awareness of limited workforce available (issues of prioritizing); 5) the relative lack of subject analysis of artistic texts in bibliographical databases: How it should be accounted for? May it be supplemented with the results of textual studies? How big of an issue it is for the development of further research? Issues like these cannot be solved only by data producers, but rather through cooperation of data producers and users, especially the researchers. The broader cooperation between both sides can significantly develop the possibilities for the data-driven research (not only) in the field of literary studies. Funding: Activities of Czech Literary Bibliography Research Infrastructure are supported by the Ministry of Education, Youth and Sports of the Czech Republic (Project Code LM2015059).  ",
        "article_title": "From a Reference Book to Research Data: Literary Bibliographies as Sources for the Data-driven Research",
        "authors": [
            {
                "given": "Vojtěch",
                "family": "Malínek",
                "affiliation": [
                    {
                        "original_name": "Institute of Czech Literature of the Czech Academy of Sciences, Czech Republic",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tomasz",
                "family": "Umerle",
                "affiliation": [
                    {
                        "original_name": "Institute of Literary Research of the Polish Academy of Sciences, Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Piotr",
                "family": "Wciślik",
                "affiliation": [
                    {
                        "original_name": "Institute of Literary Research of the Polish Academy of Sciences, Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "digital research infrastructures and virtual research environments",
            "bibliographic methods / textual studies",
            "interdisciplinary & community collaboration",
            "English",
            "cultural analytics",
            "library & information science",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Abstract We describe BigSense, a neural network-based approach for highly efficient word sense disambiguation (WSD). BigSense uses the entire English Wikipedia disambiguation pages to train a model that achieves state-of-the-art results while being many times faster than its competitors. In this way it is possible to disambiguate very large amounts of text data with reference to the largest freely available disambiguation model, while the time complexity of the model remains manageable. Thus, our approach paves the way for large-scale disambiguations in text-related digital humanities.   1. WSD is an indispensable task in the field of Natural Language Processing (NLP). In this paper, we describe BigSense, a neural network based approach for efficient WSD. This approach is based on a previous model (Anonymous, 2018), which worked well for the German language. BigSense is extended to English using all disambiguation pages of the English Wikipedia. We improved BigSense’s predecessor so that we again achieve state-of-the-art results, despite being much faster than our competitors. In our Wikipedia-based experiment we achieve an F-score of almost 90 %. This enables the use of WSD also for larger corpora and brings us one step closer to the goal of machine reading on the basis of largely disambiguated texts.   2. Related Work The general approach of BigSense was motivated by fastText (Joulin et al., 2016). This is because fastText is a very efficient way to classify data. We transposed fastText to WSD in order to efficiently determine the meaning of ambiguous words. By this we mean scenarios in which hundreds of thousands of different words are ambiguous.  (Mihalcea and Csomai, 2007; Ferragina and Scaiella, 2010; Ratinov et al., 2011b; Ratinov et al., 2011a; Agerri et al., 2014; Moro et al., 2014) describe similar tools to BigSense since they perform Entity Linking, where they link text segments to knowledge databases such as Wikipedia. BigSense, however, has its focus on the disambiguation of ambiguous words.  The creation of the disambiguation corpus, using Wikipedia’s link structure, was similarly performed by (Mihalcea, 2007). However, they did not use the corpus to disambiguate all ambiguous words, but compared them with the Senseval 2 dataset using a subset of 30 nouns. (Mihalcea et al., 2004; Chaplot et al., 2015) utilize graph based algorithms operating on semantic networks to perform WSD. (Yuan et al., 2016) present two WSD algorithms, achieving the best results by means of a semi-supervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity. (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Raganato et al., 2017a; Melamud et al., 2016) define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network (Hochreiter and Schmidhuber, 1997).  Unlike these approaches, we present a method that can handle big data: in terms of the number of senses to be distinguished and in terms of the number of units to be disambiguated. On the one hand, knowledge driven approaches using, for example, WordNet and related resources are limited in terms of the number of senses distinguished by them. On the other hand, approaches that rely on algorithms like PageRank or classifiers like SVMs or LSTMs are limited in terms of their time efficiency. Their runtime can last up to weeks and months when handling the amounts of data used in this paper (see Table 1). Therefore, we need a versatile and efficient method for WSD, as presented in the next section.   3. Model  3.1. Architecture The artificial neural network (ANN) used by BigSense builds upon a previous model architecture by adding an additional fully-connected hidden layer and replacing the sigmoid output layer with a dynamically reduced softmax layer (Figure 1). We found out that this additional hidden layer improves performance. Adding more hidden layers did not bring a performance boost anymore, however it slowed down the runtime. Each ambiguous word is assigned a distinct set of senses (i.e. each sense/output class belongs to exactly one word), which are gathered together for all ambiguous words in a training batch and make up the reduced output layer. This enables fast training, despite relying on the computationally more expensive softmax function. Possible senses are chosen based on the word to be disambiguated and are part of the input of the ANN. Instead of using global averaging pooling to combine embedding vectors, their sum is divided by the square root of the number of input words.  We decided against using Dropout, since it did not appear to have any beneficial impact.    Figure 1: Model architecture of BigSense.   3.2. Data Training and test corpora are generated using links of Wikipedia articles. Disambiguation page titles are used to find ambiguous words. Each title is simplified (i.e. lowercased, no leading articles, and no trailing text in parentheses) and assigned as word to a new sense group. Titles of redirects to disambiguation pages are also added.  Sense groups combine similar ambiguous words which can assume the same set of senses. Senses are URLs to Wikipedia articles that can refer to full articles or single sections. A sense may only belong to exactly one sense group, to ensure the same subset of senses are always present in a reduced output layer of the ANN. Therefor the same URL may be used for multiple senses (see Figure 2).    Figure 2: Relationships between words, sense groups, senses and article URLs. Senses are found using two methods: First, links on disambiguation pages are added as senses to the assigned sense group of the page. Links in section  See also and links that do not contain the simplified disambiguation page title are ignored. Second, if there is an article or section that is linked at least five times using a link with the same simplified title as a sense group, it is added as sense to that group. Redirects are resolved before counting.   Each paragraph in Wikipedia is used as a training example for the senses corresponding to the article or section it is from. In addition, any paragraph that contains links is used as example for senses that are assigned to the link destinations. To improve the quality of examples, paragraphs need to contain at least 5 tokens. Paragraphs that contain HTML or Wikicode tags are discarded, as well as tables. Text generated by most templates will simply be ignored.  A training example consists of one tokenized paragraph, a list of possible senses for one ambiguous word and the correct sense. Most paragraphs are assigned to multiple senses in different sense groups and therefor reused multiple times. A sense group needs to have at least one sense or it will be removed as well.    4. Experiment We applied BigSense to our Wikipedia generated corpus as well as to Senseval and SemEval tasks.  4.1. Wikipedia-based Disambiguation The large number of articles in the English Wikipedia allowed us to generate 121,275,847 training examples (173,236,161 including test data) from 31,842,587 relevant paragraphs. Our approach yielded 549,770 senses for 478,077 article URLs in 168,546 sense groups. On average a sense group contains 3.3 senses. Around 40% of all sense groups only contain one sense, which is reflected in the MIN baseline (worst-case accuracy).  A wide range of parameter configurations were tested and were able to achieve an F-score of up to 89.5 % (Table 2). Instead of using tokens directly, we exclude punctuation marks, build bi-grams and hash them using 10,000,000 buckets. The embedding and hidden layer both have a size of 25. Micro batches of 32 examples each were used to train the network using gradient decent with exponentially decaying learning rate (starting at 1.0) over 8 epochs.   Tools 1,000 5,000 all   Wikifier  16 min 41 s 1 h 24 min ≈ 302 days   Illinois 6 min 53 s 24 min ≈ 77 days   IXA 58 min 49 s 4 h 47 min ≈ 3 years   Babelfy 1 min 50 s - ≈ 33 days   TAGME 5 min 42 s 28 min 40 s ≈ 98 days   BigSense 10s 13 s 16 min 49 s   Table 1: Runtime-related evaluation regarding similar tools using 1000, 5000 and all test instances.   Type F1 micro F1 macro   BigSense 0.895 0.779   MFS Baseline 0.591 0.405   MIN Baseline 0.057 0.124   Table 2: Evaluation of BigSense in the Wikipedia experiment.   4.2. Senseval and SemEval related Disambiguation SemCor 3.0 (Mihalcea, 2016) was used to train models to evaluate Senseval and SemEval related tasks. We used the same parameters as we did for the Wikipedia data, except for gradient clipping, which we had to enable, because the learning rate of 1.0 was too high. Training duration was also extended, to suit the smaller data set. We evaluated models on the test data after each epoch until the moving average of the cost function did no longer improve. In some of these experiments we had lemma and part-of-speech (POS) information, which we also considered as parameters. Table 3 lists the results for Senseval 2 (English all-words) (SE2) and Senseval 3 (English all-words) (SE3).    NG Epochs SE2 SE3   Token 1 95 0.718 -   Token 2 79 0.710 -   Lemma 1 73 0.708 -   Lemma 2 88 0.706 -   Token + PoS 1 66 0.709 0.688   Token + PoS 2 87 0.705 0.693   Lemma + PoS 1 76 0.709 0.702   Lemma + PoS 2 100 0.708 0.694   Table 3: F1-score for Senseval tests. (NG = n-grams, Ep. = epochs) Table 4 lists results for SemEval-2013 Task 12 (SE13), and SemEval-2015 Task 13 (SE15). Some test cases require the classification of senses that are not included in the training data. However, the neural network cannot predict classes it has never seen before. In these cases we proceed by classifying the most frequent sense in WordNet.    NG Epochs SE13 SE15   Token 1 64 0.682 0.697   Token 2 76 0.685 0.708   Token + PoS 1 47 0.642 -   Token + PoS 2 52 0.623 -   Table 4: F1-score for SemEval tests. (NG = n-grams, Ep. = epochs) We also conducted an experiment where we ignored all test classes that did not occur in training set (see Table 5). In this way, we can find out what classification quality we achieve for a subset of the test set for which we have training data available.    SE2 SE3 SE13 SE15   BigSense* 0.906 0.889 0.842 0.788   Table 5: F-score of BigSense, when ignoring unknown test cases.   4.3. Discussion We have successfully developed a classifier that can not only efficiently disambiguate huge amounts (see Table 1) of data, but can also compete with the state-of-theart. In order to compare with other WSD methods, we have carried out Senseval and SemEval tests. Here we were able to show that we can keep up with state-ofthe-art methods and even surpass them in some fields (see Table 6). In addition, we have shown that we can achieve 90% F-score if we only consider senses that were included in the training set (see Table 5).   Model SE2 SE3 SE13 SE15   Iacobacci, 2016 0.634 0.653 0.673 0.715   Yuan, 2016 0.736 0.692 0.670 -   Chaplot, 2015 0.605 0.586 - -   Raganato, 2017 0.720 0.702 0.669 0.724   Melamud, 2016 0.718 0.691 0.656 0.719   BigSense 0.718 0.702 0.685 0.708   Table 6: Comparison of BigSense to state-of-the-art methods.    5. Conclusion We presented a novel approach called BigSense for disambiguating large amounts of data. In order to present the efficiency and quality of BigSense, we have created a huge disambiguation corpus using the English Wikipedia. Here we have classified almost 550,000 senses with an F-score of 89.5 % (see Table 2). In Senseval and SemEval tests we can keep up with state-of-the-art methods and in some cases even surpass them. In future work we will analyze the influence of topic classifiers on BigSense.   ",
        "article_title": "BigSense: a Word Sense Disambiguator for Big Data",
        "authors": [
            {
                "given": "Tolga",
                "family": "Uslu",
                "affiliation": [
                    {
                        "original_name": "Goethe University of Frankfurt, Germany",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            },
            {
                "given": "Alexander",
                "family": "Mehler",
                "affiliation": [
                    {
                        "original_name": "Goethe University of Frankfurt, Germany",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            },
            {
                "given": "Clemens",
                "family": "Schulz",
                "affiliation": [
                    {
                        "original_name": "Goethe University of Frankfurt, Germany",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Baumartz",
                "affiliation": [
                    {
                        "original_name": "Goethe University of Frankfurt, Germany",
                        "normalized_name": "Goethe University Frankfurt",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04cvxnb49",
                            "GRID": "grid.7839.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "semantic analysis",
            "artificial intelligence and machine learning",
            "natural language processing",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " DHK12  Short Paper Proposal  DHK12 (  www.dhk12.com)  is a free, unrestricted, online open-access collaborative network of educators who seek to:    use digital tools to make the humanities come to life for students draw on the scholarship of women and people of color to diversify curricula support students as they do the work of historians by creating knowledge.    This year, DHK12 is in the process of launching two digital projects -- built by students, the people who will need the tools to deal with the complexities of the future -- that will contribute to public scholarship in digital indigenous studies, digital black studies, Africana and diasporic studies, digital queer studies, and digital feminist studies. With these interdisciplinary and transnational, student-driven digital archive projects (currently in progress; please see full abstract for project descriptions), we will build complex models of memory and commemoration, analysing our data with computational methods and communicating the results to a broader public.   DHK12 is part of an ethical and political imperative for a growing number of teachers and scholars committed to accountable and reciprocal research practices and knowledge-sharing. As producers of an open-access interdisciplinary curriculum and network, we are organizing academic knowledge production away from the profit motive of textbook publishers. Instead, we use primary source documents and digital archives, and work collaboratively with local cultural institutions to teach DH thinking and skills to primary and secondary school students. DHK12 develops projects to teach students and teachers how to use computational text analysis, digital mapping and timelines, image processing, and 3D modeling to develop new epistemologies, ontologies, and ways of knowing and understanding public humanities and societal engagement.  DHK12 Student Project #1 (Fall and Spring 2018-2019)  Our digital indigenous studies project,  The Red Atlantic (  www.theredatlantic.com)  , explores “internal settler colonialism” versus “external imperial colonialism.” Questions that we explore include: Is there a different representation or story about Indigenous people and what happened to them produced by the American Museum of Natural History (NYC) versus the Natural History Museum (London)? How do different types of colonialisms tell different stories about the encounter or the colonial project in 3 dimensions? Using computational text analysis, we will analyze internal records to ask: What are the internal debates? What do the debates around changing exhibits sound and look like? What are the issues?    This year, K-12 students in NYC photographed dioramas and mounted exhibits at the American Museum of Natural History (AMNH) and we built a digital archive that places the museum’s narrative side by side with historical and artistic representations of culture created by indigenous scholars. Our objective is to use digital tools to decolonize the museum and open up spaces for indigenous scholars and artists within locations traditionally identified with dominant representations of colonialism. As a local and international public space, AMNH is a powerful producer of historical narratives and our digital archive seeks to re-center the voices and ontologies of indigenous scholars within the epistemological “origins story” of the American continent. For next year, we plan to partner with a K-12 school in London to build phase two of our archive, in which we decolonize the representations of indigeneity at the Natural History Museum by placing current exhibitions and collections in conversations with contemporary work created by indigenous scholars and artists.  DHK12 Student Project #2 (Spring and Fall 2019)  Rainbow is Enuf (  www.rainbowisenuf.com)  is a digital archive documenting the remarkable tenacity of black women, trans women, and femmes’ visual, cultural, and political influence on American history. This black joy and black excellence archive draws on and contributes to digital black studies, digital queer studies, and digital feminist studies. Currently, in public and independent American schools, children learn about black American life by studying enslavement, the 13th Amendment of the U.S. Constitution, Jim Crow Laws, redlining/blockbusting of housing, and most recently, the prison industrial complex. To offer students an opportunity to understand the complexities of these histories more completely, our project combines digital research and digital production that allows students to explore the joy and excellence of black labor and resistance through the study of music, film, dance, art, comedy, theatre, and food. Students will utilize  digital resources such as  Digital Public Library of America  ( DPLA) and Stanford's Tooling up for DH   http://toolingup.stanford.edu  .   The historiography of the project highlights and centers contributions of black women as creators of knowledge. By presenting students with new information using primary sources and giving voice and volume to marginalized histories, we seek to decolonize U.S. History curricula. Our contribution to digital humanities in general and to the arts, Africana and diasporic studies, and social justice research in particular is a digital archive that is free, unrestricted, and open access, for use by researchers, teachers, students, writers, curators, community organizers, and activists from around the world who are dedicated to the interdisciplinary study of U.S. culture and history in a global context.   As a K-12 teacher and junior scholar in American Studies, I hope this short paper presentation will enable me to build partnerships with DH scholars worldwide to continue to develop interdisciplinary K-12 digital humanities curricula that use and contribute to interdisciplinary scholarship, teaching, and creative student-centered work.  The presentation will have a “lessons learned” recommendation section for others who wish to utilize digital humanities for pedagogy within the K-12 systems, as well as students’ reflections about their experience.   ",
        "article_title": "DHK12: Open-Access Digital Humanities Curricula for K-12 Schools",
        "authors": [
            {
                "given": "Rosalie",
                "family": "Uyola",
                "affiliation": [
                    {
                        "original_name": "Trevor Day School, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-08",
        "keywords": [
            "diversity",
            "open content and open science",
            "interdisciplinary & community collaboration",
            "african studies",
            "English",
            "pedagogy",
            "teaching",
            "indigenous studies",
            "and curriculum",
            "scholarly publishing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " There is a critical point to be made about proper use of metadata in digital history. With any computational analysis of a large historical dataset, there is a strong temptation to approach the dataset as holistic representation of the language and intellectual landscape of its era. Digital history projects are often rightly criticised for having a naive approach to sources resulting in simplification of complex phenomena (Leca-Tsiomis, 2013; Bode, 2017). In this paper we demonstrate a way to avoid this, and how proper use of metadata is necessary for serious corpus control and digital source criticism. This work makes two contributions to the history of the book and digital history. First, we present a methodological approach for creating a historical biographical database from a bibliographical catalogue. Second, we demonstrate solutions for forming a uniform dataset from a noisy and heterogenous starting point. This opens new opportunities which earlier historical research using bibliographical data has missed due to problems of data quality and coverage (Raven, 2007: 193). For example, while publisher networks had a greater impact on the distribution of ideas in early modern period than has been realised, publisher information as a source has not previously been extracted at scale, despite its potential to change the way we study intellectual history. Additionally, as this work is part of wider intellectual history research project, and the dataset produced here is combined with other bibliographical research strands, there are more general claims with regard to the utility of proper metadata in quantitative computational book history.  Data The English Short-Title Catalogue (ESTC,  http://estc.bl.uk) is a standard source for analytical bibliographic research (Lahti et al., 2015) holding close to half a million titles with varying but substantial coverage of printed items published in English from 1473 to 1800 (Raven, 2014: 14). Following MARC 21 guidelines, the data in the catalogue closely matches that found in the original published titles: names, years, publisher imprints, etc. are documented as they appeared on the title pages of the printed originals. The raw data presents major challenges for computational approaches to analysis, however. Bibliographical data is compiled around published titles, and all data points are handled as simple strings of text, instead of independent and unique objects. Thus a new, more structured, data model is required. To this end, we implemented a relational model, where each actor is an independent and unique object connected to all the titles they were involved with.  The names of entities in the ESTC which this study focuses on (book trade actors and their geographic locations) are ‘hidden’ in the text of the full publisher statements, and thus had to be identified in the imprint. As a first step, a machine learning based named entity recognition (NER) package offered by the Stanford NLP Group (Finkel et al., 2005) was used for entity name identification. Following extraction, various heterogeneous textual representations (spelling conventions varied widely in the period) of the same entity were identified, linked, and collected to create objects with unique identifiers for each historical actor. A data matching process was applied to test if names detected in the publisher statements could be linked with distinct entities found in external databases (British Booktrade Index ( http://bbti.bodleian.ox.ac.uk) and Virtual International Authority File ( http://viaf.org)). In the cases where such entries did not exist, similar entities were grouped with rules based logic. Previous efforts to distill the publisher information from the ESTC have not incorporated these essential linking and unification steps, or approach the problem with labour intensive solutions (eg. Shakeosphere ( https://shakeosphere.lib.uiowa.edu), Map of Early Modern London ( ) and ImprintDB ( )).  The end result of the unification process is a bibliographic database of 900,000 non-unique records harmonized into fewer than 200,000 actors, of which 30,000 are identified as being part of the book trade. Between these actors we have been able to make roughly 800,000 individual connections. These relations are documented in linking tables similar to a linked data database. The benefit of a flexible linked data model is that it allows natural extension and modification as research based needs arise. We claim that this is currently the state of the art dataset covering the early modern English book trade.   Discussion Multiple previous historical hypotheses, based on both geographically and chronologically limited sources, can now be tested at a scale that covers all the publications in the ESTC. Questions of location (Harris, 1982; Raven, 2014), spread of the trade outside London and the importance of networks and connections to that process (Maxted, 1982; Raven, 2007: 141), and questions of authors’ and publishers’ relations (Treadwell, 1983; Isaac and McKay, 1999) can all benefit from a statistical re-examination. Previously unidentified niches, subgroups, and structures in the social networks of the book trade can be discovered through quantitative data exploration. Another potential for this type of book trade metadata can be identified with regard to corpus control for researchers utilizing large full text collections (eg. Early English Books Online (EEBO), or Eighteenth Century Collections Online (ECCO)). While large scale historical text corpora strive to encompass “everything”, by their they nature introduce multiple layers of statistical bias into the data. By making use of linked metadata one can focus text mining efforts on historically meaningful subsections of large text corpora. In fields such as historical computational linguistics, the standard solution has been to limit the corpus to a relatively small manually curated one. Large historical text collections do not typically come supplied with the kind of metadata that would allow properly subsetting them on a large scale, but with the methods presented in this paper, that becomes possible. This work demonstrates a general method for generating a linked biographical database from library metadata catalogue, and shows the benefit of using this as starting point for historical research. While at this stage the primary users of the data are the historians in the research group, as the project progresses the tools and data will be published following good practices for open science, such as adhering to a “tidy” data model (Wickham, 2014), proper code documentation, and open repositories. Additionally, the methods can be adapted to a variety of existing national and transnational bibliographical metadata resources.   ",
        "article_title": " Publishers, Printers and Booksellers - Implications of Properly Structured Metadata for Digital History  ",
        "authors": [
            {
                "given": "Ville",
                "family": "Vaara",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Hill",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            },
            {
                "given": "Mikko",
                "family": "Tolonen",
                "affiliation": [
                    {
                        "original_name": "University of Helsinki, Finland",
                        "normalized_name": "University of Helsinki",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/040af2s02",
                            "GRID": "grid.7737.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "bibliographic methods / textual studies",
            "history and historiography",
            "English",
            "theory and methodology)",
            "digital archives and digital libraries",
            "metadata"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The project  The Riddle of Literary Quality  http://literaryquality.huygens.knaw.nl  aimed to find correlations between texts of novels and judgments of their literary quality. In other words: is the literariness of novels associated with or even explained by text-intrinsic properties? The 2013  National Reader Survey (NRS) collected a wealth of information on perceptions of literary quality of contemporary novels. It turns out that a machine learning model can predict the literary judgments based on the texts to a substantial extent; based on word frequencies and syntactic patterns, 61% of the variation in ratings of novels is predictable from purely textual features (van Cranenburgh & Bod, 2017; van Cranenburgh et al., 2018). This demonstrates that the text contains enough clues to distinguish literary texts from non-literary texts. However, we do not know to what extent humans rely on textual features when rating how literary a text is, since we collected judgments on whole novels by presenting the participants with the title and author of each novel. For the same reason it was not possible to identify the contribution and influence of particular aspects of the text. What we need is a blind experiment in which literariness is judged purely on the basis of text, without revealing any other information.  We therefore propose a new survey, based on fragments from the novels used in the NRS, to collect evidence that text-intrinsic characteristics play a role in ratings of literary quality, and investigate exceptions where we suspect various biases may play a role (cf. Koolen, 2018). The results will tell us more about how perceptions of literariness are formed and which particular textual aspects play a role. They will also enable a direct comparison between the performance of humans and a computer model on this task.   Motivation The NRS made clear that genre plays a role in judging literary quality. In the survey, Dutch respondents were asked to rate recently published novels on a scale of literary quality (1–7) and asked to motivate one of their ratings by an answer to the question “Why did you rate this book with the score for literariness as you did?” Respondents gave roughly three types of response, exemplified by Examples 1–3.  “It is suspenseful, the storyline is perfect, but in a literary novel I expect a deeper layer.”  “It’s chicklit”  “Too light, simple, chicklit reads easily, but does not amount to much.”  First, as expected, style and narrative structure are important (1). But in explaining why they found a novel not to be literary, respondents also often found it sufficient to refer to genre, without referring to textual qualities (2). It is possible that those textual qualities are implied. Some respondents did elaborate and explained low ratings in terms of both genre and style (3). However, genre exclusion may also point to bias. If a novel with a pink cover is excluded from a high rating without further explanation, what does that mean? Are we judging the text or repeating ‘common sense’ ideas on literary quality without questioning? The first indication that extrinsic factors play a role are large gaps between the prediction of the computer model and reader judgments. The translation of  The sense of an ending, for instance, received the highest average rating, 6.6, whereas the model predicted 5.4. This novel was awarded the Man Booker Prize the year before, which has probably influenced respondents. For  Eat, Pray, Love, this was the other way around: the computer predicted 4.7, while readers gave it a 3.5.  A preliminary survey, conducted at a meeting of the KNAW Computational Humanities Program, showed that bias might play a role. We offered a handful of visitors five fragments (approximately one page of text), extracted from novels surveyed in the NRS. Respondents were asked: does this fragment originate from a novel with a high or low rating in the NRS? We anonymized the text by abbreviating names as initials. Remarkably, a fragment from Elizabeth Gilbert’s  Eat, Pray, Love was the only fragment that all respondents picked as a highly rated novel—which it was not.  Simkin (2013) conducted an online quiz, showing that average readers perform no better than chance at distinguishing a canonical (Dickens) from a non-canonical (Bulwer-Lytton) author. However, the fragments were short (3-4 sentences) and participants were not selected to have affinity with literature. Given these results, it is interesting to test the influence of text and bias on literariness in a carefully designed survey.   Survey setup The two most important questions for the survey setup are who the participants will be, and what they will rate. We aim to select participants with literary affinity or expertise. To prevent the influence of author prestige, respondents should not see any metadata; nor do we want to cherry pick fragments. A double-blind setup with anonymized fragments will allow for this—we will set up a computer program to select equally sized fragments at fixed or random points from several novels. A trade-off needs to be made for fragment length; several sentences is too short, but more than a few pages takes too much time. Instead of a 7-point Likert scale, as in the National Reader Survey, we will present pairs of fragments, and ask the rater which is the more literary one (pairwise ranking aggregation). This has the advantage of forcing the rater to make a concrete comparison, instead of expecting each rater to have an existing, well-calibrated scale. Rankings can be computed with the Elo rating system, the same system used to rank chess players. In addition, we can ask for a motivation. We intend to run two experiments. The first experiment tests whether participants pass 'the challenge' and measures how humans perform at the task of recognizing literariness from unmodified text fragments. The second experiment introduces manipulations of fragments to confirm the influence of particular features, e.g., protagonist gender, sentence construction, topic. This approach is followed by Blohm et al. (2018), who present an experiment on lines of poetry rated for poeticity and grammaticality.  ",
        "article_title": " The Literary Pepsi Challenge: intrinsic and extrinsic factors in judging literary quality  ",
        "authors": [
            {
                "given": "Andreas",
                "family": "van Cranenburgh",
                "affiliation": [
                    {
                        "original_name": "University of Groningen, The Netherlands",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            },
            {
                "given": "Corina",
                "family": "Koolen",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, The Netherlands",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-21",
        "keywords": [
            "crowdsourcing",
            "stylistics and stylometry",
            "English",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Background and Over the past 2 years a number of researchers from various backgrounds have been working on the exploitation of digital techniques and tools for working with oral history (OH) data.  The result of a CLARIN workshop in Arezzo    https://oralhistory.eu/workshops/arezzo   , May 2017, was the idea of a so called Transcription Chain as a webportal where researcher could upload their audio files, have them transcribed by Automatic Speech Recognition (ASR) and could edit/correct the text results with a speech editor (Van den Heuvel et al., 2017).   The Transcription Chain (TC) can be considered as a couple of concatenated different software tools that ingest Audio and or Video documents and output Time-stamped Transcriptions (TT)    A Timed Transcription is a transcription of the spoken content where each transcribed object (mostly words, but, when possible, laughing, crying, and other non-verbal utterances) has a start-time and a duration or end-time.  . A TC can be a set of software packages stored and run on a personal computer, but in this proposal, we see a TC as a set of web based tools, running on one or more computer servers “in the internet”. A TC typically uses different tools that run on different servers in different countries.   The TC as defined in the Arezzo-workshop contains two basic elements: Transcription and Alignment. Transcription of the (spoken) content of an AV-document can be done in two ways:   Automatically by an ASR-engine eventually followed by manual checking and correcting the recognition results Manually, eventually followed by a forced alignment to receive a TT with the start- and end-times of all spoken words.  After (post)editing of a transcription it needs to be (re)aligned with the speech signal. Several alignment tools can be used for this operation. The TC was implemented as a OH Transcription portal by developers of the Bavarian Archive for Speech Signals (BAS) in Munich. In this contribution we address the implementation of the portal (and its URL), the first experiences as reported in a follow-up CLARIN workshop in Munich (see also Van Hessen et al, 2019), and our future plans with the portal.   Implementation of the Portal A prototype implementation of the OH portal has been set up at the Institute of Phonetics and Speech Processing. It can be accessed via the following URL:   https://www.phonetik.uni-muenchen.de/apps/oh-portal/   The portal works like a dynamic spreadsheet: the columns represent files and processes, the rows are individual files. Files enter the leftmost column, and then proceed from left to right through the spreadsheet. This way, one can monitor the progress of one’s data through the transcription chain. At every step in the processing can intermediate results be downloaded to the local machine. This transcription chain currently consists of four steps (see also Fig.1):  Data upload and verification Automatic speech recognition (ASR) Manual verification and correction of the recognised text Automatic word and phoneme alignment and segmentation  The upload and verification step transfer the audio data to the server and check the file format, e.g. convert stereo files to two mono files. Then, the user is asked to select the ASR language. Currently, the portal supports English, Dutch, Italian and German. ASR is performed by academic partners such as the University of Twente, Radboud University Nijmegen, Sheffield University, or European Media Lab, or commercial service providers such as Google. Note that most ASR service providers store the audio data and to use them to improve their services – this is a severe problem for privacy reasons. Steps 2, 3 and 4 are mandatory: if the results of the ASR are known to be very reliable, then the manual verification can be omitted. On the other hand, if for some reason ASR does not work for the given files, one can skip ASR and proceed to the manual transcription of the file directly. In some cases, fine-grained word alignment is not needed, and hence it can be switched off. Figure 1: Main screen of the TC portal showing the four processing steps   This is indicated by the checkbox in the column head. Figure 2: Output formats of the web interface   It is the nature of a portal that many sites can access the portal server simultaneously, and that the portal does not perform the services itself. Instead, it calls external service providers, e.g. ASR services, passes the data to these servers, and processes the results. In the current beta version of the portal, queuing of incoming requests is a bottleneck, because the portal has to wait until one job has been processed before starting the next. Hence, the portal cannot estimate how long processing will take, it cannot inform its users about estimated time of termination, and it cannot reorder the queue to optimise throughput. The verification and correction of the ASR outcome is done using the Octra editor (Pömp, 2017). Octra features three different graphical user interfaces for efficient transcription. The innovative 2D editor displays longer signal fragments on the screen with good time resolution. Human transcribers set boundaries in signal pauses and then transcribe the signal fragments between these pauses.     Figure 3: OCTRA interface for manual correction of the transcriptions    Octra is fully integrated into the portal, so that files opened with Octra will automatically be sent back to the portal for the subsequent processing steps.   User Experiences  Most OH researchers indicated that they prefer flawless transcriptions because they use the textual results for the final analyses. Only a few indicated that they used the transcriptions to quickly find the audio passage in question.  Another issue is that most of the recorded speech is not grammatically correct. However, solving this problem (the ungrammatical speech) is impossible because it would be tantamount to interpreting the text. Nonetheless, to increase the readability of the text, we tried to make “sentences” by adding a full stop after a pause of 400 msec or more. This isn’t a perfect solution but it made the text more “readable” according to the scholars in the Munich-workshop. The disadvantage is that you get a lot of short sentences when people speak hesitantly. Finally, we most recordings (interviews) contain multiple speakers. This is solved by speaker clustering: indicating when someone else was speaking. As a result, we get a transcript where a new paragraph is started each time the speaker changes. Diarization is not the same as speaker recognition, so we do not get a name or ID of the speaker but only an indication of the speaker (M1 is the first speaker, probably male). In general, we get more speakers than there are in reality.   Future work The OH portal is currently being updated to improve throughput and to adapt to changes in ASR services. We plan to implement a traffic light system to distinguish ASR services by their privacy policy, file quotas and language support. Furthermore, an authentication mechanism will be installed. In the first official release (version 1.0) the commercial engines were removed but the participating scholars were informed about the inclusion of commercial engines in the previous releases. To our surprise, some scholars argued that they had no objection at all to the use of commercial software because they had already posted their OH-interviews on YouTube. So, in the next release we will include them again and offer the scholars the option to use them or not. Furthermore, we will extend the service to more languages. Contacts are established for Polish, Czech, Swedish and Finnish. Finally, scholars in Munich asked us if it will be possible in the near future to add their own vocabularies. At the moment it is not, but hopefully it will be possible in one of the next versions.  ",
        "article_title": "A Transcription Portal for Oral History Research and Beyond",
        "authors": [
            {
                "given": "Henk",
                "family": "van den Heuvel",
                "affiliation": [
                    {
                        "original_name": "Radboud University, the Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Draxler",
                "affiliation": [
                    {
                        "original_name": "LMU, Munich, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Arjan",
                "family": "van Hessen",
                "affiliation": [
                    {
                        "original_name": "University Twente, Enschede, the Netherlands",
                        "normalized_name": "University of Twente",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/006hf6230",
                            "GRID": "grid.6214.1"
                        }
                    }
                ]
            },
            {
                "given": "Louise",
                "family": "Corti",
                "affiliation": [
                    {
                        "original_name": "University of Essex, UK",
                        "normalized_name": "University of Essex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02nkf1q06",
                            "GRID": "grid.8356.8"
                        }
                    }
                ]
            },
            {
                "given": "Stefania",
                "family": "Scagliola",
                "affiliation": [
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            },
            {
                "given": "Silvia",
                "family": "Calamai",
                "affiliation": [
                    {
                        "original_name": "DSFUCI, University of Siena, Italy",
                        "normalized_name": "University of Siena",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01tevnk56",
                            "GRID": "grid.9024.f"
                        }
                    }
                ]
            },
            {
                "given": "Norah",
                "family": "Karouche",
                "affiliation": [
                    {
                        "original_name": "Erasmus Studio, University of Rotterdam, the Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "multimedia",
            "speech processing",
            "data mining / text mining",
            "English",
            "computer science and informatics",
            "audio",
            "digital archives and digital libraries",
            "video",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Dealing with the complex data challenges offered by newspapers has never been easy, and for years historians have struggled to delve deeper into an archive that had only received the minimum metadata – title and date. However, since digital archives, historical research of newspapers has become easier than it ever was. For that, they deserve full credit. Yet when archives change their medium or storage, certain aspects of the documents they hold inevitably get lost. Similarly, when modes of access change, certain aspects of a source, particularly the aspects that are more complex than the interface offers easy access to, drift from the mind (Mussell, 2017). In a physical newspaper, the researcher would become intimately familiar with the spatiality of the texts they read from them. Front page or tucked in the back, set apart or clustered with articles about similar topics, these were all aspect that a researcher could use to gauge the importance of an article to the editor of the paper. Not so much with a digital archive. Keyword searches deliver the historian right to the text they requested, dropped directly on target without needing to consider a text’s surroundings. To address the question of textual spatiality, we developed a tool that allows (comparative) visualisation of the location of articles on newspaper pages in the form of a set of heatmaps. It relies on metadata included in the archives to deliver the researcher directly to the article they request, mapping image coordinates to the article. The coordinates of the area covered by these bounding boxes can be extracted at large volumes, tidied, and plotted on a page-by-page basis. This further improves upon existing tools, which estimate article size based on word count (Beals, 2018). These visualisations allow researchers to contextualise the space in which relevant articles appeared, by showing what a ‘typical’ newspaper’s pages looked like, and how the articles under investigation fit into that model. This paper proofs this technique on a case study on Imperial sentiment. By investigating the spatiality of the articles within the paper as a whole, it is possible to see the context in which British readers encountered the Empire, compare it to context of appearance for foreign news, and explore change over time. For example, we noted a prevalence of Imperial keywords on page 7, which continues through the 35 years under review – a total of 1800 issues. Closer reading of the paper reveals this to be the section for adverts and notices. Just this brief application shows the ways in which a transformative tool can be employed in conjunction with more traditional research methods.  These visualisations allow not only individual article sets to be plotted, but also allows a relative comparison between two sets of newspaper pages. The placement of articles mentioning Calcutta or Australia within Reynold’s Newspaper, a national weekly paper, can easily be compared with the placement of articles in, say, the Birmingham Daily Post, which only distributed regionally. Such a comparison shows both papers place articles on the Empire in very similar contexts, with patterns of appearance closer to national news rather than international or foreign news. Additionally, visualisations can be integrated with other textual analysis tools, such as topic modelling, as a way to allow more intuitive understanding of the contents of each topic, or even a comparative analysis of the places within a paper where certain topics occur. This technique allows an extra handle during the arduous process of discovering the semantic meaning of a topic. Reducing the complex data to a two-dimensional graphic is non-trivial, as a multitude of aspects need to be considered, such as scaling pages to the same size, Binning per pixel or binning per column, and rough versus precise binning. It also needs to draw data from various parts and indices in the archive to produce the kinds of meaningful visualisations that facilitate new and more substantive research questions. This kind project underlines the need for digital archives to allow researchers access to the data ‘raw’, not only limited access through a web interface (Fyfe, 2018).  In the context of newspaper research, these digital tools enable a whole field of study in newspapers as objects, with articles having their own spatiality made accessible for research on a larger scale than previously possible. In the context of the Digital Humanities, these tools show how digital tools can drive methodological innovation in other fields, and how computers can, while on the one hand disconnecting us from the complexities of our sources, also serve as vehicle for making those complexities understandable. ",
        "article_title": "Durchdruck im Fokus: Visualising the Spatiality of Articles in Historical Newspapers",
        "authors": [
            {
                "given": "Quintus",
                "family": "Van Galen",
                "affiliation": [
                    {
                        "original_name": "Edge Hill University, United Kingdom",
                        "normalized_name": "Edge Hill University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/028ndzd53",
                            "GRID": "grid.255434.1"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Hall",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-Universität, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Bob",
                "family": "Nicholson",
                "affiliation": [
                    {
                        "original_name": "Edge Hill University, United Kingdom",
                        "normalized_name": "Edge Hill University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/028ndzd53",
                            "GRID": "grid.255434.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-28",
        "keywords": [
            "corpus and text analysis",
            "content analysis",
            "English",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this study discussions about the punishment of collaborators and war criminals in the Netherlands are investigated by analysing the verbatim minutes of parliament. The application of text mining techniques to this digitised historical text corpus allows for a diachronic perspective on a historical case study. With this paper, I present the historical case, the materials and techniques used, and some insights based on preliminary results. I will also address general advantages and limitations of using text mining in historical research. Aim of this investigation is to explicitly investigate and discuss the validity of the use of emotion lexicons in diachronic historical research.  Historical case After the Netherlands was liberated in May 1945, the issue of the punishment of collaborators and war criminals became acute. At peak, during the summer of 1945, more than hundred thousand suspected people were kept in custody within the Netherlands. For contemporary politicians, this issue was pressing and demanded quick action. Heated debates in Dutch parliament followed. This was, however, not the end of the story. For the next five decades, the question caused recurring heated political controversies. The debates in Dutch parliament about the punishment, penalty reduction, or release of these people are not only among the longest debates in Dutch parliamentary history, but are generally considered to have been the most emotionally charged (Bootsma and Griensven, 2003; Tames, 2013).  Controversies kept recurring in 1952, 1966, 1972, and 1989. The 1952 controversy started when Queen Juliana proposed (by royal decree) to turn the death sentence of German war criminal Willy Lages into life imprisonment. This proposal caused lengthy debates in parliament and mass demonstrations in Amsterdam, attended by approximately 15.000 people. This manifestation of public anger stands in apparent contrast with the common assumption in Dutch historiography about ‘the silence of the 1950s’. This silence refers not only to talking about past war experiences, but also to (not) expressing related emotions (Withuis, 2002).  Decades later the issue proved to still have the potential to cause controversies. By 1972 only three foreign war criminals were still incarcerated in the Dutch city of Breda. Public unrest rose when the potential release of these last three German war criminals in Dutch custody was discussed in parliament. Newsreels show the manifestations of sadness and anger of the – again – more than 15.000 people that demonstrated in- and outside the parliamentary building in The Hague. This Breda Three controversy became one of the best-known emotional Second World War-related affairs in the Netherlands (Piersma, 2005). The Minister of Justice at the time, Dries van Agt, called this debate the most emotional episode of his entire political career (Bootsma and Griensven, 2003).   Questions This presumed high emotional charge of well-known controversies seems to be taken for granted. This leaves historians with not only substantive but also epistemological questions. To start with the latter: how can we observe, interpret, and compare emotionality in the context of historical research? Investigating emotionality is full of complexities. First, emotions are volatile and thus difficult to grasp – especially in retrospect. Next, it is to be expected that historians reading parliamentary proceedings, trying to identify elusive emotionality, are influenced by their own preconceptions. This is especially the case in ethically charged controversies in the relative recent past. By approaching a well-researched case historically with an innovative text mining approach, I hope to be able to also investigate these personal judgments of emotion in historiography. This study aims to answer the following substantive historical questions: how emotional were these debates? Which emotions were distinctive? How does the 1952 parliamentary discussion and the uproar it caused relate to the generally assumed ‘silence of the 1950s’? How do the different (war-related) discussions relate to each other? How exceptional are the war criminal debates when they are compared to other contemporary issues?    Methodology This investigation uses emotional word use to investigate emotions present in the discussions. I rely on generic emotion lexicons that are produced outside the field of history. They are derived from the fields of computational linguistics and psychology (Boot et al., 2017; Mohammad and Turney, 2013; Pennebaker, 2013; Tausczik and Pennebaker, 2010). The emotion lexicons used are generic in the sense that that their creation is not based on a single (type of) dataset. Using lexicons to identify, analyse, and evaluate manifestations of emotions, can be considered as a rather crude method. On the level of a single particular sentence, these lexicons may not always be reliable in determining whether a certain emotion is manifest. However, when these lexicons are applied to large corpora, they are reliable in determining whether a certain part of the corpus has more (or less) manifestations of a certain emotion when compared to others (Mohammad, 2012). Taking into account the ambition to address the influence of personal judgment in this investigation, I consider it an important advantage that the lexicons are from outside the specific domain of this study, and not influenced by personal judgments of the investigator(s) towards the historical sources or themes under scrutiny. The use of emotion mining as a method in historical research forces researchers to formalize what they want to know (what is emotional), and formalize how their analytical process leads to their answers and conclusions (Rockwell and Sinclair, 2016).  The historical parliamentary debates are analysed by using open-source text mining packages (e.g. quanteda and tm) within the R-programming environment (Benoit et al., 2018; Feinerer et al., 2008; Feinerer and Hornik, 2018). The digitised parliamentary debates are pre-processed: they are lemmatised and stopwords and interpunctions are removed (Marx et al., 2012). Next, the corpora are transformed into Document Term Matrices (DTM). The lexicons are used to score the parliamentary records on manifestation of words relating to different categories of emotion (e.g. anger, sadness, trust, fear). In this process, a Term Frequency – Inverse Document Frequency (TF-IDF) weighting is assigned to each unique word in the corpus. The DTMs are used to measure how distinctive words from an emotion category (from the lexicons) are for a specific document relative to all other documents. The output is then turned into boxplots and distribution graphs that are used for statistical evaluation and comparison of the results.  This output generates insights in which different emotional words were distinctive, and in which proportions they were manifest in the parliamentary discussions. Outputs are compared over time to come up with a long-term perspective on historical developments in discussing collaborators and war criminals in Dutch parliament. These quantitative results are also confronted with more traditional historical analysis of both primary and secondary sources.  ",
        "article_title": "Tantrums and Traitors: a Diachronic Analysis of Emotions in Parliamentary Debates on War Criminals and Collaborators",
        "authors": [
            {
                "given": "Milan Mikolaj",
                "family": "van Lange",
                "affiliation": [
                    {
                        "original_name": "NIOD Institute for War, Holocaust, and Genocide Studies, Netherlands, The",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "corpus and text analysis",
            "semantic analysis",
            "content analysis",
            "data mining / text mining",
            "English",
            "logic and epistemology",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper reflects on the problem of the ontological status of text in the digital environment. Referring to Briet (1951), Van Zundert and Andrews (2017) drew on Briet’s fluid concept of documentation and pointed to a similar fluidity in the concept of text. Here we draw a parallel to debates in the field of philosophy of consciousness to examine how this may help us to understand the specific textuality of digital text.  In his 2003 article, P.M.S. Hacker confronted Thomas Nagel’s reasoning in “What Is It Like to Be a Bat?” (Nagel, 1974). Nagel had argued that consciousness has a unique individual subjective quality that defies reduction to the materiality of body and mind. Hacker’s refutation of Nagel’s reasoning is almost exclusively based on the observation that Nagel’s argumentation in key places is syntactically faulty (Hacker, 2003:170). For Hacker—a Wittgensteinian philosopher of language—erroneous syntax implies faulty logic and thus incorrect argument. His counter-argument implicitly reduces consciousness to that which can be understood and explained linguistically. But this move ignores thereby the evidence and arguments put forward in fields beyond linguistics and philosophy that speak against the idea of consciousness as a solely linguistic construct: empirical evidence from the neurosciences (cf. Koch, Massimini, Boly, and Tononi, 2016), theories on consciousness that relate consciousness to mind and body such as came forward from integrated information theory (cf. Tononi, 2012), and ideas surrounding embodied cognition (cf. Shapiro, 2012). Sources from many disciplines thus seem to suggest we cannot understand consciousness solely as a linguistic construct, or one that must be explained exclusively by linguistic means. A concrete understanding of consciousness is likely to integrate types of knowledge and experiences that are distinctly non-linguistic and hard to properly linguistically express. Arguably then, mind and consciousness are better understood as emergent properties of a body orchestrating multiple types of sensations and information. Explaining consciousness through linguistic properties alone would be akin to trying to explain plastic arts by words solely. As early 20th century artist Vernon Blake (1926) put it in his phenomenological treatise on art: “By means of words one cannot hope to attain to precise transmission of ‘plastic’ thoughts. The plastic arts exist because they are the natural and only way to transmit that species of thought from one human being to another. They necessarily deal in thought factors which are inexpressible in words.”  We observe a parallel in digital textual scholarship, which is the field’s preoccupation with the digital mimetic reproduction of printed text. This is the prevalent paradigm (comprehensively described in e.g. Pierazzo, 2013), and most digital scholarly editions that have been produced follow it meticulously. This mimetic philosophy has historically been expounded through the socio-technical system known as TEI-XML and its supporting community.  The vocabulary and syntax of the TEI is geared exclusively towards the structural, documentary, and content description of non-digital texts. Its philosophy and technology confine text to an existence as a book or as a digital reproduction thereof; itsvocabulary and syntax treat textuality only by means of the non-digital idiom of print, manuscript, score, script et cetera. The application of this idiom consequently leads to the reification of text as that which was contained in a document. Just as it is an illusion to think that consciousness can be explained entirely by means of linguistics, so it is an illusion to think that text and a full understanding of it can be achieved by digital means that reduce the text to a print paradigm. Barring some eccentric exceptions there are, to our knowledge, no scholarly editors that hold that only what is on the page is the text, that the text exists solely as a semiotic representation. It is in reading that the text becomes to exist. Umberto Eco (1981) regarded a written text as a series of instructions for the reader to create meaning. The meaning of a text is a co-creation that emerges from the interaction between the text and the situated knowledge and embodied cognition of the reader. The uniqueness of this interaction—that stems from two uniquely situated sites of knowledge, namely the text and the reader, mingling—gives rise to the indeterminacy Jerome McGann (1991) calls the textual condition. Like consciousness cannot be reduced to linguistics, textuality cannot be reduced to documentary description. We may never be able to experience the uniqueness of another reader’s mind. Consequently we may never be able to fully explain what text is. But the salient point is that we can apply digital technology to understand text beyond its being as a sign on the page. We contend that digital technologies can (and should) be used to explore the properties of these different beings of text. Analogous to what Alan Kay (1997, 2007) said about the computer revolution, we may perhaps say that the digital text has not happened yet.  The medium which differentiates the digital from linear, non-interactive text is still in the process of being imagined. So we textual scholars must become more imaginative, and ask ourselves: is there anything it is like to be a text? For us this question serves to displace an understanding of digital text as an inanimate series of electronic signs with an understanding of text as a more dynamic and interactive agent. Being a text in a digital environment involves formal complexities and computational procedural elements that are alien to print or manuscript text. Furthermore, being digital allows a text to transgress the boundaries of the document and to connect to other texts and information in ways that reify connections usually only present in human cognition. Lastly there is an element of code to the existence of text in the digital environment. Software code is text’s animated computational twin. Text will often interact with code, to the point of symbiosis. Our use cases demonstrate these properties of digital text. Our first use case focuses on Codex (Neill and Kuczera, 2019), a system for combining text annotation with graph database networks. A standoff property text editor is a central tool allowing users to create multiple overlapping annotations (avoiding ‘OHCO impedance’). The graph network can be constructed out of entities derived from within texts as well as managed independently of the texts themselves. The aspiration of Codex is not so much the reproduction of a manuscript within a document (like a single cell), but the emergence of a multi-cellular network of texts linked by the connective tissue of the graph database. The goal of Codex is to enable scholars to create a kind of ‘meta-text’ formed from out of the relations existing within texts across the corpus. Our second use case focuses on the creation of a digital critical edition as a computational model of the text and its witnesses, where a documentary model of edition was considered insufficient to represent not only the individual texts, but the relationship between the text witnesses themselves as well as to the information the text carries. The process of transcription, transformation to valid TEI-XML, collation of the chosen layers of witness text, and analysis of the variation is done with available tools, such as tpen2tei (Andrews, Veigl, and Kaufmann, 2018) and CollateX (Dekker and Middell, 2011), that are as general-purpose as possible. Even so, the editors had to write custom code ‘plugins’ in order to facilitate this process, for example to specify a normalization string for collation, to handle the expansion of abbreviations, or to convert certain elements of TEI markup to HTML display. We consider that this custom software code, insofar as it carries an editor’s interpretation of the text and the significance of its parts, is itself part of the edition that is produced, and thus part of the edited text. Our third use case considers the object-oriented modeling of a text using a general-purpose computer language. The objective in this case is to demonstrate that adequate transcription as the scholarly community has performed it up to now does not necessarily encompass adequate re-mediation of that text. Computer languages facilitate the meticulous modeling not only of the linguistic layer of a text, but also of many dimensions of the text beyond that. We may actually program a digital scholarly edition so that, when the code is run, the result is the scholarly edition. But we may also model the objects, events, and relations that exist in the narrative world of the text, in which case running the code becomes a re-enactment of the text’s narrative. In this way digital technology calls into question the limits of transcription and how digital transcription may add to understanding in novel ways. These use cases show that to imagine what it is like to be a digital text invites us to reconceive of text in a specifically digital fashion that points to affordances we fail to see and use were we to regard digital text as a mere mimesis of print and manuscript texts. This, we contend, adds to our understanding of what being a text is, and ultimately to understanding what text is. ",
        "article_title": " Is There Anything It Is Like To Be a Text?  ",
        "authors": [
            {
                "given": "Joris J.",
                "family": "Van Zundert",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands - Royal Netherlands Academy of Arts and Sciences",
                        "normalized_name": "Royal Netherlands Academy of Arts and Sciences",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/043c0p156",
                            "GRID": "grid.418101.d"
                        }
                    }
                ]
            },
            {
                "given": "Iian D.",
                "family": "Neill",
                "affiliation": [
                    {
                        "original_name": "Digitale Akademie der Akademie der Wissenschaften und der Literatur Mainz",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tara L.",
                "family": "Andrews",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities, Institute for History, University of Vienna",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            },
            {
                "given": "Kuczera",
                "family": "Andreas",
                "affiliation": [
                    {
                        "original_name": "Digitale Akademie der Akademie der Wissenschaften und der Literatur Mainz",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "digital humanities (history",
            "digital textualities and hypertext",
            "linking and annotation",
            "scholarly editing",
            "theory and methodology)",
            "English",
            "text encoding and markup languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Abstract  Several centuries of manuscript music sit on the shelves of libraries, churches, and museums around the globe. On-line digitization programs are opening these collections to a global audience, but digital images are only the beginning of true accessibility since the musical content of these images cannot be searched by computers. In the SIMSSA (Single Interface for Music Score Searching and Analysis) project (Fujinaga, Hankinson, and Cumming, 2014) we aim at teaching computers to read music and assemble the data on a single website. However, the automatic retrieval and encoding of music from score images has many complexities. In this paper, we describe our current workflow to perform end-to-end optical music recognition (OMR) of early music sources.   1  The process of reading, extracting, and encoding the content from digitized images of music documents is called optical music recognition (OMR). Despite more than 50 years of research, OMR is still a complex problem. Some characteristics of music notation—such as the graphical properties of musical objects and the layout and superimposition of musical features—make OMR a remarkably difficult problem (Bainbridge and Bell, 2001). The slow development in OMR, particularly when dealing with early music, also lies in the variability of documents. Since most OMR systems have been developed using heuristic approaches, they usually do not generalize well to documents of a different type.   2 End-to-end OMR workflow for medieval and renaissance music  For high scalability, we are taking a machine learning-based approach to OMR. The computer is trained with a large number of examples for each category of musical element to be classified and creates a model. Once a model is created, it is used to classify new examples that the computer has not yet seen. We have implemented this approach in a workflow that performs OMR in medieval and renaissance music scores images. The workflow is divided into four stages: document analysis, symbol classification, music reconstruction and encoding, and symbolic score generation and correction. The entire workflow is depicted in Figure  1.       Figure 1. End-to-end optical music recognition workflow for early music. Boxes indicate the software applications on each step. Human symbols indicate interactive, adaptive stages.    2.1 Document analysis  Digitized music scores are the input to the system and document analysis is applied to segment the music document into layers. We developed Pixel.js (Saleh et al., 2017), an open source, web-based, pixel-level classification application to label pixels into their corresponding category or to correct the output of other image segmentation processes. We use this tool interactively with a convolutional neural network (Calvo-Zaragoza et al., 2018) to segment the document into a number of user-defined layers. After a few iterations of training and classification for optimizing the classifier, we obtain a number of image files corresponding to the segmented layers of the original score. For example, these layers may contain notes, staff lines, lyrics, annotations, or ornamental letters. The recognition of the music symbols and the analysis of their relationship is achieved once the symbols are isolated and classified in the found layers.  2.2 Symbol classification  The application we developed for the symbol classification stage is called Interactive Classifier (IC). IC is a web-based version of the Gamera classifier (Droettboom, MacMillan, and Fujinaga, 2003). We use it to automatically group the connected components of a specific layer into glyphs. Then, we manually label a series of these musical glyphs into classes. For neume music notation we implement neume-based and neume component-based classification. In either case, IC will extract a set of features for describing each of the neume or neume component classes and will model a classifier. With this model, new glyphs will be classified based on k-nearest neighbours. Once the symbols of the score are classified, we proceed to add their musical context and encode them into a symbolic music format.   2.3 Music reconstruction and encoding  We obtain the pitches of neumes or neume components by finding their absolute position in the corresponding staves and use the recognized clef of each system to assign a relative pitch (Vigliensoni et al., 2011). The output of IC conveys the position and size of each musical element in the original image, and so we add this information to the estimated pitch as well as the staff number to which each neume belongs. Finally, this musical information is encoded into the Music Encoding Initiative (MEI) machine-readable symbolic music format (Roland, 2002).   2.4 Symbolic score generation and correction  The last two stages of our OMR workflow, music reconstruction and encoding and symbolic score generation have a common interactive breakpoint for visualizing and correcting the output of the automatized OMR process. This human-driven checkpoint is embedded as a web-based interface called Neon (Neume Editor Online) (Burlet et al., 2012). Neon overlays the original music score image and the rendered version of the output of the OMR process. By visual inspection, the user can assess the differences between the layers, and manually add, edit, or delete music symbols in the browser. So far, however, corrections entered by the user are not fed back into the learning system, but they change the encoded music file output.   2.5 Workflow management system  All the constituent parts of our OMR workflow are handled by Rodan (Hankinson, 2015), a distributed, collaborative, and networked adaptive workflow management system that allows specifying interactive and non-interactive tasks.   3 Future work   In future iterations of the project we will focus on: (i) implementing a non-heuristic, machine learning-based approach for pitch finding (similar to the approach proposed by Pacha and Calvo-Zaragoza (2018)); (ii) appending neumes to syllables (since most neume notation is used to set music to an existing text); and (iii) devising a way of feeding back into the workflow the corrected output in Neon. We hope that this infrastructure, in combination with the proper teaching strategies and tactics developed by human teachers in the interfaces for training OMR systems (Vigliensoni, Calvo-Zaragoza, and Fujinaga, 2018), will enable the end-to-end recognition and encoding of music from music score images. ",
        "article_title": "Overcoming the Challenges of Optical Music Recognition of Early Music with Machine Learning",
        "authors": [
            {
                "given": "Gabriel",
                "family": "Vigliensoni",
                "affiliation": []
            },
            {
                "given": "Alex",
                "family": "Daigle",
                "affiliation": []
            },
            {
                "given": "Eric",
                "family": "Liu",
                "affiliation": []
            },
            {
                "given": "Jorge",
                "family": "Calvo-Zaragoza",
                "affiliation": []
            },
            {
                "given": "Juliette",
                "family": "Regimbal",
                "affiliation": []
            },
            {
                "given": "Minh Anh",
                "family": "Nguyen",
                "affiliation": []
            },
            {
                "given": "Noah",
                "family": "Baxter",
                "affiliation": []
            },
            {
                "given": "Zoe",
                "family": "McLennan",
                "affiliation": []
            },
            {
                "given": "Ichiro",
                "family": "Fujinaga",
                "affiliation": []
            }
        ],
        "publisher": null,
        "date": "2019-04-09",
        "keywords": [
            "digital humanities (history",
            "software design and development",
            "artificial intelligence and machine learning",
            "theory and methodology)",
            "English",
            "OCR and hand-written recognition",
            "computer science and informatics",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper is the first official report on a Marie Curie project entitled  Last Letters from the World Wars: Forming Italian Language, Identity and Memory in Texts of Conflict, which started in September 2018. This project deals with a linguistic and thematic analysis of the last letters of people sentenced to death during the First and the Second World Wars, conducted with digital humanities tools. In this very first part of the project, I am preparing the lexicon analysis that will be the focus of my methodology. I am also creating a geographical representation of the corpus because this project is intrinsically geographical in its approach. Indeed, mapping appears necessary considering the linguistic particularities of the Italian territory, which is characterised by a linguistic diversity on the local level.   In this part of my research activities, I am developing some tools that are specific to the analysis of the Second World War. Notably, I have already collected and georeferenced four datasets (heretofore named as DS) regarding the Italian Resistance against fascism and the Nazi occupation. I have reworked or created data on this topic thanks to my collaboration with the staff of several Italian institutions and archives:  DS_1 - The network of all the massacres committed by Nazis and Fascists since the beginning of the Italian dictatorship. This data was originally collected in the framework of a previous project,  The Atlas of the massacres of Nazis and Fascists (http://www.straginazifasciste.it/), by the Central Institute of the Resistance in Milan. The original project led to the creation of a basic database, managed by a SQL system that communicates with Google's API. I added dates and a new style to this project.   DS_2 - The political and racial arrests and deportations committed by Nazis and Fascists against civilians and the partisan army. This data is contained in the database of the Italian Documentation Centre of Jewish Culture, with which I have started a collaboration in order to create a mapping of their very rich material.   DS_3 - The entire corpus of letters written by partisans, generally Italians, before their execution. This data is directly connected with the project Last Letters. The letters are georeferenced and analysed so that an historical and linguistic analysis could be applied.   DS_4 - The movements of the Italian fighters who participated in the Spanish Civil War beside the Spanish Republic against Franco. This data is collected by the Ferruccio Parri Institute, which asked me to exploit it digitally.  These four datasets present a huge amount of data (approximately between 20.000 and 30.000 elements already categorised and georeferenced) that perfectly display the repression acted by Nazis and Fascists on members of the Resistance and populations, but also the activities of Italian partisans in reaction to this oppression. Indeed, the letters were usually written in the areas where Fascists had crushed their opponents through military attacks.  This research has already appeared that a digital mapping approach is essential in dealing with this historical data. This method has never been applied before, and can highlight new connections and differences between Nazi and Fascist abuses. This map, which I have already built,  https://goo.gl/BZSo3L, shows how a digital visualisation of the massacres can help to understand in what parts of the country repression was led by Germans or Italians. These peculiarities highlight the dualism of the Italian Resistance, which can be considered both as a phenomenon of resistance and a civil war. This is a very debated problem in historiography, which is linked with ideological positions. Indeed, admitting that Italy was involved in a civil war means that partisans were soldiers of a regular army, as opposed to a group of patriots who fought against foreign invasion. If partisans were construed as a regular army, their war against the Italian Social Republic could be considered a war against other Italians and not against Fascism, which was defeated on 8 September 1943 with the armistice signed between the Italian army and the Allies.  The data I have collected so far will be presented using a combination of digital tools such as Gephi, Carto and Google Earth. I am particularly interested in the construction of networks in order to understand what categories of people were mainly repressed. I have georeferenced the data with R and Openrefine and, after correcting coordinates by checking on the maps the potential errors every ten records, I have built datasets divided into several categories regarding politics, social background and gender. Through the use of network analysis combined with spatial analysis, I would like to shed light on the differences between Nazi and Fascist repression, insofar as different categories of people were targeted by one or the other.  The idea is to compose the very first atlas of Fascist and Nazi repression in Italy using only open source tools. The mapping is thought and coded mainly with the use of R, without the use of Shiny, but also in JavaScript for some special layers, especially the chronological ones.  The meaning of this work is to shed a new light on this period considering that, even in the archives, the actions of Nazis and Fascists were often covered for political reasons after the war. For instance, at the end of the Second World War, politicians probably understood that it was impossible to punish everyone for their crimes and therefore, they often had files disappear from the dossiers. The case of the “cupboard of shame” is a very famous example of these tendencies: in 1994, a wooden cabinet was discovered inside a large storage room in the Palazzo Cesi-Gaddi, in Rome which, at the time, housed the chancellery of the military attorney's office. The cabinet contained an archive of 695 files documenting war crimes perpetrated on Italian soil under Fascist rule and during Nazi occupation after the armistice on September 8, 1943, between Italy and the Allied armed forces. The cupboard’s doors were locked and facing the wall when they were discovered. As this case shows, Italy still has trouble dealing with its past. The Fascist regime represents an uneasy heritage for today’s political parties and citizens. Mapping the Fascist repression of that time is a way to display the historical responsibilities of Fascism through the violence expressed against racial and political categories, and the use of a digital map enables us to visualize the phenomenon under a new light. Connecting those historical events with the geographical places where they happened facilitates the emergence of patterns in the repression of populations in the course of the most tragic event of twentieth-century Italian history. ",
        "article_title": "Mapping Fascist Repression, Following The Italian Resistance",
        "authors": [
            {
                "given": "Giovanni Pietro",
                "family": "Vitali",
                "affiliation": [
                    {
                        "original_name": "University College Cork, Italy",
                        "normalized_name": "University College Cork",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/03265fv13",
                            "GRID": "grid.7872.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "English",
            "network analysis and graphs theory",
            "cultural studies",
            "italian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The notion of “editorialization” has a fundamental place within the francophone scientific community as a key-concept for understanding and interpreting the digital culture. The concept has been at the center of theoretical works of the academic community for the last ten years (in April 2015, Jérôme Valluy completed an almost exhaustive bibliographical work on the term, finding more than seventy academic papers that had used it, Valluy 2015). It has been recently the subject of a book in English (Vitali-Rosati 2018). In my communication I will present the result of ten years of work on this concept and explain how the theory of editorialization can help DH scholars to think about the architectural space of the digital environment, and how it impacts knowledge production, circulation and legitimation. More specifically, I will argue that this theory can be a very powerful theoretical framework to take into account the political implications of our practices as dh scholars.   What is Editorialization?   The word ‘editorialization’, in the sense that it is used here, is a neologism in English. It comes from the French  éditorialisation. If, in English the word is a derivative of editorialize, which means – according to most dictionaries – ‘to express an opinion in the form of an editorial’ or ‘to introduce opinion into the reporting of facts’, in French it has acquired a broader meaning and is related in particular to digital culture and to digital forms of producing knowledge (Guyot 2004; Bachimont 2007; Merzeau 2014). This shift in meaning, from an idea that denotes the expression of opinion to one that suggests the production of knowledge in the digital age, is actually quite useful; as we will see, the French version of the term retains its association with the notion of opinion in that it refers to the production of content that expresses a kind of opinion or that offers a better way to see or interpret the world.  In 2008 Gérard Wormser (see Wormser 2010) used the term editorialization in a broad sense, to describe any digital editorial activity and to signify how knowledge is produced in the digital age in general. If ‘the digital’ is not only about tools but in fact refers to a whole cultural environment (see the notion of “digital culture” in Doueihi 2011), then editorialization – the way of producing contents in digital environments – must have a cultural dimension as well. In other words, the difference between publishing and editorialization is not only a difference of tools, but rather signifies a broader cultural difference: editorialization is not the way we produce knowledge using digital tools; it is the way we produce knowledge in the age of the digital, or, better, in digital society. In this sense, the term editorialization expresses an idea quite close to notions like ‘knowledge design’ (Schnapp 2011) or ‘information architecture’ (Broudoux, Chartron, and Chaudiron 2013). According to a restrictive definition, editorialization is a set of technical devices (networks, servers, platforms, CMS, search engine algorithms), structures (hypertext, multimedia, metadata), and practices (annotation, comments, recommendations via social networks) that produces, organizes, and enables the circulation of content on the web (Gac 2016; Vitali-Rosati 2014). In other words, editorialization is the process of producing and diffusing content in a digital environment. We could say that, in this sense, editorialization is what publishing becomes under the influence of digital technologies. Clearly, this has an impact on the content itself: the concept of editorialization tries to stress how technology shapes content. The obvious limitation of this first definition is that it considers the digital environment as a discrete, separated space. In this sense it is a web-centered definition that does not take into consideration the fluidity that exists between digital and pre-digital space. Further analysis shows that we can take all the acts of structuring content online – on the web or on other forms of the connected environment, like mobile apps – and consider these acts in their function of shaping our whole reality. In this sense, we can define editorialization as a set of individual and collective actions that take place in a digital online environment and that aim to structure the way we understand, organize, and judge the world. These actions are shaped by the digital environment in which they take place, and so editorialization, just as the first definition makes clear, is not only about what people do but also how their actions are shaped and oriented by a particular environment. But the emphasis needs to be put not only on how we produce content, but also on the fact that these contents are actually the world in which we live. It is important to stress that, if we consider the word digital in a cultural sense, digital space is our primary space, the space in which we live. With this in mind, we can make a distinction between various digital environments – for instance, the web and other forms of connected environment – and digital space as a hybridization of these environments with the totality of our world. These considerations allow us to arrive at a final definition: Editorialization is the set of dynamics that produce and structure digital space. These dynamics can be understood as the interactions of individual and collective actions within a particular digital environment. The object of editorialization is not content, but the world itself: we editorialize things, or, better, we editorialize the space in which we live. We could say that the encyclopedic project as it was conceived during the 18th century by Diderot and D’Alambert was realized with the world wide web: the totality of our knowledge has been organized and linked in a unique and huge architectural framework (Melançon 2004). But it goes further: with editorialization we are not only structuring the knowledge, we are structuring the world itself. We can say that editorialization is not only an architecture of knowledge (or, to use Schnapp’s concept, a ‘knowledge design’) but more precisely an architecture of being. This is why the concept of space becomes so crucial: editorialization is a way of organizing space not in a metaphorical sense (as the space of knowledge or the space of information): editorialization is an actual architectural action, it organizes our actual space.    DH scholars as architects   Now, the fact that editorialization is a way of producing the space we inhabit raises an important problem: who are the actors of editorialization? Who is producing this hybrid space? This problem is the main political issue when we analyse our digital culture; addressing it is the first step towards developing what Geert Lovink calls ‘Net criticism’ (Lovink 2002). As Morozof points out (Morozov 2012, 2013), digital space seems to be completely driven by a small number of private companies. Morozov’s analysis of the role of Silicon Valley corporations starts from the idea that what should be public on the web is actually private. The domains of public interest are owned by big companies, and the services that should be granted by public authorities are instead organized and provided by private corporations. The problem is thus twofold: private corporations own most of digital space and they impose their ideologies on the whole social space, leaving no room for a public sphere. Private corporations decide what the structure of digital space is and what values it holds. But according to the theory of editorialization, what we do as digital humanists is also a way of producing and organizing the space. In this sense, as digital humanists, we play the role of architects of our actual world. Our ways of organizing and structuring contents and information should be interpreted as an architectural gesture. Editorialization is the action of organizing the world, and this is why our academic practices have a huge political implication. Producing tools, platforms, data visualizations, and proposing different epistemological models to produce, organize and disseminate knowledge means to allow the existence of a plurality of spaces and to avoid the risk of a monolithic space produced and managed by a small number of companies who share the same values and the same visions of the world. As DH scholars we should be the producers of a public sphere - heterogeneous and plural (on this subject see McPherson 2018) - a space where commons (for a definition of commons see Ostrom 2015) are possible and where democracy can take place. The theory of editorialization can be a useful theoretical framework to drive our DH practices and projects, because it allows us to understand that dealing with digital objects (texts, data, information of any kind) means also being an architect who organizes and structure a living space. My presentation will be structured as follows:  A history of the term editorialization. Different definitions of the concept. Editorialization as a way of producing space. DH scholars as architects.  Some examples of DH projects as forms of editorialization     ",
        "article_title": " Toward a Theory of Editorialization  ",
        "authors": [
            {
                "given": "Marcello",
                "family": "Vitali-Rosati",
                "affiliation": [
                    {
                        "original_name": "Université de Montréal, Canada",
                        "normalized_name": "University of Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0161xgx34",
                            "GRID": "grid.14848.31"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-01",
        "keywords": [
            "digital humanities (history",
            "digital textualities and hypertext",
            "open content and open science",
            "scholarly editing",
            "theory and methodology)",
            "English",
            "philosophy",
            "scholarly publishing",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The “assertive edition” Historians have since long considered historical documents as an information carrier. Editing documents for historical research thus often meant to create modernized or abridged texts and offered a wide range of tools to access the content of the texts (abstracts, rich indices or commentaries). In the digital world this approach has ended in a type of digital scholarly edition which is still lacking an accepted term: drawing from the Semantic Web it could be named “semantic edition”, in the context of “fact-checking” it might be termed “factual edition”, or putting it into logical reasing the term could be “assertive edition” (Vogeler, 2018). Scholarly editions in this field range from early adopters of OWL like the Henry III Fine Rolls (Ciula et al., 2008; Ciula, Veira, 2007), extensive RDF representations of indices like in sandrart.net (Schreurs et al., 2007-2012) or burkhard-source   http://burckhardtsource.org/  , or infrastructures based on the integration of text and database as in Symogih (Beretta, Vernus, 2012; Beretta, 2015).   http://symogih.org/   Projects like Old-Bailey-Online (Howard, 2015)   https://www.oldbaileyonline.org/   conceptualised the information part as relational databases, recent activities in the context of the Historical Atlas of Poland project created a resource labelled “edition” as database of annotations to images of the original documents (Borek et al., 2018, Słoń, Zachara-Związek, 2018)   https://atlas.ihpan.edu.pl/gis/agad_wschowa_i2_pub/index.php  .  The main change in the recent years in these digital editions is the extension of the concept of database representation from annotations similar to printed indices (Poupeau 2006) to representations of full information conveyed. In particular, the simple representation of information in RDF statements is attractive and supports this effort. RDF additionally has the advantage to be an accepted data exchange standard, which is flexible and expressive enough to cover a wide range of projects. On the other hand, the TEI has been extended to include formal descriptions of persons, places, names and basic knowledge information organisation. Classic relational database systems are powerful tools to create datasets representing complex knowledge, and the recent discussion on graph technologies is moving the discussion about the best suited technical solutions forward. The paper will give an insight in a group of projects trying to integrate the wide range of technologies around. The DH repository and publication platform GAMS (Stigler / Steiner, 2018) in Graz holds several projects which can be considered “semantic”/”factual”/”assertive” editions: the scholarly editions of the annual accounts of the City of Basel (Burghartz, 2015), the Urfehdebuch Basel (Burghartz et al., 2017; Pollin, Vogeler, 2017), or the Cantus-Network (Klugseder, Praßl, 2018). Others are in preparation (Klug: Tegernseer Wirtschaftsbuch; Klug et al.: Cooking Receipts of the Middle Ages; Wareham: British Hearth Tax Online; Haug-Moritz: Imperial Diet 1576   https://reichstagsakten-1576.uni-graz.at/  ). The tasks to be solved with these resources are multiple: Which assertion made by the texts should be represented? How good the XML structure helps to extract assertions? How to link between data representation, text, and image? How to organise an efficient workflow?    Which content should be represented? All projects come from specific domains: accounting, criminal history, history of liturgy, taxation, political institutions, and dietary history. This leads to the development of individual ontologies. In the case of accounts the projects could benefit from an international activity in the development of an ontology for historical accounting (MEDEA   http://medea.hypotheses.org/  , DEPCHA   http://gams.uni-graz.at/context:depcha  ). The criminal records used a “local” ontology which should be harmonised in the wider range of criminal history research. The Cantus Network reused identifiers from liturgical study (cantus database)   http://cantus.uwaterloo.ca/   and started to work on an ontology for liturgical feasts based on the reference work by Hermann Grotefend (1898). Cooking receipts can reuse food ontologies for modern purposes or generic data from wikidata. For the imperial diet the historians in the project suggested to work on an ontology representing forms of communication as they are considered core in the interpretation of the political event. The experiences with these ontologies demonstrate that a core functionality in the implementation of assertive editions it he possibility to model information which goes beyond the flat text, i.e. information that can hardly be extracted with standard information extraction methods.    How to link between data representation, text, and image? The GAMS system integrates TEI transcriptions with a IIIF image server. This makes it easy to link between text and images. The relationship between assertions and text is handled by a local ontology considering the data entries as a kind of factoid (Bradley, Short, 2005), which carries a link to the source. In practice the textual fragments supporting the formal assertions carry identifiers by which they can be stored together with the data with a local <g2o:textualContent> property.   http://gams.uni-graz.at/o:gams-ontology/#textualContent   This is an approach similar to Web Annotation so it seems suitable to reuse the W3C Web Annotation Vocabulary.   https://www.w3.org/TR/annotation-model/   This would even allow the full range of relationships between the main representations of the text (digital image, transcription, database).    How to organise an efficient workflow? Centring the technical solution on well-established standards helps to create a framework of different tools. While Oxygen-XML is a main tool for XML encoding and plug-ins like Ediarum   www.bbaw.de/en/telota/software/ediarum   help in transcription and basic markup, tools like Transkribus   https://transkribus.eu/   are used in the Cooking Receipt project to create topographically precise transcriptions. In the Cantus project a workflow based on the conversion of word files to TEI was chosen. The TEI offers with the @ana attribute a powerful method to extract assertions with XSLT. For the criminal records, the accounts and the hearth tax documents, customized TEI are used, which combine the text structure (e.g. in simple <p> tags) with the interpretative level (with @ana as a URI references). XSLT-Transformations during ingest in the GAMS convert extracts the RDF representation of the text. The DEPCHA project demonstrates that this workflow can be used as a hub between various input forms.    Conclusion The experiences in the projects show that DH provides methods for historians to create rich editions including formal representations of the information conveyed. The GAMS infrastructure does this by combining a set of tools around standards like IIIF, TEI and RDF. Still, the standards for data exchange in many humanities domains do not exist. Future work will have to focus on creating reused domain ontologies, which the data-for-history initiative is promoting.   http://dataforhistory.org/   It has to be evaluated, if W3C Web annotation can become the major link between the various representations of texts in digital scholarly editions. All of this can form the assertive edition many historians dream of.   ",
        "article_title": "Implementing the Assertive Edition for Historians – Some samples",
        "authors": [
            {
                "given": "Georg",
                "family": "Vogeler",
                "affiliation": [
                    {
                        "original_name": "Universität Graz, Austria",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-02",
        "keywords": [
            "scholarly editing",
            "standards and interoperability",
            "English",
            "text encoding and markup languages",
            "semantic web and linked data",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In recent years cross-disciplinary, cross-organisational and cross-sectoral research has been widely practised, linking across digital tools, research fields and actor groups. As our world is becoming increasingly more complex, this phenomenon is also particularly visible in academic fields, where complexities have arisen at several levels. In the field of Digital Humanities (DH) the benefit of bringing together different disciplines and research areas is the opportunity to work with complex data structures, thus providing a springboard for further scientific work. In this context, digital humanists not only have on an important role in the Humanities themselves, but also in relation to society. The processing of complex data provides increased opportunities to access Humanities knowledge across actor groups, and also enables more interested citizens to engage with it. As a result, DH have the opportunity to prove their importance to society, but at the same time participation of further actors groups yet again increases the complexity regarding the interaction with the society.  In this paper, we provide insights into the Digital Humanities project  exploreAT! - exploring austria’s culture through the language glass (Wandl-Vogt et al., 2015), against the complexities it has addressed and for which we introduce our solutions (cultural, semantic, visual, societal) by drawing on cross-disciplinary efforts in an international framework.   The project has addressed complexities on several levels and from several perspectives. To exemplify our approach, we here report on three complex aspects that were tackled: the focal topic food, the interaction with society within a open innovation framework and the use of existing and new technologies. These three aspects are met within a cultural analysis framework, in line with one of the definitions of culture in the literature (cf. Longhurst et al., 2008). As a general background, the project has been based on a digitised German non-standard language collection from the time of the former Austro-Hungarian monarchy (Database of Bavarian Dialects in Austria, DBÖ) (Wandl-Vogt, 2008), counting around 3.5 million digitised entries. The project has aimed to rethink dialect lexicography, by connecting a language resource to real-world objects for cultural exploitation, bring it into European networks and infrastructures and open it for sustainable use and re-use for various actor groups such as scholars and citizens (cf. Dorn et al., 2018). Through the setup of complex data models (cf. Abgaz et al., 2018a; Abgaz et al., 2018b), the linking to and enrichment with other datasets and frameworks (LOD) was enabled and visualisation of this multi-modal content and underlying complex system has been achieved by different prototypes (cf. Benito et al., 2016; Benito et al., 2018). Through participatory methods, e.g. design thinking and agile research, community groups have been connected to these scientific efforts by looking into links to cultural and societal challenges. In addition, it has also given rise to exploration space , an experimental Open Innovation Research Infrastructure (OI-RI) for the Humanities, which has been listed as a best practice example by the Austrian Federal Ministry of Science, Research and Economy, and is also as an example in the citizen science policy report.  Citizen science has provided the project the possibility to engage non-experts in the scientific process, to raise e.g. their understanding of scientific work, but also to raise the understanding of social needs of the involved scientists (Silvertown 2009). With the widespread availability and use of the internet and especially smartphones, the past decade has seen a remarkable increase in successful web-based citizen science projects (e.g. FoldIt, GalaxyZoo, PatientsLikeMe) and showed the potential of these approaches. While these have highlighted the advantages of citizen involvement in science, they have also triggered debate (Riesch and Potter 2014). Legal and ethical questions have been raised regarding data security (Kaye et al., 2012), the risk of harming participating citizens (Vayena and Tasioulas 2013) or using them only as a free labour for scientific research projects. Our project aimed to tackle these issues by implementing citizen science within an Open Innovation framework, giving participating citizens the possibility to engage with the project from the beginning and to deliver their inputs and ideas with low barriers. Learning to understand each other is a complex undertaking that requires time and, above all, physical space. For this reason, the exploration space was established, which in addition to the digital aspects of the project also provided physical space for interaction with citizens. In this room, among other things, workshops were held in which where a crucial part to interact with interested citizens. This physical space offered the opportunity to interact directly, to jointly answer sensitive and difficult questions and to develop concrete ideas for solutions.  In this context, citizens and researchers engaged through participatory formats such as co-creation and co-design workshops in different scenarios around the cultural topic of food, which provides a wide field with numerous perspectives of engagement across actor groups and also with different community groups (cf. Palfinger et al., 2018). With food being treated as either a service, a product or an emotion, the complexity of the subject offered also several possibilities for new cultural research questions with insights for lexicographic analysis. By applying different technological tools and methods, combining state-of-the art (i.e. thesauri) and novel technologies (Artificial Intelligence), the processing of these complex data was enabled to access, structure and analyse the derived cultural content.  Finally, we aim to share our learnings from this project with communities and future projects with similar interests and challenges. On the one hand, we share our processes and learnings in face-to-face workshops (brainfood lectures, etc.), but also the applied methodologies are made available online and can thus be readily applied to datasets or projects who wish to follow a similar approach. In our endeavours we follow the FAIR data principles and aim to make our data and insights better findable, useable and reusable for the benefit of scholars and actor groups beyond. ",
        "article_title": "\"A Project Review Under The Focus Of Complexities On The Example Of ExploreAT!\"",
        "authors": [
            {
                "given": "Amelie",
                "family": "Dorn",
                "affiliation": [
                    {
                        "original_name": "Austrian Academy of Sciences, Austrian Centre for Digital Humanities-ÖAW, Austria",
                        "normalized_name": "Austrian Academy of Sciences",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03anc3s24",
                            "GRID": "grid.4299.6"
                        }
                    }
                ]
            },
            {
                "given": "Eveline",
                "family": "Wandl-Vogt",
                "affiliation": [
                    {
                        "original_name": "Austrian Academy of Sciences, Austrian Centre for Digital Humanities-ÖAW, Austria",
                        "normalized_name": "Austrian Academy of Sciences",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03anc3s24",
                            "GRID": "grid.4299.6"
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": "Palfinger",
                "affiliation": [
                    {
                        "original_name": "Austrian Academy of Sciences, Austrian Centre for Digital Humanities-ÖAW, Austria",
                        "normalized_name": "Austrian Academy of Sciences",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03anc3s24",
                            "GRID": "grid.4299.6"
                        }
                    }
                ]
            },
            {
                "given": "Roberto",
                "family": "Theron",
                "affiliation": [
                    {
                        "original_name": "University of Salamanca, Visual, Spain",
                        "normalized_name": "University of Salamanca",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02f40zc51",
                            "GRID": "grid.11762.33"
                        }
                    }
                ]
            },
            {
                "given": "Andy",
                "family": "Way",
                "affiliation": [
                    {
                        "original_name": "Dublin City University, Adapt Centre, Ireland",
                        "normalized_name": "Dublin City University",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/04a1a1e81",
                            "GRID": "grid.15596.3e"
                        }
                    }
                ]
            },
            {
                "given": "Yalemisew",
                "family": "Abgaz",
                "affiliation": [
                    {
                        "original_name": "Dublin City University, Adapt Centre, Ireland",
                        "normalized_name": "Dublin City University",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/04a1a1e81",
                            "GRID": "grid.15596.3e"
                        }
                    }
                ]
            },
            {
                "given": "Alejandro",
                "family": "Benito",
                "affiliation": [
                    {
                        "original_name": "University of Salamanca, Visual, Spain",
                        "normalized_name": "University of Salamanca",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02f40zc51",
                            "GRID": "grid.11762.33"
                        }
                    }
                ]
            },
            {
                "given": "Antonio",
                "family": "Losada",
                "affiliation": [
                    {
                        "original_name": "University of Salamanca, Visual, Spain",
                        "normalized_name": "University of Salamanca",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02f40zc51",
                            "GRID": "grid.11762.33"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-24",
        "keywords": [
            "german studies",
            "lexicography",
            "interdisciplinary & community collaboration",
            "etnography and folklore",
            "English",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital humanities (DH) as a field has been grappling with the significant issue of interoperability. In response, many have proposed that DH needs basic infrastructures behind research projects to ensure its long-term success. In Europe, for instance,  CLARIN and  DARIAH are two such large-scale research infrastructures for humanities. While they have done a tremendous job in centralizing available digital resources, much of their infrastructures remain at the administrative level, and their generic coverage across the entire humanities meant that their utility for specific (and smaller) disciplines and non-European languages is limited. Furthermore, while their focus on open-access resources should be lauded, many textual resources—especially in the Asian context—remain licensed and protected. How can we, as scholars in DH and Asian studies, design a digital research infrastructure fit for our specific needs, taking past experiences with these large-scale infrastructural projects into consideration?  In this paper, we present our technical answers to this question.  RISE stands for  Research Infrastructure for the Study of Eurasia. Formerly known as  Asia Network, it is a pioneering approach for resource dissemination and emerging data analytics (such as text mining and other fair-use but consumptive research techniques) in the humanities, developed by the Max Planck Institute for the History of Science (MPIWG).  We developed  RISE and SHINE to facilitate the secure linkage between third-party research tools to various third-party textual collections (both licensed and open-access ones). Our goal is to revolutionize how scholars can work with textual sources via existing research tools that have already emerged within DH. Although many research tools for analyzing and visualizing texts are already available, it is usually difficult for humanities scholars to download texts from various databases and to prepare them in formats that individual research tools require before analyses. By designing a set of standardized APIs to link texts to digital research tools, we allow scholars to apply digital research tools on texts, regardless of their locations or formats.  When it comes to licensed texts, a common situation for resources in many Asian languages, it is impossible for scholars to use digital research tools to analyze them without illegally downloading or scraping the full texts. By securely linking these licensed texts to digital research tools, we allow scholars to work in a legal manner and ensuring commercial publishers the safety of their collections under a secured virtual research environment. Such flexible, networked approach to e-infrastructure development avoids re-creating silos of resources in the digital realm and allow scholars to fully leverage the potential of material digitization and digital research tools. To do so, we at MPIWG developed a range of tools:  SHINE is a set of standardized APIs for exchanging textual resources, both open-access ones and protected (or licensed) ones that require authentication and authorization. Inspired by the success of  IIIF, we aim to develop a similar standard for a wide range of communities working with textual resources. Resource providers and research tool developers will find SHINE useful because it supports interoperability among resource repositories and research tools in a decentralized manner. SHINE adopts a generic data model based on metadata from resource provider’s lists of collections and texts. Given a resource ID, SHINE supports API calls to obtain metadata and full-texts in a hierarchical organization. Researchers will find SHINE useful because they will gain unprecedented access to textual resources in a machine-readable format, so that textual resources can be analyzed in research tools in a seamless and legal research workflow.  Here you can find a full list of API endpoints currently implemented by SHINE.   RISE is a middleware that protects resource exchanges via SHINE. It authenticates and authorizes these exchanges, especially for protected (or licensed) resources. It connects with authentication servers from research organizations to authenticate their users and to authorize the access right for users to access certain resources based on their organizational affiliation. It also includes a browser-based administrative interface for resource providers, research tool developers, and research organizations to regulate protocols for specific resource exchanges. It is worth noting that SHINE, as an exchange format, could be adopted independently to facilitate interoperability without RISE.    RISE's architecture overview  To demonstrate the benefits of adopting RISE and/or SHINE and to encourage third-party development of SHINE-compatible technical solutions, a suite of open-access software modules linked to RISE have already been developed and will be made available for others to freely adopt and adapt for their own purposes. They are available on our  GitHub.    RISE-RP is a reference implementation for resource providers to publish their resources via SHINE. By installing RISE-RP, a resource provider (which can be a library, a research institute, a research project, or an individual scholar who manages a text collection) can easily publish its texts via the SHINE API protocol for other scholars to use via SHINE-compatible research tools.   RISE-Catalog is a browser-based user interface that research organizations could customize and implement to facilitate resource discovery and resource-tool linkage for their internal researchers. For example, a research library can customize RISE-Catalog so that it only shows the texts collections that the library has access to. A library user could then browse it and open the selected texts in specific research tools.   RISE-JS-Library is a JavaScript module for research tool developers to enable their research tools’ resource consumption via SHINE. After adopting RISE-JS-Library, a research tool can immediately provide to its users browse-and-search function for all resources in RISE-Catalog within its own user interface.   At the moment, RISE-Catalog has used SHINE to link to several collections of varying licenses, with resources in Chinese, Arabic, Latin, Greek, and English. Open-access collections include Kanseki Repository, Corpus DB, Oxford Text Archive, Perseus Digital Library, and other miscellaneous repositories. Licensed collections include the Chinese Buddhist Electronic Texts, Chinese Text Project, and the Taiwan History Digital Library. It also provides the option for users to directly open selected texts in  MARKUS, a popular research tool for semi-automatic tagging historical Chinese texts. A user in MARKUS could also browse-and-search the RISE-Catalog directly, as MARKUS had implemented RISE-JS-Library in a  beta version (see Figure 2). We are eager for connect to more resources and research tools in order to make RISE and SHINE more useful.    Implementation of RISE-JS-Library within digital research tools  Despite its current name, this suite of products can handle resources in all languages and can be customized to fit existing IT and content management systems. Many projects and infrastructures have proposed similar ideas, creating complex new initiatives that largely centralize resources in digital silos. Yet centralization does not necessarily address interoperability or the challenges with licensed texts ( HathiTrust being a notable exception in this regard). Ours, by comparison, is a modular solution that works, allowing for interoperability between collections and research tools without centralizing resources. Scholarly research and structural design therefore remain intimately connected. This demonstrates the significant returns from our early investment into DH research.  While we promote open access whenever possible, the reality is that many digitized resources in the humanities are still sold by publishers or private vendors. We have had to navigate this complex licensing terrain during our everyday work, and RISE and SHINE is now a prototype primed for a model of decentralized e-infrastructure for humanists. We believe that this approach is a fruitful one that would provide a sustainable and interoperable research environment for humanist scholars. ",
        "article_title": "RISE and SHINE: A Modular and Decentralized Approach for Interoperability between Textual Collections and Digital Research Tools",
        "authors": [
            {
                "given": "Sean",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            },
            {
                "given": "Pascal",
                "family": "Belouin",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            },
            {
                "given": "Hou Ieong",
                "family": "Ho",
                "affiliation": [
                    {
                        "original_name": "Staatsbibliothek zu Berlin",
                        "normalized_name": "Berlin State Library",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02ysgg478",
                            "GRID": "grid.461943.d"
                        }
                    }
                ]
            },
            {
                "given": "Shih-Pei",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for the History of Science",
                        "normalized_name": "Max Planck Institute for the History of Science",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/0492sjc74",
                            "GRID": "grid.419556.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-29",
        "keywords": [
            "copyright",
            "open access",
            "digital research infrastructures and virtual research environments",
            "digital humanities (history",
            "software design and development",
            "oriental and asian studies",
            "theory and methodology)",
            "standards and interoperability",
            "English",
            "licensing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  With the rise of the Internet, it has been pointed out that the Babylonian Talmud, with layers of texts commenting and referring to other texts, was an ancient hyper-text (Jerry Waxman, 1993; David Porush, 1998) Now, with the rise of social media, we can start to think of the Talmud as an ancient social network. True, Abaye and Rava did not use Facebook or Twitter. Rava cites Rav Nachman rather than retweeting him or sharing his status update. Although Abaye didn’t “friend” Rava, they studied under a shared teacher, Rabba, both headed the academy at Pumpedita, and they frequently take opposing positions and argue with one another. We set out to create a social network graph of the Babylonian Talmud. Using Named Entity Recognition of a statement-aligned Aramaic / English bitext, we found nodes (rabbis) and edges (scholastic interactions, such as citation). We induced formal scholastic relationships from these interactions, and propagated biographical information across the graph. We highlighted rabbis in the Talmudic text, using color to indicate scholastic generation, and displayed relevant subgraphs to show how the participants on a folio relate to one another both locally and globally. This biographical and scholastic information can aid a student of Talmud in understanding the dynamics of the discourse and why specific rabbis say what they say. For instance, the modern Revadim approach to Talmud (Hayman 2004) study makes great use of which generation a Tanna or Amora lived, which academy he belonged to, and so on.    Related Work The study of social network graphs of scholastic relationships has occurred in other, non-Talmudic, contexts. See for example Kofia et al. (2015). For the Babylonian Talmud, many printed works and a few existing digital resources can prove useful to identifying rabbis and their interactions. Satlow (2017) created a spreadsheet with approximately 5000 rabbinic names listed by Aaron Hyman in  Toldot Tanaʾim ṿe-Amoraʾim (1910), as an intended first step to performing social network analysis of rabbinic literature. Parker (2009) assembled a database of approximately 250 Tannaim and Amoraim, their scholastic generations, and teacher / student relationships between them. Sefaria is an open and free online library of rabbinic literature. Among the items in their database is a digital version of the Koren Noé Talmud, with the Aramaic and the English translation aligned statement by statement or paragraph by paragraph, and with linked commentary.     Our Approach We performed Named Entity Recognition on the statement-aligned bitext corpus to mark up the scholars and their interactions, in both the Hebrew and English text. Such NER is challenging on purely Hebrew text (due to such ambiguities as where a name begins and ends), but we exploited the aligned text and a transliteration model to attain fairly accurate results. We used customized fuzzy string matching to match these names to those in the Parker database, when the names are spelled differently or have a patronymic.   The Talmud consists of 5,894 folio pages, of which we have processed 4965 (84%), to recognize approximately 145,000 named entities and 12,000 interactions. Testing on a small tractate consisting of 59 folio pages, for just the English named entities, we achieve 81% precision, 94% recall, and an F1 score of 87%. For pairs of (English, Hebrew) in the literal text, performance is slightly lower (76% precision, 89% recall, 82% F1).  We built two graphs. The first was based on Parker’s data. The second graph was populated from the NER- extracted names and interactions. Rabbis are represented as nodes, and weighted edges exist for each type of interaction (such as inquires, cites, speaks_to, and so on). Where possible, we transfered generational information from the Parker data, which sometimes required disambiguation based on context. For instance, “Rabbi Eleazar” would refer to Rabbi Eleazar ben Shamua in a Mishna or  brayta but usually Rabbi Eleazar ben Pedat, an Amora, otherwise.  Because many rabbis are not encoded in Parker’s database, our graph includes nodes not marked for scholastic generation. We are exploring a few approaches to bootstrap off our initial generational knowledge, using our detected relationships. As an example, if known scholar A, of generation 3, cites unknown scholar B, then we can assume that B is  probably of generation 1 or 2, that is, a near but previous generation. If known scholar A speaks to unknown scholar B, and B replies, they are probably within a generation of one another. By considering these constraints and iterating, we can propogate generational knowledge across the graph.  We have begun to summarize interactions as scholastic relationships (e.g., if A often cites/inquires of B, then A is likely B’s student). Other identifiable relationships are primary teacher, colleague, and frequent disputant.    Results and Conclusions The current release version of the system is available at  www.mivami.org. Here, we include a few figures to illustrate the system output, drawn from Menachot 2b.      Figure 1 is a single Talmudic statement. The literal translation is bolded while the gloss text is not. Rabba is colored red and Abaye green, matching their Amoraic generation as given in a legend (A3 and A4). Rabbi Shimon is colored as a 5th generation Tanna. Hover text presents patronymic and generation.       Figure 2 is the teacher/student relationship graph of that statement, again color coded. Circles with outlines are rabbis who appear on the page. Circles without outlines are shared teachers. Thus, we see that Abaye and Rava are contemporaries (A4) and were both students of Rav Yosef and Rav Nahman.       Figure 3 displays local interactions; the bidirectional arrow shows Rabba and Abaye speaking to one another.     Finally,  Figure 4 shows the rabbis local to the page, as they interact across the entire Talmud. Rava both speaks to and cites Rabba, making it apparent that they interacted face-to-face with Rabba as teacher.  Together, these graphs provide valuable insight into the meaning of the texts and dynamics of Talmudic discussions by highlighting relevant biographical information that is not readily accessible on the page of the Talmud.   ",
        "article_title": "A Graph Database of Scholastic Relationships in the Babylonian Talmud",
        "authors": [
            {
                "given": "Joshua",
                "family": "Waxman",
                "affiliation": [
                    {
                        "original_name": "Stern College for Women, Yeshiva University, United States of America",
                        "normalized_name": "Yeshiva University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/045x93337",
                            "GRID": "grid.268433.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "digital textualities and hypertext",
            "theology and religious studies",
            "natural language processing",
            "English",
            "network analysis and graphs theory",
            "electronic literature",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Historical photographs are important sources for documenting the past of humanity. They can serve as evidence of our society’s history but are also objects of research themselves. Today, the standard format for describing the geographical origin of a photograph is to assign a geoname or point coordinate of the location of the photographer to the image. In view of historical photographs, we are dealing within this operation with spatial uncertainty. In most cases it is not possible to define the exact location of the photographer in form of coordinates in e.g. regions that afflicted strong environmental changes over time. The aspect of spatial uncertainty is also present, when describing the location of the photograph`s origin by a so-called geoname. A geoname can be comprehended to different extents. For example, it can name a region, a city or just a small part of a city. Therefore the question of the exact spatial origin cannot be answered by solely using a geoname. Such aspects of uncertainty in the localization of historical photographs complicate the retrievability in archives for spatially oriented search. Since the photographs cannot be found by entering search queries in the archive, their potential to serve as a data source for research cannot be fully exploited. From a technical and mathematical perspective, photographs are a two-dimensional representation of information from a three-dimensional space due to its central perspective character. Each object, such as a building or a landscape element, has two-dimensional coordinates in the image and corresponding three-dimensional coordinates in the world coordinate system. This approach describes a reverse geographical search of historical photographs based on this underlying principle. This means, that spatial information will not be defined by one geoname with various possible spatial extents describing where the picture has been taken, but instead we allow to assign global coordinates to each depicted building and object in the respective photograph for a better georeferencing of the whole picture. This method counteracts to the given issue of uncertainty in spatial metadata of historical photographs (Dorner et al., 2018). Map based retrieval techniques will be able to better satisfy the information demand of spatially oriented and object centric scientific disciplines such as archaeology, monument conservation, architecture or landscape planning. The presentation demonstrates how an object centric search approach could help to better find photographs for the purpose of e.g. spatially oriented humanities. A web-mapping system allows to use web based tools, to georeference objects depicted in a photograph on a dynamic web map. Users can place markers of different types to define objects such as a building, person or landscape element (e.g. mountain, river, forest, etc.) on historical photographs and can then assign them to a real location on a world map. The more items have been located on the map, the more accurate the image origin can be determined and the more information can be assigned to the photograph stored in the archive (visualized in Figure 1).     Figure 1: Visualization of the principle of pasting elements in a photograph and their assignment to world coordinates in a map based on OpenStreetMap data (  https://www.openstreetmap.org  ).  An overview map allows to access photograph collections via spatial interests. Photographs shown there can be filtered by entering metadata search queries such as geonames or assigned tags in a search bar (Figure 2).    Figure 2: Screenshot of the overview map with georeferenced photographs, villages and components in the Bavarian-Czech border area (https://photostruk.de) An additional automated computer-vision-based pre-classification of photographs, helps to filter search results for image categories like e.g. landscape, building, group of persons, portrait. This machine-based image analysis allows a rough categorization of photographs in great archival collections (Eiler et al., 2018), that are not at all or not well documented due to missing manpower or financial resources. As an outlook, an application in archaeology will be presented. Archaeologists from the University of South Bohemia České Budějovice use the web-mapping tool for historical photographs in order to reconstruct abandoned settlements that were destroyed after the eviction of the German population in the Czech countryside near the national border to Bavaria (Paclíková et al., 2018). The data collected via the online tool will be used for the orientation of the photographs in three-dimensional space. Further on, the oriented images are used for creating 3D reconstructions for the digital preservation of abandoned sites. The online tool has been developed in 2018 in order to serve crowd-sourcing methods to the public in the middle of 2019. The procedure is supported by the European Union within the Cross-Border Cooperation Program Freistaat Bavaria - Czech Republic Objective ETZ 2014-2020. ",
        "article_title": "How to Better Find Historical Photographs in an Archive - Geographic Driven Reverse Search for Photographs",
        "authors": [
            {
                "given": "Anne",
                "family": "Weinfurtner",
                "affiliation": [
                    {
                        "original_name": "Technische Hochschule Deggendorf, Germany",
                        "normalized_name": "Deggendorf Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02kw5st29",
                            "GRID": "grid.449751.a"
                        }
                    }
                ]
            },
            {
                "given": "Wolfgang",
                "family": "Dorner",
                "affiliation": [
                    {
                        "original_name": "Technische Hochschule Deggendorf, Germany",
                        "normalized_name": "Deggendorf Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02kw5st29",
                            "GRID": "grid.449751.a"
                        }
                    }
                ]
            },
            {
                "given": "Simon",
                "family": "Graf",
                "affiliation": [
                    {
                        "original_name": "Technische Hochschule Deggendorf, Germany",
                        "normalized_name": "Deggendorf Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/02kw5st29",
                            "GRID": "grid.449751.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-03-28",
        "keywords": [
            "crowdsourcing",
            "software design and development",
            "artificial intelligence and machine learning",
            "English",
            "computer science and informatics",
            "digital archives and digital libraries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This short paper reports on the development of a system that incorporates agent-based modeling (ABM) in art/architecture historical research and scholarship. ABM is a computational process simulating agents and their behaviors; the relationships between agents; and the interaction between agents and their environments. ABM has been used in a range of fields including predicting the spread of epidemics, behavior in economic systems, movement within the built environment, egress modeling (e.g. stadiums, submarines) and many more. (Macal, 2005; Lynne, 2015; Simeone, 2012 ) The development of this ABM system is supported by the Getty Foundation as part of the Advanced Topics in Digital Art History: 3D and (Geo)spatial Networks Summer Institute, Venice, 2018-2019. In art/architectural history, ABM enables simulating inhabitants in addition to space itself and its formal qualities. Agents, programmed with basic rules or data to move autonomously within space, are modeled to recognize and “sense” their environment. This approach can expand both art historical questions and narratives by observing emergent movement in space and interactions between inhabitants. As a highly iterative computational process ABM allows for experimentation and new outcomes emerging from slight variations.  In our prototype of the Istanbul Toptasi Insane Asylum (functioned 1876-1924), we model medical and daily routines of the asylum inhabitants. (Ozludil, Wendell, 2016) This setting presents both a computational and philosophical challenge as ABM agents are typically assumed to be active, autonomous individuals with decision-making capability in a non-restricted environment. In contrast, the asylum is a highly-regulated environment with unpredictable (and arguably “irrational”) agents. Based on scattered evidence on life in the asylum and the scripted strict schedule, we model interactions between various agent types, as well as exposure of patients to natural light, air, and ventilation, all of which are measurable with the autonomous sensing of agents. The asylum presents a productive case study of applying ABM to art/architectural history as the movement of agents provides insight into the functioning of a nineteenth century imperial medical facility.  Objectives The specific objective of the ABM in this case study is analyzing whether or not the scripted rules of daily life in the asylum could be implemented given the number and types of admitted patients in the year 1911. We iterate the process by adjusting the attributes and detailing the rules (where they are ambiguous) to produce various potential scenarios. While the objective here is limited to the simulation of the routines of patients, we do believe that the experiment provides some insight into the daily experience of patients, in the absence of textual evidence produced by them, such as letters, diaries. Our claim is not that this will be an all-encompassing understanding of the patient experience, but that it has the potential to open a window into these now-lost lives and, at a larger scale, to allow us to elaborate theoretical implications in question.   Method Agents in the Toptasi Asylum simulation adhere to a strict set of rules. These rules allow the agents to behave autonomously over a long simulation time using only their initial programmed instructions. A central simulation clock is referenced for choreographing daily routines, instructing agents to find the shortest path to their next itinerary location using staircases and walkable surfaces. Below are three tables showing the attributes, rules, and the model timing that govern one agent type, the patient, chosen for this paper. (We used the framework presented in Axtell, Robert L., et al. 2002.)    Figure: Screen captures from the 3D model in SpatioScholar and historical photographs showing a male patient ward and the female patient courtyard. Left bottom image shows the ABM system within Unity3D. Sources: Right top: Ergin, Müessesat-ı Hayriye-yi Sıhhiye Müdiriyeti, 1911; right bottom: Mazhar Osman, Sıhhat Almanakı, 1933.  Toptasi Patient (Agent) Attributes  Each agent represents an asylum inhabitant that belongs to one  type (In this case:  Patient. Others are Medical Officer, Medical Attendant, Administrative Officer, Administrative Attendant)  Each agent belongs to one  ward in the asylum  (Classification of the Toptasi Asylum into separate wards based on disease type)  Each agent belongs to one  sex that ties into the block and ward that they are admitted (Female/Male)  Each agent is in one  state (Classification of patients based on their recorded diagnosis)     Toptasi Patient (Agent) Simulation Rules  An agent starts the day, cleans self and makes their bed at 07:00  An agent proceeds to have breakfast in a dining hall at 08:00 An agent needs to be in their ward between 09:00-11:00 for daily medical visits An agent proceeds to have lunch in a dining hall at 11:00  An agent has time dedicated to a mixture of airing, exercise, socializing, education etc. between 12:00-16:00 An agent proceeds to have dinner in a dining hall at 16:00 An agent proceeds to their ward for down time and eventually sleep at 17:00    Toptasi Agent Simulation Model Timing A single clock increments each minute of simulated time. This clock broadcasts the current time to all agents for their internal itinerary movement. The simulation may run in either real time, where a minute within the simulation equates to a minute outside the simulation, or in a compressed timeline where the simulation clock calculates at a higher rate.    The Platform The ABM system is built within the Unity3D application as a series of prefabricated objects and C# scripts. Agents are scripted objects that use the existing Unity3D codebase to path find and navigate within any imported 3D historical model. A natural language text file defines the attributes and daily itinerary of each agent cohort, allowing alternative and multiple agent setups to be quickly loaded from outside the simulation. An example itinerary line from this file, \"1200\": \"Target DiningHall\" defines a change in location by time and target object. he simulation agents continuously test a number of conditions using iterative and highly accurate raycasting methods: the proximity and visual connection to other agents, exposure to natural light and intervisibility to specific architectural and programmatic features. The outcome of these conditions are stored as data within each agent and are downloaded into a master external CSV file. The output CSV file makes the agents’ data available for data processing and visualization.  ",
        "article_title": "Agent-Based Modeling in Art History: Simulating an Insane Asylum",
        "authors": [
            {
                "given": "Augustus",
                "family": "Wendell",
                "affiliation": [
                    {
                        "original_name": "New Jersey Institute of Technology, United States of America",
                        "normalized_name": "New Jersey Institute of Technology",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05e74xb87",
                            "GRID": "grid.260896.3"
                        }
                    }
                ]
            },
            {
                "given": "Burcak",
                "family": "Ozludil",
                "affiliation": [
                    {
                        "original_name": "New Jersey Institute of Technology, United States of America",
                        "normalized_name": "New Jersey Institute of Technology",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05e74xb87",
                            "GRID": "grid.260896.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "art history and design studies",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "theory and methodology)",
            "English",
            "3D/4D modeling",
            "modeling",
            "simulation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This study applies computer vision techniques to examine the representation of gender in historical advertisements. Using information on the relative size, position, and gaze of men and women in thousands of images, we chart gender displays in Dutch newspaper adverts between 1920 and 1990. In the 1925 edition of  Psychology in Advertising psychologist Alfred Poffenberger encouraged ad makers to ‘short-circuit the consumer's mind through vivid, pictorial appeals to fundamental emotions’ (Marchand, 1985). The images and visual clichés tap into a representational system that produces meaning outside the realm of the advertised product (Goldman, 1992). In the late 1970s, sociologist Erving Goffman examined the semiotic content in advertisements printed in contemporary newspapers and glossy magazines. He noted that displays of gender are often ‘conveyed and perceived as if they were somehow natural, deriving, like temperature and pulse, from the way people are and needful, therefore, of no social or historical analysis’ (Goffman, 1976). Of course, almost the exact opposite is true. Goffman conceived of five different categories to study the depictions of, and the relations between men, women, and children in advertisements: relative size, feminine touch, function ranking, ritualization of subordination, and licensed withdrawal. Drawing from this approach, Jonathan Schroeder, contends that in advertisements the male regularly represents the ‘the active subject, the business-like, self-assured decision maker, while the female occupies the passive object, the observed sexual/sensual body, eroticized and inactive’ (Schroeder, 2004).  Studies like that of Goffman and Schroeder relied on a limited number of advertisements, and they also do not take the historicity of ads into account. For example, Goffman looked at around 400 different advertisements, which he more-or-less randomly selected from ‘newspapers and magazines easy to hand - at least to my hand.’ As Kang (1997) notes, he was often criticized for this method. Instead of relying on a random sample, he purposefully ‘selected images that mirrored gender differences.’ Goffman emphasized that he choose his categories partly on the basis of the fact that he could find almost no images that disproved his categories. For example, images in which women were relatively larger and had the executive role. In an earlier project, we developed methods to extract visual material from historical newspapers (Wevers and Smits, 2019). Using a large-scale data set of digitized historical advertisements extracted these methods, we examine continuity and change in displays of gender using computational means. By applying state-of-the-art computer vision methods to a diachronic set of digitized Dutch newspapers advertisements published between 1920-1990, we can test existing hypotheses and contribute to existing replication studies about gender displays in advertisements (Kang, 1997; Belknap, 1991; Lidner, 2004).  The data is kindly provided by the National Library of the Netherlands.  We will start our research after the First World War, because Dutch advertising practice changed rapidly in the Interbellum. Technological advances made it cheaper to print images in newspapers. Influenced by American ad agencies, Dutch ad makers saw the potential of using visual material to increase sales and to convey particular brand identities (Schreurs, 2001).  In this short paper, we operationalize Erving Goffman's theory on gender displays in two ways. First, Goffman argues that ‘differences in size will correlate with differences in social weight’ (Kang, 1997). Using facial recognition software (Geitney, 2018; Zhang, 2016), we select adverts that include people, and then train a gender detection algorithm using a convolutional neural network to estimate whether men or woman were represented in the images (Levi, 2015). This allows us to visually represent the changing faces of men and women in advertisements. Part of the process also entails a reflection on the inherent bias in these algorithms. Second, Goffman contends that body postures and the engagement in social settings show that men are often portrayed in ‘executive roles’ and that women are often depicted as being removed from the social situation gazing off into the distance. Using information extracted in the first step, we quantify whether how people were represented on image, e.g. how was position where on the image? In Figure 1, for example, we see a social gathering of four people with the central figure drinking a bottle of Coca-Cola. Using the described computer vision techniques, we can extract faces and determining where on the images these are located. This reveals a man as a central figure with two women and man surrounding the central man. By extracting these features from a large set of advertisements and applying clustering methods, we can detect patterns in advertisements over time that can be compared to findings by Goffman and others. (Impett, 2017).   Coca-Cola advertisements.  Nieuwsblad van het Noorden, August 14, 1959.   We argue that it is not only possible to test existing theories on displays of gender in a more extensive database, but also to shed light on their historicity: the fact that these displays seem natural and constant, but can, and will, change, sometimes rapidly, over time. In line with the conference theme of complexity, we are aware that it is problematic to reduce displays of gender to binary categories. In this study, we are operationalizing displays of gender to a limited set of features, thereby reducing the complexity of gender. However, the coarse-graining of displays of gender into features that are comprehensible and computable allows us to extract trends from collections of advertisements. These trends offer context to particularities and can reveal the use of visual clichés in advertising discourse. Moreover, in the paper, we evaluate and reflect on the use and performance of models trained on contemporary training data as well as relying on probabilities to define gender. ",
        "article_title": "Advertising Gender - Using Computer Vision to Trace Gender Displays in Historical Advertisements, 1920-1990",
        "authors": [
            {
                "given": "Melvin",
                "family": "Wevers",
                "affiliation": [
                    {
                        "original_name": "KNAW Humanities Cluster, The Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": "Smits",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-27",
        "keywords": [
            "digital humanities (history",
            "content analysis",
            "theory and methodology)",
            "English",
            "cultural analytics",
            "image processing",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In the TerInfo project, students and experts from Pedagogy, Religious Studies, Psychology, History and Information Science (Utrecht University) collaboratively aim to increase the societal resilience against terrorism in Dutch primary and secondary education. The project formally started in July 2017 and is supported by the city of Utrecht. Project leader is Beatrice de Graaf, professor of History of International Relations & Global Governance. Utrecht’s population has a very diverse background, which is reflected in the classrooms. Especially when terrorist incidents occur, fragmented and unverified information reaches the pupils very quickly though different channels (with an ever-increasing role for the social media). Such information may have immediate negative consequences, for example it may increase anxiety with children or create tensions between them. Teachers who consequently feel an urge to comfort, provide perspective or de-escalate, often experience a 'reluctance to act' (de Graaf, 2018). Therefore, TerInfo aims to support teachers by providing them with:  reliable and compact information about terrorism and radicalisation practical ideas and assignments for discussing these issues in class in an inclusive manner quick and helpful interpretation of recent developments expert support tailored to the school’s specific needs.   The project is supported by a mobile website (https://ter-info.nl/) containing a growing collection of carefully prepared materials. This paper describes the creation of this site and the design philosophy behind it. Much effort has gone into designing it to optimally match the requirements of the teachers and to empower them in their work. The website was developed, in several iterations, by a team of seven bachelor students of Information Science, six following the honours programme. This programme offers students a range of learning opportunities beyond the standard programme, which they can tailor to their own interests. All six honours students decided to spend a significant part of their programme working on the Ter Info website.   Prototype The first iteration of website development took place in April-June 2017 during the 'introduction project', a group project that completes the first year of the programme. The assignment for the introduction project is to design and prototype an innovative interactive system that answers a societal need. Students are expected to apply all relevant knowledge and skills they have acquired so far, but are otherwise given considerable freedom in the choice of topic, methods and organisation. All projects, prototypes in particular, are publicly presented during a symposium. When this student team embarked on this project, the general problem outlined above was available to them, but beyond this they had very little information to go on. Taking a human-centred design approach (Benyon, 2014) they decided to first investigate the teachers’ experiences and needs, and to familiarise themselves thoroughly with the topics of terrorism and radicalisation. Next, they created a set of draft articles, providing information on various terrorist attacks and organisations as well as interpretation (e.g. an article 'Terrorism comes and goes'), and supporting materials for use in class. The design of the website started from a set of personas and scenarios based on the interviews with the teachers and went through several iterations of lo-fi and hi-fi prototypes. The final prototype holding the created materials was realised in PHP using the Laravel framework (Laravel, 2018) and includes a backend for the creation of new content, based on AsgardCMS (AsgardCMS, 2018). The prototype was evaluated both by school teachers and through heuristic evaluation by team members.   Pilot Once the project formally started in summer 2017, the platform was further developed by the Information Science students, with domain experts and students in Pedagogy and History taking responsibility for the content of the site. The content is structured in five levels:  About Ter Info Terrorist attacks Terrorism in a nutshell Terrorists Discussing terrorism in class  Articles typically take 2-4 minutes to read, contain a FAQ section, and provide links to further materials on the Internet and to academic sources. There are also links to the relevant articles on the site itself: for example, the article “What is terrorism” links to “What terrorists want to achieve” and “How scared should we be of terrorism” (with some sobering statistics). In March-June 2018, a pilot study was done with five schools from the city of Utrecht participating. These schools display a large diversity both in level (primary, secondary) and ethnic background. At the beginning and end, seminars were organised with expert presentations and discussion sessions. Between seminars, workshops were held at each school in order to fine-tune the pilot to the specific circumstances. Overall, participants were quite satisfied with the project, yet they had a great deal of feedback on all aspects of the project. It was observed for example that article length was good and the FAQs were often used, but that academic sources were never consulted. Many suggestions were made for adding pages (including some open pages to inform the general public, as the entire site is now password-protected). There is a clear wish for personalisation based on type of school and for a push notification system to announce new materials and developments.   Next steps Phase 2 of TerInfo is currently in full swing, with 19 schools participating. In parallel, a next version of the site is being developed by a new team of students, from the University of Applied Sciences Utrecht. In my presentation I will discuss the evaluations and experiences on which this next version will be based. I will also present a long-term vision of the website, with particular attention to the issue of scale. Currently the university experts provide intense individual support to the schools. In the long run this is not sustainable. A likely avenue thus seems to be to reshape the site as an online community, where participants gradually develop expertise themselves and share their experiences with their peers.   Acknowledgement TerInfo’s website was developed by Information Science students Mirkan Davarci, Max Groot, Lisa Hordijk, Mitchell Klijs, Hessel Laman, Herman Nelissen and Govert Vermeer. We greatly appreciate their contribution to the project.  ",
        "article_title": "A Mobile Website To Support Teachers In Discussing Terrorism In The Classroom",
        "authors": [
            {
                "given": "Frans",
                "family": "Wiering",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, Netherlands, The",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-21",
        "keywords": [
            "software design and development",
            "interdisciplinary & community collaboration",
            "English",
            "computer science and informatics",
            "public humanities and community engaged scholarship",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " There has been greater inclusion of teaching in DH debates in recent years, yet in practice, UK digital humanities teaching tends to be found in explicitly DH postgraduate courses or as specific skills training for self-selecting staff or (often postgrad) students; it also appears less in English literature teaching than in history, classics, media studies or linguistics. This paper offers a case study where TEI XML encoding was introduced to first-year undergraduates in an established course on Shakespeare, to facilitate discussion on how digital humanists can enable integration of digital skills into ‘traditional’ English literature teaching by capitalizing on affinities between DH, book history, and textual scholarship. Integrating digital approaches into undergraduate teaching of English literature enables us to introduce students early on to reflecting critically on the digital, and lets us embrace the complexities of encoding as editing, rather than allowing undergraduates to only interact with the digital through a GUI. Early modern studies provides an ideal context in which to teach text encoding and digital literacy skills. In part this is due to the availability of encoded early modern texts for analysis and reuse, for example in the massive release into the public domain of lightly encoded EEBO-TCP texts in January 2015, and in part to fewer issues around copyright than with more modern texts (see Brown and Zimmer, 2017). More specifically, there is a synergy between the existing concerns of the Shakespeare course under discussion and those of digital publication, where the latter finds a natural fit in conversations on book history, text technologies, and editorial agency. Olin Bjork, in an article comparing American composition and computing classes to the “new media studies” side of digital humanities, suggests that “A weakness of digital humanities is that it undertheorizes the transformation of material objects into digital objects” (Bjork, 2012: 103). The materiality and the instability of the text are complexities that go to the heart of early modern literature studies: these are issues that our students grapple with when they consider the nature of the early modern play-text, yet rarely do we recognise that the digital text must be part of this conversation. Accordingly, in a course that unsettles the Shakespearean play as single authoritative text, there is a real need to push conversations on early print and textual scholarship into the realm of the digital, to unsettle and critique the digital texts the students encounter more often than they open their assigned course book. The course under discussion is a first year module with c.200 students enrolled, and the digital element ran as a successful pilot in 2018 and was expanded in 2019. The digital element is introduced in a guest lecture by a digital humanist; issues raised are debated in seminar groups, and existing digital texts and projects are introduced to the students, including A Digital Anthology of Early Modern English Drama (EMED), Internet Shakespeare Editions, the Queen’s Men Editions, Digital Renaissance Editions, and Folger Digital Texts, where students use the Folger API to explore the possibilities opened up by text encoding. The students then have the opportunity to create their own digital edition of the ending of King Lear, opting in to an additional workshop on TEI XML. They can elect to do their final assessment as a TEI-encoded play excerpt coupled with traditional essay, which allows them to reflect on the differences between early print, modern printed editions, and digital media. Writing about the medium and the choices it encourages or enforces allows them to critically reflect on digital texts at a crucially early point in their university careers. In the encoding part of the assignment, by taking ownership of a digital play text, they come to see both the scholarly edition and the digital medium as less an unquestionably authoritative black box and more something that they themselves can have agency over and interrogate. By pulling back the curtain on the scholarly text and the digital medium, they are more able to critique both text and textual manifestation.  DH in the undergraduate classroom often tends towards GUI publication, with wikis, Wordpress sites, or platforms like the Alliance for Networking Visual Culture’s Scalar (Bjork, 2012: 99–100). While that is a valuable part of the picture, this paper explores what happens when students are introduced to the complexities of XML encoding as an editing practice, and how exposure to the component parts of a digital publication opens up that black box. This reveals to the students the variety of people involved in publication, in modern as well as early modern times, and demonstrates that encoders are editors and developers are intellectual partners who have concrete influence over the resulting output. Significantly, this message is also imparted to fellow academic staff teaching on the course. This approach thus invites fellow academic staff into the digital humanities, developing by proxy their understanding of digital resource creation and consequently their ability to critique this growing area of scholarly production. The aim of this paper is to share experiences and create discussion around the natural affinity between early modern studies and digital publication and digital critical literacy, especially in an undergraduate context. A parallel can be seen between the instinct to view the digital object as something that appears from thin air and the unquestioning acceptance of a particular critical edition as the immutable authoritative text. It is too easy to ignore the digital provenance of a text online or the multiple agents involved in producing any (especially canonical) text. To do this is to remove those who construct the text, and obscure the encoding as well as editorial choices made at every stage of its creation. To direct attention towards these is to put them back. This is a conversation that we need to begin early on in students’ academic career, to situate digital critical literacy within an existing tradition of literary criticism: we need to teach students to close-read the material digital object at the same time as the literary text and its early print origins. ",
        "article_title": "Encoding Early Modern English Drama: Embedding Digital Approaches In Undergraduate Literature Courses.",
        "authors": [
            {
                "given": "Elizabeth",
                "family": "Williamson",
                "affiliation": [
                    {
                        "original_name": "University of Exeter, United Kingdom",
                        "normalized_name": "University of Exeter",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03yghzc09",
                            "GRID": "grid.8391.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-09",
        "keywords": [
            "bibliographic methods / textual studies",
            "renaissance studies",
            "English",
            "pedagogy",
            "text encoding and markup languages",
            "teaching",
            "literary studies",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The digitization of large time-labeled bibliographies has resulted in corpora such as the Google Ngram data set (Lin et al., 2012). Such corpora extremely accurately reflect how individual words are used over time. They are expected to reveal novel insights into the evolution of language and society, provided adequate analysis systems are available. In this context, developing a comprehensive query algebra that allows domain experts to formalize complex hypotheses would be a major contribution to successfully unlock this potential. The case of conceptual history serves as our example from the humanities. In conceptual history, researchers examine the evolution of concepts represented by words such as “peace” or “freedom”. In exploring the history of a concept, scholars commonly make use of, but are not restricted to, word-usage frequencies, word contexts, sentiment analysis, how words refer and relate to and contrast with each other, or they look for word pairs or word families whose usage is correlated (Brunner et al., 2004; Ritter and Gründer, 1971). Consider our example: how the words “East” and “West” change from merely cardinal directions to politically charged concepts after 1945. In this paper, we present a query algebra for empirical analyses of temporal text corpora, the Conceptual History Query Language (CHQL). A  temporal text corpus in our sense is a set of words and word chains, i.e., ngrams, together with their usage frequency at various points of time. Our query language is meant to be useful for domain experts, i.e., be descriptive and complete (match all actual and potential hypotheses of conceptual history), and bear optimization potential to allow fast query processing on large data sets. We focus on an algebra inspired by the German tradition of  Begriffsgeschichte (conceptual history), as exemplified by the work of Reinhart Koselleck (Olsen, 2012).    Related Work Existing query algebras, like the one for the Structured Query Language (SQL), do not feature specific support for analyses of the kind we envisage. Other approaches from the literature, e.g., the Contextual Query Language (The Library of Congress, 2013), the Corpus Query Language (Jakubíček et al., 2010), or the ANNIS Query Language (Zeldes et al., 2009), have similar issues. The common relational algebra (Maier, 1983; Abiteboul et al., 1994), does not contain sufficiently specific operators, e.g., temporal or linguistic operators. Extensions exist to add temporal operators (Snodgrass, 1987; Snodgrass, 1995), but not linguistic operators. To query relations between words, there are special-purpose query languages. For example, SQWRL is a language to query an ontology (O'Connor and Das, 2009). Querying word relations, e.g., from an ontology, does not include all required linguistic relationships. Further, ontologies do not provide temporal information. SQWRL does not contain any temporal operator. All of these algebras have in common that they do not cover both linguistic and temporal operators required for research on conceptual history. Related work in the digital humanities mainly consists of data processing and the analysis of text corpora (Warwick et al., 2012; Hai-Jew, 2017). Some frameworks focus on linguistic and reflective properties as well as their evolution such as (Hamilton et al., 2016a; Hamilton et al., 2016b; Prabhakaran et al., 2016; Englhardt et al., 2019). Respective systems cannot output the required information to conduct research on conceptual history in a comprehensive way. In addition, such systems do not provide a sufficiently  abstract interface, a reason why experts are reluctant in using them (Hai-Jew, 2017).    Concept Types and Operators This section shows in the abstract how the operators of CHQL allow searching for concept types. A formal definition of all of our operators is given in (Willkomm et al., 2018) and will be presented at DH2019. Conceptual history claims that pragmatic properties of historical, cultural and economic relevance are incorporated in concepts, irrespectively of whether individual users are aware of this or not. It attempts to track changes of particular concepts (such as “socialism”) over time to determine how their pragmatic relevance changes (it might mostly express generic hopes at some moment and mostly specific fears at some other). Thus, concepts will be categorized as belonging to a particular  concept type at a particular moment in time.  Conceptual historians typically read and interpret large masses of texts which provide a variety of information types (e.g. word frequencies, what words appear in the context, how these words function pragmatically (individually as well as in sentences etc.)) which help to determine the concept type. Because we want to do the same using  Distant Reading techniques (Moretti, 2013), these information types need to be translated into observable data characteristics for which individual operators in the query language are defined. Finding an adequate number of helpful information types, structuring them and converting them into computable and combinable items is the main challenge of our project.  Since there is no accepted formal specifications of information types, we describe an interpretation of Koselleck’s information types in order to map them on to data characteristics. Data characteristics are quantitative feature either directly present in our data (e.g., the usage frequency of the word “socialism” in 1848), or a derived piece of information (e.g. the difference between the usage frequency of words “socialism” and “communism” from 1848 to 1989). We describe which data characteristics are needed to simulate Koselleck’s information needs and explain our realization of all data characteristics and their implementation as operators.   The relationship between concept types, information types, data characteristics and operators to hypothesize concept types  One of Koselleck’s implicit assumptions is that each concept type has specific characteristics. In our terminology: any concept type can be described using a specific combination of information types. For example, Koselleck may plausibly be read as claiming that words that form a  parallel concept (concept type) would have “similar”  word frequencies and have a significant number of identical  surrounding words (information types). By contrast,  counter concepts would also have similar word frequencies yet their surrounding words would behave differently. For instance, if “enlightenment” and “reason” are parallel concepts for a particular period, their relative word frequencies should be similar, and if “emancipation” occurs near “enlightenment”, it should occur near “reason” too, and both concepts should be endorsed rather than criticized (in some sense). By contrast, if “East” and “West” are counter concepts, their word contexts should contain different words, and there should be some sort of contrast in attitude between them.  If every concept type has its own specific linguistic and pragmatic properties and hence should be representable by a specific  combination of information types, it should be possible to develop a system that finds these information types in large corpora that are not amenable to conventional close reading. To this end, we need a formal definition of any information type which is observable and quantifiable.  We present a selection of some of the data characteristics with the information type they are intended to represent:   Individual Context: This requires two data characteristics: a set of surrounding words for a target word, i.e., the linguistic context, and the sentiment for this context, by summing up the sentiment values of the words of the context. Our  surroundingwords operator and  sentiment operator implement this.   Topic Grouping: Using  topic modeling, groups of words may be classified as belonging to a particular topic (e.g. geography or politics).   Sentence Structure: This again requires two data characteristics: the function of a word, i.e., differentiate between parts of speech, and completing phrases, i.e., search for missing words in a phrase. The first data characteristic is implemented by our operator  pfilter. We implement the second one as a pattern-matching operator which we call  textsearch.   Frequency Data: Neologisms, which might be evidence for radical changes, would display abrupt increases in word-usage frequency over time. To find this and similar characteristics, we propose an operator  time series-based selection that compares the time-series values with a constant. To allow for a temporal restriction, we also provide a  subsequence operator that limits the selection to an arbitrary time interval. The combination of both operators facilitates the search for neologisms.   Sentiment Analysis: Using well-proven resources such as LIWC (Wolf et al., 2008) or customized dictionaries, our  sentiment operator represents the emotions associated with a concept, relying on the words in its context.     Results Using CHQL, we have tested the hypotheses that (1) “East” and “West” have acquired a political context after 1945, whereas “North” and “South” haven’t, and that (2) the former have turned into counter concepts in the political sphere, their contexts expressing diverging attitudes, whereas the latter have remained parallel concepts in the geographical sphere. The operator trees 1 and 2 shown in Figures 2 and 4 illustrate how CHQL allows combining the operators mentioned to perform a single search, yielding the results shown in Figures 3 and 5.   Formalisation of hypothesis 1 in CHQL    The result of Query 1 on the Google Books Ngram Corpus    Formalisation of hypothesis 2 in CHQL    The result of Query 2 on the Google Books Ngram Corpus   ",
        "article_title": "The CHQL Query Language for Conceptual History Relying on Google Books",
        "authors": [
            {
                "given": "Jens",
                "family": "Willkomm",
                "affiliation": [
                    {
                        "original_name": "Department of Informatics, Karlsruhe Institute of Technology",
                        "normalized_name": "Karlsruhe Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04t3en479",
                            "GRID": "grid.7892.4"
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Schmidt-Petri",
                "affiliation": [
                    {
                        "original_name": "Department of Humanities, Karlsruhe Institute of Technology",
                        "normalized_name": "Karlsruhe Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04t3en479",
                            "GRID": "grid.7892.4"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Schäler",
                "affiliation": [
                    {
                        "original_name": "Department of Informatics, Karlsruhe Institute of Technology",
                        "normalized_name": "Karlsruhe Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04t3en479",
                            "GRID": "grid.7892.4"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Schefczyk",
                "affiliation": [
                    {
                        "original_name": "Department of Humanities, Karlsruhe Institute of Technology",
                        "normalized_name": "Karlsruhe Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04t3en479",
                            "GRID": "grid.7892.4"
                        }
                    }
                ]
            },
            {
                "given": "Klemens",
                "family": "Böhm",
                "affiliation": [
                    {
                        "original_name": "Department of Informatics, Karlsruhe Institute of Technology",
                        "normalized_name": "Karlsruhe Institute of Technology",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04t3en479",
                            "GRID": "grid.7892.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-26",
        "keywords": [
            "corpus and text analysis",
            "data mining / text mining",
            "English",
            "cultural evolution",
            "computer science and informatics",
            "philosophy",
            "information retrieval and query languages"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Interest among Digital Humanities (DH) practitioners in semantic annotation of corpora continues to grow, while at the same time linguistic resources for niche domains or languages underperform, or are simply unavailable. Since DH research often involves multilingual and multi-domain corpora—drawing on different genres, periods and cultures, and ranging in type from unstructured prose to semi-structured text—annotation can require significant time and language skills that not all contributors in research groups have. Annotation in collaborative projects is a case in point (Dossin et al., 2016). Furthermore, off the shelf tools for annotation, despite being trained with language agnostic architectures, only process one language in one domain at a time (Al-Rfou et al., 2014; Lample et al., 2016). Since available tools do not perform well for niche domains (Boeten, 2015; de Naegel, 2015), new semi-automatic semantic annotation solutions must be sought for creating data for downstream DH tasks (de Wilde and Hengchen, 2017). For annotating diverse corpora, we propose a language-agnostic, robust and customizable named entity recognition (NER) resource. It can identify any type of user-specified entities in any textual domain, although here we have largely focused on the annotation of place names for use in the spatial humanities. Instead of relying on pre-existing NER taggers not tailored to the domain of interest, our resource enables humanists to build their own customized NER taggers. Such taggers require manually annotated data to learn to identify named entities automatically. However, since annotation is costly, we propose an active learning pipeline in which the most informative sentences in a corpus are identified prior to annotation. Our approach is to optimize performance while minimizing the time and energy of exclusively manual annotation (Kettunen et al., 2017).  To this end, we developed the Humanities Entity Recognizer (HER), which uses the conditional random field (CRF) machine learning architecture, both to identify sentences containing named entities most crucial for annotation and to identify named entities once sufficient manual annotation is completed (Nadeau and Sakine, 2007; Erdmann et al., 2016).  The system is designed to work with unstructured texts in any language, especially humanities texts where entities are unevenly distributed. It assumes no language resources beyond a tokenizer.  Our inspiration comes from related NER research that demonstrates that feature-based architectures like CRFs can, in fact, be language agnostic (Curran and Clark, 2003). However, such systems do not anticipate noisy DH domains. A promising active learning strategy for low resourced neural NER exists (Shen et al., 2018), although it requires computational resources we do not assume to be available to most humanities researchers. Finally, a transfer learning solution has been proposed, whereby a model for the under-resourced Uyghur language leverages better resourced NER models for two related languages (Bharadwaj et al., 2016). This pipeline however, requires access to multiple specific sources of data beyond the average humanities data scenario.  Our design merits particular attention for its careful consideration of the workflows of the humanist. Using limited entity lists (for example, placeographies or personographies built from annotated data on the fly), we delexicalize features and introduce other factors to encourage the algorithm to generalize rapidly in identifying new entities with high recall, sorting sentences based on likelihood to contain frequent, previously unannotated named entities. The system learns on its own that capitalization is important in languages where it is, but can ignore capitalization in other languages. Since the textual scholar working with the HER system is actively involved in the iterative annotation and correction, a choice was made to favor recall over precision for the simple reason that  it is easier to remove, or hand correct, an inaccurate entity than to lose one in the “black box.” The annotation process on the initial seed and successive batches resembles a close reading of the corpus from the perspective of its potential named entities.  At present the scripts work on the command line and subsequent annotation is carried out in a text editor. Integration into community-based, social annotation interfaces is a desirable next step, but beyond the scope of this paper.   Our choice of CRF machine learning stems from a commitment to under-resourced domains, since neural models are  notoriously data hungry. Nonetheless, we tested neural models (BiLSTM-CRF and CNN-BiLSTM) extensively both for active learning and/or for performing the final NER tagging, with the result that CRFs outperform them until the amount of manually annotated data exceeds about 30,000 words. Should the user exceed this threshold, HER also supports use of neural models. Additionally, the interpretability of CRFs allowed us to maximize multiple criteria (uncertainty, representativeness, and diversity) when choosing the best sentences to annotate, whereas the neural models performed poorly for active learning, only capable of predicting sentences’ uncertainty. We also introduce the notion of an “inclusive” evaluation framework whereby the accuracy of the model is determined by both manual and automatic annotations, rather than “exclusive” frameworks that look only at the final model’s prediction on a held out test set (Erdmann et al., 2019).     Figure 1: Inclusive and Exclusive Evaluation of Learning Architectures (shallow and deep) with the Humanities Entity Recognizer (HER) The code and documentation for the Humanities Entity Recognizer (HER) are freely available at http://github.com/alexerdmann/HER. Our research stems from a session of a 2018 NYU-PSL Global Alliance funded workshop devoted to named entities and spatial humanities research. We began with non-English materials already annotated for named entities: the FranText corpus (around 200 works of pre-1920s French prose). We chose six sample texts across very different domains—a travel narrative, a gastronomic treatise, novels, an autobiography and a memoir—of varying named entity densities and distributions. The texts’ proper names were further annotated to distinguish place and person names. Performance of models trained using the HER system for annotation increased significantly. After annotating just 40,000 words, error was reduced 68.6% as compared to annotating 40,000 words from randomly selected sentences. In addition to this previously annotated corpus—not the norm for most DH research—we worked with three other unannotated corpora of significant typological and structural difference:  A German corpus of approximately 1M words (novels, travel narratives and philosophical texts) from the Weimar period, partially sourced from Project Gutenberg; A medieval French corpus composed of 1.1M words in both prose and verse for which a pre-existing placeography was available (drawing on the Open Medieval French corpus); A Portuguese corpus consisting of approximately 250K words from catalogs of the São Paolo exhibitions (1951-1959) containing a significant amount of structured information (lists of artists’ names, artwork titles, dates, places, etc.) for which a personography and placeography were available from the Artl@s Global Exhibition Catalogues Database.  We discuss results using these corpora, as well as challenges encountered using the Humanities Entity Recognizer (HER) across diverse domains and different kinds of entities. We report performance over a learning curve of quantities of manual annotation and qualitatively evaluate predictions in the absence of gold-standard data (van Hooland et al., 2015). We conclude with our key contributions in the development of this system: “whiteboxing” the NER process, handling totally unresourced domains and messy corpora in a language agnostic manner, and designing complete flexibility to granularity and types of entities. ",
        "article_title": "Active Learning from Scratch in Diverse Humanities Textual Domains: Optimizing Annotation Efficiency for Language-Agnostic NER",
        "authors": [
            {
                "given": "Alexander",
                "family": "Erdmann",
                "affiliation": [
                    {
                        "original_name": "Computational Approaches to Modeling Language Lab, New York University Abu Dhabi, United Arab Emirates; Department of Linguistics, Ohio State University, Ohio, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "David Joseph",
                "family": "Wrisley",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities, New York University Abu Dhabi, United Arab Emirates",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Béatrice",
                "family": "Joyeux-Prunel",
                "affiliation": [
                    {
                        "original_name": "Département d'histoire et de théorie des arts, Ecole normale supérieure, Paris Sciences Lettres, Paris, France",
                        "normalized_name": "École Normale Supérieure",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/05a0dhs15",
                            "GRID": "grid.5607.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-10",
        "keywords": [
            "corpus and text analysis",
            "multilingual / multicultural approaches",
            "semantic analysis",
            "natural language processing",
            "geography and geohumanities",
            "English",
            "computer science and informatics"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Brief Summary  In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources. PARTHENOS develops educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information.   Background Digital Research Infrastructures play an increasing role in the Humanities and in Cultural Heritage Studies (ESF 2011; Benardou, Champion, Dallas and Hughes (ed.) 2017). While some definitions of research infrastructures focus resources and tools provided, that is their “hard”, rather technical aspects, more and more their “soft” aspects, that is their function as networks of knowledge and people come into focus, with both aspects being in the service of aggregating resources to make us better connected and more informed (Edmond and Garnett 2017). Digital research infrastructures are relatively recent additions to the humanities and cultural heritage landscape; therefore, their transformative presence needs to be embedded in university curricula and beyond because the ongoing digital transformation of research affects not only (future) researchers, but also cultural heritage practitioners in the sense of lifelong learning. Those who use digital research infrastructures for their research and those who contribute to their development and their extension alike, need to acquire additional theoretical knowledge and practical skills to fully harvest the fruits of their resource-intensive development and sustenance and to make use of their potentials to advance research. This need for a broad implementation of data skills into curriculum frameworks and training for all disciplines was only recently underlined by the European Commission Expert Group on FAIR data (EU 2018). Before the start of PARTHENOS, a cluster project that consists of important European eHumanities and eHeritage Research Infrastructures and institutions and is funded by the European Commission, eHumanities and eHeritage infrastructures already offered training events and contributed to the development, extension, and sustenance of platforms for online training materials or course overviews, e.g. #dariahteach or the DH Course Registry. However, many of these training materials and events are highly project specific or thematically or methodologically specialised and require prior knowledge that most potential new users often lack (Edmond et al. 2016). This knowledge gap not only prevents them from successfully engaging with the material, it often forms a barrier that prevents them from taking interest into infrastructural themes in the first place. PARTHENOS aimed to fill these gaps by developing educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information (Spiecker et al. 2017, Edmond and Garnett 2017, Wuttke et al. 2019).   Developing PARTHENOS Training  The development of the cross-disciplinary PARTHENOS training materials is spearheaded by the PARTHENOS Training Team (lead by Trinity College Dublin, Dr Jennifer Edmond) with additional input from the User Requirements Team (lead by CLARIN, Steven Krauwer). Conforming to the results of the analysis of the user requirements carried out in the first phase of the project, PARTHENOS Training focuses on online-materials for self-study and for reuse by educators for teaching purposes in different contexts (synchronous and asynchronous) using the PARTHENOS Training Suite as its main carrier. Additionally, the training-portfolio encompasses (virtual) events to test the educational materials against users, such as the PARTHENOS eHumanities and eHeritage Webinar Series. PARTHENOS Training develops on the basis of the wealth of knowledge inherent in the project through its project partners introductory, cross-disciplinary learning resources that address infrastructural metatopics and are suited to foster the skills and knowledge that empower and unleash the potentials of eHumanities and eHeritage research(ers). The focus is on knowledge transfer about the roles, functions, and potentials of eHumanities and eHeritage Research Infrastructures for researchers, practitioners, developers, data and computing centre staff, policy makers, and managers. During the first phase of PARTHENOS Training the focus was on more generic levels of information. The first modules were centred around the creation of general knowledge, skills, and abilities that foster the understanding what eHumanities and eHeritage Research Infrastructures are, how they can be of benefit for different communities, and what kind of knowledge and skills are needed to successfully work with them. After an intensive assessment exercise, the focus was shifted in the second phase to more specialized areas, especially highlighting how outputs and products of Research Infrastructures and particularly of PARTHENOS can help users to navigate successfully through the increasingly complex Digital Humanities and Cultural Heritage research landscape. The modules created in the second phase address topics such as research data management, creating and assessing research impact, the use of community standards and ontologies, and how to develop research questions using Digital Humanities methods and tools from a broader Humanities and Cultural Heritage perspective. Additionally, all the materials of the PARTHENOS eHumanities and eHeritage Webinar Series (Drenth and Wuttke 2018) can be accessed via the PARTHENOS Training Suite for reuse.   Guiding Principles and Modes of Delivery The educational resources produced by PARTHENOS address two levels of user needs: the ‘need to know about’ (awareness-raising) and the need to know how’ (skills building) and are presented online mainly via the PARTHENOS Training Suite Website, a WordPress-based eLearning platform. In contrast to print-based learning and teaching resources, the modules of the PARTHENOS Training Suite make use of up-to-date means of communicating knowledge and information, and technologies, allowing multimedia content and multimodal forms of acquisition and delivery of knowledge and skills by learners and teachers. Within the modules the range of materials consists of video-lectures, interviews, short (animated) explanatory clips, presentation slides, exercises and explanations of basic principles, case studies, collections of further links and reading suggestions, and brochures. The Training Suite consists of several modules that allow flexible access. The users are guided through the modules in a linear fashion for better orientation, but can also access specific points of interest via the side navigation menu. Through this they have full control over the learning process and can chose which materials they want to explore in more detail as well as different modes of mediation. PARTHENOS provides its training materials as Open Educational Resources (OER). Thus, it takes up one of the foundational principles of the culture of the Digital Humanities that is based on sharing and reuse. It also adheres to the principles of co-creation during the development process, another principle of the Digital Humanities (“collaboration as creation”, Burdick et al. 2016, p. 84). There is an ongoing process of exchange with the PARTHENOS partners and beyond about the further development of the training materials. This intensive exchange aims to ensure the uptake of new developments in the ever-changing complex world of digital infrastructures and eResearch methods and tools in the Humanities and Cultural Heritage field. It ensures the direct integration of research communities into the development process (bottom-up approach) and enhances the innovation potential and quality.  PARTHENOS partners not only to develop materials, but also to facilitate their uptake, either by inclusion into other educational contexts (training modules devised by others, incorporation into in situ and virtual training events, and lately a try out at an HEI), but also by using our partners’ networks to spread the news about available new resources. For PARTHENOS training is an ideal means of communicating project outcomes and to reach its disciplinary and geographically diverse communities. It is thus closely connected to the project’s outreach and dissemination activities. For example, to extend the reach of the PARTHENOS Training materials, a short movie featuring two aliens who talk about standards (promoting the Standardization Survival Kit developed by PARTHENOS), taking a rather light stance to a seemingly difficult topic, and reflecting the linguistic diversity of the potential users by voices in different languages, has been developed. In this context, PARTHENOS has also launched in 2018 the highly successful “PARTHENOS eHumanities and eHeritage Webinar Series” (Drenth and Wuttke 2018). In this series of online seminars, international experts from PARTHENOS and beyond took the learners on a journey along the research life cycle, highlighting how using and contributing to eHumanities and eHeritage research infrastructures empowers research(ers) (Wuttke 2019).     Outline and Proposal for a Long Paper In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources.        Acknowledgements: PARTHENOS Training is a cross PARTHENOS effort and includes input from external experts. We especially express our gratitude to past and present members of PARTHENOS Training that are not authors of this abstract: Elizabeth Burr, Stefanie Läpke, Rebecca Sierig (all University of Leipzig), Helen Goulis (Academy of Athens), Jenny Oltersdorf (University of Applied Sciences Potsdam).  ",
        "article_title": "The PARTHENOS Training Suite: Empowering eHumanities and eHeritage Research(ers) with essential Knowledge and Skills",
        "authors": [
            {
                "given": "Ulrike",
                "family": "Wuttke",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Heike",
                "family": "Neuroth",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Rothfritz",
                "family": "Laura",
                "affiliation": [
                    {
                        "original_name": "University of Applied Sciences Potsdam, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Edmond",
                "family": "Jennifer",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Garnett",
                "family": "Vicky",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Uiterwaal",
                "family": "Frank",
                "affiliation": [
                    {
                        "original_name": "NIOD Institute for War, Holocaust and Genocide Studies",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Annisius",
                "family": "Marie",
                "affiliation": [
                    {
                        "original_name": "University of Leipzig",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-23",
        "keywords": [
            "digital humanities (history",
            "digital research infrastructures and virtual research environments",
            "libraries",
            "museums",
            "theory and methodology)",
            "English",
            "pedagogy",
            "GLAM: galleries",
            "archives",
            "teaching",
            "and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper studies the graphs which are created when a narrative's fabula (the timeline of events within its story) is plotted along the vertical axis of a coordinate plane and its syuzhet (the order in which these events are presented to readers) is plotted along the horizontal axis. Such graphs were recently dubbed \"time maps\" by William Nelles and Linda Williams, and though time maps offer a new means for the study of narrative, they have received minimal scholarly attention. To remedy this, I survey the relevant theoretical landscape, outline a methodology for generating time maps, and theorize on the myriad forms they may take. Time maps are particularly relevant for temporal narratologists because they visualize the ordering framework Gérard Genette developed in  Narrative Discourse. But instead of simply tracking the order of a scene's position in the fabula, as Genette and others have done, I incorporate numerical values from the text to create time maps of unrivaled accuracy.  Theory Many contemporary studies visualize literary complexity.  Their most direct forerunner is Kurt Vonnegut's framework for graphing the emotional arcs of stories. Vonnegut also plots syuzhet along the horizontal axis, but plots emotional valence along the vertical axis. Vonnegut created qualitative sketches that captured the essential shapes of several stories and predicted that most narratives could be described by a few patterns. His work inspired independent studies by Matthew Jockers and Andrew Reagan which implement automated machine learning techniques to quantitatively construct this same structure for a large number of texts. In sync with Vonnegut's prediction, Reagan's group found that six basic shapes account for the emotional arcs of approximately 95% of the texts in Project Gutenberg.   Franco Moretti and the Stanford Literary Lab have also generated a number of papers in this vein. I highlight one, written by Maria Kanatova's group. They studied patterns of anachrony in recent mystery films to conclude that a new subgenre has emerged, which features heavy use of analeptic flashbacks that are external to the fabula's principal reach. Complementing these studies of time, last year's Digital Humanities conference also featured a number of projects which digitally documented the spatial worlds within texts. Of particular note is Randa El Khatib's survey of  Paradise Lost's moral geography, as well as the anotational mapping that Gimena del Rio Riande, Romina De León, Nidia Hernández, and Leif Isaksen have done with Recogito 2.  Other researchers have more directly studied time maps. Perhaps most famously, Steve Aprahamian published a time map of  Memento to the film's Wikipedia page. Daniel Aureliano Newman created a time map of Aldous Huxley's  Eyeless in Gaza, using it to interpret the novel in terms of the biological concept of neoteny. Elisa Pezzotta created similar graphs for several science fiction films featuring time-travel. David Wittenberg has also performed a detailed study of time travel narratives which included a time map of Heinlein's \"By His Bootstraps.\" The most comprehensive study of time maps to date, though, was the aforementioned analysis by Nelles and Williams. Yet their work is still limited in scope, merely concluding that narratives are inherently anachronous. My findings are in accordance with Nelles and Williams, yet I reach further. I formulate a general theory of time maps, outline a systematic methodology for creating them, and use them to perform interpretive work.  Methodology Genette's scheme for organizing the scenes of disordered stories is a precursor to my work. He assigns each scene a letter, corresponding to its placement in the syuzhet, and a number, corresponding to its order in the fabula. The key difference is that I represent each scene as a line segment, instead of a point. This adds many layers of difficulty to data collection, since each scene now requires four pieces of information instead of two, but also allows for distinguishing scenes of varying durations.  The n-th line segment has its origin at   (s ni , f ni)   where s ni  denotes the scene's initial location in the syuzhet and f ni  denotes the scene's initial location in the fabula. The terminus of this line segment is at   (s nf , f nf )   where s nf  denotes the scene's final location in the syuzhet and f nf  denotes the scene's final location in the fabula. A narrative's time map is simply the complete set of these line segments and, consequently, the study of time maps is a geometric approach to narratology.   Each line segment will have its own values for s ni , f ni , s nf , and f nf . In my data sets, each scene corresponds to a row containing all of these values, along with secondary columns which track the temporal cues essential to the compilation process. The works in my corpus were chosen because they contain a large number of temporal cues. My data sets for works such as  Infinite Jest ,  A Visit from the Goon Squad , and  House of Leaves  are several hundred lines each. Machine learning techniques may eventually be implemented to collect this information, but entry currently requires a large amount of human input due to the fluid nature of fabula timeflow in narrative and the myriad ways it may be represented.  Analysis  I begin my analysis by examining the time map of  Rashomon , which is shown in Figure 1. This film is particularly interesting because it simultaneously demonstrates two kinds of parallel storytelling. The first features three storylines being simultaneously told in the syuzhet, though they are nested in the fabula. These storylines manifest as three groups of line segments vertically displaced from one another: the red corresponds to the framing narrative of the monk and the woodcutter stranded on a rainy day; the green corresponds to their recollections of a trial; and the remaining colors sync up with the witness's accounts of the crime. The second form of parallel storytelling in  Rashomon  is what Genette called repetition, which occurs when a single scene is revisited. Repetition is central to  Rashomon  because each witness recalls the same crime, but narrates the events differently. This film highlights the capacity for time maps to track narratological complexity for viewers. It also serves as a theoretical model: stories with multiple timelines will be vertically offset from one another, while those with repeated events will contain line segments which are horizontally offset.   Hitchcock’s  Rope , portrayed in Figure 2, is a perfectly linear narrative and stands in contrast to  Mrs. Dalloway , which is approximated in Figure 3 by plotting the singular tolls of Big Ben that occur throughout the novel. The latter's time map is nearly linear, but thrown off by Woolf's extended discussion of the characters' actions at lunchtime. This demonstrates a key difference between the camera and the written word: cameras can easily produce purely linear stories through the use of real-time filming, but this is extremely difficult to achieve in writing, due to the slippery nature of language.   Figure 4 displays the time map for  Citizen Kane , which is very nearly a triangle. Its sides are the vertical green dots on the left, the horizontal black line on top, and the multicolored line segments below. These respectively correspond to the opening newsreel, the reporter's interviews, and the events of Kane's life as recounted by the interviewees. The film notably features several narrators, whose spans of syuzhet are demarcated by vertical gray lines. Yet because these narrators recall Kane's life in almost exactly the order it unfolded, they produce a hypotenuse which is very nearly linear. As a result, almost every scene from  Citizen Kane  lies along the perimeter of this triangle, with one glaring exception: when Kane is forced to sell controlling interest of the  Inquirer  to Thatcher. This loss quite literally lies at the center of the film — and, thus, the viewer's understanding of Kane's life — albeit in a complex way which can only be recognized by plotting its time map. The interpretive inroads multiply when the corners of the triangle are taken into consideration. The film opens in the top left corner with Kane's last word, the bottom left corner depicts an early memory of him sledding as a child, and the top right corner shows that same sled being thrown into a furnace. Just as a triangle is geometrically defined by its three endpoints, Kane's life — or at least the audience's view of it — is defined by Rosebud. Inverting the trope of having an \"inner child,\" Kane's core is failed ambition which is padded by memories of simpler times.  Conclusion  In short, time maps reveal many types of complexity, ranging from interpreting individual texts to formulating narratological theory. In addition to the works discussed in this abstract, I have created and studied many others. Their elaborate structures require more space to unpack than is available here. In the future, natural language processing may be used to algorithmically generate time maps, but my experiences unpacking the ambiguities of temporal language lead me to believe that this would be a herculean undertaking.      Figure 1: Time map of  Rashomon      Figure 2: Time map of  Rope      Figure 3: Time map of  Mrs. Dalloway      Figure 4: Time map of  Citizen Kane  ",
        "article_title": "Visualizing the Temporal Space of Narratives",
        "authors": [
            {
                "given": "Sean A.",
                "family": "Yeager",
                "affiliation": []
            }
        ],
        "publisher": null,
        "date": "2019-04-30",
        "keywords": [
            "digital humanities (history",
            "corpus and text analysis",
            "modeling and visualization",
            "spatial & spatio-temporal analysis",
            "data mining / text mining",
            "English",
            "theory and methodology)",
            "text encoding and markup languages",
            "literary studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Much has been, and is, made of the transformative potential of digital resources and ‘data’ for humanities’ and historical research. Historians are flooded with digital and digitized materials and tend to take them for granted, grateful for the opportunities they afford. As the late Roy Rosenzweig observed in 2003, historians “may be facing a fundamental paradigm shift from a culture of scarcity to a culture of abundance” (Rosenzweig, 2003: 739). Yet, if we accept that we do indeed live in a culture of abundance, that abundance is still rarely questioned and qualified, let alone contextualized in time and space. To put it simply: the question of why, where and how we can access what we can access is rarely posed. Digitization, however, is far from neutral. In an academic world that increasingly privileges what is online, where “analogue” archives are sometimes even referred to as “hidden archives”, we need to start imagining what a world of historical scholarship based upon digital resources looks like. The online documentary record affects historical research and we need to understand how and in what ways our online evidentiary basis is constituted, a question that goes without saying for historians when dealing with “analogue” archives but is often ignored when digital resources are used. Indeed, there is a marked discrepancy between the use of digital resources by many historians and their lack of interest in how these are created and constituted.  Digitization first and foremost means selection. Archives, libraries, museums and other heritage institutions select materials to be digitized on the basis of a variety of criteria such as the preservation of fragile materials, easy access to collection highlights and/or often-used material, and the research value of certain collections ( Economou, 2015). Legal and ethical questions can play a role too and, given the costs involved, the availability of funding, public or private, plays a key role in enabling digitization projects in the first place. Funding is not only influenced by the aforementioned criteria and concerns but also by memory politics and the way in which a given country’s or group’s past, or aspects thereof, resonate in public discourses and debates.    That is, if funding is available at all. Retro-digitization of heritage can be a luxury that many countries cannot afford. In this respect Barringer et al. have noted the “political and economic inequality between North and South, which has shaped not only the form and content of digital libraries, but also access on the continent to material about the continent”(Barringer et al., 2014). The point is well demonstrated by contrasting the European Union’s policies of promoting and financing digital cultural heritage   Taking as a starting point the Commission Recommendation of 27 October 2011 on the digitization and online accessibility of cultural material and digital preservation (2011/711/EU): eur- lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2011:283:0039:0045:EN:PDF . See also: ec.europa.eu/digital-single-market/en/policies/digital-cultural-heritage .    with the more limited resources that states on the African continent have, where privately funded endeavors such as the British Library’s Endangered Archives program acquire a relatively more important role (Kominko, 2015).   See:  eap.bl.uk/.     In view of all this, the question becomes: what are the politics of digitization and what are its implications for historical research? Is the often-lauded democratizing potential of digitization also offset by risks, inherent in selection processes that might privilege the digitization of heritage corresponding to existing national master narratives, the availability of funding and/or the priorities set by certain research agendas? How does transnational heritage fit into this picture when most digitization projects are, in one way or another, nationally framed? And how does all of this play out globally?  When answering these questions, two things need to be taken into account. The first is the broader historical and technological context of heritage preservation and its politics. For instance, there are remarkable parallels to be drawn between the era of microfilming, as a new means of reproduction, and the current phase of digitization, from processes of selection, metadating practices and questions of accessibility, to the reasons for preserving analogue materials and utopian claims as to   how research practices could, and would, change. Unlike the current era of digitization, however, the microfilm never succeeded in penetrating and saturating scholarly practices to the extent that   digitization does.   The second is the role of the nation in processes of digitization and selection. As in the era of the microfilm, the role of the nation matters, and it matters a lot, contrary to some earlier scholarly predictions (Putnam, 2016).  This is not to suggest that digitization, especially when it comes to heuristics, cannot  facilitate transnational history approaches, as recently argued by (Putnam, 2016).   Around 15 years ago, for instance, Rosenzweig rhetorically asked: “If national archives were part of the projects of state-building and nationalism, then why should states support post- national digital archives?” (Rosenweig, 2003: 752). Yet, even a cursory glance at the “about” pages and mission statements of many digital libraries and archives demonstrates that national concerns have far from disappeared when it comes to efforts to digitize the past. The existence of a supra-national digital resource like Europeana might seem to contradict this point, but it is very clear that part of Europeana’s mission is to promote and create a sense of common European heritage (e.g. Valtysson, 2012).  This paper will present the first findings of an ongoing research project. It is about the digital resources we work with as historians; in the first place with regard to what is being digitized, the sources and data, and to a lesser degree the metadata, notwithstanding the latter’s profound political aspects and effects, e.g. with regard to access (see, for instance: Fernandez,2018). To avoid misunderstandings: it is not about the politics of digital humanities more broadly conceived; e.g. about addressing claims that it is incumbent upon DH to fulfil a political mission or become a more self-critical discipline by consciously investigating its own gender, diversity and other biases (e.g. Liu, 2012). The paper explores the question of the politics of digitization by focusing on one specific dimension: the question of digitization and selection, and its implications for historical research, by using the example of the digitization of Jewish heritage. It combines a theoretical, critical-reflexive approach with concrete examples and is structured as follows:   Introducing the politics of digitization: why is this an important question? What politics are we speaking about? And how does this affect historical research? Scholarly context: A very short overview of the state of the art in DH literature, with a particular focus on what insights can be drawn from debates in the fields of heritage studies, information, archival and library science. Spatial context: How can we conceive of digitization within a global context? As an example, I will briefly discuss the state of digitization of European versus African cultural heritage and pinpoint the major issues involved.   For Europe, see the eNumerate reports on the state of cultural heritage digitization available at:    www.egmus.eu/ .  Statistical information on Africa has to be compiled from various sources. For a general discussion of digitization and the nation in relation to Africa see (Tanner and Kahn, 2014).    Analysis of the political dimensions of digitization of Jewish heritage and selection criteria for digital preservation:   As mentioned, this paper derives from a bigger project with deals with various dimensions and contexts   relevant to the politics of digitisation, not only the issue of selection. As for the latter, this main project will provide a systematic overview of how digital libraries/archives in a number of countries advertise themselves online and what selection criteria for digital preservation they use, which will be contextualised with information and data from these countries from, inter alia , the Compendium of Cultural Policies and Trends . For the purposes of the paper, I will outline this approach and then provide a couple of brief examples. The Compendium of Cultural Policies and Trends is “a web-based and permanently updated information and monitoring system of national cultural policies and related development”. See:   www.culturalpolicies.net/web/compendium.php .     Concluding remarks on the politics of digitization and its implications for historical research: Outlook: what can we learn from the examples provided, and how should we conceive of this question  within a global context?   ",
        "article_title": "Digital History and the Politics of Digitization",
        "authors": [
            {
                "given": "Gerben Zaagsma",
                "family": "Zaagsma",
                "affiliation": [
                    {
                        "original_name": "Luxembourg Centre for Contemporary and Digital History (C²DH), Luxembourg",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2014-12-19",
        "keywords": [
            "digital humanities (history",
            "methods and technologies",
            "sustainability and preservation",
            "theory and methodology)",
            "English",
            "globalization & digital divides",
            "digital archives and digital libraries",
            "cultural artifacts digitisation - theory",
            "history and historiography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Considering the stunning advancements in communication technology, some may find it hard to imagine that at the outset of the digital age 50% (Harrison, 2007) to 90% (Kraus, 1992) of languages are at risk of extinction this century.  This dire forecast relates to a complex amalgamation of factors including the history of colonization (Mufwene, 2002; Osborn, Anderson, and Kodama 2009), oppression of minority and Indigenous communities, migration, innovations in global transport, international media consolidation, and urbanization (Simons & Lewis, 2013).  Digital technologies also appear to be contributing to a pattern of mass language extinction.  This may be particularly surprising considering the rhetoric around digital technologies’ potential to connect the world and democratize access to information and communication.  When the diversity of human languages and their written scripts are considered, the challenge of achieving these aspirations without undermining language and script diversity comes into focus.  While there exists a potential for digital technologies to support linguistic diversity in all its richness, this trajectory is by no means fated.  This paper addresses key factors and efforts required to achieve such a future as well as the roadblocks that may prevent many languages from surviving the digital age.    “Digital extinction” will be the fate, according to Rehm (2014), of languages that suffer from insufficient technology support. This is a three-pronged process. First, if digital technologies make the use of a language impossible or inconvenient, a language community will suffer a loss of “function” as other languages take over tasks such as email, texting, search, e-commerce, etc. Second is a simultaneous loss of “prestige” associated with the absence of a language within the high-prestige technological realm. Digital technologies were first developed in English-speaking contexts and spread internationally during a period of increasing English dominance after the Cold War; thus, English has gained prestige in the digital sphere while the prestige of many other languages has declined. The third prong of digital extinction is the loss of “competence” which occurs as it becomes increasingly difficult to raise a “digital native” competent in the language. With digital technologies playing an increasing role in human communication, the lack of digital use of a language is likely to become an increasingly central factor in language extinction more generally. Considering that Kornai (2013) predicts that less than 5% of languages will achieve full vitality in the digital realm, the scope of this concern becomes apparent.   Ultimately, what does it matter if global communication is achieved at the expense of language diversity?  Language is interconnected with identity, culture, and an intergenerational sense of belonging (Harrison, 2007).  Therefore, the users of under-resourced languages clearly stand to lose the most from digital extinction.  However, the loss of language diversity impacts us all.  Language is tied up with human knowledge, and when we lose one we lose the other (Harrison 2007; Evans 2010).  Harrison (2007) argues that “the extinction of ideas we now face has no parallel in human history” (p. viii), while Hale states that losing a language is like “dropping a bomb on a museum, the Louvre” (cited by Harrison 2007, p. 7).  While the digital age promises us global access to a grand storehouse of knowledge, we may lose more than we gain if digital extinction goes unchecked.    Technological support for full digital viability of a language consists of a variety of factors.  Foundational are digital standards, or the “protocols” that determine which languages’ written scripts are supported by digital devices, software, and platforms.  As such, digital standards play a key role in determining which languages are included or excluded from digital communication (DeNardis, 2014).  While more and more languages are supported by foundational standards, language “inclusiveness” has primarily taken place through the process of technology companies targeting new, profitable markets of language users.  Left behind are language communities too small or too poor to be considered viable target markets.    The central research questions are: How is language diversity threatened or bolstered by digital communication technologies? What can be done to ensure languages survive the digital age? I identify the primary hurdles in terms of technological design and governance of digital technologies that disadvantage minority and Indigenous languages. Last, I identify best practices for how these barriers can be overcome, including policy best practices for digital governance institutions to meet the needs of digitally-disadvantaged language communities.  The impact of digital technologies on language diversity has been radically understudied because of the complex and cross-disciplinary nature of the research area.  While linguists and anthropologists study language shifts, they typically lack the technical expertise to understand how digital design and governance impact language choices in the digital sphere.  Similarly, tech designers and computer scientists frequently lack awareness of the implications of their work on language diversity.  This interdisciplinary research responds to the conference’s call to build complex models of complex realities, analyze them with computational methods, and communicate the results to a broader public.     I utilize an instrumental case study method, suited to exploratory and cross-disciplinary research.  I focus on the case of the digitization and standardization of the Ethiopic script culminating in its inclusion in the dominant character encoding standard Unicode as well as ISO/IEC 10646, which is kept in-synch with Unicode.  I also consider other forms of digital support developed for the Ethiopic script and its languages, including Ethiopia’s national language Amharic.  The case of Ethiopic is uniquely informative in that it was the first indigenous African script to be included in Unicode and ISO/IEC 10646.  This case also demonstrates many common challenges that affect other digitally-disadvantaged languages and scripts, such as Mongolian and N’Ko (Rosenberg, 2011).   Research methods include in-depth qualitative interviews with key actors, including Ethiopic digital pioneers who built early word-processing programs, keyboards, and other basic digital supports for the script and its languages. I also interviewed members of the Unicode Technical Committee and the ISO’s Subcommittee 2/Working Group 2 on character encoding, observed their ongoing work to support digitally-disadvantaged scripts and languages, and did research in their archives about inclusion of the Ethiopic script in their standards.  Furthermore, in order to determine to what extent foundational supports for Ethiopic have allowed for use of the script and its languages in the digital sphere, I conducted a non-traditional content analysis of comments on popular Ethiopian-themed Facebook pages. While rates of utilization of the Ethiopic script have seen increases over the last decade, there are still more Amharic comments written in the Latin script than those written in Ethiopic. This indicates ongoing barriers in support for the script, since transliteration of Amharic into Latin is uncommon outside of the digital sphere.  Last, I chronicle ongoing work, for which I am a participant-observer, to develop layout and formatting rules for Amharic in partnership with the digital governance institution World Wide Web Consortium (W3C). These rules will allow software and platforms to accurately support the unique characteristics of the Ethiopic script and Amharic language. Challenges include collecting Ethiopic publishing expertise from stakeholders unfamiliar with the W3C, some of whom are not online. The “instrumental” nature of the Ethiopic case means that throughout I situate this history in the context of larger trends affecting digitally-disadvantaged scripts and languages as a group.   Despite concerning trends in terms of digital support for language diversity, the digital age is still young.  The digital technologies we use today were shaped by forces in the recent past, but these are all subject to change.  People write code.  And as Russell (2014), DeNardis (2009), and Osborn (2010) assert, we have an opportunity and a responsibility to shape our technologies to support the future we wish to inhabit.  If we wish to preserve and revitalize the diversity of human language, and the wealth of knowledge it contains, digital technologies can help us do that (Yacob 2014; H. B. Russell 2010).  If we, and particularly those who design and govern digital technologies, prefer to “let the market decide,” digital technologies will contribute heavily to global linguistic homogenization and the mass extinction of minority and Indigenous languages.    Recommendations include a focus on support for language diversity as the “corporate social responsibility” of global tech giants, an issue of accessibility and equity. This can be promoted by advocates reaching out to companies, voicing needs and connecting them with language expertise as necessary. Digital governance organizations should lower barriers of entry for digitally-disadvantaged language communities to participate and voice their concerns. This may include working with trusted third-party intermediaries, such as the Script Encoding Initiative, which bridges script communities’ needs with the technical requirements of Unicode proposals. Governments can support academia to build digital tools for non-market languages, as well as purchasing digital tools that support national and local languages, creating market incentives to develop them. Collaborations between linguists and technologists are also essential. This presentation will present how we can shape the future of language diversity by closing the linguistic digital divide through advocacy, digital design, and governance of the digital sphere.  ",
        "article_title": "Global Language Justice in the Digital Sphere: The Ethiopic Case",
        "authors": [
            {
                "given": "Isabelle Alice",
                "family": "Zaugg",
                "affiliation": [
                    {
                        "original_name": "Institute for Comparative Literature and Society, Columbia University, United States of America; Academy of Ethiopian Languages and Cultures, Addis Ababa University, Ethiopia",
                        "normalized_name": "Columbia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hj8s172",
                            "GRID": "grid.21729.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-05-04",
        "keywords": [
            "content analysis",
            "linguistics",
            "communication and media studies",
            "English",
            "globalization & digital divides",
            "diversity",
            "public humanities and community engaged scholarship"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This short paper will examine notions of, and practices in relation to, digital data in museums. On the one hand, it will discuss how digital data are conceptualized, identified, and considered in museums; on the other hand, it will explore how they are produced, collected, curated, shared, and preserved within the heritage sector. Digital data have now a liminal position in museums, where they are increasingly being recognized as part of 21 st century heritage. Despite the UNESCO  Charter on the Preservation of Digital Heritage dating to 2003, and a CIDOC Digital Preservation Working Group active since 2006, museums have begun to recognise the value of preserving digital data as sources and representations of contemporary heritage only in recent years. For example, MoMA announced its first collection of videogames in 2012; at the Museum of London, in 2017, the installation  Pulse tracked social media and displayed live what Londoners were tweeting; in 2018, a major exhibition on videogames was held also at the Victoria and Albert Museum in London; and, outside of the museum sector, the Daily Show programme (on Comedy Central) organised the pop up  The Donald J. Trump Presidential Twitter Library, which has now visited a few North American cities and it is the subject of a virtual museum. However, this recent attention for digital data as potential museum objects implies also a reconsideration of their position within museum taxonomies, where in the past they have mostly been treated as auxiliary information and interpretative support.  In addition, user generated content related to the museum and its collections, and generated by visitors during museum visits and by online audiences on social media, will be considered also for its potential in expanding object biographies (Kopytoff, 1986; Hill, 2012). Previous research has focused on participatory practices and digital engagement (Simon, 2010; Adair et al., 2011; Giaccardi, 2012; Kidd, 2014) and the evaluation of digital programmes (Villaespesa, 2016), although there are not yet clear frameworks and methodologies for harvesting, managing, and using this data (Marstine, 2011; Kidd and Cardiff, 2017). Less work has been done on the exploration of objects online biographies, as emerging from the museum digitisation practices and its collection management systems, and continuing through social media photos and digital engagement material, and this paper will highlight some of the emerging challenges. While the inclusion of digital data in collections might prompt a redefinition of the values and position of digital heritage in a museum and might provoke new questions on the digital lives of museum objects, this data causes also a series of curatorial and methodological challenges. Firstly, this data will come in different forms, each one presenting different technical challenges in relation to their collection, archiving, and representation. Secondly, it might even be part of assemblages of digital and analogue items which, together, represent the heritage of 21st century events (e.g. a political protest leaves behind social media posts as well as placards). The acquisition of both physical and digital material within a museum system poses a series of additional challenges to its ontologies and vocabularies, which are not prepared to acquire born digital materials in the same digital infrastructure as that of more ‘traditional’ museum objects. Throughout the discussion, the paper will observe how museum approaches to digital collecting and to digital preservation diverge or complement existing practices in parallel fields. The idea that history and heritage are now increasingly shared and produced online, and thus we ought to preserve digital outputs and research the circulation of news, opinions, and debates in the digital sphere has a longer story in the field of digital humanities, and web archiving in particular (Rosenzweig, 2004; Graham et al., 2015; Brügger, 2013, 2017; Giaccardi and Plate, 2016; Winters, 2017). Hence, the paper will observe how discussions and projects in the digital humanities field could be productively inspire new forms of curating in museums, in order to improve practices in relation to the acquisition, recording, management, and preservation of this data. Similarly, the field of digital ethnography (Hine, 2008; Kozinets, 2011; Pink et al., 2016) and research in social sciences have emphasised the values of collecting, analysing, and eventually preserving our online lives. For the purposes of this paper, it is particularly the ethical discussions around the collection of contemporary data which are of interest. In conclusion, the paper will focus on the one hand on questions on the collection, management, and use of digital data, which are increasingly crucial for future museum curating practices. On the other hand, the paper will discuss how the repositioning of digital data as heritage raises new questions in relation to the materiality and authenticity of the ‘digital’, to the politics and impact of co-production and knowledge creation online, to the management of digitised and born-digital content, and to the ethics of collecting digital data and its consequent implications. ",
        "article_title": "Data in Museums: Digital Practices and Contemporary Heritage",
        "authors": [
            {
                "given": "Chiara",
                "family": "Zuanni",
                "affiliation": [
                    {
                        "original_name": "University of Graz, Austria",
                        "normalized_name": "University of Graz",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/01faaaf77",
                            "GRID": "grid.5110.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2019-04-25",
        "keywords": [
            "social media",
            "libraries",
            "museums",
            "interdisciplinary & community collaboration",
            "English",
            "library & information science",
            "GLAM: galleries",
            "archives",
            "cultural studies",
            "public and oral history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]