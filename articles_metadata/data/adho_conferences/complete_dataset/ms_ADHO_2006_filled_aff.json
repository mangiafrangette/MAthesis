[
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. Introduction In this paper, we describe the approaches taken by two teams of researchers to the identification of spelling variants. Each team is working on a different language (English and German) but both are using historical texts from much the same time period (17th – 19th century). The approaches differ in a number of other respects, for example we can draw a distinction between two types of context rules: in the German system, context rules operate at the level of individual letters and represent constraints on candidate letter replacements or n-graphs; in the English system, contextual rules operate at the level of words and provide clues to detect real-word spelling variants i.e. ‘then’ used instead of ‘than’. However, we noticed an overlap between the types of issues that we need to address for both English and German and also a similarity between the letter replacement patterns found in the two languages. The aims of the research described in this paper are to compare manual and automatic techniques for the development of letter replacement heuristics in German and English, to determine the overlap between the heuristics and depending on the extent of the overlap, to assess whether it is possible to develop a generic spelling detection tool for Indo-European languages (of which English and German are examples). As a starting point, we have manually-built letter replacement rules for English and German. We will compare these as a means of highlighting the similarity between them. We will describe machine learning approaches developed by the German team and apply them to manually-derived ‘historical variant’-‘modern equivalent’ pairs (derived from existing corpora of English and German) to determine whether we can derive similar letter replacement heuristics. Using the manually-derived heuristics as a gold-standard we will evaluate the automatically derived rules. Our prediction is that if the technique works in both languages, it would suggest that we are able to develop generic letter-replacement heuristics for the identification of historical variants for Indo-European languages. 2. German spelling variation The interdisciplinary project “Rule based search in text databases with non-standard orthography” which is funded by the Deutsche Forschungsgemeinschaft [German Research Foundation] developed a rule-based fuzzy search-engine for historical texts (Pilz et al. 2005). Its aim of RSNSR is to provide means to perform reliable full text-search in documents written prior to the German unification of orthography in 1901. On basis of around 4,000 manually collected one-to-one word mappings between non-standard and modern spellings, RSNSR follows three different paths to come up with an efficient rule set. Those are manual rule derivation, trained string edit distance and automatic rule learning. Additional mappings will be collected to further enhance the quality of those approaches. The manual derivation uses an alphabet of 62 different sequences, in parts historical n-graphs (e.g. <a>, <äu>, <eau>), built from combinations of the 30 standard graphemes of the German language. Being built manually, the alphabet considers linguistic restraints. Neither in context nor at the position of substitution non-lingual n-graphs (i.e. grapheme sequences that directly correspond to phonemes) are allowed. The context may also feature regular expressions using the java.util.regex formalism. The manually derived gold standard features the most elaborate rules. However the design of a rule set for the period from 1803 to 1806, based on only 338 evidences took about three days to create. Furthermore, the manual derivation is prone to human-error. This is especially true as soon as the rule set exceeds certain limits where side effects become more and more likely. The algorithm used to calculate the edit costs was proposed in 1975 by Bahl and Jelinek and taken up 1997 by Ristad and Yianilos who extended the approach by machine learning abilities. The authors applied the algorithm to the problem of learning the pronunciation of words in conversational speech (Ristad and Yianilos 1997). In a comparison between 13 different edit distances, Ristad and Yianilos’ algorithm proofed to be the most efficient one. Its error rate on our list of evidences was 2.6 times lower than the standard Levenshtein distance measure and more than 6.7 times lower than Soundex (Kempken 2005). The automatic generation of transformation rules uses triplets containing the contemporary words, their historic spelling variant and the collection frequency of the spelling variant. First, we compare the two words and determine so called ‘rule cores’. We determine the necessary transformations for each training example and also identify the corresponding context. In a second step, we generate rule candidates that also consider the context information from the contemporary word. Finally, in the third step we select the useful rules by pruning the candidate set with a proprietary extension of the PRISM algorithm (Cendrowska 1987). For this paper, we compared the German gold standard, mentioned above, with the two different machine learning algorithms. The string learning algorithm produces a fixed amount of single letter replacement probabilities. It is not yet possible to gather contextual information. Bi- or tri-graph operations are reflected by subsequent application of letter replacements. Therefore they do not map directly onto the manual rules. However, the four most frequent replacements, excluding identities, correspond to the four most frequently used rules. For the period from 1800 to 1806 these are T→TH, Ä→AE, _→E and E→_. The manual and the automatic derived rules show obvious similarities, too. 12 of the 20 most frequently used rules from the automatic approach are also included in the manually built rules. For six other rules equivalent rules in the manual rule set exist. The rule T→ET from the automatic approach, for example, corresponds to the more generalised form _→E taken from the manual approach. And again do the first four rules match the four most frequent gold standard ones. The automatic approaches, rule generation as well as edit distance, could be enhanced by a manual checking. Nevertheless, even a semi-automatic algorithm allows us to save time and resources. It is furthermore obvious, that the machine learning is already able to provide with a highly capable rule set for historical documents of German language. 3. English spelling variation The existing English system called VARD (VARiant Detector) has three components. First, a list of 45,805 variant forms and their modern equivalents, built by hand. This provides a one-to-one mapping which VARD uses to insert a modern form alongside the historical variant which is preserved using an XML ‘reg’ tag. Secondly, a small set of contextual rules which take the form of templates of words and part-of-speech tags. The templates are applied to find real-word variants such as ‘then’ instead of ‘than’, ‘doe’ instead of ‘do’, ‘bee’ for ‘be’ and detection of the genitive when an apostrophe is missing. The third component consists of manually crafted letter replacement heuristics designed during the collection of the one-to-one mapping table and intended to reduce the manual overhead for detection of unseen variants in new corpora. The rationale behind the VARD tool is to detect and normalise spelling variants to their modern equivalent in running text. This will enable techniques from corpus linguistics to be applied more accurately (Rayson et al., 2005). Techniques such as frequency profiling, concordancing, annotation and collocation extraction will not perform well with multiple variants of each word type in a corpus. The English manual and automatically derived rules show a great deal of similarity. Nine of the twenty most frequent automatically derived rules are in the manual set. Eight other automatically derived rules have equivalents if we ignore context. Three automatically derived rules do not have a match in the manual version. 4. Conclusion The motivation behind the two approaches of VARD and RSNSR differs. This reflects on the overall structure of rules as well. While VARD is used to automatically normalise variants and thus takes more accurate aim to determine the correct modern equivalent, RSNSR focuses on finding and highlighting those historical spellings. Therefore its demands for precision are diminished while recall is the much more important factor. However, the approaches are highly capable of supporting each other and expanding their original field of application.",
       "article_title":"The Identification of Spelling Variants in English and German Historical Texts: Manual or Automatic?",
       "authors":[
          {
             "given":"Dawn",
             "family":"Archer",
             "affiliation":[
                {
                   "original_name":"Department of Humanities,     University of Central Lancashire",
                   "normalized_name":"University of Central Lancashire",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/010jbqd54",
                      "GRID":"grid.7943.9"
                   }
                }
             ]
          },
          {
             "given":"Andrea ",
             "family":"ERNST-GERLACH",
             "affiliation":[
                {
                   "original_name":"Universität Duisburg-Essen",
                   "normalized_name":"University of Duisburg-Essen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04mz5ra38",
                      "GRID":"grid.5718.b"
                   }
                }
             ]
          },
          {
             "given":"Sebastian  ",
             "family":"KEMPKEN",
             "affiliation":[
                {
                   "original_name":"Universität Duisburg-Essen",
                   "normalized_name":"University of Duisburg-Essen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04mz5ra38",
                      "GRID":"grid.5718.b"
                   }
                }
             ]
          },
          {
             "given":"Thomas  ",
             "family":"PILZ",
             "affiliation":[
                {
                   "original_name":"Universität Duisburg-Essen",
                   "normalized_name":"University of Duisburg-Essen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04mz5ra38",
                      "GRID":"grid.5718.b"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"RAYSON",
             "affiliation":[
                {
                   "original_name":"Department of Computing, Lancaster University",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Large digital libraries typically contain collections of heterogeneous resources intended to be delivered to a variety of user communities. A key challenge for these libraries is providing tight integration between resources both within a single collection and across multiple collections. Traditionally, efforts at developing digital collections for the humanities have tended toward two extremes [5]. On one side are huge collections such as the Making of America [14, 15], Project Gutenberg [18], and Christian Classics Ethereal Library [13] that have minimal tagging, annotation or commentary. On the other side are smaller projects that closely resemble traditional approaches to editorial work in which editors carefully work with each page and line providing markup and metadata of extremely high quality and detail, mostly by hand. Projects at this end of the spectrum include the William Blake Archive [21], the Canterbury Tales Project [11], the Rossetti Archive [19], and the Cervantes Project [12]. These extremes force library designers to choose between large collections that provide an impoverished set of services to the collection’s patrons on the one hand and relatively small, resource intensive projects on the other. Often, neither option is feasible. An alternative approach to digital humanities projects recasts the role of the editor to focus on customizing and skillfully applying automated techniques, targeting limited resources for hand coding to those areas of the collection that merit special attention [6]. The Perseus Project [16] exemplifies this class of projects. Elucidating the internal structure of the digital resources by automatically identifying important features (e.g., names, places, dates, key phrases) is a key approach to aid in the development of these “middle ground” projects. Once identified, the internal structure can be used to establish connections between the resources and to inform visualizations. This task is complicated by the heterogeneous nature of digital libraries and the diversity of user community needs. To address this challenge we have developed a framework based approach to developing feature identification systems that allows decisions about details of document representation and features identification to be deferred to domain specific implementations of the framework. These deferred decisions include details of the semantics and syntax of markup, the types of metadata to be attached to documents, the types of features to be identified, the feature identification algorithms to be applied, and the determination of which features are to be indexed. To achieve this generality, we represent a feature identification system as being composed of three layers, as diagramed in Figure 1. The core of the system is a “Feature Identification Framework” (FIF). This framework provides the major structural elements for working with documents, identifying features within documents, and building indices based on the identified features. Implementations customize components of the framework to interface with existing and new collections and to achieve domain specific functionality. Applications then use this framework, along with the appropriate set of customized modules, to implement visualizations, navigational linking strategies, and searching and filtering tools. Figure 1: Three layered approach to designing a feature identification system The document module implements the functionality needed to represent documents, manage storage and retrieval, provide an interface to searching mechanisms and facilitate automatic feature identification. It provides the following features:1. Multiple types of documents (e.g., XML, PDF, RTF, HTML, etc) can be supported without modifying the APIs with which the rest of the system will interact. 2. Arbitrary syntactical constraints can be associated with a document and documents tested to ensure their validity. Notably, this helps to ensure that the markup of identified features does not violate syntactic or semantic constraints. 3. Metadata conforming to arbitrary metadata standards can be attached to documents. 4. Storage and retrieval mechanisms are provided that allow documents persistence to be managed either directly by the framework or by external systems. 5. Large documents can be broken into smaller “chunks” for both indexing and linking. Work in this area is ongoing. The feature module builds on this base to provide the core toolset for identifying the internal structure of documents. Our design of this component reflects the highly contextualized nature of the feature identification task. The relevant features of a document can take many forms (e.g., a person or place, the greeting of a letter, a philosophical concept, or an argument against an idea) depending on both the type of document and the context in which that document is expected to be read. Equally contextualized are the algorithms used to identify features. Dictionary and statistically based methods are prevalent, though other techniques focusing on the semi-structured nature of specific documents have also yielded good results [3, 1, 4, 9, 2, 7]. Ultimately, which algorithm is selected will depend heavily on the choice of the corpus editor. Accordingly, our framework has been designed so that the only necessary property of a feature is that it can be identified within the text of a document and described within the structure provided by the document module. For applications using the framework to effectively access and present the informational content, an indexing system is needed. Given the open ended nature of both document representation and the features to be identified, the indexing tools must inter-operate with the other customized components of the framework. We accomplish this, by utilizing adapters that are implemented while customizing the system. These adapters work with the other customized components to specify the elements of each document to index. To demonstrate and test this framework, we have implemented a prototype for a collection of official records pertaining Miguel de Cervantes Saavedra (1547- 1616) originally assembled by Prof. Kris Sliwa [10]. This collection contains descriptions, summaries, and transcriptions in Spanish of nearly 1700 documents originally written from 1463 to 1681. These documents bear witness to the life of both Cervantes and his family and include inventory lists, birth and death certificates, and court testimonies. Our application provides two primary points of access to the collection; a timeline navigator and a browsing interface. Following Crane, et al. [7], we have utilized proper names (people and places) and time as the two primary dimensions for structuring the documents in this collection. The timeline navigator, shown in Figure 2, displays a bar chart showing the distribution of the documents over time. Selecting a bar takes the reader to a more detailed view of the time period. Once the chart displays documents as single years, clicking on the bar for a single year brings up a display listing all documents from that year. The browsing interface, shown in Figure 3, allows readers to browse lists of both the people and the places identified within the collection. Upon selecting an item to view, a page presenting the resources available for that person or place is displayed. Currently, this includes a list of all documents in which the specified person has appeared and a bar graph of all documents in which that individual has been found as shown in Figure 4. Figure 2: Timeline interface to the Sliwa collection Once the user has selected an individual document to view, through either the timeline or browsing interface, that document is presented with four types of features identified and highlighted. Identified people and places are used to automatically generate navigational links between documents and the pages presenting the resources for the people and places identified within a document. Dates and monetary units are identified and highlighted in the text. One challenge with any framework based system is to ensure that the framework is not so general that customizing it requires more time and effort than writing an equivalent application from scratch. Our experience developing the Sliwa collection prototype suggests that our framework offers significant benefits. With the framework in place, we were able to develop and integrate new features in days; sometimes hours. Moreover, as sophisticated, general purpose features (e.g., pattern matching, grammatical parsers, georeferenced locations) are implemented, it becomes possible to customize and apply these features in new collections via a web-based interface with no additional coding involved. Custom document formats are more complex to implement, but can serve in a wide variety of applications. The current implementation sufficient for most XML formats and work is underway to more fully support TEI encoded documents. Our approach provides strong support for the general components of a feature identification system thereby allowing individual projects to focus on details specific to the needs of particular collections and user communities. We are currently working to apply this framework to a number of other projects, including diaries written during early Spanish expeditions into southern Texas [8], scholarly comments about the life and art of Picasso from the Picasso Project [17], and the Stanford Encyclopedia of Philosophy [20]. This will include further enhancements to the framework itself including support for feature identification that utilizes the structure of the document (including other identified features) in addition to the text and better support for accessing “chunks” within document in addition to the document as a whole. For the long term, we also plan to explore ways in which this framework can be used assist and shape editorial practices.",
       "article_title":"A General Framework for Feature Identification",
       "authors":[
          {
             "given":"Neal",
             "family":"AUDENAERT",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"FURUTA",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Eduardo",
             "family":"URBINA",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. Introduction Even the largest libraries struggle to maintain a comprehensive journal collection. In 2003 Australian universities subscribed to over 1,300,000 journals of which of which 974,000 were in aggregate digital collections. This represented over 273,000 new serial titles and over 150,000 cancellations (Council of Australian University Libraries, 2005). One emerging approach is for libraries to form consortia that take a joint subscription to digital resource collections. The consolidation of substantial collections with direct delivery has seen the gradual attrition of subscriptions to the traditional print format (Fox & Marchionini, 1998; Weiderhold, 1995), but this cost saving is offset by the substantial increase in digital resources. The wealth of international research resources presents an even greater dilemma for small research institutions: how to effectively and economically access such a wide base of information resources within sometimes highly constrained budgets. The cost reductions obtained through aggregate subscriptions and consortia do not necessarily offset the net growth of fee-for-use published resources, and may have the consequence of centralizing subscriptions through a few large distributors – with the long-term collection risk that this centralization presents. Small research libraries that cannot afford participation in national inter-library loan networks have formed fee-free networks of collaborating libraries that share their journal resources. While a fee-free Inter-Library Loan (ILL) service offers obvious attractions to smaller participating libraries, alternative economic approaches are needed to avoid excessive demand on resource-rich members, and to avoid the phenomenon of “free-riders”. This paper presents the resource distribution approaches that have been used to balance resource demand in GratisNet, an Australian network of 250+ health research libraries, where collaboration is fee-free but resource holdings among member are unequal. Dynamic ranking resource-based approaches are used to encourage the equitable distribution of resource load. 2. Economics of demand balancing in a fee-free network Hooke (1999) highlighted the need for evidence -based approaches in the management of information services. In a fee-based environment, the metrics for efficiency may centre of cost versus speed of supply. Fee-free collaboration does not have the same economic driver for equilibrium between demand and resource supply that emerges in the long term in a fee-based service. Furthermore, resource sharing networks operating in a fee-free environment face several risks that are common to voluntary online communities. In Gaming Theory “outcomes” and “payoffs” are differentiated (Shubik, 1975). In the case of ILL collaboration, the payoffs are the supply of particular ILL requests in exchange for the provision of requests raised by other libraries at the risk of absorbing the costs of supplying requests raised by other libraries. The outcomes include access to a wider base of research resources than would otherwise be available to the library, and the potential for requests to exceed loans and constraints on the limit of demands based on membership of a closed community. One of the risks is the “free-rider” phenomenon, or those who take the benefit of membership of a collaborating community but provide no net contribution of resources. “Free-riders” can be managed in a number of ways: through “closed shops” (the example of unions that limit benefits to those who are members only), or through adjustment of the payoffs(Hamburger, 1979). 3. Demand balancing Imbalanced distribution of workload in a fee-free environment, if unmanaged, can create imperfect resource management through inequitable distribution of demand over time. These imperfections may be expressed in terms of a reluctance to declare resources or supply requests (a form of compliance failure), or through inequitable distribution of demand resulting in a delay in supply (a queuing problem). This reduces the payoff potential for the larger members of the network. While there is a risk that libraries may reduce their own collections through reliance on wider networks, the trade-off is the delay in fulfilling requests when they are completed through an ILL rather than directly out of their own collection. The rational choice in fulfilment of an individual ILL request in a fee-free environment is the selection of the nearest library that has the highest probability of fulfilling the request. This provides the best payoff in probability of fulfilment and timeliness of supply. However, aggregated over time this choice is likely to place a larger burden on those participating libraries with the largest collection of resources in a given region. Where staff represents as much as 80% of document supply cost (Morris, 2004), this can be a considerable burden on larger participating libraries. In the GratisNet network, search results for resources held by members of the GratisNet network are inversely ranked based on historical workload contribution. Participating libraries are requested to select from resources in the top-ranked selections presented, but compliance is voluntary. Participating libraries supply ILL requests at no charge to members and with no specific reciprocity. The objective of the ranking process is to adjust the payoff implied by a ration selection of the largest, nearest library by tempering this choice through ranking of search results based on previous workload of participating libraries. Libraries with a higher historical workload are ranked lower in search results. Libraries are encouraged to select from one of the first three listed libraries that have holdings in the journal they are requesting. Table 1 shows the percentage of libraries that by-passed the computer-recommended ranking when raising ILL requests for the years 2002, 2003 and 2004. Since compliance is voluntary, this change demonstrates increasing trust in the workload distribution mechanisms. While participating libraries do exercise a measure of discretion in selecting outside the recommended rankings, voluntary compliance to the ranking recommendations is generally good and has improved over time. Game-theoretic formulations can provide a useful approach to the design of co-operative IT systems(Mahajan, Rodrig, Wetherall, & Zahorjan, 2004). To illustrate the contrast between a time-efficient system for ILL delivery and one which distributes workload across the network, these same transactions were reprocessed under to a game scenario which simulated a rational select on the basis of proximity and breadth of holdings matching the request for the most recent two years. The objective of this scenario was to contrast the aggregate effect of load-based ranking with a utility-based approach to request ulfilment (see Table 2 below). In a time-efficient approach, larger libraries are consistently net providers, reducing their aggregate payoff from participation. Pure Egalitarianism takes the approach over time that yields the highest combined utility to participating libraries. The voluntary element of the ranking yields a “relative egalitarianism” which balances the result that yields utility achieved overall with the lowest level of frustration.(Moulin, 1988). 2003 2004 Transactions 150155 128365 Distribution of transactions to the top 20 largest libraries (demand-balanced) 39003 33212 Distribution of transactions to the top 20 largest libraries (utility-based) 56815 46760 Distribution of transactions to the 20 smallest libraries (demand-balanced) 1454 1154 Distribution of transactions to the 20 smallest libraries (utility-based) 779 754 Table 2 Contrasting load-based ranking to utility-based ranking The risk facing groups collaborating on a fee-free basis is that inequity of resource distribution could result in the resignation of members where their level of “frustration” exceeds the benefit they gain from participation. 4. Conclusion Participating libraries in the GratisNet network commit to supplying ILL requests at no charge and with no specific reciprocity, on the basis that they can be confident that an increase in demand on their library will be balanced progressively with a lower ranking in search results. Transactions for the period 2003 to 2005 are analysed to illustrate the ways in which a ranking-based approach to resource discovery improves workload distribution for participating members overall. Results from the GratisNet network illustrate the effectiveness of formal approaches to resource distribution in fee-free collaborative networks. This analysis also gives an insight into the ways in which service metrics can help in the management of workload in a fee-free environment.",
       "article_title":"Demand Balancing in Fee-free Resource Sharing Community Networks with Unequal Resource Distribution",
       "authors":[
          {
             "given":"Edmund",
             "family":"BALNAVES",
             "affiliation":[
                {
                   "original_name":"University of Sydney     Prosentient Systems Pty Ltd",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The Text Encoding Initiative Guidelines for Electronic Text Encoding and Interchange are a community based standard for text encoding that aim to “apply to texts in any natural language, of any date, in any literary genre or text type, without restriction on form or content.” The basic idea is not to tell scholars, librarians, and other encoders *what* to encode, but rather *how* to encode that which they choose to record. Of course the Guidelines cannot possibly anticipate every feature that users may wish to encode, and therefore the capability to extend the encoding scheme described by the Guidelines in a consistent, easily understood, and interchangeable manner is paramount. Over the past few years the Text Encoding Initiative Consortium (the organization charged with maintaining, developing, promulgating, and promoting the Guidelines) has been working steadily toward a new release of the Guidelines. This much anticipated version, referred to as “P5”, is significantly different from the current Guidelines (“P4”), and yet performs the same basic function of providing a community based standard for encoding literary and linguistic texts. In this presentation, the TEI editors will present P5 as of the latest release, with an emphasis on the ease with which P5 can be customized to particular uses. The talk will start with an overview of what P5 is, what it is good for, and why one would want to use it, and then progress to some of the detailed differences between P4 and P5. Topics addressed will include: * The general goal of the TEI Guidelines - TEI is a community initiative, driven by the needs of its members and users * How work gets done in the TEI - technical council and work groups - open source using Sourceforge - special interest groups * Why do this -- isn’t P4 good enough? - P4 is just P3 in and using XML - a lot has happened since P3 was released, including the creation of the W3C and the acceptance of Unicode - there are arenas P4 does not cover - lots of improvements, repairs, etc. are in order * What’s new and different - infrastructural + schemata + datatypes + classes + customization - attributes used wherever textual content allowed - major updates + manuscript description + character sets & language identification + feature structures (now an ISO standard) + pointing mechanism - less major updates + dictionaries + multiple hierarchies + support for graphics and multimedia + support for stand-off markup - new updates + personography + terminological databases + collation sequences * customization - customizations permit a project to select which parts of the TEI scheme they will use, and to add new bits if needed - in P5, all uses of TEI are customizations of one sort or another - customizations, and the user’s documentation for them, are written in a TEI file - thus customizations themselves can be interchanged or even shared - in theory, a customization can use another customization as its starting point - thus permitting customizations of customizations * The TEI universe - the TEI universe is one where all projects share a common base, but many use additional, local markup constructs - clusters of similar projects can share common subsets of additional markup constructs ",
       "article_title":"TEI P5: What’s in It for Me?",
       "authors":[
          {
             "given":"Syd",
             "family":"BAUMAN",
             "affiliation":[
                {
                   "original_name":"Women Writers Project, Brown University",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          },
          {
             "given":"Lou",
             "family":"BURNARD",
             "affiliation":[
                {
                   "original_name":"Oxford University Computing Services,     Oxford University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"It is traditional, in philology, to date a text by using as a “terminus post quem” the date of first attestation of certain words. The use of a great data base as Frantext makes it possible to envisage the automation of this operation, and consequently its application to all the words of a text, which would increase the precision of the dating. A certain number of precautions (choice of the reference corpus, recognition of the graphic variations...) must be taken to perform this operation, which can be improved by the taking into account, beyond the only date of first attestation, of the frequency of use, throughout the history of the language, of the words of the text to be dated. The dating of the vocabulary of a text also makes it possible to appreciate the degree of archaism and neology implemented by the author, which has applications in stylistics and history of the genres. One will consider also the use of dictionaries of attestations to carry out these operations of dating, dictionaries existing or to be constituted to this end. The examples will relate as well, for validation, to texts whose date is known in literary history and to texts whose dating is the subject of controversies.",
       "article_title":"Méthodes automatiques de datation d’un texte",
       "authors":[
          {
             "given":"Michel",
             "family":"BERNARD",
             "affiliation":[
                {
                   "original_name":"Centre de recherche Hubert de Phalèse.     Université Sorbonne Nouvelle",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"As discussed by McCarty, Beynon and Russ in a session organised at ACH/ALLC 2005, there is a remarkable convergence between McCarty’s concept of ‘model-building in the role of experimental end-maker’ (McCarty 2005:15) - a cornerstone in his vision for humanities computing (HC) - and the principles of Empirical Modelling (EM) (EMweb). More problematic is the tension between the pluralist conception of computing that is an essential ingredient of McCarty’s stance on HC, and the prominent emphasis on ‘dissolving dualities’ in McCarty, Beynon and Russ (ACH/ALLC 2005:138). Resolving this tension transforms the status of HC from one amongst many varieties of computing to that of first amongst equals. The plurality of computing In presenting his “rationale for a computing practice that is of and foras well as in the humanities”, McCarty (2005:14) emphasises the plurality of computing. Following Mahoney (2005), he calls into question the search for “the essential nature of computing” and appeals to history as evidence that ‘what people want computers to do’ and ‘how people design computers to do it’ determine many different computings. The audacity of McCarty’s vision in recommending his readers, as would-be practitioners of a variety of computing, to “[turn] their attention from working out principles to pursuing practice”, is striking. It is hard to imagine a reputable computer science department encouraging its students to see computing primarily in terms of its practice - congenial to the students themselves as this might be. In promoting computing as an academic subject, there is no recognised focus for developing ‘scientific’ principles other than the theory of computation at its historical core (Turing). McCarty instead sets out to characterise the practice of HC in such terms that it has its own integrity. When contemplating McCarty’s boldness, it is instructive to consider the alternatives. The problematic nature of the relationship between computer science and the humanities is notorious. Consider, for instance, Chesher’s observation (ACH/ALLC 2005:39) that - in teaching a course in Arts Informatics: “The Humanities critiques of science and technology (Heidegger, Virilio, Coyne) are difficult to reconcile with scientific conceptions of humanities practices (Holtzman). Each of these areas places quite different, and often clearly conflicting discourses, techniques and systems of value”. From this perspective, seeking to characterise HC as a unified entity seems to be the only plausible strategy, though it resembles conjuring a stable compound from an explosive combination of improbable ingredients. Invoking EM is helpful in critiquing McCarty’s treatment of this unification (2005: 195-8). To elaborate the chemistry metaphor, it illuminates the precise nature of the reaction and identifies it with a more general phenomenon. How modelling and computer science interact in humanities computing The semantic orientation of HC is crucial to understanding its chemistry. Where computer science emphasises prescribing and representing precise intended meanings, humanities is of its essence obliged to engage with meanings that are ambiguous and unintended. The authentic spirit of McCarty’s notion of HC is captured in Ramsay’s ‘placing visualisation in a rhetorical context’ (in his presentation at ACH/ALLC 2005:200) - the creative construction of an artefact as a subject for personal experience, whose interpretation is to be negotiated and potentially shared. This theme is amplified in many topical contributions to the proceedings of ACH/ALLC 20051. Interpreting such activities from an EM perspective obliges a more prominent shift in emphasis from the accepted view of computing than is acknowledged in (McCarty, 2005) - the rich diversity of HC activities cannot be attributed primarily to the versatility of the Turing Machine as a generator of functional relationships2. In EM, the focus is upon the role that observables, dependency and agency play in the modelling activity, and each of these concepts appeals to a personal experience of interaction with technology that defies merely functional characterisation. On this basis, EM trades first and foremost not in objective ‘formal’ interpretations, but in speculative constructions that cannot be realised or mediated without skillful and intelligent human interaction. Appreciation of observables, dependency relationships and potential agency is acquired through developing familiarity and evolving skills. This is in keeping with Polanyi’s account - cited by McCarty (2005:44) - of how awareness is transformed through skill acquisition. Functional abstraction can express transcendental computational relationships, but does not encompass such issues, which relate to what is given to the human interpreter in their immediate experience. A useful parallel may be drawn with musical performance. Though one and the same functional relationship between visual stimulus and tactile response is involved, a virtuoso pianist can perform an extended extract from a complex score in the time it takes a novice to identify the initial chord. In this context, it is significant that - in elaborating his vision for HC, McCarty (2005:53) drew upon his experience of making a specific model - the Onomasticon for Ovid’s Metamorphoses - whose construction and interpretation can be viewed as an EM archetype. Model-building in the Onomasticon, being based on spreadsheet principles, supplies the framework within which McCarty’s experimental ‘end-maker’ role can be played out most effectively. It is implausible that the same qualities can be realised on account of adopting other model-building principles, such as the use of object-orientation, since - in conventional use - their primary purpose is to rationalise the specification of complex functional abstractions. This challenges Galey’s - no doubt pragmatically most sensible! - contention (ACH/ALLC 2005:198) that “In order to bring electronic editing projects like the eNVS to the screen, humanists must think past documents to embrace the principles of object-oriented and standards-compliant programming and design.”. Humanities computing as the archetype for all varieties of computing Though McCarty (2005:14) first discusses plurality in computing in relation to communities of practice quite generally, his interest in a conceptual unification of HC and computer science (2005:195-8) acknowledges the plurality of HC itself. Where McCarty (2005:198) identifies “general-purpose DH.indb 18 6/06/06 10:55:14 DIGITAL HUMANITIES 2006 Single Sessions P. 19 modelling software, such as a spreadsheet or database” as one component within a more diverse unity, Beynon and Russ have a radically different conceptualisation in mind. Their account identifies EM as hybrid part- automated-part-human processing within a framework for generalised computation similar to that implicit in McCarty’s Onomasticon3. Within this framework, the functionality of the Turing Machine is subsumed by closely prescribed and highly automated modes of interaction, whilst modelling with the Onomasticon is a more open-ended human-centred form of processing - though by no means the most general activity of this nature. This places EM at the centre of a broader pragmatic discourse on programming that complements the conventional rational discourse (Beynon, Boyatt and Russ, 2005). The emphasis in (McCarty, Beynon and Russ, 2005) on dissolving dualities within the frame of Radical Empiricism (James, 1996) may still appear to be mismatched to the plurality of HC. Klein’s reaction to EM exemplifies the issues. In seeking a technology to support a world-wide collaborative creative venture4, he recognises the qualities of EM as supporting a concept of creativity that is expressed in the motto: “Build the camera while shooting the film” (cf. Lubart 1996, Klein 2002, METISweb). For Klein, this recognition calls to mind Joas’s concept of creative action, and the processes that shape the evolving meaning of context in Lévy’s ‘universe in perpetual creation’ (1997). The relevance of Radical Empiricism even where such diverse perspectives are being invoked stems from the subject-independent association it establishes between sense-making and the classification of relationships between experiences. For instance, whatever meaningful relationships inform the semiotics of Lévy’s Information Economy Meta Language (2005) should somewhere be ‘experiencable’ (cf. James, 1996:160), and in this manner be amenable to EM. Seen in this light, Radical Empiricism and EM relate to universal learning activities that are orthogonal to the subject of the learning (cf. Beynon and Roe, 2004). This accords with James’s monist view of experience and pluralist view of understanding (James, 1996:194). It is also resonates best with cultures where understanding through relationship has higher priority than objectification. In emphasising interaction and the interpretation of relationships, EM does not prescribe a rigid frame for understanding, but exhibits that positive quality of blandness5 (Jullien, 2004) that affords participation in many relationships. Even within the small community of EM practitioners, this potential for plurality can be seen in different nuances and idioms of elaboration, as in relation to analogue, phenomenological or ecological variants of computing. The aspiration of EM to connect computing decisively with modelling was also that of object-oriented (OO) modelling, as first conceived nearly forty years ago (Birtwistle, Dahl, Myhrhaug and Nygaard, 1982). As a young technology, EM cannot yet compete with OO in tackling technical challenges in HC, such as devising adaptive web interfaces for the ‘end-maker’. Perhaps, unlike OO, it can be more widely adopted and developed without in the process being conscripted to the cause of supporting functional abstraction. If so, it may yet demonstrate that the modelling activity McCarty has identified as characteristic of HC is in fact an integral and fundamental part of every computing practice: that all computings are humanities’ computings. Notes 1. For instance: acknowledging that there is no definitive digital representation (Galey, ACH/ALLC 2005:198); recognising the essential need for interactive playful visualisation (Ramsay, ibid: 200; Wolff, ibid: 273; Durnad and Wardrip-Fruin, ibid: 61); and appreciating the importance of collaborative modelling and role integration (Best et al, ibid: 13; van Zundert and Dalen-Oskam, ibid: 249). 2. For more background, see McCarty (2005) Figure 4.2 and the associated discussion on pages 195-8. 3. The framework alluded to here is that of the Abstract Definitive Machine, as described at (EMweb). 4. The Metis project (METISweb) is exploring collective creativity of global virtual teams of students and professionals in the movie industry. 5. The Chinese ‘dan’, which Jullien translates as ‘fadeur’: Varsano notes that she “would have liked to find an English word that signifies a lack of flavor and that at the same time benefits from the positive connotations supplied by a culture that honors the presence of absence” (see Schroeder 2005).",
       "article_title":"Humanities’ Computings",
       "authors":[
          {
             "given":"Meurig",
             "family":"BEYNON",
             "affiliation":[
                {
                   "original_name":"Computer Science, University of Warwick,     Coventry UK",
                   "normalized_name":"University of Warwick",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01a77tt86",
                      "GRID":"grid.7372.1"
                   }
                }
             ]
          },
          {
             "given":"Roderick",
             "family":"R. KLEIN",
             "affiliation":[
                {
                   "original_name":"Syscom Lab, University of Savoie,     Bourget du Lac, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Steve",
             "family":"RUSS",
             "affiliation":[
                {
                   "original_name":"Computer Science, University of Warwick,     Coventry UK",
                   "normalized_name":"University of Warwick",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01a77tt86",
                      "GRID":"grid.7372.1"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"This paper discusses the applicability of modelling methods originally meant for business applications, on the design of the complex markup vocabularies used for XML Web-content production. We are working on integrating these technologies to create a dynamic and interactive environment for the design of document markup schemes. This paper focuses on the analysis, design and maintenance of XML vocabularies based on UML. It considers the automatic generation of Schemas from a visual UML model of the markup vocabulary, as well as the generation of DTDs and also pieces of software, like input forms. INTRODUCTION Most authors that treated the relationship between UML and XML [5, 7] only targeted business applications and did not consider complex document modelling for massive and systematic production of XML contents for the Web. In a Web publishing project, we need to produce hundreds of XML documents for Web publication. Digital Library XML documents that model the structure of literary texts and include bibliographic information (metadata), plus processing and formatting instructions, are by far much more complex than the XML data we usually find in business applications. Figure 1 shows a small document model based on the TEI. Although it may seem complex, it is only a very small TEI subset. This type of markup is not as simple and homogeneous as conventional structured data. In these documents we usually find a wide variety of elements nested up to deep levels, and there are many exceptional cases that can lead to unexpected markup situations that also need to be covered. Complex markup schemes like TEI [9] and DocBook [1] are good examples of this versatility. However, no matter how heterogeneous and unpredictable the nature of humanities markup could get to be, software engineers have to deal with it in a systematic way, so that automatic processes can be applied to these texts in order to produce useful output for Web publishing, indexing and searching, pretty printing, and other end user facilities and services. There is also a need to reduce content production times and costs by automating and systematizing content production. For these, software, documentation and guides of good practice have to be developed. The building of all these automation, methods and procedures within the complexity of humanities content structuring can be called Document Engineering. The purpose is to reduce costs, and facilitate content production by setting constraints, rules, methods and implementing automation wherever and whenever is possible. XML, DTD or Schemas, XSL transforms, CSS stylesheets and Java programming are the usual tools to enforce the rules, constraints and transformations necessary to turn the document structuring problem to a systematic automated process that lead to useful Web services. But the wide variety of Schema types, and the individual limitations of each of them, make the task of setting a production environment like this very difficult. On one hand we need a markup vocabulary that can cover all document structuring requirements, even the most unusual and complex, but that is simple enough for our purposes. In other words, we need the simplest DTD/Schema that fits our needs. We previously treated the problem of DTD/Schema simplification in [2, 3]. But DTD/Schema simplification, although useful, doesn’t solve all the problems of Document Engineering, like building transformations to obtain useful output or assigning behaviour to certain structures (like popup notes, linking, and triggering services). This kind of environments are usually built incrementally. The design information, if any, is dispersed into many pieces of software (Schemas, transformation, Java applets and servlets), or does not exist at all. A system like this includes document design (DTD/Schemas), document production techniques and tools (XSL and Java), document exploitation tools (indexing, searching, metadata, dictionaries, concordances, etc.) and Web design altogether. UML modelling may be the answer to join all those bits and pieces into a coherent design that reduces design cost, improves the quality of the result, provides documentation and finally may even simplifies maintenance. UML modelling for massive Web content production may also lead to automatic generation of some of the tools mentioned. ADVANTAGES OF MODELING XML DOCUMENTS WITH UML Apart from modelling the structure of a class of documents (as DTDs and Schemas do), UML can capture other properties of elements: - Behaviour: this is related to event oriented functions (e.g. popup notes) - Additional powerful validation features (e.g. validating Fig. 1: a small document model based on the TEI. DH.indb 22 6/06/06 10:55:15 DIGITAL HUMANITIES 2006 Single Sessions P. 23 consistency of certain fields like author name against a database.) - Customization of document models to provide different views or subsets of the markup scheme to different users (e.g. DTDs for development of different types of news) We believe that the dynamic and interactive environment described here will be very useful to professionals responsible for designing and implementing markup schemes for Web documents and metadata. Although XML standards for text markup (like TEI and DocBook) and metadata markup (e.g. MODS, EAD) are readily available [8], tools and techniques for automating the process of customizing DTD/Schemas and addingpostprocessing functionality are not. PREVIOUS RELATED WORK As Kimber and Heintz define it [7], the problem is how do we integrate traditional system engineering modelling practice with traditional SGML and XML document analysis and modelling? According to David Carlson [5], eXtensible Markup Language (XML) and Unified Modelling Language (UML) are two of the most significant advances from the fields of Web application development and object- oriented modelling. DESCRIPTION OF THE PROJECT We are working on integrating these technologies to create a dynamic and interactive environment for the design of document markup schemes (see figure2). Our approach is to expand the capabilities of Visual Wade1 to obtain a tool that allows the visual analysis, design and maintenance of XML vocabularies based on UML. Among the targets we are working on the automatic generation of different types of DTD/Schemas from a visual UML model of the markup vocabulary, code generation when possible (like generating HTML forms or XSLT), documentation and special enhanced validators that can perform verifications beyond those allowed by DTDs or Schemas (like verification of certain element content or attribute values against a database). Fig. 2.: a environment for the design of document markup schemes. Carlson [5] suggests a method based on UML class diagrams and use case analysis for business applications which we adapted for modelling document markup vocabularies. A UML class diagram can be constructed to visually represent the elements, relationships, and constraints of an XML vocabulary (see figure 3 for a simplified example). Then all types of Schemas can be generated from the UML diagrams by means of simple XSLT transformations applied to the corresponding XMI representation of the UML model. Fig. 3.: Example of a UML class diagram (partial view). The UML model information can be stored in an XML document according to the XMI (XML Metadata Interchange) standard as described by Hayashi and Hatton [6]: “Adherence to the [XMI] standard allows other groups to easily use our modelling work and because the format DH.indb 23 6/06/06 10:55:15 P. 24 Single Sessions DIGITAL HUMANITIES 2006 is XML, we can derive a number of other useful documents using standard XSL transformations”. In our case, these documents are Schemas of various types as well as DTDs. Like Schemas, DTDs can be also generated from the XMI representation of the UML model (doted line), but as DTDs are simpler than Schemas, and all types of Schemas contain at least the same information as a DTD, DTDs can also be directly generated from them. POSTPROCESSING AND PRESENTATIONAL ISSUES In many cases, code generation from a high level model is also possible. Code generation may include JavaScript code to implement behaviour for certain elements like popup notes, hyperlinks, image display controls, etc. This is the case of input HTML forms that can be generated from Schemas as shown by Suleman [10]. We have successfully experimented on the generation of XSLT skeletons for XML transformation which save a lot of time. Usually XSL transforms produce fairly static output, like nicely formatted HTML with tables of contents and hyperlinks, but not much more. In exceptional cases we can find examples of more sophisticated interaction. This high level of flexible interactivity is the real payoff from the UML-XML-XSLT-browser chain. This sort of functionality is usually programmed specifically for individual projects, given that it’s highly dependent on the nature of the markup in any given document. We aim to provide the ability to specify this at the UML level. For instance, a note could be processed differently according to its type attribute and then be displayed as a footnote, a margin note, a popup note, etc. In certain cases it can be hooked to a JavaScript function to be popped up in a message window or in a new browser instance according to attribute values. In this sense, we could provide a set of generic JavaScript functions which could retrieve content from elements and display it in various ways (popup, insertions, etc.) or trigger events (like a dictionary lookup). We should look for document models that allow al kinds of presentation, navigation and cognitive metaphors. - Sequential reading - Text reuse (links and includes) - Non-sequential reading - Hyperlinks - Collapsible text - Foot notes, margin notes, popup notes - The folder metaphor - TOCs, indexes and menus All the elements in a structured document have an associated semantic and a behaviour or function (as in the above example, a popup note must appear on a popup window when a link to it is pressed). This is not reflected in conventional document models: a DTD/Schema may say that a note is a popup note: ... but the behaviour of this note is not stated at all. Some postprocessing must be implemented for the popup effect to happen. A UML based document model can incorporate the expected behaviour like methods in a class diagram. OTHER AUXILIARY TOOLS FOR DOCUMENT DESIGN AND OPTIMIZATION As additional aiding tools for this project we have incorporated two of our earlier developments: First the automatic simplification of DTDs based on sample sets of files [2, 3]. This tool can be applied to obtain simplified DTDs and Schemas customized to fit exactly a collection of documents. Second, automatic element names and attribute names translation can be applied when multilingual markup is required. A detailed explanation of the multilingual markup project can be found in [4]. See figure 2 for an idea of how these tools interact with the UML document modelling. The techniques described here can also be used for modelling metadata markup vocabularies. CONCLUSIONS Concerning the described set of DTD/Schema design tools, the integration of UML design with example based automatic simplification and multilingual vocabulary capabilities, is expected to be a very useful and practical design aid. However, we experienced some limitations in the use of UML. While commercial non UML products like XML Spy or TurboXML use custom graphical tree representation to handle XML schemas, comprising very handy collapsing and navigating capabilities, most general purpose UML design environments lack these specialized features. One of the downsides of UML is that it is less friendly when working with the low-level aspects of modelling [11]. For instance, it is easy to order the elements of a sequence in a tree, but it is very tricky to do so in UML. Although UML proves very useful for modelling document structures of small to medium complexity (metadata applications and simple documents), UML models for medium to big sized schemas (100 to 400 elements), like those used for complex DL documents, become practically unmanageable2. The diagrams become overloaded with too many class boxes and lines, which end up being unreadable. This problem could be solved, or at least mitigated, by enhancing the interfaces of UML design programs with newer and more powerful display functions. Facilities like intelligent collapsing or hiding of diagram parts or elements, overview maps (see figure 3), zooming, 3-D layouts, partial views, and other browsing capabilities would certainly help to solve the problem. Footnotes ♣ This work is part of the METASIGN project, and has been supported by the Ministry of Education and Science of Spain through the grant number: TIN2004-00779. 1 VisualWade is a tool for software development based on UML and extensions. It was developed by our research group, named IWAD (Ingeniería Web y Almacenes de Datos - Web Engineering and Data-Warehousing), at the University of Alicante. This group also developed the OOH Method (for more information see http://www.visualwade.com/) 2 The DTD used by the Miguel de Cervantes DL for its literary documents contains 139 different elements. The “teixlite” DTD, a simple and widely used XML-TEI DTD, contains 144 elements.",
       "article_title":"Using Software Modeling Techniques to Design Document and Metadata Structures ♣",
       "authors":[
          {
             "given":"Alejandro",
             "family":"BIA",
             "affiliation":[
                {
                   "original_name":"U. Miguel Hernández (Spain)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jaime",
             "family":"GÓMEZ",
             "affiliation":[
                {
                   "original_name":"U. Alicante (Spain)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"CTION Markup is based on mnemonics (i.e. element names, attribute names and attribute values). These mnemonics have meaning, being this one of the most interesting features of markup. Human understanding of this meaning is lost when the encoder doesn’t understand the language the mnemonics are based on. By “multilingual markup” we refer to the use of parallel sets of tags in various languages, and the ability to automatically switch from one to another. We started working with multilingual markup in 2001, within the Miguel de Cervantes Digital Library. By 2003, we have built a set of tools to automate the use of multilingual vocabularies [1]. This set of tools translates both XML document instances, and XML document validators (we first implemented DTD translation, and then Schemas [2]). First we translated the TEI tagset, and most recently the Dublin Core tagset [3] to Spanish, and Catalan. Other languages were added later1. Now we present a Multilingual Markup Website that provides this type of translation services for public use. PREVIOUS WORK At the time when we started this multilingual markup initiative in 2001 there were very few similar attempts to be found [4]. Today they are still scarce [5, 6]. Concerning document content, XML provides built-in support for multilingual documents: it provides the predefined lang attribute to identify the language used in any part of a document. However, in spite of allowing users to define their own tagsets, XML does not explicitly provide a mechanism for multilingual tagging. THE MAPPING STRUCTURE We started by defining the set of possible translations of element names, attribute names, and attribute values to a few target languages (Spanish, Catalan and French). We stored this information in an XML translation mapping document called “tagmap”, whose structure in DTD syntax is the following: <!ELEMENT tagmap (element)+ > <!ELEMENT element (attr)* > <!ATTLIST element en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED> <!ELEMENT attr (value)* > <!ATTLIST attr en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED> <!ELEMENT value EMPTY > <!ATTLIST value en CDATA #REQUIRED es CDATA #REQUIRED fr CDATA #REQUIRED > Fig. 1. Structure of the original tagmap.xml file This structure is pretty simple, and proved useful to support the mnemonic equivalences in various languages. It was meant to solve ambiguity problems, like having two attributes of the same name in English, who should be translated to different names in a given target language. For this purpose, this structure obliges us to include all the attribute names for each element and their translations. The problem with this is global attributes, which in this approach needed to be repeated, once for each element. This made the maintenance of this file cumbersome. Sebastian Rahtz then proposed another structured, under the assumption that an attribute name has the same meaning in all cases, no mater the element it is associated to, and accordingly it would have only one target translation to a given language. This is usually the case, and although theoretically there could be cases of double meaning, as above mentioned, they do not seem to appear within the TEI. So the currently available “teinames.xml” file follows Sabastian’s structure. Note that “element”, “attribute” and “value” appear at the same level, instead of nested: <!ELEMENT i18n (element | attribute | value)+ > <!ELEMENT element (equiv | desc)* > <!ATTLIST element ident CDATA #REQUIRED > <!ELEMENT attribute (equiv | desc)* > <!ATTLIST attribute ident CDATA #REQUIRED > <!ELEMENT value (equiv)* > <!ATTLIST value ident CDATA #REQUIRED > <!ELEMENT equiv EMPTY > <!ATTLIST equiv xml:lang CDATA #REQUIRED value CDATA #REQUIRED > In 2004, we discussed the idea of adding brief text descriptions to each element, the same brief descriptions of the TEI documentation, but now translated to all supported languages. This would allow the structure to provide help or documentation services in several languages, as another multilingual aid. This capability was then added to the “teinames.xml” file structure, although the translations of the all the descriptions still need to be completed: <!ELEMENT desc (#PCDATA) > <!ATTLIST desc xml:lang CDATA #REQUIRED > DH.indb 27 6/06/06 10:55:16 P. 28 Single Sessions DIGITAL HUMANITIES 2006 Fig. 2. Structure of the teinames.xml file. THE MULTILINGUAL MARKUP WEB SERVICE By means of a simple input form, the markup of a structured file can be automatically translated to the chosen target language. The user can choose a file to process (see figure 3) by means of a “Browse” button. Fig. 3. The Multilingual Markup Translator form. Currently, only TEI XML document instances are allowed. In the near future, the translation of TEI DTDs, W3C-Schemas and Relax-NG Schemas will be added, and later, other markup and metadata vocabularies will be supported, like Docbook and DublinCore. The system uses file extensions to identify the type of file submitted. Allowed file extensions are: .xml for document instances, .dtd for DTDs, .xsd for W3C Schemas, and .rng for RelaxNG schemas. The document to be uploaded must be valid and well-formed. If the document is not valid, the translation will not be completed successfully, and an error page will be issued. Once the source file has been chosen, the user must indicate the language of the markup of this source file, as well as the target language desired for the output. This is done by means of radio buttons. It would not be necessary to indicate the language of the markup of the source file if it was implicit in the file itself. We thought of three ways to do this: To use the name of the root tag to indicate the language of the vocabulary of the XML document. In this way, TEI.2 would be standard English based TEI, TEIes.2 would indicate that the document has been marked up using the Spanish tagset, and in the same way TEIfr.2, TEIde.2, TEIit.2 would indicate French, German, and Italian, for instance. To add an attribute to the root element, to indicate the language of the tagset, for instance: <TEI.2 markupLang = “it”> would indicate that the markup is in Italian. Use the name of the DTD to indicate the language of the tagset. TeiXLite.dtd would be English, while TeiXLiteFr.dtd would be the French equivalent. Option 3 is by far the worst method, since a document instance may lack a DOCTYPE declaration, and there may be lots of customized TEI DTDs everywhere with very different and unpredictable names. However, options 1 and 2 are reasonably good methods to identify the language of the markup. Consensus is needed to make one of them the common practice. IMPLEMENTATION DETAILS For the website pages we used JSP (dynamic pages) and HTML (static pages), and these are run under DH.indb 28 6/06/06 10:55:17 DIGITAL HUMANITIES 2006 Single Sessions P. 29 a Tomcat 5.5 web server. For the translations, we used XSLT, as described in [1, 2, 3] AUTOMATIC GENERATION OF MARKUP TRANSLATORS USING XSLT The XSLT model is thought to transform one input XML file into one output file (see figure 4), which could be XML, HTML, XHTML or plain text, and this includes program code. It does not allow the simultaneous processing of two input files. Fig. 4. The XSLT processing model. There are certain cases when we would like to process two input files altogether, like markup translation (see figure 5). Fig. 5. The ideal transformation required. As XSLT does not allow this, two alternatives occurred to us, both comprising two transformation steps. The first approach is to automatically generate translators. As Douglas Schmidt said: “I prefer to write code that writes code, than to write code” [7]. This is what we have done for the MMWebsite, i.e. to pre-process the translation map in order to generate an XSLT translation script which includes the translation knowledge embedded in its logic. Then this generated script can perform all the document-instance translations required. The mapping structure supports the language equivalences for various languages, so we should generate a translator for every possible pair of languages. Whenever the mapping structure is modified, a new set of translators must be generated. Fortunately, this is an automated process. Fig. 6. Pre-generation of a translating XSLT script, to then translate the document instance. The other alternative would be to merge the two input files into a new single XML structure, and then to process such file which would contain both the XML document instance, and the translation mapping information (see figure 7). This implies joining the two XML tree structures as branches of a higher level root. Fig. 7. Merging the two files before applying XSLT. Although this approach may prove useful for some problems, we did not use it for the MMWebsite, because the file merging preprocessing must be done for each file to translate, increasing the web service response time. Using preprocessed translators instead proved to be a faster solution. This limitation, which is proper of the XSLT processing model, could be avoided by using a standard programming language like Java instead. HOW WE ACTUALLY DO IT The mapping document which contains all the necessary structural information to develop the DH.indb 29 6/06/06 10:55:17 P. 30 Single Sessions DIGITAL HUMANITIES 2006 language converters is read by the transformations generator, which was built as an XSLT script. XSL can be used to process XML documents in order to produce other XML documents or a plain text document. As XSL stylesheets are XML, they can be generated as an XSL output. We used this feature to automatically generate both an English-to-local-language XSL transformation and a local-language to English XSL transformation for each of the languages contained in the multilingual translation mapping file. In this way we assured both ways convertibility for XML documents (see figure 8).Fig. 8. Schema translation using XSLT. For each target language we also generate a DTD or a Schema translator. In our first attempts, this took the form of a C++ and Lex parser. Later, we changed the approach. Now we first convert the DTD to a W3C Schema, then we translate the Schema to the local language, and finally we can (optionally) generate an equivalent translated DTD. This approach has the advantage of not using complex parsers (only XSLT) and also solves the translation of Schemas. In our latest implementation, the user can freely choose amongst DTD, W3C Schema and RelaxNG, both for input and output, allowing for a format conversion during the translation process. Many other markup translators can be built to other languages in the way described here. CONCLUSIONS Amongst the observed advantages of using markup in one’s own language are: reduced learning times, reduction of errors and higher production. It may also help spread the use of XML vocabularies like DC, TEI, DocBook, and many others, into non-English speaking countries. Cooperative multilingual projects may benefit from the possibility of easily translating the markup to each encoder’s language. Last, but not least, scholars of a given language feel more comfortable tagging their texts with mnemonics based on their own language. FUTURE WORK Multilingual Help Services: As already said, brief descriptions for elements and attributes in different languages have been added to the mapping structure. This allows for multilingual help services, like generating a glossary in the chosen language of the elements and attributes used in a given document, or a given DTD/Schema. We are working on adding this feature. Footnotes ♣ This work is part of the METASIGN project, and has been supported by the Ministry of Education and Science of Spain through the grant number: TIN2004-00779. 1 Translations of the TEI tagset by: Alex Bia and Manuel Sánchez (Spanish), Régis Déau (French), Francesca Mari (Catalan), Arno Mittelbach (German)",
       "article_title":"The Multilingual Markup Website ♣",
       "authors":[
          {
             "given":"Alejandro",
             "family":"BIA",
             "affiliation":[
                {
                   "original_name":"U. Miguel Hernández (Spain)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jaime",
             "family":"GÓMEZ",
             "affiliation":[
                {
                   "original_name":"U. Alicante (Spain)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Recent discussion about the scholarly digital edition has focussed on ways to change the edition from a passive text, only there to be consulted, into a dynamic research environment. Siemens in (Siemens 2005) asks why as yet we have seen no convincing integration between text analysis tools and the hypertext edition. Best in (Best 2005) speculates on the possibilities this vision offers for Shakespeare research. To some extent it seems to be what Mueller is realising in the Nameless Shakespeare (Mueller 2005). An essential step towards seamless integration of text analysis tools into the digital edition (TAML, the Text Analysis Mark-up Language) is suggested in (Sinclair 2005). The most visionary statement of the dynamic edition’s potential is no doubt given by Robinson in (Robinson 2003). A dynamic edition, in his view, while offering text analysis and other tools that may shed light on some aspect or another of the edited texts, would also be open to the addition of new content, of corrections, of many different types of annotations. Integrating third-party annotations into the edition is something that seems especially interesting, as it would open up the edition to the results of interpretive studies. The output of scholarly processes of textual analysis (as e.g. suggested in Bradley 2003) could be fed back into the digital edition, and made available for querying by other scholars. This paper will focus on a solution for adding third party annotations into a digital edition. It will propose a REST (Representational State Transfer, Fielding 2000) API for the exchange of annotation information between an edition and an annotation server. The edition display software (which transforms the edition XML source file into HTML) will ask the annotation server for the annotations that apply to text fragments that are being displayed to the edition user. Depending on the parameters the annotation server will return either annotations formatted for display or instructions for hyperlinking the text to the annotations. Thus, the digital edition will be able to include a display of external a nnotations without knowing about the annotations’ contents or even the annotation data model. The paper presentation will include a brief demonstration of a prototype implementation of the protocol. The demonstration will be based on a digital emblem book edition at the Emblem Project Utrecht (http://emblems.let.uu.nl) and use the EDITOR annotation toolset under development at the Huygens Institute (http://www.huygensinstituut.knaw.nl/ projects/editor, Boot 2005). EDITOR at present consists of an annotation input component that runs on the user’s workstation and an annotation display component that runs on a web server. The input component displays the CSS-styled edition XML to the user and facilitates the creation of multi-field user-typed annotations to arbitrary ranges of text in the edition. The display component, still at an early stage of development, shows the annotations in conjunction with the edition XML, has some facilities for filtering and sorting, and will offer, one day, advanced visualisation facilities. The EDITOR server component will serve up the annotations for display in other contexts, first and foremost, presumably, in the context of the digital edition that they annotate. As Robinson notes, one of the more complex issues in annotating the digital edition is the problem of concurrent hierarchies and the mark-up overlap problems to which this gives rise. The EDITOR annotation toolset assumes the edition and its annotations will be stored in separate locations. Each annotation stores information about the start and end locations of the text fragment to which it applies. There is no need to materialize the annotations into tagging interspersed between the basic edition mark-up, and the overlap issue therefore does not arise (the solution in that respect is similar to the Just In Time Markup described in Eggert 2005). Similarly, as the edition XML remains unmodified, there is no need to worry about potential corruptions during the annotation process. Making available third-party annotations from within the digital edition will go a long way towards establishing a ‘distributed edition fashioned collaboratively’, to borrow Robinson’s words. My paper will briefly look at some of the wider issues the integration of third-party scholarship into the digital edition raises. How will the presence of DH.indb 34 6/06/06 10:55:18 DIGITAL HUMANITIES 2006 Single Sessions P. 35 Edition’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Siemens_casta02.htm, accessed 2005-11-13. Sinclair, Stéfan (2005), ‘Toward Next Generation Text Analysis Tools: The Text Analysis Markup Language (TAML)’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/ Sinclair_casta02.htm, accessed 2005-11-13. third-party material influence the edition’s status? Should there be a review process for third-party contributions? Or is it old-fashioned to even think in terms of ‘third parties’? Robinson speaks of the edition as a ‘mutual enterprise’. Editorial institutes, such as the Huygens Institute, will need to rethink their role, as scholarly editions evolve into centrepieces of ever-expanding repositories of text-related scholarship. References Best, Michael (2005), ‘Is this a vision? is this a dream?’: Finding New Dimensions in Shakespeare’s Texts’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Best_casta02.htm, accessed 2005-11-13. Boot, Peter (2005), ‘Advancing digital scholarship using EDITOR’, Humanities, Computers and Cultural Heritage. Proceedings of the XVI international conference of the Association for History and Computing 14-17 September 2005 (Amsterdam: Royal Netherlands Academy of Arts and Sciences). Bradley, John (2003), ‘Finding a Middle Ground between ‘Determinism’ and ‘Aesthetic Indeterminacy’: a Model for Text Analysis Tools’, Lit Linguist Computing, 18 (2), 185-207. Eggert, Paul (2005), ‘Text-encoding, Theories of the Text, and the ‘Work-Site’, Lit Linguist Computing, 20 (4), 425-35. Fielding, Roy Thomas (2000), ‘Architectural Styles and the Design of Network-based Software Architectures’, (University of California). Mueller, Martin (2005), ‘The Nameless Shakespeare’, CH Working Papers, http://www.chass.utoronto.ca/epc/chwp/Casta02/Forest_casta02.htm, accessed 2005-11-13. Robinson, Peter (2003), ‘Where we are with electronic scholarly editions, and where we want to be’, Jahrbuch für Computerphilologie, http:// computerphilologie.uni-muenchen.de/jg03/ robinson.html, accessed 1005-11-13. Siemens, Ray (with the TAPoR community) (2005), ‘Text Analysis and the Dynamic Edition? A Working Paper, Briefly Articulating Some Concerns with an Algorithmic Approach to the Electronic Scholarly",
       "article_title":"Third-Party Annotations in the Digital Edition Using EDITOR",
       "authors":[
          {
             "given":"Peter",
             "family":"BOOT",
             "affiliation":[
                {
                   "original_name":"Huygens Instituut, Department of e-Research",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The Orlando Project, which has regularly reported on its work in progress at meetings of ACH/ALLC, is due for publication in early 2006. In this paper the three originating literary scholars on the project will look back at the its original goals, consider significant turning-points in the process, and reflect on what the project ended up producing for the first release. The project takes its name from Virginia Woolf’s fantastic narrative of an aspiring English writer who begins life as an Elizabethan male and is transformed in the course of the novel’s romp through history into a woman who lives up to the year of the novel’s publication in1928. The transformation occurs while Orlando is abroad as ambassador extraordinary for King Charles II in Turkey. Woolf’s narrator has much to say about the difficulties this poses for the historian, lamenting that “the revolution which broke out during his period of office, and the fire which followed, have so damaged or destroyed all those papers from which any trustworthy record could be drawn, that what we can give is lamentably incomplete.” The charred fragments offer some clues, but “often it has been necessary to speculate, to surmise, and even to use the imagination.” We would locate the electronic Orlando, like Woolf’s protagonist, as the site of an extraordinary transformation associated with the challenges of moving between cultures, the limitations of paper, and the necessity for speculation, imagination, and new approaches to scholarship. None of us were experienced in humanities computing when the project was begun; we set out to write a feminist literary history. In the process of trying to figure out how to do it, we decided to use computers. Ten years later, we have produced an extensively tagged XML textbase comprising a history of women’s writing in the British Isles in a form quite unlike any previous history of writing. At a glance, it may look like another translation into electronic form of the genre of alphabetical companion or literary encyclopedia, and it is indeed deeply indebted to that flexible and enduring form. However, our custom markup system has been used to tag a wide range of semantic content in the textbase, and a backend indexing and delivery system allows that markup to be exploited to produce on-the-fly documents composed of portions of the project’s digitally original source documents. The result is a very flexible and dynamic approach to literary history that challenges users to harness the power of the encoding to pursue their own interests. Recent argument about the crisis in scholarly publishing, such as that marshaled by Jerome McGann in support of the NINES initiative, has focused on the need to draw a larger community of scholars into best-practice methods of electronic markup and publication of texts. This is both crucial as a means of addressing the crisis, and indispensable to the continued development of electronic tools to serve the humanities research community. We offer ourselves as a kind of case study in such a process, given that our project did not originate as humanities computing endeavour but was completely transformed in the course of becoming one. In going electronic, we became radically experimental, tackling problems and producing results that we could not have foreseen at the outset. We will reflect on the results of taking an already very ambitious project electronic in relation to a range of factors including: o the impact on the intellectual trajectory of the project of engaging with, in addition to our disciplinary subject matter, a whole new field of inquiry and undertaking what became an interdisciplinary experiment in knowledge representation; o the impact on the project’s temporal trajectory; o funding, and its relationship to funding structures and opportunities; o the intensification of collaboration, increase in project personnel, and transformation of roles and responsibilities; o the impact on research and writing methods of composing an extensive scholarly text in XML; o the shaping of the research itself by the use of XML; o the development a delivery system that aimed at once to be reassuringly accessible and to challenge users to employ the system in new ways; o dilemmas regarding modes of publication While the paper will, given the constraints of time, necessarily touch briefly on some of these various areas, these reflections will be framed as an inquiry into what it means to bridge the gap between the community of researchers deeply invested in humanities computing and the wider scholarly community. We have come to see Orlando as a kind of emissary of humanities computing, in that we hope it will prove to be a major step towards establishing methods for encoding critical scholarly materials. It provides a test case of the feasibility and benefits of employing XML to encode large semantic units of critical discourse. It offers a model which we hope will be employed and adapted by other projects, and we will indicate the direction we would like to take the project in the future. But perhaps most importantly, the Orlando Project offers a substantial resource that in its design will, we hope, alert scholars beyond the humanities computing community to the potential of encoding as a means of scholarly inquiry and a tool of critical expression. The proof of that will be in the pudding, of course, so the paper will also report as far as possible on the initial reaction from the scholarly community in the field of English studies to the project’s public release.",
       "article_title":"Orlando Abroad : Scholarship and Transformation",
       "authors":[
          {
             "given":"Susan",
             "family":"BROWN",
             "affiliation":[
                {
                   "original_name":"School of English and Theatre Studies",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Patricia",
             "family":"CLEMENTS",
             "affiliation":[
                {
                   "original_name":"Department of English studies     University of Guelph",
                   "normalized_name":"University of Guelph",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01r7awg59",
                      "GRID":"grid.34429.38"
                   }
                }
             ]
          },
          {
             "given":"Isobel",
             "family":"GRUNDY",
             "affiliation":[
                {
                   "original_name":"Department of English studies     University of Guelph",
                   "normalized_name":"University of Guelph",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01r7awg59",
                      "GRID":"grid.34429.38"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Web applications are becoming more sophisticated and offer a rich user experience that is similar to native client applications. Examples of these applications include Google maps and Flickr. Interactive applications in the humanities can use some of these same design patterns to improve the user experience. The asynchronous update pattern is evaluated in this paper. Asynchronous update allows web applications to update without a complete page reload and goes by the popular name of Ajax. “Traditional web applications essentially submit forms, completed by a user, to a web server. The web server responds by sending a new web page back. Because the server must submit a new page each time, applications run more slowly and awkwardly than their native counterparts. Ajax applications, on the other hand, can send requests to the web server to retrieve only the data that is needed - usually using SOAP or some other XML-based web services dialect. On the client, JavaScript processes the web server response. The result is a more responsive interface, since the amount of data interchanged between the web browser and web server is vastly reduced. Web server processing time is also saved, since much of it is done on the client.” (from http://en.wikipedia.org/wiki/AJAX) This paper presents a case study of an ongoing project to create a digital library of Navajo language texts. After such texts are put into the database, the texts can be annotated with interlinear linguistic information using an interactive web application. The model for interlinear information that exists in TEI was determined to be inadequate for the present application and a different model is used. The design of the application for interactive and collaborative entry of interlinear linguistic information consists of a browser client using JavaScript and a server-side Exist native XML database that responds to XQueries. The acquisition and parsing methods for the texts are described in Canfield 2005. The Navajo language texts with annotated interlinear information are compliant with a Relaxng schema based on the XML from Bow 2003. Since this is a pilot application, it allows the schema to be tested and perhaps modified before finally adding the schema to TEI using an ODD specification. For example, the interlinear XML for the sentence “t’11’1ko [a’ sisi[“ is: <phrase> <item type=”txt”>t’11’1ko [a’ sisi[</item> <item type=”gls”>then one grabbed me</item> <words> <word> <item type=”txt”>t’11’1ko</item> <item type=”gls”>just then</item> <item type=”pos”>adv</item> </word> <word> <item type=”txt”>[a’</item> <item type=”gls”>one</item> <item type=”pos”>pro</item> </word> <word> <item type=”txt”>sisi[</item> <item type=”gls”>3p grabbed me</item> <item type=”pos”>verb</item> <morphemes> <morph> <item type=”txt”>shi</item> <item type=”gls”>me</item> <item type=”pos”>1st person obj </item> </morph> <morph> <item type=”txt”>yii</item> <item type=”gls”>he/she/it</item> <item type=”pos”>3p subj pro</item> </morph> <morph> <item type=”txt”>NULL</item> <item type=”pos”>classifier</item> </morph> <morph> <item type=”txt”>zi[</item> <item type=”gls”>grab</item> <item type=”root”>ZIID(1)</item> </morph> </morphemes> </word> </words> </phrase> All the XML in the database is transformed to XHTML for the user interface of the application. This page uses a tabular interface to allow the user to see and update the interlinear information for each sentence in the text. For example, if a word has already been annotated, it will appear with all the annotated information. If the word has not been annotated, the user double clicks on the word and the word appears in an editable html text input box. The user can then edit the detailed interlinear information for the text informed by an on-line Navajo lexicon. The display for each sentence appears as below, but the font for Navajo is not active so the characters display as the base ASCII. The sentence is « t’11’1ko [a’ sisi[ « which means «then one grabbed me.» All fields can be edited when double-clicked except for the base word in each sentence. Note that the last word is a verb «sisi[» and each underlying morpheme is annotated. t’11’1ko [a’ sisi[ t’11’1ko gls=just then pos=adverb [a’ gls=one pos=pronoun sisi[ gls=3rd person grabbed me pos=verb shiyiiNULLzi[mehe/she/itgrab1st person object3rd person subject pronounclassifierstem root=ZIID(1) gls= then one grabbed me DH The JavaScript code that updates each of the fields uses XML HTTP Request which allows the page to be updated without a page reload. Note that traditional web applications must reload the entire page for each update. This is time consuming and disruptive to the user experience. A sample of 25 chapter-length documents was used for this evaluation from the Navajo language digital library. The average document size was about 150kb, which is not very large for documents common in humanities applications. Each document update was timed (using JavaScript) in each mode - with asynchronous requests and with traditional page reloads. The average time for an update of a single field using XML HTTP Request was about 8 ms. The average time for a traditional (whole page reload) update of a field was about 400 ms. The traditional method shows a large update time that will cause unneeded user waiting. The traditional method also makes for a disruptive user experience where the page visually reloads while the user is trying to accomplish a task. Many interactive web applications in the humanities would benefit from this asynchronous update design pattern. Whenever a document is large and requires many small updates, the user experience will be improved with asynchronous requests due to shorter load times and a smoother experience with the user interface.",
       "article_title":"Asynchronous Requests for Interactive Applications in the Digital Humanities",
       "authors":[
          {
             "given":"Kip",
             "family":"CANFIELD",
             "affiliation":[
                {
                   "original_name":"Information Systems, University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"THE TEXTS This image markup project fits into the larger context of an electronic anthology, “Le marriage sous l’Ancien Régime: Une anthologie critique.” Since 1998, C. Carlin has been collecting texts about early modern marriage in France for her forthcoming book, L’imaginaire nuptial en France, 1545-1715. Given that the majority of documents studied for the book have not been republished since their original appearance in the sixteenth and seventeenth centuries, the idea of an electronic anthology should be appealing to scholars in several disciplines (history, literary studies, linguistics, cultural studies, art history, philosophy, religious studies). The proposed anthology is discussed in the article “Drawing Knowledge from Information: Early Modern Texts and Images on the TAPoR Platform” [1]. The radical changes undergone by the institution of marriage in France during and after the Counter Reformation generated texts of several different genres. Included in the anthology will be medical, legal, religious, satirical and literary documents and engravings, all heavily annotated. It is the engravings that interest us for this presentation. As part of a prototype for the anthology, several verse and prose polemics against marriage were encoded with XML markup in 2004 and early 2005. Most engravings of the period whose subject is marriage also fall into the polemical or satirical genre. Six from the collection of the Cabinet des Estampes of the Bibliothèque Nationale de France were requested on CD Rom, and are the test images for this project: Jacques Lagniet, “Il cherche ce qu’il voudrait ne pas trouver” Abraham Bosse, “La femme qui bât son mari” Jean Le Pautre, “Corrige, si tu peux, par un discours honneste” François Guérard, “Le Grand Bureau ou la confrèrie des martires” Nicolas Guérard, “Présage malheureux” Nicolas Guérard, “Argent fait tout” [2] THE SCHEMA Marking up annotations in XML required a framework that allowed for both the text of the annotations, and the image areas to which they correspond, to be encoded in a single document. Given that well-established tagsets exist for each of these functions, an XML model was developed based on a marriage of the Scalable Vector Graphics (SVG) 1.1 specification [3], and a subset of the Text Encoding Initiative (TEI) P5 guidelines [4]. This union allows TEI and SVG markup to operate concurrently. The TEI markup forms the overarching structure of a document, while elements belonging to the SVG vocabulary may appear in specific locations within the TEI encoding. Elements belonging to the SVG vocabulary are permitted within [div] tags and must be enclosed by the [svg] root element. Therefore, [svg] elements may appear anywhere with the TEI markup where [div] elements are permitted. Within an [svg] element may be any number of [rect] elements whose coordinates demarcate the area on an associated image to which an annotation applies. The texts of the annotations are enclosed in [div] blocks of their own, separate from the SVG encoding. This allows for annotation text to be encoded in any TEI-conformant way, presenting the possibility of integrating annotations with larger corpora. A [div] element containing an annotation text is associated explicitly with a set of annotation coordinates through references to the coordinates’ svg:id attribute. Markup validity is enforced through XML Schema or RELAX NG schema files which are bundled with the application. The schema which validates the TEI portion of the encoding has been generated by the ROMA suite of tools provided by the TEI for the purposes of specifying and documenting a customization. The TEI schema is supplemented by the addition of a schema describing the SVG 1.1 specification. The W3C provides the SVG 1.1 schema in either RELAX NG or DTD format, from which an XML Schema version may be derived using Trang [5]. Integrating schema from two different tagsets in this way is greatly facilitated by the modular construction inherent to both the TEI and SVG schema models. SVG may be ‘plugged-in’ to TEI by adding the [svg] root element to the list of allowable content in a particular context, and then associating the requisite schema documents with one another for the purposes of validation. Taking this approach to schema marriage has several advantages. The TEI guidelines for textual encoding provide a tagset whose usage rules are well-defined and understood, facilitating the portability of the encoding between projects, and easing the integration of corpora from different sources. An earlier method of encoding image annotations in XML, Image Markup Language [6], is based on a standalone markup structure which does not offer the same high degree of interoperability as the current model. The TEI encourages customization of its guidelines to accommodate for a wide range of implementations, an approach this project demonstrates. More generally, working with XML allows for the encoded material to be transformed into other formats as requirements dictate, such as XHTML, PDF, or OpenDocument format. THE IMAGE MARKUP TOOL Having decided on our approach to a schema, we then began to look at how we might create the markup. We wanted a straightforward tool for defining areas in an image and associating them with annotative markup, and we looked initially at two possible existing tools, INote [7] and the Edition Production Technology [8]. INote, from the University of Virginia, is a Java application for annotating images. It does not appear to have been updated since 1998. In some ways, INote is an ideal tool; it is cross-platform (written in Java), and covers most of our requirements. However, we rejected INote for several reasons. The program can load only GIF and JPEG images, and we wanted to be able to handle other common image formats such as BMP and PNG. INote also allows only limited zooming (actual size, double size, and half size). We required more flexible DH.indb 40 6/06/06 10:55:19 DIGITAL HUMANITIES 2006 Single Sessions P. 41 At the time of writing, the program is in at the «alpha» stage, and the first public version will be released under an open-source licence in December 2005. The program is written in Borland Delphi 2005 for Windows 2000 / XP. Development of the tool is guided by the following requirements: The Image Markup Tool should: - be simple for novices to use - load and display a wide variety of different image formats - allow the user to specify arbitrary rectangles on the image, and associate them with annotations - allow such rectangles to overlap if the user wishes - provide mechanisms for bringing overlapped rectangles to the front easily - require no significant knowledge of XML or TEI - allow the insertion of XML code if the user wishes - save data in an XML file which conforms to a TEI P5-based schema with embedded SVG - reload data from its own files - come packaged with an installer, Help file, and basic tutorial Using the Image Markup Tool, we have been able to perform several types of direct annotations, including the text within the engravings, commentary on that text, commentary on significant gestures depicted, and information about the engraver and the seal of the library at the time the engraving entered the library’s collection. The tool allows for distinction among types of annotation, and the use of a TEI-based file format allows us to link easily between the markup of the engravings the TEI-encoded polemical texts which are also included in the collection. We are now planning to use the program for a future project which involves marking up scans of historical architectural plans. One of the aims of this project will be to make the plans available to the public, so that (for example) the current owners of heritage buildings will be able to do renovation and restoration work with more detailed knowledge of the original building plan. zooming to handle larger images. Finally, INote uses a proprietary file format. However, INote does allow for polygonal and elliptical annotation areas, something not yet implemented in our own tool. The Edition Production Technology (EPT) platform is an Eclipse-based software suite developed by the ARCHway project [9]. Its ImagText plugin allows the association of rectangular areas of an image with sections of transcribed text. Although it promises to be a very powerful tool, especially for the specific job of associating document scans with transcription text, the interface of the program is complex and would be confusing for novice users. In addition, the tool developers expect and encourage the use of customized DTDs (“We do not provide support or guarantees for the DTDs included in the demo release - it is expected that users will provide their own DTDs and thus their own specific encoding practices.” [10]) The EPT also supports only JPEG, GIF, TIFF, and BMP files; other formats such as PNG are not supported ([http://rch01.rch.uky.edu/~ept/Tutorial/ preparing_files.htm#images]). We therefore decided to write our own markup program, which is called the Image Markup Tool [11]. Fig 1: scrshot_main_1.jpg, avalable at [http://mustard.tapor.uvic.ca/~mholmes/image_markup/scrshot_main_1.jpg]",
       "article_title":"Problems with Marriage: Annotating Seventeenth - Century French Engravings with TEI and SVG",
       "authors":[
          {
             "given":"Claire",
             "family":"CARLIN",
             "affiliation":[
                {
                   "original_name":"Dept. of French, University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Eric",
             "family":"HASWELL",
             "affiliation":[
                {
                   "original_name":"HCMC, University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"HOLMES",
             "affiliation":[
                {
                   "original_name":"HCMC, University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1 Overview Genre and its relation to textual style has long been studied, but only recently has it been a candidate for computational analysis. In this paper, we apply computational stylistics techniques to the study of genre, which allows us to analyze large amounts of text efficiently. Such techniques enable us to compare rhetorical styles between different genres; in particular, we are studying the communication of scientists through their publications in peer-reviewed journals. Our work examines possible genre/stylistic distinctions between articles in different fields of science, and seeks to relate them to methodological differences between the fields. We follow Cleland’s (2002) work in this area and divide the sciences broadly into Experimental and Historical sciences. According to this and other work in the philosophy of science, Experimental science attempts to formulate general predictive laws, and so relies on repeatable series of controlled experiments that test specific hypotheses (Diamond 2002), whereas Historical science deals more with contingent phenomena (Mayr 1976), studying unique events in the past in an attempt to find unifying explanations for their effects. We consider the four fundamental dimensions outlined by Diamond (2002, pp. 420-424): 1. Is the goal of the research to find general laws or statements or ultimate (and contingent) causes? 2. Is evidence gathered by manipulation or by observation? 3. Is research quality measured by accurate prediction or effective explanation? 4. Are the objects of study uniform entities (which are interchangeable) or are they complex entities (which are ultimately unique)? The present experiment was designed to see if language features support these philosophical points. These linguistic features should be topic independent and representative of the underlying methodology; we are seeking textual clues to the actual techniques used by the writers of these scientific papers. This paper is partially based on our previously presented results (Argamon, Chase & Dodick, 2005). 2 Methodology 2.1 The Corpus Our corpus for this study is a collection of recent (2003) articles drawn from twelve peer-reviewed journals in six fields, as given in Table 1. The journals were selected based both on their prominence in their respective fields as well as our ability to access them electronically, with two journals chosen per field and three fields chosen from each of Historical and Experimental sciences. Each article was prepared by automatically removing images, equations, titles, headings, captions, and references, converting each into a simple text file for further processing. 2.2 Systemic Functional Linguistics We base our analysis on the theory of Systemic Functional Linguistics (SFL; Halliday 1994), which construes language as a set of interlocking choices or systems for expressing meanings, with general choices constraining the possible more specific choices. SFL presents a large number of systems, each representing a certain type of functional meaning for a potential utterance. Each system has conditions constraining its use and several options; once within a system we can choose but one option. Specific utterances are constrained by all the systemic options they realize. This approach to language allows the following types of questions to be asked: In places where a meaning of general type A is to be expressed in a text, what sorts of more specific meanings are more likely to be expressed in different contexts? We focused on several systems for this study, chosen to correspond with the posited differences between the types of science we study: Expansion, Modality, and Comment (Matthiessen 1995). Expansion describes features linking clauses causally or logically, tying in to dimensions 1 and 4 above. Its three types are: Extension, linking different pieces of information; Elaboration, deepening a given meaning via clarification or exemplification; and Enhancement, qualifying previous information by spatial, temporal, or other circumstance. The second system, Modality, relates to how the likelihood, typicality, or necessity of an event is indicated, usually by a modal auxiliary verb or an adjunct adverbial group; as such it may serve to indicated differences on dimensions 2, 3, and 4. There are two main types of modality: Modalization, which quantifies levels of likelihood or frequency, and Modulation, which qualifies ability, possibility, obligation, or necessity of an action or event. Finally, the system of Comment is one of assessment, comprising a variety of types of ``comment” on a message, assessing the writer’s attitude towards it, its validity or its evidential status; this provides particular information related to dimensions 1 and 3. In our analysis, it will be most helpful to look at oppositions, in which an option in a particular system is strongly indicative of one article class (either Experimental or Historical science) while a different option of that same system is indicative of the other class. Such an opposition indicates a meaningful linguistic difference between the classes of articles, in that each prefers a distinctive way (its preferred option) of expressing the same general meaning. 2.3 Computational analysis Because hand analysis is impractical on large document sets the first analyses were done via computer. We built a collection of keywords and phrases indicating each option in the aforementioned systems. Each document is first represented by a numerical vector corresponding to the relative frequencies of each option within each system. From here, machine learning was applied in the form of the SMO (Platt 1998) algorithm as implemented on the Weka machine learning toolkit (Witten & Frank 1999), using 10-fold cross-validation in order to evaluate classification effectiveness. This method was chosen in part because it generates weights for each feature; a feature has high weight (either positive or negative) if it is strongly indicative for one or the other class. 2.4 Human annotation To measure the validity of our computational analysis, we are also performing hand tagging of systemic features on a subset of the corpus articles. Two articles from each journal have been chosen, each to be tagged by two trained raters. Part of the tagging process is to highlight key words or phrases indicating each option; we will compare these statistics to our previously generated feature lists in order to test and refine them. The tagging is currently under way; we will present results at the conference. 4 Results To determine the distinctiveness of Historical and Experimental scientific writing, the machine learning techniques described above were applied to pairs of journals, giving for each pair a classification accuracy indicating how distinguishable one journal was from the other. These results are shown in Figure 1, divided into four subsets: Same, where both journals are from the same science; Hist and Exper with pairs of journals from different sciences, but the same type; and Diff indicates pairings of Historical journals with Experimental ones. The thick black line indicates the mean for each set, and the outlined box represents the standard deviation. As we see, journal pairs become more distinguishable as their methodological differences increase. Interestingly, Historical journals appear more stylistically homogenous than the Experimental journals, which is a subject for further study. This shows that SFL is capable of discriminating between the different genres presented. We also examined the most important features across the 36 trials between different journals. The most consistently indicative-those features that are ranked highest for a class in at least 25 trials-are presented in Table 2. The table is arranged as a series of oppositions: the features on each row are in the same system, one side indicating Historical, the other Experimental. In the system of Expansion, we see an opposition of Extension and Enhancement for Historical and Experimental sciences, respectively. This implies more independent information units in Historical science, and more focused storylines within Experimental science. Furthermore, there are oppositions inside both systems, indicating a preference for contrasting information (Adversative) and contextualization (Matter) in Historical science and for supplementary Information (Additive) and time-space (Spatiotemporal) relations in Experimental science. DH.indb 44 6/06/06 10:55:20 DIGITAL HUMANITIES 2006 Single Sessions P. 45 Figure 1: Learning accuracy for distinguishing articles in different pairs of journals. ‘Same’ are pairs where both journals are in the same field, ‘Historical’ and ‘Experimental’ represent pairs of journals in different Historical and Experimental fields, and ‘Different’ pairs of journals where one journal is experimental and the other historical. Means and standard deviation ranges are shown. System Historical Experimental Expansion Extension(26) Enhancement(31) Elaboration Apposition(28) Extension Adversative(30) Additive(26) Enhancement Matter(29) Spatiotemporal(26) Comment Admissive(30) Validative(32) Predictive(36) Modality Type Modalization(36) Modulation(35) Modulation Obligation(29) Readiness(26) Modality Value High(27) Modility Orientation Objective(31) Subjective(31) Table 2. Consistent indicator features within each of the systems used in the study. Numbers in parentheses show in how many paired-classification tests the feature names was an indicator for the given class of documents. References Argamon, S., Chase, P., and Dodick, J.T. (2005). The Languages of Science: A Corpus-Based Study of The system of Comment also supports the posited differences in the sciences. The Experimental sciences’ preference for Predictive comments follows directly from their focus on predictive accuracy. On the Historical side, Admissive comments indicate opinions (as opposed to factual claims), similarly Validative comments show a concern with qualifying the validity of assertions, comprising more of strong evidence than rigid proofs. Finally in Modality we see interesting contrasted features. On the top level we have near-perfect opposition between Modalization and Modulation in general; Historical sciences speak of what is ‘normal’ or ‘likely’, while Experimental sciences assess what ‘must’ or ‘is able’ to happen. 5 Conclusion This work is the first step in developing new automated tools for genre analysis, which promises the possibility of automatically analyzing large corpora efficiently or stylistic aspects while giving human interpretable results. The specific research presented has implications for the understanding of the relationship between scientific methodology and its linguistic realizations, and may also have some impact on science education. Future work (beyond the hand annotation and analysis already in progress) includes looking into stylistic variation within different article sections, as well as other analysis techniques (such as principle components analysis). Journal #Art Avg. Words J. Geology 93 4891 J. Metamorphic Geol. 108 5024 Biol. J. Linnean Society 191 4895 Human Evolution 169 4223 Palaeontologia Electronica 111 4132 Quaternary Research 113 2939 Physics Letters A 132 2339 Physical Review Letters 114 2545 J. Physical Chemistry A 121 4865 J. Physical Chemistry B 71 5269 Heterocycles 231 3580 Tetrahedron 151 5057 Table 1: Journals used in the study; the top represents historical fields with experimental sciences below.",
       "article_title":"Methods for Genre Analysis Applied to Formal Scientific Writing",
       "authors":[
          {
             "given":"Paul",
             "family":"CHASE",
             "affiliation":[
                {
                   "original_name":"Linguistic Cognition Laboratory Dept.     of Computer Science Illinois Institute of     Technology 10 W 31st Street     Chicago, IL 60616, USA",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Shlomo",
             "family":"ARGAMON",
             "affiliation":[
                {
                   "original_name":"Linguistic Cognition Laboratory Dept.     of Computer Science Illinois Institute of     Technology 10 W 31st Street     Chicago, IL 60616, USA",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Studies in the computational analysis of texts have been successful in distinguishing between authors and in linking anonymously published texts with their authors but computational tools have yet to be accepted as mainstream techniques in literary analysis. Criticisms are generally centred around the belief that computational analyses make false claims about scientific objectivity and are in fact no less subjective than any other critical approach. This is perhaps because computational projects are in conflict, at a fundamental level, with contemporary post-structuralist notions of subjectivity, meaning and the arbitrary nature of language. This paper will argue that these objections rest on assumptions about language that need to be examined in light of developments in linguistics and cognitive psychology and that cognitive linguistics has the potential to bring a more interpretive framework to computational stylistics, a practice that has traditionally been applied in fairly narrow, empirical way. Whilst computational analysis points to the possibility of subjectivity that is more coherent than some theoretical approaches imply, it does not necessarily diminish the role of culture and context in the formation of texts and subjectivity as highlighted by materialist readings. The application of cognitive linguistics in a computational study provides a model of syntax and semantics which is not independent of context but deeply bound up in context. Cognitive linguistics can explain the existence of computational results in a way that Saussurean based theories can not. It can offer a rich interpretive model that does not neglect the importance of author, reader, or context through its approach to language and literature as an expression of an innately constrained and embodied human mind. Computational stylistics, particularly in studies of attribution, generally makes use of function words in order to distinguish between texts. As Craig (2004) explains, independent variables like genre, are compared with counts of internal features, or dependent variables, like function words. Correlation of these two kinds of variables is the primary tool of computational stylistics (275-76). Critics of stylistics, most notably Stanley Fish, tend to privilege individual instances of particular words in interpretive communities over more general rules, and question the validity of the stylistic project. From a cognitive perspective, however, language possesses universal features because it emerges from the interaction of “inherent and experiential factors” that are “physical, biological, behavioural, psychological, social, cultural and communicative” (Langacker 1). Langacker claims that each language “represents a unique adaptation to common constraints and pressures as well as to the peculiarities of its own circumstances” (1). Computational stylistics of the kind undertaken in this study provides us with evidence of the peculiarities and creative adaptations of an individual user, and also highlights more general trends which can be used for comparative purposes. Our attitude to what we can say about a text depends largely on our account of language. Widely shared post-structuralist assumptions about language and indeterminacy have contributed to the lukewarm reception of computational stylistics in literary interpretation. In cognitive linguistics “Semantics is constrained by our models of ourselves and our worlds. We have models of up and down that are based on the way our bodies actually function. Once the word “up” is given its meaning relative to our experience with gravity, it is not free to “slip” into its opposite. “Up” means up and not down” (Turner 7). Cognitive stylistics views a text as the product of a human action and it therefore carries the mark of that action. The cognitive belief that language and conventional thought emerge from “our perception of a self within a body as it interacts with an environment” suggests that meaning is somewhat constrained and that “some form of agency is fundamental to language” (Crane 22). The idea of authorial agency is one that is rejected by structuralist and post-structuralist critics. In proclaiming the death of the author Barthes suggests that the text becomes an ‘open sea’, a space of ‘manifestly relative significations, no longer tricked out in the colors of an eternal nature’ (Barthes 170). The notion of the “transcendental signified” rejected by post-structuralist critiques is not, however, the notion of agency proposed by cognitive stylistics. The view of the “Author” rejected by Barthes, Derrida and Foucault is as, Seán Burke explains “a metaphysical abstraction, a Platonic type, a fiction of the absolute” (27). Cognitive stylistics tends not to deal in absolutes. Stylistics is one way of getting evidence and making sense of texts as human actions. Through its approach to thought and language, cognitive stylistics points to issues that are of concern to scholars of literature, such as “subject formation, language acquisition, agency and rhetoricity” (Richardson 157). Cognitive philosophy claims that the mind is embodied and that concepts are therefore created “as a result of the way the brain and body are structured and the way they function in interpersonal relations and in the physical world” (Lakoff and Johnson 37). The links between the brain and the body mean an objective reality is impossible given the role our sensorimotor system plays in perception. But as Lakoff and Johnson explain, it is our sensorimotor system’s role in shaping conceptual systems that keeps these systems in touch with the world (44). The embodied cognitivism of Lakoff and Johnson, also known as second generation cognitivism, argues that our access to the external world is mediated through cognitive processes. Cognitivism provides a framework in which we can still legitimately engage with psychoanalytic interpretations, gender focused readings, and the material conditions of production while using computational and cognitive techniques of analysis. Computers enable us to draw together instances of common forms, and other features of a text, in a way that would be simply impossible to an individual human reader. Cognitive linguistics provides a theoretical justification for paying attention to common forms in the first place, and reveals a way in which the features highlighted by computational approaches can contribute something of value to traditional literary analysis.",
       "article_title":"Combining Cognitive Stylistics and Computational Stylistics",
       "authors":[
          {
             "given":"Louisa ",
             "family":"CONNORS",
             "affiliation":[
                {
                   "original_name":"School of Humanities and Social Science     The University of Newcastle, Australia",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction In earlier analyses of the introduction of word types in literary works authors came to contradictory conclusions. Some suggested, in accordance with reader’s intuition, that the launch of new chapters and a sudden increase in the number of the newly introduced word types (NWT) usually coincide. Others, on the other hand, found that there is no clear connection in the rise of NWT and the beginning of chapters, rather an increase in NWT appears at longish descriptions with rather stylistic reasons. Words do not occur randomly in texts, so the ultimate goal of building models based on word frequency distributions may not be the reproduction of the original text. Nevertheless, models based on the randomness assumption give reliable information about the structure of the texts (for review see Oakes, 1998; Baayen, 2001). To further our knowledge in this field the source of the bias between the original and the model-based texts should be examined. Baayen (1996; 2001) described a systematic overestimation for the expected vocabulary size and found that this bias disappears when the order of the sentences is randomized, indicating that the bias should not be attributed to constrains operating on sentence level. To prove that this misfit is due to significant changes on discourse level we introduced several new concepts during the process of building the model and analyzing the results (Csernoch, 2003). Among these the fundamental step was to scrutinize NWT in hundred- token-long intervals rather than examining the overall vocabulary size. Next, instead of eliminating the bias between these artificial texts and original works, the significant protuberances on the graphs of NWT were examined. First monolingual sets of works were processed then, to improve the comparison we also analyzed original English texts and their Hungarian translations together with English and German translations of a Hungarian text. Assuming that the changes occur on discourse level, the language in which the text is written should have no significance. In other words, neither syntactic nor semantic constrains on sentence or paragraph level should matter, and only events that occur on discourse level will provide substantial alterations in the flow of the text, and thus produce considerable protuberances on the graphs of NWT. Methods Building the model To analyze a text first the number of different word types was counted, the frequency of each was determined, and then based on these frequencies a dynamic model was built (Csernoch, 2003). The model generated an artificial text whose word types had the same frequencies as in the original text and was able to reproduce the trends of the original text. However, changes which are only seasonal – protuberances – did not appear in the artificial text. To locate these protuberances the difference between the original and the model text was calculated. We then determined the mean (M) and the standard deviation (SD) of the difference. Protuberances exceeding M±2SD were considered significant. The distribution of the hapax legomena was also examined. Assuming that they are binomially distributed their expected mean (Mh) and standard deviation (SDh) were calculated and again those points where considered significant which exceeded Mh+2SDh. texts compared to their translations Original texts were not only compared to the model-generated artificial texts but to their translations in other natural languages. In this study we analyzed the Hungarian novel, SORSTALANSÁG from Imre Kertész and its English (FATELESS) and German (ROMAN EINES SCHICKSALLOSEN) translations, Rudyard Kipling’s THE JUNGLE BOOKS and their Hungarian translations (A DZSUNGEL KÖNYVE), and Lewis Caroll’s ALICE ADVENTURES IN WONDERLAND and THROUGH THE LOOKING GLASS and their Hungarian translations (ALICE CSODAORSZÁGBAN and ALICE TÜKÖRORSZÁGBAN). These three languages were chosen because they are different in their morphological structures, it is hard to trace any common syntactic characteristic which all three share. Analyzing lemmatized texts To check whether the analyses of the raw, un-lemmatized texts give reliable information for the introduction of NWT the lemmatization of both the English and the Hungarian texts was carried out. The English texts were tagged and lemmatized by CLAWS (the Constituent Likelihood Automatic Word-tagging System) [1], while the morphological analysis of the Hungarian texts was carried out by Humor and the disambiguation was based on a TnT tagger [2]. Results Comparing the texts and their translations it was first found that the morphologically productive Hungarian texts had the smallest number of running words and lemmas while the largest number of hapax legomena both in the lemmatized and un-lemmatized versions. In contrast, the English texts contained the most running words but the smallest number of hapax legomena. To each text and language an individual model was created. Based on these models the positions of the significant protuberances were traced and compared to each other in the original texts and their translations. It was noticed that regardless of the actual language these protuberances occurred in most cases at the same position, that is, at the same event in the flow of the story. We could clearly establish that the protuberances were found at places where new, usually only marginally connected pieces of information were inserted into the text rather than at new chapters. This idea was strengthened by a peculiarity of the English translation of SORSTALANSÁG, namely that the boundaries of chapters are different from those of the Hungarian and German texts, which further substantiates that the protuberances do neither necessarily coincide with the beginning nor are hallmarks of a new chapter. Similarly, in the original Alice stories the boundaries of the chapters are eliminated by unusual typographic tools, while in the Hungarian translation these boundaries are set back to normal. Neither the English nor the Hungarian texts produced any protuberances at these places. In THE JUNGLE BOOKS we again found that the significant differences between the original text and the model are not necessarily at the beginning of a new tale, except for cases when a new setting is introduced. The fact that these descriptions have only a stylistic role in the text was further substantiated by examining the distribution of hapax legomena. The number of hapax legomena was found to be high exactly at the same positions of the text where protuberances in the number of the newly introduced word types occurred. To examine the lemmatized version of the texts carried some risk since loosing the affixes might eliminate the change in mode, time, style, etc., while, on the other hand, might reveal events lost in word types carrying the affixes. Since our dynamic model is capable of giving a relatively good estimation for the introduction of words, the question was whether using lemmas instead of word types would provide additional information gained by comparing the artificial texts and the translations to the original text. In the English texts the lemmatization did not reveal any additional information, the protuberances occurred at exactly the same places in the lemmatized as in the un-lemmatized versions. In un-lemmatized Hungarian texts the first protuberance usually occurred later than in corresponding English and German texts, although we were able to locate them by examining protuberances that were somewhat below the level of significance. In these cases lemmatization helped, and we got clear protuberances reaching the level of significance in lemmatized Hungarian texts. The comparison of the dynamic model built to lemmatized texts in different languages might also be used to analyze and compare the vocabulary of the original texts and their translations. It would, furthermore, enable the comparison of the stylistic tools used by the original author and the translator in the introduction of new words. Summary Using lexical statistical models for analyzing texts the explanation for the difference between the original and the model-based artificial text was examined. It was found that changes on discourse rather than on sentence or paragraph levels are responsible for these differences. Two methods were used to prove this. First, texts and their translations, both lemmatized and un-lemmatized versions, were analyzed and compared to a dynamic model built on the randomness assumption to find that the significant changes on the graphs of the newly introduced word types occurred at corresponding positions within the translations. Second, the distribution of hapax legomena was compared to a binomial distribution, again to find that the significant differences between the original and the predicted distributions occurred at descriptions, only in loose connection with the antecedents and what follows. More importantly, these coincided with the significant changes of the newly introduced word types.",
       "article_title":"The Introduction of Word Types and Lemmas in Novels, Short Stories and Their Translations",
       "authors":[
          {
             "given":"Mária ",
             "family":"CSERNOCH",
             "affiliation":[
                {
                   "original_name":"University of Debrecen, Hungary",
                   "normalized_name":"University of Debrecen",
                   "country":"Hungary",
                   "identifiers":{
                      "ror":"https://ror.org/02xf66n48",
                      "GRID":"grid.7122.6"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The Baedeker project, presented in this proposal, is initiated within the framework of the Austrian Academy Corpus, a research department at the Austrian Academy of Sciences. The sub corpus comprises first editions of German travel guidebooks brought out by the Baedeker publishing house between 1845 and 1914. The texts cover exclusively non-European destinations. Today they are rare, which is due to low print runs. The aim of the project is threefold: 1) it partly deals with the genre from a literary point of view, 2) it looks at travel guides as cultural historical resources as well as artefacts that represent various discourses on culture, and 3) it examines the capacities of digital resources. Travel guidebooks, not regarded as a literary form in their own right within the classical canon, played a minor role in comparison with travel narratives for a long time. Substantial contributions to the historical development of the genre and its specific language of expression are still few. Even postcolonial literary criticism cannot be regarded as an exception in this respect. Surprisingly enough, few approaches appreciate the importance and impact of this genre on the establishment and maintenance of Orientalist discourses and colonial practises. Linguistic accounts mostly concentrate on contemporary travel guides and exclude historical development and change. Thus, digital versions of early travel guidebooks can provide an incentive to improve both comparative linguistic and literary genre studies. Furthermore, research on tourism history, its influence on modern society, its bearing on social and cultural change has increased quantitatively in many fields of the humanities since the 1980s: history in general, art history, colonial studies, social and cultural anthropology, economics, geography and tourism studies, today an accepted sub branch of sociology. In these disciplines it goes without saying that travel guidebooks are valuable sources. Nonetheless, they are often dealt with as sources among many others. (The few exceptions are e.g. James Buzard 1993; Kathleen R. Epelde 2004, Sabine Gorsemann 1995, Rudy Koshar 2000 and the research group Türschau 16 1998.) The Baedeker, appearing from 1832 onwards, set the standard and defeated all competition both inside and outside the German-speaking countries. One cannot talk about travel guides without Baedeker coming to mind. During the first decades of the Baedekers, the focus was on Europe. However, they were issued for non-European destinations as well, an aspect missing from critical literature. The guides in the Baedeker-Corpus cover a variety of regions such as Palestine and Syria (1875), Lower and Upper Egypt (1877, 1891), North America and Mexico (1883), Asia Minor (1905), the Mediterranean coastline of Africa (1909) as well as India (1914). Dealing with a wide range of cultural environments the “Tourist Gaze” upon the “Other” has to be scrutinized in greater depth. Assuming that images of the “Other” reflect cultural self-perception to a great extent, travel guidebooks tell at least as much about the “Self” as about the “Other”. For well known reasons, none of the components involved here can be taken for granted as precise, unambiguous or fixed and independent entities. Taking this argument seriously, self-images - like all the other components - are to be understood as flexible phenomena. Moving away from the very frequent restriction on one region or country the Baedeker project turns towards a wider geographical diversification to explore the German repertoire of how one used to speak at the turn of the 19th century about one’s own culture, and at the same time, that of others. As concerns the digital methods by which these phenomena are to be investigated, the following has to be pointed out: while XML is now an accepted standard for the creation and exchange of digital data, it is necessary to move towards a closer consideration of domain-specific XML vocabularies. All Baedekers have undergone scanning, OCR and basic XML annotation as usual with all AAC projects. The task at hand is to devise a schema to markup the features of the Baedekers, relevant for their role in travel history. Existing standards like AnthML and Archaeological Markup Language are focused on material artefacts. A markup language which considers immaterial cultural aspects is missing so far. As a matter of course the development of a standardized language needs the expertise of the wider scholarly community and has to be a team effort - a requirement not feasible within one institution. Thus, the Baedeker project should be seen as a small contribution in preparing such a markup language, designing a sub-set of tags focusing on a well defined segment of cultural life. The main challenge is encoding what travel guides are essentially supposed to do, namely introducing foreign cultures and people/s, recommending an itinerary, assessing sites and festivals, cultural and social conditions, suggesting modes and attitudes of behaviour to adopt in these places and on these occasions. Since cultural knowledge as well as recommendations, valuations, stereotypes or comparisons often is articulated in an implicit manner, which is difficult to encode, the project targets subject-matters such as people/s, languages and religions, social, political, and other cultural concepts as well as sights being recommended, valuated, stereotyped, and compared in the travel guides. As an example I will refer to the repertoire of “group designations”, be it ethnic, national, social, religious, political, and occupational, showing how they relate to the historical discourse on culture. The paper will demonstrate that subject-matters can be easily marked up, they allow for an appropriate access to context - i.e. different routes to topics and explicit as well as implicit knowledge - and they provide a basis for comparative analysis. In addition, this strategy separates annotation from interpretation and limits the risk of encoding preconceived assumptions. Detailed domain-specific markup can be applied to other texts dealing with similar topics - to primary and secondary sources, historical as well as contemporary. In this respect the Baedeker can be seen as a starting point. Using open standards allows for ongoing quality enhancement and adjustment. XML annotation, in this sense, is not a single-serving tool, but a permanent enrichment, accessible and shareable with the wider scholarly community. Retrieval results can be reviewed by different scholars, paving the way for reinterpretation and new questions. It is expected that differing results will come from the same markup.",
       "article_title":"Digitizing Cultural Discourses. Baedeker Travel Guides 1875 - 1914.",
       "authors":[
          {
             "given":"Ulrike ",
             "family":"CZEITSCHNER",
             "affiliation":[
                {
                   "original_name":"Ulrike Czeitschner, AAC-Austrian Academy Corpus",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The Berlin-Brandenburg Academy of Science and Humanities (BBAW) [1] is an international and interdisciplinary association of excellent scholars with a distinctive humanities profile. The Academy hosts about 30 long term projects in the humanities with a project runtime often longer than 100 years. Examples of these projects include work on academic dictionaries, printed and manuscript editions, documentations, bibliographies, archival and publishing projects and more. These project groups have access to information and data made by outstanding scientists like Gottfried Wilhelm Leibniz, Johann Wolfgang von Goethe, Immanuel Kant, Albert Einstein, Jacob and Wilhelm Grimm, Alexander von Humboldt and many more who were members of the Academy during the last 300 years. Throughout its history the Society could rank 76 Nobel Laureates among its members. According to the “Berlin Declaration on Open Access in the Sciences and Humanities” [2], which was signed by the President of the Academy, an initiative was founded to provide a sustainable, interactive and transparent way to inspire activities in supporting research, communication and presentation with electronic media or in other words to “electrify” the long term humanities projects hosted by the Academy. This initiative is called “TELOTA – the electronic life of the academy”. [3] One part of TELOTA is the so called “Project of the Month” (POM) [4] working group which started work in January 2005. The main task of this working group is to provide solutions for the mentioned issues which are cost efficient, future proven (especially to be independent from commercial vendors) and freely extensible. Every month the data of a selected academy project is processed, (re-)structured and presented on the web to give researchers in the humanities, scholars and the interested public a new view in the extensive knowledge inventory of the academy. To identify and process information from the long term projects is one of the central tasks for POM. Further goals are: 1. To replace older, cost-intensive proprietary tools which are often not very suitable for presentations in the World Wide Web: • Improve the unification of the developed solutions for the different projects with respect for already existing solutions and open standards as well as concentration on third party open source software. • Production of reusable software modules. 2. To offer to the interested public an overview and provide an insight into the work done by the projects hosted by the academy: • To make new information and resources available on the World Wide Web. • Adaptation, unification and customization of existing data to offer a new point of view on a certain project. • Give access to the raw data which can be queried by arbitrary applications using XQuery. [5] 3. To benefit each humanities project hosted by the academy: • The projects should be able to access and administrate the data they produce on their own for a gradual extension of their web presence and research material. • Guarantee of long term accessibility and preservation as a result of consistent data and coherent administration. • Real time accessibility to the projects’ results in the World Wide Web. • Support of the project’s work flow with tools especially developed for their needs. All the applied technologies and third party tools are reused, like all the gained experience is transferred from one project to the next. In addition new technologies are adopted to the working group’s portfolio so it is able to react properly to the monthly changing requirements. This paper will introduce the work of the “Project of the Month” working group and exemplary present two systems for humanities projects from the viewpoint of an “in-house” working group. It shows the possibilities of developing electronic resources of long term projects in a very short time period. Additionally it demonstrates a way how the mentioned technologies can be combined as flexible as possible. The first system, the “scalable architecture for electronic editions”, uses the opportunities of web services applied on critical text editions and was developed while processing prominent projects such as the “Corpus Medicorum Graecorum/Latinorum” or the “Marx- Engels Gesamtausgabe”. The main component is a native XML-Database [6] which is able to interpret XQuery-scripts to form the web application. The user, according to his needs, dynamically decides on the view of the presented texts and translations and the information which is displayed like line numbers or links to the apparatus. So he can customize the electronic edition depending on his scientific position or interests. If possible, facsimiles are linked to text, translation and apparatus and if needed it is possible to search the electronic edition in different scripts, like ancient Greek. The second system, an approach for digital dictionaries, shows the development stages of an interactive on-line dictionary which currently is work in progress and could contain the digital versions of dictionary projects of the academy in the future. Such a system is necessary for the real time digital presentation of dictionary project results. Examples are the “Dictionary of contemporary German language”, the “Dictionary of Goethe” or the “German Dictionary of Jacob Grimm and Wilhelm Grimm”. One main feature besides arbitrary querying the database using XQuery is the possibility to add and edit own dictionary articles, if the user is authorized to do so. The search results than can be displayed in HTML or PDF. Both systems are conceptually designed with a general attempt but currently serve as sample applications. The use of a more technically matured version of this systems should not be limited to one project or just the academy long term projects rather than potentially be open to any kind of critical text edition or dictionary.",
       "article_title":"TELOTA - the Electronic Life of the Academy. New Approaches for the Digitization of Long Term Projects in the Humanities.",
       "authors":[
          {
             "given":"Alexander ",
             "family":"CZMIEL",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Laurence Sterne (1713-1768) is better-known nowadays as a novelist, but he was also an Anglican minister whose literary style was initially developed in the pulpit. He wrote many sermons before he turned to novel-writing. Indeed, his first publications were single sermons. Melvyn New, who cannot be praised enough for publishing the first scholarly edition of his sermons, claims that it is “foolish” to “argue Sterne’s uniqueness as a sermon-writer”. Others have followed suit (e.g. Elizabeth Kraft). However, Paul Goring and Judith Hawley have cogently argued that Sterne’s contemporaries often commented on the distinctiveness of Mr Yorick’s sermons. As a scholar who has devoted much of her research to eighteenth-century English pulpit literature, I have long had the impression that some of Sterne’s sermons stand out above all the rest. I therefore propose to test this intuition/assumption through a comparison between Sterne’s homiletic discourse and a corpus of contemporary sermons, in order to assess whether it is possible to reconcile the viewpoints of New and Goring. My working hypothesis is that Sterne as a preacher may have dealt with typical homiletic ideas in a very original (hence ‘eccentric’) idiolect. His innovation may be more stylistic than doctrinal. Besides, eccentricity seems to be the characteristic feature of only a relatively small number of Sterne’s sermons, which strike the reader as being more narrative, imaginative or even novelistic texts than standard post-Restoration pulpit oratory. Most of his other homilies sound much more conventional. Whether this is due to extensive plagiarism -- as first systematically analysed by Lansing Van der Heyden Hammond -- or to typically Latitudinarian theology, as argued by New, remains to be seen. Sterne’s printed sermons will be compared with a full-text corpus of eighteenth-century English sermons, comprising works by Jonathan Swift, John Wesley, perhaps George Whitefield, the manuscripts of John Sharp (1723-92), and a subset of political sermons published in the first two decades of the century. Furthermore, internal comparisons between the collections of sermons which Sterne himself prepared for publication after the first instalment of Tristram Shandy and the three posthumous volumes published by his daughter are also necessary. This paper will be based on the approach developed by the predominantly French school of lexicometry and stylometry, which emphasizes the use of exploratory multivariate statistical methods such as correspondence analysis and cluster analysis. For lack of a single software program that would ideally carry out all the necessary tasks, several packages will be used, especially Hyperbase, Lexico3, Weblex, Wordmapper, maybe Wordsmith. The linguistic and stylistic features identified by Douglas Biber as underlying different text types, especially the dimensions labelled by him as “narrative versus non-narrative concerns,” “informational versus involved production,” and “persuasion” will be explored.",
       "article_title":"Eccentricity and / or Typicality in Sterne’s Sermons ? Towards a Computer - Assisted Tentative Answer",
       "authors":[
          {
             "given":"Françoise ",
             "family":"DECONINCK-BROSSARD",
             "affiliation":[
                {
                   "original_name":"Université Paris X, U.F.R.     d’Etudes anglo-américaines",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Background In a paper presented at ACH/ALLC 2005, Allen H. Renear et.al. describe a problem of potentially great significance (Renear 2005). They argue that: “In ordinary linguistic communication we often use a name to refer to something in order to then go on to attribute some property to that thing. However when we do this we do not naturally construe our linguistic behavior as being at the same time an assertion that the thing in question has that name. (Ibid, p. 176)” Further, they claim that this distinction is over-looked when conceptual models based on encoded texts are developed. In our work at the Unit for Digital Documentation at the University of Oslo, we have used XML encoded material as sources for several of our databases (Holmen 1996, Holmen forthcoming). The way this is done is by marking up texts both descriptively and interpretatively, followed by the use of software to extract information which is included in the databases. If Renear’s argument is correct, we may infer that the databases include assertions which are based on information in the source texts that is, strictly speaking, not grounded in these texts. For example, we could be using a text as the source of a naming in the database while the naming is merely exhibited, and not asserted, in the text. The false resolutions Renear et.al. propose three possible resolutions to this problem, but they also state that all of these are false. Their resolutions are the following: 1. TEI encoding represents features of the text only. 2. The use of two arcs, i.e. “The Semantic Web community solution”, which will be discussed below. 3. Exhibition is a special case of presupposition. Based on the description of our work above, it should be obvious that resolution no. 1 is not an alternative for us. Semantic modelling of the real world on the basis of descriptions in texts is part of our work. I find it difficult to understand how resolution no. 3 may represent a possible solution. Whether exhibition is a type of presupposition or not does not change the basic problem; i.e. in our case, the use of a text as the source of a naming which is merely exhibited in the text. The problem remains the same if the naming is also presupposed in the text, as long as it is not asserted. I claim that resolution no. 2 is not false after all, and below I will demonstrate how the Conceptual Reference Model (CIDOC-CRM) will solve a similar problem in my example text. The CIDOC-CRM is a ontology developed in the museum community to be used for cultural heritage documentation. My example text In this paper, no general solution to the problems identified above will be proposed. However, I believe that the special solution that I propose could easily be generalized. The text used in my example is based on the work of Major Peter Schnitler. In the 1740s, Major Schnitler was appointed by the Danish government to explore the border area between the northern parts of Norway and Sweden/Finland. Significant parts of the text in the manuscript that he handed over to the Danish government consist of transcripts of local court interviews which were carried out by Schnitler in order to gather information about the local population as well as what they had to say about the border areas. The material includes information directly relevant to the border question, as well as general information of the areas in question, which corresponds to similar material collected through work carried out in Europe at the time (Burke 2000, pp. 128 f.). The text fragments below are taken from the very first meeting described in the text (English translation from Danish by me): [1] Of the Witnesses, supposed to be the most Cunning on the border issue, Were and stood up in the court 1: Ole Larsen Riise. [2] For these the Kingly order was read out loud [...] and they gave their Bodily Oath [3] Question: 1: What his name is? Answer: Ole Larsen Riise (Schnitler 1962, p. 1) In these quotes, we find that several facts are asserted by the text. Excerpt 1 claims the existence of a witness. We will call this witness x. Being a witness implies being a person. Thus, x is a person. We may also note that x is referred to by using the name “Ole Larsen Riise.”, abbreviated “OLR” below. Excerpt 2: Person x gave an oath to speak the truth. Excerpt 3: Person x, according to the text, claims that his name is OLR. The source of the naming is person x, as spoken out loud at a specified place at a specified date in 1742. The text puts forward an assertion by person x that he is named OLR. Modelling the semantic content from our perspective My semantic model of these facts will include the following information: It is easy to describe the source of the three first assertions through CIDOC-CRM, by stating that they are documented in Schnitler’s text: In this figure, as well as in the next one, the boxes with names starting with E represents entities, while the boxes with names starting with P represents the properties linking them together. But how do we describe the source of the naming event? We start with the event in which the attribute was assigned (the naming event, a speech act), which is an E13 Attribute assignment which states that x carried out this particular speech act: Figure 2 When looking at these two model figures, it is striking to what extent the modelling of the giving of the oath in Figure 1 compares to the naming of x in Figure 2. The explanation is that those are similar situations. Our traditional way of reading made us structure them differently in the table above, whereas represented in the CIDOC-CRM structure they came out the same in Figure 1 and 2. In order to show clearly in what way they correspond, note that line 4 in the table above could be rewritten as follows: Assertion Source 4) x named himself ORL The text This is a good example of the way modelling may help us understand a text better. What we have done is to rethink the difference between an event (x gave an oath) and a fact (ORL is the name of x). In order to model the fact correctly, i.e. to show that it was exhibited rather than asserted in the text, we had to consider it as a naming event. Considering it as an event is more feasible in that an event typically has actors who are responsible for the outcome. Further, this makes more sense in that both expressions are speech acts. When it is considered as a speech act, the naming event is the same kind of event as the giving of an oath. Why solution 2 is not false after all In order to be able to see the problem with Renear’s solution no. 2, or to realize that the problem is not really there, we have to quote his text in extensio: “Another approach, this one anticipated from the Semantic Web community, is simply to insist on an unambiguous corrected conceptual representation: one arc for being named “Herman Melville”, one for authoring Moby Dick. But this resolution fails for the reasons presented in the preceding section. Although this model would be in some sense an accurate representation of “how the world is” according to the document, it would not represent what is asserted by the document. The authorship arc in the corrected RDF graph model will correspond to relationships of exhibition, not assertion; and there is no accommodation for this distinction in the modelling language. (Renear, p. 178)” In the first couple of sentences in this paragraph, the resolution of using an “unambiguous corrected conceptual representation” is said to have failed. The next couple of sentences weakens his statement by saying that only RDF does not accommodate this; “there is no accommodation for this distinction in the modelling language” (my emphasis). There are no arguments to support why a different modelling language could not solve the problem. In fact, the CIDOC-CRM does solve this, by giving the modeller an opportunity to state explicitly who is the source of an assertion, as demonstrated in Figure 2. In the example above, we knew who made the assertion exhibited in the text. But even if we did not know, we could still make a similar model as long as we accept that it was made by somebody. In CIDOC-CRM, the modelling of entities we infer to exist without knowing who or what they are is quite possible. Generalization The example described above is quite special, as it includes an explicit naming. But it can be argued that all person names, at least in 18th century Scandinavia, are based on naming events, as people are baptised. As long as we believe that this is the case, we can include in the model an explicit attribute assignment event as the one in Figure 2 for each name used in the text. This will be an event of which we do not know who carried it out or when it took place, but that is not necessarily a problem. The will always be things we do not know in historical texts. The naming event we model this way will also be an event that is not documented in the text we are basing the model on. Whether this is acceptable is a decision one has to take when building up such a model. Conclusion There is reason to believe that the problem described in Renear’s paper is an important one. But a solution to the problem has been identified. I have shown that for one specific type of text, the problem may be solved by using CIDOC-CRM modelling including explicit statements of the sources of the assertions exhibited in the text. Further research may disclose whether this solution will work for other types of texts as well.",
       "article_title":"The Exhibition Problem. A Real Life Example with a Suggested Solution",
       "authors":[
          {
             "given":"Øyvind ",
             "family":"EIDE",
             "affiliation":[
                {
                   "original_name":"Unit for Digital Documentation at the Faculty of Arts, University of Oslo",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In the work of the TEI Ontologies SIG there have been an interest in finding practical ways of combining TEI encoded documents with CIDOC-CRM models. One way of doing so is including CIDOC-CRM information in a TEI document and linking CIDOC-CRM elements to TEI elements where appropriate. In this paper, this method is described through an example, together with an outline of the additional elements necessary in the TEI DTD used. Background In projects at the Unit for Digital Documentation, University of Oslo, we have created SGML and later XML encoded versions of printed and hand-written museum documents, such as acquisition catalogues, for more than ten years (Holmen 1996). To be able to store such documents in a standard format, we are planning to use TEI. Much of our material are archaeological documents, and there have been a growing interest in the use of XML in general and TEI in specific in archaeological community the last few years (Falkingham 2005, sec. 3.3, cf. sec. 4.3 and 5.2.3). We also use CIDOC-CRM as a tool for modelling the contents of such tagged documents as they are read by museum professionals. We use this method to be able to include information from XML encoded documents in our museum inventory databases, with references back to the encoded documents (Holmen forthcoming). We would like to store CIDOC-CRM models in close relation to the TEI encoded document. This paper describes an example of how we try to define a syntax in which to store such datasets. Extension of a TEI DTD There are two different ways in which to extend a TEI DTD for inclusion of CIDOC-CRM models. We may include an element for each and every entity and property used in the model, or we may just include one TEI element for CIDOC-CRM entities and one for properties. We have chosen the latter method. This gives a limited and rather simple expansion of the DTD. This is similar to the way the XML version of the bibliographic standard MARC is designed (MARCXML). This method will make it possible to create one document storing both textual markup and semantic interpretations of a text, while keeping the two parts of the document separate, except for links between specific elements in the two parts. This means that the document can be published as a text as well as form the base of an import to a database of records based on the interpretation, keeping the links back to the original text. In this paper, we use a DTD fragment to show an outline of the extensions we need. The extensions is composed of a root crm element including a number of crmEntity elements and a number of crmProperty elements. Example of use A typical situation in which this approach could be used is in archaeological documents. We have created a short dummy document containing some of the informations types commonly existing in our museum documents, as shown in Example 1. The excavation in Wasteland in 2005 was performed by Dr. Diggey. He had the misfortune of breaking the beautiful sword in 30 pieces. There are many objects and relations of interest when modelling the archaeological world described in this text. A typical museum curator reading could include the elements shown in Table 1. 1. A place identified by a name documented in n1. 2. A person identified by a name documented by n2. 3. A time identified by a date documented in d1. 4. An event (the excavation) documented in e1. 5. An event (the breaking) documented in e2. 6. An object (sword) documented in o1. 7. Dr. Diggey participated in the excavation 8. Dr. Diggey and the sword participated in the breaking 9. The excavation took place at the place identified by a name documented in n1 and at a time identified by a date documented in d1. Table 1 A possible CIDOC-CRM representation of one of the entities in Table 1, the excavation in line 4, is shown in Example 3. Included are also references to lines 2, 3, 7 and 9. Note that Example 3 is only showing part of a model that would represent a normal archaeological reading of the paragraph above. E.g., the date should have a “is documented in” property such as the ones for the activity and the person, and the place (Wasteland) should be documented in a way similar to the person Dr. Diggey. 1) archaeological excavation 2) Dr. Diggey 3) the element identified by the id “n2” in the text of Example 2 above 4) 2005 5) the element identified by the id “e1” in the text of Example 2 above Example 3 Example 4 shows this using the TEI-CRM syntax outlined in the DTD addition above. The crm element holds the small CIDOC-CRM model we have expressed in a TEI syntax, while the link element holds connections between the CIDOC-CRM model and the TEI text from Example 2. In this example we see that although all the CIDOC-CRM information may be expressed in such a syntax, an XML validation of the document will only validate a part of the information. It will not check whether the model adheres to the rules for e.g. which CIDOC-CRM properties may be used in connection to which entities. Conclusion and further research While different uses of ontological models in connection to TEI documents will differ in their technical solutions, e.g. whether the ontological model rests in a separate document or not, and which syntax is chosen for the model, the three main elements shown here have to be present: • a TEI document • an ontological model expressed in some XML syntax • link elements to connect the specific elements from the two together We have described a way of expanding TEI that gives us the tools we need to include a CIDOC-CRM model in a TEI document, and connect specific CIDOC-CRM entities to specific TEI elements in the non-CRM part of the document. We would like to see research into similar methods of connecting informations in other ontological systems to TEI documents, to discover whether a similar method is feasible. It would also be interesting to see if it is possible to make a general addition to TEI for this use, or if each ontological system needs its own tag set. In our own research, we will write out an ODD to test this method on samples of our own data, and then continue to implement this model on real data, so that the usability of this method for complete documents and CIDOC-CRM models can be examined.",
       "article_title":"TEI, CIDOC - CRM and a Possible Interface between the Two",
       "authors":[
          {
             "given":"Øyvind ",
             "family":"EIDE",
             "affiliation":[
                {
                   "original_name":"Unit for Digital Documentation at the Faculty of Arts, University of Oslo",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Christian-Emil ",
             "family":"ORE",
             "affiliation":[
                {
                   "original_name":"Unit for Digital Documentation at the Faculty of Arts, University of Oslo",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"At the Humanities Faculty of University of Pisa, we started a big project devoted to the study of the language and culture of the young people. The enquiries were initially held in the area of provenance of students of the University of Pisa, including the districts of Massa-Carrara, Lucca, Pistoia, Pisa, Livorno, Grosseto and the district of La Spezia in the Liguria region. We distributed a questionnaire among students of the last two years of secondary school (around 18 years old). The enquiries took place in towns that host secondary schools, i.e. towns and big villages of the urbanized country. The questionnaire includes: 1. a socio-linguistic section enquiring on the social condition, cultural preferences and style of life; usage of the dialect inside the family and usage of forms exclusive for young people; 2. a lexical section that counts 36 onomasiologic questions (“How do you say for…”) referred to the different spheres of the life of young people (family, school, external world, interpersonal relations, judgements, etc.) 3. open fields for spontaneous linguistics insertions. The enquiries involved 2.500 informants and produced over 70.000 forms. Because of this huge mass of data we have looked for a suitable storage solution that would have let us easily query the data. First of all, we wanted to query the forms themselves in a quick and simple way. Secondly, we wanted to group the results according to different parameters such as place of enquiry, sex of the informants, and socio-cultural divisions. Others goals were the possibility of measuring the dialectal or the mass-media language influence and focalizing any lines of development of the Italian language. It was immediately clear that the results of the enquiries could not be directly reversed into a digital format because of a lack of structured information. Furthermore, we felt the need of analyzing and classifying the produced forms from a linguistic point of view, for example tracing every form to the relevant lemma and recognizing the grammar category in order to enable sophisticated queries. Firstly we tried to mark data in the XML language, using the TEI Terminological Databases tag set, but this attempt showed some limitations from the very beginning. As we know, XML works at its best with semi-structured data such as texts, on the contrary our lexical entries are strongly structured data, and are connected with both linguistic information and personal data supplied by informants. Performing such links through XPath or ID-IDREFS system proved to be quite a farraginous mechanisms. Furthermore the TEI Terminological Database tag set offers just generic elements, providing poor description of lexical entries; the TEI Consortium itself has considered unsatisfactory such tag set that will be strongly revised in the forthcoming P5 version (see http://www.tei-c.org/P5/Guidelines/TE.html). In addition, reversing in XML the data pertinent to two points of enquiry, we obtained so large files that most common XML applications showed remarkable difficulties in managing them. For all these reasons, we decided to reverse the data in a MySQL relational database that would have let us to bypass such limitations. A relational DB is certainly a more suitable and performing solution for strongly structured data as we conceived our entries. We called our database BaDaLì, acrostic for Banca Dati Linguagiovanile, but also an expression that means ‘look over there’ or ‘mind you’ or even, in an antiphrastic sense very common in young people language, ‘never mind’. Figure 1 shows a simplified diagram of the BaDaLì database. BaDaLì can be ideally subdivided in four main modules: the informants module (green area), the questionnaires module (yellow area), the lemmas module (blue area) and the forms module (orange area); table lingue (‘languages’) is a lookup shared by lemmas and forms modules, while table inchieste (‘enquiries’) connects the informants module with the questionnaires module. Forms module The very central point of the database is the table forme (‘forms’) and contains the forms produced by informants. Every form is traced to its grammar category (categorie grammaticali table). In some cases the form is also related to a specific dialect (dialetti table), or to a language (lingue table) in case of foreignism. In this way an eventual existing distance between form and lemma (that we call gradiente ‘gradient’) is measured in terms of dialectal influence, foreign features or innovative traits on a graphical level. The forms module is connected to all other modules. Every form, in fact, is traced to its relevant lemma, is produced by an informant, under the stimulation of a question. Lemmas module Tracing forms to their relevant lemmas is a crucial point, especially for innovative forms not recorded by dictionaries. For this reason, we selected a reference dictionary and established a number of criterions to create the suitable lemma for the unattested occurrences. We decided to adopt the most complete dictionary of modern Italian, the Grande Dizionario Italiano dell’Uso (Gradit), edited by Tullio De Mauro. When a new lemma is inserted in the database, specific codes are added to mark its absence or a semantic innovation in regards of Gradit. Relevant lemmas (lemma table) have been categorized too, following an updated version of the classification of lexical components of the language of young people (componenti lessico table) proposed by Cortelazzo 1994. Informant module Table parlanti (‘informants’) collects all the information pertinent to the informants. A number of questions pointed out the need of typifying data collected by the enquiry. For example, the questionnaire asked the informer to declare his/her birthplace and residence. The result was a list of towns and villages that had little relevance in the case of a large scale enquiry. As the question about birthplace was included to retrieve information about the origin of the informant’s family in order to determine if an influence of a non local dialect can be assumed, we decide to group answers in macro-regional categories. The question on the residence was introduced to retrieve information about the commuting, to enquire on eventual differences between the language of towns and small villages. As the secondary schools where the enquiries were held are located in towns or in big villages, we decided to consider only if the informant lives in the same town where the school is located or elsewhere. In such cases a relative loss of information is compensated for the opportunity of comparing the results of different points of enquiry. Questionnaires module Questionnaires module collects data about questionnaires and questions. We took into account the possibility of inserting lexical entries produced by different questionnaires or by updating of our questionnaire. Therefore the term enquired by the questions (voce indagata table) is isolated. For example, in case of the question “How do you say for money?”, the word “money” is the enquired term; the comparison of forms produced by different questionnaires enquiring on the same term is easier by isolating the enquired term form the body of the question. Our database includes onomasiologic questions (‘How do you say for…’) and thematic questions (‘Which words do you know about…’), so a classification for different types of questions (tipi domanda table) was needed. BaDaLì public interface The database is currently freely available on the Web at the address http://dblg.humnet.unipi.it. [FIG. 2] BaDaLì home page The provided interface allows a number of possible queries: 1. starting from a lemma (Tipi lessicali), it is possible to retrieve all the forms traced to such a lemma; a further step allows grouping the results according to three parameters: place of the enquiry, sex, and kind of school. 2. starting from the enquired term (Voci Indagate), it is possible to retrieve all the forms produced under the stimulation of such a term. A further step allows grouping the results according to the same three parameters as in 1. 3. starting from a form (Forme), it is possible to retrieve forms grouped according to the three parameters as in 1. Modelling is certainly a crucial point in designing new projects, since the design will determine from the very beginning which requests a tool will be able to satisfy. In this frame we propose our experience, hoping to stimulate a reflection on such a topic.",
       "article_title":"Modelling Lexical Entries: a Database for the Analysis of the Language of Young People",
       "authors":[
          {
             "given":"Fabrizio ",
             "family":"FRANCESCHINI",
             "affiliation":[
                {
                   "original_name":"University of Pisa - Department of Italian Studies",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          },
          {
             "given":"Elena ",
             "family":"PIERAZZO",
             "affiliation":[
                {
                   "original_name":"University of Pisa - Department of Italian Studies",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In this paper, we argue that, although collaborative indexing of cultural resources shows promise as a method of improving the quality of people’s interactions with those resources, several important questions about the level and nature of the warrant for basing access systems on collaborative indexing are yet to receive full consideration by researchers in cultural informatics. Specifically, we suggest that system designers look to three cognate fields---classification research, iconographical analysis, and annotation studies---for pointers to criteria that may be useful in any serious estimation of the potential value of collaborative indexing to cultural institutions. Collaborative indexing (CI) is the process by which the resources in a collection are indexed by multiple people over an ongoing period, with the potential result that any given resource will come to be represented by a set of descriptors that have been generated by different people. One community of researchers that has demonstrated heightened, ongoing interest in collaborative indexing is that which is active in cultural informatics, and specifically in the design and development of systems that provide patrons of cultural institutions such as libraries, archives, and museums with networked access to digital representations of the objects in institutions’ collections (see, e.g., Bearman & Trant, 2005). Justifying the collaborative-indexing approach At a simple level, the quality of any cultural information system (or any component of a system such as an indexing mechanism) may be evaluated by considering its performance in relation to three imperatives, each of which corresponds to a separate aspect---cultural, political, economic---of the complex mission of contemporary cultural institutions. 1. How effectively does the system allow its users to find the resources in which they have an interest, and to derive optimal value from those resources once found? In order that patrons derive positive value from their experience of interacting with the objects preserved in institutions’ collections, they should be actively supported in their efforts to develop an interpretive understanding of those objects and the contexts in which they were produced---both by being given high-quality access to information about (including visual images of) objects, and by being encouraged to express their understanding and share their interpretations with others. 2. How broadly and inclusively does the system serve all sections of its parent institution’s public? Managers of cultural institutions commonly express a concern that the opportunity to derive positive value from the services offered by those institutions should be distributed justly among, and enjoyed equally by, members of all social groups. 3. How well does it do at delivering maximal quality at minimal cost? The institution which consistently allows the costs incurred in the collection, preservation, interpretation, and provision of access to its resources to exceed the value of the benefits enjoyed by its public will not survive for long. Justifications of the CI approach tend to proceed by drawing attention to the ways in which it can be viewed as responding to one or other of these three imperatives. Proponents commonly highlight several distinctive characteristics of CI in this regard: (a) CI is distributed: No single person is required to index all resources; no single resource needs to be indexed by all people. (b) CI is cheap: Indexers typically volunteer their efforts at no or low cost to collection managers. (c) CI is democratic: Indexers are not selected for their expertise by collection managers, but are self-selected according to indexers’ own interests and goals. (d) CI is empowering: People who might in the past have been accustomed to searching databases by attempting to predict the descriptors used by “experts” are now given the opportunity to record their own knowledge about resources. (e) CI is collaborative: Any given record is potentially representative of the work of multiple people. (f) CI is dynamic: The description of a given resource may change over time, as different people come to make their own judgments of its nature and importance. All of these characteristics are relevant, in various combinations and to various degrees, to any estimation of the success with which CI-based systems are likely to meet the cultural, political, and economic imperatives described above. But each additionally raises issues of a more problematic nature than is typically admitted. Given the distributed nature of CI, for example, how can it be ensured that every resource attracts a “critical mass” of index terms, rather than just the potentially-quirky choices of a small number of volunteers? Given the self-selection of indexers, how can it be ensured that they are motivated to supply terms that they would expect other searchers to use? Empirical, comparative testing of the utility of different prototypes---focusing, for example, on forms of interface for elicitation of terms, or on algorithms for the ranking of resources---is undoubtedly an essential prerequisite for the future development of successful CI-based systems (Bearman & Trant, 2005). But it is also important, we argue, that the results of prior research in a variety of cognate fields be taken into account when addressing some of the more problematic issues that we have identified. Classification research In classification research, for example, it has long been argued that indexers and searchers benefit from having the opportunity to browse or navigate for the terms or class labels that correspond most closely to the concepts they have in mind, rather than being required to specify terms from memory (see, e.g., Svenonius, 2000). Indexer--searcher consistency, and thus retrieval effectiveness, can be improved to the extent that a system allows indexers and searchers to identify descriptors by making selections from a display of the descriptors that are available to them, categorized by facet or field, and arranged in a hierarchy of broader and narrower terms so that the user can converge on the terms that they judge to be of the most appropriate level of specificity. Current implementations of CI-based systems shy away from imposing the kind of vocabulary control on which classification schemes and thesauri are conventionally founded: the justification usually proceeds along the lines that indexers should be free, as far as possible, to supply precisely those terms that they believe will be useful to searchers in the future, whether or not those terms have proven useful in the past. Yet it remains an open question as to whether the advantages potentially to be gained from allowing indexers free rein in the choice of terms outweigh those that are known to be obtainable by imposing some form of vocabulary and authority control, by offering browsing-based interfaces to vocabularies, by establishing and complying with policies for the specificity and exhaustivity of indexing, and by other devices that are designed to improve indexer--searcher consistency. Theories of iconographical interpretation Another related subfield of library and information science is that which is concerned with the effective provision of subject access to art images (see, e.g., Layne, 1994), and commonly invokes the theory of iconographical interpretation developed by the art historian Erwin Panofsky (Panofsky, 1955). Current implementations of CI-based systems for art museums focus on eliciting generic terms for (what Panofsky calls) pre-iconographic elements, i.e., pictured objects, events, locations, people, and simple emotions---the assumption apparently being made that such terms are those that will be most useful to searchers (Jörgensen, 2003). There is very little evidence supplied by studies of the use of art image retrieval systems, however, to suggest either that pre-iconographic elements are indeed what non- specialists typically search for, or that generic terms lead non-specialist searchers to what they want. We do know from analyses of questions that visitors ask in museums that non-specialists typically do not have the specialist vocabulary to specify precisely what they are looking for (see, e.g., Sledge, 1995). This does not necessarily mean, however, that searchers always default to using pre-iconographic terms whenever they wish to get at more complex themes and ideas, nor that searches for higher-level elements using pre-iconographic terms will be successful. Further studies of the question- formulating and searching behavior of non-specialist art viewers and learners are clearly necessary. Annotation studies Researchers in the human--computer interaction (HCI) community are continuing to develop an agenda for work in the emerging subfield of annotation studies (see, e.g., Marshall, 1998), focusing on ways to improve interfaces that support annotation behavior of a variety of kinds, in a variety of domains. In this research, an annotation is commonly considered as evidence of a reader’s personal, interpretive engagement with a primary document---a form of engagement that is not so different from that which cultural institutions seek to encourage in their patrons. A cultural annotation system that allowed patrons not only to supply their own descriptions of an institution’s resources, but also to add comments and to build communities around personal collections, could be envisaged as a vital service that would help patrons interact with and interpret those resources, largely outside the authority and control of curators and other specialists. It remains an open question as to whether a system that allows patrons to supply their own descriptions of institutions’ resources is most appropriately evaluated as a tool for creating and accessing personal annotations, as a tool for sharing and accessing collaborative descriptions, as a retrieval tool pure and simple, or some combination of all three. Unfortunately, our understanding of the purposes and intentions of users of CI-based systems is still spotty, and further research in this area is necessary. Conclusion In general, we suggest that particular care needs to be taken by cultural institutions in examining and adjudicating between potentially conflicting motives for inviting patrons to provide basic-level descriptions of resources. Classification research shows us that simple assignment of single-word descriptors unsupported by vocabulary control or browsable displays of the semantic relationships among descriptors is not enough to guarantee effective access; theories of iconographical interpretation demonstrate how important it is that non-specialist indexers should not be led to assume that listing what one sees is somehow all that art-viewing and meaning-making involves; and annotation studies encourage us to consider how cultural institutions may go beyond simple systems for collaborative description, and develop more-sophisticated systems for truly collaborative annotation that support deeper levels of interpretation and learning.",
       "article_title":"Collaborative Indexing of Cultural Resources: Some Outstanding Issues",
       "authors":[
          {
             "given":"Jonathan ",
             "family":"FURNER",
             "affiliation":[
                {
                   "original_name":"OCLC Online Computer Library Center, Inc., Dublin, OH, USA",
                   "normalized_name":"Online Computer Library Center",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02nv42w72",
                      "GRID":"grid.423017.3"
                   }
                }
             ]
          },
          {
             "given":"Martha ",
             "family":"SMITH",
             "affiliation":[
                {
                   "original_name":"Information School, University of Washington, Seattle, WA, USA",
                   "normalized_name":"University of Washington",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00cvxb145",
                      "GRID":"grid.34477.33"
                   }
                }
             ]
          },
          {
             "given":"Megan ",
             "family":"WINGET",
             "affiliation":[
                {
                   "original_name":"School of Information and Library Science, University of North Carolina at Chapel Hill, NC, USA",
                   "normalized_name":"University of North Carolina at Chapel Hill",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0130frc33",
                      "GRID":"grid.10698.36"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The Septuagint is in many ways a remarkable collection of texts. It represents the first known attempt to translate the Hebrew Bible into an Indo-European language, namely Hellenistic Greek. As such, it functions as an invaluable source for understanding pertinent linguistic, translational, text-critical, socio-cultural, and philosophical-theological issues that led to its creation and reception. Spanning in its inception from the first translations in the early- to mid-third century BCE to the later revisions in the second century CE, it gives scholars an insight not only into the development of the Greek language, but also into the influence of a Semitic language on its vocabulary and possibly even its syntax. Furthermore, being one of the rare cases where both a translated Greek text and the Semitic source text are extant, it also offers a rich source of insight into contemporary translation techniques and philosophies, albeit influenced by its hagiographic status, and helps in establishing the possibility of a clearer understanding of other Greek texts that are generally deemed to be translations from Semitic originals. Last, but not least, is its reflection of the culture and ideology of diaspora communities in the eastern Mediterranean metropoles, which led to the emergence and shaping of two important religious groupings. The Septuagint is also amongst the ancient texts to receive early concerted applications of Humanities Computing approaches. The most prominent project in this line is the Computer Assisted Tools for Septuagint Studies (CATSS) project, which was called into life through the initiative of the International Organization for Septuagint and Cognate Studies in the early 1970s. Located at the University of Pennsylvania’s emerging Center of Computer Analysis of Texts (CCAT), this project sought the use of computing resources towards three goals: (1) a morphological analysis of the Greek text, resulting in a tagged electronic text, (2) the comparison of Hebrew and Greek texts, resulting in an electronic parallel aligned Hebrew and Greek text, and the recording of published critical variants, resulting in an electronic Greek texts, with variants encoded. All these texts are now freely available (upon the signing of a user agreement/declaration) at the CCAT’s gopher and form the basis of most, if not all, current Septuagint projects and studies making use of computing. Departing from this pioneering, indispensable, and foundational application of Humanities Computing approaches to the study of the Septuagint, this paper will present an appraisal of the Humanities research questions presently asked of these texts, on the one hand, and of the potential of applying Humanities Computing in answering them, on the other. Beyond the widely agreed proposals of transforming such resources into established formats, e.g. Unicode for character encoding and TEI XML for text encoding, I will seek to discuss concrete problems and possibilities in pursuing Humanities Computing applications to the Septuagint, while generalising some of the insights within a wider context. The wider question will be: What should be done to and with electronic text(s) of the Septuagint in order to enrich it as a resource for answering the philological, historical, socio-cultural, and theological questions currently asked about it? One of the aims of this paper will be to tease out the current disciplinary boundaries between traditional Humanities approaches and emerging Humanities Computing ones and to identify important developments in their relationship. An important presupposition in this discussion will be the understanding that Humanities Computing, as a hybrid discipline, will only be truly successful if it reflects a thorough understanding of both Humanities research questions and Computing approaches and develops a balanced negotiation of models and concepts that successfully bridge between the two. The direction of proposing research questions has to be pursuit in both directions – both ‘how can one exploit Computing approaches to answer Humanities questions?’ and ‘How do Computing approaches alter the Humanities questions we ask about a research object?’ have to be asked. To push further the metaphor in the name of the aforementioned CATSS project: It is a matter of using computing approaches as collaborators, rather than as mere assistants. There are a number of issues in the case of the Septuagint that complicate straightforward conceptual models. To choose but one illustration: Both the textual bases of Hebrew source text and Greek translation text are disturbed not only by the textual variants on each side, but also by the fact that the Septuagint texts soon encountered rival Greek translations, in some cases clearly influencing later revisions. Furthermore, the Hebrew source text for the Septuagint clearly departs occasionally from the Hebrew Masoretic Text. Moreover, the Septuagint also includes a number of apocryphal books, some of which were probably written directly in Greek. It is evident that the conceptual model to deal with this scenario cannot just consist of the juxtaposition of two clearly delimited texts. But how does one model such a complicated picture and how does one approach such a picture by computational means? This paper will attempt to propose some answers to this question. It will seek to do so by sketching an ontology and incipit model to accommodate the complication. As an example of the wider context dealt with in this paper, I will discuss another, more general current development in Humanities research projects, not least influenced by contemporary communication technologies: the collaborative nature of the research undertaken. While the negotiation of consensus remains a crucial achievement and necessity in such endeavours, what are we to do with disagreement? How do we encode minority opinions and use them as a resource in computational approaches? If the underlying arguments are important: How do we record them for both agreements and disagreements?",
       "article_title":"The Septuagint and the Possibilities of Humanities Computing: from Assistant to Collaborator",
       "authors":[
          {
             "given":"Juan ",
             "family":"GARCES",
             "affiliation":[
                {
                   "original_name":"Centre for Computing in the Humanities",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction The Digital Songlines project differs from the approach taken by most others in the field of virtual heritage. While there are many examples of recreated cultural sites, most of them are of a built form, such as temples, monuments, cities and townships. They are frequently re-created in 3-dimensions with a high level of realism. On the other hand, the Digital Songlines project’s focus is on more than simple visualisation, rather its mission is to recreate an experience; a way of interacting with the simulated environment by identifying the key elements give to each place and its special cultural significance that an Aboriginal group identifies as being within their own tribal boundaries. Integrating the key cultural elements in a synthetic environment goes some way towards providing a setting for exploring otherwise inaccessible or previously destroyed significant sites. While traditional virtual heritage reconstructions frequently depend on technological solutions, Digital Songlines depends more on an understanding of the traditional cultural values attached to specific landscape by the participating Aboriginal people and then on a methodology and process for integrating those values in the digital environment with a focus on cultural relevance independent of its level of visual realism. The continuing traditional culture of Australian Aborigines is one of the most ancient in the world. Recent research suggests that it is at least 40000 years old. Europeancolonisation of Australia since the late eighteenth-century, and farming, mining, tourism and social impacts of modern civilisation have since threatened this most remarkable cultural heritage. Aboriginal cultural custodians realise the urgent need to preserve the evidence of Australian Aboriginal heritage and culture to give young and future generations of Australian Aborigines a chance to identify with their aboriginal roots. However, creating an Australian Indigenous cultural heritage environment causes some difficulties. Australian Aboriginal people perceive, in the landscape, details that non-indigenous people often fail to appreciate. Such details are very much a part of Aboriginal knowledge, spirituality and survival as well as cultural heritage. The landscape is perceived as a cultural entity, and needs to be recognised in a synthetic environment by Aboriginal people if the cultural heritage environment is to be perceived as authentic. One of the difficulties in undertaking such a task is the re-presentation of Aboriginal knowledge. There are few written records, hence it is mostly through the process of interviewing of cultural custodians that information can be gathered. This poses another problem: Aboriginal cultural custodians are not always comfortable with the traditional western research methods of interviewing and recording (AITSIS, 2000); they may prefer to tell their stories in a location which relates to the cultural context of the story, “Aboriginal reading comes out of the land, each place is a repository for information that is rarely commented upon elsewhere in the abstract but is released or stimulated by the place itself” (Strang, 2003, p200) (see figure 1). It is in this context that the Digital Songlines project has set itself the task of collecting cultural knowledge from Australian Aboriginal cultural custodians and Knowledge keepers, and, to provide a means of sharing that knowledge with future generations of Australian Aboriginal people through a virtual environment. Collecting Indigenous Cultural Knowledge The need to ‘locate’ the telling of a story by an Aboriginal cultural custodian where access to the original environment is not possible involved reconstructing some locations using computer generated 3D models. This offers the advantage of portability and flexibility. However, the success of this method relies on its acceptance by the cultural custodians of the synthetic environment as a valid context for sharing their cultural knowledge. Two steps were implemented to try to achieve this. The first step was to identify the elements of the natural landscape that gave it a cultural meaning in the eyes of Indigenous people. The second step was to define a methodology to recreate these elements in a synthetic environment in a way that cultural custodians could recognise the cultural elements. Within the context of this paper the following describes the approach used to identify the elements of the landscape; it explains the protocols for approaching Australian Aboriginal people and a way of enlisting their trust. Understanding the cultural elements of the landscape. To-date some research with Australian Aboriginal people has resulted in suspicion and mistrust. As such , it is essential to inculcate participants in any study at every stage. The Cultural Custodians are the elders of their communities and not generally familiar with virtual reality and multi-media technologies. Therefore, it is necessary to demonstrate the potential of the technology. In this report we discuss an initial study that used a virtual environment showing landscape and approximately 10,000 year-old rock art from Mt Moffat in the Carnavon Gorges National Park in Central Queensland, Australia (see figure 2). It was shown to a group of Australian Aborigines from central Queensland and their reactions observed. As the initial goal was to gain the trust of the community, no recording or formal interview took place. Initially the community was suspicious of the researchers and of the technology but after three hours of community consultation, the community gained a better understanding of the technology and, most importantly, the intentions of the researchers. Following the success of the initial contact, a cultural tour of the region was organised by two of the cultural custodians of the community. This allowed for more observation even though there could still not be any formal recording of the event. One example of an observation related to the tradition of collecting food. Gathering bush food is a cultural activity. Australian Aboriginal people don’t find food in the wild, they believe food is provided to them by Country (a Eurocentric analogy might be a form of benevolent land genie). However, there are conditions. Aboriginal people believe that one has to look after Country if one expects Country to provide for one. That is why, in their view, so many white people died of thirst and starvation in the early white settlement days, because they did not respect the Aboriginal Country law. Looking after Country means more than caring for it, it includes respecting the rituals taught by the ancestors Converting the Observations to VR The most important thing to come out of these observations was the attention to detail leading to a “contextual accuracy” that is very important to all the Aboriginal people encountered. Hence, in re-presenting a specific landscape in VR the following information should be gathered from the site. The importance of flora and fauna to a culturally recognisable landscape means more than realism alone. Realism has only a marginal effect on immersion in a virtual environment (Gower, 2003; Lee, 2004; Lombard and Ditton, 1997; Slater, et al, 1996). Hence, minimal realism yet culturally accurate virtual environments allows people without access to advanced technology, including Indigenous people in remote communities, to gain better access to the Digital Songlines Project and satisfies the needs of those communities for cultural sensitivity to representation of their stories about the land. Conclusion There are at least two issues facing the design of Aboriginal virtual heritage environments, contextual and cultural accuracy and realism. As the context is one of the most important aspects of culture sharing within Australian Aboriginal society then we need to know more about how to re-create a meaningful context in a virtual environment. It may be easier to evoke this by using reduced detail than highly realistic environments. In order to achieve this we need to learn from Aboriginal people what are the best ‘signs’ that can be used to identify their environments as unique and culturally significant. Acknowledgment This work is supported by ACID (the Australasian CRC for Interaction Design) established and supported under the Cooperative Research Centres Programme through the Australian Government’s Department of Education, Science and Training.",
       "article_title":"Capturing Australian Indigenous Perceptions of the Landscape Using a Virtual Environment",
       "authors":[
          {
             "given":"Stef ",
             "family":"GARD",
             "affiliation":[
                {
                   "original_name":"School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sam ",
             "family":"BUCOLO",
             "affiliation":[
                {
                   "original_name":"School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Theodor ",
             "family":"WYELD",
             "affiliation":[
                {
                   "original_name":"School of Design, Queensland University     of Technology Information Environments     Program, ITEE, University of Queensland     Australasian CRC for Interaction Design (ACID), Brisbane, Australia.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"This presentation proposes a procedure for using frequency comparisons to help resolve challenging word choices in French-to-English literary translations. It explores a computer-assisted approach for enhancing the human translation of literary texts by two principal and complementary means: 1) through the comparison of existing translations, when available, as metatranslational, cognitive choices; 2) through the interlinguistic comparison by word frequency of cognitively admissible word choices ostensibly available to the source-language (SL) author and the chronologically distanced target- language (TL) translator. The methodology explored here does not purport to innovate in regards to machine translation but rather attempts to show how techniques in part developed by researchers in that field can assist human translators working with literature, an area where machine translation would not normally be used. In translating “L’Illustre Magicien” (The Illustrious Magician) and “L’Histoire de Gambèr-Aly” (The Story of Gamber Aly), of Arthur de Gobineau (1816-1882), the presenter compares word frequencies in both languages—French and English—to help determine the most suitable choice where several reasonable ones exist. This procedure consists of what he calls interlingual and intralingual frequency comparisons which expand on the concept of componential analysis (CA) proposed by Newmark (Approaches to Translation, 1981) which the latter proposed as an improvement on the matrix method, also addressed by Hervey and Higgins (Thinking French Translation, 2nd ed., 2002). In one example of his CA, Newmark develops an “...‘open’ series of words...and the use and choice of such words is determined as often by appropriate collocation as by intrinsic meaning (i.e. componential analysis): this particularly applies to generic terms or head-words such as ‘big’ and ‘large’, which are difficult to analyze” (29). Newmark also notes that the word-series he chooses (bawdy, ribald, smutty, lewd, coarse, etc.) creates a problem in that it is particularly “...closely linked to any SL and TL culture in period of time and social class....” While the approach proposed here does not distinguish social class, the interlingual frequency comparisons can usually rely on corpora and subsets established from literature written in the same time period as the works being analyzed and as early translations. In this case, several different corpora are used. For French: ARTFL (American and French Research on a Treasury of the French Language) and other tools made available by Etienne Brunet and the University of Nice. For English: Bartleby; the British National Corpus (BNC), the British Women Writers Project and Chadwyk-Healy’s LION (Literature Online). Besides the two translations, the overall project includes critical essays which treat the tales’ major themes of love, death, and intellectual or emotional obsession. Honoré de Balzac’s La recherche de l’Absolu (1834) floats tempingly in the background of “The Illustrious Magician” since the latter’s author even published an essay on Balzac in 1844. While several different examples will be used for applying the frequency approach, one example of using interlinguistic frequencies occurs when translating the following French from “The Illustrious Magician”, a several pages before its conclusion: “En effet, en entrant dans une des grottes, après en avoir visité deux ou trois, il aperçut son maître assis sur une pierre, et traçant avec le bout de son bâton des lignes, dont les combinaisons savantes annonçaient un travail divinatoire” (Nouvelles asiatiques, 1876, p. 150). Focusing for the purposes of this abstract on the clause containing the word “divinatoire,” we can initially compare the two existing translations. Both were published in New York, the first by Appleton Press in 1878, the second by Helen Morganthau Fox with Harcourt Brace in 1926: 1) Appleton (259): “...the profound combination of which announced a work of divination.” 2) Fox (268): “...the learned combinations of which showed that it was a work of divination.” 3) Draft of presenter’s translation: “ whose learned combinations revealed a divinatory work.” One might wonder whether the word “divinatoire” for a French writer in the 1870’s is as recherché as the word “divinatory” in English. Would it be reasonable to substitute the collocation “divinatory work” for the earlier one, “work of divination”? The BNC reveals two (2) occurrences of «divinatory» in 100,000,000 words: 1) A6C Seeing in the dark. ed. Breakwell, Ian and Hammond, Paul. London: Serpent’s Tail, 1990 (32,621 words); 2) CS0 Social anthropology in perspective. Lewis, I M. Cambridge: Cambridge University Press, 1992, pp. 5-130, (35,946 words). In the British Women Writers Project (http://www.lib.ucdavis. edu/English/BWRP/) we find no occurrences for eighty (80) texts, 1789-1832. We should conclude that the word “divinatory” is a rare word in English. However, within the ARTFL database of French works for the years 1850-1874 alone, out of 11,214,324 words, there are four (4) occurrences. Out of the 9,548,198 words in the database for the period 1875-1899 there are ten (10) occurrences or a total of 14 in 20,762,522 words or about 67 occurrences per 100,000,000 words, about 30 times more than in the BNC. In the period 1875-1899, its rate of occurrence was about once per million words, the highest of the four quarters in the century (see http://www. lib.uchicago.edu/efts/ARTFL/databases/TLF/freqs/tlf.timerseries.html). We must conclude that the French use of “divinatoire” at the time when Gobineau was using it was significantly more common than is its English counterpart though both uses are nonetheless relatively rare. And the bibliography for the fourteen occurrences covering the period in which the Nouvelles asiatiques was published includes works by well-known authors such as Amiel, Bourget, Flaubert, Garnier, the Goncourts, Mallarmé and others, as well as Gobineau himself. The one occurrence of «divinatoire» from this last author’s work should of course be subtracted from the total before any comparison with the same publication. Intralinguistically for French, there are forty-two (42) occurrences of the word “divination,” which is exactly three times more frequent than “divinatoire” in the period 1850-1899, out of almost 21 million words. Forty-two versus fourteen in that number is almost a third of an order of magnitude and worthy of notice. We can preserve the rarer use of divinatoire from French in the translation although the usage appears to be even rarer in English. However, we should note that there are seven (7) occurrences of “divinatory” in the 20th-century poetry on Chadwyk Healy’s LION site, so the word can be well represented in the poetic genre. And in comparisons that will be made in the presentation in both vocabulary and themes between Gobineau and Balzac, the interest of both authors in divination will be highlighted. Brunet’s database shows seven occurrences of divination/divinations in the Comédie Humaine (CH) and four of divinatoire, a typical total of the two words (11) for the 4,242,038 words in that corpus (CH) and for his time, 1825-1849: 35 occurrences in the 12,352,370 words contained in the ARTFL database. However, these words are only about one-half as common in French literature for the last quarter of the nineteenth century as in the second quarter. There are 18 occurrences in 9,548,198 words for the ARTFL database during the period 1875-1899 when the Nouvelles asiatiques were published. Besides the few sample words above where the computer-assisted techniques have been applied by a human translator of French literature, the presentation will suggest how such frequency-based comparisons can assist in the translation of thematic groups of words representing the literary authors’ symbolic universes. Often such clusters of associated words can be determined from methodically searching the secondary literature: the thematic areas where critics have focused their interest over the centuries. Furthermore, a complementary technique for using advanced search engines on the Internet to aid in solving translation problems will be illustrated or demonstrated. As long as French and English databases remain available with tools that allow for the appropriate date-stamping, so to speak, of word usage, a methodology can be developed using frequencies and simple statistical tests such as z-scores for comparing the ranking of words across chronological gaps. Such resources offer new and useful tools to the translator both in aiding the development of metatranslations and justifying both them and final translations. Additionally, this process can facilitate greater detail and support for literary criticism that makes use of intertextual and intratextual linguistic materials.",
       "article_title":"French-English Literary Translation Aided by Frequency Comparisons from ARTFL and Other Corpora",
       "authors":[
          {
             "given":"Joel ",
             "family":"GOLDFIELD",
             "affiliation":[
                {
                   "original_name":"Modern Languages and Literatures,     Fairfield University",
                   "normalized_name":"Fairfield University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04z49n283",
                      "GRID":"grid.255794.8"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Contemporary stylistic and stylometric studies usually focus on an author with a distinctive style and often characterize that style by comparing the author’s texts to those of other authors. When an author’s works display diverse styles, however, the style of one text rather than the style of the author becomes the appropriate focus. Because authorship attribution techniques are founded upon the premise that some elements of authorial style are so routinized and habitual as to be outside the author’s control, extreme style variation within the works of a single author seems to threaten the validity of the entire enterprise. This apparent contradiction is only apparent, however, for the tasks are quite different. Successful attribution of a diverse group of texts to their authors requires only that each author’s texts be more similar to each other than they are to texts by other authors, or, perhaps more accurately, that they be less different from each other than from the other texts. The successful separation of texts or sections of texts with distinctive styles from the rest of the works of an author takes for granted a pool of authorial similarities and isolates whatever differences remain. Recent work has shown that the same techniques that are able to attribute texts correctly to their authors even when some of the authors’ styles are quite diverse do a good job of distinguishing an unusual passage within a novel from the rest of the text (Hoover, 2003). Other quite subtle questions have also been approached using authorship attribution techniques. Nearly 20 years ago, Burrows showed that Jane Austen’s characters can be distinguished by the frequencies of very frequent words in their dialogue (1987). More recent studies have used authorship techniques to investigate the sub-genres and varied narrative styles within Joyce’s Ulysses (McKenna and Antonia, 2001), the styles of Charles Brockden Brown’s narrators (Stewart, 2003), a parody of Richardson’s Pamela (Burrows, 2005), and two translations of a Polish trilogy made a hundred years apart (Rybicki, 2005). Hugh Craig has investigated chronological changes in Ben Jonson’s style (1999a, 1999b), and Burrows has discussed chronological changes in the novel genre (1992a). I am using authorship attribution techniques to study the often-remarked differences between Henry James’s early and late styles.1 I begin by analyzing a corpus of 46 American novels of the late 19th and early 20th century (12 by James and 34 by eight other authors) to determine the extent to which multivariate authorship attribution techniques based on frequent words, such as principal components analysis, cluster analysis, Burrows’s Delta, and my own Delta Primes, successfully attribute James’s early and late novels to him and distinguish them from novels by eight of his contemporaries.2 Because all of these techniques are very effective in this task, all are appropriate for further investigation of the variation within James’s style, but DeltaLz produces especially accurate results, correctly attributing all 40 novels by members of the primary set in eleven analyses based on the 2000-4000 most frequent words. All of the results also reconfirm recent findings that large numbers of frequent words are more effective than the 50-100 that have been traditionally used, and that the most accurate results (for novel-sized texts) often occur with word lists of more than 1000 words (see Hoover, 2004a, 2004b). The PCA analysis in Fig. 1, based on the 1001-1990 most frequent words, clusters the novels quite well–better, in fact, than analyses that include the 1000 most frequent words. When cluster analysis, PCA, Delta, and Delta Prime techniques are applied to nineteen novels by Henry James, they show that the early (1971-1881) and late styles (1897-1904) are very distinct indeed, and that an “intermediate” style (1886-1890) can also be distinguished. DeltaLz again produces especially accurate results, correctly identifying all early, intermediate, and late novels in 24 analyses based on the 200-4000 most frequent words. These results paint a remarkable picture of an author whose style was constantly and consistently developing, a picture that is congruent with James’s reputation as a meticulous craftsman who self-consciously transformed his style over his long career. A comparison with Charles Dickens and Willa Cather shows that Dickens’s early and late novels tend to separate, but do not fall into such neat groups as James’s do, and that Cather’s novels form consistent groupings that are not chronological. These authors seem not to have experienced the kind of progressive development seen in James. It is dangerous, then, simply to assume chronological development of authorial style. Finally, these same techniques show that the heavily revised versions of The American (1877), Daisy Miller (1878), and The Portrait of a Lady (1881) that appear in the New York edition of James’s novels (1907-09) are consistently and dramatically closer to the style of the later novels. Yet even his notoriously detailed and extensive revisions do not allow PCA to group the revised early novels with the late novels. Instead, the revised versions fall at the border between the early and intermediate novels in PCA graphs (see Fig. 2), and consistently join with their original versions in cluster analyses. The results obtained using Delta show that even the errors make sense. In analyses that are not completely accurate, The Portrait of a Lady and Washington Square, the latest of the early novels (both 1881) are sometimes identified as intermediate. The other errors involve the identification of The Spoils of Pointon, the first of the late novels (1897), as intermediate; no analyses incorrectly identify an early novel as late or a late novel as early. In the analyses that are completely correct for the 19 unambiguously early, intermediate and late novels, the New York edition versions of earlier novels are identified as follows: the revised early novels The American and Daisy Miller are universally identified as early, the revised intermediate novel The Reverberator is universally labeled intermediate, and the revised early The Portrait of a Lady is usually labeled intermediate, but sometimes early. This is an intuitively plausible result, with the latest of the early novels pulled far enough toward the late novels to appear intermediate, but, interestingly enough, DeltaLz, which produces much more accurate results overall, labels all of the novels according to their original publication dates in eighteen analyses based on the 200-2800 mfw. In the remaining six analyses, the early Daisy Miller is identified as late, and in one analysis (based on the 4000mfw) The Portrait of a Lady is identified as intermediate. Further investigation of the implications of these results is ongoing. Authorship attribution techniques thus confirm the traditional distinction between early and late James, establish the existence of an intermediate style, and lay the groundwork for a fuller analysis of the linguistic and stylistic differences upon which they rest. Based as they are on a very large proportion of the text of the novels (the 4000 most frequent words typically comprise more than 94% of a novel), these results provide a wealth of material for stylistic analysis. The huge numbers of words involved will, however, require new methods of selection, analysis, and presentation if they are not to prove overwhelming and incomprehensible. Meeting these challenges will advance and refine authorship attribution techniques, and, at the same time, further illuminate the linguistic bases of James’s style and his process of revision.",
       "article_title":"Stylometry, Chronology and the Styles of Henry James",
       "authors":[
          {
             "given":"David L. ",
             "family":"HOOVER",
             "affiliation":[
                {
                   "original_name":"Department of English, New York University",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"This paper connects research by the nora Project (http://noraproject.org), a study on text mining and humanities databases that includes four sites and scholars from many areas, with current critical interests in nineteenth century American sentimental literature. The term sentimental has been claimed and disparagingly applied (sometimes simultaneously) to popular fiction in this time period since its publication; academic study of sentimental fiction has enjoyed widespread acceptance in literature departments only in the past few decades. Academic disagreement persists about what constitutes sentimentality, how to include sentimental texts on nineteenth century American syllabi, which sentimental texts to include, and how to examine sentimental texts in serious criticism. Most of the well-known and widely-taught novels of the time period exist in XML format in the University of Virginia’s Etext Center, one of the libraries in partnership with the nora Project; the original XML data for three texts discussed below was taken from this source. The term sentimental novel is first applied to eighteenth century texts such as Henry Mackenzie’s Man of Feeling, Samuel Richardson’s Pamela, and Lawrence Sterne’s Sentimental Journey and Tristam Shandy. Usually included in courses on the theory of the novel or eighteenth century literature, these works illustrate the solidification of the novel form. Sentimental novels emphasize, like Mackenzie’s title, (men and women of) feeling. Feeling is valued over reason and sentimental is used with the term sensibility (recall Jane Austen’s title Sense and Sensibility.) Although definitions of sentimentality range widely, and are complicated by the derogatory deployment of the term by contemporary and current critics, the group of texts loosely joined as being in the mid- nineteenth century sentimental period is a crucial link for humanities scholars that work on the novel and British and American texts in the nineteenth century connecting Victorian texts with their predecessors. Sentimental texts are a particularly good place to look at how a group of texts may exhibit certain recognizable features; sentimental fiction uses conventional plot development, stock characters, and didactic authorial interventions. The emphasis is the exposure of how a text works to induce specific responses in the reader (these include psychophysiological responses such as crying and a resolve to do cultural work for nineteenth century causes such as temperance, anti-slavery, female education, and labor rights); readers do not expect to be surprised. Instead, readers encounter certain keywords in a certain order for a sentimental text to build the expected response. Although the term sentimental is used to represent an area of study and title literature courses, there is no set canon of sentimental texts because scholars do not agree on what constitutes textual sentimentality. Using text- mining on texts generally considered to exhibit sentimental features may help visualize levels of textual sentimentality in these texts and ultimately measure sentimentality in any text. Two groups of humanist scholars scored three chapters in Harriet Beecher Stowe’s text, Uncle Tom’s Cabin, the most well-known and critically-acknowledged text in the group often considered sentimental. Although chapters in this text may be quite long and contain varying levels of sentimentality, the chapter as a unit was preferred as the original division structure of the text and the fact that humanist scholars expect this division and assign class reading and research by chapter units. UTC was later adapted into theatrical productions, and the idea of scenes (within chapters) may be a fruitful place to begin studying the sentimental fluctuations with a chapter unit in later phases of this project. For the initial rubric, though, chapters were scored on a scale of 1 to 10: low is 1-3, medium 4-6, high 7-10. 10 is considered a “perfect”ly sentimental score, and as such, is only to be used when the peak of sentimental conventions is exhibited: a character nears death and expires in a room usually full of flowers and mourners who often “swoon.” The training set for this experiment includes two other texts that were scored on the same sentimentality scale, Susanna Rowson’s 1794 novel Charlotte: A Tale of Truth and Harriet Jacobs’s Incidents in the Life of a Slave Girl. Since these texts were considered sentimental, most chapters were scored in the medium or high range, so the categories were changed to “highly sentimental” and “not highly sentimental.” With D2K, the Naive Bayes method was used to extract features from these texts, which we might call markers of sentimentality. Looking at the top 100 of these features, some interesting patterns have emerged, including the privileging of proper names of minor characters in chapters that ranked as highly sentimental. Also interesting are blocks of markers that appear equally prevalent, or equally sentimental, we might say: numbers 70-74 are “wet,” lamentations,” “cheerfulness,” “slave-trade,” and “author.” The line of critical argument that goes that the sentimental works focus on motherhood is borne out by “mother” at number 16 and “father” not in the top 100. As we move into the next three phases of the project, we will include stemming as an area of interest in classifying the results. Phase two will use two more novels by the same authors as those in the training set; phrase three may include ephemera, broadsides, and other materials collected in the EAF collection at the UVa Etext Center. Phase four will run the software on texts considered non-sentimental in the nineteenth century and other phases might include twentieth and twenty-first century novels that are or are not considered sentimental. We hope to discover markers that can identify elements of the sentimental in any text.",
       "article_title":"“Quite Right, Dear and Interesting”: Seeking the Sentimental in Nineteenth Century American Fiction",
       "authors":[
          {
             "given":"Tom ",
             "family":"HORTON",
             "affiliation":[
                {
                   "original_name":"University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Kristen ",
             "family":"TAYLOR",
             "affiliation":[
                {
                   "original_name":"University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Bei ",
             "family":"YU",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Xin ",
             "family":"XIANG",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. Introduction Arecent development in the study of language and gender is the use of automated text classification methods to examine how men and women might use language differently. Such work on classifying texts by gender has achieved accuracy rates of 70-80% for texts of different types (e-mail, novels, non-fiction articles), indicating that noticeable differences exist (de Vel et al. 2002; Argamon et al. 2003). More to the point, though, is the fact that the distinguishing language features that emerge from these studies are consistent, both with each other, as well as with other studies on language and gender. De Vel et al. (2002) point out that men prefer ‘report talk’, which signifies more independence and proactivity, while women tend to prefer ‘rapport talk’ which means agreeing, understanding and supporting attitudes in situations. Work on more formal texts from the British National Corpus (Argamon et al. 03) similarly shows that the male indicators are mainly noun specifiers (determiners, numbers, adjectives, prepositions, and post-modifiers) indicating an ‘informational style’, while female indicators are a variety of features indicating an ‘involved’ style (explicit negation, first- and second-person pronouns, present tense verbs, and the prepositions “for” and “with”). Our goal is to extend this research for analyzing the relation of language use and gender for literary characters. To the best of our knowledge, there has been little work on understanding how novelists and playwrights portray (if they do) differential language use by literary characters of different genders. To apply automated analysis techniques, we need a clean separation of the speech of different characters in a literary work. In novels, such speech is integrated into the text and difficult to extract automatically. To carry out such research, we prefer source texts which give easy access to such structural information; hence, we focus on analyzing characters in plays. The natural choice for a starting point is the corpus of Shakespeare’s plays. We thus ask the following questions. Can the gender of Shakespeare’s characters be determined from their word usage? If we are able to find such word use, can we glean any insight into how Shakespeare portrays maleness and femaleness? Are the differences (if any) between male and female language in Shakespeare’s characters similar to those found in modern texts by male and female authors? Can we expect the same kind of analysis in understanding Shakespeare’s characters’ gender, to the ones we discussed above? Keep in mind that here we examine text written by one individual (Shakespeare) meant to express words of different individuals with differing genders, as opposed to texts actually by individuals of different genders. To address these questions, we applied text classification methods using machine learning. High classification accuracy, if achieved, will show that Shakespeare used different language for his male and for his female characters. If this is the case, then examination of the most important discriminating features should give some insight into such differences and to relate them to previous work on male/female language. The general approach of our work is to achieve a reasonable accuracy using different lexical features of the characters’ speeches as input to machine learning and then to study those features that are most important for discriminating character gender. 2. Corpus Construction We constructed a corpus of characters’ speeches from 34 of Shakespearean plays, starting with the texts from the Moby Shakespeare1. The reason behind choosing this edition is that it is readily available on the web and has a convenient hierarchical form of acts and scenes for every play, while we do not expect editorial influence to unduly affect our differential analysis. The files collected from this web resource were converted into text files from hypertext media and then we cleaned the text files by removing stage directions. The gender of each character was entered manually. A text file for each character in each play was constructed by concatenating all of that character’s speeches in the play. We only considered characters with 200 or more words. From that collection, all female characters were chosen. Then we took the same number of male characters as female characters from a play, restricted to those not longer than the longest female character from that particular play. In this way, we balanced the corpus for gender, giving a total of 83 female characters and 83 male characters, with equal numbers of males and females from each play. This corpus is termed the ‘First Corpus’. We also built a second corpus based on the reviewer’s comments, in which we equalized the number of words in male and female characters by taking every female character with more than 200 words and an equal number of the longest male characters from each play. The longest male and female characters were then matched for length by keeping a prefix of the longer part (male or female) of the same length (in words) as the shorter part. This procedure ensured that the numbers of words per play for both genders are exactly the same. This corpus is termed the ‘Second Corpus’. We also split each corpus (somewhat arbitrarily) into ‘early’ and ‘late’ characters. We used the term early to those plays which were written in 16th century and late to those in 17th century. This chronology in plays as captured from Wikipedia1. The numbers of characters from each play for ‘First Corpus’ and ‘Second Corpus’ are shown in Table 1. 3. Feature Extraction We processed the text using the ATMan system, a text processing system in Java that we have developed3. The text is tokenized and the system produces a sequence of tokens, each corresponds to a word in the input text file. We use two sets of words as features. A stylistic feature set (FW) is a list of more-or-less content-independent words comprising mainly function words, numbers, prepositions, and some common contractions (e.g., “you’ll”, “he’ll”). A content-based feature set comprises all words that occur more than ten times in a corpus, termed Bag of Words (BoW). We calculate the frequencies of these FWs and BoWs and turn them into numeric values by computing their relative frequencies, computed as follows. We first count the number of times two different features occurring together; then we divide this number to the count of the feature in reference. In this way we calculate the relative frequency for each feature and a collection forms a feature vector, which represents a document (i.e. a character’s speech). The FW set has 645 features including contractions; the BoW set has 2129 features collected from the first corpus and 2002 BoW features collected from the second corpus. The numeric vectors collected for each document is used as an input for machine learning. 4. Text Classification The classification learning phase of this task is carried out by Weka’s (Frank & Witten 1999) implementation of Sequential Minimal Optimization (Platt 1998) (SMO) using a linear kernel and default parameters. The output of SMO is a model linearly weighting the various text features (FW or BoW). Testing was done via 10 fold cross validation. This provides an estimation of generalization accuracy by dividing the corpus into 10 different subsets. The learning is then run ten times, each time using a different subset as a test set and combining the other nine subsets for training. In this way we ensure that each character is tested on at least once with training that does not include it. Tables 3 and 4 present the results obtained by running various experiments. It is clear that BoW has performed better than the FW in both selection criteria, as expected, since it has more features on which to operate. This shows that both style and content differ between male and female characters. As expected, the FWs have proven the stylistic evidence and not the content, which are visible from the Table 4. BoW gives a high 74.09 on over all corpuses with the equalizing on number of words selection strategy. Interestingly, FW gives highest accuracy of 74.28 in Late plays with only 63 training samples. This indicates that there is a greater stylistic difference between the genders in late Shakespeare than in early Shakespeare. 5. Discussion The feature analysis phase is carried out by taking the results obtained from Weka’s implementation of SMO. SMO provides weights to the features corresponding to both class labels. After sorting the features based on their weights, we collected the top twenty features from both character genders. Tables 5-8 lists the top 20 features from male and female characters and is shown with their assigned weights given by the SMO, for FWs and BoWs respectively. Tables 9-12 list the same for the Second Corpus. These tables also show the ‘Average frequency of 100 words’, which finds the frequency of a particular feature divided by total gender characters, and then for easy readability this figure is scaled by 100 times. To discriminate binary class labels, SMO uses positive and negative weight values in Weka’s implementation. We see from the Tables 5-10, male features are designated as negative weights and female characters are given as positive weights. In top 20 male features, this can be observed that ‘Average Frequency of 100 Words’ value of male is more than the corresponding value for female. This hold same in the case of the top 20 female features where female ‘Average Frequency of 100 Words’ value is more than the male for the same feature. Feature Analysis: BoW We can see cardinal number usage is found in male characters. Plural and mass nouns (‘swords’, ‘dogs’, ‘water’) are used more in males than females. On the other hand, there is strong evidence for singular noun (‘woman’, ‘mother’, ‘heart’) usage in females. The use of ‘prithee’ as an interjection is found in female character. This may represent a politeness aspect in their attitude. The past participle form is generally found in females (‘gone’, ‘named’, ‘known’). Present tense verb forms (‘pour’, ‘praise’, ‘pray’, ‘love’, ‘dispatch’, ‘despair’) are used in female characters. In the case of male characters, Shakespeare used these verb forms (‘avoid’, ‘fight’, ‘wrought’). Male characters seem to be aggressive while female characters seem to be projected as supporters of relationships. Feature Analysis: FW We observed that Shakespeare’s female characters used more adverbs and adjectives, as well as auxiliary verbs and pronouns. On the other hand, cardinal numbers, determiners, and some prepositions are generally indicative of male characters. These observations are in line with previous work (Argamon et al. 2003) on discriminating author gender in modern texts, supporting the idea that the playwright projects characters’ gender in a manner consistent with authorial gender projection. We did observe some contrasting results in the FW features from the second corpus. Number (i.e. twice) is found in female characters. Certain prepositions are used for females, while negation only appears distinctive for early females. Determiner ‘the’ which is a strong male character indicator in first corpus is found only in early part of second corpus. Some negation (‘cannot’) is found in late males as well. Clearly, more and deeper analysis is needed. 6. Conclusion This is the first work, to our knowledge, in analyzing literary character’s gender from plays. It seems clear that male and female language in Shakespeare’s characters is similar to that found in modern texts by male and female authors (Argamon et.al 2003), but more work is needed in understanding character gender. We have also observed possible differences between early and late Shakespeare in gender character classification. In particular, the later Shakespeare plays appear to show a greater stylistic discrimination between male and female characters than the earlier plays. We are particularly interested in collaborating with literary scholars on this research to explore these issues further.",
       "article_title":"Performing Gender: Automatic Stylistic Analysis of Shakespeare’s Characters",
       "authors":[
          {
             "given":"Sobhan",
             "family":"HOTA",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science,     Illinois Institute of Technology",
                   "normalized_name":"Illinois Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/037t3ry66",
                      "GRID":"grid.62813.3e"
                   }
                }
             ]
          },
          {
             "given":"Shlomo ",
             "family":"ARGAMON",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science,     Illinois Institute of Technology",
                   "normalized_name":"Illinois Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/037t3ry66",
                      "GRID":"grid.62813.3e"
                   }
                }
             ]
          },
          {
             "given":"Moshe ",
             "family":"KOPPEL",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science, Bar-Ilan University",
                   "normalized_name":"Bar-Ilan University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/03kgsv495",
                      "GRID":"grid.22098.31"
                   }
                }
             ]
          },
          {
             "given":"Iris ",
             "family":"ZIGDON",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science, Bar-Ilan University",
                   "normalized_name":"Bar-Ilan University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/03kgsv495",
                      "GRID":"grid.22098.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. INTRODUCTION There are many networked resources which now provide critical consumer-generated reviews of humanities materials, such as online stores, review websites, and various forums including both public and private blogs, mailing lists and wikis. Many of these reviews are quite detailed, covering not only the reviewers’ personal opinions but also important background and contextual information about the works under discussion. Humanities scholars should be given the ability to easily gather up and then analytically examine these reviews to determine, for example, how users are impacted and influenced by humanities materials. Because the ever-growing volume of consumer-generated review text precludes simple manual selection, the time has come to develop robust automated techniques that assist humanities scholars in the location, organization and then the analysis of critical review content. To this end, the authors have conducted a series of very promising large-scale experiments that bring to bear powerful text mining techniques to the problem of “criticism analysis”. In particular, our experimental results concerning the application of the Naïve Bayes text mining technique to the “criticism analysis” domain indicate that “criticism mining” is not only feasible but also worthy of further exploration and refinement. In short, our results suggest that the formal development of a “criticism mining” paradigm would provide humanities scholars with a sophisticated analytic toolkit that will open rewarding new avenues of investigation and insight. 2. EXPERIMENTAL SETUP Our principal experimental goal was to build and then evaluate a prototype criticism mining system that could automatically predict the: 1) genre of the work being reviewed (Experimental Set 1 (ES1)). 2) quality rating assigned to the reviewed item (ES2). 3) difference between book reviews and movie reviews, especially for items in the same genre(ES3). 4) difference between fiction and non-fiction book reviews (ES4). In this work, we focused on the movie, book and music reviews published on www.epinions.com, a website devoted to consumer-generated reviews. Each review in epinions.com is associated with both a genre label and a numerical quality rating expressed as a number of stars (from 1 to 5) with higher ratings indicating more positive opinions. The genre labels and the rating information provided the ground truth for the experiments. 1800 book reviews, 1650 movie reviews and 1800 music reviews were selected and downloaded from the most popular genres represented on epinions. com. As in our earlier work (Hu et al 2005), the distribution of reviews across genres and ratings was made as evenly as possible to eliminate analytic bias. Each review contains a title, the reviewer’s star rating of the item, a summary, and the full review content. To make our criticism mining approach generalizable to other sources of criticism materials, we only processed the full review text and the star rating information. Figure 1 illustrates the movie, book and music genre taxonomies used in our experiments. The same data preprocessing and modeling techniques were applied to all experiments. HTML tags were removed, and the documents were tokenized. Stop words and punctuation marks were not stripped as previous studies suggest these provide useful stylistic information (Argamon and Levitan 2005, Stamatatos 2000). Tokens were stemmed to unify different forms of the same word (e.g., plurals). Documents were represented as vectors where each attribute value was the frequency of occurrence of a distinct term. The model selected was generated by a Naïve Bayesian text classifier which has been widely used in text mining due to its robustness and computational efficiency (Sebastiani 2002). The experiments were implemented in the Text-to- Knowledge (T2K) framework which facilitates the fast prototyping of the text mining techniques (Downie et al 2005). 3. GENRE CLASSIFICATION TESTS (ES1) Figure 2a provides an overview of the genre classification tests. The confusion matrices (Figure 2b, 2c and 2d) illustrate which genres are more distinguishable from the others and which genres are more prone to misclassification. Bolded values represent the successful classification rate for each medium (Figure 2a) or genre (Figure 2b, 2c and 2d). As Figure 2a shows, the overall precisions are impressively high (67.70% to 78.89%) compared to the baseline of random selection (11.11% to 8.33%). The identification of some genres is very reliable e.g., “Music & Performing Arts” book reviews (89%) and “Children” movie reviews (95%). Some understandable confusions are also apparent e.g., “Documentary” and “Education” movie reviews (31% confusion). High confusion values appear to indicate that such genres semantically overlap. Furthermore, such confusion values may also indicate pairs of genres that create similar impressions and impacts on users. For example, there might be a formal distinction between the “Documentary” and “Education” genres but the two genres appear to affect significant numbers of users in similar, interchangeable ways. 4. RATING CLASSIFICATION TESTS (ES2) We first tested the classification of reviews according to quality rating as a five class problem (i.e., classification classes representing the individual ratings (1, 2, 3, 4 and 5 stars)). Next we conducted two binary classification experiments: 1) negative and positive review “group” identification (i.e., 1 or 2 stars versus 4 or 5 stars); and 2) ad extremis identification (i.e., 1 star versus 5 stars). Figure 3 demonstrates the dataset statistics, corresponding results and confusion matrices. The classification precision scores for the binary rating tasks are quite strong (80.13% to 86.25%), while the five class scores are substantially weaker (36.70% to 44.82%). However, upon examination of the five class confusion matrices it is apparent that the system is “reasonably” confusing adjacent categories (e.g., 1 star with 2 stars, 4 stars with 5 stars, etc.). 5. MOVIE VS. BOOK REVIEW TESTS (ES3) We first formed a binary classification experiment with movie and book reviews of all genres. We then compared reviews in each of the six genres common to books and movies. To prevent the oversimplification of the classification task we eliminated words that can directly suggest the categories: “book”, “movie”, “fiction”, “film”, “novel”, “actor”, “actress”, “read”, “watch”, “scene”, etc. Eliminated terms were selected from those which occurred most frequently in either category but not both. The results in Figure 4 show the classifier is amazingly accurate (consistently above 94.28% precision) in distinguishing movie reviews from book reviews both in mixed genres and within single genre classes. We conducted a post-experiment examination of the reviews to ensure that the results were not simply based upon suggestive terms like those we had eliminated pre-experiment. Therefore, it can be inferred that users criticize books and movies in quite different ways. This is an important finding that prompts for future work the identification of key features contributing to such differences. 6. FICTION VS. NON-FICTION BOOK REVIEW TEST (ES4) As in ES3, we eliminated such suggestive words as “fiction”, “non”, “novel”, “character”, “plot”, and “story” after examining high-frequency terms of each category. The classification results are shown in Figure 5. The precision of 94.67% not only verifies our system is good at this classification task but also indicates reviews on the two categories are significantly different. It is also noteworthy that more non-fiction book reviews (9%) were mistakenly predicted as fiction book reviews than the other way around (2%). Closer analysis on features causing such behaviors will be our future work. 7. CONCLUSIONS AND FUTURE WORK Consumer-generated reviews of humanities materials represent a valuable research resource for humanities scholars. Our series of experiments on the automated classification of reviews verify that important information about the materials being reviewed can be found using text mining techniques. All our experiments were highly successful in terms of both classification accuracy and the logical placement of confusion in the confusion matrices. Thus, the development of “criticism mining” techniques based upon the relatively simple Naïve Bayes model has been shown to be simultaneously viable and robust. This finding promises to make the ever-growing consumer-generated review resources useful to humanities scholars. In our future work, we plan to undertake a broadening of our understanding by exploring the application of text mining techniques beyond the Naïve Bayes model (e.g., decision trees, neural nets, support vector machines, etc.). We will also work towards the development of a system to automatically mine arbitrary bodies of critical review text such as blogs, mailing lists, and wikis. We also hope to construct content and ethnographic analyses to help answer the “why” questions that pertain to the results.",
       "article_title":"Criticism Mining: Text Mining Experiments on Book, Movie and Music Reviews",
       "authors":[
          {
             "given":"Xiao ",
             "family":"HU",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"J. Stephen ",
             "family":"DOWNIE",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"M. Cameron ",
             "family":"JONES",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Background Before the advent of standards for generic markup, the lack of publicly documented and generally accepted standards made exchange and reuse of electronic documents and document processing software difficult and expensive. SGML (Standard Generalized Markup Language) became an international standard in 1986. But it was only in 1993, with the introduction of the World Wide Web and its SGML-inspired markup language HTML (Hypertext Markup Language), that generic markup started to gain widespread acceptance in networked publishing and communication. In 1998, the World Wide Web Consortium (W3C) released XML (Extensible Markup Language). XML is a simplified subset of SGML, aimed at retaining HTML’s simplicity for managing Web documents, while exploiting more of SGML’s power and flexibility. A large family of applications and related specifications has since emerged around XML. The scope of XML processing and the complexity of its documentation now surpasses its parent. Although proprietary formats (like PostScript, PDF, RTF etc.) are still widely in use, there has been an explosion of markup languages and applications based on XML. Today, XML is not only an essential part of the enabling technology underlying the Web, but also plays a crucial role as exchange format in databases, graphics and multimedia applications in sectors ranging from industry, over business and administration, to education and academic research. Problems addressed For all the developments in XML since 1998, one thing that has not changed is the understanding of XML documents as serializations of tree structures conforming to the constraints expressed in the document’s DTD (Document Type Definition) or some form of schema. This seems very natural, and on our analysis the tight integration of linear form (notation), data structure and constraint language is one important key to XML’s success. Notwithstanding XML’s many strengths, there are problem areas which invite further research on some of the fundamental assumptions of XML and the document models associated with it. XML strongly emphasizes and encourages a hierarchical document model, which can be validated using a context-free grammar (or other grammars that encourage a constituent structure interpretation, like context sensitive and regular grammars). Consequently, it is a challenge to represent in XML anything that does not easily lend itself to representation by context-free or constituent structure grammars, such as overlapping or fragmented elements, and multiple co-existing complete or partial alternative structures or orderings. For the purpose of our work, we call such structures complex structures, and we call documents containing such structures complex documents. Complex structures are ubiquitous in traditional documents — in printed as well as in manuscript sources. Common examples are associated with the physical organization of the document and the compositional structure of the text, in other words, such things as pages, columns and lines on the one hand, and chapters, sections and sentences on the other. Sentences and direct speech tend to overlap in prose, verse lines and sentences in poetry, speeches and various other phenomena in drama. Complex structures occur frequently also in databases, computer games, hypertext and computer-based literature. In the last few years problems pertaining to complex structures have received increasing attention, resulting in proposals for - conventions for tagging complex structures by existing notations, by extending such notations, or by designing entirely new notations; - alternative data structures; - explications of the semantic relationships cued by markup in a form that is more easily machine- processable. The MLCD (Markup Languages for Complex Documents) project aims to integrate such alternative approaches by developing both an alternative notation, a data structure and a constraint language which as far as possible is compatible with and retains the strengths of XML-based markup, yet solves the problems with representation and processing of complex structures. MLCD started in 2001 and is expected to complete its work in 2007. The project is a collaboration between a group of researchers based at several different institutions. The remainder of this paper presents an interim report from the project Data Structure One of the early achievements of MLCD was the specification of the GODDAG (Generalized Ordered-Descendant Directed Acyclic Graph) structure. It was originally based on the realization that overlap (which was the first kind of complex structure we considered) can be represented simply as multiple parentage. A GODDAG is a directed acyclic graph in which each node is either a leaf node, labeled with a character string, or a nonterminal node, labeled with a generic identifier. Directed arcs connect nonterminal nodes with each other and with leaf nodes. No node dominates another node both directly and indirectly, but any node may be dominated by any number of other nodes. We distinguish a restricted and a generalized form of GODDAG. Conventional XML trees satisfy the requirements of generalized as well as restricted GODDAGs. In addition, restricted GODDAGs lend themselves to representation of documents with concurrent hierarchies or arbitrarily overlapping elements, whereas generalized GODDAGs also allow for a convenient representation of documents with multiple roots, with alternate orderings, and discontinuous or fragmented elements. The similarities between trees and GODDAGs allow similar methods of interpreting the meaning of markup: properties can be inherited from a parent, overridden by a descendant, and so on. There is some chance for conflict and confusion, since with multiple parents, it is possible that different parents have different and incompatible properties. Recent work has revealed a weakness in the current specification of GODDAG, which leads to problems with the representation of discontinuous elements. In the full version of this paper we will present the results of our work towards a solution to these problems. Notation It is always possible to construct GODDAGs from XML documents. In the general case, they will be trees, which are subsets of GODDAGs. It is also possible to construct GODDAGs from the various mechanisms customarily used in order to represent complex structures in XML. However, these mechanisms depend on application-specific processing and vocabularies, and tend to be cumbersome. Thus, one may either try to establish standards for the representation of complex structures in XML, or provide an alternative notation which lends itself to a more straightforward representation of complex structures. We believe that these options are complimentary, and that both should be pursued. Thus, we have defined an alternative notation to XML, TexMECS. The basic principles of its design are: - For documents that exhibit a straightforward hierarchical structure, TexMECS is isomorphic to XML. - Every TexMECS document is translatable into a GODDAG structure without application-specific processing. - Every GODDAG structure is representable as a TexMECS document. A particular advantage of TexMECS is a simple and straightforward notation for what we have called complex structures. We also plan to design algorithms for translating widely recognized XML conventions for representation of complex structures into GODDAGs, and vice versa. In the full version of this paper we will report on our latest work in this area. Constraint Language One of the most important remaining tasks for the MLCD project is the identification of a constraint mechanism which relates to GODDAGs as naturally as constituent structure grammars relate to trees, which constitute a subset of GODDAGs. Constraint languages for XML documents exist in the form of XML DTDs, XML Schema, Relax NG and others. These methods invariably define context-free grammars allowing the representation of XML documents in the form of parse trees. However, since GODDAG structures are directed acyclic graphs more general than trees, they cannot easily be identified with parse trees based on context-free grammars. Several possible ways forward exist and remain to be explored. MLCD has decided to focus on two approaches, one grammar-based and one predicate-based. The grammar-based approach starts from the observation that GODDAGs can be projected into sets of tangled trees. One way to achieve at least partial validation of complex documents, therefore, is to write grammars for each such tree and validate each projection against the appropriate grammar. Each such grammar will treat some start- and end-tags in the usual way as brackets surrounding structural units, but treat other start- and end-tags as if they were empty elements. This allows some measure of control over the interaction and overlapping of specific elements in different grammars; whether it provides enough control remains to be explored. Another approach to validation is to abandon the notion of document grammars, and regard validation simply as the establishment of some set of useful invariants. A schema then takes the form of a set of predicates; the document is valid if and only if all of the required predicates are true in that document. In the XML context, this approach is represented by Schematron. It is clear that it can also be applied to documents with complex structures, if the language used to formulate the required predicates is extended appropriately. In the full version of this paper we will report on our attempts to pursue each of these two approaches",
       "article_title":"Markup Languages for Complex Documents – an Interim Project Report",
       "authors":[
          {
             "given":"Claus ",
             "family":"HUITFELDT",
             "affiliation":[
                {
                   "original_name":"Department of Philosophy,     University of Bergen, Norway",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          },
          {
             "given":"Michael ",
             "family":"SPERBERG - MCQUEEN",
             "affiliation":[
                {
                   "original_name":"World Wide Web Consortium, MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)",
                   "normalized_name":"World Wide Web Consortium",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0059y1582",
                      "GRID":"grid.507688.4"
                   }
                }
             ]
          },
          {
             "given":"David ",
             "family":"DUBIN",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science, University of Illinois     at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Lars G. ",
             "family":"JOHNSEN",
             "affiliation":[
                {
                   "original_name":"Department of Linguistics and Comparative Literature, University of Bergen, Norway",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"New concepts in the visualization of time-based events are introduced and applied to the fields of historiography and criticism. These techniques (perpendicular timelines, dynamic confidence links, and time-slice relationship diagrams) extend the semantic power of timelines so that they can show the development of complex concepts and interpretations of underlying events. An interactive software tool called “TimeVis” illustrates these techniques with both 2D and 3D views. History is a referent discipline. Later events build on earlier events, though in unpredictable and complicated ways. Historiography and literary criticism are the histories of accumulated comments on a subject. The underlying history and literature (the “base events”) occur in one era, and commentary and subsequent events (“secondary events”) are added later. However, commentary is not made in the same order as the base events; scholars might spend decades analyzing a writer’s later works, and subsequently change emphasis to her earlier works. Visualizing such referent-based relationships through time is very difficult with a single, conventional timeline. The concept of stacked timelines of different eras [Jen03] was introduced to align commentary and consequent events with their referents (Figure 1). This is useful when secondary events are evenly distributed, but less useful when they are concentrated on subsets of the base events. Crossing lines are difficult to interpret, and important early events can end up leading to a forest of arrows. What is more, the x-axes of the two timelines have no relation to each other. This lack of relation is in fact the cause of the criss-crossing lines. This paper describes three new timeline techniques that can be applied to the study of history, criticism, and other fields with a temporal or referent component. Each technique serves a different research need. Techniques “Perpendicular timelines” are like stacked timelines, but with the second turned 90 degrees, and a second dimension added (Figure 2). The added dimension is the time dimension of the first timeline. This means that secondary events can be visually grouped by the base events that they refer to, yet also be ordered by their own time. In effect, perpendicular timelines allow each original event or topic to spin off its own timeline of commentary and follow-up, arranged perpendicular to the base events. “Dynamic confidence links” build on perpendicular timelines, and provide interactive feedback. Each timeline within TimeVis can have one or more “time slices”, which are markers indicating the current point of interest in that timeline (Figure 3). As we move the current time slice marker in the secondary timeline, we are focusing on “what we knew at time t about the events in the base timeline”. As those secondary events occurred in the real world, it changed our interpretation and understanding of the base events. (For example, when Boswell’s papers, thought to be lost, were discovered in the early 20th Century, they revealed details of his life that were formerly hidden.) These changed interpretations and understandings can be represented by gray event or concept markers in the base line, connected to the revelatory events of the secondary timeline by gray lines. When the current time slice marker passes a secondary event, its lines and the base events to which it connects can turn from gray to black, indicating that this was the time at which those facts or interpretations became more plausible. That is, the tool dynamically indicates our confidence in different claims, as a function of time. (Note that the inverse applies as well. If secondary events tend to reduce our confidence in earlier interpretations, those revelations can cause the base events to turn gray.) Perpendicular timelines, and the dynamic confidence links they enable, can also be extended from two dimensions to three dimensions (Figure 4). Just as the move from one simple timeline to perpendicular timelines frees the secondary events to be organized both by topic and by time, the extension of perpendicular timelines into an additional dimension allows data to be organized by time as well as two other criteria. Figure 4. 3D timeline, with two slice markers showing time-slice relationship diagrams. This illustrates the growing importance of the green actor as we compare time T1 to time T2. Both the base timeline (now a 3D timeline space) and the secondary timeline (also a 3D timeline space) have more flexibility. Rather than use strict definitions for the two free axes in the base 3D space, they can act as a 2D surface for organizing the time-oriented topic/subject bars. By shuffling the time-oriented bars around, the links between them can become more understandable. This is similar to the behavior of force-directed 2D network graph tools such as Visual Thesaurus. The secondary 3D space can now be organized with one axis for its own events’ time, another axis for theme or subject, and the third axis for actor. This allows us to illustrate concepts such as “who knew what when?” “Time-slice relationship diagrams” show how different actors are related to each other at the time pointed to by the current time slice marker. If events are organized around the concept of actors, and laid out in a 3D space, then a 2D time slice through the 3D space can show a relationship diagram, indicating the “small world” connections from one actor to another. The diagram is equivalent to looking down the time axis of the 3D space, backwards in time, with older connections appearing distant (and thinner) and newer connections appearing large. As the current time slice marker is moved through time, the user can see relationships forming and decaying. Applications TimeVis is being used to visualize controversies and cover-ups in history, including Watergate and the Dreyfus Affair. It is being used to investigate the historiographical record of acceptance of Vertot’s Roman Revolutions (1719), and the literary response, over the centuries, to Boswell’s writings. Those studies should conclude shortly, and while no scholarly breakthroughs are to be expected, what should emerge is a set of visualizations (diagrams, videos, and data files) of use to students and researchers who seek to capture the big picture of such topics that stretch out over time.",
       "article_title":"Semantic Timeline Tools for History and Criticism",
       "authors":[
          {
             "given":"Matt ",
             "family":"JENSEN",
             "affiliation":[
                {
                   "original_name":"NewsBlip",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction Information about place is an essential part of the study of the humanities. People live, events occur, and artefacts are produced by human hand at specific geographical locations and much of what people do is spatially determined or leaves spatial signatures. In order to gain insight into human activity, past and present, the influences of geography must be taken into account. Digital scholarship makes powerful new methodologies freely available at relatively low cost. However, the new research opportunities offered by spatial and spatial-temporal data remain relatively unexplored. This paper examines the reasons for this and discusses possible ways forward for the community. GIS methodology is much more than digital cartography, it gives the researcher the ability to analyse and display data in a variety of maps, networks or hierarchy trees. The need to represent and model time is leading humanities scholars to experiment with the emerging methodologies of dynamic mapping, an approach that was impossible before the advent of digital scholarship. There are many ways that methods and tools for structuring, visualising and analysing space, spatial behaviour and spatial change can benefit humanities research. It is over fifteen years since GIS software with reasonable functionality became available in a PC environment at a relatively low cost. Despite this the use of geographical information in digital scholarship by humanists has been poor. This paper will explore some of this potential but, possibly more importantly, it will also examine why that potential continues to be ignored by many. The author believes there are many reasons, some reflect weaknesses in the methodology and current technologies but possibly the most significant concern our scholarly institutions. The Developing Role of Geographic information Systems in Humanities research The use of geographical information in digital humanities research has passed through a sequence of phases of development. Initially the technology was used to replicate pre-existing methodologies and styles of work as in projects such as the Atlas of Mortality in Victorian Britain (Woods and Shelton, 1997). This printed publication used largely standard graphs and cartographic representations to produce an atlas of patterns of health and death in Victorian England and Wales. Later the new methodologies made available by GIS methodology were applied to a variety of new areas for example, the use of 3D digital elevation models to explore the effect that terrain, specifically the gradient required for railway lines, influenced the development of railways in Victorian Britain. Other examples include the dynamic maps used in the Valley of the Shadow and Salem Witch Trial projects. The boundaries of the more rigorously quantitative methods are also being pushed back as the use of 3D and 4D work is explored. So far the majority of the humanities research performed with GIS has been largely quantitative in nature, but recently there has been increased interest in the use of geographical information for more qualitative work. A number of examples of this style of work can be seen in the Perseus Digital Library, for example the Edwin C Bolles Collection and Boyle Papers. These projects make use of traditional map materials and geographical information in a variety of ways. The Perseus project itself links a number of different digital libraries using geographical information as an integration tool. This work covers a broad range of activities which links texts, images and numerical data to the places they describe on interactive maps to produce an immersive learning environments. Geographical information also has immense potential both for research and the delivery of information. It provides an unambiguous method for indexing and searching of information. Recent work on map based front-ends to text and image collections has resulted in source discovery tools that are more intuitive and less culturally specific than traditional textual indexing. Factors Inhibiting the use of Geographical Information in the Digital Humanities It is clear that geographical information has immense potential both for research and the delivery of information. It provides an unambiguous method for indexing and searching of information offering the ability to build source discovery tools that are more intuitive and less culturally specific that traditional textual indexing. Despite this it has had little effect in the digital humanities. There area number of limiting factors which need to be addressed Existing Methodologies: Current GIS methodology is rooted deeply in the origins of the software in the earth sciences. The traditions of rich data sources led to the development of software that is ill-equipped to cope with the sparse and fuzzy data of humanities scholarship. We need to consider how to represent the complexities of the subjects of humanities research visually. What does it mean to think spatially and how do we represent the complex phenomena at work in the humanities visually? Current GIS software has very limited facilities for the handling of time and even these are based on the scientist’s view of time as being derived from the phenomenon under study. Humanists require a view of time that is determinative and that can work at different rates and scales moving backwards as well as forwards on a continuous scale. Humanities Data: Methodologies must be found for tackling issues such sparseness, fuzziness and ambiguity but there are many other broader issues concerning data. The number of digital datasets is growing rapidly and these are often of interest to researchers in fields other than the often highly specialised one that the data was originally derived for but how does one locate them? There is a need for a central archive and a metadata schema that would allow these resources to be discovered. Archivists could provide a community hub for encouraging and implementing the use of spatial content into their retrieval models thus providing a further means of linking different items of evidence. Research Practice: Ambitious work with GIS requires a high level of expertise and has a high threshold of usability. The most effective way of facilitating this style of work is through collaboration with researchers and practitioners having backgrounds in fields such as geography, history, information science, computer science, graphic design, and so on. This marks a move away from scholars working independently to a model that necessitates team working which in turn raises further problems. The teams required for this style of collaborative work will be composed of people with very different backgrounds and credentials. There is a strong need to transmit research traditions between disciplines which may have very different agendas and ways of working. There also many issues concerning how the contributions of each member of the team can be acknowledged. For example, for those whose technical work requires considerable expert knowledge far beyond ‘technical support’ but is fundamentally different from the traditional academic content of the journals and review boards where the published works will be assessed in research assessment exercises such those in the UK. It also raises the issue of how best to prepare students for work in an interdisciplinary team after graduation. Scholarly Institutions: This innovative style of work is often seen as ‘dangerous’ and can be seen as posing a career risk for new academics working in environments where successful research assessment is critical. There are limited opportunities for publishing the work and because there are relatively few people with relevant experience at the top of the profession there can be problems with peer review. An additional problem is that projects requiring GIS are often very large and require sustained funding. They also frequently produce resources that will need to be maintained after the completion of the original project. These two funding requirements can pose problems in current research and funding environments. Scholarly Perception of Geographical Information Science and visualisation: In order to encourage the greater use of geographical information we need a clear statement of the advantages of GIS, and spatial information generally, expressed in terms of research outcomes. This should be supported by a set of exemplar projects and be made available through a ‘one stop’ source of information. Such a resource could also encourage creative thinking about geographical information and how it can be used in original ways. The fact that humanists are used to working primarily with textual sources may be an inhibiting factor. Many claims are made for the value of GIS as a visualisation tool but it may be that some training is needed in thinking visually and, importantly, the interpretation of the results of visualisation. There could also be more fundamental issues concerning the status and function of images, especially those used for visualization, in humanities scholarship. A similar situation exists with spatial thinking too. Conclusions Although we are used to the idea of GIS as a positivist tool its big contribution to the humanities may be as a reflexive one. It can be used to integrate multiple perspectives of the past allowing them to be visualised at various scales. Ultimately it could create a dynamic representation of time and place within culture. This abstract has introduced some of the many factors that are currently limiting the use of geographical information in humanities teaching and research, the final paper will discuss these in more detail and suggest some immediate solutions. The greater use of geographical information could allow us to experience a view of the past that is highly experiential, providing a fusion of qualitative and quantitative information that could be accessed by both naive and knowledgeable alike.",
       "article_title":"The Inhibition of Geographical Information in Digital Humanities Scholarship",
       "authors":[
          {
             "given":"Martyn ",
             "family":"JESSOP",
             "affiliation":[
                {
                   "original_name":"Centre for Computing in the Humanities     King’s College London, Strand     London WC2R 2LS",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Humanities scholars today are faced with information overload created by the explosive growth of resources and services on the web, by globalised instant communication and by the spawning of new personal and intellectual networks outside the confines of discipline and geography. The pace and scale of developments is both exciting and challenging, necessitating the adoption of new strategies which capitalize on the potential of digital methods. The issues The explosion in information has not been matched by developments in the conceptual framework and tools we use to manage information, by availability of digital infrastructure or by the widespread adoption of new methods (other than the basic generic tools of email, wordprocessing and web browsing). Where we have adopted digital tools it is often within existing structures – faster communication, easier preparation of publications, easier access to library catalogues and content – rather than in novel approaches to research, teaching or collaboration. Uptake of digital methods in the Humanities is hindered by lack of funding for infrastructure development, by the richness and heterogeneity of our domain, by the lack of agreed methods and classificatory systems and by our relatively slow adoption of technology. If we are to overcome the problems of information overload we must develop – with very limited resources – e-Research infrastructure and tools which support and enable our tradition of individual scholarship and interpretation, rather than the method- and data-driven tools of the sciences (or commerce). Our tools must reflect the needs of Humanities scholars – low entry barriers and intuitive structures which reflect the richness and complexity of domain practices. While technologically literate scholars will continue to build or adopt tools and find new ways to use them, how do we enable the rest of the discipline to understand and leverage their potential? The critical need is to develop and propagate authoritative information on Digital Humanities methods repackaged in a form digestible by less technologically-oriented Humanities scholars. We need to get the message out. SHSSERI Collaborative Knowledge Space The Sydney Humanities and Social Sciences e-Research Initiative (SHSSERI) is a project to develop e-Research tools and information which reflect the needs of Humanities (and Social Sciences) scholars, particularly tools and which move beyond the mere collection and delivery of digital information into active engagement with research methods and the structure of academic communities. Our aim is to provide a resource which becomes a ‘must-have’ by providing a single point of access to scattered resources, tools to manage those resources and authoritative information which shortcuts the process of adopting digital tools (and increases the chances of success). Building on existing work at the University of Sydney and using Open Source software, we are developing a one-stop-shop – the SHSSERI Collaborative KnowledgeSpace (CKS) - to support the information, communication, data management, analysis and archiving needs of Humanities and Social Sciences researchers. In developing the SHSSERI CKS we have identified as our highest priority the management of basic research information (references, bookmarks and notes) and access to authoritative information about the practice and potential of the Digital Humanities. A key focus is the integration of resources. Most researchers store bibliographic references, internet bookmarks and research notes in separate systems (eg. EndNote, Firefox and a Blogg – or on paper). Their CV may be in Word, research observations in Access, research expenditure and grad student details in Excel, email in Outlook, travel arrangements in a corporate system, … the list goes on. Our resources are all too often tied to the desktop of a specific computer, scattered, fragmented, unfindable, inconsistent, redundant, unlinkable and insecure. We work inefficiently, waste time and effort, and stress out trying to manage these disparate, inconsistent resources. TMBookmarker The TMBookmarker application (name undecided) tackles the management of bibliographic references, internet bookmarks and note-taking, as well as access to authoritative sources of information on the Digital Humanities. TMBookmarker is a web-accessible knowledgebase which will handle conventional bibliographic information, internet bookmarks and note-taking in a single integrated database. It aims to replace all these forms of machine-specific or special-purpose referencing with a single, integrated searchable database available anywhere one has access to the Internet – from University desk to Internet café in Khatmandu. In addition to providing consistent anywhere-access and capture/annotation of notes, bookmarks and bibliographic references, the database will generate selective lists for inclusion in web sites, course readings or bibliographies, and allow keywords and annotation to be attached to any resource. It aims to reduce a multiplicity of special-purpose programs, information folders, bloggs, bookmark lists and hand-built web pages to a single, easily understood, web-accessible resource. It will open new avenues of possibility for those who have not ventured into the blogosphere, mastered EndNote or managed to migrate their bookmarks from one machine to the next. Social bookmarking TMBookmarker also implements concepts of social bookmarking using a structure we are calling a databliki (database + blogg + wiki). We allow users to discover and share bookmarks/references through the database, while attaching their own personal notes and classifications to them. We provide wiki-based and blogg-based public editing and annotation of references, allowing the community of scholars to participate in expansion and refinement of the reference database, and the development of additional knowledge around the core resource. By mining the database we can identify patterns of bookmarking and knowledge development which link people with similar interests and use the patterns to provide relevancy-based searches of the database. Unlike generic social bookmarking systems such as del.icio.us or CiteULike, the SHSSERI CKS identifies users by name and institution. This provides a mechanism for identifying colleagues with similar interests and the serendipitous discovery of relevant references through folksonomic tagging tied to specific communities of users. We aim to provide improved folksonomy-based searches by calculating co-occurrence of tags across the database, independent of their actual text values, and using these correlations in combination with user ratings against specific user-defined groups of colleagues and subject domains to generate targeted relevancy measures. TMBookmarker has many smarts such as instant web bookmarking, automatic DOI identification and lookup, reference import/export from common systems, RSS feeds, saved custom searches, custom list generation and wiki-style change tracking. It has been in use by a test group since Nov 2005 and should be ready for general release by second quarter 2006. It is written in php and MySQL. We intend to release the code as Open Source. In this presentation I will explore the basic concepts of managing information through a collaborative reference/bookmark/annotation system rather than a conventional desktop reference management system. I will focus on the benefits of integrated access to information in a single database, and the advantages of social bookmarking in an academic referencing system, compared with less structured systems such as del.icio.us or CiteULike. I will also compare the social bookmarking methods developed with those of other academic referencing systems, such as Connotea, and present preliminary results on defining communities of interest through database mining. I will conclude with observations on the relationship between social bookmarking and peer-review methods in establishing relevancy and value of published resources.",
       "article_title":"The SHSSERI Collaborative KnowledgeSpace : a New Approach to Resource Fragmentation and Information Overload",
       "authors":[
          {
             "given":"Ian ",
             "family":"JOHNSON",
             "affiliation":[
                {
                   "original_name":"Archaeology, University of Sydney",
                   "normalized_name":"University of Sydney",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/0384j8v12",
                      "GRID":"grid.1013.3"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The field of “Digital Humanities” has been plagued by a perceived neglect on the part of the broader humanities community. The community as a whole tends not to be aware of the tools developed by HC practitioners (as documented by the recent surveys by Siemens et al.), and tends not to take seriously many of the results of scholarship obtained by HC methods and tools. This problem has been noticed recently by a number of groups focusing on issues regarding humanities tools, most notably the Text Analysis Developers Alliance (Text Analysis Summit, May 9-11, 2005 at McMasters University) and IATH (Summit on Digital Tools for the Humanities, September 28-30, 2005, at the University of Virginia). One possible reason for this apparent neglect is a mismatch of expectations between the expected needs of audience (market) for the tools and the community’s actual needs. A recent paper by Gibson on the development of an electronic scholarly edition of _Clotel_ may illustrate this. The edition itself is a technical masterpiece, offering, among other things, the ability to compare passages among the various editions and even to track word-by-word changes. However, it is not clear who among Clotel scholars will be interested in using this capacity or this edition; many scholars are happy with their print copies and the capacities print grants (such as scribbling in the margins or reading on a park bench). Furthermore, the nature of the Clotel edition does not lend itself well either to application to other areas or to further extension. It is essentially a service offered to the broader research community in the hope that it will be used, and runs a great risk of becoming simply yet another tool developed by the DH specialists to be ignored. Matthew Jockers has observed that much of the focus in humanities computing is on “methodologies, not results.” (Bradley, 2005). This paper argues for a focus on deliverable results in the form of useful solutions to genuine problems, instead of simply new representations. The wider question to address, then, is what needs the humanities community has that can be dealt with using HC tools and techniques, or equivalently what incentive humanists have to take up and to use new methods. This can be treated in some respects like the computational quest for the “killer application” -- a need of the user group that can be filled, and by filling it, create an acceptance of that tool and the supporting methods/results. For example, MS-Word and similar software has revolutionized writing, even for non-specialists, to the same degree that Email has revolutionized communication and the World Wide Web has revolutionized publication. They have not only empowered non-specialists to do more, but also created inspiring opportunities for further secondary research in extending the capabilities of these software tools. Digital Humanities needs a “killer application” -- a Great Problem that can both empower and inspire. Three properties appear to characterize such Great Problems. First, the problem itself must be real, in the sense that other humanists (or the public at large) should be interested in the fruits of its solution. Second, the problem must be interesting, in the sense that the solution is not obvious and the process of solving it will add materially to human knowledge. Third, the problem itself must be such that even a partial solution or an incremental improvement will be useful and/or interesting. As a historical example of such a Problem (and the development of a solution), consider the issue of resource discovery. With the advent of the Web, billions of resources are now broadly available, but no one knows how to find them. Traditional solutions (journal publications, citation indices, etc.) are no longer adequate as publication can happen through informal channels. Google provides a partial solution to this problem by automatically searching and indexing “the Web,” specifically to solve the general problem of finding stuff. At the same time, its algorithms are demonstrably inadequate both in terms of accuracy and in what it can search, leaving much room for incremental development -- but the partial solution that exists has still revolutionized scholarship, and created a huge economic opportunity precisely to extend and improve the solution. To the three criteria above can thus be added an additional, more political aspect of any proposed “killer app.” Any proposed application should be extremely user-friendly, possibly even at the expense of complete generality -- the Perfect should not be the enemy of the Good, especially if the Perfect system is unusably general. I have argued elsewhere that a possible candidate for such a killer app would be authorship attribution: determining who (if anyone) from a candidate pool of authors wrote a particular document under discussion. This question is obviously of interest, for example, to scholars who wish either to validate a disputed authorship, or for authors wishing to investigate if a document of unknown authorship (say, an unsigned political pamphlet) can be assigned to a known author (and help illuminate some political views). Less obviously, “questioned documents” are often extremely important in a legal and forensic environment -- but traditional forensic analysis, such as handwriting, cannot address questions about (e.g.) born-digital documents, transcriptions, purported copies, and so forth. At the same time, enough papers have been published recently to demonstrate a strong interest in the problem from a humanities standpoint -- and even an analysis that is not strong enough to be conclusive prove can still suggest lines and approaches for further investigation and scholarship. Another candidate that has been argued elsewhere is automatic back-of-the-book indexing. A third, discussed at the recent IATH summit, is an automatic tool for annotating fully-electronic multimedia documents. These are difficult problems, and a full solution will involve (and illuminate) many subtle aspects of human cognition and of the writing process. At the same time, other scholars will be grateful for the results -- on the one hand, by relieving them of the difficult and expensive burden of generating indices for their own works, and on the other by supporting them in the ability to read and annotate electronic documents in the way they traditionally interact with paper. It would be to the overall benefit to the DH community to focus at least some effort and resources on the identification and solution of such Great Problems and on the development of such killer apps. The apparent alternative is the status quo, where digital research tools are brilliantly developed, only to languish in neglected disuse by the larger community.",
       "article_title":"Killer Applications in Digital Humanities",
       "authors":[
          {
             "given":"Patrick ",
             "family":"JUOLA",
             "affiliation":[
                {
                   "original_name":"Duquesne University, Dept. of Mathematics and Computer Science",
                   "normalized_name":"Duquesne University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02336z538",
                      "GRID":"grid.255272.5"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Most of the video information retrieval systems today rely on some set of computationally extracted video and/or audio features, which may be complemented with manually created annotation that is usually either arduous to create or significantly impaired at capturing the content. In this research we set out not only to find the computational features relevant to movies, but also to investigate how much information could be semi- automatically extracted from the production documentation created during the actual production stages a film goes through. It was our hypothesis that this documentation, which has been carefully created for the realization of a movie, would prove highly useful as a source of metadata describing the finished movie. This paper presents research done at the MediaTeam Oulu research group [1] on identifying and structuring the key metadata descriptors applicable to motion picture content analysis and use of motion picture content in video-on-demand applications. The underlying research includes a study of the concepts involved in narrative structure and a series of empirical viewer tests through which a key set of metadata describing the content and structure of motion picture material was identified. Parts of this research were conducted in co-operation with the Department of Finnish, Information Studies and Logopedics at the University of Oulu [2]. First, established theories and concepts of narration were utilized to examine how movies are structured and what mechanisms are used to package and convey information to the viewer. Studying the formalisms and conventions governing film scripts and screenplays, we attempted to identify and characterize the key elements in movies, e.g. characters, actions and various types of plot points [3]. In addition to these primary elements, we also looked at supporting elements [4], i.e. the kinds of production-related techniques and instruments the filmmakers use in trying to guide the viewer’s attention throughout a movie. We found that, for example, editing rhythm and various changes or events in the audiovisual composition of a movie are among the most frequently utilized instruments for highlighting certain sections of a movie where the user should pay more (or less) attention. Our goal in studying these conventions and story-telling instruments was to first understand the domain of the filmmaker - his intended form and function for a movie - before looking at what actions or reactions the movie causes on the part of the viewer. Secondly, a series of tests using short clips, trailers and an entire movie was carried out in order to investigate how people perceive and process movies [5]. Questionnaires and interviews provided information on what kinds of things viewers notice and how they later describe what they have seen. This information was used to arrive at a key set of metadata that models movies using the same concepts as viewers do in describing them. It was then our task to match these concepts to the instruments used by the filmmakers, in order to find a metadata model that is both usable and feasible to create through a semi-automatic process from what is offered by the movie. Furthermore, the model thus constructed was designed hierarchical in order to facilitate dynamic control over the level of detail in any given metadata category, thereby enabling, for example, the smart summarization of movies on multiple levels. This metadata model then became the starting point for the design of the actual browser that an end-user would utilize in navigating and searching movies. The next step was to identify the main sources for obtaining this metadata and the best methods for obtaining it. In studying the movie production stages and the documentation relating to each stage, we found that the final script and storyboards, as well as the audio and video tracks of the finished movie were the most interesting and also most practical sources. A comprehensive suite of both automatic and semi- automatic tools for processing these documents and media objects was developed in order to extract the necessary features and information, such as the participants in any given scene, the speakers and the level of interaction between them, motion activity detected on the video track and a wide range of sound properties extracted from the audio track. These tools included a ScriptTagger, StoryLinker, Scene Hierarchy Editor and a management tool for the underlying database. The ScriptTagger is a tool which takes in the raw text of the script and turns it automatically into a structured XML document, which can then be further refined semi-automatically to facilitate the tagging of more advanced features. The StoryLinker is a tool used to bring together the script, the storyboards and the video and audio tracks of the movie itself to form individual scenes, where all of the above are linked. The Scene Hierarchy Editor is a semi-automatic tool for grouping together individual scenes using the model constructed on the basis of narrative structure and viewer tests, thus constructing a hierarchical description of the movie. In addition to these, a number of tools were used to extract the audio and video features. A management tool was used to combine the output produced by all the tools above to construct a uniform database. Ultimately, a prototype of a content-based browser for accessing movies using the metadata model and specialized feature visualization was developed. The implemented browser prototype is a multimodal, content-based retrieval system that enables a wide range of searches based on the hierarchical metadata. This prototype offers users customized views into a movie using user-selectable criteria. The main view offered by the prototype is a navigable hierarchical map of the movie, where the user-selected features are visualized as navigation aids. Users can then move from a higher level down to increasingly detailed descriptions of the movie, i.e. from acts to segments and ultimately to scenes, enabling them to navigate their way down to a particular sequence, ultimately allowing cross comparison of similar sequences in other movies in the database. Alternatively, users may progress through a selected movie on some chosen level of detail, thus enabling them to see the structure of the movie based on the criteria they have chosen. The criteria can be changed on any level or at any point while browsing, i.e. features can be added or removed, as the user sees fit based on how well the features are applicable to the material on any given level and how well they answer the overall search needs of the user. The browser can visualize any new features as long as those features are submitted into the system in the correct format. The browser and its associated metadata creation tools have numerous applications ranging from, for example, commercial video-on-demand applications for both consumers and media publishing houses to more research oriented ones, such as analysis tools not only for film studies but indeed also for linguistic research into dramatic content, for example of movies and television series. The system could, for example, be used to find certain kinds of conversations or sequences of interest based on their content, structure or audiovisual makeup. The tool is suitable for incorporating non-linguistic information into linguistic information, which has various applications when studying multimodal content, for example in the investigation of expressions of stance, where paralinguistic features complement the linguistic realization of attitude and emotion. The ideas developed and lessons learned from the construction of these tools and browser will also be applied to a new electronic framework for the collection, management, online display, and exploitation of corpora, which is being developed within the LICHEN project (The Linguistic and Cultural Heritage Electronic Network) [6].",
       "article_title":"Novel Tools for Creating and Visualizing Metadata for Digital Movie Retrieval",
       "authors":[
          {
             "given":"Ilkka ",
             "family":"JUUSO",
             "affiliation":[
                {
                   "original_name":"Department of Electrical and Information Engineering, University of Oulu, Finland",
                   "normalized_name":"University of Oulu",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/03yj89h83",
                      "GRID":"grid.10858.34"
                   }
                }
             ]
          },
          {
             "given":"Tapio ",
             "family":"SEPPÄNEN",
             "affiliation":[
                {
                   "original_name":"Department of Electrical and Information Engineering, University of Oulu, Finland",
                   "normalized_name":"University of Oulu",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/03yj89h83",
                      "GRID":"grid.10858.34"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In my conference presentation, “Cybercultural Capital: ASCII’s Preservation of the Digital Underground,” I will examine independent electronic magazines published in the American Standard Code for Information Interchange, between the years of 1984 and 1993. This period begins with the emergence of the first organized ASCII magazines and ends with the creation of Mosaic, the WWW browser that incorporated HTML and put an end to ASCII’s reign as the most widely-used electronic file-type. This nine-year span saw the creation of many independent ASCII magazines, 288 of which can still be accessed through the textfiles.com archives, currently storing over ten thousand issues from the era. These magazines include fiction, poetry, articles, and a plethora of subversive technical manuals on topics such as hacking, virii, and sabotage. Just as Russian Samizdat publishers attempted to undermine the hegemony of the Soviet state through subversive literature, ASCII publishers of North America attempted to undermine Corporate hegemony. In my presentation, I will examine the ruling ethos in ASCII literature, considering cybercultural resistance to corporate paradigms, the cultural need for cyberwriters, and the influence of hacking, sabotage, and computer culture on ASCII fiction and poetry. In an age before the World Wide Web, ASCII text files were a powerful medium for independent publishing, offering disenfranchised suburban cyberpunks easy access to the means of textual production and distribution. While thousands of ASCII texts are currently archived on websites like etext.org and textfiles.com, these websites are maintained by amateurs with no formal training as archivists. As the Internet continues to grow, websites are updated, websites become defunct, and old files are often overwritten by new files. Internet archives are unstable and their documents are at risk of becoming corrupted or erased. My presentation will highlight the literary importance of ASCII texts and explain why an archival project must be undertaken immediately to ensure that the writings of this movement are not entirely lost. Biography Ihave been active in the Canadian independent publishing scene since 1995. In the past decade I have published nearly 400 issues of various ASCII zines, maintained “The Current Text Scene” (a website dedicated to tracking contemporary ASCII zines), and published an ASCII-related article with Broken Pencil. My other areas of interest include creative writing and experimental fiction.",
       "article_title":"Cybercultural Capital: ASCII’s Preservation of the Digital Underground",
       "authors":[
          {
             "given":"Joel ",
             "family":"KATELNIKOFF",
             "affiliation":[
                {
                   "original_name":"Department of English & Film Studies,     University of Albe",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction Traditional methods in sociolinguistic analysis have often relied on the repeated close listening of a set of audio recordings counting the number of times particular linguistic variants occur in lieu of other variants (a classic sociolinguistic example is the tabulating of words using final –in’ for final –ing; cf. Fischer 1958, Trudgill 1974, etc.). These tabulations are normally recorded into a spreadsheet using a program such as Microsoft Excel, or even just into a hard-copy tabulation sheet. The results are then presented as summaries in publications or conference papers as the “data” used for description, explanation, and theory building. Some approaches in linguistics, such as discourse analysis, rely heavily on the development of transcripts of the audio recordings and often the focus of analysis is on the transcript itself and not the original recording or interview event. However, scholars following a wide variety of sociolinguistic approaches have repeatedly highlighted the confounds that arise from these treatments of “pseudo-data” (i.e., analysts’ representations of the data) as data. Linguists such as Blake (1997) and Wolfram (e.g., 1993) have discussed problems relating to the tabulation and treatment of linguistic variables and raised the issue that individual scholars’ methods are often not comparable. In discussing transcription theory, Edwards has repeatedly pointed out that “transcripts are not unbiased representations of the data” (Edwards 2001: 321). In general, the understanding that linguistic data is more elusive than traditional “hard science” data is widespread but not acted upon. In this paper, we present a project underway at North Carolina State University to argue that computer-enhanced approaches can propel sociolinguistic methodology into a new, more rigorous era. The North Carolina Sociolinguistic Archive and Analysis Project The North Carolina Language and Life Project (NCLLP) is a sociolinguistic research initiative at North Carolina State University (NCSU) with one of the largest audio collections of sociolinguistic data on American English in the world. It consists of approximately 1,500 interviews from the late 1960s up to the present, most on analog cassette tape, but some in formats ranging from reel-to-reel tape to digital video. The collection features the interviews of Walt Wolfram, Natalie Schilling-Estes, Erik Thomas, and numerous other scholars. The NCLLP has partnered with the NCSU Libraries on an initiative titled the North Carolina Sociolinguistic Archive and Analysis Project (NC SLAAP). NC SLAAP has two core goals: (1) to preserve the NCLLP’s recordings through digitization; and (2) to enable and explore new computer-enhanced techniques for interacting with the collection and for conducting sociolinguistic analysis. NCSU Libraries has as one of its chief goals the long-term preservation of the recordings made by the NCLLP, and it regards digitization as an appropriate means of preservation. Academic libraries may still be less expert than some commercial organizations when it comes to digitizing and storing audio, but they may be even less equipped to maintain analog audio collections properly (cf. Brylawski 2002, Smith, Allen, and Allen 2004). Archivists and librarians also sometimes point out that digitization and storage of audio may not be worth the expense and difficulty if the sole goal is preservation (cf. Puglia 2003). However, when scholarly digital projects can contribute significantly to the advancement of a discipline, as in the case of NC SLAAP, surely significant investments are called for. The NC SLAAP project has from the beginning planned to integrate sociolinguistic analysis tools into the archive. This has been achieved to a large degree by integrating the open source phonetic software application Praat (http://www.praat.org) into the web server software. In brief overview, the NC SLAAP system is an Apache web server currently housed on a Macintosh G5 computer running Mac OS 10.4. Data are stored in a MySQL database and application pages are written in PHP. The web server communicates with third-party open source applications to do most of its “heavy” processing. Most importantly, the web server communicates with Praat to generate real-time phonetic data (such as the pitch data and the spectrogram illustrated in Figure 1). Figure 1: Transcript Line Analysis Example While certain feature sets are still under development, NC SLAAP, even in its current state, provides a range of tools that greatly enhance the usability of the audio data. These features include an audio player with an annotation tool that allows users to associate notes with particular timestamps, an audio extraction feature that allows users to download and analyze particular segments of audio files, sophisticated transcript display options (as partly illustrated in Figure 1, above), and extensive search and query tools. Importantly, the NC SLAAP software helps to address concerns about the treatment of “pseudo-data” as data, because it enables scholars to better access, check, and re-check their (and their colleagues’) variable tabulations, analyses, and conclusions. In short, the NC SLAAP software is an attempt to move us one step – hopefully, a large step – closer to the “real” data. The features of the NC SLAAP software have potentially tremendous implications for a wide range of linguistic approaches. We focus on only one such feature here: the implications relating to transcription theory. Transcription Method and Theory Improvements to the traditional text transcript are extremely important because the transcript is often the chief mediating apparatus between theory and data in language research. Language researchers have long been concerned with the best method and format for transcribing natural speech data (cf. Edwards 2001). Researchers frequently incorporate a number of different transcription conventions depending on their specific research aims. Discourse analysts (e.g., Ochs 1979) traditionally focus most heavily on transcription as theory and practice, but researchers studying language contact phenomena (as in Auer 1998) also have their own transcription conventions for analyzing and presenting their data. At the other end of the spectrum are variationists and dialectologists, who also use transcripts, even if often only for presentation and illustration. Despite the importance of the transcript for most areas of linguistics, little work has been done to enhance the usability and flexibility of our transcripts. Yet the way a researcher builds a transcript has drastic effects on what can be learned from it (Edwards 2001). Concerns begin with the most basic decision about a transcript: how to lay out the text. Further decisions must be made throughout the transcript-building process, such as decisions about how much non-verbal information to include and how to encode minutiae such as pause-length and utterance overlap. Furthermore, the creation of a transcript is a time- and energy-intensive task, and researchers commonly discover that they must rework their transcripts in mid-project to clarify aspects of the discourse or speech sample. The NC SLAAP software seeks to improve the linguistic transcript by moving it closer to the actual speech that it ideally represents (Kendall 2005). In the NC SLAAP system, transcript text is treated as annotations on the audio data: transcripts are broken down into utterance-units that are stored in the database and directly tied to the audio file through timestamping of utterance start and end times. Transcript information can be viewed in formats mimicking those of traditional paper transcripts, but can also be displayed in a variety of dynamic ways – from the column-based format discussed by Ochs (1979) to a finer-level focus on an individual utterance complete with phonetic information (as shown in Figure 1, above). Conclusion NC SLAAP is a test case for new ways of approaching linguistic analysis, using computers to maintain a strong tie between the core audio data and the analysts’ representations of it. In many senses the project is still in a “proof of concept” stage. However, we feel that it has made large steps towards new and more rigorous methods for sociolinguistic analysis and data management. In addition, it can serve as a model for academic libraries as a project that incorporates digital preservation with significant scholarly advancement.",
       "article_title":"Digital Audio Archives, Computer-Enhanced Transcripts, and New Methods in Sociolinguistic Analysis",
       "authors":[
          {
             "given":"Tyler ",
             "family":"KENDALL",
             "affiliation":[
                {
                   "original_name":"Duke University",
                   "normalized_name":"Duke University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00py81415",
                      "GRID":"grid.26009.3d"
                   }
                }
             ]
          },
          {
             "given":"Amanda ",
             "family":"FRENCH",
             "affiliation":[
                {
                   "original_name":"North Carolina State University",
                   "normalized_name":"North Carolina State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04tj63d06",
                      "GRID":"grid.40803.3f"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In 2003 a project Paper and Virtual Cities, financed by the Netherlands Organization for Scientific Research was started. The ultimate goal is to make explicit choices in the use of historical maps in virtual (re-) constructions for research and design. In order to do so three projects started, focusing on the historical reliability, the technical reliability and on the creation of a mark-up language and visualization tool that could combine the historical and technical data. This presentation is on the toolbox and the mark-up that is created in this third project. Researchers often use historical maps in order to describe the changes that took place in cities. By using techniques as used in Geographical Information Systems, e.g. rubber sheeting it is possible to make overlays of maps in order to combine data from several sources. Earlier research (Koster 2001) has shown that such combination can provide new insight in the transformation processes in the 17th century townscape. But this process can introduce new errors, for example due to the fact that later copies of maps can depict outdated information. Historical and Architectural Historical research is needed in order to trace this kind of errors. A second kind of errors is due to the fact that, although the measuring techniques used by 17th century cartographers were very precise, we still need to transform the map in order to make it fit with a modern map. The method of triangulation, measuring angles between high points, e.g. church towers makes that distant points are depicted with high precision, but the area in between might be less accurate. This kind of technical irregularities become visible in the process of rectifying the map. This process is used to bring all the maps to the same scale and orientation so they can be layered and compared. By storing the technical evidence and the historical evidence in a standardized format, a mark-up language, we are able to connect the two. Such a format visualizes the reliability and veracity of historical town plans and virtual (re-) constructions (separately or in combination) in relation to function and context. This might help an (urban) historian in making choices in studying and describing the change of urban form. But how would such a standardized format look like? And how can we visualize the reliability and veracity? The extensible markup language (XML) offers different languages that can be used. Historical Events can be stored in Historical Event Markup Language (HEML) while the technical evidence can be captured in the Geography Markup Language. In combination with library standards (Marc-21) used to describe the physical document and RDFPIC to describe the digitized image we are able to describe the document in such a way that data from several maps can be combined into one new standard. In order to do so a new tool has been created that offers the user a way to annotate the digital representation of a map. This tool, with the working title DrawOverMap contains a number of layers in which a researcher, of even a team of researchers can register their data and store in the new markup language. The data can be used to create a new data layer that visualizes the reliability of the map, or even a combination of maps. With mouse-overs a researcher can query the map, see annotations made by others, and decide what data might be used in research. The user can do so using the DrawOverMap toolbox, but since the XML data is also stored as Scalable Vector Graphics (SVG), again another Markup Language the data can also be browsed interactively over the Internet giving other researchers access to the annotations of historical maps. With the growing interest in historical maps on the Internet we need a tool to annotate them, rank them and give a valuation on its usability in historical research. We hope that the markup language produced in this research might help historians in choosing which maps to use in their research. On a cultural level this research is important for cultural heritage and for new cultural creations. In the domain of cultural heritage it facilitates the search, the organization and presentation of different sources that are necessary for understanding our urban history. A better understanding of historical developments can lead to an improvement of tools used in town planning.",
       "article_title":"How to Annotate Historical Townplans?",
       "authors":[
          {
             "given":"Elwin ",
             "family":"KOSTER",
             "affiliation":[
                {
                   "original_name":"Humanities Computing, Groningen University",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The global wordnet is an extensive lexical-semantic network that constitutes of synonymous sets (synsets) linked with the semantic relations existing between them. The cross-lingual nature of the global wordnet is provided by the establishing of relations of equivalents between synsets that express the same meaning in different languages. The global wordnet offers not only the extensive data for the comparative analysis over lexical densities and levels of lexicalization but furthermore presupposes the successful implementation in different application areas such as cross-lingual information and knowledge management, cross-lingual content management and text data mining, cross-lingual information extraction and retrieval, multilingual summarization, machine translation, etc. Therefore the proper maintaining of the completeness and consistency of the global wordnet is an important prerequisite for any type of text processing to which it is intended. The EuroWordNet (EWN) extended the Princeton wordnet (PWN) with cross-lingual relations [Vossen, 1999], which were further adopted by BalkaNet (BWN) [Stamou, 2002]. The languages covered by the EWN are Czech, Dutch, Estonian, French, German, Italian, and Spanish, respectively, and those covered by the BWN are Bulgarian, Greek, Romanian, Serbian and Turkish. The equivalent synsets in different languages are linked to the same Inter-Lingual Index (ILI) thus connecting monolingual wordnets in a global lexical-semantic network. The Inter-Lingual Index is based on the PWN (ILI is consecutively synchronized with the PWN versions), the synsets of which are considered as language independent concepts. Thus a distinction between the language- specific modules (English among them) and the language-independent module (the ILI repository) has to be focused. The ILI is considered as an unstructured list of meanings, where each ILI-record consists of a synset (if the language is not English, a proper translation or at least transliteration must be ensured), an English gloss specifying the meaning and a reference to its source. Both EWN and BWN adopted the hierarchy of concepts and relations’ structure of the English wordnet as a model to be followed in the development of each language-specific wordnet. For the monolingual wordnets a strong rule is observed – strictly to preserve the structure of the PWN because via the ILI a proper cross-lingual navigation is ensured. It is natural, that some of the concepts stored in ILI are not lexicalized in all languages and there are language specific concepts that might have no ILI equivalent. In the first case, the empty synsets were created (called non-lexicalized synsets) in the wordnets for the languages that do not lexicalize the respective concepts. The non-lexicalized synsets preserve the hierarchy and their purpose is to cover the proper cross-lingual relations. Regarding the second case, the ILI is further extended both in EWN and BWN with some language specific concepts. The language specific concepts that are shared between Balkan languages are linked via a BILI (BalkaNet ILI) index [Tufis, 2004]. The initial set of common Balkan specific concepts consisted mainly of concepts reflecting the cultural specifics of the Balkans (family relations, religious objects and practices, traditional food, clothes, occupations, arts, important events, measures, etc). There are four morpho-semantic relations included in PWN and mirrored in EWN and BWN, Be in state, Derivative, Derived and Participle [Koeva, 2004]. Those relations semantically linked synsets although they can actually be applied to the literals only (graphic and compound lemmas). Consider the following examples: Be in state is an asymmetric inverse intransitive relation that links derivationally and semantically related adjectives and nouns. The English synset {attractive:3, magnetic:5} with a definition ‘having the properties of a magnet; the ability to draw or pull’ is in a Be in state relation with the synset {magnetism:1, magnetic attraction:1, magnetic force:1} with a definition ‘attraction for iron; associated with electric currents as well as magnets; characterized by fields of force’; also the synset {attractive:1} with a definition ‘pleasing to the eye or mind especially through beauty or charm’ is in a Be in state relation with {attractiveness:2} denoting ‘a beauty that appeals to the senses’. Derivative is an asymmetric inverse intransitive relation between derivationally and semantically related noun and verb synsets. For example the English synset {rouge:1, paint:3, blusher:2} with a definition ‘makeup consisting of a pink or red powder applied to the cheeks’ is in Derivative relation with two synsets: {rouge:1} with a meaning ‘redden by applying rouge to’ and {blush:1, crimson:1, flush:1, redden:1} denoting ‘turn red, as if in embarrassment or shame’. Derived is an asymmetric inverse intransitive relation between derivationally and semantically related adjective and noun synsets. For example the synset {Cuban:1} with a definition ‘of or relating to or characteristic of Cuba or the people of Cuba’ is in a Derived relation with {Cuba:1, Republic of Cuba:1}. Participle is an asymmetric inverse intransitive relation between derivationally and semantically related an adjective synset denoting result of an action or process and the verb synset denoting the respective action or process. Consider {produced:1} with a definition ‘that is caused by’ which is in a Participle relation with {produce:3, bring about:4, give rise:1} denoting ‘cause to occur or exist’. As can be seen by the examples, although the synsets are semantically linked, the actual derivational relations are established between particular literals. For the best performance of the multilingual data base in different text processing tasks a specification of the derivational links must to be kept at the level of literal notes (LNotes). There are systematic morpho-semantic differences between English and Slavic languages – namely derivational processes for building relative adjectives, gender pairs and diminutives. The Slavic languages possess rich derivational morphology which has to be involved into the strict one-to-one mapping with the ILI. A vivid derivational process rely Slavic nouns with respective relative adjectives with general meaning ‘of or related to the noun’. For example, the Bulgarian relative adjective {стоманен:1} defined as ‘of or related to steel’ has the Serbian equivalent {čelični:1} with exactly the same definition. Actually in English this relation is expressed by the respective nouns used with an adjectival function (rarely at the derivational level, consider wooden↔wood, golden↔gold), thus the concepts exist in English and the mirror nodes have to be envisaged. The gender pairing is systematic phenomenon in Slavic languages that display binary morpho-semantic opposition: male↔female, and as a general rule there is no corresponding concept lexicalized in English. The derivation is applied mainly to nouns expressing professional occupations. For example, Bulgarian synset {преподавател:2, учител:1, инструктор:1} and Serbian synset {predavač:1} that correspond to the English {teacher:1, instructor:1} with a definition: ‘a person whose occupation is teaching’ have their female gender counterparts {преподавателка, учителка, инструктурка} and {predavačica} with a feasible definition ‘a female person whose occupation is teaching’. There are some exceptions where like in English one and the same word is used both for masculine and feminine in Bulgarian and Serbian, for example {президент:1} which corresponds to the English synset {president:3} with a definition: ‘the chief executive of a republic’, and as a tendency the masculine noun can be used referring to females. Diminutives are standard derivational class for expressing concepts that relate to small things. The diminutives display a sort of morpho-semantic opposition: big ↔ small, however sometimes they may express an emotional attitude too. Thus the following cases can be found with diminutives: standard relation big ↔ small thing, consider {стол:1} corresponding to English {chair:1} with a meaning ‘a seat for one person, with a support for the back’ and {столче} with an feasible meaning ‘a little seat for one person, with a support for the back’; small thing to which an emotional attitude is expressed. Also, Serbian synset {lutka:1} that corresponds to the English {doll:1, dolly:3} with a meaning ‘with a replica of a person, used as a toy’ is related to {lutkica} which has both diminutive and hypocoristic meaning. There might be some occasional cases of the expression of that kind of concepts in English, {foal:1} with a definition: ‘a young horse’, {filly:1} with a definition: ‘a young female horse under the age of four’, but in general these concepts are expressed by phrases. There are several possible approaches for covering different lexicalization at different languages [Vitas & Krstev, 2005]: - treat them as denoting specific concepts and define appropriate synsets; - include them in the synset with the word they were derived from; - omit their explicit mentioning, but rather let the flexion-derivation description encompass these phenomena as well. Treating morpho-semantic relations, relative adjectives, gender pairs and diminutives, in Slavic languages as relations that involve language specific concepts requires an ILI addition for the languages where the concepts are presented (respectively lexical gaps in the rest). This solution takes grounds from the following observations: - relative adjectives, feminine gender pairs and diminutives denote an unique concept; - relative adjectives, feminine gender pairs and diminutives are lexicalized with a single word in Bulgarian, Serbian, Czech and other Slavonic languages; - relative adjectives, feminine gender pairs and diminutives in most of the cases belong to different word class comparing to the word from which they are derived (there are some exceptions, like diminutives that are derived from neuter nouns in Bulgarian) . Moreover, as with the other morpho-semantic relations, a special attribute assigned at the LNotes must provide information for one-to-one derivational relations. Although PWN’s coverage does not compare yet with new wordnets, the latter are continuously extended and improved so that a balanced global multilingual wordnet is foreseen, thus the task of the proper encoding of different level of lexicalization if different languages is in a great importance regarding the Natural Language Processing.",
       "article_title":"Towards the Global Wordnet",
       "authors":[
          {
             "given":"Cvetana ",
             "family":"KRSTEV",
             "affiliation":[
                {
                   "original_name":"Faculty of Philology, University of Belgrade",
                   "normalized_name":"University of Belgrade",
                   "country":"Serbia",
                   "identifiers":{
                      "ror":"https://ror.org/02qsmb048",
                      "GRID":"grid.7149.b"
                   }
                }
             ]
          },
          {
             "given":"Svetla ",
             "family":"KOEVA",
             "affiliation":[
                {
                   "original_name":"Institute for Bulgarian Language - Bulgarian Academy of Sciences",
                   "normalized_name":"Institute for Bulgarian Language",
                   "country":"Bulgaria",
                   "identifiers":{
                      "ror":"https://ror.org/00f23qy62",
                      "GRID":"grid.493346.f"
                   }
                }
             ]
          },
          {
             "given":"Duško ",
             "family":"VITAS",
             "affiliation":[
                {
                   "original_name":"Faculty of Mathematics, University of Belgrade",
                   "normalized_name":"University of Belgrade",
                   "country":"Serbia",
                   "identifiers":{
                      "ror":"https://ror.org/02qsmb048",
                      "GRID":"grid.7149.b"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"A French linguist, P. Cotte, remarks that the verb be collocates with qualifying adjectives having a positive or negative connotation (i.e. be courageous, be cowardly) whereas the verb have is followed by nouns expressing a positive quality only (i.e. have courage vs. * have cowardice). This have + Noun construction is quite special in that it expresses a characterisation of the subject with nouns expressing modal qualities which are focalised. Contrary to the usual expression of possession (John has a car) in which the object refers to an entity different from the subject, these qualities are part and parcel of the subject. The verb have seems to introduce qualities which differentiate this subject from other subjects, and among them, the speaker. But it does not introduce his/her shortcomings most probably because of an empathy akin to that delineated by C. Boisson. The use of nouns preceded by a zero determiner to express the positive characteristics of the subject implies that these characteristics can be shared by people other than the subject. On the other hand, the verb be can somehow distance the speaker from the referent of the subject although it is said to express an identity between the subject and its complement. This is probably due to the fact that its history is linked to deixis. The use of adjectives to express the characteristics of the subject implies that these characteristics apply to the subject only, not to the speaker. The corpus linguist can use various corpora to verify this hypothesis even if research on these corpora cannot be fully automated. One needs to check each and every hit to eliminate constructions such as have diesel models (plural compound nouns), Sometimes she had to have water fetched from miles up in the mountain (causative constructions) and, less frequently, … the emotional strain ambulance men have day in, day out (time adverbials). Then one can make the proper calculations and use the appropriate statistical tools. First I counted how many occurrences of several relevant combinations of have + Noun (beauty, courage, intelligence, patience, tact and their opposites) or be + Adjective (beautiful, courageous, intelligent, patient, tactful and their opposites) there were in the British National Corpus. The χ² test nearly always showed a significant difference between the two constructions. To refine this result I compared the percentage observed for each construction expressing a negative characteristic with its theoretical percentage (variance test). The results always confirmed the hypothesis. But it was not the same with constructions expressing a positive characteristic: a majority of be + Adjective combinations proved to be more frequent than the corresponding have + Noun combinations. This shows that a corpus linguistics approach helps isolate the constructions that do not comply with the hypothesis, which is a first step towards finding an explanation to these exceptions. Moreover, as the research progresses, the concordance lists highlight new characteristics of the constructions under study, such as the [± animate] and [± human] features of the subject. Preliminary results tend to show that there are significantly more inanimate subjects with the verb be. Finally, we see that searching through different corpora (the BNC and the BNC Baby) with slightly different software programmes (SARA or XAIRA) enables the corpus linguist to give a more complete answer to the question raised. SARA does not answer queries about grammatical categories while XAIRA can. This, together with the fact that the BNC Baby is much smaller than the BNC, allows me to find out what qualities or shortcomings are more frequently expressed with the verbs be or have, something that would be almost impossible to do with the BNC. Preliminary results show that the typical qualities and shortcomings in which the theoretical linguist is interested do not appear so frequently in the qualities and shortcomings associated with the verbs be and have. Among the five most frequent quality nouns associated with have (experience, knowledge, confidence, responsibility, character), only one (character) is mentioned by the theoretical linguist. The situation is even worse if one considers the ten most frequent qualities or shortcomings associated with be, which are, in decreasing order: able, nice, careful, quiet, honest, difficult, good, bold, important, strong. None of those traits is mentioned by the theoretical linguist, although further research is necessary to confirm those findings. This ongoing study thus shows that a corpus linguist can consider the problem from a point of view which is quite different but complementary to that of the theoretical linguist: the latter is mainly interested in concepts; the former both in concepts and occurrences.",
       "article_title":"Be and have: qualities and shortcomings",
       "authors":[
          {
             "given":"Pierre ",
             "family":"LABROSSE",
             "affiliation":[
                {
                   "original_name":"English Dept, University of Paris-Sorbonne (Paris IV)",
                   "normalized_name":"Sorbonne University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/02en5vm52",
                      "GRID":"grid.462844.8"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Like many people all over the world today, I’m involved in a project to transform a scholarly work from book form to online form. There is no longer any serious doubt about the value of such efforts, especially when the book in question is a reference work; but everyday scholarly experience shows that there are many questions about what sort of online form is best, and in a few cases reviewers have written analyses that detail the obstacles that can exist to the most advanced applications (see, for example, Needham, “Counting Incunables”; Daunton; Williams and Baker). One lesson of such reviews is how different the nature and uses of our various reference works are. All the same, some reflections on the general questions involved can be productive; this talk seeks to cover the relatively familiar issues of searching, and the less frequently discussed question of results display and organization. Our book is the Index of English Literary Manuscripts covering the period from 1450 to 1700, originally compiled by Peter Beal and published in four volumes from 1980 to 1993. Its aim was to catalogue all known literary manuscripts of a selection of writers; it was organized by author and work, not by the contents of manuscripts as is the norm. Those working on more modern material expect to encounter manuscripts that are in an author’s own hand; but most of those in the Index are copies by other hands, often combined in miscellanies with works of many other authors. Apart from facilitating work on the individual authors who were covered, the Index also spurred work on the nature of textual transmission in early modern Britain, where scribal publication continued to be important despite the advent of print (see Love). In making the case for the value of an online version of such a work, a standard claim is to point to the improved access that can be provided. It is less expensive to distribute the completed work online than as a set of hefty books, and it is also easier to find certain kinds of information in a searchable online publication. But as we know from the World Wide Web, even badly-done and inaccurate online resources can be put to use, so we need to look to some goal beyond this absolutely minimal one. As the analyses I’ve cited by Needham and others show, many uses that scholars can imagine for indexes and catalogues turn out not to be supported in online versions. In their transformation into online resources, many books go through a process of atomization into individual items, which may then be reunited in various ways: so that information once accessible only through the sequential order of the book, or through manually constructed aids such as indexes, might now be available in many ways. But, of course, these alternative routes depend on the data and on the machinery used to work with it: problems searches involving difficult forms of information such as dates are by now very familiar, and they stem from inconsistency of practice, lack of sophistication in search machinery, and problems in dealing appropriately with uncertainty. In our case the information was compiled not only following the usual sort of guidelines, but by only one person, so there is at least some chance of doing a reasonable job on this score. A further question is how well we can provide not just search results but an orderly view of a different perspective on the information. By this I mean a display that is organized and focused following the desired point of view, rather than merely being a set of search results. We are familiar with the way that a search on minimally-structured full-text resources produces a set of results that need some working through: the scope of the resulting passages is often not clearly delimited and you need to read around to figure out for yourself what the relevant piece of text is. The sequence of such results is also approximate: experienced scholars learn not to pay too much attention to it. These are systems that do not focus on exactly the results called for, and do not organize the results in the best manner; and improving on these matters in a full-text system is difficult. In a catalogue, or anything else that is based on more readily atomized information, there should be more scope for building new perspectives and not just lists of results, but this is another problem that has often proven difficult to solve (see Needham, “Copy Description”). This is more than a mere question of agreeable presentation: as work in the field of visualization has shown, finding ways to present known data such that our minds can work on it is a powerful method, and one that we need more of in the humanities. The printed Index of English Literary Manuscripts was an extreme example of a resource that offered only one perspective on information of interest from many perspectives, since there was never an index to offer alternative ways into the information. As examples will show, though, it is not always straightforward to build a display from a new perspective: in our case we are trying to preserve the original author/work perspective but also offer an organization by manuscript. That calls for more than just a reordering of entries: we find that the material within entries needs (at the least) to be rearranged, because even its organization expresses a perspective on the material. There may be a limit to the flexibility of this sort of catalogue, but an awareness of the issue when it’s being built can help us improve the design.",
       "article_title":"Constructing the <em>Catalogue of English Literary Manuscripts 1450 - 1700</em>",
       "authors":[
          {
             "given":"John ",
             "family":"LAVAGNINO",
             "affiliation":[
                {
                   "original_name":"Centre for Computing in the Humanities, King’s College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"As twenty-first century Australian citizens, the main way in which we “do” democracy is through the media. Every important issue in public life comes to us via the mass media, almost always structured as some form of ‘argument’. It is therefore important that these arguments are not only analysed, dissected and evaluated, but are also presented in ways that enable as wide a section of the population as possible to understand them, so that when citizens engage in deliberation about public issues, their dialogue will be logical, systematic and dialectical. Dialectic is two-sided. It is argument that is constructive and balanced. Now more than ever, at a time when “social, cultural and religious issues are of growing significance due to the insecurities of globalization and the increasing role of non-state players in the security environment” (Aust. Govt. 2004-2005: 9), we need not only greater scrutiny of mediatised arguments, but also stronger strategies for “[E]nhancing our nation’s understanding of social, political and cultural issues [which] will help Australia to engage with our neighbours and the wider global community and to respond to emerging issues” (Aust. Govt.: 8) To this end, my aims are threefold: to analyse a range of important public issues presented as arguments in the newspapers read by the majority of Australians, using the argument-diagramming computer software Araucaria (Reed and Rowe 2001); to evaluate each argument thus represented as diagrams, assessing each one in terms of its logical and dialectic reasoning; lastly, to use the diagrams to develop strategies for maximizing public understanding of important issues. A discussion of a small section of the research leading to the first two aims forms the content of the current paper, research that involves the trialling of the conjunction of argumentation schemes (Walton 1996) with the software Araucaria (Reed and Rowe 2001), capable of diagramming the construction of reasoning in a particular set of argumentative texts. The texts analysed are two sets of ‘columns’ of argumentative journalism (‘op-eds’) from the two ‘rival’ daily newspapers in Melbourne Australia, The Age, and the Herald Sun (the readership of both together comprising the largest per-capita newspaper reading population in Australia), owned respectively by Fairfax Ltd., and Rupert Murdoch’s News Ltd. Research of this type has not been done before in Australia in relation to newspaper arguments. Despite many studies on propaganda, bias, ethics, and news framing in the media, often focusing on the Australian media (see for example Hirst and Patching 2005), systematic empirical research into argumentative structures of news writing is tantalizingly absent. Not only is it important that mediated arguments be subject to analysis and critique, but also that the findings from such research be presented so that a wide section of the public can understand logical argument structures, and thus learn to decide for themselves whether arguments in the media are logically and dialectically reasoned. This is turn will aid citizens to engage optimally in deliberation about important public issues. Research suggests that currently this is not occurring (Kuhn 1991, Van Gelder 2003). As Van Gelder states: “Deliberation is a form of thinking in which we decide where we stand on some claims in the light of the relevant arguments. It is common and important, whether in our personal, public or working lives. It is also complicated, difficult and usually poorly done” (Van Gelder 2003, emphasis added). My project seeks to address this problem by presenting some of the results in computer-generated graphic format—by making argument maps. In this way, the inconsistencies, biases, and other fallacies will be able to be clearly understood by as wide a section of the public as possible. The benefit of such maps over verbal explanations has been articulated by Van Gelder (2003): “Although prose is the standard way to present reasoning, it is not a good tool for the job. Extracting the structure of evidential reasoning as typically presented in prose is very difficult and most of the time we do it very badly…” It is hoped that my project will serve to make argument-mapping more widely known and understood, in terms of both its use in assessing arguments presented in the media, as well as in its potential to improve reasoning and deliberation in the public sphere. It is likely that some or many arguments will be found—by way of the argument-diagramming and subsequent evaluation—to be one-sided or dialectically ill-reasoned. The proportion found to be like this will have a major bearing on the conclusions of my project. If, for example, the majority of the arguments are found to be invalid, and not to contain a sequence of reasoning, this has serious implications for public understanding of and reasoning about important issues—that is, the skills of deliberation. Some research has suggested that these skills are not possessed by up to half the population (Kuhn 1991). But is this inability to deliberate over public arguments due to lack of “basic reasoning and argument skills”, or due to arguments presented by the media in ways that are illogical, faultily-reasoned or one-sided? Is it possible that flawed or fallacious arguments in the public sphere might in some way be causally linked to this lack of basic reasoning and argument skills, and so to the inability of many to deliberate over important issues? Such questions underpin the aims of my project, because deliberation is crucial in any public sphere, not simply because it is about achieving consensus, but rather because it about seeking knowledge.",
       "article_title":"Our Daily Dose of Rhetoric : Patterns of Argument in the Australian Press",
       "authors":[
          {
             "given":"Carolyne ",
             "family":"LEE",
             "affiliation":[
                {
                   "original_name":"Media & Communications Department,     University of Melbourn",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The aim of this study is to explore the mechanisms of digital contextualisation with respect to literary studies. Reversing Willard Mc Carty’s definition of the field (“Humanities computing is an academic field concerned with the application of computing tools to arts and humanities data or to their use in the creation of these data”), I wish to examine in what way literary tools, in the broadest sense, apply to digital and digitised documents; and, in return what these findings add to literary studies and more specifically to literary analysis proper. More specifically, I intend to show in what respect the notion of the “Gradiva complex” in its widest acceptation (breathing life into a fixed representation such as a statue), can apply to multimedia representation through the concrete study of two different interfaces. In reference to this, I shall first briefly comment upon the vexed link between modernity and tradition, a paradox emblematic of Humanities Computing which harmoniously blends the old and the new, giving a new lease of life to past ideas. Humanities Computing provides old representations with a brand new and dynamic frame. Such original context necessarily alters what it displays and may therefore lead to new discoveries. This is why we should take a fresh look at old methodological habits and old academic divides. Thanks to elaborate interactive devices, what up until now seemed fixed and almost fossilised is literally brought back to life and made more accessible to each one’s sensitivity. This is highlighted in the CD-ROM Freud, Archéologie de l’Inconscient where a sophisticated multimedia environment stages, among other things, the way in which memories are repressed. The section entitled “Gradiva” - referring to Freud’s famous 1906 essay Delusions and Dreams in Jensen’s Gradiva – digitally illustrates how Jensen’s short story acts as a literary parable for the psychoanalytic process of repression, disclosing both the workings of the unconscious and those of literature - more specifically here, the metaphorical process. As we explore the CD-ROM and use the pointer to drag to the centre icons or hot-spots that would otherwise remain partly hidden in the corners1, an eerie music starts playing. The sounds echo the whirl of smoke that runs across the screen acting as some kind of Ariadne’s thread for the whole information display; it is an aesthetic and vivid reminder of Freud’s presence. “Gradiva”, the name given to the Pompeii bas-relief representing the protagonist’s obsessive dream, is visually present at the centre but gradually buried under a pile of cinders. Meanwhile, a male voice-over reveals that the stone woman is a masked figure for a long-forgotten love. The way in which information is made accessible prevents the user from random investigation. If he or she remains passive, the screen is blurred; it is only when the user deliberately points to a specific element that the latter figure emerges from an ever-changing background (the process sometimes requires both patience and reflection, as when the next page is solely made accessible by piecing a jigsaw together). The designing choices (roll-over, invisible/visible layers) coincide with the issue at stake: the will to make sense of symptoms on the one hand, of signs and symbols on the other; a difficult quest for knowledge and understanding that mirrors both the approach of psychoanalysts and of scholars. The digital extract briefly evoked here can be interpreted as a modern expression of the “Gradiva complex”: the desire to breathe life into a fixed and cold representation. Such complex typifies the graphic designer’s art or perhaps his or her fantasy. In this particular case, even though the CD-ROM is meant as a general introduction to Freud’s discoveries, the fact that it relies on a multimedia support implies constant cross-references not only to different forms of representation (static or dynamic, visual or auditory), but to different sources of knowledge (cinema, literature, photography, biology, medicine) that shed light on each other and add to the revivifying effect of the tool. Digital contextualisation enhances our own understanding of each field’s specific nature by hyperlinking it to what it isn’t: maps, music, historical data etc. As McCarty puts it: “Humanities computing [...] is methodological in nature and interdisciplinary in scope”. In the CD-ROM Georgian Cities, broaching 18th century Britain, a short animation accessible in the section entitled “A rhetorical comparison”, aptly illustrates ICTs’ metonymic nature: a quotation taken from Henry Fielding’s The History of Tom Jones mentioning one of Hogarth’s prints (The Morning) pops up unawares from each corner of the screen while the parts of the engraving successively referred to, are highlighted on the copy displayed next to the words: “[Miss Bridget] I would attempt to draw her picture, but that is done already by a more able master, Mr Hogarth himself, to whom she sat many years ago, and hath been lately exhibited by that gentleman in his print of a winter’s morning, of which she was no improper emblem, and may be seen walking (for walk she doth in the print) to Covent-Garden church, with a starved foot-boy behind, carrying her prayer book.” [book one, chapter XI]. The preterition device used by the narrator who initially “refers to [Miss Bridget] by “professing” not to do so, consists in the ekphrastic depiction of one of Hogarth’s works. In the text, the witty depiction leads to the sketching-out of an almost grotesque figure. In the CD-ROM, the duplication of the print, juxtaposed with the text discloses the writer’s technique. Once faced with the actual visual representation, the reader is made to understand the ekphrasis’ literary purpose – to add distance between the subject and the imaginary object, to conceal whilst feigning to disclose. The object of literature and that of art in general remains unattainable, it lies beyond signs, beyond the visible. In the examples aforementioned, the processes depicted seem to be duplicated by the multimedia device. The promotion of freedom that ICTs permit, along with their inherent dynamism, go hand in hand with the surfacing of cognitive processes invisible up to that point. Therein lies ICTs’ main interest in terms of innovative academic research. Humanities Computing formally overcomes the paradox between the old and the new, it also gives rise to unique questioning. It stresses the link which exists between the invisible but apt guidance involved in browsing and the interactive dimension of the tool. This link is an inextricable entanglement between apparent freedom and subtle guiding that has forever existed (notably with the literary notion of narrative viewpoint), but which is made more apparent here. Moreover, ICTs bring to light the tie that exists between creativity and learning; from objective data gathering and display, to the freest associations of ideas, thanks to interactivity. All echo the way we learn what we learn: willingly and objectively but also haphazardly and subjectively. We learn because we want to or we have to, but also because the world leaves its mark onto us. Instead of showing how new technologies can implement humanities technically-wise, I would therefore like to show how the new medium enhances literary theory in itself; in other words - how does praxis affect theory? How from the objective compilation of documents, all rationally organised and displayed, does one move to the more irrational, and the more creative? How does one shift from the objective, the technical and the logical to the more poetical association of ideas; from knowledge acquisition to creativity? If one follows the statements put forward by the philosopher Paul Ricoeur in The Rule of Metaphor, the metaphorical process - at the core of all literary practices - stems from the context not the actual word. It is the whole that prevails, not the part; or rather, the whole is meant in each of its parts and vice-versa. Once again, isn’t it most fortunate that the new medium should now be utilized in order to enhance these very ideas? To illustrate these points, I will use a Powerpoint presentation displaying relevant recorded extracts taken from the two cultural CD-ROMs aforementioned.",
       "article_title":"Digital Humanities or the Gradiva Complex",
       "authors":[
          {
             "given":"Séverine ",
             "family":"LETALLEUR",
             "affiliation":[
                {
                   "original_name":"English Department,     Université de Paris-Sorbonne (Paris IV)",
                   "normalized_name":"University of Paris",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05f82e368",
                      "GRID":"grid.508487.6"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"For the ‘hard-core’ computing humanist, e-learning seems to be a non-topic.1 While foundational issues and Humanities Computing (HC) curriculum development have been central to our debates, the technological and didactical nitty-gritty of e-learning appears to offer little insight into questions of theoretical and conceptual relevance. - This contribution argues to the contrary: (1) The current form of e-learning is shaped to a significant degree by business and political interests. (2) Consequently, some of the dominant commercial e-learning platforms are conceptually flawed: they are implicitly based on a simplistic ‘cognitive pipeline’-model of learning. (3) HC can help to deconstruct simplistic e-models of learning and contribute towards a more ‘intelligent’ computational modelling of learning processes. (4) The latter will be demonstrated by way of a practical example, the intermedial narratological e-course ‘NarrNetz’. (1) Vested interests The impression of e-learning as a non-inspiring ‘coal-face’ activity is partly a result of its swift appropriation by commercial software vendors. Indeed: there’s considerable money in e-learning – but where does it come from? In Europe, the EU and its member states have created various funding structures reserved for e-learning projects, notably eLearning: a programme for the effective integration of Information and Communication Technologies (ICT) in education and training systems in Europe which aims at the strategic proliferation of e-learning across the European school and university systems. 2 Meanwhile, the initial euphoria about e-learning – fuelled in no small way by university administrators’ and politicians’ expectation to introduce a cheaper, less staff-intensive way of teaching – has certainly worn off. The new buzzword is ‘blended learning’: the combination of physical classroom teaching with auxiliary e-learning. According to a survey presented in Schulmeister (2005), the vast majority of e-courses at non-virtual colleges and universities is indeed of an auxiliary (preparatory, supplementary or remedial) nature. Schulmeister concludes that in our present situation “E-learning ist die Reparatur am System, das wir einführen” – e-learning is the quick-fix for the BA/MA-system whose current introduction at German universities leads to a dramatic over stretching of teaching resources. (2) The pipeline model of learning However, not all short falls can be attributed to context. In our own practical experience, part of the methodological and didactical problems associated with e-learning stem directly from restrictions imposed by its platforms. One of the key functionalities of a good e-learning platform is the personalized, dynamic assignment of navigation options (so-called ‘content release’) on the basis of results achieved by a user in preceding tests. Depending on the scoring, hyperlinks may be made visible or activated, or the system may send some additional feed-back. If the content manager decides to use these functionalities, the course content must allow for tests and parallel navigation paths. Under these conditions, this kind of interactivity can be hard-wired into the course by the content developer who uses system provided templates for content and test modules (questionnaires, multiple-choice etc.) and combines these with logical or quantitative operators that specify navigation rules. To understand the conceptual premises built into commercial e-learning platforms (also known as LMS = Learning Management Systems), it is important to take a closer look at the kind of binary logic that drives this type of interactivity. For example, the currently still widely used WebCT Campus Edition 4 allows neither for the specification of complex (conjunctive or additive) rules of the type ‘if student X passed test 1 with > 65% and has already looked at modules 15-17, allow him/her to jump to module 22, else do Y’, nor for a logical ‘or’ in the control of a user’s navigation - fairly trivial process control decisions which a human teacher will make on the fly in any given class-room situation.3 The impact of such concrete technological restrictions on e-learning’s current shape is severe, considering that the recently merged WebCT and Blackboard alone account for a joint 2/3 market share in institutional and commercial e-learning.4 Courses implemented on the dominant proprietary platforms, unless their content modules are made available in pure (non-conditional) hyperlinked form, are thus implicitly based on what we call a ‘pipeline’ model of learning: the learning process within the system is organised in terms of a linear accumulation of pre-defined knowledge and skills. While the genuine dynamics of interactive learning in a real class room is the result of engagement in recursion, exploration and the adoption of a different perspective onto the same problem, systems based e-learning thus reduces much of this functionality to mere repetition (‘repeat until you pass’), while outsourcing true interactivity to humans (via chats, discussion list, e-mail etc.). Another principled problem with out-of-the-box e- learning systems is that they are themselves not designed to learn. Whereas commercial data-base driven websites like Amazon’s combine continuously updated user- profiling (items bought, pages looked at, duration of visit, etc.) with probability based feed-back (‘other users who have bought item X also bought item Y’), similarly intelligent behaviour has as yet not been implemented in the leading commercial e-learning or course-ware systems. While most e-learning platforms are of course able to track user behavior, they remain unable to act on process-relevant conclusions that might be drawn from this data.5 (3) Towards adaptive e-learning So, is (e-)learning by necessity a dumber activity than shopping? Certainly not. Current AI research explores various possibilities of making it ‘feel’ more intelligent. For example, Nakano, Koyana and Inuzuka (2003) explore the role of probabilistic algorithms in the automated generation of questions; Steinemann et al (2005) as well as Yang et al (2005) look into the design principles of more sophisticated course management systems. Providing the user with intelligent corrective feed-back is the main goal of INCOM,6 a project dedicated to developing an e-learning supported logic programming course (Le 2005, Le and Menzel 2005). On a more general level, Moreno et al. (2005) emphasize the need for so-called ‘adaptive e-learning’ systems: ‘ Adaptive e-learning is a teaching system which adapts the selection and the presentation of contents to the individual learner and his learning status, his needs, his learning style, his previous knowledge and preferences.’ This topic receives particular attention in the AH (Adaptive Hypermedia and Adaptive Web-Based System) conference series.7 ALFANET, a project supported by the EU-financed IST- (Information Society Technologies) initiative, also aimed to introduce adaptivity.8 According to the project’s self-description, the prototype can dynamically evaluate a user’s knowledge and then selectively assign different content material in order to reinforce or broaden a subject.9 However, the final project report proves this to be an aim, rather than a demonstrable accomplishment.10 (4) Opening the black box While we stake no claim to developing an intelligent e-learning system, we do believe that an individual e-learning course can certainly be turned into a more stimulating and challenging experience through comparatively simple system adjustments – provided the system itself is not a proprietary black box. Whether such adaptations and extensions can later be implemented as generic system functionalities remains to be seen. ‘NarrNetz’ – short for ‘Narratology Network’ – uses the widely adaptable PHP/MySQL-based open source platform ILIAS11. Our project is conceptually based on a learning metaphor and an architecture that combines linear modules with explorative, highly personalised learning experiences. Elements of NarrNetz try to emulate the user experience of an interactive computer game – a paradigm widely expected to revolutionize the ‘look and feel’ of e-learning (Begg, Dewhurst, Mecleod 2005; Foreman/Aldrich 2005; Schaffer 2005). In NarrNetz, the user is initially confronted with a problem modelled on a gaming ‘quest’. However, the system and its architecture allow the combination of linear with non-linear, systematic (deductive) with immersive (inductive) learning methods. If learners want to fast-track their progress, they can switch to a text-book like learning style. Conversely, while sections of the ‘learnflow’ follow a compulsory linear trajectory, others may be chosen and arranged at will. Fig. 1 sketches out the NarrNetz-learnflow which maps the user’s navigation through the landscape of the metaphorical quest onto the didactically desired cognitive progression. In our presentation we will demonstrate how comparatively simple modifications and modular extensions of the open-source e-learning platform ILIAS and the integration of multi-media objects can lead to a significantly enriched, more ‘intelligent’ learning experience. Specifically, we will discuss the following additional functionalities: • integration of user-specific avatars with dynamically attributed ‘real-time’ properties symbolizing the user’s past progress and future navigational options at any given point in time; • automated, personalized navigation control based on disjunctive reasoning and the concept of multiple, functionally equivalent learning paths rather than a pipeline-approach; • representation of the newly acquired skills and knowledge elements in the form of a personalized dynamic conceptual glossary; glossary entries can be cross-referenced, individually annotated and linked to content elements of the course. Conclusion The conceptualisation and implementation of non- standard, experimental e-learning architectures constitutes a genuine task of Humanities Computing. As in other fields of applied humanities computing, computing humanists are called upon to point out the conceptual shortcomings in simplistic attempts at reverse engineering human cognitive competences for the mere sake of making them ‘computable’. Learning is the most basic cognitive competence in human beings: a good enough reason not to turn a blind eye on how it is being conceptualised – technologically and ideologically.",
       "article_title":"De - Constructing the e - Learning Pipeline",
       "authors":[
          {
             "given":"Jan Christoph ",
             "family":"MEISTER",
             "affiliation":[
                {
                   "original_name":"University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          },
          {
             "given":"Birte ",
             "family":"LÖNNEKER",
             "affiliation":[
                {
                   "original_name":"University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The aim of the Pcube (Policarpo Petrocchi Project) is to create a digital archive about the cultural and intellectual production of Policarpo Petrocchi. This archive will preserve and disseminate digital objects and their associated metadata, using current technology and metadata standards and will implement features and functionality associated with the Semantic Web. Policarpo Petrocchi was an important intellectual and political figure during the second half of the ninetheenth. Petrocchi is famous as the author of the “Il Novo dizionario universale della lingua italiana” (The new universal dictionary of Italian language), used widely in the early twentieth century, and he was also the author of many literary works, reviews and translations, as the L’Assommuàr of Émile Zola, and the founder of the society “Italia e Lavoro”. Petrocchi’s cultural production has left us with an “information legacy” composed of extremely heterogeneous materials, which can be divided in three main categories. The first category is archival materials, including letters, press clippings and manuscripts. Second is the published works, including literary works, educational and grammar books, novels and translations. The final category is the iconographic material, composed of loose photographs and a family photo album. Moreover none of these materials has been digitized until now, and the relative metadata have been catalogued in two separated databases: the first, for the archival description, with WinISIS <http://www.unesco.org/ webworld/isis/isis.htm>, using a structure that has been designed to be ISAD(G) compatible; the second, for the bibliographic records, has been created with EasyCat <http://www.biblionauta.it/biblionauta/easycat.php>, using the ISBD(G) <http://www.ifla.org/VII/s13/pubs/isbdg.htm> format, with the possibility of an UNIMARC <http://www.ifla.org/VI/3/p1996-1/sec-uni.htm> export. Even if widely used in library and archival contexts, these two databases are far from optimal starting points, because, even if both of them can be implemented as an OPAC using plug-in and extensions, they cannot be directly integrated, and this is contrary to the goal of having a homogeneous and open system as the infrastructure for this digital archive, which must integrate electronic texts, digital images and metadata about both the analog and digital objects. Currently, the best model for this kind of infrastructure is certainly the Open Archival Information System (OAIS), with its concept of the different types of “Information Packages” and in particular the Archival Information Package (AIP). With the OAIS Model in mind the architecture of the Policarpo Petrocchi Digital Archive has been designed with three different levels. The first level is the data layer, perhaps the most delicate one, because it’s where actually lie the roots for any possibility of interoperability between the different data sets. Another classification of the different materials can be made using the nature of the digital representation that will be obtained from them, being either visual or textual. All the photos, most of the letters and manuscripts, and the most important part of the literary works will be digitized in an image format, following the guidelines of the Digital Library Federation<http://www.diglib.org/>, with a high quality format for preservation and a lower quality format for dissemination. XML will be used for the digitizing the content of some of the archival documents and novels, and for the encoding of the metadata of all the analog items. EAD (Encoded Archival Description) <http://www.loc.gov/ead> is the most suitable metadata format for the archival documents, and EAD allows links to digital representations, wheter textual or iconographic, from the EAD finding aid. For published articles and books, the MODS <http://www.loc.gov/standards/mods> schema a rich bibliographic metadata format, will be used. Another issue which will be described, is the possibility of conversion into EAD and MODS from the legacy data of the databases, in a direct way, using XML export features, with some string manipulation through a programming language, or, when needed, in a manual way. The content of the books and the manuscripts will be encoded using the TEI Guidelines. The coherence and cohesion of all these standards, and of all the related digital objects, can be achieved using the METS <http://www.loc.gov/ standards/mets> schema, the primary function of which is the encoding of descriptive, administrative and structural metadata of the items constituting a digital object(14). The second level is the framework layer, the implementation of the software architecture which has to provide the basic functions of a digital library, using as a base the data of the first level. During the last couple of years the number of this kind of software programs, and their availability in open-source mode, has constantly increased(15). Notwithstanding this, due to the characteristics of the Policarpo Pretrocchi Digital Archive, what is needed is an high level of customization, and the integrated use of the Apache Cocoon Framework <http://cocoon.apache.org> with the XML Database eXist <http://exist.sourceforge.net> its probably the most suitable choiche. Using these two programs, the potentialities of the XML technologies XQuery and XSLT offer a lot of possibilities, from textual and metadata researches to electronic edition with multiple output format and text/image visualization. The final level is the semantic layer, which aims to create a network of relationships among the items contained in the archive and possibly external resources and thereby to offer advanced navigation functionalities to the archive users. The ontology takes as a model the CIDOC-Conceptual Reference Model (CRM) <http://cidoc.ics.forth.gr>, which “provides definitions and a formal structure for describing the implicit and explicit concepts and relationships used in cultural heritage documentation”. This abstract model must be rendered to an actual syntax and for this role has been chosen the ISO standard of the TopicMaps <http://www.topicmaps.org> with the XTM serialization. The reason of this choice is the growing adoption of TopicMaps in the Digital Humanities community, compared for example to RDF/OWL. Starting from the particulare case of the Pcube project, this paper will analyze, extract and outline the general guidelines and the best strategies in the creation of a digital archive composed by very different starting materials, in order to make these strategies applicable to several other projects, which are currently facing the same issue in crossing the borders from the old models of the cultural heritage preservation towards the new paradigms of the digital library.",
       "article_title":"Pcube – Policarpo Petrocchi Project : The Architecture of a (Semantic) Digital Archive.",
       "authors":[
          {
             "given":"Federico ",
             "family":"MESCHINI",
             "affiliation":[
                {
                   "original_name":"Tuscia University",
                   "normalized_name":"Tuscia University",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03svwq685",
                      "GRID":"grid.12597.38"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"While electronic databases have constantly improved as regards their quantitative and qualitative validity and relevance, compromises have sometimes been made in their tagging by relying on pre-corpus- linguistic grammatical descriptions, resorting to automatic (i.e., non-interactive) tagging, or imposing neat category labels on the data. One of the motivating factors in my present project, which aims at producing a web-based tagged corpus of Scottish letters (Corpus of Scottish Correspondence, 1500-1730, CSC), is my conviction that, besides ensuring the authenticity of data by digitising original manuscripts rather than editions, it is also necessary to create text annotation principles and methods appropriate for dealing with the great degree of linguistic variation and variability recorded in historical documents. Tagging may distort evidence by applying overly rigid rules in categorisation, so ignoring the inherent fuzziness of categories, and by simplifying complex patterns of variation or using tags that fail to reflect processes of change over a long time-span. These problems are particularly challenging in the tagging of corpora for the study of those regional and local varieties of English which may significantly differ from previously described standardised varieties. One of these is Scots, an internally quite heterogeneous variety. In my paper I will discuss variationist principles of tagging, focusing on how multiple category membership resulting from variation over time and space has been dealt with in the system applied to the CSC. The general practice is that the tag, consisting of a lexel and a grammel, provides descriptive information about structural and contextual properties, and may contain comments permitting, for example, semantic specification or disambiguation, but the properties listed in the tag do not suggest a straightforward categorisation. Instead, the tag indicates the variational pattern that the tagged item is a member of, allowing the creation of valid inventories for the study of the continual variation and change inherent in any variety, idiolectal, local, regional, or supraregional. In creating variationist typologies, the deciding factor is that variants in a particular typology have been observed to show patterning at a particular level of analysis, be it structural or syntactic, or related to communicative or text-structuring functions. Thus the textual, discoursal, sentential and clausal levels are distinguished from one another. For example, subordinators such as since, as, because and for (the first two in the role of cause) and connective adverbs such as therefore may be members of the same semantically defined inventory, but their membership of a syntactically defined group is not as straightforward as the traditional categorisation into conjunctions and adverbs would suggest. For in particular requires a careful analysis of the data. When the qualifications of these items for membership are assessed at the discoursal and textual levels, there is evidence that they are not members of the same patterns of variation (Meurman-Solin 2004a). Secondly, the system has been tailored to meet the challenge of tracing developments over a long time-span. This requires that the source of a particular item or collocate is kept transparent over time, even though a later grammaticalisation or reanalysis, for instance, would permit recategorisation. Thus, according to is tagged 'accord/vpsp-pr>pr and 'to/pr<vpsp-pr, seeing that 'see/vpsp-cj{c} and 'that/cj<, and exceedingly 'exceed/vpsp-aj-av and '-ly/xs-vpsp-aj-av, indicating that the ultimate source of all these is a present participle. Tags providing information about potential rather than established membership of categorial space are intended to ensure that comprehensive inventories can be created for examining the full scale of variation. For example, the tag for the variational pattern of upon (the) condition that includes upon this condition that ('condition/n-cj<pr), even though the determinative element this positions the variant at the end of the cline where an abstract noun followed by a nominal that-clause as the second unit would be analysed as an appositive structure. The approach to lexicalisation is the same. Consequently, in an invariable collocate such as anyway the two units are tagged separately, and their interrelatedness is indicated by arrows: 'any/aj>n-av and 'way/n-av<aj. The third factor stressed in the theoretical approach is the inherent fuzziness and polyfunctionality recorded in language use when examined by drawing on representative large-scale corpora. The tagging principles have been influenced by the discussion of notional or conceptual properties, elaborated tags making the interrelatedness of the members of a particular notional category explicit (Anderson 1997). Even though conventional part-of-speech labels are used in the tags, strings of such labels indicate fuzziness and polyfunctionality by referring to the co-ordinates of items on a cline, rather than insisting on membership of a single category. Concepts such as ‘nouniness’ and ‘adverbhood’ reflect this approach. Right honorable as a term of address is tagged 'right/av, 'honour/aj-n{ho}-voc and '-able/xs-aj-n{ho}-voc ({ho} commenting on the honorific function, ‘voc’ being an abbreviation of vocative). The tag type with two core category names combined by a hyphen (here aj-n) permits searches concerned with fuzziness or polyfunctionality. This practice can be illustrated by conform to, 'conform/aj-pr>pr and 'to/pr<aj-pr, and as soon as, 'as/av>cj 'soon/av-cj 'as/cj<av. Ambiguity is made explicit by using comments such as {syntactic ambiguity>} and {syntactic merger>}, which alert the user to re-examine the problematic structure that follows the comment. In applying the variationist approach to the reconstruction of historical language varieties, it is necessary to keep in mind that due to scarcity of texts, in the early periods in particular, there may be significant gaps in our knowledge base. Ideally, members of a particular pattern of variation have all been attested in the data as genuine alternatives, and can be related to one another by addresses provided by the tags. In practice, if only fragments are available, rather than a balanced and representative collection of texts, we will also have to deal with potential, though unattested, variants. In these cases, we must resort to typological information on corresponding systems in later periods, or those representing other languages or language varieties. Therefore, another essential ingredient in the theoretical approach is that zero realisations are included in variationist paradigms. There is a wide range of zero realisation types, and the tagging principles vary accordingly. Elliptical items, including features carefully discussed in grammars such as that-deletion, are indicated by comments such as {zero v}, {zero aux}, {zero that}, and {zero S}, while in the relative system both a comment and a tag introduced by a zero are used: {zero rel} followed by '/0RO{y1}, for instance. Since the explicit marking of semantic relations at various levels is a salient feature in early letters, zero marking being much less frequent in the historical data examined than in similar registers in Present-day English (Meurman-Solin 2004b), comments are also used to indicate the absence of logical connectors. It should be noted that, in principle, the term zero-realisation can be considered misleading, since it may not always be possible to verify empirically that something has been omitted. However, in the present approach a zero realisation is indicated when a variational pattern comprising explicitly expressed variants and those left implicit has been repeatedly attested in the data.",
       "article_title":"Tagging Categorial Fuzziness and Polyfunctionality",
       "authors":[
          {
             "given":"Anneli ",
             "family":"MEURMAN-SOLIN",
             "affiliation":[
                {
                   "original_name":"Helsinki Collegium for Advanced Atudies, University of Helsinki",
                   "normalized_name":"University of Helsinki",
                   "country":"Finland",
                   "identifiers":{
                      "ror":"https://ror.org/040af2s02",
                      "GRID":"grid.7737.4"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The SMIL Annotation Film Engine (presented at Digital Resources in the Humanities 2002) was a middle scale computing humanities project that sought to combine a real time video stream, an idiosyncratic metadata scheme, and the annotation and presentation affordances of SMIL. The first version of this project “Searching”, was based on John Ford’s 1956 western _The Searchers_ and a metadata scheme was defined from a hermeneutic claim, that “doorways in _The Searchers_ represent liminal zones between spaces that are qualities”. Doors, as they appear in the film, were encoded around a small data set (camera is inside, outside, or between, and is looking inside, outside, or between) and still images from the film are provided. A search by a user yields all the stills that meet the search criteria, and clicking on any still loads the appropriate sequence from the film for viewing in its cinematic context. As I argued in 2002, “Searching” operated as a ‘discovery engine’ rather than a metadata archival project, so what it developed in the engine is a process for the unveiling of patterns of meaning, and this would seem to offer an engaged middle ground for distributed humanities content that provides access to material while also foregrounding an applied critical or interpretive activity. Since 2002 such idiosyncratic data schemes have emerged via social software as folksonomies, and I have adopted the implications of this earlier project to develop a series of interactive video ‘sketches’ that foreground and explore the implications of a critical videographic essay practice. This is the writing of video, sound, text, and image into audiovisual knowledge objects that are porous to the contemporary network. These works have implications for new media and humanities pedagogy, as they offer a possible heuristic for teaching and learning that is between the traditional essay and the ‘interactive’ (whether online or CDROM hardly matters) reference work. These sketches are academic works that aim to reverse the usual hierarchy (and ideology) of relations between text and image that exists in academic research by allowing video and sound to ‘drive’ the writing (and vice versa). The model proposed allows academics or students to write their own interactive video based essays that are multilinear, academically sound, and network appropriate. The work has implications for humanities disciplines that study time and image based media, new media theory, and contemporary multimedia pedagogies, and the ‘paper’ will be such an ‘networked knowledge object’.",
       "article_title":"Writing With Different Pictures : New Genres for New Knowledges",
       "authors":[
          {
             "given":"Adrian ",
             "family":"MILES",
             "affiliation":[
                {
                   "original_name":"Applied Communication. RMIT University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Digital gazetteers—databases of named places--are becoming widely recognized as an integral element of any application or library that includes geographical information. In the humanities, where historical maps and texts are replete with place names but precise spatial information is often limited, it is often a good practice to conceive of the development of geographical information as beginning from place names (a gazetteer) rather than geometry (a GIS). As a reference work, digital gazetteers are an exceptionally useful tool in their own right, making it possible to trace the history of named places, relate places to one another, link multiple names for the same place across languages, and (with even rudimentary georeferencing), and make maps. As a component of a digital library or web application, gazetteers permit place-based or map-based search, display and integration of any kind of content that includes named places. Finally, using existing service protocols, multiple gazetteers can be searched and used together, allowing very specialized research in historical geography to be used in new contexts. This paper has several components. First, I introduce recent research in digital gazetteers for digital libraries and the humanities, with a primary focus on work by the Alexandria Digital Library and modifications to its standard by the Electronic Cultural Atlas Initiative. Second, I work through several examples of gazetteer design from my personal research in early modern Chinese history, looking at frequently changing places in a time of rapid political change, complex and localized religious spatial hierarchies, and multiple, coexisting civil and military spatial systems. I reflect on a classic cultural geography notion of space from Yi-fu Tuan that “as centers of meaning, the number of places is enormous, and cannot be contained in the largest gazetteer.” In response to Tuan, who made this claim in 1975, I respond both that more recent developments in database design and networked systems mitigate the “largest gazetteer” concern; but also, I claim that it is possible—and, for use in the humanities and related fields of cultural heritage management, essential—to create a gazetteer that acknowledges places as centers of meaning, not just administrative spaces. Finally, I discuss on emerging work by myself and other colleagues in the design of named time period directories. While digital gazetteers are a well-recognized genre (indeed, their antecedents begin long before the digital era), databases of time periods are not. Nevertheless, search, display and integration of historical and cultural information demands that such a resource be developed. Times, like places, have names—overlapping, multiple, hierarchical, and rich with information. Just as time modifies place (as cities are founded, rivers change course, or empires are vanquished), so too does place modify time (as the Abbasid Caliphate refers to a political formation on a particular part of the earth at a given time, or the Neolithic Period begins and ends at different times in, say, the Yellow River valley or the Andes). This paper introduces some of the issues concerned with data modeling and visualization of time period directories along with gazetteers in digital atlases, and discusses next steps.",
       "article_title":"Digital Gazetteers and Temporal Directories for Digital Atlases",
       "authors":[
          {
             "given":"Ruth ",
             "family":"MOSTERN",
             "affiliation":[
                {
                   "original_name":"Founding Faculty     School of Social Sciences, Humanities and     Arts University of California, Merced",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In response to a demand made by LUNO (Lorraine Open University), the teaching team of the CTU (University Distance Teaching Centre) of Nancy 2 offered to create a module entitled “Chairing a meeting in English / Conduire une réunion en anglais”. LUNO is an open university regrouping the main higher education establishments of Lorraine, which are bound by a charter. The project was mainly funded by the Regional Council of Lorraine in September 2000. The members of LUNO have developed numerous course modules in their respective fields of competence. The online courses are accessible to any employee or job-seeker in Lorraine. The “Chairing a meeting in English” module is thus aimed at learners in vocational training schemes who wish to follow on-line courses. The specificity of the learners was the first criterion to be taken into account in the preliminary draft. Contrary to academic students, who are extrinsic goal orientated (main objective: to obtain a grade), such learners are more intrinsic goal orientated (main objective: personal challenge) and are consequently more likely to set specific learning goals. However, we endeavoured to arouse the interest of those “non academic” students even more and to adapt the teaching material to the requirements of learners who are already in the labour market. The other criterion was the learners’ differences in language level. Some, who have just obtained their professional baccalauréat, just needed to learn expressions belonging to a specific lexical field. Others, who had finished secondary education a long time ago, wanted to revise the basic language skills before acquiring this specialized vocabulary. Our ambition was to elaborate a module whose flexibility enabled the needs of the broadest and most heterogeneous public possible to be met. We had also to take into account the degree of motivation and commitment of the participants. The rate of dropout being particularly high in distance teaching, we wished to find a means of captivating the learner’s attention throughout the module. Streaming video seemed to us the most appropriate tool to fulfil this role providing that we used it advisedly. We focused our efforts both on the teaching contents and on the storyboard. After the initial work consisting in looking for linguistic tools, mainly in books specialized in commercial and business English, we wished to write dialogues with the objective of arousing the learner’s interest. We planned to give a touch of humour to the content. Thus took form the LMD company (Lorraine Mirabelle Drinks), a liquor manufacturer, whose exports to the United Kingdom are threatened. The agenda of the meeting would primarily be devoted to the analysis of the financial situation and to the measures taken to put the company on its feet again. The “Chairing a meeting” module would be used within the framework of LUNO with a reduced tutorial scheme (complete self-training or reduced tutorial). It was a collaborative training rather than a co-operative one. Collaboration (learner-machine) asking for more autonomy than co-operation (learner/teacher and machine), we wished to simplify ergonomy (video and audio quality, screen visibility, design, weight) to offer the best learning environment possible and to facilitate the learner’s orientation within the module. We had to adapt the technique to the requirements of pedagogy and not the reverse. The video recording was conducted by VIDEOSCOP, a production company of Nancy 2, with significant technical means and staffing. Five native speakers played the various parts: managing director, personnel manager, sales manager, managing director, staff representative, salesperson for Britain. We decided to divide the recording into 11 sequences to allow the learner to adapt his use of the tools to his own pace. For each sequence, we privileged interaction: the possibility of reviewing a passage of the recording, of making a pause, etc. The originality of the module lies in the “surgical cutting” of very short sequences. It offers the learner the possibility to view only the sentences useful to “chair a meeting” .Isolated from their context (the problems encountered by the LMD company...), learners are invited to use them again in a different context. They are accompanied by synonymous sentences, available in the form of audio recordings, in order to offer a wide range of linguistic tools that are used for taking or giving the floor, reformulating, concluding, etc. Different learning activities provide diverse forms of formative assessment. The variety of short activities encourages the learner. Some exercises comprise several levels of difficulty to try to meet the needs of each learner. Lastly, a lexicon (French-English and English-French) is also available. We made the choice to oblige the learner to click on a word if he wants to obtain the translation of it, in order to enhance his receptiveness. An evaluation of the module by the learners has been carried out. The priority of this questionnaire is to evaluate the use of video. The form includes a rating scale and open-ended questions that ask what the learner thinks about the training materials and activities. This research does not include the evaluation of the trainer and the training environment. Our main objective is the upgrading of new video based material that we want to realise. We also intend to revise the different tasks offered in this module to fit the needs of future learners. After the presentation of the “Chairing a meeting” module, the conference will be the occasion to analyze the results of this evaluation, by stressing the contribution of video in the module.",
       "article_title":"The teaching contribution of video in the “Chairing a meeting” module",
       "authors":[
          {
             "given":"Marc ",
             "family":"NUSSBAUMER",
             "affiliation":[
                {
                   "original_name":"Université Nancy 2, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Rev. Patrick Stephen Dinneen’s Irish-English Dictionary is widely regarded as the most authoritative scholarly dictionary of modern Literary Irish currently available. Published in 1934, it has seen numerous reprints, but is not available in digital form. Indeed, no scholarly Irish-English dictionary of modern Literary Irish is available in digital form. The Corpus of Electronic Texts, supported by the Irish Research Council for the Humanities and Social Sciences, has embarked upon a three year project to deliver a digital version of Dinneen’s Dictionary. The Research Associate responsible for the preparation of the Digital Dinneen is Julianne Nyhan, and the Principal Investigator on the project is Professor Donnchadh Ó Corráin, University College Cork (hereafter UCC). Dr Gregory Toner, from the University of Ulster at Coleraine, and Dr Seán Ua Súilleabháin from the Department of Modern Irish at UCC, are associate investigators. Dinneen’s dictionary is a complicated document that contains a wealth of information. Its data comprises, inter alia, headwords, grammatical information, definitions, usage examples and translations, as well as references to dialectical sources and to informants used by Dinneen. This paper will briefly discuss the TEI mark-up of other Irish language dictionaries, such as the eDIL (electronic Dictionary of the Irish language). The use of TEI to encode Dinneen will then be illustrated, and the necessary customisations briefly discussed. In addition to creating a digital edition of Dinneen, the research assistant is endeavouring to develop an edition that is more user-friendly that the hard copy edition of the same work. Much of the information contained in the dictionary remains inaccessible even to experienced speakers of the language, because the hard copy contains mixed font (Cló Gaelach/Roman), and many people are not able to read Cló Gaelach. This barrier will be removed from the digital edition, and end-users will have the choice of viewing the text in either mixed font or Roman font only. Furthermore, the dictionary contains orthographic forms that pre-date the spelling reform of 1946, and as many current day speakers are unfamiliar with such orthography, they have difficult locating headwords. The systematic incorporation of modern orthographic forms as meta-data will enable access for modern speakers who are unfamiliar with historical spelling - and today they are probably the majority of Irish speakers. This paper will focus on how the Digital Dinneen will be integrated into the existing CELT infrastructure, and into the wider infrastructure of Humanities Computing in Irish. An electronic Lexicon of variants of medieval Irish, the subject of Julianne Nyhan’s PhD research, will become available on the CELT website in 2006/2007. Incorporating id references into Dinneen will allow end users to trace a word back to its medieval form in the Lexicon of variants. Furthermore, for the last two years, CELT has been involved in a cross-border collaboration with the University of Ulster at Coleraine, where an electronic edition of the Dictionary of the Irish Language (eDIL) is being prepared. It is hoped that links between Dinneen and the eDIL will be easily generated. The research carried out at CELT into an XSLT lookup tool to facilitate links between works cited in eDIL, and the corresponding text in the CELT corpus, will also be extended to Dinneen. This mechanism will enable scholars on-line access to works cited by Dinneen - many of which are available only in the best research libraries. A Javascript plug-in, that enables end users of the CELT corpus to highlight a word, click on a lookup icon and retrieve the word in question from either the Lexicon of medieval Irish or the Digital Dinneen will also be discussed. Finally, the possibilities for future research will be touched upon, for example, the possibility of augmenting Dinneen’s citations with examples from the CELT corpus as more texts are added to it.",
       "article_title":"The Digital Dinneen Project : Further Avenues for CELT.",
       "authors":[
          {
             "given":"Julianne ",
             "family":"NYHAN",
             "affiliation":[
                {
                   "original_name":"CELT Project, History Department,     University College Cork, Ireland.",
                   "normalized_name":"University College Cork",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/03265fv13",
                      "GRID":"grid.7872.a"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Monumenta frisingensia – the 10th century Freising Manuscripts (Slovenian: Brižinski spomeniki) – are the oldest written Slovenian text and also the oldest Slavic text written in the Latin alphabet. The manuscripts consist of three religious texts, comprised within the Codex latinus monacensis 6426 (Munich State Library). The Monumenta frisingensia (MF) were written in a script based on Carolingian minuscule after AD 972 (MF II and MF III probably before 1000) and before 1039 (MF I most probably by 1022 or 1023). The provenance of the MF is either Upper Carinthia or Freising (now in Austria or Germany respectively); in any case, they were used in the Carinthian estates of the Freising diocese. Slovenian historians, linguistic and literary scholars consider the MF to be the very first document of early Slovenian language. This is the reason why these manu scripts bear an outstanding importance not only for the scientific comprehension of the development of Slovenian language and literature but also for Slovenian national identity and historical consciousness. In this respect Monumenta frisingensia represent the genesis and the beginnings of Slovenian national individuality. The digital edition of the Monumenta is based upon a printed one (the critical edition by the Slovenian Academy of Sciences and Arts), which encompasses a complex apparatus with facsimile, diplomatic, critical and phonetic transcriptions, translations into Latin and five modern European languages, and a dictionary covering these transcriptions and translations (plus Old Church Slavonic). This edition furthermore contains introductory studies, apparatus, a bibliography and an index of names. The MF are preserved in one single manuscript only. For this reason, the traditional philological and critical task – to examine the ms. witnesses – is not possible in this case. On the contrary, it is of great importance that the edition is founded on the opposite concept: on that of reception. In what way the MF were studied, understood and published? What were the main differences between the transcriptions and editions? Can a digital edition grasp at least crucial points of their research during the two centuries from their discovery, 1806–2006? For this reason, besides all the components of the printed edition, the digital one contains a selection of earlier diplomatic and critical transcriptions (in full text) and enables parallel views to compare them. This is of considerable importance as the Frisingensia were often a subject of polemical scientific interpretations, especially with regard to the place of their origin and their linguistic genesis. The edition should, therefore, enable a pluralism of readings and understandings as well as a hierarchy of proofs and ascertainments. By inclusion of all major transcriptions of the Freising Manuscripts since the very first diplomatic transcription in 1827 (P. Koeppen), the e-edition offers a panoramic retrospective of 200 years of research of these precious manuscripts. Such a retrospective, evidenced by historical transcriptions themselves, can reveal cultural and philological history of the Frisingensia. As another extension, specific for digital edition, we decided to include the digitized audio recordings of the read manuscripts and integrate the segmented audio files to both phonetic transcription and modern Slovenian normalization. The scope of this audio extension is to offer a reconstruction (based on historical linguistics) of medieval Slovenian speech which can be sensibly juxtaposed and compared with transcriptions. We plan to publish a pedagogical adaptation of this e-edition as well, aimed for school-reading, where audio component with the archaic pronunciation can produce a clear image of historical development of the Slovenian language. Such materials present significant challenges for encoding, esp. the high density and variety of markup, extreme parallelism (per-line alignment between the text views), and special historic and phonetic characters used in the transcriptions. The paper details our methodology used to turn the printed edition (plus extensions) into a Web edition with a standardized encoding, extensive hyperlinking, and multimedia capabilities. We concentrate on the following issues: - Structuring of materials in a text editor - Adoption of standard solutions: Unicode/XML/XSLT/TEI-P4/ - Collaborative practices using fast prototyping and cyclical improvement: up-converting to XML–TEI --> down-converting to HTML --> proofs, corrections --> re-applying the up-conversion … Our presentation details these issues, and highlights the most challenging aspects of entire process, where the central goal is to enable a complex and lively communication with the 10th century Monumenta frisingensia.",
       "article_title":"Monumenta Frisingensia — a Digital Edition of the Oldest Slovenian Text",
       "authors":[
          {
             "given":"Tomaž ",
             "family":"ERJAVEC",
             "affiliation":[
                {
                   "original_name":"Institute of Slovenian Literature,     Scientific Research Centre",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Matija ",
             "family":"OGRIN",
             "affiliation":[
                {
                   "original_name":"Department of Knowledge Technologies,     Jožef Stefan Institute",
                   "normalized_name":"Jožef Stefan International Postgraduate School",
                   "country":"Slovenia",
                   "identifiers":{
                      "ror":"https://ror.org/01hdkb925",
                      "GRID":"grid.445211.7"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Based on a consideration of recent editorial criticism concerning issues of marginality, this presentation calls for developing a greater number of hypertext editions of children’s literature. To demonstrate an engagement with and a response to such criticism, this presentation introduces a hypertext edition of the literary tale “The Grey Wolf” by George MacDonald (1871), in which the guiding methodology and digital design, interface, materials, and critical apparatus fit the particular research and educational needs of scholars and students of children’s literature. This presentation proposes that more critical hypertext editions of children’s literature will continue to help reduce scholarly assumptions that dismiss children’s literature as straightforward, transparent, and marginal to the more central concerns of literary criticism. Recent feminist and minority textual studies critics have already identified some of the ways in which editorial and publishing practices reproduce hegemonic and positivist discourses that relegate certain authors and genres to a marginal status in bibliographical and literary studies. For example, feminist critic Ann Thompson argues that the masculinist discourses implicit in editorial conventions have produced critical editions that re-inscribe male-biased assumptions, selections, omissions, and interpretations of texts. Similarly, William Andrews points out that while the literature of minority authors is beginning to appear more frequently in electronic archives and student editions, this editorial work is done in place of developing research-based critical editions for the use of professional academics. But although many kinds of children’s literature infrequently contain textual allusions, challenging language, and references that require annotative explanations, children’s literature scholar Jack Zipes argues that the simplistic narratives and symbolic elements that characterize children’s literature often carry the most potent and subversive political agendas for the very reason that they are considered harmless and transparent. Taking cues from these critics’ awareness of how the elision of marginal voices and texts creates disproportionate literary interpretation, dissemination, and canonization, this presentation considers the dynamic potential of communicating the relatively marginalized genre of children’s literature in critical hypertext editions in ways that would benefit scholars and students. With each new publication of a text that belongs to a “marginal” genre, hypertext critical editions produce what D.C. Greetham describes in Theories of the Text (1993) as the “deferral and dispersion” (5) of the pervasive hierarchical and hegemonic editorial conventions that continue to privilege certain texts, authors, and genres over others. Thus, this presentation aims to demonstrate the theoretical, practical, and pedagogical fit between research strategies in children’s literature studies and the capabilities of hypertext to present and connect visual, contextual, literary and bibliographical material. Because the oral folk tale has no single origin, and because critical work in the area of children’s literature often compares thematics and narrative structures in related historical, national, and transnational texts, hypertext editions of children’s literature can be designed to present multiple versions of narrative material in ways that de-centre traditional editorial principles of “best text” and “authorial intention.” Moreover, critics have recently argued that the editor’s annotations contribute to the (re)production of the text in fundamental ways, further problematizing the notion of “authorial intention.” For example, in discussing some of the methodological issues that the editors faced in creating The Prufrock Papers: A Hypertext Resource for “The Love Song of J. Alfred Prufrock” (1999), Peter Stoicheff and his colleagues regard the hypertext edition as that which “permits a variety of copy-texts” whose hyperlinks and critical apparatus create “a new kind of interpretive ‘copy-text’.” Feminist editor Brenda Silver further proposes that the adaptation of a text is a form of editorializing that does more than interpret an original text; adaptation, in the author’s view, performs its interpretation. In a related vein, Julia Flanders argues that the xml transformation of the codex text into hypertext necessitates semantic tagging, and that editor’s interpretive annotations combine with the text at the xml level. It follows, then, that any hypertext edition is not only a re-presentation of the author’s text but is itself an adaptation, in which the author’s content and the editor’s presentation of the content join as a single, collaborative performance. Guided by this editorial criticism, then, an annotated hypertext edition of a children’s literary tale would enact the editorial theory that encourages a de-centering and de-privileging of traditional editorial conventions by offering a maximum amount of relational play between textual versions and literary interpretations, of which the hypertext edition itself is one. Not only does the editorial theory concerning issues of marginality encourage a de-centring of versions and authorial intent, but hypertext editions that prescribe to such methodologies can also offer the user diverse interpretive materials for analyzing children’s literature. For example, literary tale scholars often trace the historical development and codification of oral folktales, collate and compare versions, and examine codex editions as cultural artifacts in that they reflect and reproduce political, economic, and moral norms and values. Also, the hypertext edition can offer additional interpretive dimensions to the text by reproducing bibliographical information in digital images, including watermarks and signatures, as well as graphics such as the scripts, borders and illustrations. The hypertext edition can also present audio or visual cinematic adaptations of the literary tale for comparison. Thus, there is immense potential for reproducing children’s literature in hypertext format, particularly since the electronic text may – with hyperlinks, split screens, digital image rendering and audio/visual clips – present a variety of research materials relevant to children’s literature studies that the traditional codex edition could not. In the spirit of Peter Shillingsburg’s assertion that “experimentation and a variety of approaches are to be encouraged” in hypertext edition production, this presentation suggests ways in which the digital transmission of children’s literature de-centres editorial conventions while facilitating scholarly research in this area. To demonstrate this fit between certain editorial theories concerning marginality and adaptation and the practice of studying and comparing textual and literary elements of children’s literature, this presentation introduces a hypertext edition of “The Grey Wolf,” first published in 1871 in a ten-volume set of MacDonald’s work, entitled Works of Fancy and Imagination. This hypertext edition offers different interpretations of the text through colour-coded popup annotations and through the presentation of related texts in a split-frame interface. It also contains an enumerative and analytical bibliography of the 1871 edition, and compares this edition to a paperback edition published by Eerdmans over one hundred years later (1980). Furthermore, this bibliographical component of the edition familiarizes student users with the field of textual studies, and suggests how bibliographical elements may contribute to literary interpretations in meaningful ways. My approach to editing “The Grey Wolf: A Hypertext Edition” is guided by an integrated matrix of textual studies theory and literary criticism, an approach that has been called for by a growing number of textual studies and literary critics, such as Michael Groden and Marie-Laure Ryan. This approach also draws attention to the issues of marginality, versioning and the hypertext apparatus through which the text is mediated, as well as how this remediation inevitably influences the reception and interpretation of the text.",
       "article_title":"“The Margins of the Scholarship: Children’s Literature and the Hypertext Edition”",
       "authors":[
          {
             "given":"Elan ",
             "family":"PAULSON",
             "affiliation":[
                {
                   "original_name":"English - University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In the last few years a consistent number of papers focussed on new theoretical frameworks for scholarly digital editions practice (Vanhoutte 2000a, Vanhoutte 2000b and Flanders 1998). The main attention has been paid on text encoding, on the production of apparatus criticus or variourum and its transposition form the paper to the digital format (Vanhoutte 2000a, Vanhoutte 2000b, McGann 1996 and Lavagnino 1996). Another focus is the role of the editor and whether a scholar edition must have as a goal the production of a text (a quotable text) or just different textual materials in order to allow the reader to choose his/her version of the text; a third possibility being an equilibrium (to be found) between the two school of thinking (Vanhoutte 2005). Connected to the latter point is the consideration that a digital edition based on encoding can be re-used for many purposes (e.g. electronic inspection, computer assisted analysis), and gives the possibility of producing different versions of the same text (critical, diplomatic, facsimile, reading editions, hypertext editions), according with different kind of users and readers from the same encoded text by the application of different stylesheets. In this paper I will address to a different consideration of stylesheets to be seen not only as a tool to produce different layers of the encoded texts, but as an essential component of the scholar’s work and a possible shortcut toward a compromise in the dichotomy textual criticism vs. cultural criticism, as in Vanhoutte 2005. Some time ago I started to prepare a digital edition of an Italian Renaissance’s comedy, Lo Stufaiuolo, written by Anton Francesco Doni, presumably in 1550-1551. We posses two different autographic manuscripts of the text, each of them witnessing a different and remarkable version, a fact that involves the necessity of a synoptic edition. The first question to be answered in digital edition field is certainly which kind of edition to produce: an editor-oriented or a user-oriented edition. In my opinion, according with the Italian editorial school’s theories, an editor must produce a quotable, reliable text or at least, to provide an orientation to the reader: the editor is probably the person who know the most about the text, and can not skip the task to provide the readers his circumspect opinion/version of the text. On the other side, an editor must also provide all the documentation possible of his work, according to a fundamental presumption of the scientific nature of a critical edition that lays on the possibility for the reader to reproduce/ control/verify the editorial work. For that reasons I chose to provide different versions of the text (of both texts, in this case), ranging from a minimum to a maximum of editorial intervention, in order to satisfy different readers’ needs, accompanied by ancillary documentation on the editorial work and by the photographic reproduction of the source. Due to the big interest of the Doni’s language, a peculiar point of the edition would have been the preserving of the original face of the text, allowing at the same time to non-scholar readers to appreciate the text itself, that is very funny and with a huge literary value. Starting the work, I focussed my attention to the very first level of the language: orthography and punctuation. In paper-based critical editions there is often the necessity of re-write the text in order to allow modern reader to easy decrypt it. The orthography and punctuation rules of most modern languages have sensibly changed from Middle Age since today. At the dawn of the writing of modern languages we can not even speak of orthography but of different writing habits, relayed to different cultural centres. Furthermore, very early texts are in scriptio continua and for many centuries the word-borders have been sensibly different from the present ones. Orthography and punctuation have reached the present relative stability through different phases and approximation levels in relative recent years; for Romance languages, for example, the Latin graphical habits had a long persistence in writing. Normally, in the editorial practice, editors “translate” from ancient writing-face, according to modern rules, declaring in preliminary sections of the printed book the transcription’s criteria. In this way paper-based editions are able to give just a summary of the original’s reality and can not be used by scholars who want to study the original language face, punctuation, orthography or the writing supports: as for apparatus criticus or variorum, transcription criteria in prefatory pages do not allows de facto the possibility of reproducing the editorial work. In such cases to turn back to original or facsimile editions is the only possibility, but such editions are totally resistant to electronic inspections and limit the scholar’s chance of availing large corpora of data. This limitation can be overruled by electronic editions based on text encoding. In fact, the encoding allows adding new meaningful layers without losing any aspect of the original face of the text. For example, adopting the TEI encoding schema, it is possible to encode the original face of the text as follow: <p><orig reg=”Perché”>Per che</orig> le lettere sono state sempre poste <orig reg=”nei”>ne i</orig> degni <orig reg=”e”>et</orig> <orig reg=”onorati”>honorati </orig> luoghi…</p> We can also preview a more sophisticated encoding able to classify the kind of regularization provided (after having extended the TEI encoding schema in order to enable a “type” attribute for the <orig> element): <p><orig reg=”Perché” type=”wordBorder”>Per che</orig> le lettere sono state sempre poste <orig reg=’nei’ type=”wordBorder”>ne i</orig> degni <orig reg=’e’ type=”lat”>et</orig> <orig reg=’onorati’ type=”lat”>honorati</orig> luoghi…</p> With such encoding it will be possible to display the text according to the <orig> face or to the reg face using suitable XSLT stylesheets. In case of multiple editions from the same text, we can also think to editions regularizing each time particular sets of “type”, according with different purposes, for example considering just punctuation or just orthography regularization. The usage of stylesheets can be also anticipated from the delivery phase to the encoding/editorial phase. A useful application of stylesheets during the encoding can be managed by the encoder/editor to monitor its own work and its interventions on the text. Stylesheets, in fact, are able to produce at any moment of the work statistical data in real time, a fact which can help the editor to better understand the writing habits of the author or of the copyist and, at the same time, to keep under control its own work. Similar considerations can be made over the encoding of readings and lemmas, where lists of refused or accepted readings can profitably support theeditorial work in every phase. Stylesheets are a low-cost (or a non-cost), flexible and suitable technology, relatively easy to understand in its principal applications and can be managed directly by the editor, not a secondary concern in low budget projects. In the delivery phase, the same lists and data can be produced to the scientific community, giving explicitness to the editorial practice and overruling the compactness of the introductory notes of paper-based editions. Stylesheets can be reasonably considered not only an ancillary part of the editorial practice, a technology to be recalled after the finish of the scholar work (maybe committed to computer science people), but a part of the work committed to the editor himself/herself, able to support his/her tasks in monitoring the work step by step and able to produce different texts for different tastes, under the editorial control. The paper will conclude by presenting the digital edition of the Doni’s Stufaiuolo, in order to exemplify the results obtained with such methodological approach.",
       "article_title":"Just Different Layers? Stylesheets and Digital Edition Methodology",
       "authors":[
          {
             "given":"Elena ",
             "family":"PIERAZZO",
             "affiliation":[
                {
                   "original_name":"University of Pisa - Department of Italian Studies",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The elaboration of critical edition of sources basically includes the setting up of indexes. In the paper media, the index provides an overview of the content, and makes it possible to access directly a piece of information : the name of a place or a person in the case of index nominum, or a concept in the case of index rerum. Though, indexes are hardly ever integrated in electronic publishing and textual databases, and this can be easily explained. On the first hand, in most cases, the goal of such initiatives isn’t to propose critical editions of sources, but to gather vast amounts of texts. Search forms are the main access tool, and the primacy of full text used to make indexes seem useless. On the other hand, setting up an index is a long and fastidious work, and the cost is not affordable for commercial publishers. Though, even within the digital media, and particularly on the Web, indexing is a worthy effort. It can solve the problem of lemmatisation, which is very hard to proceed automatically on a medieval corpus, especially regarding the diversity of writings of the names of people and places. Moreover, the index provides an overview of the names that appear in the work, when in textual databases there is no way to have a global approach of the corpus. Today, we’re beginning to see on the Web real critical editions, that rely upon the original structure of this kind of scholarly work ; logically, they also include an index, since this is part of the academic tradition of critical edition. Through this approach, it is possible to outline innovative uses of indexes within the context of electronic publishing. While in the paper media the index was only a hard to use location tool, within hypertextual organisation of information, it becomes a quick access tool, gives the overview of the corpus when discovering it for the first time, and reveals new courses of reading. Beside some disparities between different works, an index is composed of four stable parts : • the entry • the different writings in the text corresponding to this entry • biographical and/or genealogical explanations • the location of occurrences in the text. Name indexes are often organized hierarchically, inducing links between entries. All this information represents a first interpretative layer. Though, the index is stuck in this purpose of finding and accessing information, and these data aren’t used to analyse the corpus. It would be possible to use this work as the basis of an ontology, which would be a common index for various works or corpus, playing a role similar to a list of authorities. A computer ontology, not to be confused with philosophical ontologies, allows to describe concepts and relations between these concepts. The advantage of an ontology over a relational database is to enable logical rules between concepts and relations, and to go further than the hierarchical classification traditionally used for indexes. For instance, the symmetric relation “ brother of ” and the property “ A is brother of B ” allows to automatically deduce the inference “ B is brother of A ”. Similarly, the relation “ child of ”, transitive to the relation “ parent of ”, and the property “ A is parent of B ” the inference “ B is child of A ” is deduced. This mechanism, for simple and obvious that it seems to a human being, is actually quite complex to reproduce in the context of information treatment automation. There are various languages to set up an ontology. The OWL language, defined by the W3C and based on RDF, allows, thanks to the XML syntax, a simple XSLT transformation of the data encoded in the corpus in XML towards the OWL ontology. To populate the ontology, we can use the XML structure of historical works encoded following the TEI DTD, to gather all the names of people and places indexed in a regular form, and the relation of these names with the documents in the corpus. For a corpus encoded with the P4 version of TEI, the empty tag <index/> can be used to locate a point in the text, but it is not possible to reuse the different forms of the indexed entries. On the contrary, the <persName> tag with the reg attribute allows to encode the name of a person and to indicate the regular form, and so is it with the <placeName> for places. Once we defined the structural framework on index entries, we can work on the intended structure of the ontology in order to represent social networks. The location of the occurrences of the entry in the corpus structured in XML allows to automatically create relations between names of persons and places, and the structural entities of the corpus. Thus we can also indicate the role or place of the persons : in the case of a charter, we can indicate if it’s the author of the juridical action, the beneficiary or a witness, for example. Finally, we have to add the relations between people and placing, basing ourselves upon the relations revealed by the document, or other primary or secondary resources. Three visualisation methods can be used to fully reveal the benefits of this approach. In the first place, a visualisation close to the presentation of traditional indexes is required in order to ensure the transition towards assimilation of the digital media by the scholars. Second, the ontology is proposed in a manner proper for giving an overview of the content : faceted navigation interfaces propose combined lists that allow the end-user to define by himself different courses in the corpus. Finally, the graphical visualisation reveals the full potential of the ontology. The graph makes the relations between people, documents and places tangible, as well as the relations types that have been defined. Thus we can unveil the social networks that exist in a hidden state in the documents. The visualisation in the form of a graph proves the usefulness of ontologies when encoding an index nominum. Because a relational database cannot show relations other than hierarchical, or deduce inferences of expressed relations, it cannot completely realise the modelling of social networks. On the contrary, an ontology allows this process and transforms the index from a classical access point into an real tool for analysing the corpus. The social networks revealed by the ontology in the graph form become easier to apprehend, and this will constitute, once this technology is implemented on significant historical corpus, an essential manner to study the presence of a group of people in a geographic space according to their relations. The setting up of the index nominum as an ontology relying on XML structured works shows how a technology like RDF can grow the interest of a well identified tool using the modelling of information.",
       "article_title":"De l’index nominum à l’ontologie. Comment mettre en lumière les réseaux sociaux dans les corpus historiques numériques ?",
       "authors":[
          {
             "given":"Gautier ",
             "family":"POUPEAU",
             "affiliation":[
                {
                   "original_name":"École Nationale Des Chartes",
                   "normalized_name":"École Nationale des Chartes",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/013xvg556",
                      "GRID":"grid.462175.3"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Most current conceptual modeling methods were originally designed to support the development of business-oriented database systems and cannot easily make computationally available many of the features distinctive to cultural objects. Other modeling approaches, such as traditional conceptual analysis can complement and extend contemporary conceptual modeling and provide the computing humanist with methods more appropriate for cultural material and humanistic inquiry. The Humanities and the Problem of Method Dilthey famously distinguished the methods of the cultural sciences from those of the natural sciences, claiming that the natural sciences seek to explain whereas the sciences of culture seek to understand as well. Although there is no generally accepted account of this distinction, it is still a not uncommon belief that when humanists analyze, explain, and interpret the cultural world, they are, at least in part, using distinctive methods. The question has a long history but it is now especially acute in the practice of humanities computing. One compromise is to accept the separation and treat computational support as preliminary or ancillary — or, even if constitutive, partial, and the lesser part. We believe that such a resolution will result in missed opportunities to develop intrinsic connections between the methods of managing computational support and traditional methods of advancing humanistic insight. Conceptual Analysis The early Socratic dialogues focus on cultural concepts such as justice, piety, courage, beauty, friendship, knowledge, and so on. Socrates asks what these are and attempts to determine what features are significant, sometimes considering hypothetical cases to elicit modal intuitions, sometimes reasoning discursively from general principles. This now familiar style of reasoning may be called “conceptual analysis”, or, when formalized with the general principles articulated as axioms, “axiomatic conceptual analysis”. Often discussions of cultural objects by humanist scholars can be seen to be some variation of this sort of reasoning or situated within a framework of concepts which could be explicated in this way. This approach to understanding cultural facts has been widely criticized, from both hermeneutic and positivist quarters; however recent work on the nature of social facts may provide some support. Searle and others have argued that social and cultural facts are established through acts of “collective intentionality” (Searle, 1995). If so then at least part of the nature of that reality would seem to be directly accessible to the participating agents. We cannot investigate galaxies, electrons in this way, because we in no way create them as we do poetry, music, and social institutions. Searle’s account is consistent with the approach taken by the phenomenologists of society and culture, such as Reinach and Ingarden, as well as with classical Anglo-American philosophical analysis (Smith, 2003). An Example: Bibliographic Entities In the Functional Requirements for Bibliographic Records (FRBR) the conceptual modeling is explicit and the conceptual analysis latent. In the text of FRBR we read (IFLA, 1998): Work: “a distinct intellectual or artistic creation” Expression: “the intellectual or artistic realization of a work in the form of alphanumeric, musical, or choreographic notation, sound, image, object, movement...” (e.g., a text). Manifestation: “the physical embodiment of an expression of a work”. (e.g., an edition). Item: “a single exemplar of a manifestation”. (e.g., an individual copy of a book) The novel Moby Dick, a work, is realized through various expressions, the different texts of Moby Dick, including different translations. Each one of these expressions may be embodied in a number of different manifestations, such as different editions with different typography. And each of these manifestations in turn may be exemplified in a number of different items, the various individual copies of that edition. Each entity is also assigned a distinctive set of attributes: works have such things as subject and genre; expressions a particular language; manifestations have typeface and type size; and items have condition and location. Below is the “entity relationship diagram” representing these entities and relationships: Figure 1: ER Diagram of FRBR Group 1 Entities and Primary Relationships [diagram from IFLA (1998)] Entity relationship diagrams are a widely used conceptual modeling technique in the development of information management systems and there are algorithms for converting ER diagrams into robust lower level abstractions, such as normalized relational tables, that can be implemented in database systems. However standard ER diagrams cannot make all aspects of cultural material computationally available. There is no method for saying explicitly under what formal conditions entities are assigned to one entity set or another, for distinguishing entities from relations and attributes, or for identifying necessary or constituent features. Moreover, relationships are understood extensionally, and modal or other intentional assertions, including propositional attitudes and speech acts that are critically important in the study of society and culture cannot be expressed. (Renear and Choi, 2005). Extending Conceptual Modeling with Conceptual Analysis [Caveat: In what follows we intend no position on the plausibility of any ontological theory of cultural objects. Our claim is only that there are such positions, that they cannot be easily represented with current conceptual modeling techniques, and that they can be represented with other techniques.] The text of FRBR provides much information that, despite appearances, is not represented in the FRBR ER diagram. Some of this disparity has been discussed elsewhere (Renear and Choi, forthcoming); here we take up features especially relevant to cultural material. For example, the FRBR ER diagram does show embodiment, realization, and exemplification relationships, of course, but it does not indicate their particular significance. The “is” of “...is the physical embodiment...” is not the “is” of mere predication. It is a conceptually constitutive “is”: we are being told not just a fact about manifestations, but what manifestations (conceptually) are. The cascade of definitions suggests this formalization: work(x) … x is an artistic or intellectual creation expression(x) =df (∃y)[realizes(x,y) & work(y)] manifestation(x) =df (∃y)[embodies(x,y) & expression(y)] item(x) =df (∃y)[exemplifies(x,y) & manifestation(y)] Now we can see that the concept of work is taken as a quasi-primitive entity, the three characteristic relationships are also each primitive, and essentially involved in the definitions of the entities, and the appearance of interdefinition is made explicit. Because none of this is modeled in the FRBR ER model, that model does not fully represent FRBR’s perspective and, moreover, these features will not be reflected in information systems generated from that model and will not be computationally available for analysis. Bibliographic Platonism Already we see inferences not entirely trivial, such as the theorem that bibliographic items imply the existence of corresponding (abstract) manifestations, expressions, and works: P1 item(v) ⊃ (∃x)( ∃y)( ∃z) [manifestation(x) & expression(y) & work(z) & exemplifies(v,x) & embodies(x,y) & realizes(y,x) ] But consider the converse of that conditional: A1 work(v) ⊃ (∃x)( ∃y)( ∃z) [item(x) & manifestation(y) & expression(z) & exemplifies(v,x) & embodies(x,y) & realizes(y,z) ] A1, a bibliographic analogue of the Aristotelian thesis that only instantiated universals exist does not follow from the definitions. Represented in this way FRBR now raises a traditional problem for Platonist ontologies of art: if works are abstractions existing independently of their instantiations, then how can they be created? Bibliographic Aristotelianism An alternative approach could take items as primitive. work(x) =df (∃y)[IsRealizeBy(x,y) & expression(y)] expression(x) =df (∃y)[IsEmbodiedBy(x,y) & manifestation(y)] manifestation(x) =df (∃y)[IsExemplifiedBy(x,y) & item(y)] item(x) =df a (material) artistic or intellectual creation This is a “moderate” realism in which A1 is now a theorem and P1 no longer one. Here abstract objects cannot exist independently of their physical instantiations, although they do exist (as real objects) when their corresponding items exist. However P1 will certainly need to be added as an axiom to support our intuition that items do imply works in any case. Or, another approach to the same end is to leave work as primitive, as before, but add P1 as an axiom. Either might better fits our commonsense intuitions about artistic creation, But we may now have problems characteristic of moderate realism: how to exclude abstract objects which have an intermittent being, going in and out of existence as their instances do — which would be in contradiction to another commonsense intuition: that “a thing cannot have two beginnings in time” (Locke). A Third Way In FRBR the notion of a work seems poorly accounted for, tempting further development. Jerrold Levinson defends this definition of musical work: x is a musical work =df x is a sound/performance_means-structure-as-indicated- by-S-at-t. Levinson argues that works are “initiated types” (other examples: the Ford Thunderbird and Lincoln penny) which do not exist until indication but once created exist independent of their concrete instances. Our intuitions about artistic creation are now accommodated, but at a cost: a special class of abstract object which, at least arguably, has a beginning in time but never an end, as in Karl Popper’s “third world” of cultural objects. Revising the formalization to represent this view is left to the reader as an exercise. It is a little harder than you might think.",
       "article_title":"Axiomatizing FRBR: An Exercise in the Formal Ontology of Cultural Objects",
       "authors":[
          {
             "given":"Allen ",
             "family":"RENEAR",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library     and Information Science,     University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Yunseon ",
             "family":"CHOI",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library     and Information Science,     University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Jin Ha ",
             "family":"LEE",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library     and Information Science,     University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Sara ",
             "family":"SCHMIDT",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library     and Information Science,     University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"INTRODUCTION Based at Brown University, the Virtual Humanities Lab [1] is one of twenty-three “models of excellence” in humanities education, supported by the National Endowment for the Humanities for 2004-06. [2] The project is being developed by the Department of Italian Studies in collaboration with Brown’s Scholarly Technology Group and with scholars in the U.S. and in Europe. This paper will report on the achievements of VHL’s work during the first two years of its existence as a platform for collaborative humanities research. We will discuss the editing process as we envision it: as a form of interdisciplinary and collaborative knowledge work. We will present issues arising from our experiment with subjective (or “idiosyncratic”) text encoding; challenges we face in organizing the work of an international group of collaborators and the procedures for that work; and the process of annotating and indexing large texts collaboratively. Finally, we will hint at VHL’s potential applications for pedagogical purposes. TEXT ENCODING We have made three Early Modern Italian texts available online: Giovanni Boccaccio’s Esposizioni sopra la Comedia di Dante; portions of Giovanni Villani’s Cronica Fiorentina; and Conclusiones Nongentae Disputandae by Giovanni Pico della Mirandola. These three texts were selected as representative of different textual typologies (commentary, chronicle and treatise) that solicit different encoding and annotating strategies. The first two are large (around 700 and 200 modern print pages respectively) and heavily semantically encoded. The third is organized as a textual database meant to provide a flexible platform for annotation. All texts share the technical infrastructure of the VHL. The encoding was performed along interdisciplinary lines by scholars of Italian literature and history. The Cronica was encoded by two collaborators; the Esposizioni had one principal encoder and several researchers investigating specific issues. All three encoders were asked to annotate without a DTD, using whatever elements they deemed appropriate based on two criteria: – that the categories elucidated by the encoding are broad enough to produce interesting search results; and – that, in their estimation, researchers interested in these texts would generally find their encoded aspects interesting as well. All three encoders received training; further guidance was available upon request. None of the three scholars had had previous semantic encoding experience. Although the encoding proceeded separately for each text, similarities in what seems most interesting have emerged. Both texts contain encoding of proper names (including people, places, literary works mentioned) and the themes most prevalent in the narratives. These similarities, and the exigencies of the Philologic [3] search engine being built, have prompted us to homogenize encoding across texts, and make it TEI-compliant to the extent possible. It is important to note that this step was taken after the encoding was completed. This afforded our encoders freedom of analytical thought without burdening them with an unfamiliar and very complex set of encoding guidelines. The encoding process itself presented a challenge on several fronts. As often happens, it took the encoders a while to get used to doing work almost entirely at the computer. Because of the collaborative nature of the project, and because it is good practice in general, we used a versioning system, and the encoders had to deal with the necessity of having an internet connection at least at the beginning and end of each working interval. The two editor-encoders of the Villani text faced particular challenges, working as they were at different institutions, neither of which is Brown. So in addition to juggling the unintuitive (to them) practice of encoding with their other commitments, they faced the need to coordinate their schedules and responsibilities within the editing process. Combined with sometimes unreliable internet access, these circumstances channeled most communication into email and our work weblog. Blogging, particularly posting incomplete reflections on a work in progress, was initially an obstacle. However, this work has brought two remarkable benefits. First, we have received feedback from people not directly involved in the project. Second, each participant was constantly updated on others’ progress. This gave all involved an idea of where VHL as a whole was going, and encouraged discussion at the grant-project level. ANNOTATING Built by the Scholarly Technology Group, the annotation engine allows scholars with sufficient access privileges to annotate texts. [Figure 1] Annotations can be visualized, anchored to one or more passages of one or more texts. [Figure 2] A contributor’s own annotations can always be modified or deleted by that contributor. At the moment, a feature of the annotation interface is in development that will allow (again, registered) scholars to reply to annotations made by others. We hope that this will foster collaborative thinking and maintain an informal, workshop-like environment for research. An international group of around thirtyforty-five scholars has agreed to begin annotation of the Pico text. Having completed first-pass encoding of the other two works, we are assembling similar groups for them as well. Invited annotators serve as alpha testers of the search and annotation engines. Depending on the results, we plan to open up the process to the scholarly web community at large. One issue we face is whether to leave annotation open-ended (according to individual scholars’ interests and will) or to provide stricter guidelines – a working plan to be followed by all annotators. For now we have opted for an open process: participants will be free to annotate the portions of the text that they prefer. The VHL discussion forum provides a venue were issues arising from the annotating process may be critically addressed. INDEXING We have generated indexes of the encoded texts. Merely compiling them took weeks – automatically generated lists revealed encoding mistakes to be corrected, and highlighted many entries to be researched further. We are not yet confident in the indexes’ accuracy, but have neither time nor resources to properly address the issue by ourselves. Here, again, the feedback of the scholarly community will be essential. We see this as an opportunity to test out the already mentioned discussion forum that completes the VHL toolkit, and to gather potentially interested users for alpha-testing and feedback on the collaboration process itself. A call for participation was disseminated on relevant mailing lists, and mailed directly to relevant academic departments at many North American and European universities. We aim to gather a group of qualified [post]graduate and undergraduate students to help us verify the sizable indexes. At the time of this writing (March 2006), several young scholars have expressed interest in contributing. CONCLUSIONS AND A LOOK TO THE FUTURE As a final step during the present grant period, we are organizing the existing toolset (search and annotation engines, indexes, weblog and discussion forum) under an umbrella category of the Virtual Seminar Room, which will serve as the venue where editing practices will be consistently linked to pedagogical activities. This move is prompted in part by the success of the Decameron Web’s Pedagogy section, which continues to receive positive feedback from teachers of Boccaccio all over the world. It is too early to state definitively how the VHL will be used by humanists. Based on our prior experience with the Decameron Web [4] and the Pico Project [5], however, we are cautiously optimistic. It is true that work performed entirely online, and collaboration as a mode of research, have been slow to catch on in the humanities. Recent publications and tool developments point to a desire on the part of humanist academics to have spaces akin to science labs, where they can mingle and talk informally about their research. [6] Such labs are difficult and impractical to set up in physical space. So we have created a place online where scholars may interactively edit and annotate texts, and develop pedagogical modules for their individual purpose. With user feedback, we hope to make the VHL attractive enough to humanities scholars that they’ll be convinced to come play with us, even if the modes of interaction may be unusual or confusing at first. The past two years have resulted in a long wishlist of features to implement in the future, given time and resources: – addition of automatic lemmatizers and other pedagogical parsing and mapping tools, aimed at the various textual typologies of VHL content; – hosting and inclusion of texts uploaded by users; – possibility of using the editing process as part of a seminar-like pedagogical experience; – possibility of adding images as a consistent part of the editing/illustrating process; – tools for transcription of manuscripts and incunabula; and others. The future of the humanities is shaping up to be both online and collaborative. The question is not whether humanists will work together, but where they will do so, and what forms their knowledge work will take in the public research arena provided by the web. The VHL is one practical step towards answering this question.",
       "article_title":"Collaborative Scholarship: Rethinking Text Editing on the Digital Platform",
       "authors":[
          {
             "given":"Massimo ",
             "family":"RIVA",
             "affiliation":[
                {
                   "original_name":"Brown University Italian Studies",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Vika ",
             "family":"ZAFRIN",
             "affiliation":[
                {
                   "original_name":"Brown University Italian Studies",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"INTRODUCTION. BACKGROUND AND INITIAL GOALS The main purpose of this article is to introduce and describe a new digital resource, the Tesauro Terminológico-Conceptual (TTC), designed to perform research online, and the Atenea Corpus. The structure of the TTC thesaurus is conceptually innovative, and it has been specifically build to assist the user in the epistemological, terminological, textual and theoretical aspects of art research. The TTC is a development of the research done by Nuria Rodriguez for her doctoral dissertation [1]. The goal was to find a satisfactory answer to the terminological/conceptual problems that hinder research in the history, theory and criticism of art, and the TTC is the solution proposed(1). This project is closely related to the Getty Research Institute (Los Angeles, USA), and particularly to its Vocabulary Program, and the Art & Architecture Thesaurus (AAT).(2) The TTC is also linked to the Spanish version of the AAT, translated and managed by the “Centro de Documentación de Bienes Patrimoniales” based in Chile.(3) The TTC can be broadly defined as a knowledge tool for art specialists and other users, that helps to obtain a compilation of described, analyzed, classified and linked terms and concepts, primarily those having to do with theory, criticism and aesthetics. In order to build this type of tool it is essential to perform an interpretative analysis of these concepts and terms due to the ambiguity that define them. After that, these terms and concepts must be incorporated into the structure of the TTC with all the relevant information associated to them. An important outcome of this process is the development of a digitalized set of artistic texts: the Atenea Corpus, since one of the epistemological and methodological basis of this project is the study of the terms in their textual contexts. Therefore, this project creates a virtual net made up of two complementary components: a text database, and the terminological conceptual thesaurus (TTC). The textual database consists of all the digitalized texts that supply the terms and concepts recorded by the thesaurus [see figure 1]. INITIAL GOALS Why to create a new type of thesaurus? The initial context in which the project began substantially determined the conception and the characteristics of what would become the TTC. For further information on this type of thesaurus see also [2, 3, 4 and 5]. 1. Terminological/conceptual problems: ambiguity and interpretative inaccuracy. The main purpose of this project was to provide a satisfactory answer to the terminological/conceptual problems that encumber the task of conducting research in the history of art. Some of the problems most frequently found in the field of art history are terminological and discursive ambiguity, vagueness, and inaccuracy, particularly when dealing with theory and criticism. This situation derives from the high degree of semantic density typical of artistic terminology, as well as from frequent semantic alterations and shifts in meaning that terms undergo as a result of multiple reinterpretations and new uses. Thus, studying the semantic properties of terms and their diverse domains of meaning became a priority from the beginning. This approach required identifying and organizing the various meanings of each term and the semantic scope of every concept by comparing the variations found in the different discourses. None of the available thesauri was capable of performing these tasks: 1. Conventional thesauri do not take into account the theoretical and discursive contexts in which concepts appear; 2. The brief definition (or scope note) attached to each descriptor in conventional thesauri barely satisfies the needs of specialists in theory or criticism. The scope note has a very specific function, which is to limit the meaning of the descriptor to the framework of the thesaurus itself. As a result, theoretical and critical analyses are irrelevant in defining and describing concepts. Lacking these sorts of analyses, the complexity inherent to artistic concepts, with all their nuances and variations, ends up either simplified or ignored. 2. Linguistic and expressive richness. Besides categorizing the semantic component of the terminology and solving its ambiguities, a further objective was to compile and analyze the rich variety of verbal and expressive resources that characterizes the field of art history. In order to reach this goal it was necessary to collect, describe and classify all the expressive resources used by specialists in their writings about art, whether the lexical expressions were specific, or not (such as metaphoric or metonymic terms, literary or rhetorical resources). The criteria applied to the compilation of expressive resources went beyond the standards of conventional thesauri, for they only register terms identified as belonging to a particular field of study. 3. Textual and discursive approach. The essential notion underlining the project is that natural languages have to be considered in the context of their use, as they appear in a particular text; with respect to terms, they are regarded as functional entities with a contextually determined role and meaning. Consequently, the identification and analysis of terms and concepts had to be done at the discursive level, paying especial attention to the way in which authors and specialists used and defined them. Since these terminological and conceptual uses needed to be examined within their textual context, the thesaurus had to include the relevant discursive fragments. Again, conventional thesauri could not offer this type of information because, even though they provide the bibliographical sources for the terms, they do not include the textual citations themselves.In view of the shortcomings of conventional thesauri, it became clear that only the development of a new thesaurus prototype would achieve the desired results.Fig. 2: structure and parts of the TTC. It follows that the TTC simultaneously develops in three dimensions: • Conceptual and Epistemological Dimension: comprises a collection of artistic concepts described, classified into a conceptual structure and linked among them. • Linguistic and Terminological Dimension: consists of every term and verbal expression that acquires artistic meaning in the discourses being analyzed; all of them are described, classified according to terminological typologies, and assigned to the concepts they denote. • Textual and Discursive Dimension: contains the texts that are the sources for the concepts and terms registered in the TTC. Thus, the structure of the TTC, as we will explain with detail in the presentation, can be simply described as the way in which these dimensions are organized [see figure 2]. THE TEXTUAL DATABASE The database comprises the set of Spanish artistic texts used to extract all the concepts and terms registered in the TTC. This is why the textual database and the TTC can be described as two complementary tools that are virtually linked. Once there is a suitable computer application to implement the project, it will be possible to go from the textual fragments found in the TTC to the full textual sources; likewise, it will be possible to access the TTC from the textual database to obtain detailed information about the terms and concepts that appear in the texts. In addition to its complementary role, the textual corpus has also been designed to function as an independent tool that can be used by any specialist or researcher interested in aesthetics, theory, or art criticism. Indeed, a key goal of this project is to turn the textual corpus ATENEA into an exhaustive repository of pictorial and artistic texts that could serve as a basic reference tool for theoretical and critical studies, and, more generally, for research dealing with texts, sources and documents. This textual corpus represents an important contribution to the study of Spanish art history and theory since there are no other systematically digitalized texts in this field. The encoding of the texts was done at the Miguel Hernández University(4), following the TEI standards(5). The markup was performed using the Spanish set of TEI tags, allowing for automatic conversion to standard English-based TEI tags when necessary [4]. We considered that marking Spanish texts with Spanish mnemonics is more coherent than using English based marks, and improve their understanding and usability by Spanish speaking scholars [5]. For Web management and operation we used the PhiloLogic System.(6) This system was developed by the ARTFL project in collaboration with the University of Chicago Library’s Electronic Text Services(7) for the purpose of running texts compiled on digital corpuses. The textual database offers a diversity of search options that are redundant in terms of their versatility and polyvalence [see figure 3]. Depending on their interests, users will be able to: • Access a digitized copy of the complete text or texts of the author or authors being examined. • Find a listing of all the data related to a specific term, such as how it is used in a particular treatise, the frequency with which it appear, the various meanings it assumes, or even compare different treatises. • Get information about the sources or documents themselves. To this effect the textual corpus is being implemented with an additional database containing all the information related to the digitalized texts in specially structured and codified fields.Fig. 3: uses and possibilities of ATENEA database. USES AND POSSIBILITIES OF THE TTC The TTC has been conceived and developed as a research tool whose main emphasis is the terminological, conceptual and theoretical/critical analysis of artistic texts. However, the potential uses of the TTC are not limited to textual analyses for it has the capability of becoming a polyvalent tool applicable to a variety fields to the study of art in the Web. In general terms, there are four main potential applications that deserve to be emphasized: 1. Terminological and conceptual research 2. Tasks related to documentation and indexing of visual resources and textual material 3. In connection with this last point, retrieval of artistic information mounted on the Web. 4. Finally, the structure of TTC can also work as an abstract and standardized model that could be used by other specialists or other projects. These “other” TTCs could be linked as microthesauri.",
       "article_title":"Use of Computing Tools for Organizing and Accessing Theory and Art Criticism Information: the TTC - ATENEA Project",
       "authors":[
          {
             "given":"Nuria ",
             "family":"RODRÍGUEZ ORTEGA",
             "affiliation":[
                {
                   "original_name":"U. Málaga (Spain)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Alejandro ",
             "family":"BÍA",
             "affiliation":[
                {
                   "original_name":"U. Miguel Hernández",
                   "normalized_name":"Miguel Hernandez University",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/01azzms13",
                      "GRID":"grid.26811.3c"
                   }
                }
             ]
          },
          {
             "given":"Juan ",
             "family":"MALONDA",
             "affiliation":[
                {
                   "original_name":"U. Miguel Hernández",
                   "normalized_name":"Miguel Hernandez University",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/01azzms13",
                      "GRID":"grid.26811.3c"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In 1996, the Visible Language journal published, according to Eduardo Kac, “the first international anthology to document a radically new poetry, one that is impoosible to present directly in books and that challenges even the innovations of recent and contemporary experimental poetics” (98). The enthusiasm for the “radical newness” of new media poetry with which Kac introduces the anthology has been realized in part over the last ten years by a veritable explosion of new media poetry available online and in CD/DVD-ROM format. The interest of new media poets in semantic experimentation as well as with investigations of materiality have lead most critics to locate the roots of this genre in the tradition of the many avant-garde movements and experimental poetries of the twentieth-century, including surrealism, cubism, Dadism, forms of visual poetry, the L=A=N=G=U=A=G=E movement and futurism, just to new a few. Not surprisingly, this critical approach has tended to favor artists and those works, like Kac’s “Insect.Desperto” and Melo e Castro’s “The Cryptic Eye,” that most noticeably demonstrate certain tendancies of the avant-garde such as multi or non-linearity, non-narrative or non-grammatical strings of words, and the priveleging, to use Roberto Simanowski’s term of the “optic gesture” of the word over its “semantic meaning” (7). By contrast, works that fall under the purview of new media poetry by nature of their digital rendering and their use of multi-media software, but that, nonetheless, bear resemblances to more traditional twentieth-century poetries are often dismissed from consideration. This presentation will begin by exploring this critical resistance to digital work that bears resemblance to traditional print-based poetry; a resistance, I will argue, that belies the great number of new media poems that incorporate formal techniques and conventions characteristic of non-experimental poetry--in particular, techniques and conventions typical of the free verse lyric style that characterized much of the poetry of the twentieth century. By glancing at any of the online databases for new media poetry, we can see poets utilizing conventions such as iambic meter, stanzaic organization, alliteration, white space, and enjambment, among others, to organize and present the poetic line to the reader. For example, in Farah Marklevit’s “How They Sleep” the author uses a combination of end-stopped and enjamed lines to juxtapose what can be said to be the internal movement of the verse with the scrolling motion of the text on screen. The stop/start motion of the poem underscores the story being told--a romantic struggle between a husband and wife whose heads are “tuned to different pitches / like glasses of water.” As we can see in this example, the use of the rolling text serves not as an alternative to enjambment and punctuation, but rather as a vehicle by which these formal techniques are more fully realized by their incorporation into the multimedia presentation. The above is just one example of how attention to traditional formal poetic conventions can enhance our understanding of the aesthetic strategies of new media work; there are many others. It will be the contention of this presentation that, contrary to what most of the existing scholarship on new media poetry would suggest, the influence of traditional poetic forms and techniques is pervasive in the genre of electronic literature. Rather than rendering these formal strategies obsolete, moreover, the electronic environment offers poets the opportunity to render these strategies in ways that cannot be replicated on the printed page. In addition, I will attempt to show how situating new media poetry solely within the heritage of experimental poetry movements can not fully account for the aesthetic qualities of a great number of works. Rather than attempt an inclusive survey of these works, my presentation will focus specifically on the web-based work of Thomas Swiss, a poet who began, and continues, to write in print format and has recently forayed into the genre of new media poetry. I will focus on those poems that both explore the ramifications of new writing technologies as well as lay bare their debt to more traditional print-based poetry. It will be my argument that it is precisely by utilizing formal conventions typical of the free verse lyric and by preserving the visual style of the printed poem that Swiss interrogates the ways in which our pre-established patterns of reading are affected by the migration of the poem from page to screen.",
       "article_title":"The Ghost of the Printed Page: New Media Poetry and Its Ancestry",
       "authors":[
          {
             "given":"Jennifer ",
             "family":"ROWE",
             "affiliation":[
                {
                   "original_name":"University Of Maryland,     Department of English",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"INTRODUCTION Black’s Law Dictionary (648) defines forensic linguistics as:A technique concerned with in-depth evaluation of linguistic characteristics of text, including grammar, syntax, spelling, vocabulary and phraseology, which is accomplished through a comparison of textual material of known and unknown authorship, in an attempt to disclose idiosyncracies peculiar to authorship to determine whether the authors could be identical. At the 2002 ALLC/ACH Conference in Tuebingen, Laszlo Hunyadi et al. discussed some of the contributions that humanities computing makes to forensic linguistics. In this paper I point out the many contributions that forensic linguistics has and is making to the larger field of non-traditional authorship attribution -- contributions that are unknown or largely ignored by most non-forensic practitioners of non- traditional authorship attribution (this statement is based on the lack of references to the wealth of studies in forensic linguistics -- a quick glance at the bibliography will show some exceptions): 1) Immediacy 2) Techniques 3) Scientific Validity 4) Gatekeeping 5) Levels of Proof 6) Rules of evidence 7) Organization The paper goes on to propose closer formal ties between the ALLC/ACH and the IAFL. In each of the following sections, there is an emphasis on how the forensic techniques should be employed by the non-forensic practitioner and on the contributions of humanities computing to the field. IMMEDIACY This section discusses how immediacy forces a more careful, more restrictive methodology on the forensic practitioner (versus the non-forensic): Forensic linguistics is a sub-set of authorship attribution that is much more immediate and in many ways demands a “correct” attribution. Forensic linguistics often deals in criminal guilt or innocence -- with serious ramifications -- even life or death! TECHNIQUES This section discusses: 1) The necessity of employing corpus linguistic techniques. a) Looking at style markers as deviations from the norm. b) Looking at grammatical, stylistic, spelling, punctuation, and orthographical errors as style markers. c) The limiting of one control group to “suspects.” 2) The need for non-forensic practitioners to acquire the skills necessary to navigate bibliographic research in forensics (primary and secondary resources in case law -- and the many commentaries). SCIENTIFIC VALIDITY This section discusses: 1) The concept that all scientific methods should be brought to bear on an authorship problem -- e.g. handwriting analysis, paper analysis, type font analysis. 2) The strict definition of expert witness -- and the role they are allowed to play -- e.g. guides to help the jury interpret the facts. 3) How methodology must be “generally accepted by the community of scholars” to be allowed. GATEKEEPING This section discusses: 1) The role of the courts (of various countries) as gatekeepers. 2) The role of the International Association of Forensic Linguists IIAFL as gatekeeper and certifier of gatekeepers. This does not mean that there are not flaws in the system -- e.g. allowing the Morton CUSUM method as a valid technique even after its debunking on live TV. LEVELS OF PROOF This section discusses: How the “answer” is presented - while non-forensic practitioners for the most part present “probabilities”, forensic linguistics presents a “preponderance of evidence” concept and one of being “beyond a reasonable doubt.” RULES OF EVIDENCE This section discusses: The intricacies of the rules of evidence and how these rules can give direction to non-forensic attribution studies. Rules of evidence are not universal -- different countries have different rules -- in the United States, different states have different les. A set of rules distilled from all those available is advocated or the non-forensic practitioner. a) Dauber b) Post-Dauber ORGANIZATION This section discusses: 1) The IAFL, a “professional” organization with the requirement that its full members show evidence of “linguistic qualifications.” a) The IAFL’s journal -- Speech, Language and the Law (Formerly - Forensic Linguistics) Having a paper published here -- the implicit nihil obstat of the IAFL -- gives added weight to the credentials of the practitioner. Among other important journals are, Expert Evidence, Forensic Science International, and Journal of Forensic Document Examination. b) The IAFL’s conference The IAFL holds a biennial conference. The last one was in July, 2005 at Cardiff University, UK. Seven of the presented papers are of interest (and importance) to non-forensic practitioners -- e.g. Sanchez et al.’s “Intra and Inter-author Comparisons: The Case of Function Words: Are Function Words Really Functional in Stylometric Studies of Authorship Attribution.” c) There are some members (formal and contributing) of ALLC and ACH that are also members of the IAFL but their work published in non-forensic journals is quite different. CONCLUSION Forensic linguistis is not a “perfect” discipline. One unfortunate aspect of forensic linguistics is the adversarial role in presenting evidence -- many forensic linguistic presentations, while not necessarily fraudulent or even unethical are not in the best interests of practitioners who want to present the “whole truth.” Another unfortunate side-effect of the judicial system on the complete reporting of authorship studies is the all too common practice of “sealing” court records when a settlement is reached outside of the courtroom. I have seen some of these sealed records and only hope that the techniques will be duplicated and published elsewhere.I do not want to give the impression that non-forensic attribution is a poor cousin with nothing to offer -- the many disciplines that form the bulk of the field (e.g. computer science, stylistics, statistics) are the “core” -- but this is for another time.",
       "article_title":"Non-traditional Authorship Attribution : The Contribution of Forensic Linguistics",
       "authors":[
          {
             "given":"Joseph ",
             "family":"RUDMAN",
             "affiliation":[
                {
                   "original_name":"Department of English,     Carnegie Mellon University, USA.",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"This paper suggests a new approach to the study of potential interface tools, by examining not the tools themselves, but instead a set of factors that contribute to the possible benefit that might be provided by the tools. Since the proposed “affordance strength model” does not require a working version of the tool to study, it can therefore be applied at several points, beginning even very early in the research cycle, at the initial concept stage. Many standard usability instruments, such as GLOBAL, include questions that cover different aspects of the user’s perception of the tools, but require a working prototype. Other usability protocols exist for studying systems at an early design stage, such as TAM (Morris and Dillon 1997). However, these do not include a complete range of the factors in the proposed “affordance strength model.” This proposed model can also be used at later stages, both at the point where prototypes have been created, and later still, once working versions are in production. Researchers can also begin to compare the affordance strength of different kinds of software tools. An affordance is an opportunity for action (Gibson 1979). For computer interface designers attempting to create new software tools—that may in some cases offer new opportunities for action—a perennial problem exists concerning how best to study an affordance that was not previously available. Comparisons against previous interfaces with different affordances tend toward category error (comparing apples to oranges), and comparisons against interfaces with similar affordances but different designs tend to be studies of design rather than of opportunities for action. Given the need to specify the significant relational factors that characterize the strength of an affordance, it is possible to distinguish eight factors that together represent the relational aspects of the object, the perceiver, and the dynamics of the context. These factors together can be used to create a vector space that defines the relational aspects of affordance strength in an operational way. Although these eight factors are not the only possible candidate factors, it is possible to explain how they work together to create a relatively strong picture of the total affordance strength: affordance strength = (tacit capacity, situated potential, awareness, motivation, ability, preference, contextual support, agential support). Tacit Capacity The first necessary factor is the tacit capacity of the object to provide the affordance in situations of the kind being studied. For example, if a given adult wishes to keep dry while walking two blocks in the rain, the unfactored affordance of the object is the twin capacity to be carried while walking and, simultaneously, keep someone dry. In this case, the tacit capacity of the umbrella in situations where a person needs to walk two blocks in the rain while staying dry would be very high, while the tacit capacity of, for example, a wrench, would be zero. The wrench has an excellent tacit capacity for other types of actions. In fact, because it is a specialized tool (like the umbrella), it has a primary affordance. But for the work at hand it is useless. Situated Potential The second necessary relational factor is the situated potential of the object, not generally in circumstances of the kind under investigation, but in one particular situation at one particular time. It is all very well for the person about to walk in the rain to realize that an umbrella has an excellent tacit capacity for keeping a person dry, when at the point of setting out there is no umbrella available, or the umbrella that is available is torn. These two factors – tacit capacity and situated potential – are relational attributes where the attention of the researcher is directed toward the object or environment and its relevant affordances for action. There are other factors that treat the relational aspects of the agent, where the researcher’s attention is directed at what have been called the perceiver’s effectivities (Turvey and Shaw 1979). The first of these factors is awareness. For the person about to walk in the rain, a perfectly good umbrella might be sitting to hand, but if the person is distracted or confused or in a rush, the umbrella might not be perceived, and for all of its high tacit capacity and situated potential, the umbrella still stays dry while the person gets wet. Motivation The second factor is motivation. If the person in question wants to walk in the rain and would prefer not to get wet but does not really mind it all that much, that person’s tendency to seek and adopt an available affordance is significantly reduced in comparison with the person who hates getting wet, has just had a cold, and is wearing clothes that will be damaged by the rain. The former person may casually take up an available umbrella if one were available, since the tacit capacity and situated potential are high enough that the action has an appropriately low resource load. If only a newspaper is available, the lower tacit capacity might be such that the person would prefer to simply get rained on. For the latter person, it is likely that the high motivation and absence of an umbrella would lead to extremes of behavior such as deciding not to walk but take a taxi instead, or perhaps going back into the building to see if an umbrella could be found somewhere. Like many of the other factors, motivation is a composite of a wide range of sub-factors, however, it is not unreasonable to ask someone with respect to a given scenario: “how motivated would you say you would be to carry out such and such an action, on a scale of zero to five?” Ability The third relational factor that is associated with the perceiver is ability. For a person with a physical disability that makes grasping difficult or lifting the arm problematic, the option of carrying anything above the head may simply not be available. In this case, all the other factors may be present, including an umbrella with high tacit capacity and an excellent situated potential, a strong awareness of the umbrella on the part of the perceiver and a correspondingly strong motivation to use it. But inability to grasp the handle renders the affordance zero for this particular person at this particular time. Preference The last factor related to the perceiver represents the role played by individual preference. All other factors being equal or even roughly equal, it is often the case that individual adoption of affordances depends at least to some extent on established preferences. In the case of the person who wants to stay dry in the rain, if there are two umbrellas available and one is a favorite, that will probably be the one that gets employed. Preference can be based on any one of a dozen sub-factors, ranging from aesthetic considerations to interpersonal influence to previous personal experience. Preference is distinct, however, from ability, and although preference is related to motivation, the two are not equivalent. The final factors in the proposed vector space are needed in order to adequately account for features of the situation that are relevant but are not directly related to the relationship between the perceiver and the object. They stand instead for the relationship between the affordance and its context. Contextual Support The first of these factors is contextual support, where factors in the environment that are not part of the affordance have an influence one way or the other on the perceiver’s interaction with the affordance. There are a wide range of possible contextual supports, including aspects of the situation that are physical, cognitive, and environmental, and the precise nature of the contextual supports in a given situation should be outlined during the process of analyzing the affordance as a whole. In the example of someone who wishes to stay dry in the rain, the contextual factors would include environmental facts such as how hard it is raining, whether it is warm or cold outside, how hard the wind is blowing and in what fashion, and so on. Agential Support The other feature that has not been accounted for yet in an explicit form is the role of other agents in the scenario. Contextual support includes all those factors (excluding the affordance itself) that are present in the environment at the time of the perceiver becoming involved with the affordance. Agential support, on the other hand, includes those features relating to the roles of the other people, animals, insects, and so on who are also potentially part of the situation. Agents are distinct from other factors of the environment in that they have agency, which is to say volition, goals, and actions of their own, which may have some bearing either directly or indirectly on the particular affordance. For instance, for the person who wishes to stay dry in the rain, it may turn out that there are other people present who also wish to walk outside. One of them might be elderly or frail and lacking an umbrella, in which case our perceiver could be motivated to behave altruistically and turn over the superior affordance of the umbrella to the other perceiver, choosing instead an inferior solution such as a folded newspaper. Applying the Model One straightforward means of applying this model to the study of interface tools is to have participants consider a particular affordance, in order to rate each factor on a Likert scale from 0-5. A rating of zero for any factor effectively zeroes out the strength of the entire affordance, suggesting that it might be further worthwhile to obtain a composite number by multiplication of the individual items. The number could then be used to compare the affordance strength of different kinds of tools. Combining this rating with comments for each factor would add a further layer of information that can contribute to the interface designer’s decision process.",
       "article_title":"Proposing an Affordance Strength Model to Study New Interface Tools",
       "authors":[
          {
             "given":"Stan ",
             "family":"RUECKER",
             "affiliation":[
                {
                   "original_name":"Humanities Computing in English and Film Studies, University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Over a decade has passed since Ward Cunningham created his Portland Pattern Repository and coined the term “wiki” (from the Hawaiian word for “quick”) to describe the application that enabled its pathbreaking web-based collaborative editing capabilities. Since that time, wikis have flourished on the Web. There are now well over 100 freely available wiki software applications, in a wide variety of programming languages. Wikipedia, the best-known wiki instance, features 2.3 million articles written in over 200 languages by almost 90,000 individual contributors. Despite this popular success, wikis have been slow to catch on in the academic world, and in the humanities in particular -- surprising given their enormous potential to transform the nature of scholarly communication. Certainly this is due in part to the inherently solitary and linear nature of most academic writing; blogs seem a more intuitive fit for this type of writing, as evidenced by their much greater popularity among scholars. Yet even in the hands of an individual researcher working alone, a wiki can be a valuable tool for organizing and archiving complex scholarly material. In a passage that evokes Vannevar Bush and his mythical memex, Leuf and Cunningham invite scholars to imagine abandoning their scattered scraps of paper and Post-It notes and instead «directly asking your scattered notes where references to ‘thingamy’ are and having the appropriate bits of paper levitate into view, slide out of bookshelves, and be there at your fingertips ... eventually everything starts to interconnect: notes, files, e-mail, contacts, comments, relational cross-links, Internet resources, and so on ... the whole thing evolves almost organically in response to your growing body of notes” (84). Of course, the real fun with wikis begins when a complex knowledge representation like the one described above is opened up to a large community of contributors. By virtue of its inherent design, the wiki breaks down the process of text creation into component functions (structuring, drafting, revising, annotating) typically performed by a solitary author, and allows these to be performed by different persons. Wikis are highly unique in this regard. With blogs and discussion boards, earlier postings cannot be altered except by an administrator and the chronological or topical ordering of the content cannot be restructured. With wikis, however, content is continuously reorganized and revised, often by many different hands. This dispersed, depersonalized method of content creation explains certain rhetorical features of the typical wiki article, which is usually impersonal, unsigned, POV-neutral prose written in the 3rd person. When individual comments and suggestions about a wiki article, often signed and highly personal, are debated, voted upon, and gradually incorporated into the article itself, we see a polyphony of opinion slowly coalesce into a kind of collective understanding. The ideal wiki article would be a highly polished nugget of scholarly consensus, carefully crafted and continuously updated by an informed community, embedded in a dense associative web of related content. Clearly, there are roadblocks on the way to this ideal. A wiki must achieve a certain critical mass of content before it is truly useful, and without a dedicated core of regular contributors at the outset it is apt to languish unused. “Document mode” -- the anonymous, impartial style of the typical wiki article -- may be familiar from encyclopedias and other reference texts, but it is largely alien to writing in the humanities. Because the precise expression of ideas in words is the very essence of their work, humanities scholars may be particularly reluctant to subject their writing to revision by strangers. Conversely, they are more sensitive to the “legitimation crisis” that arises when text is not attributed in an obvious way (Barton). These problems might exist even if the wiki was restricted to a closed and vetted group of scholarly peers. The final paper will explore these issues through a series of case studies, and will suggest some optimal uses for wiki technology in an academic setting.",
       "article_title":"Exploring the Wiki as a Mode of Collaborative Scholarship",
       "authors":[
          {
             "given":"Christine ",
             "family":"RUOTOLO",
             "affiliation":[
                {
                   "original_name":"University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Humanities scholarship has traditionally viewed a literary work of art as an act of production belonging to larger social and cultural networks, yet remaining relatively fixed within a single medium. The recent identification of patterns of convergence in technological, social/organic, economic and global contexts seems to suggest, however, that although these same traditional models of literary production still constitute a significant portion of the cultural output, they are being transformed and shifted in order to accommodate increasingly intersectional exchanges between media forms and content. In many cases, these shifts have made it possible to develop new structures that shatter the fixity of narrative as a single-medium endeavor and establish instead a multiply-mediated storyworld, a cross-sited narrative, defined here as multisensory “clustered” or “packeted” stories told across a divergent media set. The proliferation of cross-sited narratives—across film, literature, music, video games, live performance and the internet—presents significant challenges to the current modalities of humanistic theory and practice. As both a product of and a reaction to the process of the discrete nature of digitization, cross-sited narratives require us to not only “imagine an infinitely segmentable media market” (Coit-Murphy 91) but also, it seems, an infinitely segmentable and infinitely mediated story that, as a network, draws and exchanges narrative information from site to site. Evidence of this sort of networking can be seen in works such as Mark Danielewski’s House of Leaves, which operates across no less than 5 media channels (novel, novella, live performance, recorded music, the web), each integral to the establishment of the narrative storyworld. The question posed by this paper, then, is whether we can use current models of digital archiving and editioning as the means through which to preserve and distribute a narrative network such as Danielewski’s. Is it possible, in the context of contemporary textualities, to retain even a semblance of such a work? Although similar crisis points have always plagued the arts, exposing in many ways the utter ephemerality of even venerated and “durable” technologies such as codex book, we’ve looked towards the digital as the means through which the fragile materials of paper and print are hardened and made permanent from a bitstream that flows from an electronic fountain of youth. Paradoxically, studies of new media have recently (and perhaps belatedly) moved toward the preservation of digital objects such as early computer games and interactive fictions, recognizing rightly that an entire generation of artifacts is in danger of being obliterated by hardware and software advancements. Although it is universally acknowledged that there is obviously no possible way to replicate the historical moment during which a given work is produced, we have, for the most part, been content to instead refashion the work into whatever single medium seemed to have the greatest potential for preservation. Friedrich Kittler’s assertion that “the transposition of media is always a manipulation and must leave gaps” (1990:267) does hold some sway here but, in the sense that a given text (such as Emily Dickinson’s letters or William Blake’s illuminated manuscripts) has content that is somewhat extractable from its form, such gaps are acceptable when the tradeoffs are vastly improved distribution and conservation. It will be argued here that such an approach will inevitably fail when confronted with the preservation of a cross-sited narrative, as the transcoding of a narrative that relies on the tensions between multiple media sites into a single medium will irrevocably disrupt the network that constitutes its storyworld. We’ve found ways to overcome the displacement of a medium, but can we hope to approximate a narrative/media network? In short, we can’t. Drawing upon (cognitive) narratology (Herman 2004; Ryan 2003, 2004; Bortolussi and Dixon 2004), cognitive linguistics (Turner 2003; Fauconnier and Turner 2002), convergence theory (Jenkins 2004, 2006; Kittler 1990), archiving strategies for “network” fictions (Montfort and Wardrip-Fruin 2004; Liu, Durand, et al. 2005) and Pierre Levy’s theories of virtualization (1998), the focus of this paper will be as follows: 1) to outline the structures of cross-sited narratives, focusing particularly on their network structure, 2) to assess the current methods of archiving/ preserving literature, such as those proposed by the Electronic Literature Organization, and especially methods that deal with transient texts and networked stories (such as hypertext and interactive fiction) 3) to propose that such methods are inadequate for transcribing the complex interactions between media that occurs within cross-siting and 4) to suggest a new model of temporal textuality that argues that these networks cannot be transposed except through primary materials, and that, often, this primacy is fleeting and not reproducible. In fact, it is possible that the only remnant of these textual networks that will remain are in the annotations and collaborations left by users on message boards, blogs and chat rooms. The structures that will be recommended for cross- siting are presented as a series of gradient models representing the continuum between a text’s materiality and it’s narrative. The works that will be studied include Danielewski’s House of Leaves, The Matrix, Neil Young’s Greendale and the alternate reality game, I Love Bees, a selection that provides a range of blindspots in current practices of textual preservation. Through text, gaming, comic books, live phone calls, the texture of paper and the spontaneity of live performance, each of these works exposes the single-medium logic through which most textual preservation operates. If we are truly entering an era of convergence where media come together in conversation over narrative, then we must also be aware that this coming together is not without consequence. Indeed, the end product of this convergence just might be the erasure of many of the networks it produced.",
       "article_title":"Many Houses, Many Leaves: Cross-Sited Media Productions and the Problems of Convergent Narrative Networks",
       "authors":[
          {
             "given":"Marc ",
             "family":"RUPPEL",
             "affiliation":[
                {
                   "original_name":"Textual and Digital Studies,     University of Maryland College",
                   "normalized_name":"University of Maryland, College Park",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047s2c258",
                      "GRID":"grid.164295.d"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In a presentation at a previous ALLC/ACH conference, later developed in a paper for Literary and Linguistic Computing (Rybicki 2005), one of the authors has begun to investigate the relationship between a literary original and its various translations, basing on Burrows’s well-established method first used in his study of Jane Austen (1987) and later developed, evaluated and applied by a number of scholars, including Hoover (2002). Although the results obtained with Delta (Burrows 2002) seem equally promising for computer-assisted translation studies, the first author of this paper (himself a translator of British, American, and Canadian literature) feels that the potential of the older method has not been exhausted in this particular domain – and that it is especially well-suited for case studies such as the one presented here. The above-mentioned first study investigated character idiolects in a classic Polish trilogy of 19th - century historical romances and its two English translations (made a century apart) and found that many relationships (“distances”) between characters in the original were preserved in both, or at least one, of the translations. This time, the works chosen were two “most literary spy novels” by John le Carré, A Perfect Spy (1986) and Absolute Friends (2003). Written 17 years apart, they were translated by Rybicki into Polish less ten months one from the other in 2003 and 2004. From the very start, it was evident for the translator that the two novels will be an interesting subject of study due to their being built according to a very similar model, especially where characterization is concerned. Both feature a slightly foolish British agent (le Carré’s famous trademark), his highly intellectual yet physically handicapped East German nemesis, the British agent’s boss/friend, etc. Since these two very similar works shared their Polish translator – who continued to experience a very strong feeling of déjà vu while working on the two novels, this case seemed perfect for a study of stylistic relationships between original and translation. The following observations have been made: (1) In the narrative, the styles of the originals are more unique than those of the translation. This may be a consequence of the 17 years’ distance between the English versions as opposed to the 10 months that separate the translations. (2) Of the three above-mentioned couples of corresponding characters, two are very expectedly similar, while one (the two East-German double agents) are not. Their similarity is “regained” in the translation – an interesting corroboration of the translator’s “intuitive” suspicion during his work on the Polish version. These results show that, at least in this – very special – case, the accuracy of studies performed by Multidimensional Scaling of correlation matrices of relative frequencies of the most frequent words is quite considerable when applied to translation. This is true despite the disquieting fact, to quote Hoover’s statement given in a somewhat different context, that “like previous statistical authorship attribution techniques, (this correspondence) lacks any compelling theoretical justification” (2004). The tentative explanations proposed so far by Opas and Kujamaki (using the van-Leuven-Zwart postulate of microstructural changes influencing the text’s macrostructure, 1995) or McKenna, Burrows and Antonia (“common words influence syntactic structures and translations of them can influence the meanings we read in a text”, 1999), are certainly not enough. Even more telling is the silence that Burrows maintains on the subject in his most translation-oriented study (2002), already quoted above. Since overlapping semantic fields of the most frequent words of texts and divergent linguistic systems make one-on-one correspondences impossible, a more general underlying mechanism must be found. At the same time, empirical studies that have hinted at the existence of such a mechanism have still been very few. This is why more are needed to explain the compelling yet somewhat mystical successes of Burrows’s “old” method, and Delta and its various clones. The results presented in this paper are at least a good incentive to study this phenomenon in translations by the same translator; they are, in fact, part of a greater project to investigate the stylometry of all Polish versions by Rybicki, covering a wide range of modern English-language literature. ",
       "article_title":"Can I write like John le Carré?",
       "authors":[
          {
             "given":"Jan ",
             "family":"RYBICKI",
             "affiliation":[
                {
                   "original_name":"Institute of Modern Languages, Pedagogical University, Krakow, Poland",
                   "normalized_name":"Pedagogical University",
                   "country":"Mozambique",
                   "identifiers":{
                      "ror":"https://ror.org/0331kj160",
                      "GRID":"grid.442441.3"
                   }
                }
             ]
          },
          {
             "given":"Paweł ",
             "family":"STOKŁOSA",
             "affiliation":[
                {
                   "original_name":"Institute of Modern Languages, Pedagogical University, Krakow, Poland",
                   "normalized_name":"Pedagogical University",
                   "country":"Mozambique",
                   "identifiers":{
                      "ror":"https://ror.org/0331kj160",
                      "GRID":"grid.442441.3"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Background Since the beginning of Humanities Computing, text theory has been a fundamental issue. The question „What is text?“ has been repeatedly raised at ALLC/ACH conferences. One of the answers, “text is an ordered hierarchy of content objects” (the OHCO-model), has been dominating the discussion for a while and has laid ground for well-established and paradigmatic standards such as the TEI Guidelines. But there has also been criticism since the beginning which questioned this model from different perspectives (Buzzetti, McGann, Olsen, Caton, Huitfeld). Do we in fact have an unsolvable problem? As Jerome McGann puts it: “What is text? I am not so naïve as to imagine that question could ever be finally settled. Asking such a question is like asking ‘How long is the coast of England?’.” Starting Points. Questions are there to be answered. I’ll start with the authorities. First, I’ll introduce Isidore of Seville, the patron saint of the Internet, who described “oratio” as threefold. Second, I’ll take up Willard van Orman Quine’s famous ontological slogan “no entity ithout identity” and use it as a lever. Using as a guiding question “When are two things one text?”, I will develop a pluralistic theory of text which will integrate a wide range of traditional notions of text. This theory will give us the means to describe the identity conditions of text. The Wheel of Text. In a rather free reading, Isidor distinguishes the three components “sense”, “expression” and “sign”. I relate Isidor’s components to the notions of text as meaning and intention (text-i), text as speech and linguistic utterance (text-l), and text as object and document (text-d). There are other notions of text, which fill the space between those concepts. We talk about the text as structure or ‘work’ (text-w), placed between text-i and text-l. We also talk about the text as fixed written version (text-v) between text-l and text-d. And we talk about the semiotic text as a picture and as a complex sign (text-s) between text-d and text-i. This concludes the wheel of text and constitutes a comprehensive theoretical model of “What text really is”. Practical implications. But what is such a theory good for? It has at least two fields of application. First, it has applications in the field of history and in analytical fields. The pluralistic model gives us a tool to describe and locate historical text technologies in relation to certain notions of text. All text technologies promote certain concepts of text while hindering others. This can be shown for oral tradition, manuscript culture and print; for electronic technologies like “plain text”, “WYSIWYG” or “markup”; for concepts like hypertext or the OHCO-model; for academic trends like the linguistic turn, pictorial turn or material philology; and even for the description of certain current cultural developments. In a second application, the pluralistic theory of text has consequences for the assessment and development of future text-technologies. How can markup languages be understood in the light of a pluralistic text model? How are epistemological and ontological questions in markup theory (like the foundational distinction between text and markup) to be answered? How can markup be used and maybe even conceptually extended to truly cover and support all notions of text? How can we develop text technologies beyond the markup paradigm to cope with notions of text constantly hindered by the concept of markup and the OHCO-model? Conclusion. How long is the coast of England? The answer lies in a closer examination of the question: “What do you mean by ‘coast’?”, “How close will you look at it?” and “Which instruments will you measure it with?”. The same holds true for the text. The answer lies in the eye and in the mind of the reader. What is text? - “Text is what you look at. And how you look at it.”",
       "article_title":"What is text? A Pluralistic Approach.",
       "authors":[
          {
             "given":"Patrick ",
             "family":"SAHLE",
             "affiliation":[
                {
                   "original_name":"Universitätsbibliothek Göttingen",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"If there is one thing that can be said about the entire literary output of the world since the invention of writing it is that literary works exist in multiple versions. Such variation may be expressed either through the existence of several copies of a work, through alterations and errors usually in a single text, or by a combination of the two. A textual feature of this degree of importance ought to be at the forefront of efforts to digitise our written cultural heritage, especially at a time when printed media are becoming less important. Until now literature has been represented digitally through systems of markup such as XML, which are ultimately derived from formal languages developed by linguists in the 1950s (Chomsky 1957; Hopcroft and Ullman 1969); but over recent years it has gradually become clear that the hierarchical structure of such languages is unable to accurately represent variation in literary text. Alan Renear (1997), for example, admits that variation is one exception that does not fit into his hierarchical model of text; likewise Vetter and McDonald (2003) conclude that markup provides ‘no entirely satisfactory method’ for representing variation in the poetry of Emily Dickinson. More general discussions of the shortcomings of hierarchical markup, including the problem of variation, have recently been made by Dino Buzzetti (2002) and Edward Vanhoutte (2004). An alternative approach, not yet tried, is to use graphs to represent variation. Graphs were first studied in the 18th century by the Swiss mathematician Leonhard Euler, who is best remembered for his solution to the famous ‘Bridges of Königsberg’ problem (Trudeau 1993). The type of graph which most closely resembles textual variation does not appear to have yet been described by anyone; however, it can be derived from the following example. Consider four versions of the simple sentence: A The quick brown fox jumps over the lazy dog. B The quick white rabbit jumps over the lazy dog. C The quick brown ferret leaps over the lazy dog. D The white quick rabbit playfully jumps over the dog. Collapsing the five versions into collapsing the four versions. Such repetitions are clearly undesirable. If they were present in an electronic edition each time one copy was changed, an editor would have to check that the other copies were changed in exactly the same way. If all this redundancy is removed by collapsing the four versions wherever the text is the same, the following graph results: Figure 1 This is a type of ‘directed graph’, which we call a ‘textgraph’. Its key characteristics are: a. It has one start and one end point. b. The ‘edges’ or ‘arcs’ are labelled with a set of versions and with a fragment of text, which may be empty. c. There are no ‘directed cycles’ or loops. d. It is possible to follow a path from start to end for each version stored in the graph, which represents the text of that version. In figure 1 version D contains an insertion: ‘playfully’ and a deletion ‘lazy’. These are represented in the graph as empty edges. In fact insertions and deletions are the same thing viewed from different perspectives: every deletion is an insertion in reverse and vice versa. Transpositions, as in version ‘D’ - the transposition of ‘white’ and ‘quick’ in relation to version ‘B’, can be viewed as a deletion of some text in one place and its insertion in another. All that is then needed is some way to refer back to the original text to avoid copying, e.g. by ‘pointing’ to it. This feature has been shown in figure 1 by drawing the transposed text in grey, which does not change the structure of the graph. This model is equally applicable to variation arising from a single manuscript or from the amalgamation of multiple manuscripts of the same work. Its biggest advantage is that it can handle any amount of overlap without duplicating text. One example of a rigorous test of this model can be found in the archives of the ‘Digital Variants’ website. The poem ‘Campagna Romana’ by the modern Italian poet Valerio Magrelli exists in four drafts, the first of which is shown in figure 2. Figure 2 In original manuscripts like this it is often unclear how variants are to be combined. For example, in the line ‘Il suo arco sereno/certo/scandito/ ha la misura d’un sospiro/misura la sera/’ it is impossible to say if there ever was a version: ‘Il suo arco scandito misura la sera’. The sensible way to proceed here is simply to provide a mechanism for recording any possible set of readings, and to leave the interpretation up to the editor. Figure 3 Documents which are based on this graph structure we call ‘multi-version documents’ or mvd’s. One application of this format is the applet viewer shown in figure 3 (Schmidt, 2005). This currently allows the user to view one readable version or layer of text at a time. In reality only the differences between each layer are recorded, and the user can highlight these using red to indicate imminent deletions and blue for recent insertions. The text is also searchable through one version or all versions simultaneously. This visualisation tool is in an early stage of development and as yet it can only handle plain text. However, because it cleanly separates the content of the document (represented by the edges of the graph) from its variation (represented by the graph’s structure), the same method could also be used to record versioning information in almost any kind of document - including XML, graphical, mathematical and other formats: Figure 4 This allows a multi-version document to utilise existing technology. By removing variability from a text, and effectively representing it as a separate layer, the mvd format allows technologies like XML to be used for what they were designed to do: to represent non-overlapping content. One way this could be achieved would be to edit the text in an existing editor but to modify the editor slightly so that instead of reading and writing the document directly it would read and write only one version at a time to an mvd file, as shown in figure 4. There are a couple of possible objections to the overall technique described here. Firstly, because it is not based on markup, it is no longer practical for the user to see the contents of the document in its merged form. Secondly, existing XML technology currently utilises markup to record information about the status of individual variants. This data would have to be re-encoded as characteristics of the bits of varying text, since the document content would no longer carry any information about variation. However, the very idea of ‘variants’ embedded in the text is a structure inherited from the critical edition, which is now widely regarded as obsolescent (Ross 1996; Schreibman 2002). Through the printed medium traditional philology advanced the notion of textual ‘truth’ in its effort to restore a lost original, whereas our model is directed toward the fruition of the text as it really is. As we move forward into an age when digital text has the primary focus, some of the old ideas associated with paper-based methodologies may have to be revised or given up entirely (Fiormonte 2003). In conclusion, the use of ‘textgraphs’ to represent variation appears to overcome the problems of redundancy and overlap inherent in current technologies, and to reduce document complexity. Thus far, a file format has been devised and has been demonstrated in a working multi-version document viewer for plain text, which is capable of representing original documents of high variability. By separating variation from content it also has the potential to leverage existing document handling technologies. This technique represents a new method of handling textual variation; it is mathematical and wholly digital in character, and unlike what it purports to replace, it is not based on the inherited structures of the printed edition. ",
       "article_title":"A Fresh Computational Approach to Textual Variation",
       "authors":[
          {
             "given":"Desmond ",
             "family":"SCHMIDT",
             "affiliation":[
                {
                   "original_name":"School of Information Technology     and Electrical Engineering     University of Queensland",
                   "normalized_name":"Warsaw School of Information Technology",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/02wbgfm85",
                      "GRID":"grid.445568.e"
                   }
                }
             ]
          },
          {
             "given":"Domenico ",
             "family":"FIORMONTE",
             "affiliation":[
                {
                   "original_name":"Università Roma Tre",
                   "normalized_name":"Roma Tre University",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/05vf0dg29",
                      "GRID":"grid.8509.4"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"While many digital library initiatives and digital humanities centers still create collection-based projects, they are increasingly looking for ways of federating these collections, enhancing the possibilities of discovery across media and different themed-research. Facilitating access to these objects that are frequently derived from different media and formats, while belonging to different genres, and which have traditionally been described in very different ways, poses challenges that more coherently-themed collections may not. In the last few years it has become increasingly evident to those in the digital humanities and the digital library communities, and the agencies which fund their research, that providing federated searching for the immensely rich digital resources that have been created over the past decade is a high priority. Several recent research grants speak to this issue, such as the Mellon-funded NINES: A Network Funded Initiative for Nineteenth Century Electronic Scholarship , or The Sheet Music Consortium . While digital objects organized around a specific theme or genre typically provide opportunities for rich metadata creation, providing access to diverse collections that seem to have little in common (except that they are owned by the same institution) often poses problems in the compatibility of controlled vocabulary and metadata schema. While this problem has been noticed on much larger scales before and addressed by initiatives such as z39.50 and the Open Archives Initiative’s Protocol for Metadata Harvesting, addressing the problem within a library’s or center’s own digital collections is a vital part of making such initiatives successful by leveraging cross-collection discovery through the internal structure of the metadata scheme as well as a consistent approach to terminology. This presentation will explore the issues surrounding creating an archive of cross-searchable materials across a large spectrum of media, format, and genre at the University of Maryland Libraries . It will examine the way some of these interoperability problems can be addressed through metadata schema, targeted searching, and controlled vocabulary. Description of Research This paper will be based on the research done at the University of Maryland Libraries using two ongoing projects. The first project utilizes The Thomas MacGreevy Archive , a full-text digital repository (following the Text Encoding Initiative (TEI) Guidelines ), to explore the development of metadata and descriptors to facilitate searching across individual collections which are described at different levels of granularity. The second project involves using the knowledge based on the research carried out for the more cohesive MacGreevy Archive for the more diverse repository the UM Library is developing utilizing Fedora as its underlying repository architecture. The Thomas MacGreevy Archive is being explored as a microcosm from which to examine issues of searchability of content divided into collections that cannot be described using a single controlled vocabulary and has different modes of display. The necessity for cross-collection searches has arisen due to the Archive expanding its content from digitized versions of books and articles, to two collections of correspondence (one relatively small collection of seven letters, the other quite large, c 150 letters), and making images that are currently available only via hyperlink from within texts individually discoverable. Preliminary findings involving this research were shared at the joint 2005 ACH/ALLC Conference at University of Victoria. Another issue that the MacGreevy Archive can model is problems of controlled vocabularies across different collections. The current controlled vocabulary descriptors use a faceted approach to describe articles and books written on such topics as art, music, and literature. Since both the correspondence and images differ in form and function from the existing objects in the archive, the current controlled vocabulary descriptors are not granular enough to capture either the variety of themes common to letters, or the additional descriptors to describe what the images are of and about. The experience gained in exploring the more homogenous MacGreevy Archive is being applied to the much more diverse collections and formats being housed in the Fedora repository in which rich collection-specific controlled vocabulary across multiple formats is being developed at the same time as a vocabulary which provides users the opportunity of discovery across all collections. While specific controlled vocabularies exist that would adequately describe each collection, they are generally too specific for materials outside that collection. On the other hand, Library of Congress Subject Headings (LCSH), while sufficiently broad in scope, are unwieldy in form, taking a post-faceted approach by combining several smaller descriptors into a predefined string. These long strings cause multiple problems in searching (including not being amenable to Boolean searching) but are ubiquitous within university libraries, forming the underlying basis for the vast majority of online catalogues. LCSH descriptors will be necessary, however, for those objects that will occur concurrently in the library catalog. In exploring cross-collection search capabilities in this larger, more diverse environment, the use of controlled vocabulary for subject access is only one possible source of commonalities. Within the Fedora repository, the University of Maryland Libraries will create a metadata scheme that represents a hybrid of the elements and concepts chiefly found in qualified Dublin Core , and the Visual Resources Association Core < http://www.vraweb.org/vracore3.htm >. This scheme and hybrid approach was first used by the University of Virginia, and the UM Libraries is refining that element set and list of required elements specifically with cross-collection searching in mind. By requiring elements that include information such as the century and geographic focus of individual objects, designers aim to define and render searchable the broader topics that objects from disparate collections of narrow focus may have in common. Designers must also use or define standard vocabularies to be used to populate these broader elements to ensure successful cross-collection discovery. Previous Research The integrated design of online information retrieval systems has been studied most prominently by Marcia Bates (2001). However, most research in this field takes a more atomized approach, focusing solely on one aspect of design: metadata schemes for instance, or GUI design for search screens. Other research has examined the particular needs and searching habits of users, particularly in humanities disciplines, when faced with online search interfaces. Prominent among these has been the work of Deborah Shaw (1995) and the series of reports from the Getty Online Searching Project (Bates, 1994, 1996; Bates et.al. 1993, 1995; Siegfried et.al. 1993). The majority of this research was carried out in the late 1990s and follow-up has been more in specific application of digital library systems than in respect to user-oriented, integrated design, such as Broughton (2001). Another area of inquiry relevant to this research involves the use of faceted classification systems for web-based discovery. The most recent white paper produced by the NINES project neatly summarizes current research (NINES 2005). KM’s ‘The Knowledge Management Connection’ discusses faceted classification within the context of information-intensive business environments. Denton (2003) discusses how to develop a faceted classification scheme, while Bates (1988) surveys the various approaches to subjection description for web-based discovery. Conclusion This paper will build on previous research as mentioned above in the development of a controlled vocabulary, metadata schema, and faceted classification scheme which provides for both rich collection-specific discovery, as well as federated searching across collections.",
       "article_title":"Cross - Collection Searching : A Pandora’s Box or the Holy Grail?",
       "authors":[
          {
             "given":"Susan ",
             "family":"SCHREIBMAN",
             "affiliation":[
                {
                   "original_name":"Digital Collections &     Research, UM Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Gretchen ",
             "family":"GUEGUEN",
             "affiliation":[
                {
                   "original_name":"Digital Collections &     Research, UM Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jennifer ",
             "family":"O’BRIEN ROPER",
             "affiliation":[
                {
                   "original_name":"Original Cataloging, UM Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"This paper will describe and analyse the generic structure of linguistic articles, using a corpus-based methodology and working within a contrastive (English-French) perspective. The main question that we wish to answer is the following: “Do scientific articles – and more particularly linguistics ones – have a generic structure, and to what extent does this structure vary from one language to another?” We will answer this linguistic question using techniques from computational and corpus linguistics. The notion of genre is more and more present, as much in linguistics as in information retrieval or in didactics. Genres and texts are intimately connected, as genres could not be tackled within the restricted framework of the word or the sentence. Indeed, genres can only be perceptible using text corpora both generically homogeneous and representative of the genre studied. The progress of information technology and the possibilities of digitization have made it possible to gather homogeneous and synchronic corpora of written texts to analyse and characterize genres. Moreover, the development of computational linguistics, of linguistic statistics and more generally of corpus linguistics has led to that of tools and methods to process large corpora which make it possible nowadays to detect linguistic phenomena and regularities that could not have been traced before. In that sense, inductive typological methods and multi-dimensional statistical methods (see Biber, 1988) seem crucial to make the criteria which define the genres appear more clearly. If literary genres have been largely explored, the study of academic / scientific and professional genres has mainly been undertaken for about thirty years within a more applied trend. English for Specific Purpose, is a rhetorical-functional trend which is interested in macro-textual descriptions and in describing genres from a phrasal or propositional point of view. The description of rhetorical moves (see Swales, 1990) is rather qualitative than quantitative, as the moves can scarcely be automatically identified – although several studies have set out to demonstrate their relative identification by training classifiers on manually annotated corpora (Kando, 1999 and Langer et al., 2004). Our perspective is however different, as we do not start from a set of predefined moves: our objective is indeed to describe the genre of the article and its structure in a quantitative perspective, starting from three levels of description: the structural, the morphosyntactic and the lexical level. The study is based on a generically homogeneous corpus composed of French and English journal articles that all belong to the linguistic domain, chosen as this is the field we have the best expertise in. The French corpus is made up of 32 issues of 11 linguistic journals, that amounts to 224 articles; whereas the English one includes 100 articles, that is 16 issues of 4 linguistic journals. Texts have all been issued between 1995 and 2001 to limit the possibilities of diachronic variations. In order to describe the document structure of scientific articles, we first marked up the document structure and the article constituents according to the Text Encoding Initiative Guidelines (Sperberg-McQueen et al., 2001), to ensure the corpus reusability and comparability with other corpora: the article sections were taken into account (introduction, body, divisions, conclusion), as well as its titles, subtitles, and specific components (examples, citations, appendices, etc.). This XML markup enabled us to obtain the main characteristics of the article structure and organization in the two languages (number of sections, structure depth, etc.) and to assess their stability and differences, using XSL stylesheets. Once these characteristics were established, we focused on the article sections: as both French and English linguistics articles are not submitted to an IMRAD structure (Introduction, Materials and methods, Results, Analysis, Discussion), only introductions and conclusions could be directly observed and compared. Indeed, it would have been irrelevant to analyze “third sections” as many texts are only divided into two main parts. The linguistic properties of introductions and conclusions were described thanks to two different levels of description: the lexical and the morphosyntactic levels, which did not require the same processing. The lexical characteristics of the sections were first obtained using Alceste and its Hierarchical Descendant Classification. We then concentrated on the morphosyntactic level, on the one hand because morphosyntactic variables easily lend themselves to voluminous data as they are formal enough to be tagged and calculable and on the other hand because various studies have demonstrated their efficiency in genre processing (Karlgren & Cutting 1994; Kessler et al., 1997; Malrieu & Rastier 2001; Poudat, 2003). Although several taggers are available, they are generally little adapted to the processing of scientific texts; for instance, the French Inalf Institute trained Brill tagger on 19th century novels and Le Monde articles. Most of the English taggers are trained on the Penn TreeBank corpus and use very robust tagsets which interest is descriptively very weak. As many available taggers are trainable (Brill Tagger, TreeTagger, TnT tagger, etc.), we decided to develop our own tagset and to generate a new tagger devoted to the processing of scientific texts. We then used a specific tagset of 136 descriptors (described in Poudat, 2004) to process the French corpus. The tagset is devoted to the characteristics of scientific discourse, and gathers the general descriptive hypothesis put forward in the literature concerning scientific discourse. Among the very specific variables we developed, we can mention symbols, title cues (such as 1.1.), modals, connectives, dates, two categories for the IL personal pronoun, in order to distinguish between the French anaphoric and impersonal IL, etc. The training task is very costly, as it requires the building of a manually annotated training corpus that has to be large enough to enable the system to generate tagging rules. For this reason, it was only led on the French corpus, using TnT tagger. We then adapted the tags and the outputs of CLAWS (the Constituent Likelihood Automatic Word-tagging System developped at Lancaster University, see Garside, 1987) to get comparable data. The morphosyntactic characteristics of the French and English introductions and conclusions were then determined, using statistical methods. After having described our methodology, we will present the results obtained thanks to this same methodology. The last part of our paper will discuss the conclusions that could be drawn from these findings.",
       "article_title":"Exploring the Generic Structure of Scientific Articles in a Contrastive and Corpus - Based PersPective",
       "authors":[
          {
             "given":"Noëlle ",
             "family":"SERPOLLET",
             "affiliation":[
                {
                   "original_name":"University of Orléans, France",
                   "normalized_name":"University of Orléans",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/014zrew76",
                      "GRID":"grid.112485.b"
                   }
                }
             ]
          },
          {
             "given":"Céline ",
             "family":"POUDAT",
             "affiliation":[
                {
                   "original_name":"University of Orléans, France",
                   "normalized_name":"University of Orléans",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/014zrew76",
                      "GRID":"grid.112485.b"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Overview A good deal of the emerging research literature concerned with online information resources focuses on information retrieval, which is concerned with the use of search engines to locate desired information. Far less attention has been paid to how the found materials are read and how that critical engagement can be enhanced in online reading environments. Building on research reported in relation to Willinsky’s work in and around the Public Knowledge Project (http://www.pkp.ubc.ca/), Warwick’s “No Such Thing as Humanities Computing?” (ALLC/ACH 2004), and Siemens, et al., “The Humanities Scholar in the Twenty-first Century” (ALLC/ACH 2004), among others pertinent to our community, this paper reports on a study examines the question of whether a set of well-designed reading tools can assist a sample of 15 humanities computing scholars in comprehending, evaluating and utilizing the research literature in their area. In progress at present, the study of our group will conclude in January 2006, with results available in March 2006. Should the paper be accepted for presentation at the conference, at that time the authors will revise their abstract, fully detailing the results of the study, which will include an analysis of the degree to which different types of reading tools (providing background, related materials, etc.) contributed, if at all, to these scholars’ reading experience. Context The larger study, a subset of which is devoted to computing humanists, investigates how journal websites can be designed to better support the reading of research in online settings for a wider range of readers than has traditionally been the case with research. Given that well over 75 percent of research journals now publish online, with a number of them made free to read, the reading experience and audience for research is changing. This work looks at whether the design and structure of the journal’s “information environment” can improve the reading experience of expert and novice readers of this literature. Specifically, it examines whether providing far richer context of related background materials for a given text than is available with print, assists the online reading process. In this way, the study seeks to understand, better, reading for information in online environments. Growing out of Willinsky’s design work in online information environments for schools, policy forums, and academic journals over the last five years, and work within the digital humanities, this study evaluates whether the specific online tools and resources that journals are now able to provide can assist expert and novice readers in making greater sense and use of the research literature. By drawing on related work in reading comprehension in schools, as well as from initial design experiments with journals in online settings, it is posited that information environments that provide links to related resources will enable a wide range of readers to establish a greater context for comprehending and potentially utilizing the research they have come to read. It may also support the critical engagement of more expert readers. This study focuses on testing a context-rich Reading Tools which can accompany each journal article (and if proven useful can also be used with online conference papers, reports, and theses). This tool provides (a) background on the article and author, (b) links from each research article to directly relevant materials (based on the keywords provided by the author), and (c) opportunities for interactivity, such as commenting and contacting the author. Utilizing research studies in medicine and education as the publishing content to be read online, the contribution of this Tool will be assessed with a sample of faculty members and students in education and medicine, as well as well as with a sample of policymakers and members of the public. The study, in this case, is being conducted with computing humanities scholars. A sample of 15 scholars will read an article in their field, and utilize the Reading Tools to see which tools, if any, and to what degree, these tools contribute to their comprehension, evaluation, and interest in utilizing the work they are reading. These tools will connect the reader to the author’s other works, related studies, book reviews, summaries of literary critics; work, online forums, instructional materials, media reoports, and other data bases. The readers will be asked to reflect on their reading and use of the tools in a think-aloud protocol, with the researcher. Lessons drawn from these and other readers’ experience and assessment of the value of the Reading Tools will assist our understanding of the nature of online reading, the potential readership of online research, the role of context in reading, while the study is also intended to contribute to improving the design of journals and other informational resources in online environments.",
       "article_title":"Giving Them a Reason to Read Online: Reading Tools for Humanities Scholars",
       "authors":[
          {
             "given":"Ray ",
             "family":"SIEMENS",
             "affiliation":[
                {
                   "original_name":"English, U Victoria",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"John ",
             "family":"WILLINSKY",
             "affiliation":[
                {
                   "original_name":"Education, U British Columbia",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Analisa ",
             "family":"BLAKE",
             "affiliation":[
                {
                   "original_name":"Geography, U Victoria",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"In this paper, I will examine the phonetic/phonological patterning of a sonnet by Gerard Manley Hopkins, No Worst, There Is None, one of the sequence of ‘Terrible Sonnets’ written during a period of deep despair toward the end of his life. My paper illustrates the use of a computer program, the Pattern-Finder, to develop the conventional, close-reading techniques in literary criticism, as exemplified in Burke (1973) or Vendler (1988), beyond what can be discovered by even the most aware naked eye and ear. I will argue that by using this new computational approach, we can discover a level of meaningful sound-structure present in the poem but previously inaccessible. (A full description of the computationally-based technique which I will be using is found Smolinsky & Sokoloff’s abstract proposal for the poster presentation plus software display Introducing the Pattern-Finder (#140), immediately preceding this abstract.) Hopkins was an accomplished linguist (in the sense of ‘language-learner:’ see Davies 1998), and one of the most consciously linguistically aware of all poets writing in English. He truly knew his medium, and used it to the fullest. In No Worst, we see this in his syntax, as in the Shakespearian epithet “no-man-fathomed” or the elided “that” relative in “under a comfort [that] serves in a whirlwind.” Both compressions embody the excruciating pressure which the poet is expressing; the second one also gives a sense of uncontrollable velocity. We see Hopkins’ awareness of his medium just as much in his phonetics. One example is the striking /iy/ assonance in six of the end-rhymes, also echoed inside the lines, in “heave,” “leave,” “shrieked,” “steep or deep” and “each,” /iy/ being the vowel with the highest pitch (Peterson and Barney 1952): in this context, I would argue, the most evocative of a scream. Another is the /p/ alliteration in the first two lines, all in pre-stressed (‘opening’ (de Saussure 1959), maximally plosive, aspirated (Ladefoged 1982) and forceful) position: “pitched past pitch” “pangs” “(fore)pangs” the complete absence of /p/ in lines 3-9, and the gradual accumulation of /p/’s in final (‘closing’ de Saussure op.cit, non-aspirated (Ladefoged op cit), minimally forceful position in lines 10-14. The movement of the poem is from increasing paroxysms of pain, up to a climax and then down to an uneasy annihilation in “sleep,” the last word of the sonnet. The placing of the two groups of /p/ alliterations, the strong opening /p/’s at the beginning and the weak closing /p/’s at the end, embody this progression. So far, I have given instances of Hopkins’ control of his medium, the English language, which anyone willing to read slowly and carefully can find for him- or herself. But I would argue that his capacity to build meaningful linguistic structures in his poem goes well beyond what even the most conscientious reader can pick out. If we look below the level of normal speech-sound repetitions (rhyme, alliteration, assonance) we will see that patternings of phonetic/phonological features of the speech sounds in the sonnet also support, even embody, its meaning. Aided by the Pattern-Finder, I will give two simple examples. The first is the distribution of pre-tonic voiceless consonants, that is to say, those consonants in the positions spotlighted by stress (see illustration 1: file Hopkins C cons-stress voiceless). We can think of voiceless consonants (speech sounds which are in the minority among English consonants, and in an even smaller minority if we consider the total number of English speech sounds) as a small—and especially salient because small—number of interruptions to the stream of vocal vibration ocurring as the poet or reader recites the sonnet. These interruptions will take the form either of small explosions (plosives) or small frications (fricatives) or something of both (affricates). We see how noticeable they are in the explosive first line’s “pitched past pitch of grief,” or the dragging, elongated quality of the tenth line’s “frightful, sheer no-man-fathomed! Hold them cheap” There are other things to be said about the function of pretonic voiceless consonants in the sonnet (see, again, illustration 1: file Hopkins C cons-stress voiceless), but here we will focus only on distributional gaps—where they are absent. We note that while there may be as many as five in a line (see frequency counts (leftmost, in red) for lines 2, 5 and 10), there is only one line with a single pre-tonic voiceless consonant, and only one with none at all. Line 4, with no pre-tonic voiceless consonants, is one in which the poet appeals to the Virgin Mary for “relief.” The forcefulness of his expression of distress is softened by the maternal presence which he is addressing; he begs and wails as a small child would. The last line, 14 evokes the dropping away of painful sensation into oblivion; the paroxysms are temporarily quieted. The final thing the poet focuses on is the state of “sleep,” and this is the only word in the line picked out by an initial voiceless consonant. Another feature-distribution worth mentioning, this one a contrast, is that of the stressed front versus the stressed back vowels in the sonnet. Stressed front vowels very much predominate: they are just over twice as frequent as stressed back vowels (see illustration 2: file Hopkins V stress front vs stress back). In the literature on phonetic symbolism, an area in which much work has been done on symbolic attributions to speech sounds, such as brightness and darkness, largeness and smallness, front vowels have been found to be bright, active, small, sharp and fast, and back vowels, dark, passive, large, dull and slow (see Sapir 1929, Newman 1933, Grammont 1946 among many others). So, given that the sonnet is about suffering, our finding might at first seem counterintuitive. But then we consider that the suffering described is acute (“Pitched past pitch of grief”), and that the consciousness evoked in the poem is just as intensely pleading for release (“my cries heave”). Moreover, the form of release grudgingly granted (“Here! Creep/ Wretch”) is only the temporary one of sleep. Thus, we realize that the sharp, bright, wakeful quality of the front vowels is apt for the evocation of a painful emotional state which is bound to recurr. We see that in line 11 and the beginning of 12, where the largest cluster of back vowels in the poem is found, these dominate for only a minute stretch: “Nor long does our small durance…” but are then taken over by the front vowels again “deal with that steep or deep. Here! Creep/Wretch..” Finally, the last line is the only one without any back vowels: there will be no true escape into something larger than the suffering self. The consciousness is to be granted only a temporary respite in the “whirlwind.” This is a preliminary sample of the kind of work one is able to do using the Pattern-Finder. As I said earlier, Hopkins was an accomplished linguist; he is known for two terms, ‘inscape’ and ‘instress’ that suggest the influence of subliminal awareness on poetic meaning. His concepts would lead us to believe that his poetry is a natural candidate for our approach:“ The word ‘inscape’ (coined by analogy with ‘landscape’) varies in its implications. But its main meaning is distinctive pattern, the relationship between parts that creates the integrity of the whole, which in turn is different at different times. ‘All the world is full of inscape and chance left free to act falls into an order as well as purpose.’ So there is pattern even in natural accident…No two things, if properly seen, are identical. Individuality is irreplaceable. The key-note of inscape is therefore not just pattern, but unique pattern.” (Myitalics) “Instress is the active energy that binds parts of the inscape of the whole…It is also a faculty of the human mind when it brings things into creative relationship. It demands an act of pure attention…for ‘the eye and the ear are for the most part shut and instress cannot come…’ ” (Davies, 1998, quoting Hopkins)",
       "article_title":"Music and Meaning in a Hopkins “Terrible Sonnet”",
       "authors":[
          {
             "given":"Stephanie ",
             "family":"SMOLINSKY",
             "affiliation":[
                {
                   "original_name":"Humanities Department,     New York City Technical College, CUNY",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Below is an extract of a conventional, TEI encoded stanza from an Early Modern ballad by an anonymous author:* For what we with our flayes coulde get To kepe our house and seriauntes That dyd the freers from us fet And with our soules played the marchantes And thus they with theyr false warantes Of our sweate have easelye lyved That for fatnesse theyre belyes pantes So greatlye have they us deceaved This form of XML markup, which imbeds the markup within the artifact itself, will be familiar to most readers. In their 1997 essay, “Hyperlink Semantics for Standoff Markup of Read-Only Documents”, Henry S. Thompson and David McKelvie describe a system of “standoff markup” in which markup data resides in a physically separate file from that which it describes. Using a standoff markup system of the type described by Thompson and McKelvie, all of the TEI markup from the above citation would be stored in a separate file from the text itself, relying on a pointer system to describe its relation to the root text:* The above example uses the order of words in the root text to define inclusionary collections of words that are described by the XML elements in the standoff markup file. It provides the same markup description of the text as the more familiar, traditional example given above, but without intruding into the integrity of the root text file. As noted by Thompson and McKelvie, one major advantage of a standoff markup system is that it presents the condition of possibility for utilizing multiple, overlapping hierarchies to describe the same root artifact—something that cannot be achieved using a conventional, single-file markup approach. At the Maryland Institute for Technology and the Humanities (MITH, http://www.mith.umd.edu), we are currently collaborating with the Library of Congress (LOC, http://www.loc.gov/) to develop a platform for the encoding and delivery of digital resources in the LOC’s American Memory collection (http://memory.loc.gov/ammem/) utilizing an interactive, standoff markup system designed specifically to allow multiple, overlapping markup hierarchies, including markup provided by web-users visiting the collection via the American Memory website. This platform, named White Rabbit, is a vertical standoff markup solution that provides an easy to use Graphical User Interface (GUI) for editorially controlled base- document preparation, a collection of web-service applications that allow users to browse, search, and retrieve recourses using a standard, HTML web-browser or to retrieve raw XML source for each resource, and, most importantly, provides a web-based interface for users to add their own markup layers to texts in the collection. On the technical side, White Rabbit functions by tokenizing raw, ascii textual data at the level of base, lexically significant units (most often words, but frequently other diacritical and textual elements) and storing an ordered list of tokenized elements in a SQL database. It then allows users to construct XML using a simple point and click interface and to validate this XML against a DTD. XML “layers,” including those created by resource consumers visiting the LOCs website, are stored in a collection of related database tables. As new markup layers are added to each artifact, resource consumers gain the ability to choose which markup “layers” to apply to the text on delivery. Once a markup layer is chosen, users can then perform advanced XML searching, parsing, and manipulation of artifacts using any web browser. For example, using the example of the Early Modern ballad extracted above, the user could search within a single or collection of Ballads for all instances of the occurrence of a particular word or phrase within a refrain only. Using convential markup systems, this type of functionality is available only using specialized XML browsers. White Rabbit also provides a collection of “hierarchy analysis” tools that allow users to analyze the ways in which multiple markup layers for a given text relate to each other. Using these views, a user can identify filtered or un-filtered collections of layers and search for statistically significant patterns of convergence and divergence between multiple markup layers in these collections. Over time, this aspect of White Rabbit’s functionality provides an increasingly valuable bank of data regarding the ways in which a growing collection of users understand both formal and thematic elements of artifacts contained in the collection. With White Rabbit you can have your cake and eat it too, applying multiple markup strategies to the same text for retrieval and display when determined by particular scholarly contexts, providing robust analysis of the patterns in textual structure that emerge through multiple, overlapping markup layers, and delivering finely tuned XML parsing and searching of texts to any user with a standard web-browser. White Rabbit is comprised of a collection of cross- platform, Java-based client and server applications and applets that communicate with a SQL database. It is an open-source platform specifically designed to be easily exportable to a variety of platforms and uses and will be available for public download in both compiled and source distributions in late 2006. The proposed paper will present a brief introduction to the concept of standoff markup as described above, an interactive demonstration of the advantages of standoff markup, and, finally, an interactive demonstration of the White Rabbit platform. Detailed information about how to download and implement White Rabbit and/or participate in open-source project development will also be provided.",
       "article_title":"White Rabbit: A Vertical Solution for Standoff Markup Encoding and Web - Delivery",
       "authors":[
          {
             "given":"Carl ",
             "family":"STAHMER",
             "affiliation":[
                {
                   "original_name":"MITH, University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction While many methods have been applied to the problem of automated authorship attribution, John F. Burrows’s “Delta Method” [1] is a particularly simple, yet effective, one [2,3] The goal is to automatically determine, based on a set of known training documents labeled by their authors, who the most likely author is for an unlabeled test document. The Delta method uses the most frequent words in the training corpus as the features that it uses to make these judgments. The Delta measure is defined as: the mean of the absolute differences between the z-scores for a set of word-variables in a given text-group and the z-scores for the same set of word-variables in a target text [4]. The Delta of the test document is computed with respect to each of the training documents, and that author whose training document has minimal Delta with the test document is chosen for attribution. While this method is intuitively reasonable, we may still ask: Why does this method work so well? Why z-scores? Why mean of absolute differences? Perhaps if we understood the mathematical underpinnings of the method, we could modify it to make it more effective. Furthermore, we would know better when it is applicable and when using it would not make sense. Hoover has implemented Delta-based attribution as a spreadsheet [5]; we have re-implemented it as Java program with several options for using different variations, which we are experimenting with. This program will be made available to the research community once it has reached a stable state. Probabilistic formulation This section will briefly show how Burrows’s Delta may be profitably viewed as a method for ranking authorship candidates by their probability. Let X and Y be n-dimensional vectors of the word frequencies in two documents. Note that the z-score is obtained by subtracting out the mean and dividing out the standard deviation. Then the Delta measure between these documents can be reformulated, as shown in Figure 1. Note that the value of mean[i], the mean frequency of word i, cancels out, with the only effect of the training corpus as a whole being the normalizing factor of std[i], the standard deviation for word i. Thus, Delta is like a scaled distance between the 2 documents. It is not the ordinary distance “as the crow flies”, but rather it is the sum of each dimension independently, called the “Manhattan distance”. It is like walking the streets of Manhattan as we stay on the grid. Note that if we consider the mean of a distribution in place of Y[i], this has a form similar to a Laplace probability distribution [6]. Specifically, it is the exponent of the product of independent Laplace distributions. Thus, we are assuming that the individual document that we are comparing the testing document against is a sort of average document for that author. Taking of the z-score corresponds to the normalization in the exponent. So, in a sense, Delta is measuring the probability of a document being written by an author taking each word frequency independently and then choosing the document with the highest probability. In effect, that we are using the z-score means that we are estimating the parameters of the Laplace distribution by the sample mean and standard deviation. However, the maximum likelihood estimator of the Laplace distribution is the median and the mean absolute deviation from the median [6]. This gives us our first variation of the Delta measure. Instead of using the z-score, we should use the median and “median deviation”. Whereas the Delta measure gives a distance in a purely abstract space, this variation provides a well-founded probability. Now that we know we are looking at a probabilistic model, we can try putting in other distributions. A commonly-used distribution is the Gaussian, or normal, distribution. It is similar to the Laplace distribution except that it uses a sum of squares rather than of absolute values, based on the mean of the mean and standard deviation, hence using the z-score is appropriate here. Note that in this case, we have the “as the crow flies” Euclidean distance instead of the Manhattan distance. Further, note that the previous measures consider each dimension independently. In this sense, they are axis- aligned. This means that the use of each word is assumed to have nothing to do with the use of any other words. Of course, this assumption is false, but may be a reasonable approximation. To take this co-occurrence into account, we can use a rotated method, eigenvalue decomposition. Previously we used the z-score of individual words. Instead of using the standard deviation, we can generalize to using the entire covariance matrix. In this, we take the largest magnitude eigenvalues from the covariance matrix and use the corresponding eigenvectors as the features. Evaluation We are currently performing empirical tests. To compare these new variants to the original and each other, we will use 3 corpora. First, we will use the data in the spreadsheets from [5] to check that our implementation is working properly and so we can directly compare results. The second will be a collection of essays written by students taking a psychology course. There are up to 4 essays by each author. The third will be the 20 newsgroups corpus from http://people.csail.mit.edu/jrennie/20Newsgroups/. These corpora will be split into testing and training such that the training has only 1 or 0 of each author. Having 0 allows for the possibility of an unknown author. Each of these variations will be run on the corpora and the results of each classification will be split into 5 categories, based on who the attributed author is (lowest Delta candidate), and whether the attribution is considered reliable. This is determined via a threshold on attribution confidence, such that confidences below the threshold are considered “unknown”. To determine the confidence, we use Delta-z, following Hoover [3]. The first type of classification decision, a, is a true positive, where the correct author is attributed. Next, b is a false positive, where a known author is chosen, but not the correct one. Another false positive is g, where a known author is chosen, but the true author is not in the training. Fourth, d is a false negative, where no author was recognized, but should have been. Finally, e is a true negative, where the true author was not in the training set and was not recognized. These allow us to calculate the true positive and false positive rate, where: true positive rate = a/(a+d) and false positive rate = (b+g)/(b+g+e) These values can be used to make a receiver operating characteristic (ROC) graph (Figure 2), which shows the sensitivity of a method to the signal in the data. It is trivial to declare everything a negative, which would give 0 to both TP and FP. As you classify more instances as positive, there is more of a risk that an instance classified as positive is not. An overall measure of a method’s efficacy can be computed as the area under the ROC curve (AUC); this score will be used for comparison between the methods. AUC measures the trade off between false positives and false negatives, with the baseline at 50% (where the line goes straight from 0,0 to 1,1) and the best possible value of 1, meaning always getting true positives with no false positives. In this way, it will allow us to judge and compare how well the different variations work. Conclusion We have reformulated Burrows’s Delta method in terms of probability distributions. This allows us to extend the method to use multiple different probability distributions and to interpret the result as a probability. At the conference, we will present results comparing the effectiveness of these variations. More importantly, this work provides a more solid foundation for understanding Delta. In particular, the probabilistic assumptions that it makes, such as word frequency independence and that authors have similarly-shaped word-frequency distributions, are made explicit, allowing us better understanding of the uses and limitations of the method. For example, it is now clear why the method should only be applied to documents all of the same well-defined text type.",
       "article_title":"A Mathematical Explanation of Burrows’s Delta",
       "authors":[
          {
             "given":"Sterling ",
             "family":"STEIN",
             "affiliation":[
                {
                   "original_name":"Linguistic Cognition Laboratory     Department of Computer Science     Illinois Institute of Technology     Chicago, IL 60616",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Shlomo ",
             "family":"ARGAMON",
             "affiliation":[
                {
                   "original_name":"Linguistic Cognition Laboratory     Department of Computer Science     Illinois Institute of Technology     Chicago, IL 60616",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The development of digital technology make us revisit the traditionnal approach of text edition as a research tool such as used in the Humanities. Our proposal applies to the problem of electronic editing of Judeo-Spanish documents in Hebrew characters over which the translitteration and transcription techniques are carried out. We study the possibilities of collaborative platform construction in which these documents could be annotated and organized as a reference digital repository for the scholars of this specific domain. Beyond the meta description necessary for the document identification (Dublin Core metadata standard), we aim to promote the annotating of the uploaded documents (in the free form, complementary to the metadescription) for the purpose of study and discussion. The source documents (presented as image files, for instance) are transformed into digital documents, which become then the research tools. The role of annotation (carried out by a researcher or a domaine expert) is to convey the metadata information about the source and the “ derived ” documents (for the purpose of identification and localization of digital repository resources) and to make possible the discussion over these documents (to interprete, comment, refuse, accept, translate, and so forth). In a previous paper we proposed a conceptual model defining the data structuring necessary to this double purpose of identification and annotation (Rouissi, Stulic 2005). Our point of view is that the successful setting up of socio-technical device for the researchers of a particular scientific field depends on the definition of appropriate functionalities. In order to define them in this context of relatively small and geographically dispersed scientific community, we planned to realise a survey that should confront our initial hypothesis (our conceptual annotation model) with the declared academic practices. The survey results should allow us to identify the needs and the expectations of researchers and to determine the direction of project development and the setting up of appropriate functionalities. It should also measure the level of Information and Communication Technology (ICT) integration in their current activities (the type of technological tools in use, the individual or collective framework, the use of annotation over digital documents, the participation in collaborative work environments…). The answers will be analysed at the quantitative (closed questions) and the qualitative level (the content analysis) so as to provide a typology of declared practices and/or needs. In broad lines, our work is oriented towards the elaborating of the full Web platform and is based on the existent open source solutions. But only with the clear definition of the functionalities that are expected by the scientific community members we will be able to model the final technical choices. We envisage the proprietary format for the model in question, but it should permit the exportation through the standardized formats for data interchange like Text Encoding Initiative for document structuring or Dublin Core for meta description of working documents. The platform should promote the annotating in collaborative mode (with different levels of participation : from reader to editor) and organize the discussion (which can can different purposes : to transcribe, to translate, to comment, to propose…) by qualifiying it (following the vocabularies that are still to be defined, not necessarily taken into account in the specifications such as TEI). The final platform will include also the documentary corpus building that will be exploitable thanks to the search functionalities. The future user will be able to effectuate a search (of ‘selection’ type) among digital documents which constitute the corpus, and after the document selection, to consult the related discussion and according to his/her profile, he/she will be able to participate by adding the annotation and/or reacting to the annotation previously added by other user(s). The user will be able to generate a new version with his/her own annotations and to export it following the proposed formats and standards. Although the context to which we apply our project is very specific, our reflexion is oriented towards a larger approach. Our aim is to construct a digital environment which could be applied to the research domains with similar needs for documentary corpus organisation and work on documents.",
       "article_title":"Annotations in Collaborative Mode for Ancient Documents Study in Digital Environment",
       "authors":[
          {
             "given":"Ana ",
             "family":"STULIC",
             "affiliation":[
                {
                   "original_name":"AMERIBER, Bordeaux 3 University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Soufiane ",
             "family":"ROUISSI",
             "affiliation":[
                {
                   "original_name":"CEMIC - GRESIC, Bordeaux 3 University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"From a technical point of view, texts are represented in computer systems currently as linear strings of atomic characters, between which no distinction is being made on the technical level. In the markup discussions within the Humanities, this is usually accepted as an immutable fact of technology. We propose, that the handling of Humanities texts could be considerably easier, if an engineering model could be created, which is built upon a more complex understanding of text. 1. Basic model of “text” proposed For this we start with the proposal to understand a text as a string of codes, each of which represents “meaning” measurable in a number of ways. More detailed: Texts – be they cuneiform, hand written or printed – consist of information carrying tokens. This tokens fall into a number of categories, which are differentiated by the degrees of certainty with which they can be used in various operations. The trivial example are ASCII or Unicode characters. Less trivial are symbolic tokens, like, e.g. the (primitive) string representing the term “chrismon”, a bit map representing a Chrismon (or something similar) etc. A string made up of such tokens, which represents a text, can be understood to exist in an n-dimensional conceptual universe. Such dimensions, which have different metrics are, e.g.: • A dimension which has coordinates with only two possible values (“yes”, “no”) which describes, whether a token has an additional visible property, like being underscored. • Another dimension, which has coordinates on a metric scale, which assigns a colour value, which allows to define similarities. • Another dimension describing the position of a token like “Chrismon” with a ontology describing the relationships between Chrismons and other formulaic forms. • A real number, giving the relative closeness between a bitmap representing a Chrismon and an idealtypische Chrismon. If we view such a string from a specific point in the conceptual space – a.k.a. an individual’s research position – many of these dimensions tend to collapse in the same way, as 3 dimensional objects collapse their z-value when represented in two dimensional drawings. 2. Relationship between text, markup and processing We assume, that string processing, on a very low level of engineering, can be implemented in such a way, that the low level programming tools, which are used today for the generation of programs handle texts, can tackle the implications of this model directly. This implies, e.g., a low level function, which can compare two strings “sensitive for differences between included symbolic tokens beyond a specified ontological distance” or “insensitive for this” very much like current implementations of low level tools can compare can compare two strings as “case sensitive” or “case insensitive”. While currently all textual phenomena have to be described with one integrated system of markup, expressing attributes, which can only be observed on the character level, without necessarily being interpretable on the spot, as well as highly abstract textual structures, the proposed approach would divide textual attributes into two classes: Textual attributes in the more narrow sense, which can be handled as properties of the strings used to represent the texts and structural (and other attributes) which are handled by a software system implying the presence of the underlying capabilities of the low level textual model, while focusing itself upon a class of higher level problems: E.g. a data base operating upon an abstract content model of a drama, relying upon the handling of page references as well as critical apparatus by the underlying string handling tools. The later implies that documents will – seen from today’s perspective – usually be marked up in at least two concurrent ways. Some implications of that will be listed. 3. Possibilities of generalizing the basic model. Our model so far has assumed, that information is handled by strings, i.e. by tokens which form one-dimensional sequences. (Non linear structures are one-dimensional as well in this sense: a path within a graph has a length, measured as the number of nodes through which it passes. It cannot be measured in two dimensions, as the relative location of the nodes within a two dimensional drawing is just a property of the visualization, not the structure itself.) There is no reason, however, why the notion of meaning represented by an arrangement of tokens carrying information should not be generalized to two dimensions (images), three dimensions (3D objects) or four dimensions (e.g. 3D representations of historical buildings over time). A problem arises, however, when one compares some operations on one- with the same operations on more-dimensional arrangements of information carrying tokens. A good example is the comparison of “insertion operations” in strings v. the same operation in images. We conclude by proposing to solve that problem by the notion, that a textual string is a representation of an underlying meaning with a specific information density, which usually will transfer only part of the meaning originally available, just as a digital image represents only part of the visual information available in the original. This in turn leads to the notion, that not only the handling of information carrying tokens can be generalized from the one to the more-dimensional case, but the properties of markup languages can as well. 4. Concluding remark While the generalisation of the model quoted above is presented in Paris for the first time, the idea of a specilised data type for the representation of Humanities text goes back to the early nineties (Thaller 1992, Thaller 1993). Various intermediate work never has been published, an experimental implementation, focusing on the interaction between texts and databases administering the structure embedded into the text does exist, however and is used in the production level system accessible via http://www.ceec.uni-koeln.de (Thaller 2004). More recently a project started at the chair of the author, to implement a datatype “extended string” as a series of MA theses in Humanities Computer Science. The first of these (Neumann 2006) provides a core implementation of the most basic concepts as a class augmenting Qt and fully integrated into that library.",
       "article_title":"Strings, Texts and Meaning",
       "authors":[
          {
             "given":"Manfred ",
             "family":"THALLER",
             "affiliation":[
                {
                   "original_name":"Universität zu Köln",
                   "normalized_name":"University of Cologne",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00rcxh774",
                      "GRID":"grid.6190.e"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The submitted paper gives an overview of the recently founded project Viennese Theatre Corpus (VTC), a sub-project within the wide range of work undertaken by the Austrian Academy Corpus research group at the Austrian Academy of Sciences in Vienna. In general the presentation will deal with the specific content mark up of theatre-related materials making use of XML and discuss the application of new technologies and the possibilities of advanced research methods for the complex and unique historical and socio-political situation of theatre and its language spoken on stage in Vienna during the 19th century. Focusing on the Viennese theatre of the 19th century , the playwright Johann Nepomuk Nestroy (1801-1862), is the centre of interest and represents the main starting point for the development of the corpus. Johann Nestroy dominated the commercial stage as actor-dramatist for nearly thirty years in the Austrian theatre of the 19th century and wrote about 80 plays. Using different registers of Viennese German ranging from stagy literary German to colloquial Viennese, the work of this supreme comic dramatist is full of allusion, vivid metaphor and cynical criticism. Emphasising linguistic inventiveness his comedies reflect a deep-rooted discontent with moral double standards, static and mendacious society, false friendship and unfounded prejudices. The Historical-Critical Edition of the Collected Plays of Nestroy, which appeared between 1924 and 1930, published by Otto Rommel and Fritz Brukner represents the core text of this theatre corpus. With the aim to provide a structured overview of the critical and historical reception of Nestroy’s work and a valuable collection of different registers of language used on the stage of the Viennese popular theatre, a large amount of contemporary dramas from others of the 19th century like Friedrich Kaiser, Carl Elmar, Carl Giugno and Alois Berla will be incorporated. Furthermore historical and socio-economic descriptions, press criticism, texts from the theatre censorship of this period, legal publications and memoirs of famous actors and stage directors offer a well-assorted and wide-ranging collection of different text types. In its function as a main place for entertainment and social contact, the theatre was expanding in this period. In general the Viennese public was used to about thirty new premieres a year and new plays and new productions in European capitals like London and Paris led to a very rapid production and exchange of theatre texts. As a consequence, translations and adaptations of English and French plays reached the Austrian capital very quickly, often in pirated editions. Therefore Nestroy’s work was highly influenced by the literary production of other European countries. Works by Charles Dickens, Eugène Scribe, Paul de Kock, Dupeuty, Bayard as well as Varin served him as stimulation or models for his own plays and for this reason the incorporation of a wide selection of Nestroy’s sources from English and French literature is planned. The challenge for the development of a specific content mark up due to this diversity of text types and language resources is one of the main topics of the paper. The paper will briefly illustrate the annotation system which is used for the Viennese Theatre Corpus, discuss the scope of XML for dramatic texts and illustrate the incorporation of the critical appendix of the printed version of Nestroy’s Collected Plays. Due to the contradiction between performed speech, the action on stage and the subsequent publication as printed text, drama is often structurally complicated. The paper will address the dominant question how XML and cognate technologies can help to structure a play, to illustrate the underlying dramatic composition and its use as an analytical tool by means of a specific semantic tagging strategy rather than as a simple archiving solution. The VTC, a thematically oriented text corpus, will provide access to a variety of original source documents and secondary material. In consideration of the special textual and material requirements of theatrical works the paper will finally discuss the potential contribution of this specific corpus of Viennese plays of the 19th century to scholarly knowledge within the wide range of philological, historical and literary research domains.",
       "article_title":"Modelling a Digital Text Archive for Theatre Studies -- The Viennese Theatre Corpus",
       "authors":[
          {
             "given":"Barbara ",
             "family":"TUMFART",
             "affiliation":[
                {
                   "original_name":"Austrian Academy of Sciences, Vienna",
                   "normalized_name":"Austrian Academy of Sciences",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/03anc3s24",
                      "GRID":"grid.4299.6"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Since its initiation in 1995, the Cervantes Project has focused on creating a comprehensive on-line resource centered on the iconic Hispanic author Miguel de Cervantes (1547-1616). The project, a collaboration of researchers in Hispanic Studies and Computer Science, provides bibliographical, biographical, and textual materials, including an Electronic Variorum Edition based on first editions of Cervantes’ best known work, Don Quixote. Recent additions include an exhibit of bookplates (ex libris) inspired by the Quixote. Soon to be released are a significant collection of illustrations associated with key Quixote editions (the textual iconography) and a presentation of the musical aspects and influences of Cervantes’ works. These latter two collections will be discussed further in this paper. As our work with the Cervantes Project has evolved, we have become increasingly aware of the need to develop rich interlinkages between the panoply of resources collected for and produced by this project [3]. Within the broader scope of the project as a whole, our textual iconography collection offers a fertile ground for exploring the needs of large, interdisciplinary collections, and the techniques for integrating the resources they contain. The textual iconography collection (Figure 1) is intended to facilitate a more complete understanding of the iconic transformations of the Quixote [5] throughout history by assembling a digital collection of more than 8,000 illustrations taken from over 400 of the most significant editions of this seminal book [15]. In the process of assembling and working with this collection, we have found traditional approaches to presenting image-based collections in both print and digital media to be lacking [12]. While print has long been the dominant media of scholarly communication, it does not scale well to projects of this scope [8]. Digital image collections, on the other hand, are often focused on presenting large numbers of images and associated metadata but lack the scholarly commentary and integration with other textual resources required for scholarly investigations. Moreover, we have found a need to adopt a rich model of hypertext, based on automatically-generated links and multiply-rooted to reflect multiple interpretations of simultaneous significance.We have aggressively pursued the development of this collection 2 with the intention that it will enable new forms of textual, visual and critical analysis. In particular, it enables scholarly research in two major directions that remain poorly understood despite the incredible amount of scholarly attention devoted to the Quixote. First, the textual iconography of the Quixote is a key resource to help literary scholars better understand its reception and interpretation throughout history—the illustrations acting as a “hand-mirror 3 ” that allows us to see how successive generations of readers, including our own, have literally painted themselves into Cervantes’ story [7]. Second, this collection is an invaluable dataset for art historians enabling them to better explore the tools and techniques employed in the often neglected field of book illustration 4. The illustrations found in the pages of the Quixote trace the evolution of graphic art in modern printed works, from the first wood cuts of early 17th century, to the copper engravings, etchings aquatints of the middle and late 17th century and 18th century, to the xilographs and lithographs of the 19th century, and finally the mechanical techniques of the 20th century including offset printing [10,13,14]. These areas remain poorly understood, not simply because of a lack of attention, but in large part because traditional approaches do not adequately support investigations of this nature. The value of the textual iconography collection is not limited to the scholarly community. Its canonical status in world literature courses, its iconic nature in Hispanic culture, and the renewed interest awaked by the recent celebrations surrounding the 400th anniversary of its publication, ensure a continued popular interest in the Quixote. The illustrations of the adventures continue to captivate the popular imagination and help to bring the book alive to millions of readers. By making the collection available to the public in digital format, we are able to radically increase the resources available to students of the Quixote from all walks of life. Our goal, however, is not simply to provide a large pile of pictures for students to peruse, but to further enrich their understanding of both the Quixote and of art history by supplementing the illustrations with commentary about the significance of the images, their relationship to the text, and the artistic achievement of the illustrators and engravers who created them. To adequately support both the scholarly and popular communities that will access the collection, it is not sufficient to merely provide an online catalogue of the illustrations, regardless of the completeness of the collection or the detail of the metadata. Instead, we have identified three levels of information required to successfully present these materials: 1. Descriptive metadata: The first level of information includes factual descriptions of the items it describes. For our collection, descriptive metadata is provided for both individual illustrations and the editions in which those illustrations were published. This information includes the title of the image, its physical size and the size of the page it is printed on, the name of the artist and/or engraver, the style of the image, and the printing technique used. 2. Scholarly commentary: The second level goes beyond simple descriptions of the items in the collection to provide a critical assessment of those items, their narrative context, and their hermeneutic and aesthetic significance. Within our collection we are developing biographical commentary about artists and engravers and technical and artistic commentary about each individual image. 3. Hypertextual connections: The third level consists of the information needed to develop dense hypertextual structures and the tools to effectively navigate them. A key challenge in constructing collections of this type is developing appropriate strategies for interconnecting the resources being continuously added to the collection. While most traditional editorial approaches require editors to carefully edit each resource by hand, this is beyond the scope of our current resources and would greatly limit the size of our collection, the speed with which it could be made available, and the degree of interlinkages that could be provided. Accordingly, we are using two strategies for automatically interlinking digital resources. One approach relies heavily on the metadata associated with each digital image. We use this metadata information to automatically discover relationships between existing and new resources and then to generate navigational structures based on those relationships. A prototype of this system has been employed internally for use in a project centered on the music in Cervantes’ works [9] (Figure 2). One key application of this approach allows us to integrate multiple resources, including illustrations, with the textual and narrative structure of the Quixote [2]. To accomplish this we have developed a formal taxonomy of the narrative and thematic elements of the Quixote, which is then used in cataloging each image and can be used for the associated commentary. This allows us to codify the connections between the illustrations and commentary without requiring a particular version of the text. The second approach to interlinking resources we have developed involves elucidating the internal structure of the textual resources in the collection and using that structure to automatically generate navigational links and to inform visualizations [1]; an approach that generalizes Crane’s [4]. In this context, we have developed a toolkit to implement this approach for a collection of historical documents pertaining to Cervantes and his family. The hypertextual collections created for our purposes create separately rooted structures over a common set of interlinked elements. For example, in the music collection referred to above, natural collection roots include the compositions, the composers of the pieces, the instruments associated with the pieces, and the texts that refer to the pieces. Each rooted collection includes information specific to the collection (e.g., biographies of the composers and musical scores are associated with their respective collections), but ultimately collections are cross-linked (e.g., between compositions and composers, between instruments and compositions). The texts provide a unifying framework that draws the other components together, yet the texts themselves represent a distinct collection. The structures in the textual iconographic collection show similar relationships, with the added complexity implicit in the three-level categorization of collection information. In conclusion, by adding detailed scholarly commentary in addition to descriptive metadata and to providing sophisticated tools for automatically building a hypertextual structure into the collection, we are able to provide a new resource that better meets the research needs of scholars and to assist the general public in understanding and appreciating the significance both of the Quixote itself, as well as its reception and visual history. Our approach allows the readers of scholarly commentaries more direct access to the primary source materials used to develop and support those commentaries 5 [6,11]. Conversely, it allows individuals focusing on the primary materials access to secondary scholarly works to better understand a variety of reading perspectives and to explore and formulate their own interpretations of these unique materials. Moreover, the nature of the collection and its information relationships has required the addressing of canonical hypertextual structuring issues. Taken as a whole, our collection and hypertextual archive opens previously unavailable opportunities for scholarly study of the Quixote and its unique literary, cultural and iconic status.",
       "article_title":"Textual Iconography of the Quixote: A Data Model for Extending the Single - Faceted Pictorial Space into a Poly - Faceted Semantic Web",
       "authors":[
          {
             "given":"Eduardo ",
             "family":"URBINA",
             "affiliation":[
                {
                   "original_name":"Hispanic Studies, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Richard ",
             "family":"FURUTA",
             "affiliation":[
                {
                   "original_name":"Computer Science, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jie ",
             "family":"DENG",
             "affiliation":[
                {
                   "original_name":"Computer Science, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Neal ",
             "family":"AUDENAERT",
             "affiliation":[
                {
                   "original_name":"Computer Science, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Fernando González ",
             "family":"MORENO",
             "affiliation":[
                {
                   "original_name":"Universidad de Castilla La Mancha",
                   "normalized_name":"University of Castile-La Mancha",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/05r78ng12",
                      "GRID":"grid.8048.4"
                   }
                }
             ]
          },
          {
             "given":"Manas ",
             "family":"SINGH",
             "affiliation":[
                {
                   "original_name":"Computer Science, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Carlos ",
             "family":"MONROY",
             "affiliation":[
                {
                   "original_name":"Computer Science, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. Introduction The recent experiments on digital support of Joyce (1989), Moulthrop (1995), Jackson (1995) or Amerika (1997) have drawn attention to the potential of the electronic medium as a medium for literary creation. The emergence of the electronic literature has also determined the coming out of a new form of aesthetics mainly concerning the possible replacement of the printed book by its electronic counterpart (Bolter, 1991; Landow, 1997; Birkerts, 1994), the dilemma of the end of book versus the book without end (Douglas, 2000), the prospective capacity of the hypertext (Joyce, 2000) or of the ergodic literature (Aarseth, 1997), and the reconciliation of immersion and interactivity as a “model for purely language-based literature” (Ryan, 2001). Unlike the previous interfaces (tablets, roll, codex), the electronic interface supposes the existence of two types of texts: on the one hand, the text intended to be read and on the other, the code written in a programming or markup language, and determining the performances of the interface. This interaction between the visible and the hidden text, between the huge potential of expression of the natural language and the constraints of the programming language, represents one of the central elements in the production of new forms of electronic textuality. This kind of dependence implies a double sense relationship: the code supports the creation of new textual forms and these forms can inspire new types of encoding, increasingly flexible. 2. Theoretical and practical assumptions From a theoretical point of view our project deals with the relationship literature – science – digital technology, writing - programming, visual – textual, in the creation of new forms of electronic textuality. We are also interested in the implications of this kind of interaction upon the domain of literary criticism and especially in the possible relationships between the aesthetics of the electronic medium and other fields of the contemporary criticism (close reading, new historicism, genetic criticism), or of the information and cognitive sciences. The present study consists in an experimental approach dealing with the unexplored possibilities of the electronic support as a medium for textual investigation. Its goal is the construction of a new type of interface (an editor written in Java programming language and using XML annotated texts) allowing the writer and the reader to increase or decrease the degree of detail of the text, by a procedure evoking the “magnifying glass effect” and the fractals geometry. Inspired by the fictional construction of Stephenson (2000) and by the fractal theory of Mandelbrot (1983), this new kind of textuality would be a layout on levels of “depth”, accessible by operations of zoom-in and zoom-out, the text of the most abridged level being reproduced and appropriately augmented on the subsequent, deeper levels. The study deals with the new types of relationship author – text – medium – reader, characters, plot, narrative strategy or knowledge organization determined by this form of “scalable” textual and conceptual structure. The main question addressed by the study would be therefore related to the possible applications of this form of electronic text intended to be written and explored “under the magnifying glass”, and implying a halfway between linearity and non-linearity, immersion and interactivity, features usually associated with the printed book and respectively with the hypertext. 3. Possible Applications The first application would be related to the concepts of close reading, a method using short excerpts from a text and carefully examining its style, rhetorical strategies, philosophical and sociological assumptions (Gallagher and Greenblatt, 2000), and of new historicism, an approach bringing to light some aspects of a literary work by trying to reconstruct its historical and cultural context (Greenblatt, 2004). The use of the magnifying glass editor would therefore allow a sort of fusion of the text with the literary or historical analysis, developed on several levels of detail. We could thus imagine the editor as a tool relating, at different “scales”, the literary text with the historical and cultural context having produced it, or with the critical commentaries and marginal notes, in a complex process of reading, re-reading and writing. The second type of application would concern the field of genetic criticism interested in the dynamics of the process of writing (Grésillon, 1994), and in the intertextual dimension and reading/writing dialectics of the “work in progress” (D’Iorio and Ferrer, 2001). From this point of view, an analysis “under the magnifying glass” would imply a layout on levels relating the definitive form, through different variants, to the first plan sketching the idea of the text. This layout could therefore facilitate the understanding of the gradual dynamics of the act of writing or the recognition of the eventual traces of previous readings. Another type of application may concern the narrative strategies. We can imagine, for instance, a sort of auto-reflexive text, conceived as a set of reflections on the act of writing and trying to retrace by details accumulation the path backwards from writing to reading, from verbal expression to life experience, and involving different “degrees of immersion” as part of the storytelling. Other approaches could be related to the development of the characters in a literary text. Greenblatt (2004) suggests that in Venus and Adonis, Shakespeare uses a technique of approaching or distancing the reader from the characters, by increasing or decreasing his “physical and emotional proximity”. On the other hand, Alan Palmer (2003) discusses the term of “behaviorist narrative” defined as an objective narrative focalized on the characters’ behavior, i.e. on their actions rather than on their feelings and thoughts. Could we therefore imagine a narrative allowing an alternation of proximity and distance or starting with a behaviorist approach and gradually investigating the psychological depths of the characters? Other applications of the model could include: the cognitive and pedagogic domain (knowledge organization on levels of complexity, from intuitive to abstract descriptions); the construction of dictionaries and encyclopedias (as collections of expanding articles intended to larger categories of readers); the domain of information science (systems providing several degrees of precision in response to users’ queries), etc. 4. Conclusion Of course, there is not an exhaustive list of possible applications, but rather some directions of study. Our presentation will concern the reconsideration of the “magnifying glass” and fractal geometry as metaphors for a new form of electronic textuality, drawing attention to some aspects of the dialectics: text / textual or historical analysis, written expression / idea or life experience, essential / detail, intuitive / abstract, precision / vagueness, in textual production and interpretation.",
       "article_title":"The Book “under the Magnifying Glass”. New Metaphors for New Forms of Electronic Textuality",
       "authors":[
          {
             "given":"Florentina ",
             "family":"VASILESCU ARMASELU",
             "affiliation":[
                {
                   "original_name":"Littérature comparée, Université de Montréal, Canada",
                   "normalized_name":"University of Montreal",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0161xgx34",
                      "GRID":"grid.14848.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"The LAIRAH (Log Analysis of Internet Resources in the Arts and Humanities) project aims to determine whether, how and why digital resources in the humanities are used, and what factors might make them usable and sustainable in future. Based at UCL and funded by the UK Arts and Humanities Research Council (AHRC) ICT Strategy scheme, LAIRAH is a year long study which will analyse patterns of usage of online resources through real time server log analysis: the data internet servers collect automatically about individual users. This paper will discuss the findings of our research to date, and the techniques of log analysis as applied to major digital humanities projects. In doing so, we address concerns about the use, maintenance and future viability of digital humanities projects, and aim to identify the means to which an online project may become successful. Context Hundreds of projects have been funded to produce digital resources for the humanities. In the UK alone, over 300 of them have been funded by the AHRC since 1998. While some are well known, others have been relatively quickly forgotten, indicating financial and intellectual wastage. Little is known of user centred factors determining usage. (Warwick, 1999) The aim of the LAIRAH study (http://www.ucl.ac.uk/slais/ LAIRAH/) is to discover what influences the long-term sustainability and use of digital resources in the humanities through the analysis and evaluation of real-time use. We are utilising deep log analysis techniques to provide comprehensive, qualitative, and robust indicators of digital resource effectiveness. The results of this research should increase understanding of usage patterns of digital humanities resources; aid in the selection of projects for future funding, and enable us to develop evaluator measures for new projects. Technical problems that can lead to non-use of digital projects are relatively well understood.(Ross and Gow, 1999) However, evidence of actual use of projects is anecdotal; no systematic survey has been undertaken, and the characteristics of a project that might predispose it for sustained use have never been studied. For example, does the presence in an academic department of the resource creator, or enthusiast who promotes the use of digital resources, ensure continued use? Do projects in certain subject areas tend to be especially widely used? Are certain types of material, for example text or images, more popular? Is a project more likely to be used if it consulted with the user community during its design phase? An understanding of usage patterns through log data may also improve use and visibility of projects. Project aims and objectives This project is a collaboration between two research centres at UCL SLAIS: CIBER (The Centre for Information Behaviour and the Evaluation of Research), (http://www.ucl.ac.uk/ciber) and the newly created CIRCAh (The Cultural Informatics Research Centre for the Arts and Humanities) (http://www.ucl.ac.uk/slais/ circah/). CIBER members are leading researchers in the use of deep log analysis techniques for the evaluation of online resources. (Huntington et al. 2003) Their strong record in user site evaluation in the health, media and scholarly publishing sectors is now being applied to the arts and humanities. We believe that no one has undertaken log analysis work in this sector: the LAIRAH project therefore offers great opportunities for knowledge and technology transfer. The LAIRAH project is analysing raw server transactions of online digital resources (which automatically record web site use) and will relate these to demographic user data to provide a comprehensive and robust picture of resource effectiveness. CIBER have developed robust, key metrics and concepts such as site penetration (number of views made in a session), returnees (site loyalty), digital visibility (impact of positioning on usage), search success (search term use and the number of searches conducted) and micro-mining (mapping individual tracks through websites) in order to understand usage in the digital environment and relate this to outcomes and impacts. (Nicholas et al., 2005) By applying this knowledge in the LAIRAH project, we are bringing quantitative and robust analysis techniques to digital resources in the Arts and Humanities. Methods Phase 1: Log analysis The first phase of the project is the deep log analysis. Transaction and search log files have been provided by the three online archives supported by AHRC: the AHDS Arts and Humanities Collection (http://www.ahds.ac.uk/); Humbul Humanities Hub (http://www.humbul.ac.uk/) and the Artifact database for the creative and performing arts (http://www.artifact.ac.uk/), (Humbul and Artifact merged at the end of 2005). This provides rich data for comparing metrics between subject and resource type. The search logs show patterns of which resources users are interested in, and in the case of the AHDS (which provides links through to resources themselves), which ones users go on to actually visit. This project is of limited duration, thus we are not able to analyse logs from individual projects at this stage of funding, given the difficulty of accessing log data from projects which may have limited technical support. We are analysing a minimum of a year’s worth of transaction log data (a record of web page use automatically collected by servers) from each resource. This data gives a relatively accurate picture of actual usage, is seamless, and is easily available, providing: user information on the words searched on (search logs), the pages viewed (user logs), the web site that the user has come from (referrer logs), and basic, but anonymous, user identification tags, time, and date stamps. (Huntington et al., 2002) We have also designed short online questionnaires, covering user characteristics and perceived outcomes, which will be matched to actual search and usage patterns. We are also sharing the results of these and the log data analysis with another project, also funded under the AHRC ICT scheme, (http://www.ahrbict.rdg.ac.uk/) the RePAH project (User Requirements analysis for Portals in the Arts and Humanities http://repah.dmu.ac.uk/), based at De Montfort and Sheffield universities. This project is studying the use of Portal sites in the arts and humanities, and is testing new prototype portal designs, including applications such as personalisation functions used on commercial portals, to determine whether they are appropriate for humanities users. As part of the initial phase of the project, we have also carried out a study to determine how humanities users find digital resources and portal sites, when beginning their search from their university library or faculty web pages. This study is described in a separate poster proposal. Phase 2: Case Studies Through the above log analysis, we have identified ten projects that have high and low patterns of use across different subject areas and types of content and these are being studied in depth. Project leaders and researchers are being interviewed about project development, aims, objectives, and their knowledge of subsequent usage. Each project is analysed according to its content, structure, and design and whether it has undertaken any outreach or publicity. We are seeking to discover whether projects have undertaken user surveys, and if so how they responded to them and whether they undertook any collaboration with similar projects. We are also asking about technical advice that the project received, whether from institutional support people, from Humanities Computing Centres or from central bodies like the AHDS. All these measures are intended to determine whether there are any characteristics which projects which continue to be used may share. For example does good technical advice predispose a project to be usable or might contact with potential users prove as important? We shall also interview a small sample of users of each resource about their opinions about the reasons why it is useful for their work. This aspect of the project will be collaborative with another CIRCAh project, which is studying the reaction of humanities users to digital projects: the UCIS project. We nevertheless recognise that it is also important to study projects that are neglected or underused. We are therefore running a workshop with the AHRC ICT Methods Network to study the possibility of the reuse of neglected resources. A small group of humanities users will be given an opportunity for hands on investigation of a small number of resources and there will be time for discussion of factors that might encourage or deter their future use. We will seek to find out whether their lack of use is simply because users had not heard of a given resource, are whether there are more fundamental problems of design or content that would make the resource unsuitable for academic work. Findings Collecting the log data has proved to be an unexpectedly difficult process. This in itself is noteworthy, since it indicates that levels of technical support even for large, government supported portals could still be increased. The situation for individual projects is likely to be even more problematic, and suggests that the issue of long term maintenance and support is one that institutions and funding councils must take more seriously. We are currently beginning to analyse data, and, by July, we will be in the final quarter of the project. We will therefore be able to report on the results of both the qualitative and quantitative aspects of the study. These should prove valuable to anyone at the conference who is currently running or planning to run a future digital resource for the humanities. Like all other matters in the humanities, building a digital resource which is successful in term of attracting and keeping users is not an exact science. We do not mean to limit the creativity of culture developers by suggesting the application of a rigid list of features to which all future projects must conform. Nevertheless, since resource creators spend such large amounts of precious time, effort and money on making their project a reality, they must surely be keen to see it used rather than forgotten. We aim to suggest factors which may predispose a resource to continued success, in terms of users: a topic of interest to project designers, project funders and users alike.",
       "article_title":"If You Build It Will They Come? The Lairah Study: Quantifying the Use Of Online Resources in the Arts and Humanities Through Statistical Analysis of User Log Data",
       "authors":[
          {
             "given":"Claire ",
             "family":"WARWICK",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Melissa ",
             "family":"TERRAS",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Paul ",
             "family":"HUNTINGTON",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Nikoleta ",
             "family":"PAPPA",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Introduction It has long been an article of faith in computing that when a resource, a program or code is being created, it ought to be documented. (Raskin, 2005) It is also an article of faith in humanities computing that the markup should be non-platform-specific (e.g. SGML or XML). One important reason for both practices is to make reuse of resources easier, especially when the user may have no knowledge of or access to the original resource creator. (Morrison et al, nd, chapter 4) However, our paper describes the problems that may emerge when such good practice is not followed. Through a case study of our experience on the UCIS project, we demonstrate why documentation, commenting code and the accurate use of SGML and XML markup are vital if there is to be realistic hope of reusing digital resources. Background to the Project The UCIS project (www.uclic.ucl.ac.uk/annb/DLUsability/UCIS) is studying the way that humanities researchers interact with digital library environments. We aim to find out how the contents and interface of such collections affect the way that humanities scholars use them, and what factors inhibit their use. (Warwick, et al., 2005) An early work-package of the project was to build a digital text collection for humanities users, delivered via the Greenstone digital library system. We chose to use texts from the Oxford Text Archive, (OTA) because this substantial collection is freely available and contains at least basic levels of XML markup. However this task was to prove unexpectedly difficult, for reasons that extend beyond the particular concerns of UCIS. Findings On examination of a sample of the files, we found that although they appeared to be in well formed XML, there were many inconsistencies in the markup. These inconsistencies often arise from the electronic history of the documents. The markup of older (Early and Middle) English texts is complex, and many of the problems stem from succeeding revisions to the underlying content. One common early standard was Cocoa markup, and many of the documents still contain Cocoa tags which meant that the files would not parse as XML. In Cocoa, the (human) encoder can provide tags that indicate parts of the original document, their form and clarity. These tags were retained in their original Cocoa form which was mistaken for potential TEI tags by the processing software. Many characters found in earlier English were encoded using idiosyncratic forms where modern (Unicode or SGML Entity) alternatives now exist. The earlier, Cocoa, form may render the modern electronic encoding unparsable in either XML or SGML. Another problem with Cocoa markup is that it was never fully standardised, and tags are often created or used idiosyncratically. (Lancashire, 1996) This complicates a number of potential technical solutions (e.g. the use of XML namespaces). Some content included unique tags such as “<Cynniges>”: not part of any acknowledged hybrid of the original standard. The nature of this is unclear. It may be an original part of the text, (words actually surrounded by ‘<’ and ‘>’), a Cocoa tag, or a TEI/SGML/XML tag. The distinction of forms known to a modern TEI/XML document is straightforward; the distinction between Cocoa and SGML/XML is not possible in this context. Even parts of the same document used the same tag inconsistently. For example, distances (e.g. “ten lines of space”) may be rendered in numeric form (‘10 lines’) or textual form ‘ten lines’) and distance units may be given in full or abbreviated. One common character notation was ‘&&’ to represent the ‘Thorn’ character (in upper case) and ‘&’ to represent the same character in lower case. This was interpreted as a SGML/XML entity, but parsers were unable to successfully interpret the original scheme. Furthermore, as the SGML/XML format was used in other parts of the document, even a bespoke parser could not successfully disambiguate the intention of every occurrence of the ‘&’ character. Thus, content is effectively lost. Other characters remain in forms such as ‘%’ for ‘&’ or ‘and’ –because of the original special use of ‘&’. Such characters thus remain unintelligible to an SGML or XML document reader. Given these complications, it is often impossible for a computer to determine the proper form of the document without human intervention, making automatic processing and indexation impossible. As Giordano (1995) argues, ‘No text encoded for electronic interpretation is identifiable or usable unless it is accompanied by documentation’. Yet in none of these cases did we find that markup decisions had been documented, nor was the code commented. The OTA supplied each file with a TEI header, which provides some basic metadata about its creation. However, the header was intended to act as the kind of metadata that aids in resource discovery, rather as code books were used to find a specific social science dataset on a magnetic tape. The <encodingdesc> element is not mandatory, and was intended to explicate transcription practices rather than detailed markup decisions. (Giordano, 1995) We certainly did not find any examples of attempts to elucidate markup schemes in the headers. Documentation was also not available for any of the files we looked at. Though the OTA strongly encourage depositors to document their work, they do not mention markup specifications as an element of basic documentation, so even documented files might not have provided the information we needed. (Popham, 1998). We were therefore forced to attempt to reconstruct decisions made from visual examination of each file. Despite the help of the OTA with cleaning up the data, the task proved so large that we had to abandon the use of these files. We have therefore used commercially produced resources, with the permission of Chadwyck Healey limited. The advantage of using their material for our project was that the markup is consistent, has been documented and conforms to written specifications. Conclusions It is to be hoped that simply by drawing attention to some of the problems that may occur in reuse, our work will cause resource creators to take seriously the importance of documentation and consistency. We have reported a case study of one UK-based repository, but since the OTA is one of the most reputable sources of good quality electronic text in the world, our findings should be of interest to the creators and users of other electronic texts well beyond this particular example. Not all electronic texts are of such high quality, nor are they always collected by an archive, and so such considerations become even more important when texts are made available by single institutions such as libraries, university departments or even individual scholars. One of the objectives of the Arts and Humanities Data Service (the organisation of which the OTA is a part) since its foundation has been to encourage the reuse of digital resources in humanities scholarship. Yet our experience has shown that the lack of consistency and documentation has made this task almost impossible. The advantage of markup schemes such as XML should be that data is easily portable and reusable irrespective of the platform within which it is used. Yet the idiosyncratic uses of markup that we found have almost negated this advantage. The creators of the resources probably thought only of their own needs as researchers and were happy with markup that made sense to them. It is still common for projects that use TEI to create their own extensions, without necessarily documenting them. Unlike computer scientists, whose collaborative research practices make them aware of the importance of adhering to standards and conventions that make their code comprehensible, humanities scholars are rewarded for originality, and tend to work alone. Research paradigms do not oblige scholars to think about how their work might be reused, their data tested, or their resource used to further research collaboration. One recommendation that follows from our work is that humanities scholars should at least take advice from, and ideally collaborate with, computer scientists or technical specialists, whose collaborative research practices make them aware of the importance of adhering to standards and conventions that make their code comprehensible. This might not matter if the creators of a resource are its only users, but given the intellectual and monetary cost of resource creation, their authors ought at least to be aware of the possible implications of applying idiosyncratic markup without comments or documentation. This paper provides the evidence of just such consequences.",
       "article_title":"Code, Comments and Consistency, a Case Study of the Problems of Reuse of Encoded Texts",
       "authors":[
          {
             "given":"Claire ",
             "family":"WARWICK",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"George ",
             "family":"BUCHANAN",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science,     University of Swansea",
                   "normalized_name":"Swansea University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/053fq8t95",
                      "GRID":"grid.4827.9"
                   }
                }
             ]
          },
          {
             "given":"Jeremy ",
             "family":"GOW",
             "affiliation":[
                {
                   "original_name":"UCL Interaction Centre,     University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Ann ",
             "family":"BLANDFORD",
             "affiliation":[
                {
                   "original_name":"UCL Interaction Centre,     University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Jon ",
             "family":"RIMMER",
             "affiliation":[
                {
                   "original_name":"School of Library, Archive and Information Studies, University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Giotto’s Fresco Giotto’s thirteenth-century fresco Exorcism of the Demons at Arezzo in the Church of San Francesco in Assisi is often referred to as marking the transition from the flattened medieval Byzantine ritualised image to the more spatially realistic perspectives of the Renaissance proper (Damisch, 1994; Edgerton, 1991; Gombrich, 2000; Kemp, 1990; Kubovy, 1989; Panofsky, 1991) (see figure 1). His achievements were recognised by his contemporaries such as Dante and Cennini, and his teacher Cimabue. He had a profound influence on Florentine painting in general and inspired the generation of artists that followed such as Masaccio and Michelangelo. In this, the tenth panel of a series of twenty eight frescos, we see an awkward (by modern standards) attempt to depict depth on a two-dimensional surface. His frescos attempted to illustrate the natural world with depth cues such as receding lines and chiaroscuro shading techniques. He also broke with the tradition of strictly depicting size relationships between people in a scene according to their hegemonic hierarchy. Instead, Giotto illustrated a spatial hierarchy between objects in a scene – including people. On the left we see the cathedral of San Donato (now the Diocesan Museum) with St Francis and Brother Sylvester attempting to drive out the demons aloft over the city, to the right of the fresco. The cathedral has been constructed with lines receding to the left suggesting distance. This is incongruous, however, with the city buildings to the right which have their diminishing lines marching to the right (see figure 2). Hence, as a complete composition, it does not portray the truly unified perspectival space we are more accustomed to that came later in the Renaissance. Nevertheless, in his clumsy way Giotto had established a sense of depth in his paintings which would have been just as profound to the uninitiated as any photograph we could produce of the scene today. Figure 1. Giotto’s thirteenth-century fresco Exorcism of the Demons at Arezzo in the Church of San Francesco in Assisi (KFKI, 2004). Was Giotto’s depiction of depth really that clumsy though? Perhaps by today’s mathematically precise algorithmic computer-generated perspectives it is. Or, perhaps Giotto was not attempting to depict a realistic scene, as much as later Renaissance paintings would, but simply hinting at the spatial arrangements of the city of Arezzo, the cathedral and surrounding countryside? The city of Arezzo depicted in Giotto’s fresco dates back to the sixth century BC. At the time, the city was situated on the top of the Donato Hill where we can now find the Prato Gardens and the fifteenth-century Medici Fortress. Between the Cathedral and the Fortress was a vast natural depression. The cavity has since been filled in to construct the Prato Gardens. Many of the original features are present in his fresco. The Three-Dimensionalisation of Giotto’s Fresco To determine how accurate, or rather what are the spatial interrelationships between the various buildings and landscape depicted in his fresco, the author of this paper embarked upon its three-dimensionalisation. In the spirit of the endeavour that Giotto himself took, I worked directly from an image of the fresco meticulously reconstructing each individual element in it. In an architectural sense, one would normally work from plans and project the three-dimensional volumes over them. In this case, there are no plans, so a different strategy had to be found. Working as Giotto himself may have done, the method adopted was based on one of simple proportions. A geometrical analysis of the image produced a grammar of sorts that appeared to be consistent across most of the city buildings. For example, there are simple geometric relationships between the window openings and the masonry that surrounds them (1:1, 1:2, 1:3, and so on). The next relationship is the relative heights. If we assume most of the main buildings were at most two storeys then, judging by their relative heights, four distinct ground levels emerge that these buildings are perched on. In the thirteenth century the extent of building technology typically only allows for a maximum of four storeys in domestic construction. Hence, the towers are typically twice the height of the average dwelling. From here a topology emerges consistent with the actual site in Arezzo today. Analysis of the Model The next stage was to assemble the city buildings. While at first, they just seem to be a dense agglomeration, by their three-dimensional modelling and organisation, and according to the picture analysis, an order emerges. This was revealed as much by the reconstructive modelling as it was by analysis of the picture itself. For example, clearly some buildings were in front of others, and others to the side of these. Hence, once the proportions were determined gaps could be detected between buildings (when viewed from above) that are not immediately obvious in Giotto’s original fresco. Indeed, an overall layout for the city (that part that was visible in Giotto’s fresco) includes what appears to be open spaces connected by access alleys between buildings. At one stage of the project a significant city square was revealed by the overall layout. However, when the different levels of the buildings were taken into account, this turned out to be simply a steep section of land that could not be built on (see figure 3). Figure 3. Bird’s-eye-view of the three-dimensionalisation of Giotto’s fresco. Note the open spaces and alleys between buildings. Other details were revealed through this procedure too. For example, all the towers house a bell. This, in conjunction with the airborne demons, the gesticulations of the priests, and citizens gathered at the city gates, suggests that Giotto’s depiction of so many bell towers had a purpose. The 3D model is an interactive spatial model that can be navigated in real-time. As such, within the multimedia reconceptualisation of his fresco it was possible to add the sounds of the bells. This adds a level of meaning to the fresco that is not apparent from the picture alone (see figure 4). Figure 4. Screen grab of interactive real-time navigable model of Giotto’s fresco three-dimensionalised. What was Revealed Together, the details derived from the analysis of Giotto’s fresco, the spatial characteristics of its subsequent three-dimensionalisation, and the addition of bell ringing provide another level of experience and understanding of Giotto’s work that he could not have anticipated. More than this, it exposes Giotto’s spatial reasoning to be more developed than many had thought previously. The fact that such an accurate reconstruction of his fresco can be produced suggests greater insights into spatial relationships between the objects in his fresco were present than previously reported. However, as much of the original city of Arezzo was dismantled in the fifteenth century to build the Medici fortress it is almost impossible to determine if the arrangement of buildings in Giotto’s city of Arezzo are a natural recording. Nevertheless, we do know that he was one of the new breed of naturalists actively seeking greater clarity in illustrating the world around him, therefore it is reasonable to assume that his pictorial depiction of the city is much more than a simple, stylised, ritualistic, scenography.",
       "article_title":"The Three - Dimensionalisation of Giotto’s 13th - Century Assisi Fresco: Exorcism of the Demons at Arezzo.",
       "authors":[
          {
             "given":"Theodor G. ",
             "family":"WYELD",
             "affiliation":[
                {
                   "original_name":"IEP, ITEE,     The University of Queensland, Aust.",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"Recent integration of advanced information technology and humanistic research has seen many interesting results that are brand new to traditional humanistic research. In the NORA project this integration was largely exemplified. In an effort to produce software for discovering, visualizing and exploring significant patterns across large collections of full-text humanities resources in digital libraries and collections, NORA project features the powerful D2K data mining toolkit developed by NCSA at University of Illinois, and the creative Tamarind preprocessing package developed by University of Georgia. In NORA project, D2K and Tamarind need to talk to each other, among other relevant components. The idea behind this connection is as follows: - Make use of existing efforts and try to avoid duplication. Natural language processing, such as part-of-speech tagging, word sense disambiguation, and bilingual dictionary creation, has long been recognized as an important technique for text data mining (Amstrong, 1994). D2K has proved to be an effective and comprehensive data mining toolkit, and Tamarind prepares data gleaned from large-scale full text archives. Getting them work together is an easy and time-saving way of achieving the goal of NORA. - Separate different tasks according to institution makes the multi-institutional project easier. D2K has been developed and used in several institutions within University of Illinois, and Tamarind was developed in University of Georgia for simplifying primary text analysis tasks. This separation keeps each institution focusing on a relatively independent module that they have the most experience with. - Prepare information about tokens once and for all. Natural language processing tasks prove time- consuming and computation-intensive. Separating these tasks from data mining part of this project obviates D2K toolkit from performing basic data analysis every time it runs, thus streamlining the whole process. The problem, however, is that D2K and Tamarind are developed using different programming languages and have different communication mechanisms. To put them together requires reconcilement and restructuring at both sides. This has eventually been achieved in a prototype application, where a collection of Emily Dickinson’s poems is classified as either “hot” (erotic) or “not hot” based on the language used (Kirschenbaum, 2006). As the size of data increases, the problem of scalability emerges. The huge size of many humanistic collections will make unrealistic the solution of storing all the tables in the database. A perfect method to address this problem has not been found, and content presented here demonstrates how we approach the text mining problem in the prototype when the size of collections is not very large. 2 The D2K Toolkit D2K - Data to Knowledge is a flexible data mining and machine learning system that integrates analytical data mining methods for prediction, discovery, and deviation detection, with information visualization tools (D2K). It provides a graphic-based environment where users with no knowledge in computation and programming can easily bring together software functional modules and make an itinerary, in which a unique data flow and a task are performed. These modules and the entire D2K environment are written in Java for maximum flexibility and portability. The data mining and machine learning techniques that have been implemented in D2K include association rule, Bayes rule, support vector machine, decision tree, etc. These techniques provide many possibilities of classifying collections available to this project, like hundreds of Emily Dickinson’s poems. Although D2K has the ability of performing basic natural language processing tasks, it is still beneficial to delegate those tasks to a toolkit that is specifically designed to do this, i.e., Tamarind. 3 Gate and Tamarind Both D2K and Tamarind use Gate as their fundamental natural language processing toolkit. Gate has been in development at the University of Sheffield since 1995 and has been used in a wide variety of research and development projects (Gate). Tamarind is a text mining preprocessing toolkit built on Gate, analyzing XML-based text collections and putting the results into database tables (Downie 2005). It serves as a bridge between Gate and D2K, and connects them through the use of persistent database. It supports JDBC-based data retrieval, as well as SOAP-based language-independent APIs. Table 1 shows a typical table in Tamarind database. The “xpath” field contains the location of a token in the TEI document in terms of XPath expression, “doc_id” is the unique ID of the TEI document, while “t_type_id” is the part-of-speech tag. Based on this table, some statistical characteristics of tokens, like term occurrence (term frequency), co- occurrence and document frequency, could be generated, thereby obviating the data mining toolkit (D2K) from performing the data-preparing task.After the whole collections is parsed and analyzed, the information related to the position, part of speech and type of each token is stored in a PostgreSQL database for future access. The Tamarind application exposes these information so that D2K as a client can connect and retrieve them through JDBC (Java Database Connectivity) or SOAP (Simple Object Access Protocol). 4 NORA Architecture Several issues were raised as to how to effectively and efficiently connect physically and institutionally distributed components in the NORA project. For example, should Tamarind expose its data to client through Java API (as a Java JAR file) or SOAP API (through Web service)? Should Tamarind just provide raw data like that in the previous table or something more advanced and complicated like the frequently used TF-IDF value? Is D2K responsible for converting the database table to a data structure more convenient for D2K to handle, like D2K table? How can the user requests be conveyed to D2K in a user-friendly and compact fashion? Experiments and discussion eventually led to the adoption of JDBC-based data retrieval and SOAP-based Web service for user request delivery. Although SOAP-based Web service providing more advanced and platform-independent API interface is a good choice for delivering Web-based requests, it seems inefficient to transmit large amount of data, like the occurrences of all tokens in the whole collection, through HTTP protocol, especially when the data store and the text mining application do not reside on the same host. This, however, does not exclude the possibility of implementing some not-so-data-intensive APIs, like metadata retrieval, through SOAP in the future. Table 2 gives sample data pairs pulled out of Tamarind database. It is a list of which token occurs in which document and is generated by a join of several tables in the Tamarind database. Table2 : Data (document-token pairs) from Tamarind Database After data about tokens is pulled out of the Tamarind database, it is converted to a structure called “D2K table” which is convenient for the D2K toolkit to handle. Actually the D2K table is the restructuring of the token-document pairs taken from the database as a matrix containing the occurrences of each token in each document. Table 3 gives an example. Depending on the collection, it could contain hundreds of rows and thousands of columns. This D2K table is ready for evaluation by several common machine learning techniques, like naive Bayes and support vector machine. For the Dickinson prototype, naive Bayes algorithm is used and an overall classification accuracy of over 70% is achieved. In the prototype, the D2K toolkit is launched by Infovis, an information visualization toolkit, through Web service.",
       "article_title":"Connecting Text Mining and Natural Language Processing in a Humanistic Context",
       "authors":[
          {
             "given":"Xin ",
             "family":"XIANG",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science, University of Illinois, Urbana, Champaign1 Introduction",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"John ",
             "family":"UNSWORTH",
             "affiliation":[
                {
                   "original_name":"Graduate School of Library and Information Science, University of Illinois, Urbana, Champaign1 Introduction",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"1. Introduction Over the past decade text mining techniques have been used for knowledge discovery in many domains, such as web documents, news articles, biomedical literature, etc. In the literary study domain, some data mining applications have emerged, among which document categorization may be the most successful example (Meunier 2005). But the overall progress of computer assisted literary study is not significant. The goal of this research is to discover more potential data mining applications for literary study. The basic belief underneath our research is that in order to better adapt data mining techniques to literary text, one has to grasp the unique characteristics of literary research and to leverage its uniqueness and its similarity with data mining. Buckland (Buckland, 1999) claimed that vocabulary is a central concept in information transition between domains. Comparing the vocabularies between the corpora in different domains may shed light on discovering the similarity and difference in the research activities between these domains. So we propose a 3-stage approach to map research activities between data miners and literary scholars as reflected in the vocabulary use in their research publications. Stage 1 is to investigate literary scholars’ unique research activities by verb analysis and topic analysis in critical literature, and see if any available data mining techniques can be applied to assist the scholars in these activities. Stage 2 is to investigate the mainstream data mining practices and the representations of the discovered knowledge by keyword analysis in data mining literature, and see if they also appear in critical literature setting. The shared research activities and knowledge representations will suggest some research problems on which data mining experts and literary scholars can start their collaboration. The two stages are complimentary to each other rather than sequential. In the last stage, potential literary text mining problems are summarized into a list of questions, and some literary scholars are interviewed to verify if these applications are useful and which of them can be specified to be ready for text mining. Up to date we have finished the first two stages. We will be interviewing 5-10 literary scholars between now and the conference. The results of the interviews will be included in our presentation at the conference. 2. Corpus Construction Three corpora have been constructed for the vocabulary use analysis in stage 1 and 2. The first is the data mining corpus (named “KDD”) which consists of 442 ACM SIGKDD conference paper abstracts from 2001 to 2005. The ACM SIGKDD conference has been the premier international conference on data mining. The paper titles and abstracts are extracted from the ACM Digital Portal. We do not use full text because it contains too many technical details that are not relevant to literary research. The second is the literary criticism corpus (named “MUSE”) which consists of 84 ELH Journal articles and 40 ALH articles downloaded from Project Muse, all on the subject of the 18th and 19th century British and American literature. The selection is based on the subject indexes assigned by the publisher. The plain text versions are generated by removing all the tags and quotations from the corresponding HTML versions. The third is the New York Times subset of American National Corpus (named “ANC-NYTIMES”) which consists of thousands of news articles with more than 2 million words. This “everyday English” corpus serves as a contrast group to test if the discovered similarities between the research behaviors in data mining and literary study are significant. 3. Stage 1: discovering literary scholars’ unique research activities This stage consists of three steps. Firstly, the plain text MUSE documents are part-of-speech tagged using GATE. Document frequency (DF) and term frequency (TF) serve as the basic indicators for a term’s popularity in a collection. Arbitrary DF is defined as the number of documents that contain the term. Normalized DF is defined as the percentage of the arbitrary DF in the collection (denote as “DF-pcnt”). Arbitrary TF is defined as the term’s total number of occurrences in the whole collection. Normalized TF is defined as the proportion per million words (denote as “TF-ppm”). The verbs are cascade sorted by their DF and TF. A literary scholar picked out some representative verbs (with both DF and TF between 5 and 10) in critical literature setting: “clarifies”, “cleared”, “Knowing”, “destabilizes”, “analyzing”, “annotated”, “juxtaposed”, “evaluated”, “recapitulates”, “merit”, “detail”, “portraying”, and “stemming”. Secondly, a unique MUSE verb list is generated by comparing the verbs in MUSE and ANC-NYTIMES, also cascade sorted by DF and TF. The top 10 unique verbs are “naturalizing”, “narrating”, “obviate”, “repudiate”, “Underlying”, “misreading”, “desiring”, “privileging”, “mediating”, and “totalizing”. Obviously the two verb lists do not overlap at all. Actually, the representative verbs (except “recapitulates”) picked out by the literary scholar turn out to be common in ANC-NYTIMES corpus too. After examining the unique MUSE verb list, two literary scholars were surprised to find many unexpected unique verbs, which means their uniqueness is beyond the scholars’ awareness. Thirdly, simple topic analysis shows that many MUSE essays are trying to build connections between writers, characters, concepts, and social and historic backgrounds. As an evidence, 56 out of 84 ELH essays and 24 out of 40 ALH essays titles contain “and” - one of the parallel structure indicator. But genre is the only topic that can be mapped directly to text mining application - document categorization. In conclusion, literary scholars are not explicitly aware of what are the unique research activities at the vocabulary-use level. They might be able to summarize their scholarly primitives as Unsworth did in (Unsworth, 2000), but does not help computer scientist to understand the data mining needs in literary criticism. 4. Stage 2: discovering the mainstream data mining activities and the representations of discovered knowledge in KDD and MUSE corpora This stage of analysis consists of two steps: 1) extracting keywords from KDD paper titles, identifying mainstream data mining activities and knowledge representations in data mining; and 2) comparing the DFs and TFs of the KDD keywords between KDD, MUSE, and ANC-NYTIMES corpora, identifying the keywords common in both KDD and MUSE but not in ANC-NYTIMES. In the first step, non-stop words are extracted and stemmed (using Porter Stemmer) from paper titles and sorted only by their TF. 18 out of 102 non-stop stemmed title words with TF>5 are identified as the representative data mining keywords. The left out terms include general terms (e.g. “approach”), technical terms (e.g. “bayesian”), terms about specific data (e.g. “gene”), and terms with different meaning in MUSE (e.g. “tree”). Table 1 compares the frequencies of the 18 words between MUSE and ANC-NYTIMES. It shows that 11 data mining keywords are common in literary essays but not in news articles. Figure 1 visualizes their significant differences in TF-ppm. The 11 keywords stand for models, frameworks, patterns, sequences, associations, hierarchies, classifications, relations, correlations, similarities, and spatial relations. It’s not surprising that none of these keywords can be found in MUSE essay titles. The context of the keywords extracted from KDD abstracts and MUSE full text also has little in common. In the left 7 KDD keywords, “rule”, “serial/seri” and “decis” are common in both corpora, “cluster” and “stream” are common in neither of them. Interestingly “network” and “graph(ic)” are much more common in ANC- NYTIMES. It seems literary scholars do not think much in graphic models. In conclusion, literary scholars are actually “data miners”, except that they look for different kinds of knowledge. For example, in terms of pattern discovery, literary scholars look for “narrative patterns”, “marriage patterns”, “patterns of plot”, etc. But data miners concern pattern in a more abstract manner - “sequential patterns”, “association patterns”, “topological patterns”, etc. 5. Stage 3: interview the literary scholars to verify the potential literary data mining applications In this stage we are going to interview 5-10 literary scholars to examine 1) how the scholars discover the kinds of knowledge identified in stage 2; 2) how to specify these kinds of knowledge so that computational algorithms can be designed to discover them for literary study purpose.",
       "article_title":"Toward Discovering Potential Data Mining Applications in Literary Criticism",
       "authors":[
          {
             "given":"Bei ",
             "family":"YU",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana - Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"John ",
             "family":"UNSWORTH",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana - Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":null,
       "date":"2006",
       "keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    }
 ]