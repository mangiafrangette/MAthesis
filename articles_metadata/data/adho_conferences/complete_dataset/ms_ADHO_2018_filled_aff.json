[
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The edition of the works of Leonhard Euler (1707-1783), entitled  Leonhardi Euleri Opera omnia (LEOO), is a monument of scholarship known to most historians of science. Leonhard Euler’s  Opera omnia consists of 81 volumes, 76 of which have already been published in paper format as four series of books. Volume IV, LEOO IV, of the fourth series contains the correspondence between Leonhard Euler and the German mathematician Christian Goldbach, encompassing 200 letters sent over 35 years (Martin Mattmüller, 2015). The aim of our project is to present this volume to researchers in science and history as a digital edition via the Bernoulli-Euler Online Platform, BEOL (Tobias Schweizer, 2017). BEOL is implemented using Knora (Benjamin Geer, 2017), a generic virtual research environment for the humanities. In this environment, scientists have access to all edited materials of LEOO IV, and can also annotate and edit material in their private workspace and share the results of their research with others. In Knora, the contents of the LEOO IV volume can be represented as a directed graph providing an overview of the network of different entities (letters, persons, bibliographic items, etc.). The tools provided in this environment are intended to facilitate research on the origin of ideas and findings.  Technical steps LEOO IV consists of two parts: one with transcriptions of the letters in the original languages (Latin and German), and another with English translations of the letters. LaTeX is used to edit both text and mathematical formulas. The volume also contains an index of persons, a bibliography of cited works by Euler, and a general bibliography. The project aims to import all this content into Knora, which represents data as RDF graphs using OWL ontologies (Pascal Hitzler, 2012). Therefore, ontologies are created to describe the structure of the texts and entities of this edition. The data itself must then be converted to XML and imported into Knora.   Specifying the structure of the data  The data model specifying the structure of the data to be imported must be given in the form of OWL ontologies.   A user interface for designing these ontologies is under development.  All bibliographical items, as well as persons in the name index of the edition, are represented internally as RDF triples. For example, every person is represented as an RDF resource belonging to the OWL class beol:Person, which has properties such as beol:hasFamilyName. The property beol:hasIAFIdentifier refers to the IAF/GND dataset maintained by German national library   Integrated Authority File, Deutsche National Bibliothek,   http://www.dnb.de/EN/Standardisierung/GND/gnd_node.html   , and ensures the uniqueness of each person mentioned in the BEOL platform.  Figure.1 illustrates a part of the generic bibliography ontology, which we have defined to describe all the bibliographical information needed in the BEOL platform (publication types, manuscripts, publishers, etc.). The prefix biblio refers to this ontology, beol refers to the ontology of BEOL-specific entities, and knora-base is the standard Knora ontology, which defines the basic data structures that Knora works with. Ellipses represent types or classes of resources, arrows semantically defined properties attached to them, and rectangles their literal values. In Knora, a text document (stored in a knora-base:TextValue) can contain markup as well as text. Internally, markup is stored separately from the text, using an RDF-based standoff format   Text with Standoff Markup,   http://www.knora.org/documentation/manual/rst/knora-ontologies/knora-base.html#text-with-standoff-markup   . A project such as BEOL defines a mapping between XML and Knora’s standoff/RDF markup; texts can then be imported from XML into standoff and exported from standoff back into identical XML   Creating a Custom Mapping,   http://www.knora.org/documentation/manual/rst/knora-api-server/api_v1/create-a-mapping.html#creating-a-custom-mapping   . Standoff/RDF markup can contain links to other resources, such as a person or a bibliographical entity mentioned in a text. The Knora API server ensures that the target of the link exists. Standoff links are directed statements, but can easily be queried as incoming links to a given resource.     Figure 1. OWL ontology for bibliographical data We have also defined a data model for letters and their metadata such as author, recipient, date, etc., which provides a network of the correspondence included in the edition. Figure.2 illustrates an excerpt from ontology of the whole LEOO IV project.    Figure 2. Excerpt from the LEOO IV project data model     Importing data into the BEOL platform  First, the index of persons and the bibliographical items of LEOO IV are written in XML format, using XML schemas that are automatically generated by the Knora API server, based on the ontologies defined for the project. This XML data is then validated against these schemas. After validation, the data can be imported in a single API request (an HTTP POST request to the Knora API server). Second, the text of the letters is imported using a similar process. Although the text has been transcribed in LaTeX, these transcriptions are first converted to XML to ensure the homogeneity of texts from different editions, and to make it possible to present texts as TEI/XML by applying XSL transformations. The LaTeXML tool (Miller, 2017), with the addition of some BEOL-specific Perl scripts, is used to convert LaTeX to XML. All references to persons and bibliographical items within the text of the letters are replaced with references to the corresponding resources in BEOL, making them queryable via the Knora API. The XML representing the letters is then imported using the same process as for the bibliographical data. Future work Since we have developed the methodology for this type of digital edition in a generic way, we expect to be able to integrate all the other recent volumes of Leonhard Euler’s  Opera omnia, which have also been edited using LaTeX. The older volumes in printed form should be scanned, their text should be recognized via OCR, and their structure should be defined with markup.       Figure 3. Original figure (left), detected labels (middle), reconstructed figure (right) Most of the older volumes contain figures that are reproduced from scanned letters. We are working on a machine learning algorithm to interpret these figures as well as their labels, so they can be automatically redrawn as vector graphics, see Figure. 3.   ",
       "article_title":"A Digital Edition of Leonhard Euler's Correspondence with Christian Goldbach",
       "authors":[
          {
             "given":"Sepideh",
             "family":"Alassi",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab, University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Tobias",
             "family":"Schweizer",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab, University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Mattmüller",
             "affiliation":[
                {
                   "original_name":"Bernoulli Euler Center, University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Lukas",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab, University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Helmut",
             "family":"Harbrecht",
             "affiliation":[
                {
                   "original_name":"Bernoulli Euler Center, University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "knowledge representation",
          "ontologies",
          "resource creation",
          "historical studies",
          "xml",
          "library & information science",
          "English",
          "and discovery",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction  While written works are often encountered by readers as linear phenomena, one of the most important conceptual advances offered by the Digital Humanities is the way that computational text analysis has permitted researchers to find non-linear patterns that speak to organizational principles embedded in even a single text. The methods developed to parse thousands, or millions, of texts can, in the context of a single work, reveal connections and patterns that are unavailable to a human reader.   Even in reference books, whose alphabetic order discourages the same kind of linearity found in novels, digital methods have proven effective at revealing alternative ordering principles. This has been particular important in eighteenth-century studies. For example, recent digital work on the French  Encyclopédie has sought to assess the compatibility of the multiple ways in which the text was organized by its authors. In their 2002 article, Gilles Blanchard and Mark Olsen measure the knowledge domains described by Diderot in his introduction by counting the number of renvois, or “see alsos” between articles in each domain. Similarly, Heuser, Algee-Hewitt and Bender have also reconstructed the French  Encyclopédie based on which articles are connected by renvois. In both cases, an alternative structure emerges: one that speaks to connections between domains of knowledge that are more meaningful than the alphabetic layout would suggest.  In this project, I employ a similar set of methodologies to explore the other foundational linguistic reference book of the eighteenth century, Samuel Johnson’s 1755  Dictionary of the English Language. While it lacks a system of renvois to counter-balance the prevailing alphabetic order it shares with Diderot’s work, it nevertheless contains a hidden system of connections between seemingly disparate articles, whose organization can only be revealed through quantitative analysis: the quotations used in the definitions of each word. These quotations are what separate Johnson’s dictionary from other, earlier dictionaries. In providing a contextual basis for assessing meaning, Johnson grounds definitions in historical usage and contingent situations. Yet, by Johnson’s own definition, the quotations have an  educational and  referential purpose that remains implicit within their use. And, by sheer volume, their presence is the most notable aspect of the dictionary. A given page of the 1775, second edition of the text, defines 17 words using 52 quotations. The typographical imbalance between the definitions and the quotations, which overwhelm the page, is striking, even while this is a fact that should come as no surprise to users of the OED, the spiritual successor to Johnson’s Dictionary.   This project, therefore, seeks to answer three questions. First, who is cited in what contexts in the  Dictionary? Here, a quantitative methodology should allow for unprecedented access to the fine-grained details of the text. Second, if Johnson’s  Dictionary were rearranged to group articles connected by shared quotations together, what patterns of relationship emerge? And finally, how does Johnson use his quotations to reflect back on the works that he cites?  Analysis  In order to answer these questions, this project begins with an html marked-up copy of Johnson’s 1755  Dictionary. Parsing this text using regular expressions, I was able to compile a table of entries for the 42,400 words identified in the second edition of the Dictionary and the accompanying 115,354 unique quotations. Following Blanchard and Olsen, the most straightforward visualization of the  Dictionary reordered by citations would be a network, where each word is a node, and each edge is a shared citation. In the  Dictionary, however, the sheer number of terms and quotations renders any naïve network graph unusably complex. Johnson’s own text, however, can be used to simplify the connections. For each word that he defines, Johnson also tags it as a part of speech (noun, verb, adjective and adverb). By reducing the articles to these parts of speech, and by consolidating the quotations within these groups, I am able to not only identify large-scale organizational patterns in the  Dictionary but also uncover the different patterns of citation that lie behind these structures.   For example, the works that Johnson cites most distinctively in his definitions of nouns are drawn from a variety of contemporaneous sources: from Dryden and Sidney, who are literary authors, to Boyle (a chemist) and Daniel (a historian) (Table 1).     Author Work(s) Number of Quotations   Dryden Dramatick Poesy; Virgil's Georgics; Annus Mirabilis 480   South Sermons 162   Hooker Sermons 134   Sidney Arcadia 85   Boyle Colours; Chymical Principles 75   Atterbury Sermons 64   Tillotson Sermons 54   Bentley Sermons 49   Taylor Rules for Holy Living; Guide to Devotions 42   Hammond Fundamentals; Practical Catechism 34   Broom Notes on Pope's Odyssey 22   Spratt Sermons 22   Daniel Civil War 19   Allestree Government of the Tongue 18    Table 1: Most distinctive authors in the definitions of nouns   By contrast, in his definitions of verbs, Johnson employs predominately Shakespearean tragedy (Macbeth, King Lear, Hamlet, etc.) as well as biblical sources (Genesis, Ecclesiasticus and Job) (Table 2). These patterns indicate that Johnson’s use of citation is not random: there is a clear logic to his choices of quotations for different parts of speech that reflects an implicit theory of the relationship between word meaning and knowledge creation. Here, science is the locus of objects, while tragedy is the locus of action. This not only reveals the ways in which the dictionary organizes language according to an implicit theory of textual meaning, but also how textual meaning is reflexively assigned by the  Dictionary.     Author Work(s) Number of Quotations   Shakespeare Macbeth; King Lear; Coriolanus; Othello; Hamlet; Henry IV 3716   Locke Education; Understanding 838   Decay Piety 146   Knolles History of the Turks 144   Genesis   102   Ecclesiasticus   99   Sidney Arcadia 84   Job   82   Rowe Jane Shore; Royal Convert; Ambitious Stepmother 75   Deuteronomy   67   Addison Cato; Ovid; Spectator 56   Acts   56   Maccabees   51   Ezekiel   50   Jeremiah   50   Psalms   50   Samson   49   Smith Phaedrus Hippolytus 48   Proverbs   48   Philips Briton 47    Table 2: Most distinctive authors in the definition of verbs  The parts of speech reveal an implicit structure to Johnson’s patterns of quotation. Their simplification of the text also allows me to visualize this structure as a meaningful network (Figure 1). In this graph, words are connected by shared authors; however the authors are limited to the group of most distinctive authors for each part of speech group (nouns, adjectives, verbs, adverbs). Similarly, the colors correspond to these groups as well: blue points are adverbs, orange points are adjectives; purple, nouns; and green, verbs. In addition to the four-quadrant structure that corresponds to the four parts of speech, there are other macro-scale phenomena in these graphs as well. Note how nouns are connected to adjectives (as are some verbs), but verbs and adverbs are the least like each other in shared quotations. More importantly, a comparison between this network, and a word embedding analysis of a contemporaneous corpus (the Eighteenth Century Collections Online corpus) using the GloVe algorithm reveals that the association Johnson draws between various terms using quotations are partially reflected in word associations through usage across the century, suggesting that Johnson both drew upon, and influenced, the language use of his time.     Figure 1: Network of Johnson's  Dictionary limited to authors distinctive of parts of speech  At a much smaller scale, the conceptual patterns that bind together definitions become more apparent. A group of verbs near the edge of the graph captures an overarching thematic set of concerns (Figure 2). “Despise,” “estrange,” “oppress” and “devour” suggest both an affective, as well as thematic unity. There is no reason for these groups to cluster together, unless Johnson sought specifically to create these conceptual groupings. In such cases, the association of words and texts is mutual: just as Shakespeare’s Macbeth provides an illustration for the act of despising, so too does the association of despise (or oppress, estrangement) with Macbeth color our reading of the play. As became apparent in the distinctive words that he uses for quotations, Johnson encodes interpretive theories into the apparently benign act of furnishing illustrative quotations.      Figure 2: Detail of \"negative affect\" cluster of verbs in part of speech network  Through a computational analysis, the underlying structure of the dictionary reveals the doubled work of lexicography. Quotations are both empirical proofs of contingent meaning, and, in turn, receive meaning themselves through accretion: slow layering of the conceptual unities between the words that they are used to define. The complexities of the dictionary reveal a set of organizing principles embedded in the quotations: this project allows us to extract these patterns and reveal their fundamental influence on the development of, not just lexicography, but language itself across that last three hundred years.  ",
       "article_title":"The Hidden Dictionary: Text Mining Eighteenth-Century Knowledge Networks",
       "authors":[
          {
             "given":"Mark Andrew",
             "family":"Algee-Hewitt",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "graphs",
          "linguistics",
          "lexicography",
          "networks",
          "historical studies",
          "text analysis",
          "relationships",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction Today, users of many modern minority and historic scripts in Unicode are not able to reliably send text electronically, because Unicode-enabled fonts and software are not available.    Especially true for scripts in Unicode versions 6.0 to 9.0 (2010 – 2016), where over 40% of the scripts have no fonts. (Unicode version 10.0 was released in June 2017, so support in fonts would not yet be expected). The Google Noto project aims to provide fonts for all approved scripts, but release of fonts is only up to fonts for Unicode version 6.2, released in 2012.   In addition, some communities have access to Unicode fonts, but the fonts aren’t used, because they do not provide features deemed necessary, such as positioning of characters (e.g., Egyptian Hieroglyphs  [Richmond and Glass, 2016]) or variant glyphs (e.g., Old Italic [Anderson, 2017]). Instead, images are used, which are not searchable or, alternatively, “hacked” fonts are employed, which require each person to have the same, non-standard font to send text. Keyboards or other input mechanisms are also not available for many of these same scripts. As a result, the promise that Unicode will “enable people around the world to use computers in any language” (Unicode Consortium, 2018a), does not yet ring true for some communities.   This short paper will highlight font-related problems with specific examples and will provide suggestions on how to address them. Problems   Creating a Unicode-enabled font for a language is often not a simple task, especially when the script for the language includes combining marks (which require correct positioning), or if the script has special rendering behavior, such as the consonant clusters found in South Asian scripts (Evans, 2017).    Font creation is made more challenging when typographic details on the script (and language) are not available. Since many recently approved scripts in Unicode are not well known, information on the typography is not readily available. Unfortunately, fine details are often not included in Unicode proposals for the scripts.    Interaction with the user community is critical in developing a suitable font, but some communities are difficult to contact. In addition, there can be differing views on the preferred shapes of glyphs. For a set of 51 Tamil numbers and fractions, for example, the community took 8 years to come to agreement on the preferred representative shapes. Specific cases will be cited, based on the author’s experience, including discussion of how to connect user communities with font providers.   Technical Issue: Glyph Variants  For some script users, access to glyph variants is important. This is true, for example, for the Old Italic Unicode block which unified several related alphabets of Italy, dating from approximately the 8 until 1c BCE. In Old Italic, the glyph in a particular alphabet may vary from that shown in the Unicode Standard.     The Old Italic block was encoded with the understanding that different fonts would be used for the different languages and alphabets (Unicode Consortium, 2017). How should the two forms of Faliscan (above) be handled in the same font then? How should a pan-Old Italic font handle the different alphabets (which use the same code points)?  This paper will describe the pros and cons of different options available, including use of:   Code points in Unicode’s Private Use Area (with the caveat that these code points would not be reliable for general interchange) (Unicode Consortium, 2018c). A Unicode variation sequence, when a distinction needs to be captured in plain-text (Unicode Consortium, 2018d). An OpenType font feature, such as character variants, stylistic alternates, stylistic sets, or localized forms (Microsoft Typography, 2018). Language-specific fonts (i.e., Faliscan1 and Faliscan2 fonts for the two forms above).  Suggested Solutions  Incorporate font creation as a part of the overall script encoding effort, such as: including a font item in the budget to pay for a font designer to develop a font; provide information on how to create a font for users; fund a font-creation workshop within the community.   Encourage user communities to submit a list of the basic repertoire of characters and auxiliary characters to the Common Locale Data Repository (Unicode Consortium, 2018b), since this information is used for by font and software developers worldwide. In addition, provide information on the shapes of the needed letters and variants, citing reference works (i.e., a book or website) on a publicly accessible webpage.   For handling glyph variants, short-term and long-terms approaches should be considered:  If a given variant is deemed by users to be necessary in plain-text, submit a Unicode proposal If OpenType features are used in a font, lobby software vendors to provide better support for the features in applications (as support for some features is still spotty [4]) For the short-term, PUA or separate fonts may be necessary.    For font designers:  Use language tags from ISO 639 (SIL International, 2017), BCP 47 (Phillips and Davis, 2009), and OpenType language/script tags (Microsoft Typography, 2017a; Microsoft Typography, 2017b) in the font internals. If a language (or script) is missing a tag, a new tag should be registered. According to Roozbeh Pournader, an expert at implementation of fonts, these tags are the way the fonts communicate with other software today.   Encourage users to review the glyphs in alpha versions of any forthcoming or any released Noto fonts, and submit comments to the Noto project (Google.com, n.d.).    Conclusion Access to a Unicode font is critical for users of lesser-used scripts, in order to participate more fully in the digital world. Unicode fonts make the user’s text interchangeable, discoverable, and able to be preserved for the long-term in a stable format. Recognition of font-related issues is a small step towards addressing the problem. Input from the audience will be encouraged in order to identify other potential approaches.   Funding  This work was supported by the National Endowment for the Humanities [grant number PR-253360-17].  ",
       "article_title":"Bridging the Divide: Supporting Minority and Historic Scripts in Fonts: Problems and Recommendations",
       "authors":[
          {
             "given":"Deborah",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"UC Berkeley, United States of America",
                   "normalized_name":"University of California, Berkeley",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an7q238",
                      "GRID":"grid.47840.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "globalization & digital divides",
          "linguistics",
          "indigenous studies",
          "resource creation",
          "library & information science",
          "English",
          "archives",
          "and discovery",
          "digitisation",
          "standards and interoperability"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Code and codework share many properties with text and writing, and code can be seen as an argument, corresponding to Galey and Ruecker’s (2010) understanding of the epistemological status of graphical user interfaces as argument. From an epistemic point of view, the practice of a programmer is no different from the practice of a scholar when it comes to writing (Van Zundert, 2016). Both are creating theories about existing epistemic objects (e.g. text and material artifacts, or data) by developing new epistemic objects (e.g. journal articles and critical editions, or code) to formulate and support these theories. However, as expressions of a technē whose inner workings are opaque to most humanities scholars, code and codework are all too often treated as an invisible hand, influencing humanities research in ways that are not transparent. The software used in research is treated as a black box in the sense of information science—expected to produce a certain output given a certain input—but at the same time often mistrusted precisely for this lack of transparency. The digital humanities (DH) does not generally engage with the code and coding parts of programming in an explicit and critical manner, which is necessary for opening up black boxes of code. The invisibility and un-critiqued use of code means that the scholarly quality and contribution of codework goes both uncredited and unaccounted for. Black-boxing the code results in neglect of its epistemological contributions and imperils one of the key components of knowledge production in the DH. Much more insight into code and codework in the humanities is needed, including how coders approach their tasks, what decisions go into its production, and how code interacts with its environment.  The purpose of this paper is to provide some of those insights in the form of an ethnography of codework, wherein we observe the decisions that programmers make and how they understand their own activities. Our study follows in the footsteps of ethnographies of technoscientific practice (see: Forsythe, 2001; Coleman, 2013), Critical Code Studies (see: Marino, 2014), and reflections on coding and tool development in the DH (see: Schreibman and Hanlon, 2010; Ramsey and Rockwell, 2012). The study does not aspire to be representative of the DH coding practice, but to initiate a debate about some still overlooked elements of that practice. This exploration applies Latour’s (1998) first rule of method to the context of narrative creation through codework, looking at the practices, dilemmas, and decisions of programmers. To do that, we use analytical autoethnography (cf. Anderson, 2006), combined with collaborative ethnography (cf. Lassiter, 2005). In our methodological design, the team ethnographer first formulated a set of ten questions aimed at generating reflexive accounts and examples of DH coding in the making. Each of the team DH programmers then individually answered the questions in a written form, providing elaborate, semi-formal accounts of his or her DH programming practice. Thus generated written accounts became the basis for a series of team discussion, both written and oral, which eventually formed the results of this contribution. This methodological design enabled us to return from the final outputs of DH coding to scholarly uncertainties and resolutions that preceded them. Through such reconstruction, we were able to document some of the key phases in epistemological construction of coding artifacts, and to identify methodologically significant moments in stabilization of those artifacts. In other words, we relied on the experiences of scholars proficient in both humanities research and coding seeking to make explicit what DH coders themselves know, maybe tacitly, about why and how they code.  We have grouped our observations into the categories known as the five canons of rhetoric, proposed in Cicero’s  De Inventione. Although originally developed for public speaking, these canons have proven to be equally potent heuristic in analyzing written and, more recently, digital discourse (Gurak and Antonijevic, 2009). Our contribution sought to extend this heuristic to the analysis of coding as argumentation, not in an attempt to fit codework and its elements into a pre-defined ontology, nor to suggest that it fully conforms or matches classical rhetoric. Rather, it was a way of presenting our experiences and claims in a form that we expected to facilitate interpretation by scholars well versed in text production but likely less so in codework.  Our study showed that codework reflects humanistic discovery ( inventio) in that humanities- specific research questions drive coding, and tasks specific to the humanities research motivate software development. Similarly, crafting and organizing code resonates with development and arrangement of a scholarly argument ( dispositio)—a programmer writes lines of code and makes many decisions on how to arrange these pieces into larger, meaningful constructs that influence the epistemological and methodological structure of research. Our study also illustrated that, like any humanities scholar, an author of software has her own style ( elocutio) in the aesthetics of code and in her way of working to create code, and this style develops through both individual and norms of coding communities. We also showed that, parallel to books or libraries, code and codework serve as memory systems ( memoria) that embed theoretical concepts in order to augment research methodology and create new theory, where code can be regarded as a performative application or explanation of theory. Finally, our ethnography illustrated how codework  actio compares to the publication and reception of the software, where DH programming is still not recognized as a locus of humanities expertise, and it is hard for humanities programmers to have their code academically evaluated as digital output.  The insights of our study demonstrate that both code as an epistemic object and coding as an epistemic practice increasingly shape research in the humanities and must be given a proper theoretical and methodological recognition in the DH, with both the consequences and the rewards that such a recognition bears. Therefore, a strategy for making code and codework visible, understandable, trustworthy and reputable within humanities scholarship is needed. Such a strategy should be comprehensive, both in the sense of accounting for the source code and the executed result of software. While we agree with Ramsay and Rockwell (2012) that providing source code is not sufficient for understanding the underlying theoretical assumptions, we disagree in viewing the “dependence on discourse” as a feature that relativises epistemic and communicative capacities of code and codework. We argue in contrast that interdependence of code and text should be embraced as a means of acknowledging their distinctive yet corresponding methods of knowledge production and communication. Just as code enhances text making it amenable to methodological and epistemological approaches of DH, text enhances code making it more visible and intelligible for the humanities community. Evaluating code and DH programming in a disengaged way would thus be similar to the literary criticism enacted on a novel without reading it. Yet currently it is practice to “criticize” software and code based only on a journal article that derived from it. As much as possible, coders should support the involved evaluation of code as opposed to its disengaged criticism. We believe that theoretical discussions of codework should become an established trajectory in the humanities, along with the development of methods for documenting, analyzing, and evaluating code and codework.  One important element of that strategy is understanding codework as necessarily shaped by its social context, which influences the attitude and perception that both coders and other scholars hold towards their work. Too often, DH programmers are treated as service instead of research focused scholars, which results in a number of negative consequences. A necessary step in the direction of a real change in how codework is received into the humanities is recognition and reward for peer-reviewed digital outputs, including code, as research outputs (cf. Nowviskie, 2011; Presner, 2012; American Historical Association, 2015). A precondition for this is to start grassroots procedures for peer review of code (Fitzpatrick, 2011), and to regard the code as alternative expressions of research or epistemologies with equal research value and validity instead of subordinating code and codework to ‘humanities proper’ (cf. Burgess and Hamming, 2011 and Ramsay and Rockwell, 2012). There is a need for peer review and critical examination of actual code, which is hardly even present in DH (Zundert and Haentjens Dekker, 2017). Also, open publishing of code in verifiable ways can be easily facilitated through existing public code repositories or institutionally-run versions of the same repositories, but it is not common practice throughout the humanities to publish code. Finally, reflexive accounts on (digital) humanities codework and ethnographic studies of actual work can help us understand how code and codework are changing the humanities (Borgman, 2009). We believe that an important step in illuminating the process and results of DH programmers’ codework is to develop and explicate reflexive insights into its key epistemological, methodological, and technical aspects. Explaining, for instance, what kind of research questions give impetus to one’s codework and how new research insights co-evolve during code development can help both DH programmers and their traditionally trained colleagues recognize the important epistemological connections between humanistic theory and scholarly programming. ",
       "article_title":" Unwrapping Codework: Towards an Ethnography of Coding in the Humanities  ",
       "authors":[
          {
             "given":"Smiljana",
             "family":"Antonijevic Ubois",
             "affiliation":[
                {
                   "original_name":"The Pennsylvania State University",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          },
          {
             "given":"Joris",
             "family":"van Zundert",
             "affiliation":[
                {
                   "original_name":"Royal Netherlands Academy of Arts and Sciences",
                   "normalized_name":"Royal Netherlands Academy of Arts and Sciences",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/043c0p156",
                      "GRID":"grid.418101.d"
                   }
                }
             ]
          },
          {
             "given":"Tara",
             "family":"Andrews",
             "affiliation":[
                {
                   "original_name":"University of Vienna",
                   "normalized_name":"University of Vienna",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/03prydq77",
                      "GRID":"grid.10420.37"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "computer science",
          "rhetorical studies",
          "digital humanities history",
          "epistemologies",
          "software design and development",
          "theory",
          "epistemology",
          "anthropology",
          "English",
          "software studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Automatic transcription of handwritten texts has made important progress in the recent years (Sanchez et al., 2014; Sanchez et al., 2015, Sanchez et al., 2016). This increase in performance, essentially due to new architectures combining convolutional neural networks with recurrent neutral networks, opens new avenues for searching in large databases of archival and library records. This paper reports on our recent progress in making million digitized Venetian documents searchable, focusing on a first subset of 18th century fiscal documents from the Venetian State Archives (Condizione di Decima, Quaderni dei Trasporti, Catastici). For this study, about 23'000 image segments containing 55'000 Venetian names of persons and places were manually transcribed by archivists, trained to read such kind of handwritten script, during an annotation phase that lasted 2 years. This annotated dataset was used to train and test a deep learning architecture, with the objective of making the entire set of more than 2 million pages searchable. As described in the following paragraphs, performance levels (about 10% character error rate) are satisfactory for search use cases, which demonstrates that such kinds of approaches are viable at least for this typology of handwritten scripts. This paper compares this level of reading performance with the reading capabilities of Italian-speaking transcribers, preselected with a test based on 100 transcriptions. More than 8500 new human transcriptions were produced, confirming that the amateur transcribers were not as good as the expert. However, on average, the machine outperforms the amateur transcribers in this transcription tasks.   Machine performance We developed a transcription system based on the combination of convolutional and recurrent neural networks as described in  (Shi et al., 2017) for handwritten text (Fig.1a) (The code is implemented in python and is available at  https://github.com/solivr/tf-crnn). On the one hand, convolutional neural networks (CNN) capture hierarchical spatial information, with the first layers capturing low level features and later ones capturing high level ones. On the other hand, recurrent neural networks (RNN) capture temporal data, with the ability to grab contextual information within a sequence of arbitrary length. Convolutional recurrent neural networks (CRNN) combine the best of both worlds to handle multi-dimensional data as sequences.  From an input image, the convolutional layers extract a sequence of compact representations which corresponds to the columns of the feature map. They are processed from the left to the right of the image to form a sequence of local image descriptors (Fig.1b).    Fig 1 (a) Network architecture. The architecture consists of three parts: 1) convolutional layers, which extract a feature sequence from the input image; 2) recurrent layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence.  (Shi et al., 2017) The sequence is then input to the recurrent layers which consist of stacked bidirectional long short-term memory (LSTM) cells  (Hochreiter et al., 1997). LSTM cells have the ability to capture long-range dependencies but are directional, and thus only use past contexts. Since in image-based sequences context from both directions are useful and complementary, one forward and one backward LSTM cells are combined to form bidirectional LSTMs which are then stacked to have several recurrent layers. The recurrent network outputs per-frame predictions (probabilities) that need to be converted into a label sequence.  In the transcription layer, the connectionist temporal classification (CTC) (Graves et al., 2006) is used in order to obtain the “sequence with the highest probability conditioned on the per-frame predictions\". The sequence label is found by taking the most probable label at each time step and mapping the separated labels to the correct sequence label (see (Graves et al., 2006) to have the detailed explanation on how the repeated and 'blanks' labels are dealt with). The CRNN was trained on data coming from various types of Venetian handwritten documents. The dataset is composed of image segments of mainly names and places that have been transcribed by archivists in Venice. Image segments are used in order to reflect only the performance of the transcriber system, without introducing possible errors from the segmentation process. Thus, the segmentation step is not part of the proposed experiment. The set was randomly split into training and testing set and the content of the image segments ranges from one to several words (Tab.1).   Set # images segments # total words size of vocabulary         Training set 20712 48628 8848   Testing set 2317 5559 2157         Full set 23029 54187 9429         Table 1: Datasets used We show in Fig.2a and  3a how words are distributed in the dataset. We de ne the vocabulary to be the set of different words. The impact factor IF is a measure of the words' distribution in the dataset and is defined as IF (i) =  c( n i) hist(i; c), with c the vector of counts of each vocabulary word, n the total number of words,  hist the histogram operation and  hist(i; c) the number of vocabulary words that occur i times. The left part of these plots shows that most of the words do not appear commonly but a few are very present in the dataset as it can be seen on the right of the figures (those are mainly prepositions such as `di', `de', `in', etc). The cumulative sums (Fig.2b and Fig.3b) show that common words have limited impact, but also that the system does not suffer from overfitting to the vocabulary since most of the words used for training are 'rare' in the dataset.     (a)    (b) Figure 2: Word distribution and impact factor in full dataset. We observe that 70% of the dataset is represented by words appearing less than 250 times (out of 54187 words) To evaluate the performance of the system we use the Character Error Rate (CER) measure on the test set defined as CER = (i + s + d)=n with i, s, d, n the number of character insertions, substitutions, deletions and total characters respectively. The numerical results are shown in Tab.  2. Several experiments were performed using different sets of characters (called 'Alphabet' hereafter) and resulted in one model per Alphabet. A few randomly selected examples can be seen in Appendix  A.  On this dataset, our transcription system is below 10% CER, which is sufficiently good to be able to search for entities in documents using regular expressions and fuzzy matching. Moreover, we believe this performance is better than the human average and in order to verify our hypothesis, we conducted an experiment described in the following section.    (a)    (b) Figure 3: Word distribution and impact factor in the testing dataset   Alphabet Set of characters # image segments CER           Capital-lowercase-symbols A-Za-z'.,: -= 24035 0.089   Capitals-lowercase-digits-symbols A-Za-z0-9'.,:; -  =()[]/ 96198 0.045   Digits 0-9   72326 0.013           Table 2: The Character Error Rate (CER) for each Alphabet   Human performance In order to quantify the human average error rates on our dataset, we conducted an experiment on Crowdflower's platform, where Italian speaking persons were paid to transcribe image segments of the testing set (see examples in App.  A). The contributors had to decipher a few units before being able to start the survey and during the experiment some of their transcriptions were evaluated. There were 103 evaluation questions that allowed to separate low accuracy contributors' answers from reliable ones. Each image segment was transcribed at least three times, and in total 11'727 units were transcribed. Only the answers of contributors maintaining at least 60% accuracy throughout the experiment and who transcribed at least 50 units were taken into account for the analysis. This resulted in a total of 8'674 valid transcriptions to analyze. The number of transcriptions (judgments) per contributor and its location can be seen in Fig.6.  We compare the performance of the system and the amateur transcribers in Tab.3 and  Fig.4,5 (onesample t-test, p < 0:005). It is clear from the graphs that the CRNN system has a better CER and WER than the human average on this dataset, and only a few contributors have lower or comparable performance to the system but is not yet as good as the expert. It is interesting to notice that the performance of the best amateur transcriber almost doubles when capital letters and punctuation are not considered (case 3) whereas the CRNN makes little improvement. Indeed, although the system has inferred some sort of weak language model, we have seen it producing unlikely transcriptions whereas the best contributor uses its knowledge of Italian proper nouns to deduce the correct transcription when some characters are di cult to read. Thus, the system's CER and WER could be reduced by using a lexicon-based transcription, where the output of the neural network would be compared to a dictionary and the closest element would be chosen.    Case   CER  WER    CRNN  contributors  CRNN  contributors                                   0 : No modifications (Fig.4a) 0.0804  0.1328  -  -   1 : Capital letters replaced by lowercase (Fig.4b) 0.0768  0.1137  -  -   2 : All punctuation removed (Fig.4c,  5a)  0.0766  0.1241  0.2709  0.4318   3 : Combination of Case 1 and Case 2 (Fig.4d,  5b)  0.0718  0.1047  0.2551  0.3507              Table 3: Comparison of Character Error Rates (CER) and Word Error Rates (WER) considering different formatting cases of the transcriptions for our system and the mean of the contributors (ground-truth and predictions are formatted in the same way)    Figure 4: Character Error Rate per contributor for different cases (refer to Tab.3).     (a) (b) Figure 5: Word Error Rate per contributor for different cases (refer to Tab.3).    Figure 6: Number of judgements made (image segments transcriptions) by each contributor and its location. The contributors' ordering is the same as Fig.4a (by increasing CER)   Perspectives The developed system shows promising results to make possible the textual search on digitized handwritten documents. These results open up new prospects for massive indexing, analyze and study of historical documents. We showed that the system had lower Character and Word Error Rates than the human average, thus being sufficiently reliable to use for searching purposes. Further work will focus on improving the architecture of the model, especially the CNN. We will also explore the possibility of lexicon- or rule-based transcription to decrease error rates. More generally, it seems that the automatic transcription is currently passing a threshold in terms of performance, now giving better results than good amateur transcribers. Future research will show how far this level of performance depends on the expert initial training set or whether, after some exposition with dozens of different scripts, the automatic transcriber may be able to generalize by itself without further specific training.   Appendix A : Transcription examples     ",
       "article_title":"Comparing human and machine performances in transcribing 18th century handwritten Venetian script",
       "authors":[
          {
             "given":"Sofia",
             "family":"Ares Oliveira",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frederic",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "computer science",
          "artificial intelligence and machine learning",
          "resource creation",
          "image processing",
          "English",
          "and discovery",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Digital has transformed the way to produce, transmit and share knowledge. The increasingly widespread diffusion of digital methods and techniques in all the social and cultural levels of the communities, in fact, brings an unheard democratization of knowledge and culture, making the citizen a privileged and intelligent actor in the sustainable development of the new smart societies which are based on the process of digitization, digital co-creation and digital design. The  art. 2 of the UE “ Council conclusions of 21 May 2014 on cultural heritage as a strategic resource for a sustainable Europe” (2014/C 183/08) states: “Cultural heritage consists of the resources inherited from the past in all forms and aspects - tangible, intangible and digital (born digital and digitized), including monuments, sites, landscapes, skills, practices, knowledge and expressions of human creativity, as well as collections conserved and managed by public and private bodies such as museums, libraries and archives. It originates from the interaction between people and places through time and it is constantly evolving. These resources are of great value to society from a cultural, environmental, social and economic point of view and thus their sustainable management constitutes a strategic choice for the 21st century”.  It is therefore inevitable to rethink digital and digitization as social and cultural expressions of the contemporary age. This implies the need to rethink data as cultural entities and no longer as mere tools for simplifying administration management, or as extemporary surrogates for enhancing the fruition of tangible and intangible cultural heritage. The current process for archiving and storing data, although they generate from the awareness of the need to preserve them, don’t solve the problem of their both current and historical reuse, because they are still strongly conditioned by the instrumental function that presides over their production and use.   Towards a first classification of Digital Culture This paper aims to provide a new definition of methodological and technological approach to digital and digitization, with the goal to guarantee data stability, sustainability, usability and reusability so as to foster their long term preservation. The research originates from observing that, in the human evolution, the survival, preservation and permanence over time of any entity has always been strictly linked to its identification as cultural heritage, because of its value of historical witness which conveys knowledge. For several years, authoritative scientific voices have highlighted how long term digital preservation is the real emergency to be faced worldwide. In 2015, Vinton Cerf raised the alarm about the risk that the Twenty-First Century will become for posterity the first black hole in human evolution since the establishment of intelligent communication. The alarm resumed what was debated in the 2012 UNESCO Conference held in Vancouver with the significant title “The Memory of the World in the Digital Age: Digitization and Preservation”. In order to start a serious and shared process for cultural identification of digital and digitization, it is therefore essential to recognize data as cultural entities, defining a clear and regulated position in the contemporary cultural scene. In fact, several existing digital entities could be considered contemporary  Digital Cultural Heritage (DCH), expression of the  Digital Culture of the Twenty-First Century smart societies.  A first useful identification could come out from a classification of  digital cultural entities, which can be traced back to the following three basic categories in which the Digital Culture could be declined:  -  Digital FOR Cultural Heritage: process, methods and techniques aimed at co-creation of digital artifacts reproducing in their contents tangible and intangible cultural heritage: e.g., digital objects, digital libraries, virtual museums, demo-etno-anthropological databases.  -  Digital AS Cultural Heritage: approach, process, methods and techniques aimed at recognizing and preserving both digital artifacts reproducing intangible and tangible cultural heritage, and dematerialization as expression of contemporary cultural  facies to be known, safeguarded, preserved and transferred in time as witness and memory of the current  Digital Age.  -  Born Digital Heritage: process, methods and techniques aimed at co-creating and managing digital entities that record the current activities of contemporary communities, to be safeguarded, preserved and transferred to future generations as witness and memory of Twenty-First Century culture and societies.    Digital Culture as identity of contemporary age According to the above classification, Digital Culture could therefore be defined as implementation of integrated cultural and training approaches, processes, methods, and techniques aimed at co-creating an ecosystem of aware digital knowledge. This, in fact, will be enabled to trigger processes for the construction of networks to safeguard, preserve, sustain, transfer, reuse  DCH through awareness of its identity as historical memory of the contemporary age and, therefore, as source of knowledge for future generations.  So, starting from the analysis and co-design of a digital entity, whatever it is – one digital artifact, a digital library, a management system for Public Administrations or an app for Augmented Reality –, the focus on preservation is primary to define it a digital cultural entity. It will determine and regulate both the co-creation process, and the methodological and technological approaches, systems, information, metadata schema, digital image content structures, data description, complex data set, and their any further development and sustainability. This approach can only exist in an ecosystem of aware digital culture, in which digital and digitization with their processes are recognized as DCH. In this regard, our opinion is that what differentiates DCH from the non-cultural digital artifacts are the descriptive metadata for indexing digital object. Above all, it is primary the correct proportion between: -  quantity: it is the correct ratio among exhaustiveness of information, knowledge to be provided, number of metadata elements and attributes necessary to retrieve, reuse and store it;  -  quality: it is the correct ratio among choice of the informative and cognitive level to be given both to each descriptor and to set of descriptors, and the variables of information and cognitive need of the users, according to whether they are current or future.    Descriptive metadata as sources of Digital Cultural Heritage The issue is addressed with regard to the preservation of  Digital AS Cultural Heritage. The case study object of the research is the metadata schema co-created for the digitization project \"Historical Archive of the G. Laterza & Figli Publishing House\", undertaken at the end of 2015 and today publishing in the Puglia Digital Library of the Puglia Region.  The metadata schema used for managing and indexing the digital artifacts scanned from the original documents has been co-created with reference to the Italian national METS-SAN standard structured by the National Archival System. The preservation of both the process of digital co-creation and of the digital resources themselves has been the focus of the project. So, attention has been focused on descriptive metadata both of the project as a whole, and of each section of the original Archive (series, sub-series, etc.), and of each one digital artifact. The starting point was the awareness that, at the state of the art, the images present great difficulty for long term digital preservation. The planning and structuring of the metadata schema has therefore been focused not only on the needs of contemporary users, but above all on the cognitive and informative needs of future users about our contemporary culture. So, metadata will be the only sources of knowledge on both the digital artifacts we produce today, and the processes by which we co-create them. We preferred to use \"granular\" indexing, describing each digital document with its metadata. In structuring the metadata schema, we considered the tag sequence as an organic structure composed of forms entities (elements and attributes) and descriptive information. The narrative contents have been articulated hybridizing methods and techniques of archival description with cataloguing solutions, and they have been written with stylistic criteria deduced from the storytelling methodology, providing information on both the whole project and the detail of each section and, inside the sections, of each partition. In each metadata, the <header> section, after the namespaces (<xlmns: --->) enbeds the descriptors related to: - project: body responsible for the project, owner of original Archive, editor of digital resources; - history of the original Archive; - structure of the original Archive; - historical/biographical profile of the owner of the original Archive; - rights that regulate the use of original documents. The <desc> section has been divided into two sub-sections: 1. context: it enbeds the data relating to entities involved in the ownership and management of the original documents; 2. description: it describes the consistency of the sub-fund to which the resource described in the sub-section <File> belongs. The <File> section dedicated to single document describes: - the original document represented in the image: subject, text abstract, creator, contributors, chronic date, topical date, support, language; - the physical position of the original in the Archive; - the editor who creates the descriptions. The section on rights follows, which describes: - ownership of the digital artifact; - accessibility and reuse of the digital artifact; - ownership and accessibility of the original document. The schema closes with the technical metadata describing the different image formats in which each digital objects relating to the respective pages of a document have been reproduced, with their structural components.  Conclusion Starting from the art. 2 of the UE “Council conclusions of 21 May 2014 on cultural heritage as a strategic resource for a sustainable Europe” (2014/C 183/08), the paper focuses on the need to rethink digital and digitization process for long term digital preservation, aiming to redefine them as the new Cultural Heritage of the contemporary era. This new way to observe digital artifacts and their co-creation process is the indispensable prerequisite for co-creating aware Digital Culture and for giving due importance to digitization and dematerialisation, whose process, from the planning stages, need an approach focused on data preservation and, to this goal, on the decisive role that the descriptive metadata play. The case study was the digitization project of the “Historical Archive of the Giuseppe Laterza & Figli Publishing House”. In particular, the attention to preservation focused on structuring the schema of metadata and, above all, on descriptive writing, with regard to the choice of tags, elements and attributes, and to draft descriptive information of each digital artefact. In fact, our opinion is that they constitute the main source for the knowledge of both the single digital artifact, and the full project and its evolution, thus configuring itself as fundamental elements to validate and certify the data, guaranteeing quality, authenticity and sustainability as witness and memory of the contemporary Digital Age, with the aim to increase the knowledge of future generations about Twenty-First Century.  ",
       "article_title":"Dal Digital Cultural Heritage alla Digital Culture. Evoluzioni nelle Digital Humanities",
       "authors":[
          {
             "given":"Nicola",
             "family":"Barbuti",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Studi Umanistici DISUM - Università degli Studi di Bari Aldo Moro, Italy",
                   "normalized_name":"University of Bari Aldo Moro",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/027ynra39",
                      "GRID":"grid.7644.1"
                   }
                }
             ]
          },
          {
             "given":"Ludovica",
             "family":"Marinucci",
             "affiliation":[
                {
                   "original_name":"Scuola a Rete per la Formazione nel Digital Cultural Heritage, Arts and Humanities - DiCultHer",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "computer science",
          "repositories",
          "sustainability and preservation",
          "digitisation - theory and practice",
          "Italian",
          "resource creation",
          "cultural studies",
          "and discovery",
          "library & information science",
          "metadata",
          "archives",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Theatre studies is a largely under-discussed topic in digital humanities research projects. It's lagging behind the first wave of digital humanities scholarship, «  focus[ing] on large-scale digitization projects and the establishment of technological infrastructure » (Presner, 2010). Theatre studies remains on the fringe of a growing phenomenon : culture analytics. In the context of big and complex datasets, culture analytics « is the data-driven analysis of culture » (IPAM, 2016). I suggest the expression « theatre analytics » (Bardiot, 2017). To paraphrase the culture analytics definition, theatre analytics is the data-driven analysis of theatre, whether it concerns theatre history (Caplan, 2016), drama or mise-en-scène. To understand what quantitative methodologies can bring to the knowledge of theatre, I propose a case study of Merce Cunningham. What can we learn about Merce Cunningham, one of the most influential  choreographers of the 20th century, thanks to theatre analytics ? A leader of the American avant-garde throughout his seventy year career from 1938 to 2009, he establishes in 2000, in the twilight of his career, the Merce Cunningham Trust, in order to preserve the integrity of his work. At the same time, he decides to dissolve the Merce Cunningham Dance Company (MCDC) two years after his death and a legacy tour. This is an unprecedented initiative. On one hand, it demonstrates exceptional effort and dedication to document the works. On the other hand, it challenges the ephemeral nature of performing arts : 86 out of 183 choreographies are documented with “digital Dance Capsules” “so that it may be performed in perpetuity”(Dance Capsules, n.d.). By the way, two groups of works are defined : the canon (key works with extensive documentation in order to perform them again and again) ; the auxiliary (minor works with no documentation available to the public and  de facto impossible to replay).  The data was collected from the Merce Cunningham Trust website. It concerns theatre production and cast, Dances Capsules documentation and the history of the MCDC. The dataset contains 183 works from 1938 to 2009 (including 86 Dance Capsules) and 347 people. We can identify three main data categories : people, works and documentation. What can we infer from beyond the data about the MCDC history, Cunningham's aesthetics and documentation strategies ? Measuring means measuring instruments. I used various and complementary tools in order to vary the approaches and analysis of the same dataset : Gephi for network analysis ; Palladio for geographic and temporal representation ; spreadsheet (Excel, Open Office, Datamatic) for statistics analysis. This paper will present the first results of this research, part of it conducted with students during a graduate « introduction to digital humanities » course. Statistical diagrams show three different periods of Cunningham's work ; a stylistic signature with a preference for pieces that are 30 minutes long, and for soli, sextets and works with 13 to 15 dancers ; a general trend towards more dancers and more length ; the special place of soli in order to articulate the canon and the auxiliary ; the organization of documents in the Dance Capsules. Network analysis let me define two different ways of collaboration, the « star » and the « spiral », and raises awareness on pivotal dancers. Geographic representation highlights relations between Europe and the United States. In a wider historical perspective, it would be interesting to compare these preliminary results with other datasets. One example : two patterns have been identified in the Cunningham collaborations network : the star (figure 1), with discontinuous, centralized collaborations and groups separated from each other; the spiral (figure 2), with continuous, collective collaborations and one group growing organically. The change from the star to the spiral takes place when the company is created. Do these patterns characterize other choreographers and directors careers ? Is the creation of the company the main factor causing the evolution from the first pattern towards the second one ? While a well-worn issue – we do know that the creation of a company plays a crucial role in a career – the fact remains that “theatre analytics” let us visualize the patterns this break constitutes (or maybe not) and define different ways of collaborations.      Figure 1 : Merce Cunningham's collaborations network before 1954. The star pattern.  Pink, dancers; orange, composers ; green, stage designers ; blue, choreographer.    Figure 2 : Merce Cunningham's collaborations network after 1954. The spiral pattern.  ",
       "article_title":" Mesurer Merce Cunningham : une expérimentation en « theatre analytics »  ",
       "authors":[
          {
             "given":"Clarisse",
             "family":"Bardiot",
             "affiliation":[
                {
                   "original_name":"University of Valenciennes, Belgium",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "GLAM: galleries",
          "creative and performing arts",
          "art and art history",
          "information retrieval",
          "museums",
          "epistemologies",
          "libraries",
          "French",
          "visualisation",
          "archives",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The question of when “digital humanities” will drop the “digital” modifier and become “humanities” has special resonance for adjunct instructors. Digital humanities senior scholars might bridge the gap between tenured working conditions and adjunct working condition when crafting field infrastructures: not just because adjuncts merit both employment protections and what I call “microbenefactions” (more on that below), but because adjuncts are the invisible mass of humanities faculty buttressing every kind of institution, from community college to elite research-1 university. Adjuncts shoulder the humanities enterprise, teaching the general education classes that free researchers to pursue critical questions that advance the field.  This talk examines the infrastructural causes of DH adjunct invisibility and proposes two remedies: to motivate DH adjunct self-identification by convening DH adjunct-specific prizes and bursaries; and to invite senior DH faculty to perform “microbenefactions” that cost little effort and give adjuncts access to prize-worthy work opportunities or other benefits, such as renewable funding.  When “digital” humanities becomes just humanities, what’s to stop “adjunctification” from converting DH tenure lines into part-time or other tenure-ineligible work, as has happened pervasively in other sub-specialties? In 2012, Stephen Ramsay problematized DH as “the hot thing.” It's a skepticism shared by many in the field, including panelists of the DH 2017 Conference panel “Challenges for New Infrastructures and Paradigms in DH Curricular Program Development,” which openly wondered whether graduate students were well served by DH certificate programs.    See the DH 2017 panel abstract here:  https://dh2017.adho.org/abstracts/176/176.pdf. Ryan Cordell pointedly observes in published version of his DH 2017 talk that “completing the hours required for our robust [DH graduate] certificate program requires students to decide their path almost immediately upon admission, and the decision to pursue the certificate dictates very particular routes through the larger Ph.D. program.” See Cordell’s “Abundance and Usurpation While Building a DH Curriculum” posted to his blog:  http://ryancordell.org/research/abundance/    Miriam Posner notes that DH’s “sexiness” today obscures the “widespread understaffing” of many DH initiatives.”    Miriam Posner, “Money and Time,” http://miriamposner.com/blog/money-and-time/   This is an analog to adjunctification, the “shortsighted” boom/bust cycles of “soft” money quickly depleted which then require maintenance with a precarious budget. Amy Earhart has documented the unsustainability of early DH passion projects, websites whose hand-built archives rusticate when the faculty author retires or moves institutions.    Earhart,  Traces of the Old, Uses of the New.     Startups are sexy, but maintenance is not. When today’s senior DH faculty retire in ten or twenty years, what infrastructures of care will be in place to stop those vacated tenure lines from being converted to part-time positions? The gender politics of “sexy,” “hot” DH cast a pall over the field when one factors in that the majority of adjuncts are women. “As a woman of color,” Liana M. Silva wonders, “I am especially interested to know what the women in contingent ranks look like. According to the Education Department’s 2009 report, 51.6 percent of contingent faculty are women. The same report says 81.9 percent of contingent faculty are white. To what extent is contingent labor a problem for white women? Or, from another angle, to what extent is this a white labor issue, where class is meant to trump race?”    Liana M. Silva:  https://chroniclevitae.com/news/1017-how-many-women-are-adjuncts-out-there; National Center for Education Statistics 2009 report to which Silva refers:  https://nces.ed.gov/pubs2011/2011150.pdf See also: “Women as Contingent Faculty: The Glass Wall,” published by the American Association of University Professors  http://archive.aacu.org/ocww/volume37_3/feature.cfm?section=1 and New Faculty Majority’s “Women and Contingency” project:  http://www.newfacultymajority.info/women-and-contingency-project/    These questions about race, gender and contingent labor are digital humanities questions.  Awarding DH Adjuncts In its mentoring, promotion, and awards structures, the humanities professoriate is legacy-bound, oriented to a tenure system that pertains to only one quarter of people working in the field.    “Adjunctification” is well documented by adjunct advocacy organization like New Faculty Majority and Adjunct Nation; professional groups such the AAUP and the Modern Language Association (2014); intra-university studies such as George Mason’s, which surveyed 240 GMU adjuncts and “has been hailed as the most comprehensive study of a university’s contingent faculty working conditions to date” (2014); trade journals like  Inside Higher Education and  The Chronicle of Higher Education; and the popular press. I am struck by  The Atlantic Monthly’s occasional series (2013-present) that features titles like “There’s No Excuse for How Universities Treat Adjuncts” and “The Cost of an Adjunct.” See also Kathi Inman Berens and Laura E. Sanders, “DH and Adjuncts: Putting the Human Back in the Humanities.”    If, as James F. English contends in  The Economy of Prestige, the key indicator of any contemporary cultural phenomenon entering the mainstream is the creation of a prize (2), then perhaps it is time for digital humanists to create criteria of DH excellence specific to DH adjunct working conditions because adjuncting is the instructional mainstream. Doing so would motivate adjunct DHers to identify their work as DH and contribute recognizably toward DH research and pedagogy field development. Lack of access to an adjunct-specific DH prize reinforces adjunct invisibility, making it highly unlikely that even very good research will attain the recognition necessary to vault the scholar out of adjuncting. Most of the seven DH adjuncts I interviewed don’t necessarily identify themselves as “digital humanists” because they are not hired specifically to teach DH, though their methods are consistent with DH pedagogy practices.    A note about method. My university’s Human Subjects Research Review Committee determined an IRB was not required for me to conduct informational interviews with adjuncts. I used a common set of questions with each adjunct. The conversations veered to the specifics of their own particular cases.    “Imposter syndrome” is intensified by employment insecurity and DH definitional heterogeneity.    The authors of the “Alternate Histories of DH” panel note in their abstract: “Matthew Kirschenbaum’s identification of the digital humanities in 2014 as a ‘discursive construction’ that ignores the ‘actually existing projects’ of the field set the stage for scholars to rethink how the digital humanities conceptualizes its work and its history (‘What Is’ 48). More recently, in the introduction to  Debates in the Digital Humanities 2016, Matthew Gold and Lauren Klein use the scholarship of Rosalind Krauss who, in 1979, described art history as emerging as ‘only one term on the periphery of a field in which there are other, differently structured possibilities.’”     How to give adjuncts access to prize-worthy work opportunities? Senior scholars are key. In my talk, I will discuss microbenefactions senior scholars gave me when I adjuncted (2011-2014). Those invitations gave me access to nationally-visible projects and let me train myself in techniques that are now a core part of my tenure track job.  “Microbenefactions” is a term I invented to signify the opposite of microaggressions. They are small actions that shift the balance of power, the order of operations, that give adjuncts access to prestige or information otherwise inaccessible to them. Note that I use the singular here: “an” adjunct. These acts of inclusion are do-able as a one-off or in the course of a given term, not the Herculean efforts of adjunct advocates such as New Faculty Majority President Maria Maisto, Adjunct Nation, and the PrecariCorps collective who publish PrecariTales, 300-500 word anonymously authored adjunct stories.    https://precaricorps.org/about/true-stories/ The pinned story at time of writing details an adjunct who’s taught at the same university for ten years and has been hired to revise materials for a large enrollment course. One chair made sure she got paid the first lump sum; the replacement chair didn’t with the second, and she’s still waiting with “no recourse except to wait.” The Twitter hashtags #AdjunctLife and #RealAcademicBios also gather adjunct (but don’t curate) stories.     Unlike state-mandated employment protections, microbenefactions are individual and hyperlocal. They layer adjuncting’s transactional dyad with the more branching, collegial conceptualization of value typical of tenure-track employment. This is human-centered DH infrastructure. We acknowledge that humans are not widgets, and that DH teaching is not a dissemination of knowledge. The medium is the message. If the medium is adjuncting, then the message our students imbibe is that learning is transactional. Microbenefactions disrupt neoliberal infrastructure that shrinks learning and collegiality to transactions. What is a microbenefaction? It’s action by a tenured or tenure-track scholar who   writes funding for adjunct salary into grant proposals advises and mentors adjuncts seeks input from adjuncts about student-centered pedagogy aids adjuncts in finding university resources or paid extra work invites adjuncts to meetings co-authors with adjuncts doesn’t eliminate adjunct applications when deciding awards and honors authorizes support for adjunct professional development, such as conference travel pays to license adjunct-authored course materials after the adjunct leaves the institution writes letters of recommendation for adjuncts  Microbenefactions enact DH’s ethical ambit, which the Global Outlook::Digital Humanities special interest group articulates as a recognition “that excellent work is being done around the world,”    Global Outlook::Digital Humanities is a special interest group of the Alliance of Digital Humanities Organization. See http://www.globaloutlookdh.org/   even in elite first-world institutions that rely on adjunct labor but largely eliminate that labor from tenure and promotion consideration.   ",
       "article_title":"Is Digital Humanities Adjuncting Infrastructurally Significant?",
       "authors":[
          {
             "given":"Kathi Inman",
             "family":"Berens",
             "affiliation":[
                {
                   "original_name":"Portland State University, United States of America",
                   "normalized_name":"Portland State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00yn2fy02",
                      "GRID":"grid.262075.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-29",
       "keywords":[
          "and curriculum",
          "teaching",
          "cultural studies",
          "diversity",
          "pedagogy",
          "feminist studies",
          "English",
          "gender studies",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 37% of the United States population is non-white, but 90% of the books published for children during the last twenty-one years contain no multicultural content. This discrepancy has been called “The Diversity Gap” (Erlick, 2015) and, more starkly, the “Apartheid of Children’s Literature” (Myers, 2014). Based on data gathered since 1985 by the Cooperative Children’s Book Center, the representation gap has barely shifted over thirty years, with books by and about non-white people hovering between 10-14% of total children’s book production (CCBC 2017). Panels and initiatives about diversity in book publishing have not actually produced more books by and about non-white people. Book discoverability is thus a significant challenge to parents, librarians, and teachers seeking picture books depicting the lives of non-white children.  As it’s currently practiced in the North American book industry, “diversity” usually tallies “how many” rather than delving into the lived experience of non-white people. The Diverse BookFinder [DBF], a database and metadata project sponsored by Bates College and funded by the Institute for Museum and Library Services [IMLS], asks: how can metadata help to tackle these entangled problems? Can we build a network of information about children’s picture books that trains users to search for and discover books using complex concepts related to their own communities rather than race or ethnicity as the sole marker? Our long paper:  Surveys the problem of whiteness as the de facto point-of-view in children’s books, and the populist social media movements that resist this phenomenon; Examines the limitations of current metadata in k-3 books about non-white children;  Presents Diverse BookFinder as a strategic disruption of current metadata practices;  Conveys the pedagogical value of Diverse BookFinder in academic and public settings.  Readers have created online, massively participatory movements to prompt the predominantly white book publishing industry to publish more books by and about non-white people (Low, “Diversity Baseline Survey,” 2016). #WeNeedDiverseBooks, #1000BlackGirlBooks, and #OwnVoices originated in Twitter hashtags then converted their social media capital (likes, shares, reposts, followers) into an array of recommendation services: published anthologies, book finder apps, a short story contest, even a granting agency. Populist interventions are welcome and useful, but they are insufficient to remedy the problem of classifying existing books using metadata that reinscribe white privilege.  One intervention in these human and machinic systems is a human-curated and -coded catalog, Diverse BookFinder (  https://diversebookfinder.org/ ). Metadata and recommendation systems are not neutral. They operationalize cultural assumptions that the creators may not have intended or even be aware of. Critical code studies, the scholarship of platforms and software that examines computer source code hermeneutically, has charted useful ground in exploring how metaphors of containment and layers, for example, rationalize logics of racial exclusion (McPherson 2012). Many metadata schemas for books relate back to the physical structures libraries and classrooms use to organize books for readers. These systems create fixed and singular ways of relating items that construct contextualized exclusion (Drabinski 2013). The common cataloging systems used in the United States, including the Library of Congress and Dewey Decimal System, are centered in whiteness and maleness and reinforce the otherness of diverse titles. The separation of topics on women and gender (including queerness) from broader topics such as literature or history, for example, reinforce the notion that women and queers secondarily contribute to history and literature. This segregation repeats in the various forms of metadata where difference is replicated and continually defined by whiteness.   Exclusion is not specific to physical location like a library. Algorithmic “overfitting” is the phenomenon where recommendations are culled from a narrow spectrum of a user’s interests. Overfitting “can occur when a user is trying to be helpful by providing explicit feedback only about the content s/he strongly likes. This leads to the creation of a very specific model that knows the exact user preferences but is unable to detect any other types of interesting items since the user has not shown any interest in it” (Kunaver & Poztlz, 2017, 156). Under this system, the typical person would have difficulty in finding diverse titles online if those books did not already match to their past search behavior. Inconsistencies in the application of metadata compound this problem.  Metadata sometimes contain errors that hide or misrepresent the books, or don’t classify the types of information that would be most relevant to communities seeking books about non-white children. Such books are “mirrors and windows,” that reflect back or “mirror” one’s own lived experience in the faces, bodies, customs and cultural milieu depicted in the book, and open “windows” onto new cultures different from one’s own (Bishop 1990). Such books develop myriad literacies beyond reading comprehension, including conflict resolution, tolerance for the unfamiliar, and awareness of cultures beyond one’s own. When used in the classroom as an intervention toward intergroup contact, diverse picture books can foster intercultural understanding among children (Aronson, et al. 2016). Unfortunately, existing metadata does not account for the intricacies of diverse titles and so these books remain difficult to comprehensively identify or locate. Hand-coding is a remedy to discoverability problem. Without controlled language, books are simply not findable. There is no eschewing metadata; there is only writing better metadata, and theorizing the best practices that writers of descriptive metadata should follow in order not to reinscribe racist stereotypes and cultural marginalization. The purpose of this vocabulary is not to undo prior standards--which are each problematic in many ways-- but to contribute to the larger representation of diverse books and fill in the information holes. Systematic SEO work is underway to add language as it is used by the communities represented; for example, a user may enter “ Boricua” into DBF and yield results about Puerto Rican characters. The goal is to write metadata that reflects the lived experience of people the books depict.   When books are entered into the Diverse BookFinder, they go through a multi-step, hand coding process to compile the metadata commonly missing from other sources. Book characters are coded for racial and/or ethnic identity, gender, setting, with additional tags such as tribal nation, immigration status, or religion where applicable. Most books fall into one of nine categories that capture the message conveyed by these books. The categories are: Beautiful Life, Oppression, Cross-group, Biography, Race/Culture Concepts, Folklore, Incidental [ensemble or background characters of color], and Informational [factual content unrelated to race or culture]. These categories arose from an application of grounded theory, and created by a rigorous analysis of commonalities in picture book stories. This analysis shows that the concept of Beautiful Life, stories about a particular racial or cultural group experience, dominates diverse book publishing, but such a message is commonly unavailable in existing metadata outside of DBF. African Americans are most likely to be depicted in situations of oppression; Native Americans are disproportionately represented in “folklore,” and Hispanic and Latinx people are underrepresented generally in picture books. DBF has engaged students at all levels, at several institutions in thinking about representation and participating in research to better understand the role of picture books in children's development.  It’s pedagogically valuable to give students a direct search experience of how imprecise book metadata impedes book discoverability. In a lab exercise designed by Bell and implemented by Inman Berens, master’s students retrieved book metadata across three venues for two books in the DBF database. Those venues were: publisher website, retailer, library catalog. Students discovered significant errors in metadata, and notable variability from venue to venue. Ensuing class discussion allowed students to trace the interoperation of human classification errors and legacy systems such as Library of Congress Subject Headings with machinic processes. The students then reviewed a Library of Congress copyright form submitted to the LoC by our student-run trade press (Ooligan Press), and discovered ambiguities in the Library form’s language that prompted misclassification of our press’s just-released young adult novel. This exercise drove home that automated processes are framed by human judgment. The Diverse BookFinder is unique precisely for the level of human labor that goes into the data entry and book coding process. The inconsistencies and inaccuracies in book metadata and the additional information added to each book’s metadata could not be done by machine. This process serves to bridge the gaps in metadata, help users identify many more diverse titles than the average search, and provides new insights into what stories dominate in picture books. As public scholarship, this project seeks to move the diverse books discussion beyond a focus simply on the lack of numbers to also consider content and impact by translating research findings so that they are accessible and useful. ",
       "article_title":"Metadata Challenges to Discoverability in Children’s Picture Book Publishing: The Diverse BookFinder Intervention",
       "authors":[
          {
             "given":"Kathi Inman",
             "family":"Berens",
             "affiliation":[
                {
                   "original_name":"Portland State University, United States of America",
                   "normalized_name":"Portland State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00yn2fy02",
                      "GRID":"grid.262075.4"
                   }
                }
             ]
          },
          {
             "given":"Christina",
             "family":"Bell",
             "affiliation":[
                {
                   "original_name":"Bates College, United States of America",
                   "normalized_name":"Bates College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/003yn7c76",
                      "GRID":"grid.252873.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-29",
       "keywords":[
          "cultural and/or institutional infrastructure",
          "diversity",
          "library & information science",
          "metadata",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Prof. David M. Berry, Professor of Digital Humanities, University of Sussex; Visiting Fellow, Wolfson College, University of Cambridge.  In this paper I examine the historical constellations of concepts that have made up the idea of an idea of a university. The aim is to provide a tentative genealogy that maps the changes in the sets of concepts and affects that were bound together at particular historical junctures to declare an idea of a university. By definition this idea changes over time, through the effects of social change, political contestation, or other social forces operating on the university. I explore the specific reasons for why an idea of a university has been thought historically to be useful, and why, perhaps, we should revisit the idea of an idea of a university in light of rapid changes taking place following new pressures on the university coming from both digital technologies and new social forces. Indeed, the idea of a university has served as an important way of discussing an institution that comes in a multiplicity of shapes and sizes, with differing national traditions, and different ways of understanding what a university is for. But one constant that remains important in the outlining of an idea of a university is that the idea of an idea of a university is a compass for theory and practice in the university itself, and often in the wider society. In this sense, the idea of a university, comes to stand for a method of scoping the function and direction of the institution which we call a university, and most importantly provides a framework for determining what a university should and should not do. The idea of a university is, then, a compass for decision-making, it is a signature, a distinctive pattern.  The idea of a university is in this sense a kind of boundary object, which allows distinctions to be made between those institutions which are, and those which are not universities. The notion of a boundary object is useful because it acknowledges that universities are heterogenous and requires cooperation between multiple actors in order to be successful (Star and Griesemer 1989). Here, I want to think about the “idea of a university” as something like a boundary object, that is as an ideal type, as an object that is abstracted from all domains, sometimes fairly vague, but that nonetheless offers a “good enough” road map for all parties (Bowker et al 2016: 191). The problem of adapting a university to changing historical and social forces was often viewed intellectually as “finding the correct ‘idea’ of a university” (Rothblatt 1997: 33). This is the notion that one needs to find a “pattern of orientation”, that is, a conception is required to relate an actor, individual or collective to a manifold of objects in their situation of action, so that through internalisation for an individual, or through institutionalisation of a group – there is an organisation of the system of action.  The university as a form has never been frozen in aspic, it has continually adapted, grown, shrunk, expanded and shifted for all of its history. This draws attention to the way in which, at certain points in history, it was considered important that one should have an “idea” in mind in relation to the institutions of higher learning. By “idea” I mean a sense of what has been described variously by a number of thinkers as the “mission”, “end”, “soul”, “aims”, “principles”, “models” and “ideals” of an institution of advanced or higher learning. This is a debate that has gone on, revived in every generation, concerning the role and purpose of a university and the education it provides. We should note that the idea of a university paradoxically changes over time, through the effects of political contestation, social change or other social forces operating on the university. However, the notion of an idea of a university, as an institution requiring an essential core which is used to guide its operation and provide its raison d'être has remained in place until quite recently. John Henry Newman, of course, wrote perhaps the most famous idea of the university in 1859 when he argued, “a University…. is a place of  teaching universal  knowledge” (Newman 1996: 3). He maintained that the university had an essential function in the conservation of knowledge and ideas and their transmission to an elite body of largely undergraduates, a model he drew from Oxford. Similarly, Abraham Flexner writing in the 1930s with John Hopkins University in mind, argued that the university is “an institution consciously devoted to the pursuit of knowledge, the solution of problems, the critical appreciation of achievement, and the training of [students] at a very high level” (Flexner 1968: 42). But as the varieties of universities began to grow and their internal complexity multiplied, it became seemingly more difficult to identify an essential idea of a university.   By the late 1920s, for example, Robert Maynard Hutchins was remarking that the modern university was a set of schools and departments held together by a central heating system. Later in the 1960s, Clark Kerr described the modern university as “a series of individual faculty entrepreneurs held together by a common grievance” over car parking (Kerr 2001: 15). And today it does sometimes seem like the 21st century university is a set of schools and departments held together by a shared grievance over the IT support. However, in this talk, I will question Kerr’s dismissal of the university as an idea whose time is over, contesting his notion of the university as a  multiversity. Kerr believed that by force of circumstances, if not by choice, “administration everywhere… becomes a more dominant feature of the university” (Kerr 2001: 21).   I will bring these strands together to think about the challenges we face today in what Cathy Davidson (2017) has called “The New Education”. Today we live in a digital age, and indeed around us we see the implications of digital exosomatization in all aspects of our lives and societies (see Stiegler 2016). From the pressures on the economy and work through new forms of automation, difficulties with our ability to concentrate from new techniques of attention control and manipulation, and effects on our sense of identity, our societies and our politics through the use of social media and Big Data, the digital presents new challenges for the 21 st century. It is, therefore, not surprising that digital transformations should come to the university. Although, it is interesting to note how long digital forms took to effect change in research and teaching, even as university administration had been computerised for quite a while beforehand. The digital revolution, if we can call it that, is notable for confounding the critics, particularly internal to the university, who doubted that digital technology would have much of an effect on the structures, practices, processes and activities of the university. It seems that computation and the digital alone was not, in and of itself, enough to provide the step-change in the university, and we had to await the arrival of a number of different technologies, including radio networking, digital archives and tools, pocket computers and social media, combined with a number of corresponding social forces, such as digital homophily, a new political economy of data, and a generation entelechy that has never bought a paper newspaper, used a vinyl record or a CD, and finds the scholarly concentration required in the historical form of close reading arduous and unfamiliar. This is where the importance of the digital humanities as a field that can act as a bridge between past and future ideas of a university and could potentially contribute to a new idea of a university for a digital age – what we might call a contributory infrasomatization (see Berry 2016). In this talk I outline this research project, and the way in which I consider the digital humanities a crucial source of concepts for thinking about the idea of a university today.   ",
       "article_title":"The Idea of a University in a Digital Age: Digital Humanities as a Bridge to the Future University",
       "authors":[
          {
             "given":"David M.",
             "family":"Berry",
             "affiliation":[
                {
                   "original_name":"University of Sussex, United Kingdom",
                   "normalized_name":"University of Sussex",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00ayhx656",
                      "GRID":"grid.12082.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "digital ecologies and critical infrastructure studies",
          "digital humanities history",
          "historical studies",
          "film and media studies",
          "theory",
          "epistemology",
          "English",
          "philosophy",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Science fiction has been theorized as a laboratory in which text serves as the medium for experimentation with perspective and epistemology.   Jones, Gwyneth,  Deconstructing Starships: Science, Fiction and Reality (Liverpool UP, 1999) 4.    Yet scientific methods are more practicably applicable to the systematic efforts of textual scholars. Computationally assisted collation demands continual refinements to verify the accuracy of textual data and metadata and challenges a singular view of any documentary edition. Moreover, collation can test hypotheses about change over time, and the output of machine collation can serve as an experiment to identify, quantify, survey, and analyze the data of textual change. Digital collation of science fiction seems to combine the practical with the theoretical in its lab space.  An early form of modern science fiction, the 19th-century novel  Frankenstein has itself been the subject of digital variorum experiment since the mid-1990s production of the Pennsylvania Electronic Edition (PAEE) by Stuart Curran and Jack Lynch, a daring effort to prioritize the critical apparatus, pulling it from the obscurity of small type at the bottom of printed pages to make it front and center on screen displays.   See a representative page at   http://knarf.english.upenn.edu/Colv1/f1101.html . Curran, Stuart and Jack Lynch.  Frankenstein; or, the Modern Prometheus. The Pennsylvania Electronic Edition. Est. 1994.   http://knarf.english.upenn.edu/ .    The PAEE challenges us to find new ways to tell the variorum narrative of change over time. Much like Victor Frankenstein's composition of the Creature from multiple bodies, the effort to aggregate the distinct versions of this novel into a variorum might succeed in communicating a multi-dimensional narrative of its own composition and decomposition, inviting the reader to evaluate its successive stages just as the reader is invited to evaluate the three storytellers within the novel.   In the history of preparing digital texts with markup languages, whether in early HTML, SGML, or XML, markup standards tensed between two poles: a) the acknowledgement of a coexistence of multiple hierarchical structures and b) the need to prioritize a single document hierarchy in the interests of machine-readability, while permitting signposts of overlapping or conflicting hierarchies as of secondary importance.   See for example, P. M. W. Robinson, \" The Collation and Textual Criticism of Icelandic Manuscripts\"  Literary and Linguistic Computing , Volume 4, Issue 2, 1 January 1989, 99–105,   https://doi.org/10.1093/llc/4.2.99  ; Dekker, Ronald Haentjens, Dirk van Hulle, Gregor Middell, Vincent Neyt, and Joris van Zundert, \"Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project\"   Volume 30, Issue 3, 1 September 2015, 452–470,   https://doi.org/10.1093/llc/fqu007;   Eggert, Paul, \"  The reader-oriented scholarly edition\"    Digital Scholarship in the Humanities   , Volume 31, Issue 4, 1 December 2016, 797–810,    https://doi.org/10.1093/llc/fqw043;   and Holmes, Martin, \"  Whatever happened to interchange?\"    Digital Scholarship in the Humanities   , Volume 32, Issue suppl_1, 1 April 2017, 163–168,    https://doi.org/10.1093/llc/fqw048 .    In this paper we present a view of texts that emerges from the experience of comparing documents encoded in conflicting ways. Like the makers of   the genetic    Faust    edition , we find that multiple encoding structures must co-exist and correlate to achieve a meaningful comparison of editions.   See Gerrit Brüning, Katrin Henzel, and Dietmar Pravida, \"Multiple Encoding in Genetic Editions: The Case of 'Faust'\", Journal of the Text Encoding Initiative (4: March 2013) .    Further, hierarchies need to be reconceived in dynamic terms—where are their flex points for conversion from containment structures to  loci of intersection? In the process of collation, hierarchies must be dismantled and flattened in order for meaningful multiplicity to be represented, and in order for us to understand a dialogic relationship among textual variants. To study variation over time vexes the organizing principle of any singular hierarchy, but hierarchy in the context of collation may nevertheless build a robust architecture that  bridges distinct encodings rather than isolating them. In this architecture, arches and connecting spans are more viable than monoliths.   An inspiration for the bridging concept are the visualizations in  Haentjens Dekker, Ronald, and David J. Birnbaum, “It's more than just overlap: Text As Graph,” Balisage: The Markup Conference 2017, Washington, DC, August 1 - 4, 2017; in  Proceedings of Balisage: The Markup Conference 2017 :   https://www.balisage.net/Proceedings/vol19/html/Dekker01/BalisageVol19-Dekker01.html#d11284e1180  . The authors conceptualize an ideal model of texts in a graph structure organized primarily by their semantic sequencing and in which structural features are a matter of descriptive annotation rather than elemental hierarchy.     This paper addresses the serious issues of collating digital editions made at different times by different editors, and it discusses the bicentennial  Frankenstein variorum project as a challenging, illustrative case in point. We are preparing a variorum edition of Frankenstein in TEI P5 based on the 1818 and 1831  Frankenstein digital texts due to be released in 2018 in celebration of the bicentennial of  Frankenstein's first publication. Our collation source documents are adapted from the 1990s encoding of the PAEE (for the 1818 and 1831 editions), and the Shelley-Godwin Archive's diplomatic edition of the manuscript notebooks.   The Shelley-Godwin Archive's edition of the manuscript notebooks of  Frankenstein builds on decades of intensive scholarly research to create TEI diplomatic encoding:   http://shelleygodwinarchive.org/contents/frankenstein/ .    We are also newly incorporating a little-known edition of 1823 produced from corrected OCR. Our collation should yield a meta-narrative of how  Frankenstein changed over time in four versions that passed through multiple editorial hands. It is widely understood that the 1831 edition diverges sharply from the first print edition of 1818, adding new material and changing the relationships of characters. Less known is how the notebook material compares with the print editions, and how much we can identify of the  persistence of various hands involved in composing, amending, and substantially revising the novel over the three editions. For example, to build on Charlie Robinson's identification of Percy Bysshe Shelley's hand in the notebooks,   See Charlie Robinson's Introduction to the Frankenstein Notebooks (Garland 1996), reproduced here:   http://shelleygodwinarchive.org/contents/frankenstein/the-frankenstein-notebooks-introduction/ .    our collation can reveal how much of Percy's insertions and deletions survive in the later print editions. Our work should permit us to survey when and how the major changes of the 1831 text (for example, to Victor Frankenstein's family members and the compression and reduction of a chapter in part I) occurred. We preserve information about hands, insertions, and deletions in the output collation, to serve as the basis for better quantifying, characterizing, and surveying the contexts of collaboration and revision in textual scholarship.   The three print editions and extant material from three manuscripts are compared in parallel, to indicate the presence of variants in the other texts and to be able to highlight them based on intensity of variance, to be displayed like the highlighted passages in each visible edition of  The Origin of Species in Darwin Online.   Barbara Bordalejo, ed.  Darwin Online. See for example the illumination of variant passages in  The Origin of Species here:   http://darwin-online.org.uk/Variorum/1859/1859-1-dns.html     Rather than any edition serving as the lemma or grounds for collation comparison, we hold the collation information in stand-off markup, in its own XML hierarchy. That XML \"bridge\" expresses relationships among the distinct encodings of diplomatic manuscript markup in which the highest level of hierarchy is a unit leaf of the notebook, with the structural encoding of print editions organized in chapters, letters, and volumes. While the apparently nested structure of these divisions might seem the most meaningful way to model  Frankenstein, these pose a challenge to textual scholarship in their own right. As Wendell Piez has discussed,  Frankenstein's overlapping hierarchies of framing letters and chapters have led to inconsistencies in the novel's print production. Piez deploys a non-hierarchical encoding of the novel on which he constructs an SVG modeling (in ordered XML syntax) of the overlap itself.   These hierarchical issues provided an application for Piez’s invented LMNL \"sawtooth\" syntax to highlight overlap as semantically important to the novel; see Wendell Piez, \"Hierarchies within range space: From LMNL to OHCO\"  Balisage: The Markup Conference Proceedings (2014):   https://www.balisage.net/Proceedings/vol13/html/Piez01/BalisageVol13-Piez01.html     Our work with collation depends on a similar interdependency of structurally inconsistent encoding.   Our method involves three stages of structural transformation, each of which disrupts the hierarchies of its source documents:   Preparing texts for collation with CollateX   CollateX software applies a graph-based model of text to locate variants in documents. See   https://collatex.net/doc/   ,   Collating a new \"braided\" structure in CollateX XML output, which positions each variant in its own reading witness.  Transforming the collation output to survey the extents and kinds of variation, and to build a digital variorum edition.   In the first stage, we adapt the original code from the Shelley-Godwin Archive and from the PA-EE to create new forms of XML to carry predictable markers to assist in alignment. These new, pre-collation editions are resequenced (as when we move marginal annotations from the end of the XML document into their marked places  as they would be read in the manuscript notebook). They are also differently \"chunked\" than their source texts, resizing the unit file so that each represents an equivalent portion small enough to collate meaningfully and large enough that each document demonstrably aligns with the others at its start and end points.   Stage two weaves these surrogate editions together and transfers information from tags that we want to preserve for the variorum. Interacting with the angle brackets as patterned strings with Python, we mask several elements from the diplomatic code of the ms notebooks so that they are not processed in terms of comparison but are nevertheless output to preserve their distinct information. In CollateX's informationally-rich XML output, these tags render as flattened text with character entities replacing angle brackets so as not to introduce overlap problems with its critical apparatus. In Stage three, we work delicately with strings that represent flattened composite of preserved tag information and representations of the text, using XSLT string-manipulation functions to construct new files for analysis. We can then study, for example, where the strings associated with Percy Shelley are repeated in the later editions, and how many were preserved by 1831. We also build a scaffolding in stand-off markup for the digital variorum that bridges multiple editions, as modelled in Figure 1.      Fig. 1 An example variant with two different readings, showing Percy Bysshe Shelley's hand in the ms notebook. While the print editions of 1818, 1823, and the manuscript agree (yellow reading), the print edition of 1831 introduces new text (blue reading). The pointers are expressed according to the TEI XPointer Schemes defined in Chapter 16 of the TEI Guidelines and are subject to change.  This example shows how the stand-off collation identifies variant readings between texts by grouping pointers as opposed to grouping strings of text according to the parallel segmentation technique described in Chapter 12 of the TEI Guidelines.   See especially the TEI P5 Guidelines, 12.2.3 and 12.2.4:   http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TC.html#TCAPPS     The TEI offers a stand-off method for encoding variants, called “double-end-point-attachment”, in which variants can be encoded separately from the base text by specifying the start and end point of the lemma of which they are a variant. This allows encoders to refer to overlapping areas on the base text, but despite its flexibility, this method still requires choosing a base text to which anchor variant readings. While choosing a lemma for each variant may be necessary for a critical edition, it is not ideal for a variorum edition that, by design, does not choose a base text.   For a related example, see Viglianti, R. Music and Words: reconciling libretto and score editions in the digital medium.  “Ei, dem alten Herrn zoll’ ich Achtung gern’”, ed. Kristina Richts and Peter Stadler, 2016, 727-755.    Our approach, therefore, simply identifies variance and groups readings from multiple sources without conflating them into one document and with accommodation of multiple hierarchies.   Though we think of XML as a stable sustainable archiving medium, the repeated collapsing and expansion of hierarchies in our collation process makes us consider that for the viability of digital textual scholarship, ordered hierarchies of content objects might best be designed with leveling in mind, and that building with XML may be optimized when it is open to transformation. Preparing diversely encoded documents for collation challenges us to consider inconsistent and overlapping hierarchies as a tractable matter for computational alignment —where alignment becomes an organizing principle that fractures hierarchies, chunking if not atomizing them at the level of the smallest meaningfully sharable semantic features.   ",
       "article_title":"Hierarchies Made to Be Broken: The Case of the Frankenstein Bicentennial Variorum Edition",
       "authors":[
          {
             "given":"Elisa",
             "family":"Beshero-Bondar",
             "affiliation":[
                {
                   "original_name":"University of Pittsburgh at Greensburg, United States of America",
                   "normalized_name":"University of Pittsburgh at Greensburg",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02b394754",
                      "GRID":"grid.447541.7"
                   }
                }
             ]
          },
          {
             "given":"Raffaele",
             "family":"Viglianti",
             "affiliation":[
                {
                   "original_name":"Maryland Institute for Technology in the Humanities, University of Maryland, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "information architecture and modeling",
          "xml",
          "english studies",
          "text analysis",
          "English",
          "literary studies",
          "encoding - theory and practice"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper is an intervention that addresses an epistemological conundrum likely to become increasingly common and acute as the digital humanities both grow more diverse and increasingly encompass knowledge that has been produced outside of the parameters of Euro-American normativity. This contribution is in the spirit of addressing the need for cultural critique in the digital humanities along the lines that Alan Liu has called for (2012). I make use of an instantiated example, a text analysis tool for visualizing properties of a particular digitized text corpus in relation to trends of usage of specific words in the corpus, but I argue that the key insights are generalizable to a large spectrum of digital humanities tools.   Techno-social ensembles, acting as apparatuses of knowledge production through which computationally inferred knowledge is produced, are themselves power-laden. Data that is epistemically heterogeneous can be rendered illegible or less legible within a representational scheme that enforces standardization, creating a situation in which it can be visible only at the cost of relinquishing, in favor of the dominant episteme’s normative assumptions, the variability that constitutes its heterogeneity — as these normative assumptions tend to privilege the homogeneity of data. I name and describe this problematic in a way that fosters a dialog between philosophy and critical theory on the one hand and digital humanities on the other hand, placing it on a theoretical footing in relation to which that dialog can happen.   I describe possible approaches to this problematic, both conceptually and in the form of actionable solutions that follow from the conceptual issues. I also suggest a way to redress the unintended illegibility or invisibility that epistemologically heterogeneous and non-normative knowledge — such as, for example, many knowledge artifacts from the global South — can undergo in computationally mediated knowledge apparatuses. In the first, critical, section of the paper — “critical” in the sense of pertaining to critique — I show, building on insights that I have described elsewhere, how even powerful tools for text analysis and visualization that are state-of-art in the field may tend to produce an undercount in the number of accumulatively retrieved records of occurrence for non-western-language material encountered written in western script within western-language text (Bhattacharyya 2017). Considering such a tool as a knowledge apparatus, I show that the problem arises because the knowledge objects in question — non-western-language words — typically tend to present, when transliterated into morphological expressions in the Latin alphabet, much more representational variation than the extent of heterogeneity that such tools implicitly assume their normative knowledge objects, namely western-language material, to present. I describe the mechanism by means of which the problem arises in this particular knowledge apparatus, and I argue that the problem is homological, and therefore generalizable, beyond the particular constellation of words, scripts and language to a wider set of similar configurations in the humanities, especially when data from the global South is at play.   Computational inquiry into humanistic knowledge regarding non-normative knowledge objects such as knowledge objects from the global south is particularly vulnerable to the general problem: an apparatus for knowledge production tends to render invisible certain kinds of inscriptions that, for one reason or another, do not conform to the epistemic normativity that the apparatus presupposes. Cultural forces, through the sociotechnical ensemble that they are a part of, shape computational, algorithmic inquiry, so that the problem becomes especially acute in the digital humanities at scale. I argue that epistemological problems concerning legibility caused by the logic of scale and accumulation on the one hand, and the complementary logic of networks on the other hand, have a relation to the logics of hierarchical production and nonhierarchical (network-based) production, to whose increasing complementarity in the sociocultural sphere Luc Boltanski and Eve Chiapello, among others, have drawn attention (2005).  I will end by describing possible ways of addressing the issue in the context of undergraduate classes in comparative literature among the likes of which I have used a tool of the above kind. These possibilities point towards one possible kind of a decolonial approach in the digital humanities. I will suggest that the most promising solution has to do with \"persistent annotation\": a way for users (students for my use case) to annotate the invisibilities/illegibilities as and when they discover them, in the form of a written record that persists (from one term of teaching (one iteration of a course) to another term (another iterarion of the course). A sophisticated implementation of this solution would incorporate such a document, in the form of a user-contributable manifest, into the software tool itself (such as by including a visible pointer to such a manifest from within the GUI for the tool). For my small-sized use case, however, something as simple as a document carried over and renewed from semester to semester across the content-management system for the class can be sufficient as such a manifest. I will argue that this is roughly similar, in principle, to the way that one can make, edits (or, more generally (and more similarly to this situation), editing suggestions in Wikipedia, whether non-anonymously or anonymously as desired (but even in the case of anonymity, with an audit trail of accountability visible to a monitoring party). The epistemological stakes of this kind of approach in the case of Wikipedia have been addressed by Lih (2009) and can provide a useful point of comparison. While my specific use case pertains to textuality, I will also make points of connection with instances of the illegibility or invisibility of non-normative knowledge in other modalities of computational media in the context of certain specific kinds of data or cultural knowledge. Shannon Mattern, for example, has examined the question of how computationally mediated representations of spatial data can produce illegibility or invisibility (2015), and Irit Rogoff has shown how curatorial practice can do the same for visual artifacts (2005, 2009). Finally, I will conclude by arguing that a connection exists between coloniality, legibility and accountability. Jon Wilson has recently argued that, rather than imperial certainty and confidence, coloniality was often distinguished by administrative anxiety about governance over strangers who are epistemically ‘other’ (2017) — an anxiety partially redressed by external informants who are tolerated, but only when their participation is underwritten by mechanisms of trust and accountability legible within the imperial episteme. There is an interesting parallel here with digital tools created by well-intentioned tool builders ending up governing the legibility of non-normative cultural artifacts that have their origin in zones of epistemic otherness. ",
       "article_title":"Non-normative Data From The Global South And Epistemically Produced Invisibility In Computationally Mediated Inquiry",
       "authors":[
          {
             "given":"Sayan",
             "family":"Bhattacharyya",
             "affiliation":[
                {
                   "original_name":"Price Lab for Digital Humanities, University of Pennsylvania, United States of America",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "globalization & digital divides",
          "knowledge representation",
          "digital humanities history",
          "cultural studies",
          "diversity",
          "library & information science",
          "theory",
          "epistemology",
          "English",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Abstract: The digital landscape for teaching and learning increasingly supports the inclusion of multimedia-based instructional strategies. In higher education, a culminating class project or final assignment often requires students to synthesize, analyze, and create content using a variety of communication modalities. Such projects, when closely aligned with course content and desired learning outcomes, allows for relevant and authentic assessment that leverages digital communication strategies. This paper presents an emerging practical model, CASPA, to promote curriculum-based integration of multimodal projects for assessment in higher education courses. Additionally, we report on initial implementation results and recommendations for practice and further research.   The CASPA Model The model has five components; consume, analyze, scaffold, produce, and assess. The following sections address each component of the model and explain the instructional design processes in each.       C: Consume The CASPA model starts by having students consume an exemplar of the mode of communication they will later be asked to produce. To be specific, have students consume podcasts that are similar in length and format to the podcasts the instructor anticipates having them create. In the consume phase, students should be asked to talk about the basic message or story and react to it. Start with whole class or small group discussions on evaluative questions.  Using both positive and negative models in the Consume stage is useful, since the former is aspirational and the latter is easier to critique and sets an achievable bar for the students’ own productions. By having students consume and evaluate on a personal level, the instructor sets the stage for the next phase, the analyze phase, where students will begin looking critically at the medium.   A: Analyze Once students have consumed and can intelligently communicate the message of the mode and media being consumed, they should be encouraged to analyze the medium. If the instructor is uncomfortable analyzing a medium they may not be expert in, we suggest using this opportunity to discuss the basic concepts of narrative, storytelling, and argument, and how those concepts all depend on the chosen mode of communication. In analyzing the story, one might ask how effective the story is; what arguments, explicit or implicit, are evident; or what aspects of the story are most powerful. In this way, instructors can start developing a rubric, with or without input from their students, from which to analyze and critique a narrative based more on the success of the message than on the expertise of the medium. Certainly, where poor use of the medium is a barrier to understanding or appreciating the message, that should also be taken into account (i.e., sound quality of a video), but unless the students are being asked to develop professional products, the medium can often take a backseat to the message. As instructors consider this analysis phase of multimodal assessments, other possible questions could include:   How does this mode of communication affect the message?  What are the strengths and weaknesses of this form of communication?  What might be the strengths and weaknesses of another form of communication?  Once the students have a basic understanding of the medium, and they have had the opportunity to analyze narratives created in those media and decide on their own criteria for judging success and failure, they will have a better idea of what makes for a compelling, well-argued, well-researched piece and where such narratives fall short. This is where the right mix of successful and unsuccessful exemplars in the Consume phase pays off. Students can often learn more from failed storytelling than from successful storytelling.    S: Scaffold The creation of a multimodal product should be completed in phases with the appropriate learning support, or scaffolds, necessary for success in each phase. In scaffolding multimodal assignments, production-based assignments are used to build skills and/or help students communicate in multiple modes. An assignment on podcasting might be built upon teaching good interview skills and, therefore, the first assignment may be to create a series of interview questions and test them out with a partner. The second assignment might be capturing sounds using audio capture, etc. Another type of scaffolding that is worth considering here asks the students to tell a story in various ways at various points in the semester. For example, first, as an elevator pitch, then as a storyboard, then as a PowerPoint, etc. In this case, the final project might be a mashup of all these different media and an analysis of the effectiveness of each in communicating the central thesis. Instructors should consider the multiple pathways available to help students arrive at their final goal. Walking through a scaffolded project will lead to a much less intimidating production phase for students as well as a more transparent assessment process for instructors. Once the pieces have been laid out and assessed in the scaffolding phase, it’s time to assemble those pieces in the production phase.   P: Produce In the production phase of the assignment, there are two basic scenarios with a multitude of variations. In the first scenario, students assemble the discrete pieces of the scaffolding assignments into a final product. Again, a podcast where students have been assessed and received feedback on the various pieces, such as interview questions, music choices, and narrative, is a good example of an assignment that is assembled in this way. In the second scenario, individual assignments are used to help students tell the same story in a variety of ways and then to identify the strengths and weaknesses of each mode. In this sort of assignment, the production phase may look like a curated analysis of the different individual mini-assignments, along with a description of how each mode communicates in different ways.The final product should reflect the recursive phases of analysis and instructor feedback in such a way that students are submitting a polished final creation rather than a simple redraft. From here, students and instructors proceed to the final stage of the CASPA model and benefit from the constructive feedback of others.    A: Assess Once students are sensitive to the affordances and constraints of the vehicle and how that vehicle affects communication, the production of the assignment is then only the penultimate step in the learning process. An assessment of the assignment, including an assessment of the effectiveness of the vehicle in conveying the story, is highly encouraged as the final step of the CASPA model. For example, students may create a video, not because it is novel, but because the video modality informs the content produced and the story told. Multimodal assignments allow for reflection and assessment of that interaction between vehicle and content. It allows the student to ask why. Why a video over a website? Why a podcast over a PowerPoint presentation? What is gained and what is lost when choosing a vehicle? This line of inquiry might be valued as an area of analysis with a simple multimedia project, but it is paramount in a multimodal project. In this final stage of the assignment, the students return to an analytical mode, critiquing their own work and the work of their peers, ideally using rubrics they have developed or used in the analysis phase. This assessment phase allows students to see the process of consumption and production as an integral whole and an iterative process. Here is where the distinction between multimedia and multimodal really matters. In the assessment phase of the assignment, for the assignment to be truly multimodal, instructors should guide students to identify how the mode of communication alters, enhances, or hinders the telling of the story. Peer review and feedback on the effectiveness of the story will inform this process and empower students to refine their work for future use or retelling.  Various mini-assessments can, and should, come at various stages in the process (e.g., after each scaffolded assignment or before students submit their final projects). However, a more formal final assessment can be a powerful culminating activity, driving home the importance of self-reflection as part of an ongoing process of self awareness and improvement. Guided questions here may also be helpful. Instructors could select a few key questions, such as:   What could be removed from the final product and why? What should be added and why? Is there anything that is in the wrong place within the narrative? Where should it go?  The tenets of the CASPA model support instructors and students alike in the important academic processes of analyzing, synthesizing, and conveying compelling arguments. The following section illustrates these processes in an application scenario derived from real experiences at William and Mary.   ",
       "article_title":"The CASPA Model: An Emerging Approach to Integrating Multimodal Assignments",
       "authors":[
          {
             "given":"Michael",
             "family":"Blum",
             "affiliation":[
                {
                   "original_name":"College of William & Mary, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "audio",
          "and curriculum",
          "teaching",
          "film and media studies",
          "video",
          "pedagogy",
          "cultural studies",
          "English",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" On September 20, 2017, Hurricane Maria made landfall in Puerto Rico. As the strongest hurricane to hit the island since 1928, the storm has caused significant damage—especially to infrastructure including roads, dams, communications networks, the electrical grid and the water supply. With much of the island still without power, and with limited aid coming from the United States, Puerto Rico is being left to deal with a humanitarian crisis on its own. The slow nature of the United States’ response, coupled with Donald Trump’s barrage of tweets, highlight the ways in which colonial narratives are feeding into disaster response efforts. For example, when San Juan Mayor Carmen Yulin Cruz requested an increase in federal aid, Trump replied, “Such poor leadership ability by the Mayor of San Juan, and others in Puerto Rico, who are not able to get their workers to help. They want everything to be done for them when it should be a community effort” (@realDonaldTrump). He later went on to claim that Puerto Rico’s need for aid was hurting the federal budget and to claim that Hurricane Maria was not “a real catastrophe” for the island (“Trump compares Puerto Rico to Katrina”). Trump’s victim-blaming behavior highlights both his lack of empathy for the citizens of Puerto Rico and the racial prejudice that undergirds the U.S. colonial enterprise. Although rarely so blatant, such behavior is not new; rather, the United States has an ongoing legacy of racialized disaster relief that is grounded in its colonial endeavors, particularly in Puerto Rico.  According to  El Nuevo Día, the most widely-circulated newspaper in Puerto Rico, “El huracán María no superó a San Felipe II según un informe preliminar”, or “Hurricane Maria did not surpass the strength of the San Felipe II Hurricane” (Ortega Marrero). Nevertheless, the two storms bear striking similarities. Both hit the island of Puerto Rico as category 5 hurricane, both crossed the island from the southeast corner and moved through the center of the island to the northwest corner, and both had significant long-term effects on the island.   While coverage of the 1928 storm’s devastation in Florida is prominently displayed in novels and journalistic reports, coverage of the damage in Puerto Rico is almost non-existent in the mainland United States. I argue that the vulnerabilities created by the hurricane of 1928 were pivotal to the United States colonial agenda in Puerto Rico, resulting in a land grab by corporations and government entities that would impede the island's agricultural industry and economy for decades. This is made evident by the fact that the U.S. downplayed effects of the storm, the U.S. implemented policies to hurt small farmers & agricultural workers, and the U.S. denied that their actions caused economic and environmental harm to Puerto Rican citizens. To make these connections clearer and to bring the stories of the storm’s underrepresented victims back into our cultural memory, I have launched a digital work called the  Hurricane Memorial project. This site includes my preliminary research, visualizations of my findings, and interviews with survivors and their family members.   As Florida and the Caribbean start to recover from Hurricane Maria, it is important to note that those living in economically disadvantaged communities will suffer the greatest from the storm’s damage—just as they did in 1928. Aid quickly was rushed to Florida, while the federal government is “killing [Puerto Rico] with inefficiency” (“I Am Mad As Hell”). Such a response demonstrates the ways in which United States’ racialized response to natural disasters is deeply rooted in its colonial enterprise. Failing to address these issues risks reinforcing harmful colonial narratives and causing irreparable harm to communities throughout the Caribbean and the world. ",
       "article_title":"Hurricane Memorial: The United States’ Racialized Response to Disaster Relief",
       "authors":[
          {
             "given":"Christina",
             "family":"Boyles",
             "affiliation":[
                {
                   "original_name":"Trinity College, United States of America",
                   "normalized_name":"Trinity College",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/019dhar59",
                      "GRID":"grid.440018.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "folklore and oral history",
          "project design",
          "organization",
          "globalization & digital divides",
          "spanish and spanish american studies",
          "management",
          "public humanities and community engaged scholarship",
          "cultural studies",
          "diversity",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction Ideophones, sometimes called “mimetics” (Akita, 2009) or “expressives” (Diffloth, 1976) are expressions that communicate sensory aspects of the physical word such as sound (i.e., onomatopoeia), movement, color, etc., or cognitive/emotional states (e.g., “ta-da” in English). Although most linguistics description of and inquiry into ideophones have focused on vocal expressions, gestures are integrated with ideophonic utterances in some languages. The analysis of these gestures and their symbolism may augment scholars’ understanding of the target language including how native speakers mentally represent their environment. In this paper, we describe a web-based tool,  Quechua Real Words, used by ideophonic linguists at [institution] to catalog and study multimedia representations of gestured ideophones as performed by native speakers of Pastaza Quichua. Research based on this tool is opening new understanding of the target language’s aesthetics, especially regarding the non-arbitrariness of gestured signs. We also discuss the relationship between this tool and other digital humanities efforts.   Project Background The indigenous people of eastern Ecuador speak Pastaza Quichua (PQ), a dialect of Northern Quechua. Descended from the language of the Incan civilization, Quechua is still spoken by as many as 10 million people in the Andes region stretching from Ecuador in the north to Argentina in the south. In 2015 [second author], a linguistic professor at [institution] led a group of student researchers who spent one semester in Ecuador recording and appreciating the indigenous culture and language. This team videoed over a hundred hours of interviews with PQ speakers, including thousands of examples of PQ ideophonic gestures. The team returned to [institution] baffled at the scope of archival work that lay between their raw footage and their research goals. In consultation with their [institution]’s Office of Digital Humanities, they constructed a WordPress-based website that facilitated their archival activities, accelerated their research, and opened their work to a global audience.  The Website  Quechua Real Words uses custom content types within WordPress and simple data entry forms that allow students and professors with little computer experience to record ideophones and link entries to specific segments of recorded videos. The project’s footage is hosted on YouTube for simplicity and accessibility, and the data entry form only requests the video segment’s URL and start and stop times.       Quechua Real Words ideophone recording form.  The entry form includes two other important features: First, the researcher may classify each ideophone by one or more “sensory modality” (e.g., color, haptic, movement, etc.). Second, each scholar—be they professor or student—may add their name to the list of the entry’s contributors. Once an ideophone is saved, it immediately appears on two indexes: the list of all ideophones, and the list of ideophones by modality. The first index allows researchers to look up specific ideophones, while the second promotes synthetic exploration where relationships between apparently unrelated ideophones can be made clear. Each ideophone page displays the pronunciation (in IPA format), definition and other information one would expect from a traditional dictionary entry. It also shows a text description of the ideophone’s paralinguistic qualities and one or more videos of native speakers expressing the ideophone in candid conversation. These videos are segments of longer YouTube videos, and the segments may be looped, paused, and replayed. (Such functionality is not native to YouTube’s standard embedded player, so the site’s video player is a custom JavaScript that connects to YouTube’s published API.)     An ideophone page from  Quechua Real Words.  As insisted on by the supervising professor, each ideophone page displays a “How to Cite” section with a citation in the Linguistics Society of America’s preferred format. To recognize the collaborative nature of the website, the credited parties in the citation include everyone who contributed to the entry, even students.   Research Potential During the first two years of its existence, [first author] used  Quechua Real Words for research published in a special issue of the  Canadian Journal of Linguistics ([second author], 2017), in three presentations at international conferences ([second author], 2015a; [second author], 2015b; [second author], 2014), and in two invited book chapters ([second author], in press; [second author], in press). Additionally, the website’s content will inform an upcoming monograph ([second author], in preparation).  These publications focus on contextually-rich methods of understanding PQ ideophones, comparing specific gestures and intonations between speakers and contexts, and discovering how the ideophones are integrated with—rather than distinct from—the language’s verbal aspects. As Akita and Tsujimura (2016) point out, the goal is to seek typological generalizations for ideophones rather than consider them in isolation. [Second author] seeks to extend these integrative studies and semantic generalizations beyond the vocal utterances into the gestured space.  Quechua Real Words as a Model for DH Collaboration When [second author] proposed this website to the Office of Digital Humanities, [she/he] had little notion that it would lead to such a level of scholarly productivity. It was only as [she/he] saw how the site could function that [she/he] began to grasp its potential. Similarly, [first author], the digital humanists who crafted the website, overlooked its potential because, quite frankly, the technology behind  Quechua Real Words is rudimentary for most DH centers.   Perhaps [first author]’s estimation was clouded by the fact that DH as a field has favored text-based literary analysis over multimedia research. Despite the work of the ARTeFACT project (Coartney & Wiesner, 2009) and a few others who have considered digital analysis of performing arts, DH has contributed much less to the analysis of video interactions, such as these ideophones, than it has to the analysis of written text. Garrard, Haigh, and de Jager (2011) demonstrate the status-quo for dealing with nonverbal communication in DH research: “…the recording and representation of various types of paralinguistic feature in transcription is somewhat idiosyncratic, and thus unreliable, suggesting that they should be removed in the interests of consistency.” This lack of emphasis on paralinguistic and nonverbal communication is in spite of those features’ apparent value. “The nonverbal channel carries important information about emotional expressions… Systems that combine multiple modalities usually outperform single-modality systems in recognizing emotional” (Truong, Westerhof, Lamers, & de Jong, 2014). Unfortunately, even Truong et al. restricted their valuation of nonverbal channels to prosodic qualities such as timing and rhythm; they did not address issues of body language or gestures. Regardless of why [first author] overlooked the website’s potential, [she/he] has since changed how [she/he] evaluates potential collaborative DH projects. [She/He] now focuses on evaluating the use of the tools, websites, and other resources [she/he] would develop  relative to the target discipline rather than relative to the state of the art within DH. This new approach has already proven fruitful (first author, 2017).   Future Plans While [second author] continues to leverage  Quechua Real Words for [her/his] scholarship, [first author] has combed the DH literature to discover methods of extending the site’s capacity. One DH project that could contribute guidance to this project is the work of Paquette-Bigras and Forest (2014) who attempted to build a descriptive vocabulary for dance movements. A similar effort to construct a vocabulary for describing non-vocal expressions may reveal yet-unnoticed relationships between expressive gestures. This would require intense, non-automated markup of the gestures, but the  Quechua Real Words website and the student-involved structure of [second author]’s courses would be facilitative. Such detailed modeling of the gestures would extend the modality-based clustering currently available on the website to include form-based clustering of the gestures.  Additionally, we are working with [institution’s library] to add  Quechua Real Words to their federated search databases. This will increase the site’s discoverability by scholars and students throughout the world.  ",
       "article_title":"Quechua Real Words: An Audiovisual Corpus of Expressive Quechua Ideophones",
       "authors":[
          {
             "given":"Jeremy",
             "family":"Browne",
             "affiliation":[
                {
                   "original_name":"Brigham Young University, United States of America",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          },
          {
             "given":"Janis",
             "family":"Nuckolls",
             "affiliation":[
                {
                   "original_name":"Brigham Young University, United States of America",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "audio",
          "corpora and corpus activities",
          "linguistics",
          "indigenous studies",
          "video",
          "anthropology",
          "English",
          "content analysis",
          "multimedia",
          "semantic analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction The relationship between the entropy of language and its complexity has been the subject of much speculation – some seeing the increase of linguistic entropy as a sign of linguistic complexification or interpreting entropy drop as a marker of greater regularity (Montemurro and Zanette 2011, Juola 2016, Bentz et al. 2017). Some evolutionary explanations, like the learning bottleneck hypothesis, argues that communication systems having more regular structures tend to have evolutionary advantages over more complex structures (Kirby 2001, Tamariz and Kirby 2016, Ferrer I Cancho 2017). Other structural effects of communication networks, like globalization of exchanges or algorithmic mediation, have been hypothesized to have a regularization effect on language (Kaplan 2014).  Longer-term studies are now possible thanks to the arrival of large-scale diachronic corpora, like newspaper archives or digitized libraries (Westin and Geisler 2002, Fries and Lehmann 2006, Lyse and Andersen 2012, Rochat et al. 2016). However, simple analyses of such datasets are prone to misinterpretations due to significant variations of corpus size over the years and the indirect effect this can have on various measures of language change and linguistic complexity (Buntinx et al. 2017). In particular, it is important not to misinterpret the arrival of new words as an increase in complexity as this variation is intrinsical, as is the variation of corpus size. This paper is an attempt to conduct an unbiased diachronic study of linguistic complexity over seven different languages using the Google Books corpus (Michel et al. 2011). The paper uses a simple entropy measure on a closed, but nevertheless large, subset of words, called kernels (Buntinx et al. 2016). The kernel contains only the words that are present without interruption for the whole length of the study. This excludes all the words that arrived or disappeared during the period. We argue that this method is robust towards variations of corpus size and permits to study change in complexity despite possible (and in the case of Google Books unknown) change in the composition of the corpus. Indeed, the evolution observed on the seven different languages shows rather different patterns that are not directly correlated with the evolution of the size of the respective corpora. The rest of the paper presents the methods followed, the results obtained and the next steps we envision. Method and Results We use the concept of kernel entropy (Buntinx et al. 2017), defined as the Shannon entropy measure applied on word occurrences distribution normalized on the kernel of a given corpus. To calculate this measure, the corpus is subdivided into yearly sub-corpora. Next, we then calculate the word occurrences for the words that are present in each sub-corpus for each year. These words form a set, called a kernel. The word frequencies are normalized on the kernel    K   for each year    y   and the formula of Shannon entropy (using napierian logarithm) is applied on these distributions providing a measure that can be compared diachronically with robustness to corpus size evolution and to noises. The kernel entropy of a kernel    K   for the year    y   is given by the formula:     Where      N   K     is the number of words composing the kernel and      f   i   K , y     the relative occurrence frequency of the word    i   normalized on the kernel    K   in the year    y  . The kernel entropy measure is computed for seven languages of Google Books corpora.  Figure 1 shows the kernel entropy variations normalized with respect to the average value (which change over the languages because kernels of different corpus also have different sizes).     Figure 1: Normalized yearly kernel entropy evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian. We observe that even if all the seven language have different patterns and inflection points, they tend generally to show an effect of negentropy with increasing years. We note that most languages have a crosspoint in 1905, except for the Russian language, showing variations particularly from 1920 to 1930. We present in Figure 2 the kernel entropy evolution for each language in comparison to the corpus size.                                         Legend:  (1) British / American English   (2) French / Italian   (3) Spanish / German   (4) Russian   Kernel Entropy: Blue   Size: Red          Figure 2: Yearly kernel entropy evolution and size evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian. Google Books corpora may experience sudden changes in composition depending on the year. For example, the addition of scientific literature and medical journals (Pechenick et al., 2015). In this case, the words kernel distribution, even if it is robust because composed of the most stable words, can change for a year which is subject to a change of composition of the corpus. However, this effect is still reduced because the words appearing and disappearing during this transition phase are not taken into account. We observe that the entropy of the kernel seems not to be affected by the size variations of corpora and when it appear to be affected, the direction of variation is unpredictable. The British English and American English are the least affected languages by the negentropic effect. Their kernel entropy increases over time until 1960 (British English) and 1940 (American English). However, American English kernel entropy decrease quickly from 1940 to 1985. We observe that the obtained curve for the French language is similar to the one corresponding to the study of language evolution through 200 years of newspapers written in French despite a different kernel size (Buntinx et al. 2017).  Interesting inflection points are detected and should be poignant to specialists of the targeted language. We present in  Figure 3 the number of words in the kernel and inflections points for the seven languages.    Language Number of words in the kernel Inflection point 1 Inflection point 2   British English 82’332 1959    American English 44’949 1931 1985   French 79’575 1825 1885   German 36’660 1850 1946   Italian 30’996 1983    Spanish 25’582 1995    Russian 5’123 1920 1988   Figure 3: Number of words in the kernel and kernel entropy inflection points for the seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian. Furthermore, it is possible to show the languages proximity in terms of kernel entropy evolution behavior through the determination of a distance based on kernel entropy correlations. A projection of the resulting matrix distance using PCA is presented in Figure 4. We observe that British English and American English are represented together to the left of the plan because they have a relative opposite pattern with respect to other languages. Russian is also particular because of the brutal effect of the negentropy observed between around 1920 and the sudden increase at the end of the 1980s. The last four languages, French, Spanish, German and Italian share a more similar behavior and are represented in the right-bottom part of the plan.  Although much more in-depth investigation must be done, it is reasonable to make the hypothesis of different internal and external factors for explaining these various patterns. The Russian case clearly invites to investigate correlations between linguistic policies during the Sovietic period and their actual effects of the Russian language. The similarity between French, German, Italian and Spanish pushes in the direction for similar processes of standardization, potentially due to linguistic convergence at national levels suppressing some regional particularities. In contrast, American and British English evolution is likely to be explained through the particular histories of the respective English-speaking populations and their relation to the rest of world. The progressive rise of English as a global language, spoken and written by many non-native speakers, is certainly playing a role in the shaping these particular curves.     Figure 4: PCA projection of distance matrix using kernel entropy correlation-based distance for Google Books corpora: British English, US English, French, German, Italian, Spanish and Russian. ",
       "article_title":"Negentropic linguistic evolution: A comparison of seven languages",
       "authors":[
          {
             "given":"Vincent",
             "family":"Buntinx",
             "affiliation":[
                {
                   "original_name":"EPFL (École polytechnique fédérale de Lausanne), Switzerland",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL (École polytechnique fédérale de Lausanne), Switzerland",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "computer science",
          "corpora and corpus activities",
          "linguistics",
          "multilingual / multicultural approaches",
          "text analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Automated lemmatization, that is the retrieval of dictionary headwords, is an active area of research in Latin text analysis. Latinists have available web-based applications like Collatinus (Ouvard and Verkerk, 2014) and LemLat (Bozzi et al., 1992) and web services like Morpheus (Almas, 2015). LatMor (Springmann, 2016) and TreeTagger (Schmid, 1994) offer lemmatization as a byproduct of their primary tasks as morphological taggers. Recent work, to name a few developments, has seen lexicon-assisted tagging and rule induction (Eger et al., 2015; cf. Juršič, 2010) as well as neural networks (Kestemont and De Gussem, 2017) used as strategies for improving Latin lemmatization. In this short paper, I describe the implementation of the Backoff Lemmatizer (https://github.com/cltk/cltk/blob/master/cltk/lemmatize/latin/backoff.py) for the Classical Language Toolkit, an open-source Python platform dedicated to developing natural language processing tools for historical languages (Johnson, 2017). The Backoff Lemmatizer is in fact not a single lemmatizer but rather a customizable suite of sub-lemmatizers, based on the Natural Language Toolkit’s SequentialBackoffTagger. The SequentialBackoffTagger allows the user to “chain taggers together so that if one tagger doesn’t know how to tag a word, it can pass the word on to the next backoff tagger” (Perkins, 2014, 92). While the backoff process was originally designed to handle part-of-speech tagging, and so, a task with a limited tagset, it works well for lemmatization (~90.34% accuracy compared to the 93.49% to 95.30% range reported in Eger et al., 2015).  A default class for sequential lemmatization, BackoffLatinLemmatizer, is available through the CLTK “Lemmatize” module using the following backoff sequence: 1. a dictionary-based lemmatizer for high-frequency, indeclinable vocabulary; 2. a unigram-model lemmatizer based on training data; 3. a rules-based lemmatizer based on regular expression patterns; 4. a variation on the previous regular-expression-based lemmatizer that factors in principal-part information; 5. another dictionary-based lemmatizer using the Morpheus lemma dictionary; and finally 6. an identity lemmatizer that returns the token as lemma.  Although currently available and tested only for Latin, the Backoff Lemmatizer is in theory language agnostic, since the sub-lemmatizers can be passed language-specific training data and models. So, for example, the UnigramLemmatizer requires training data in the form of a Python list of tuples of the form  [(‘token1’, ‘lemma1’), (‘token2’, ‘lemma2’), ...]. A Latin model with data in this form based on The Ancient Greek and Latin Dependency Treebank (Celano, Crane, and Almas, 2017) is available in the CLTK Latin corpora, but a similar model could be built for any language. Similarly, the RegexLemmatizer relies on a custom dictionary of regular expression patterns extracted from Latin morphological patterns. But again, a list of patterns could be written for any language and worked into this sub-lemmatizer. Furthermore, the sub-lemmatizers can be added or removed as necessary, and can be reordered based to optimize accuracy for a given language or language domain. Accordingly, the BackoffLemmatizer is particularly well-suited to less-resourced languages (Piotrowski, 2012, 85): a language without sufficient training data could build a backoff chain that ignores the UnigramLemmatizer and rely only on dictionary- and rules-based sub-lemmatizers.   Because of its multipass combination of probabilistic tagging based on existing Latin text, Latin lexical data, and a ruleset based on Latin morphology, the Backoff Lemmatizer can be described as following a philological method. By this, I mean that the process reflects the reading, decoding, and disambiguating strategies of the modern Latin reader (McCaffrey, 2006). For example, the process echoes the classroom process of Paul Diederich, who describes groups of students reading together and analyzing their text first through a combination of previous knowledge and dictionary lookups, but then “if no member of the group can clear up the difficulty, they resort to a formal analysis of the endings” (Hampel, 2014, 95).  One limitation of the current Backoff Lemmatizer setup is its binary sequential decision making; that is, a token is assigned a lemma based on the first match encountered in the backoff chain. By way of conclusion, I will discuss work in progress on a progressively scored Backoff Lemmatizer, or one that returns the lemma with the highest likelihood found after a token passes through and is assigned a score by every sub-lemmatizer in the chain.  ",
       "article_title":"Backoff Lemmatization as a Philological Method",
       "authors":[
          {
             "given":"Patrick J.",
             "family":"Burns",
             "affiliation":[
                {
                   "original_name":"Institute for the Study of the Ancient World, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "philology",
          "classical studies",
          "corpora and corpus activities",
          "text analysis",
          "data mining / text mining",
          "English",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The stains found on medieval manuscripts are signs that indicate a past life, more specifically the visible and invisible remains of human interaction over time. Reading these signals - in concert with conventional information such as script, collation, illumination, and size - can add to our understanding of their history and use. While recent work has been done on the uses of multispectral imaging in understanding the degradation and preservation of parchment, there is little pre-existing scholarship on the presence and nature of stains in medieval texts. Indeed, the significance of stains has traditionally been underestimated.   The multispectral imaging dataset and physical samples developed by Giacometti  et al. (2015) will be taken into consideration for comparison and as further reference material on staining substances on parchment. See also Giacometti  et al. 2012; MacDonald  et al. 2013; Campagnolo  et al. 2016; Giacometti  et al. 2016, 2017.   This project focuses on those very manuscripts that are often overlooked due to heavy soiling and damage, effects that diminish their perceived quality and value. We are a team of interdisciplinary postdoctoral scholars and collaborators working on constructing a  Library of Stains in order 1) to provide an online database that will allow scholars, librarians, and conservators to better analyze materiality, provenance, use and preservation of manuscripts/early-printed books; 2) to document and disseminate a methodological approach for analyzing stains; and 3) to provide a model for public-facing interdisciplinary collaboration. To our knowledge, this is the first interdisciplinary attempt to build a library of medieval and early-modern stains using the tools of medieval literature, medieval history, codicology and bibliography, multispectral imaging, chemical analysis, and data science.   Our presentation will include information about the the  Library of Stains project framework and methodology, as well as the dissemination of its data and results. The project timeline, supported by a microgrant from the Council on Library and Information Resources (CLIR), covers a one-year period. Imaging will be complete by December 15, 2017; processing and analyzing the images will follow and results will be documented by April 2018 and codified by August 31, 2018. Our presentation will report on the project findings, their broader implications, public engagement, and best practices for conservators, archivists and librarians who will use the project’s database.  This pilot study aims to provide an identified, open-access database of a number of common stains found on parchment, paper, and bindings in medieval manuscripts and early printed books in order to help researchers answer questions such as manuscript provenance, transmission, and material culture. It also highlights how using scientific technologies - in this case, multispectral imaging - aids in answering traditional arts and humanities questions. The database will hold metadata collected from the multispectral imaging (JSON files), as well as information about the processed image data leading to stain identification. The latter will take the form of tiff files representing images taken at different light wavelengths, ultraviolet and fluorescent settings, in conjunction with associated specific spectral curves, and the relevant data collected from XRF/FTIR/FORS scanning. The Schoenberg Institute for Manuscript Studies will preserve the database and all files as part of their open-access Colenda repository. Identifying the stains present in a book and understanding the relationship between the placement of the stain and its surrounding text brings to light new information about how manuscripts were used, read, and applied in situ. We have identified forty Western European manuscripts held in the University of Pennsylvania Libraries, the Chemical Heritage Foundation, the Library of Congress, the University of Wisconsin Special Collections and the University of Iowa Special Collections, with dates ranging from the twelfth to sixteenth centuries. The type of stains anticipated vary according to genre of manuscripts, and will likely indicate the presence of such elements as blood,   Confirming blood stains, such as those recorded on the  Declaration to the World by Agustin de Iturbide (see   https://www.wdl.org/en/item/2969/#institution=center-for-the-study-of-the-history-of-mexico-carso  - accessed 2017/11/26) is particularly interesting for obvious forensic and historical reasons.   heavy metals, candle wax, urine, various oil-based concoctions, wines or spirits, and possibly zoological matter such as crushed spiders or flies. For example, it is our hope that once we have processed the results for a manuscript entitled “On the Colors of Urine,” ( De Coloribus Urinae, University of Pennsylvania Schoenberg Institute for Medieval Studies, MS Codex 133), it will show that the liquid stains throughout are indeed urine, perhaps stemming from a doctor or patient’s accidental spill when consulting the manuscript. This type of analysis builds upon the significance of intellectual and material analysis concerning written culture, and extends beyond current analytical approaches to text, illumination, and bibliographical description.   Once the results are verified and each type of stain has been characterized, other interested parties will be able to access the database and verify their own stains against the fixed dataset. A methodological approach will be documented, disseminated, and openly accessible to those wanting to work with unknown stains so that researchers can model and replicate the workflow and process when faced with an unidentified stain. With data gathered directly from multispectral images, it will be possible to create a graphic representation in the form of spectral curves of each identified stain so that when a user seeks to identify a stain in a particular manuscript, an image can be processed and compared to the graphics held in the Library of Stains database. In this way we engage the scholarly community in an on-going collaboration resulting in the continual growth of the Library and in the open access data it creates. This is a new way for researchers, conservators, librarians, and the public to access important information and gain a greater appreciation of the material makeup of old books, their historical uses, and new approaches for modern studies.  We envision that scholarly audiences will use our data and methodology to advance knowledge into the provenance of manuscripts, their uses within a historical context, their working environment, their transmission, and their circulation. For conservators and librarians in particular new information will help determine proper storage conditions, as well as health and safety issues, in particular the identification of heavy metal contamination, such as mercury residue in alchemical manuscripts or herbaria.   Purewal (2012) has developed a UV-based methodology to identify visually the presence of mercury in herbaria.  For librarians and archivists, the results of this project will also deliver a heightened awareness of the value of interdisciplinary research and model for future collaborations that can create new content and context for rare book and special collections.  Finally, bringing together multispectral imaging experts and humanists offers an opportunity to explore and develop a working model for best practices when engaging in interdisciplinary collaboration that will actively gain the attention of public audiences. There is an enduring interest in medieval themes as a broad concept within the public sphere. Even if these themes are often caricatures or historically inaccurate, this interest in the medieval period in the public imagination offers the perfect opportunity to invite the public in to experience the academic discipline of medieval studies through an engaging and public-facing project. Accessibility to primary sources through an online database like the proposed library of stains juxtaposed with descriptive metadata will contextualize the project, connect with public interest, and provide value in the form of education. Our focus on public engagement is supported through the regular dissemination of information on the project to both public and scholarly communities through a variety of social media platforms, including facebook, twitter (#StainAlive), instagram, flickr, and a blog. With frequent posts across all formats, we hope to engage and excite both academic and public audiences interested in the medieval world and the lived experiences of medieval scribes, scholars, and readers. ",
       "article_title":"Labeculæ Vivæ. Building a Reference Library of Stains Found on Medieval Manuscripts with Multispectral Imaging",
       "authors":[
          {
             "given":"Heather",
             "family":"Wacha",
             "affiliation":[
                {
                   "original_name":"University of Wisconsin-Madison",
                   "normalized_name":"University of Wisconsin–Madison",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01y2jtd41",
                      "GRID":"grid.14003.36"
                   }
                }
             ]
          },
          {
             "given":"Alberto",
             "family":"Campagnolo",
             "affiliation":[
                {
                   "original_name":"Library of Congress, United States of America",
                   "normalized_name":"Library of Congress",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p405e02",
                      "GRID":"grid.241525.5"
                   }
                }
             ]
          },
          {
             "given":"Erin",
             "family":"Connelly",
             "affiliation":[
                {
                   "original_name":"University of Pennsylvania",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "GLAM: galleries",
          "museums",
          "linking and annotation",
          "libraries",
          "image processing",
          "visualisation",
          "archives",
          "English",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Starting in Buenos Aires with Eloísa Cartonera in 2003, Cartonera publishers emerged as a reaction to the over commercialization of the book industry and its ever-growing conglomerates. With their unique hand embellished covers and their peculiar aesthetics, these publishers have challenged how books and literature are produced and distributed. Their collective manual process is equal to the intellectual one, resulting in a more democratic mode of production.  For thirteen years, the Cartonera Publishers Database has been documenting and preserving the diverse initiatives that stem from these grassroots projects which use recycled cardboard as book covers. The database is comprised of more than 1,200 entries which include Dublin Core metadata, scanned images of the back and front covers, copyright pages, and title pages, and audio files of interviews of several members of Cartonera publishing houses. An electronic crosswalk connects these entries to local cataloging of the Cartonera Book Collection. The audio files and an online full-text book “Akademia Cartonera: A primer of Latin American Cartonera Publishers” are additionally indexed and marked using TEI. This database is the only digital reference tool on these multi-pronged publishing initiatives. The ultimate goal is to connect this locally focused digital humanities project with cartonera books held at other institutions around the world in an interinstitutional Cartonera Catalog. In the past year, I have been studying the possibility of using crowd sourcing and/or folksonomies to supplement the current content with the goal of providing a deeper understanding of the variety of contexts in which these books are created while also offering a space for the Cartonera publishers to contribute other content created directly by them. My proposed papers addresses the database and initial efforts to expand our work. ",
       "article_title":"Cartonera Publishers Database, documenting grassroots publishing initiatives",
       "authors":[
          {
             "given":"Paloma",
             "family":"Celis Carbajal",
             "affiliation":[
                {
                   "original_name":"University of Wisconsin-Madison, United States of America",
                   "normalized_name":"University of Wisconsin–Madison",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01y2jtd41",
                      "GRID":"grid.14003.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "databases & dbms",
          "spanish and spanish american studies",
          "Spanish",
          "metadata",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Chen Duxiu (1879-1942) co-founded the Chinese Communist Party in 1920, and served as its secretary general from 1921 to 1927. He was a prolific author and a cultural rebel whose writings transformed the intellectual and social landscape of 20 th century China. Yet from 1904 to about 1919, Chen advocated Western democracy and Social Darwinism as solutions to save China. His turn to communism was an abrupt transition, and many historians credited this to the influence of his colleague, and co-founder of the CCP, Li Dazhao (1888-1927). Both Li and Chen had studied in Japan, and through their interaction with Japanese Socialists and fellow students, became acquainted with literature on socialism and anarchism. Some say that Li was the theoretician who understood Bolshevism and Marxism in depth, while Chen did not become well-versed in Marxism until he founded the CCP (Yoshihiro, 2013).  In this paper, we applied topic modeling (Blei et al., 2012) to a select number of Chen’s and Li’s published articles, in an attempt to detect the difference, if any, in their interpretation on the subject of socialism, Marxism, communism and Bolshevism. We integrated two well-developed statistical methodologies, the Latent Dirichlet Allocation (LDA) and the Poisson Graphical Model (PGM), to probe in finer detail the broad themes in the 892 pieces of Chen’s essays, correspondences, and occasional poetry, comprising a total of 1,347,699 Chinese characters. Based on the word counts per topic, we then implemented the PGM method to study the association among different topics. The use of PGM minimizes any misleading inference caused by confounding variables, and it also leads to a more concise structure of the network of topics.  Specifically, we chose 263 articles written by Chen Duxiu and 53 written by Li Dazhao, containing words related to Marxism, socialism, Bolshevism, and communism (Ren, 2018; Li ,1984). (Both selections covered the length of the men’s publishing career; Chen passed way at age 63, while Li was executed at age 39). A document-term matrix (bag-of-words data) was generated from the preprocessed text. Next, we carefully selected a set of seed words for each of  K topics of interest. We then applied the topic modeling method LDA to the bag-of-words data to find the remaining mixtures of words associated with each topic. Consequently, we could interpret each estimated topic by abstracting the top ranking terms within that topic. We then generated a new document-topic matrix from the document-term matrix by calculating the counts of those top words from the same topic. Finally, we applied the Poisson Graphical Model to the document-topic matrix to infer the conditional independence between each pair of topics. The resulting graph is a network visualization where each node represents a topic, and each edge indicates the conditional dependencies among the topics, meaning the two topics that are linked by an edge are correlated even after adjusting for all the other topics in the corpus.  The results yield several initial observations: Chen used a smaller set of vocabulary words over and over again to emphasize a point, while Li adopted a more discursive style with fewer repeats of the same word. Chen used many more verbs (such as: “agitate,” “struggle,” “unite,” “lead,” “develop,” “carry out”), thereby exhorting his readers to action, while Li tended to use descriptive words. Chen focused on the present by analyzing different political groups: “Guomindang,” “warlords,” “proletariat,” “bourgeoisie,” “military,” “students,” “masses” and “imperialists.” Li painted a larger scenario by using words such as “world,” “humanity,” “philosophy,” “phenomenon,” “relationship,” “history” and “religion.” The general conclusion at this early stage of analysis is that Chen urged his readers to put into action his plans to bring China under communism, while Li tended to explain to his readers the nature of Bolshevism and Marxism. More interestingly, these calculations yielded “orbits” of vocabulary for each man’s important ideas. For instance, Chen’s use of the word “revolution” appeared three times in the 8 topics that we studied. In the first sub-topic, “revolution” appeared with words such as “class,” “bourgeoisie,” “proletariat,” “develop,” “strength,” and “movement.” In the second sub-topic, “revolution” again appeared alongside “peasants,” “bourgeoisie,” “proletariat,” “lead,” “China,” “masses,” “movement,” and “action.” In the third sub-topic, “revolution” appeared with “bourgeoisie,” “proletariat,” “struggle,” “China,” “Guomindang,” “movement.” Li, when he discussed “revolution,” which appeared twice in the four topics we studied, he often used words such as “people,” “Russia,” “movement,” “government,” “masses,” “future,” and “China.” While the general trend of these two men’s writing is clear by a casual browsing of all of these articles, but this method of calculation demonstrates in a quantitative manner the qualitative interdependence of topics, and diagrams in an easy to read manner the network configuration of the vocabulary of each man.  ",
       "article_title":"Integrating Latent Dirichlet Allocation and Poisson Graphical Model: A Deep Dive into the Writings of Chen Duxiu, Co-Founder of the Chinese Communist Party.",
       "authors":[
          {
             "given":"Anne Shen",
             "family":"Chao",
             "affiliation":[
                {
                   "original_name":"Rice University, United States of America",
                   "normalized_name":"Rice University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/008zs3103",
                      "GRID":"grid.21940.3e"
                   }
                }
             ]
          },
          {
             "given":"Qiwei",
             "family":"Li",
             "affiliation":[
                {
                   "original_name":"University of Texas Southwestern Medical Center",
                   "normalized_name":"Southwestern Medical Center",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00t9vx427",
                      "GRID":"grid.416214.4"
                   }
                }
             ]
          },
          {
             "given":"Zhandong",
             "family":"Liu",
             "affiliation":[
                {
                   "original_name":"Baylor College of Medicine",
                   "normalized_name":"Baylor College of Medicine",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02pttbw34",
                      "GRID":"grid.39382.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "graphs",
          "networks",
          "historical studies",
          "multilingual / multicultural approaches",
          "text analysis",
          "relationships",
          "English",
          "asian studies",
          "content analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In contemporary anthropology, nearly all of us work with sound – usually oral interviews – but its quality as such is often taken for granted. Audio files of interviews are often quickly transcribed or qualitatively coded into text, then analyzed and written into books. And the soundscapes of our fieldwork sites are often taken for granted as well. Their meanings and textures as sounds are thus erased. The small sounds of voices and places often invoke an intimacy that anthropologists may attempt to render in text through “thick description” (Geertz 1973) and hopefully also “sincerity” (Jackson 2005), drawing from hermeneutic and poetic approaches in literary studies (Clifford and Marcus 1986, Behar and Gordon 1995).  Meanwhile, a growing body of interdisciplinary scholarship on sound studies foregrounds sound as “a modality of knowing and being in the world,” of creating a sense of place or a narrative (Feld 2000). Performance studies scholars have also provided many contributions towards thinking about “the hegemony of textuality” (Conquergood 2002: 147) and, conversely, the “repertoire” of manifestations of knowledge and memory that exist outside the written, institutionalized archive (Taylor 2003). Needless to say, ontologies, storytelling, memories, and place- and identity- making are canonical topics of study in anthropology. As Steven Feld, an anthropologist and one of the leading theorists of sound studies, has discussed, there are many possibilities in “doing ethnography through sound—listening, recording, editing, and representation” that will hopefully one day be more than just “mostly about words” (Feld and Brenneis 2004: 461, 471). Further, as anthropologist and sound studies theorist, Roshanak Kheshti, has argued: “considering sound through the critical genealogy of feminist or race theory forces you to move beyond sound as an object and think of sound instead as an analytic or a hermeneutical tool for understanding inequality…” and the “social worlds” that scholars study (Brooks and Kheshti 2011: 330). I am interested in approaches to methods, ethics and accessibility when working with the sounds of voices that cross-cut anthropology – specifically sensory ethnography, or ethnographic methods that foreground the senses – sound studies (and sound arts), and digital humanities. Anthropologists are not that common in the realm of digital humanities. However, many of us, one could argue, do projects that could be construed as “digital humanities,” that is: “digital methods of research that engage humanities topics in their materials and/or interpret the results of digital tools with a humanities lens” (Lexicon of DH Workshop, The Graduate Center Digital Initiatives, tinyurl.com/lexicondh). And the thing with DH is, once we (scholars) start paying more acute attention to the ways in which our research is digital this can open up new questions and also new methods for doing what it is that we do, in terms of both research and pedagogy. This is particularly true, I suggest, for sound studies – given the importance of digital tools and platforms for recording, mixing, sharing and listening to audio.  Yet, new methods, digital tools and projects emerging through DH and internet research in general open up an array of rather new ethical and accessibility concerns (see e.g. Barnes 2006, Markham and Buchanon 2012). What constitutes personhood or “human subjects” on the internet? What data is or should be “public”? When should consent protocols be required? Can images or audio files of people and their voices bely anonymity? Who has access to make digital projects or to engage them, particularly in relation to differences of class, ability, and language fluency? How is the internet – its structure, its users, its algorithms – racialized and gendered (e.g. McPherson 2012, Noble 2018)? In what ways may some DH projects follow a practice of extraction without reciprocity? Indeed, anthropologists wrestle a lot with that last question in particular when extracting stories of individuals that then advance our careers, while many DH-ers may be, e.g., web scraping. This short paper presentation will examine the possibilities of cross-cutting methodological approaches to anthropology, sound studies and arts, and digital humanities, specifically when recording and sharing the sounds of peoples’ as a mode of storytelling. I will focus on oral interviews in particular. Driven by the aforementioned anthropological and interdisciplinary concerns, this paper will discuss the interplays of method and theory when cross-cutting these approaches, and issues of ethics and accessibility when recording and sharing sound. This includes being wary of institutional compliance with Institutional Review Boards but also following a feminist ethics beyond compliance, that, for example, foregrounds consent as not a one-time signature but reiterated, negotiated and subject to change (see Davis and Craven 2016). I will also consider various levels of intrusiveness and impact that the recording and sharing of the sounds – especially the sounds of peoples’ voices – may have, and the potential roles of shared sounds within larger networks of listeners and what their availability may foreclose (e.g. Sugarman 1997, Brooks and Kheshti 2011, Kunreuther 2014, Kheshti 2015). Lastly, I will discuss digital modalities for sharing research with sound and their (limited) possibilities for storytelling, specifically for doing and sharing anthropological and other research in a more accessible form – with the exceptions structured by access to technology, limited hearing ability and translatability across languages and contexts. I will highlight free and open-source resources, such as sound archives and editing and hosting technologies, as well as low-cost Do-It-Yourself (DIY) microphones and speakers.  While websites are often great platforms for sharing oral history projects and other sounds, I will also discuss examples of other modalities for sharing sounds, such as exhibits and events, as well as digital platforms for scholarly publishing (e.g. Manifold). I will include a brief survey of various free online platforms that seem to have high potential for use in scholarship and pedagogy. These include: the SoundCloud online streaming platform, the Oral History Metadata Synchronizer in coordination with Omeka, podcasting via iTunes, StoryMaps for sharing audio on a map, and Chirbit for sharing audio on social media or embedding audio on a website. I will also discuss examples for the in-person sharing of sounds during, for example, an exhibit or class, including a brief survey of different kinds of speakers and headphones and different ways of transferring pre-recorded or live sounds to them, as well as spatial considerations for sharing sound. For example, placing numerous speakers inside an enclosed space, such as a tent, may allow for a focused listening space that is still shared and not as individuated as when using headphones (an idea I learned from sound artist Grant Smith of Reveil Radio in London). While I do not plan to conduct a full comparative analysis of these platforms, I will briefly discuss what I find to be some of openings and limitations of each. In sum, this presentation aims to bridge together a number of themes: sound studies, oral histories, ethics, accessibility, and modalities for sharing sounds. I emphasize the intention that motivates my attempt to bridge these various themes: In my opinion, when recording and sharing human voices the researcher must always be vigilant in their ethical considerations (beyond IRB approval) at every step of the research design and practice, and then the sharing of these sounds is what makes their collection most worthwhile and to do so requires considerations of accessibility and modalities and ethics for such sharing.  ",
       "article_title":"Sensory Ethnography and Storytelling with the Sounds of Voices: Methods, Ethics and Accessibility",
       "authors":[
          {
             "given":"Kelsey Marie",
             "family":"Chatlosh",
             "affiliation":[
                {
                   "original_name":"The Graduate Center, CUNY, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "folklore and oral history",
          "audio",
          "knowledge representation",
          "digital humanities history",
          "cultural studies",
          "video",
          "feminist studies",
          "theory",
          "epistemology",
          "anthropology",
          "English",
          "criticism",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" We present the Scholar Index: a platform to index the literature and primary sources of the arts and humanities through citations. These resources are becoming increasingly digital, thanks in part to digitization campaigns and a shift towards digital publishing. Nevertheless, the coverage of commercial citation indexes is still poor and mostly limited to publications in English (Mongeon and Paul-Hus, 2015). This situation results in an untapped opportunity, as the literature refers to a wealth of primary sources from institutions such as libraries, archives and museums (Knievel and Kellsey, 2005). As a consequence, a comprehensive indexing of its citations would constitute a unique opportunity to greatly enhance the search capacities of scholars and interconnect collections currently set apart. The Scholar Index integrates a pipeline to extract citations from scholarly literature in the arts and humanities, along with two interfaces: a digital library (Scholar Library) and a citation index (Scholar Index proper). The prototype Venice Scholar is presented, covering the literature on the history of Venice and currently indexing nearly 3000 volumes of scholarship from the mid 19 th century to 2013, from which some 4 million references were extracted. The full citation indexing allowed us for the first time to highlight trends in the large-scale use of archival evidence and scholarly literature made by historians over such a substantial span of time (Colavizza, 2017a, b). We finally argue that a collaborative approach to the indexing of the literature and primary sources of humanists is feasible and would allow to greatly broaden and enrich access to the documentary cultural heritage at large.   Approach   The process of mining citations from digital or digitized scholarly publications entails a set of steps, as sketched below. First of all, a corpus needs to be selected and digitally acquired (including its full-text via OCR). Secondly, the literature needs to be analysed in order to grasp the location of references (typically in footnotes), and the presence of trends in the style of references. These insights can inform a selection of publications to be manually annotated in order to improve the quality of the subsequent automated extraction.   Citation mining can be divided into two tasks: parsing and extraction of references – or the identification of text segments containing a reference to a source – and their disambiguation – or the association of a reference to the unique identifier of the referred source. Having done this, a citation is represented as a relation between a citing publication and a cited source. During parsing, pre-trained text classifiers are used, possibly with adaptation to the domain at hand. During disambiguation, external repositories such as catalogues are optionally queried to establish interlinks. Lastly, citation data can be exposed for a variety of purposes, including search and browsing in a dedicated interface. For more details and evaluations see (Colavizza and Romanello, 2017; Colavizza et al., 2017). The Venice Scholar  This approach has been applied to create a prototype on the historiography on Venice. There is a sheer amount of literature on Venice, even just considering modern historiography from the 19 th century onwards (Dursteler, 2013). We selected the corpus using a variety of means available in research libraries (Colavizza et al., 2017). Once a first seed of literature had been digitized, we proceeded to further expand it with highly cited, usually old sources, as well as very recent ones. The corpus currently counts over 3000 volumes, circa 20% journal issues and 80% books. This effort has been made possible thanks to the support of several research libraries in Venice.   For the list of partners see:   https://dhlab.epfl.ch/page-127959-en.html .    The resulting data has been published in open access: circa 40,000 annotated references used to train reference parsers (Colavizza and Romanello, 2017), while citation data from nearly 4 million extracted references is gradually being ingested into OpenCitations (Peroni et al., 2017), a repository of open citations data.   See also   https://opencitations.wordpress.com/2018/03/23/early-adopters-of-the-opencitations-data-model/ .     The platform  The digital library and the citation index are connected through citations. The interfaces are accessible online.   The (Venice) Scholar Index can be accessed at:   www.venicescholar.eu , the (Venice) Scholar Library can be accessed at:   www.venicescholar.eu/library . Try searching for historian “Patricia Fortini Brown”, for example. The project’s website is at:   www.scholarindex.eu .   The digital library provides access to the digitized materials, and points to the index through disambiguated references, as shown below.   Viewer (above): allows to read a publication with image and text side by side. This is particularly important in order to appreciate the quality of the OCR.      Text view (left): allows the user to search within the full-text of a publication, highlighting all extracted references and links to the relative entries in the index.  The citation index provides instead no access to full-contents – due to reasons of copyright – but allows for the exploration of the network of citations.   Search results (left): citation data is aggregated per author, publication or primary source, with full-text access to the text of extracted references. Search results are conveyed along with their relevant citation information (citations made and received, publications for an author). Authors are linked to the Virtual International Authority File (VIAF) repository, whenever possible.       Citation timeline (left): every aggregated entity has a dedicated page with a timeline of citations (made and received), and a list of relevant sources.      Citations to primary sources (left): the index also links to external collections of primary sources, in this case documentation at the Archive of Venice. Citations to any level of the archival hierarchy are provided, following its structure. The user can easily move from a publication to a document series and see all publications which referred to it, over time.  The platform is thus able to aggregate citation data from many library collections into a unique system, allowing users to not only navigate the resulting network, but also have improved access to collections of primary sources such as archives. Citation coverage of the Archive of Venice  The Venice Scholar allowed for the first time to analyse the use made by historians of the vast Archive of Venice over almost 200 years of scholarship. The Archive hosts an estimate 80 linear km of documents and is the main reference for the history of the city. We extracted 157,575 citations to 600 distinct record groups (record groups or smaller series of documents within). Two patterns readily emerge: first, the use of documentary records is highly skewed with few record groups accruing most citations; second, the archival indexation of the records through metadata is key for their discoverability by historians.   The 600 cited record groups vary from one with almost 10,000 received citations to many having been cited only once. The cumulative distribution of citations suggests the presence of a power law ( above: the inset plot shows the log-log cumulative sum resulting in an almost straight line).  The Archive possess 14,233 record groups indexed in its information system, meaning only 4,2% of these have been cited from our corpus. Within these record groups, only 25,7% possess information regarding their size in linear meters, which is usually missing for records not yet properly inventoried. Yet 78,5% of cited record groups possess size information: a strong indication of the importance of archival indexation for access. A total of only 18,37 linear km are known to the information system, and of these 64,3% are cited at least once at the aggregated group level. In conclusion, a proportion of ~4% (by archival identifier) to ~15% (by size, (18,37*0.643)/80) of the record groups of the Archive of Venice has been cited from our corpus, and with few aggregates getting most citations: we might conclude that there is still much to explore at the Archive of Venice.     Towards a global citation index for the Arts and Humanities We believe that the approach used for Venice could be applied to many more collections of scholarly literature, from a variety of libraries. Much in the same way national or international library catalogues are collaboratively created, every library part of the system could take responsibility for an area of scholarship of its interest. This would entail for the library to be in charge for digitization. Once done, the platform would proceed to OCR and mine citations, in view of their federation into a single citation index ( left). The library could also be responsible for the quality of the so provided citation data, by running regular evaluation and correction campaigns according to its resources. A daunting volume of work would thus be divided into more manageable chunks, and possibly distributed among several players.  The expansion of digital informational ecosystems promises to greatly impact the work of humanists. Besides providing for more rapid access, digital indexing might also make information retrieval a richer experience. Towards this end, we proposed the Scholar Index: an approach to use citations contained in the scholarly literature to index and interlink collections of primary and secondary sources. We believe that our approach has the merit of being able to scale, by catalysing the joint efforts of knowledge institutions towards a common goal, as was shown for the Venice Scholar prototype. ",
       "article_title":"Linked Books: Towards a collaborative citation index for the Arts and Humanities",
       "authors":[
          {
             "given":"Giovanni",
             "family":"Colavizza",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory; The Alan Turing Institute; Odoma Sàrl",
                   "normalized_name":"The Alan Turing Institute",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/035dkdb55",
                      "GRID":"grid.499548.d"
                   }
                }
             ]
          },
          {
             "given":"Matteo",
             "family":"Romanello",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory; Odoma Sàrl",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Martina",
             "family":"Babetto",
             "affiliation":[
                {
                   "original_name":"Tate Britain",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Vincent",
             "family":"Barbay",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Laurent",
             "family":"Bolli",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory; Odoma Sàrl",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Silvia",
             "family":"Ferronato",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"École Polytechnique Fédérale de Lausanne, Digital Humanities Laboratory",
                   "normalized_name":"École Polytechnique Fédérale de Lausanne",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s376052",
                      "GRID":"grid.5333.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "repositories",
          "bibliographic methods / textual studies",
          "sustainability and preservation",
          "information retrieval",
          "historical studies",
          "library & information science",
          "archives",
          "English",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This Digital Humanities project is an interdisciplinary project effort that uses the lens of, and data from, the U.S. TV show  Seinfeld to explore questions about television and other media.  Seinfeld  has significant cultural influence over other media, but what is its  reach, meaning the many other media items cast and crew worked on, also known as the  overlap? We are starting with data from the Internet Movie Database (IMDb). This makes this project somewhat different from other Digital Humanities projects as we’re using an existing database rather than primary sources. An associate professor of media studies, accustomed to conducting critical analysis of television shows, and an associate professor of information systems, more used to working with non-media studies data, are working to populate a relational database, to use quantitative analysis, and a social science theory--social network theory, particularly “Small Worlds” theory--to explain trends in media industries, including questions of genre, gender, race, and age in entertainment businesses.   Seinfeld (NBC 1989-1998) was a US-based half-hour, multi-camera, situation comedy, one of several that featured stand-up comics in stories similar to their own lives. Although it ended nearly 20 years ago, it   heavily influences TV shows of today, including “hangout” sitcoms, one-camera comedies featuring conversation and digression, and antihero dramas. Journalist Jennifer Keishen Armstrong writes in the bestselling Seinfeldia that the show “snuck through the network system to become a hit that changed TV’s most cherished rules; from then on, antiheroes would rise to prominence, unique voices would invade the airwaves, and the creative forces behind shows would often gain as much power and fame as the faces in front of the cameras” (Armstrong, 2016). It’s a singularly important show for a variety of reasons.   Clearly,  Seinfeld has significant cultural impact on other shows and movies, but what we wanted to know is, what is its ’reach’? Reach is defined as other media that texts cast and crew from  Seinfeld  worked on before, during, and after their appearance(s) on the show. Such texts exist in every media type (movies, video games, web-based media). When two media items share cast/crew, we look for overlap.   Dr. Conaway worked on the project for two years, using cut and paste and Excel spreadsheets for items and people, before involving Dr. Shichtman, who has created a relational database that may be searched. We first used MySql and an Amazon Web Services server, have recently shifted to the college's virtual machine and the Oracle database management system. We involved two students in a grant funded practicum in the Fall term as well.   Our research revealed that the 1551 cast/crew had worked on over 32,500 other discrete media texts, starting in 1936, and with many texts still on the air today, often with an overlap of more than one. Nearly every televison series, TV movie, and TV special we could think of included overlap. Only recently, in “peak TV”—in which there are over 500 scripted TV shows in production this year alone, in addition to reality, sports, and news shows (many of which also have overlap)—are we seeing well-known US TV series with no overlap. Our research found that although most were US-based, there were media items from over 60 countries.   Social network theory would help us answer some questions. As Duncan Watts writes in  Six Degrees: the Science of a Connected Age , \"Affiliation networks . . . are . . . networks of overlapping cliques, locked together via the comembership of individuals in multiple groups\" (Watts, 2004). Small worlds theory discusses how networks of people influence each other, and each others’ connections.    Questions include, what genres did the cast/crew, presumably chosen for a common comic sensibility, work on other than comedy? What genres included the most cast/crew? What genres have less overlap, none at all, and what might be some reasons for that? What is the importance of gender, race, and age?   We looked for other, similar projects that used IMDb and found that there were few that did. Some computer scientists had used IMDb to trace the overlaps among actors involved in ’adult‘ films in the database as an example of a ’small world‘ environment. Media History scholars had traced ’race films‘ that ended before our database started, and Digital Humanities scholars used it to look at patterns of exhibition of films or specifically how Australians worked together, but not to examine how cast circulated among media.  IMDb, it turns, out, is a challenging tool for this purpose.  Deb Verhoeven, Associate Dean of Engagement and Innovation of the University of Technology Sidney, who has done a lot of Digital Humanities work on Australian films explained in 2012 that IMDb consists of “elaborated sets of lists” created by fans, writing:  Accordingly, the primary users of filmographic catalogues are not cinema historians, information managers, analytical filmographers, or cinema scholars, but members of the public, film buffs, students and so on who are content to navigate these databases using the small number of structured search fields provided. (Verhoeven, 2012)  IMDb, which started in the early 1990s, is very robust, and provides information for free download using Python, but is not usable ’as is.’ Entries may be misleading, incomplete, or unclear, with genres in particular organized in unhelpful ways. The Downloadable information includes the full cast and some types of crew members, but not others. In addition, the fields of the two faculty members made shared vocabulary difficult, and getting complete and clean data that could be turned into tables and graphs meant conducting additional research outside of IMDb, and reorganizing the data significantly from the way Dr. Conaway initially tagged it. SUNY Empire State College also lacks the structures that many institutions have for conducting Digital Humanities work. However, we have been able to create some early data visualizations that will show a microcosm of how the US entertainment industry works for various types of actors and crew members, using specifically the data from television programs. We’ve compared  Seinfeld’s numbers of actors and crew to that of other shows, analyzed how the media items break down by genre, and visualized how women’s careers wax and wane in different patterns from men’s careers. In the future we will do the same for subgenres, actors of color, and actors of various age groups.   ",
       "article_title":"Seinfeld at The Nexus of the Universe: Using IMDb Data and Social Network Theory to Create a Digital Humanities Project",
       "authors":[
          {
             "given":"Cindy",
             "family":"Conaway",
             "affiliation":[
                {
                   "original_name":"SUNY Empire State College, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Diane",
             "family":"Shichtman",
             "affiliation":[
                {
                   "original_name":"SUNY Empire State College, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "databases & dbms",
          "knowledge representation",
          "film and media studies",
          "cultural studies",
          "English",
          "content analysis",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Though quantitative methods are becoming increasingly common within the humanities, few researchers readily describe their primary texts as data. Most prefer to see their objects of study as contextually situated and socially constructed entities with independent value that resist complete digital representation. Miriam Posner argues that for many humanities researchers, describing an artifact as data implies “that it exists in discrete, fungible units; that it is computationally tractable; that its meaningful qualities can be enumerated in a finite list; and that someone else performing the same operations on the same data will come up with the same results.” Defined this way, digital artifacts and metadata seem to simultaneously insist on particular interpretations and to be bereft of deeper meaning outside of an aggregate state, thereby resisting the hermeneutic methodologies which form the core of humanistic inquiry.  This position stems from understanding data primarily through a big data mindset. As corporations, governments, and universities have increasingly addressed business problems by embracing data analytics, the essential qualities of big data (large volume, high velocity, and heterogenous variety) have created the illusion among many that such datasets can perfectly model an imperfect and unpredictable world, gaining credibility simply by increasing in volume. The computational authority of big data is persuasive because it presents a seemingly objective, number-driven way of knowing reality – an epistemology of the database, predicated on scale, comprehensiveness, and reproducibility. While an immense and complete archive possesses an undeniable allure (Manovich, 2012; Kaplan, 2015), there is still value in examining individual records and investigating the intangible stories and datapoints that hide in database gaps or reside outside of databases entirely. I use Cheryl Ball et al’s term “boutique data” to emphasize the ongoing importance of small, localized, partial, and qualitative datasets to the humanities research process. I frame boutique data as both a thing (a boutique dataset) and a theoretical approach to data-intensive work in the humanities. While big data are often automatically generated, boutique data are manually curated – subjective, created  capta as opposed to given data (Drucker, 2011). Big data hides the work and decisions that drive data processing, while boutique data foregrounds the hidden labor and assumptions that shape data. Big data fits information into a predetermined mold, while boutique data models are built from the bottom up. Where a big data mindset treats gaps in data coverage as a corrupting null to be fixed, a boutique approach to data sees these gaps not as empty voids but as evocative absences worth further investigation. In this presentation, I will examine both the successes and failures of a boutique approach to data through a case study of  Laboring-Class Poets Online and speculate about possible future improvements to the project.   The texts and histories studied by scholars of laboring-class culture are riddled with gaps. Since the publication of E. P. Thompson’s  The Making of the English Working Class over fifty years ago, researchers have increasingly viewed laboring-class poets and their writing as subjects worthy of scholarly inquiry. Rather than portraying proletarian writers as isolated anomalies or novelties, such as how George Thomson characterized Robert Burns as a “heav’n taught ploughman” in his famous obituary for the Scottish bard, modern critics acknowledge that working-class writing was a significant, widespread phenomenon. However, while some British laboring-class poets such as Burns or John Clare have achieved near-canonical status, most of these writers are still obscure figures. Information on their lives and access to their writing remains scarce and scattered, hindering research on both their personal histories and their poetry.   Laboring-Class Poets Online ( LCPO) addresses this gap by aggregating biographical and bibliographical information about the more than 2,000 British laboring-class poets who published between 1700 and 1900 and the texts they produced.  LCPO draws on collaborative research initially collected by an international distributed team of researchers over several decades and presented as biographical entries in  A Database of British and Irish Labouring-Class Poets and Poetry. LCPO transforms these freeform biographical snippets into structured, web-accessible records. This structure facilitates a prosopographic approach to British working-class literary studies. Lawrence Stone defines prosopography as  “the investigation of the common background characteristics of a group of actors in history by means of a collective study of their lives.” This methodological shift from the study of individual biographies to collective biographical and bibliographic patterns enables a more comprehensive understanding of laboring-class literary production at a time of great social and economic change. Users can ask questions about laboring-class literature holistically and map trends and themes, including the impact of industrialization; the role of religion as a vehicle for literacy and a source of aesthetic influence; the tension between increased urbanization and a celebration of regional identity, often demonstrated through writing in dialect; the transformation of the publishing industry and the role of patronage and subscription publishing; the growth of literary miscellanies and magazine publishing; and the influence of organized labor movements (e.g., Chartism or Christian Socialism) on laboring-class artistic expression. Scholars can  investigate emigration patterns, education level, labor engagement, health outcomes, poet occupations, and interactions with the criminal justice and social relief systems. Publications can similarly be filtered and searched by typical facets such as publication date, author, or location, but also by subscription lists, patronage, cost, or print run size.  Users can interact with aggregate data through numerous data visualizations including geographic maps that show poet and publication locations; timelines of individual lives or major events which shaped the working classes; and network graphs that display connections between writers based on correspondence, personal relationships, or literary influence. Each of these visual forms encourages users to shuttle back and forth between individual records and aggregate analysis. Users can also create collections of content for further interpretation and analysis, correct mistakes in poet entries, or contribute new data to the website. All data presented through  Laboring-Class Poets Online are freely available for download or access via a REST API.  This information is vital for scholars of working-class writing and culture, but it is also an instance of boutique humanities data (capta): a collaboratively and manually created and curated small dataset of several thousand entities extracted during ongoing research. While scholars often use context to interpret data points in historical documents, databases and computational methods typically lack this capability. Uncertainty is embedded in historical sources, but databases often strip away ambiguity to perform the computational functions that make their use worthwhile. By taking a boutique approach to historical and literary information,  LCPO retains much of this ambiguity and offers insight into how humanities researchers can accommodate a complex understanding of space and time as continuously unfolding events.   ",
       "article_title":"Exploring Big and Boutique Data through Laboring-Class Poets Online",
       "authors":[
          {
             "given":"Cole Daniel",
             "family":"Crawford",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "technologies",
          "graphs",
          "geohumanities; spatial & spatio-temporal analysis",
          "databases & dbms",
          "digital humanities history",
          "modeling",
          "networks",
          "english studies",
          "theory",
          "visualization",
          "relationships",
          "English",
          "epistemology",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Scholars are producing and using 3D content more than ever due the advancement and availability of 3D technology. How is this 3D content and its metadata being captured, disseminated, and preserved? How is this digital scholarship being made available and discoverable for pedagogical and research purposes?  Although there is great interest in 3D applications in research, there is currently little available guidance regarding the preservation of digital objects and associated information in perpetuity. The preservation and sharing of research data is a necessary, invaluable responsibility of libraries, museums, and other cultural heritage institutions, and although standards and best practices have been developed for many kinds of digital data to ensure assets can be accessed and reused in perpetuity, the applicability of these standards to 3D data is limited. Building off the discoveries made during the 2015/2016 NEH Advanced Challenges in Theory and Practice in 3D Modeling of Cultural Heritage Sites, this paper explores one of the main threads of discussion throughout the NEH Summer Institute: research longevity and publication. Underpinning the issue was concerns of the preservation of 3D data and their overall discoverability and (re)use beyond their creation.  This paper investigates the current state of existing standards and schemas for 3D data and explores what more needs to be done (and is being done) by practitioners, librarians and curators to ensure that this digital content is preserved and disseminated, enabling further humanistic inquiry and advancing scholarship of our shared cultural heritage. In 2017 the Institute for Museum and Library Services received several proposals regarding the advancement of 3D research and support. Two of these grants were funded which are working in tandem to discuss issues related to 3D and virtual reality, and preservation and best practices for 3D data curation. This paper will focus on the developments regarding the latter IMLS grant - the Community Standards for 3D Data Preservation (CS3DP). According to the CS3DP grant proposal (Moore et.al., 2017): The project team surveyed an international community including individuals involved in digital curation and 3D data acquisition and research, primarily at universities and museums. Of 104 respondents 70% said that they did not use best practices or standards for preservation, documentation, and dissemination of 3D data. Of those not using standards/best practices, 69% said that they did not use them because they were unaware of such standards. In order to respond to the lack of consensus around 3D data standards, the grant team will develop “a community-developed plan to move 3D preservation forward [and] recommendations for standards and best practices” for data creators and preservation specialists alike (Moore et. al., 2017). By the time of the 2018 DH conference, the CS3DP grant will have convened around 70 data creators and professionals to address the issues of 3D data preservation. This paper will report on initial findings and ongoing discussions and areas of work, as well as solicit feedback from the DH conference goers about other areas of concern, development and needs. ",
       "article_title":"Organizing communities of practice for shared standards for 3D data preservation",
       "authors":[
          {
             "given":"Lynn",
             "family":"Cunningham",
             "affiliation":[
                {
                   "original_name":"University of California Berkeley, United States of America",
                   "normalized_name":"University of California, Berkeley",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an7q238",
                      "GRID":"grid.47840.3f"
                   }
                }
             ]
          },
          {
             "given":"Hannah",
             "family":"Scates-Kettler",
             "affiliation":[
                {
                   "original_name":"University of Iowa",
                   "normalized_name":"University of Iowa",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/036jqmy94",
                      "GRID":"grid.214572.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-28",
       "keywords":[
          "3D printing",
          "archaeology",
          "repositories",
          "sustainability and preservation",
          "art and art history",
          "library & information science",
          "archives",
          "metadata",
          "English",
          "standards and interoperability"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction      The Center for Digital Research in the Humanities (CDRH) at the University of Nebraska–Lincoln is home to digital collections such as  The Walt Whitman Archive,  The Willa Cather Archive,  The Journals of Lewis and Clark, and  O Say Can You See. These projects contain overlap between subjects, individuals, and locations, yet are siloed, and many are built in aging, unsupported technologies with no interoperability or common search. In order to address this, the Center has developed an API (“Henbit”) as part of a modular software stack to index and display data and content.    Challenge Over the past twenty years, the Center has created over 30,000 TEI files in addition to other data sets such as VRACore documents, spreadsheets, and databases. Sites showcase the content and metadata of these files using a variety of technologies, many of which are no longer maintained. In addition, some sites used commercial software which became unsustainable when costs went up, cementing a commitment to open source. This experience informed and reinforced our adopted design philosophy, which can be summed up as:  Keep it simple, stable, and sustainable Embrace modularity by writing software for one purpose Avoid over-engineering solutions (i.e. graphical interfaces where command-line will do) Provide comprehensive documentation The Center has been inspired to think bigger about what can be accomplished by including existing data in a new framework. An exciting next step is creating a site to search all Center data, find commonalities between projects, and read materials across sites for comprehensive research. This approach will also help solve accessibility issues of older project sites which do not meet modern requirements. As projects become unsustainable, the Center may retire them while keeping all content available. While having one place to view and search the Center’s data is important, it’s also critical to allow the creation of independent sites which utilize unique organization and include special features requested by principal investigators for new and evolving projects. Quickly creating bare bones sites to view in-progress TEI is essential, as it allows metadata experts and PIs to refine their data and arguments. Such sites should be written for ease of maintenance, freeing future developer time to work on new projects rather than sustaining old ones.    Solution The Center explored the possibility of using existing software to address these challenges, such as XTF, Blacklight, and Fedora. These packages did not fit the Center’s needs; though comprehensive, they were not flexible enough to accommodate the variety of document types and project site requirements. Additionally, many solutions would lock the API into using Solr instead of allowing an interchangeable search engine (Blacklight, 2017; DuraSpace, 2017). Instead of heavily customizing existing software, The Center decided to create a modular solution. The system consists of several components:  data repository for project files and scripts for transformation document datastore and search engine (Elasticsearch) Ruby on Rails (Rails) API to serve data (Henbit) media retrieval system for associated images, audio, and video template generator for rapid website creation (Orchid) With a modular software stack, future changes in technology and project needs can be accommodated with independent upgrades rather than massive redesigns and rewrites.      Project Files and Scripts     The data repository houses original files for projects, such as TEI-XML, VRACore, CSV, and Dublin Core. The repository also contains CLI scripts which create HTML and populate search indexes with document content and metadata (CDRH, 2017a). New projects use generalized scripts, which are organized to allow overriding functionality in individual projects. Older websites may continue to use existing XSLT and populate legacy Solr indexes while their existing sites are supported, as well as populate Elasticsearch using the standardized script. Static HTML files derived from this process are used to create a document which can be viewed in a browser, regardless of the original data format.         Henbit (Public Endpoint)     Henbit is a Rails powered API (application program interface) which creates appropriate requests for the backend index, and returns JSON. Currently, Henbit uses Elasticsearch as a backend, but most of its features (sorting, filtering, aggregating on ranges, etc) could be ported to a different backend. The OpenAPI specification was used during Henbit's creation to fit current design practices (CDRH, 2017b).   Media Retrieval     In legacy sites, associated media lived inside the website directory. The Center has created a standard URL path for media files. It will be easier to optimize serving specific file types with this common retrieval structure. In the near future, the CDRH will be implementing a IIIF image server to serve images of varying sizes and resolutions.   Orchid (Rails Engine)     Orchid is a Rails engine which connects Rails 5 applications and Henbit. Orchid and a supporting gem, api_bridge, provide a template website that allows users to browse, search, filter, and view documents. This template is highly customizable, and can be altered to allow different URLs, search behavior, and anything possible in Rails (CDRH, 2017c).   Current Implementation and Future Plans     Beta versions of all components were released in 2017. In late 2017 the framework was used to build  The Complete Letters of Willa Cather (launched January 2018).  The Complete Letters demonstrates the customization which can be accomplished with this modular system. The CDRH is currently developing another project,  Family Letters, which will also take advantage of the data repositories, scripts, Henbit, and Orchid template.      In the meantime, older websites are being converted for the new system. Updated documents and original XSLT have been reorganized into the structure required by the data repository scripts and are being posted to the Elasticsearch index. Once a site for Centerwide projects has been created, older sites can be retired as needed, replaced by content now available through the new API and supporting website.     The decision to use custom built software rather than an existing, out of the box solution, was not easy. Though at times it felt like reinventing the wheel, our highly customizable and flexible implementation prepares for future technological developments and enables flexibility in meeting project requirements.   Notes    https://cdrh.unl.edu     http://whitmanarchive.org ,   http://cather.unl.edu ,   https://lewisandclarkjournals.unl.edu , and   http://earlywashingtondc.org     https://xtf.cdlib.org ,   http://projectblacklight.org , and   http://fedorarepository.org     https://github.com/CDRH/data     https://github.com/CDRH/api     https://github.com/OAI/OpenAPI-Specification     http://iiif.io     https://github.com/CDRH/orchid     https://github.com/CDRH/api_bridge     http://cather.unl.edu/letters     ",
       "article_title":"Legacy No Longer: Designing Sustainable Systems for Website Development",
       "authors":[
          {
             "given":"Karin",
             "family":"Dalziel",
             "affiliation":[
                {
                   "original_name":"University of Nebraska–Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Jessica",
             "family":"Dussault",
             "affiliation":[
                {
                   "original_name":"University of Nebraska–Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Gregory",
             "family":"Tunink",
             "affiliation":[
                {
                   "original_name":"University of Nebraska–Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "computer science",
          "repositories",
          "sustainability and preservation",
          "information retrieval",
          "software design and development",
          "library & information science",
          "archives",
          "English",
          "interface & user experience design/publishing & delivery systems/user studies/user needs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What does religion in the United States sound like, and where should one go to listen for it? What are the different ways that religious individuals and communities make themselves heard--to each other, to their gods, and to others? How is religious pluralism reshaping the sounds and spaces of North American religious life? How might we begin to reconceptualize religion and its place in North American life if we begin by using our auditory perception as a source of knowledge? And how might this knowledge be represented and transformed through the use of new digital media?  I co-direct “The American Religious Sounds Project,” a collaborative initiative of Ohio State and Michigan State Universities to leverage opportunities afforded by the new digital environment to consider what religion sounds like in the United States. The project centers on (1) the construction of a unique sonic archive, documenting the diversity of everyday American religious life through newly produced field recordings, interviews, oral histories, and related materials; and (2) the development of a new digital platform and website, which draws on materials in our archive to engage users in telling new stories about religious diversity in the U.S. This multi-modal platform includes a searchable archive, database-driven visualizations, which invite users to explore, discover, and listen for surprising connections among our materials, and a curated gallery of multimedia exhibits, which allow for greater interpretation and contextualization. Future phases include plans for museum installations, traveling exhibits, and community-based workshops. It has become commonplace (if arguably inaccurate) to describe the United States as the most religiously diverse country in the world. Scholars of North American religions have recognized the pressing need for new approaches to documenting and making sense of this diversity. Our approach stems from our particular interests in the material and sensory cultures of American religions and in the varied ways that religion has become newly visible and audible in American life, confounding once dominant assumptions about secularization and privatization. Rather than retreating quietly into an interiorized or immaterial realm of personal belief, religion has remained an integral feature of the modern world, and religious communities have inscribed themselves on urban landscapes and soundscapes in a variety of ways.  The working we are doing through the American Religious Sounds Project also has been stimulated by a “sensory turn” in scholarship across the humanities and social sciences. Historians, anthropologists, geographers, and others have been attending to the cultural values and social ideologies expressed through different ways of sensing the world and to the multi-sensorial modes through which modern culture was constituted. The nascent field of sound studies, defined broadly as the cultural study of sound and listening, has proven particularly generative, giving rise to new ways of thinking about critical questions that have long animated humanistic inquiry, including the legacies of industrialization and urbanization, the role of technological production and mediation, and the construction of ethnic, racial, religious, sexual, gendered, and class-based differences. Research on sound and through sound provides a rich medium for understanding religious groups, people, events, and conflicts. Religious studies scholars, however, have paid far more attention to visual and material culture than to auditory culture. In part, this can be attributed to the limitations of the textual media through which scholars have traditionally presented their research, including published monographs and journal articles. Such media have not readily lent themselves to engagement with sonic materials, for sound can be difficult to represent in such formats. Acutely sensitive to this problem, many ethnomusicologists and sound artists have begun experimenting with digital tools and platforms, like soundmapping, but such approaches have not yet made their way into the discipline of religious studies. Scholars of religion should take greater advantage of the opportunities afforded by the new digital environment, while also reflecting critically on its limitations. The American Religious Sounds Project is designed to do both. Our sound selections are robustly multi-religious, including a wide range of Christian and non-Christian traditions. We include the formal sounds of religious institutions, such as prayer, chanting, and hymns, as well as the informal, and often unintentional, sounds that arise during relaxed coffee hours and spontaneous conversations, ambient and incidental noises like laughter and crying, clapping and shouting, and the shuffling and movement of lived community during worship. We record regular weekly and daily services, as well as seasonal festivals and other special events. We move outside of formal religious institutions to capture the sounds of devotion in homes and schools, public parks and interfaith chapels, coffee shops and workplaces, as well as at ostensibly “secular” gatherings such as a school graduation, public arts festival, or college football games. For example, our researchers recently recorded the sounds of a public Christmas tree lighting, an interfaith prayer vigil against violence, a neo-Pagan brewing mead in his home kitchen, an anti-Islam protest rally, a (secular) Sunday Assembly meeting in a coffee shop, and a Bhutanese Nepali Hindu festival. By casting our net widely, we aim to build a resource that is broadly comprehensive, comparative, and even a bit provocative. We do not intend to answer definitively the question of what counts as religious, but to invite critical reflection on what is at stake in that designation and to consider the role that auditory perception plays in its constitution. In this paper, I will introduce the project and present our website, which we expect to launch in March 2018. I will solicit critical feedback and offer reflections of my own on the capabilities and limits of new digital methods for enhancing our research of the varied sonic cultures of North American religious life. One of the goals of the American Religious Sounds Project is to provide a bridge between our academic settings and our local communities. That work must be done carefully and respectfully in the present political and religious climate of the United States. I will end with some thoughts on the precarious work of the public presentation of religious sounds and communities on an open accessible digital platform. ",
       "article_title":"Listening for Religion on a Digital Platform",
       "authors":[
          {
             "given":"Amy",
             "family":"DeRogatis",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-30",
       "keywords":[
          "audio",
          "public humanities and community engaged scholarship",
          "diversity",
          "video",
          "English",
          "theology",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The study of communication networks, specifically road networks, is a topic of broad interest to the scholarly community. It allows researchers to draw conclusions that range from historical events (Antrop, 2004; Trombold, 1991) to transit and traffic (Bash et al., 2017; Yang and Yagar, 1995), while adding a tangible and understandable dimension to their work. The appearance of Geographical Information Systems (GIS) made it possible to perform such analysis efficiently and accurately. It is just recently that the study of topological and growth properties of road networks are giving us the chance of understanding the bigger picture of cities (Antrop, 2005; Kasanko et al., 2016). In the American landscape, network analysis of road networks has shown evidence that the construction of interstate highways affected the political and geographic polarization of cities, undermining representation and posing a threat to democracy itself (Nall, 2015;  Ejdemyr et al., 2005). Most of these studies, however, rely on “the only rigorous year-to-year record of the construction of interstate highways and the incorporation of existing freeways into the system” (Nall, 2018), the Federal Highway Administration PR-511 database (FHWA PR-11). While the FHWA PR-11 is the most complete database available, it is based on highway construction records, which oftentimes misrepresent the complexity of turning political promises into reality, and does not include data on the development of road networks before the interstates. One way to approach this lack of data is to resort to roadmap collections, which might be a better proxy to understand the reality of transportations networks. Unfortunately, despite the number of digitized and scanned map collections, the lack of their availability in standard network data formats still represents a burden for the study of historical road networks. Although network analysis tools exist, we are not able to fully leverage their potential regarding historical datasets without a huge amount of manual work to generate network data.  As an alternative, modern approaches of road extraction from maps promise fully automated methods that rarely generalize well (Mena, 2003,  Sharma et al., 2013), or rely on good quality labeled data (Isola et al., 2016), which is non existent or very difficult and costly to gather. We are then left to semi-automated methods where the researcher is guided to enter some crucial information needed for the automated process to start. However, these methods are usually conceived for satellite imagery or raster images of maps, lacking proper support for the variety of style and format found when dealing with collections of historical maps, and producing vector information not in network format. In order to fill this gap, we are presenting Histonets, a web-based platform to assist in the conversion of historical maps into digital networks, turning intersections into nodes and roads into edges.    Methodology The platform begins with a login screen, after which each researcher can create a number of collections of images of maps by linking them from IIIF-compliant repositories. Furthermore, researchers are able to create settings for similar images (according to their criteria). Once images are selected, the pipeline for the Histonets platform is comprised of 4 steps: image preparation and cleaning, pattern matching, pathfinding, and graph correction. Cleaning can be fully automated or fine-tuned by adjusting the parameters of several actions to be applied. Once clean, image color depth is reduced by an automatic color clustering algorithm that only needs the final number of colors (defaults to 8). With the image clean and posterized, the pattern matching step begins. In order to identify intersections and corners that will eventually become the nodes of the graph, researchers must circle around them, and, with a couple of samples, Histonets will try to find other instances in the images, taking into account rotation and orientation of the templates. Identifying roads is done by selecting their colors and a threshold. Areas under a certain threshold are removed as well. A final preview of the resulting graph is shown for the whole image. If the graph complies with the expectations the researcher can start a batch process to apply the same parameters to the whole collection. The tasks can be monitored and canceled. The final result of the process for each image map is a downloadable file in a compatible graph format, including Gephi and GraphML (sees Figure 1).   Figure 1. Sample of image input (upper left), internal output (upper right), and final graph as produced by Histonets (lower)    Discussion Although in early stages, Histonets has already proved to reduce substantially the amount of hours of manual labour, cutting down the time needed to process an entire collection. Moreover, the easy parallelization built-in in Histonets is only limited by the computational resources available, making it easier for cloud or high performance computing center deployments to further boost its performance. However, without a proper benchmarking framework it is still difficult to assess its accuracy and completeness. One of our goals moving forward is to test and measure these factors, and adjust the platform for greater reliability. While Histonets, as a whole pipeline, is focused specifically on extracting road networks from historical maps, collaborators have already identified uses outside of Political Science or History. As a general low-barrier and user friendly computer vision application, we have shown it to be useful for identifying capital letters in Medieval manuscripts, counting glyphs in Egyptian hieroglyphs, or even identifying architectural features. With its balance between meeting specific research needs and generalizable applicability, Histonets has a bright future as an adaptable tool in the Digital Humanities.  ",
       "article_title":"Histonets, Turning Historical Maps into Digital Networks",
       "authors":[
          {
             "given":"Javier",
             "family":"de la Rosa Pérez",
             "affiliation":[
                {
                   "original_name":"Center for Interdisciplinary Digital Research, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Scott",
             "family":"Bailey",
             "affiliation":[
                {
                   "original_name":"Center for Interdisciplinary Digital Research, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Clayton",
             "family":"Nall",
             "affiliation":[
                {
                   "original_name":"Department of Political Science, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Ashley",
             "family":"Jester",
             "affiliation":[
                {
                   "original_name":"Center for Interdisciplinary Digital Research, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Jack",
             "family":"Reed",
             "affiliation":[
                {
                   "original_name":"Digital Library Systems and Services, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Drew",
             "family":"Winget",
             "affiliation":[
                {
                   "original_name":"Digital Library Systems and Services, Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-01",
       "keywords":[
          "technologies",
          "graphs",
          "geohumanities; spatial & spatio-temporal analysis",
          "artificial intelligence and machine learning",
          "modeling",
          "historical studies",
          "networks",
          "image processing",
          "visualization",
          "relationships",
          "English",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" How can the unknown be organised? When working with a script and language that has not been (completely) deciphered yet, primarily an inventory of all signs used must be compiled. What at first seems to be a diligent but routine piece of work, quickly turns out to be a complex classification task, for there is still much unknown. Questions arise about a signs' use-context, the extent of the sign inventory and, above all, how the signs can be classified. Particularly the unambiguous identification of signs has some pitfalls. The difference in meaning of a sign is determined on the one hand by its graphic representation (grapheme) and on the other hand by its phonetic value (phoneme). Since the latter can only be achieved by decipherment work that has already been done, the error-free classification of undeciphered signs is a challenging task. In addition, the investigation of archaic texts creates different contexts that lead to different interpretations. The resulting hypotheses must be included in the classification and decoding tasks of a script if a resilient research basis is to be created. In our talk we present a concept for the classification and systematisation of characters for writing systems that have not been (completely) deciphered yet. We applied this concept to the Maya hieroglyphs and created a digital sign catalogue that was developed in close interdisciplinary cooperation between epigraphy and information science and technology. The digital sign catalogue can be used to identify, systematise and classify signs and it also offers a starting point for further analyses that can present reliable deciphering. We took an ontological approach to model the sign catalogue. To use this type of knowledge representation for the classification of signs is a novel approach to digital epigraphy.   Characteristics of Classic Maya Writing    Usage of logograms and syllabograms  The Maya hieroglyphic script was used between 350 BC and 1550 AD in southern Mesoamerica to record the high level language of Classic Mayan (Wichmann, 2006). The exact number of signs has not yet been determined, it varies from approximately 500 to 1000. Even though Classic Mayan is called a hieroglyphic script due to its iconic character, typologically it is a logo-syllabic writing system in which logograms and syllabograms form the main sign classes. Logograms designate specific terms, such as  PAKAL  (shield). Syllabograms represent open syllables and are also used as phonetic complements of logograms. Words could be written from logograms only ( PAKAL ), syllabograms ( pa-ka-la ), or a combination of both ( PAKAL-la ), see Fig. 1 (Montgomery, 2002). Other sign functions are numerals and diacritic signs. The signs are usually arranged within a hieroglyph block, which forms a word or a compound of morphemes, similar to Korean Hangul. However, due to its wide range of variants, Maya reveals a much greater degree of calligraphic freedom. Depending on space requirements and aesthetics, graphs can be conflated, infixed or rotated. The formation of graph variants is so complex, that it is a particular challenge to determine the grapheme of a sign.   Maya signs can be polyvalent. A sign can have more than one functional level and thus readings, either as logograms and or syllables. There is a sign labelled T528 according to Eric Thompson's standard sign catalog for Maya writing (1962) that can be read as the logograms TUN  (stone) and  CHAHUK  (a day name), and also as the syllable  ku . The graph variants of this sign do not indicate which of the readings is present.  During the investigation of a not yet (fully) decoded script and language, controversial and plausible statements about the deciphering of signs emerge during the research discourse. Each deciphering hypothesis claims to be meaningful in the investigated context. For over 150 years, work has been done on deciphering the Maya script, and the ‘birth and death rate’ of deciphering-proposals is correspondingly high. The degree of decipherment nowadays ranges between 60 to 80 percent.   Novel Concept of Sign Classification and Modelling of the Digital Sign Catalogue With our digital sign catalogue we want to establish a new concept for the systematisation and classification of signs. We chose a new and unorthodox approach that deliberately differs from previous organisational principles of linguistics and, in particular, from other (Maya) sign inventories. We have specifically questioned traditional practices in order to investigate other ways of systematising and organising signs. Therefore, we have examined existing classification systems and linguistic terminologies in order to find suitable concepts for describing Maya signs (Department of Linguistics, 2010; Chiarcos and Sukhareva, 2015). We detected that most concepts are not applicable to the development of our catalogue, as they focus too much on the applicability in a specific linguistic context. Thus, they are not applicable to a writing system with a fair deciphering degree, since they are simply not known yet. For this reason, we want to create an organisation system for describing, classifying, and systematising the signs, using linguistic categories only on a meta level and not taking further analysis levels and grammars into account.  The decipherment of Maya signs can only be done by linguistic analyses on the basis of a corpus. To be able to create such a corpus, the signs used in the texts must be identified. In order to allow the processes of sign identification and subsequent text analysis to be interlinked, an organisational system is required that can react flexibly to changes. We achieve the necessary flexibility through ontological-based modelling. To optimally represent the semantic relations between the described entities, the data model of the sign catalogue was implemented in RDF. The documentation of signs in an ontologically based knowledge organisation system has not been done in Maya epigraphy yet and thus represents a new approach in the exploration of this script.   In our catalogue w e define the sign as an entity consisting of a functional and phonemic level and a graphical representation. We modelled the class  Graph   which represents all variants of a grapheme (allographs). By the separate recording of discrete graphs we enable an exact method for their identification. Based on preliminary work by the research community (Kelley, 1962; Houston, 2001), for the first time we were able to develop rules and principles on creation of graph variants of Maya signs. 45 variation types in total were defined, which are subdivided into nine classes.   mono-, bi-, tri-, and variopartite, division, animation head, animation figure, multiplication and extraction    The  Graph -class is set in relation to the functional and phonemic level of the sign (the class  Sign , see Fig. 2). This relation is optional, so that even graphs can be recorded that could not have been assigned to any sign yet.     Sign 126 with graph variants bh and br, and phonetic value /ya/  The class  Sign  is determined by its function: the use of the sign as a logogram, syllabogram, numeral, or diacritic. The phonemic level of the sign is recorded as the transliteration value with the respective  SignFunction . Only one value is allowed per function, but one sign can have several sign functions and therefore readings.   In Maya epigraphy, signs that have not yet been deciphered arouse a lively discourse, from which constantly new proposals for their reading emerge. We are faced with the challenge of not only documenting existing as well as our own deciphering hypotheses, but also evaluating and classifying them qualitatively in such a way that they can withstand a critical examination and be used for further (linguistic) analyses. The hypotheses have different quality levels. Some seem more plausible than others. In order to make the quality of the readings formally assessable, we have developed a set of criteria for each sign function, which is oriented, among other things, to the context of use (e. g. plausible text-image-reference) or the proof in modern Mayan languages.   Notice that these criteria can only be derived from the deciphering work already carried out.    The criteria are related by means of propositional logic so that, depending on their combination, a quality level is  determined . To represent these in the model, we developed the class  ConfidenceLevel   that is related to the  SignFunction -class. Therefore, a qualitative evaluation can be made for the transliteration value (see Fig. 3).     Modelling the ConfidenceLevel of polyvalent Sign 528 The qualitative evaluation is particularly relevant to examine the plausibility of reading hypotheses in a corpus. Readings with a high level can be compared with those with a low level. New criteria for the plausibility could always be found in other texts and can then be added to the sign catalogue. This may also increase the quality level of the transliteration value.    Conclusion  Since the concept distinguishes itself specifically from the classification of signs in linguistic categories, it is also transferable to other languages that have not yet been (completely) deciphered or under debate regarding their nature, e.g. Nahuatl, Elamite, Indus, or  Rongorongo writing . In particular, the separation of the sign in a graphic and a functional-phonemic level - that can be related to each other depending on the level of knowledge - offers a flexibility that redefines the classification of signs and allows precise identification on the basis of distinguishing characteristics. The ontological modelling approach and the implementation in a RDF data model enables new insights into character classification. By incorporating known and adapting new results, the digital sign catalogue is specifically designed to deal with ambiguousness in research processes.   ",
       "article_title":" Organising the Unknown: A Concept for the Sign Classification of not yet (fully) Deciphered Writing Systems Exemplified by a Digital Sign Catalogue for Maya Hieroglyphs  ",
       "authors":[
          {
             "given":"Franziska",
             "family":"Diehr",
             "affiliation":[
                {
                   "original_name":"Göttingen State and University Library, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sven",
             "family":"Gronemeyer",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Department for the Anthropology of the Americas, Germany; La Trobe University, Department of Archaeology and History, Australia",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Christian",
             "family":"Prager",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Department for the Anthropology of the Americas, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Elisabeth",
             "family":"Wagner",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Department for the Anthropology of the Americas, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Katja",
             "family":"Diederichs",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Department for the Anthropology of the Americas, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Nikolai",
             "family":"Grube",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Department for the Anthropology of the Americas, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Maximilian",
             "family":"Brodhun",
             "affiliation":[
                {
                   "original_name":"Göttingen State and University Library, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "computer science",
          "information architecture and modeling",
          "knowledge representation",
          "ontologies",
          "linguistics",
          "library & information science",
          "English",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Stylometry has a long tradition in linguistics and literary studies and has only gained in popularity with the digitization of text corpora and out-of-the-box tools (Holmes and Calle-Martin & Miranda-García). Stilometric methods for paintings have been advanced in digital art history but remain at an early stage of development (Qu, Taeb & Hughes; Manovich). Stylometric analyses for visual narratives are not yet established.    Lev Manovich applied stylistic description to manga but his studies remained explorative and did not offer an analysis of categories such as author or genre.   Visual narratives include film and TV, comics and other illustrated print literature, and to an extent computer games, constituting some of the most popular cultural formats of the twentieth and twenty-first centuries. The relative lack of research in this area may be traced to the technical hurdles of image analysis and the absence of suitable corpora. This paper will introduce a method for visual stylometry in comics based on the analysis of a corpus of 209 book-length graphic narratives. In closing, we explore how the method may be applied to other media.    2. Corpus & Data Analysis Our analysis is based on the Graphic Narrative Corpus (GNC), the first representative collection of what is commonly called graphic novels (Dunst et al.). The GNC was conceived as a stratified monitor corpus and defines graphic narratives as comics of more than 64 pages in length that tell one continuous or closely-related stories and are aimed at an adult readership. Due to the absence of reliable bibliographies, the total population remains unknown. A random sample is therefore not feasible. To avoid bias, we sampled from a wide array of sources: academic and general audience databases, library collections, international comics prizes, Amazon.com bestseller lists, literary histories, surveys of comics scholars, and media reports. At the time of analysis in November 2017, 209 full-length graphic narratives running to nearly 50.000 pages had been digitized and checked for scanning artefacts. The focus on image analysis is due to both methodological and practical reasons: stylometric methods for text analysis are more established and are being continuously improved by an existing research community. These methods can be directly applied, or easily adapted, for analyzing text in comics. Automatic text localization and OCR for comics still represent work in progress, and text can not yet be extracted automatically with sufficient quality. This leaves time-consuming manual annotation as the only option, which excludes the analysis of large corpora. Visual style thus represents the most promising avenue for distinguishing between authors and genres. We used five basic measures for analysis, all of which are low-level features that are commonly used in computer vision and information theory. In all these cases, we were interested in finding significant relationships between these measures as indicators of visual style and the critical concepts we are investigating, i.e. genre and authorship.   Median Brightness: the mean value of all brightness values of all pixels of a page. We transformed each page into a grayscale image by computing the Luma of each pixel, i.e., the weighted sums of the gamma-compressed R’G’B’-values of the image.   Shannon Entropy: the expected value of the information in a message. The entropy H(X) of a message X=(x 1,…,x n) of length n is defined to be H(X):=Σ i=1..nP(x i)*log 2(P(x i)). The message X of the entropy is the list of the brightness values of each pixel, with the x i range between 0 and 255. In addition, n is the total number of pixels. As P(x i) denotes the probability or relative frequency of item x i, we can compute P(x i) for a given x i by P(x i):= (Number of pixels having value x i)/(n=total number of pixels).   Number of Shapes: describes an image’s agitation. To yield normalized values, we scaled each image to a height of 250 pixels. We first split grayscale images into 5 sub-images of different brightness levels and then measured individual sub-images and filled unconnected areas up to a diameter of four pixels. In a final step, we discounted components that came to less than ten pixels in size.   Color Layout: A color layout descriptor (CLD; MPEG 7) captures the spatial distribution of color using the YCbCr color space. The extraction process consists of image partitioning, representative color selection, discrete cosine transform, and zigzag scanning. The color components Cb and Cr represent the range of blue and red present in an image.   Edge Types: the edge histogram descriptor (MPEG 7) calculates the frequency of different edges in an image: vertical, horizontal, 45° diagonal, 135° diagonal, and non-directional. Each image is divided into 4x4 subframes. Each subframe consists of five bins, each of which represents the different edge types. Subframes are divided into non-overlapping blocks to extract edge types and bin values are normalized by the total number of blocks in the subframe.   After calculating the five basic measures, we derived the median for each of the 209 graphic narratives. To analyze stylistic variation within individual narratives, we calculated standard deviation from each of the five measures. We performed Anova and Tukey’s HSD, which are standard statistical methods for testing for significant differences among the means of more than two samples, with p<0.05.   3. Results & Discussion  3.a Genre The GNC consists of fictional and non-fictional texts, including graphic memoirs and journalism, which are often summarized under the somewhat misleading umbrella term graphic novel. We assigned 23 subgenre categories using plot summaries and information provided by publishers. Their distribution can be seen in figure 1. Subgenres were grouped into six larger categories for analysis: graphic novel, graphic memoir, other non-fiction, humor, graphic fantasy, and miscellaneous.     Figure 1: Larger genre categories are indicated by color ranges: graphic novel (red), graphic memoir (green), other non-fiction (blue), humor (yellow), graphic fantasy (purple), and miscellaneous (gray).  Results show highly significant distinctions for graphic novel, graphic memoir, and graphic fantasy across several measures. Graphic memoirs (including such canonical text as Spiegelman’s  Maus and Bechdel’s  Fun Home) are brighter, show less color variation (cb & cr), and are more regular in their visual style than other genres. Regularity of visual style can be seen in the lowest median scores for entropy and the high frequency of horizontal edges. Graphic fantasy is significantly darker, while showing the highest entropy and lowest number of horizontal edges. Graphic fantasy also distinguishes itself by the highest amount of color variation. Graphic novels are situated between the two extremes of graphic memoirs and fantasy, yet are statistically distinct in their visual style. The measure number of shapes did not return significant results, while the edge histogram only did so for horizontal edges. The boxplots in figures 2-4 show results for entropy, brightness, and horizontal edges.      Figure 2: Boxplot Entropy: Graphic Fantasy – Graphic Memoir (p<0.003)    Figure 3: Boxplot Mean Brightness: Graphic Memoir – Graphic Novel (p<0.016); Graphic Fantasy – Graphic Novel (p<0.000)    Figure 4: Boxplot Horizontal Edges: Graphic Fantasy – Graphic Memoir (p<0.001)    3.b Authorship The GNC includes several authors that are represented with more than one graphic narrative. The GNC also contains information on single authorship, or collaborations between one writer and one illustrator, or multiple authors. Results returned highly significant distinctions for individual authors and for authorship categories (single, two, and multiple authors). Works by authors such as Neil Gaiman and Frank Miller show consistently higher entropy and a comparatively higher mean brightness than other authors, while the opposite holds for Will Eisner, for instance. Results align with genres in which these authors publish, respectively, graphic fantasy versus graphic novel and memoir. Similarly, the number of shapes and the variation in mean brightness are significantly lower for authors who publish in the latter genres. Individual and multiple authorship also results in distinct visual styles. Graphic narratives written by a single author show lower entropy and number of shapes, are brighter and less colorful, and contain fewer diagonal edges (45° and 135°). Results were highly significant, with p<0.01 throughout. Figure 5 and 6 visualize entropy for individual authors and number of shapes for authorship categories.   Figure 5: Boxplot Entropy Authors with >3 titles    Figure 6: Boxplot Number of Shapes for Authorship Categories: 1 – 2 Authors (p<0.001); 1 – >3 Authors (p<0.001)     4. Outlook and Future Work We introduced image analyses that adapt stylometric distinctions to visual narrative. As our paper shows, comics grouped together under authorship or genre affiliation share numerous visual traits. The correlation between author and genre categories indicates that we need to disentangle these signals. We are working on neutralizing the author signal by penalizing texts from the same writer and will integrate this approach in time for DH 2018 (Tello et al.). As examples of hand-drawn still images, comics have stylistic traits that distinguish them from moving image narratives such as film and TV. Thus, the visual descriptors used here may be adapted most readily to other forms of graphic art, including drawings, woodcuts, and lithographs. Given that the measures we used are highly generic and low-level features, the method also has potential for other media in which the concepts of genre and authorship play a role. Thus, they could be adapted for investigating authorship in film, for instance.  ",
       "article_title":"Automated Genre and Author Distinction in Comics: Towards a Stylometry for Visual Narrative",
       "authors":[
          {
             "given":"Alexander",
             "family":"Dunst",
             "affiliation":[
                {
                   "original_name":"University of Paderborn, Germany",
                   "normalized_name":"University of Paderborn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/058kzsd48",
                      "GRID":"grid.5659.f"
                   }
                }
             ]
          },
          {
             "given":"Rita",
             "family":"Hartel",
             "affiliation":[
                {
                   "original_name":"University of Paderborn, Germany",
                   "normalized_name":"University of Paderborn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/058kzsd48",
                      "GRID":"grid.5659.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "corpora and corpus activities",
          "art and art history",
          "stylistics and stylometry",
          "cultural studies",
          "image processing",
          "content analysis",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In the last decades, quantitative linguistics (following exact and social sciences) has developed a considerable number of statistic methods providing an insight into measurable phenomena of natural language. Although to a lesser extent, it also applies to the analysis of diachronic changes. The basic tool used to assess the chronology of linguistic changes is a rather effective yet simple method of trend search: the examined features are analyzed by mapping the frequency of the described phenomenon on a timeline (Ellegård, 1953). This timeline-centric visualization has become a standard in several studies and corpus tools. The most spectacular example is the corpus of several dozens of million of documents (mainly in English) accompained by the service Google Books Ngram Viewer  , which, according to its authors, enables to examine changes taking place not only in the language, but also in culture (Michel et al., 2011).  A significant drawback of simple graphic representation of the trend, and hence of mapping the frequency of the examined phenomenon on a timeline, is a tacit assumption that the researcher knows in advance which elements of the language are subject to change. In other words, the method of plotting and inspecting the trend may be applied only to verify hypotheses stipulated earlier by traditional diachronic linguistics. For example, knowing in advance that Polish underwent the gradual replacement of the inflected ending  -bychmy with  -byśmy, one might draw the trendline and capture the dynamics of that change. Although many prominent diachronic works were based upon such an approach (Biber, 1988; Hilpert and Gries, 2009; Hu et al., 2007; Reppen et al., 2002; Smith and Kelly, 2002; Can and Patton, 2004), one might be interested in trend search without any  a priori selection of the analyzed linguistic changes to be traced.  Needles to say,  some selection of potential language change predictors (e.g. a predefined set of words, certain collocates, etc.) will always be the case. The strategy followed in this study was to analyze a considerably large set of 1,000 most frequent words without any further filters, with the assumption that some of them will turn out stronger than others. Arguably, in such a big set one should find a few dozen of function words, and a vast majority of content words. Another remark that has to be formulated here is that the language change cannot be reliably separated from the stylistic drift (e.g. in literary taste of the epoch). This fact is well known in stylometric approaches to style (“stylochronometry”), where the actual changes in the system and stylistic signals of, say, the predominant genres are usually difficult to be told apart.    Supervised classification and the timeline The most natural strategy to assess the discriminative power of numerous features at a time is to apply one of the multivariate methods. Since none of the out-of-the-box techniques is suitable to analyze temporal datasets, some tailored approaches have been proposed, e.g. using a variant of hierarchical clustering (Hilpert and Gries, 2009; Hulle and Kestemont, 2016). These methods, however, share a common drawback, namely their results are by no means stable. Also, no cross-validation can be considered a downside. To assess these issues, an iterative procedure of automatic text classification was applied (Eder and Górski, 2016). Its underlying idea is fairly simple: first, we formulate a working hypothesis that a certain year – be it 1835 – marks a major linguistic break. The procedure randomly picks  n text samples written before and after the assumed break; the samples then go into the  ante and  post subsets. In this study, a period of 20 years before and after the assumed break was covered (with an additional gap of 10 years), 500 text samples of 1,000 tokens were harvested into each of the subsets. To give an example: for the year 1835, 500 random samples covering the time span 1810–1830 were picked into the first subset, and another 500 samples from the years 1840–1860 into the second subset. Next, the both subsets are randomly divided into two halves, so that the training set and the test contain 500 samples representing two classes ( ante and  post). Then we train a supervised classifier – in this case, Nearest Shrunken Centroids – and record the cross-validated accuracy rates. Then we  dismiss the original hypothesis, in order to test new ones: we iterate over the timeline, testing the years 1836, 1837, 1838, 1839, … for their discriminating power. The assumption is simple here: any acceleration of linguistic change will be reflected by higher accuracy scores.    Data and results The above procedure has been applied to the Corpus of Historical American English (COHA), containing ca. 400 million tokens and covering the years 1810–2009 (Davies, 2010). The corpus provides the original word forms, part-of-speech tags, and the base word forms (lemmata). The results reported below were obtained using the lemmatized version of the corpus.    Fig. 1: Language change accelleration in the American English corpus: classification accuracy over the years 1835–1985.   In Fig. 1, the classification accuracy rates for the COHA corpus were shown (1,000 most frequent lemmata, NSC classifier). As one can observe, the scores obtained for each period are higher than the baseline, suggesting the existence of a temporal signal. Obviously, the higher the scores the faster the evolution of language, since the distinction between the period before and after the tested breakpoint is simpler for the classifier. More important, however, is the fact that the scores are not even: the signal becomes stronger in some periods, clearly indicating an acceleration of the language change. One of the stylistic breaks takes place in the 1870s (i.e. after the Civil War), the other in the 1920s (in the period of prosperity before the Great Depression); the third peak is not fully formed yet, even if one can observe an acceleration of language change at the end of the 20th century. Needless to say, any attempts at finding direct correlations between historical events and stylistic breaks are subject to human prejudices, and therefore might introduce substantial bias to the results. Even though, the coincidence of the three observed peaks and a few major changes in the American culture is rather striking.   Distinctive features The results obtained in the above experiment seem to be rather promising. However, from the perspective of historical linguistic even more interesting is the question which features (words) were responsible for an given change observed in the dataset. It has been reported in several stylometric studies that attributing authorship relies, in most cases, on many features of individually very weak discriminative power. In the context of language change, a similar question can be asked: is it but a few characteristic words that trigger the change, or, alternatively, is the stylistic drift spread across dozens of tiny changes in word frequencies? To answer the above question, one has to extract the features that played a prominent role in telling apart the  ante and  post periods as described above. The features exhibiting the biggest variance (that is, the overall impact on the results) are shown in Fig. 2. An important caveat needs to be formulated here: the plot shows the outputted weights from the classifier, rather than direct word frequencies. The underlying assumption is that the features’ weights (to be precise: the  a posteriori probabilities returned by the classifier) reflect the changes in actual word frequencies as combined with all the other frequencies being analyzed.     Fig. 2: Seventy-six linguistic features (words) that contributed considerably to the stylistic drift.   The main stylistic breaks form, again, three peaks that culminate roughly in the same years as presented in Fig. 1. What is counterintuitive, however, it is the fact that the features tend to form sinusoidal waves of their periodical discrimination power. Interestingly, these high impact features turned to be very frequent words that usually occupy the top positions on the frequency list. The 25 words of the highest discrimination strength are as follows:  the,  and,  week,  that,  ’s,  last,  is,  be,  of,  it,  we,  i,  to,  was,  mr.,  our,  my,  been,  not,  u.s.,  you,  new,  upon,  there,  has  Even more interesting are individual trajectories of the high-impact words. In Fig. 3, one can observe a collinearity of function words:  the,  and,  that,  is,  been, as opposed to the possessive  ’s. These function words seem to have impacted the language change at the turn of the 19th century. The possessive, in turn, contributed to the evolution of language roughly at the times of the Prohibition. (Again, this is not to say that any direct links between function words and actual events in history should be drawn).     Fig. 3: Function words of the highest impact on the stylistic drift.   A different pattern is revealed by the “social” words, especially personal pronouns. It has been shown that these words, e.g.  I, play prominent role in betraying someone’s personality (Pennebaker, 2011). Certainly, traces of such individual profiles will hardly be noticeable at the level of the entire corpus. One might try, however, to formulate some claims of the “personality” of the population in the function of time, in the belief that some general trends in culture might be reflected in the corpus. In Fig. 4 a few personal pronouns and some contractions have been shown. As one can see, their moderate presence over the past decades turns into a very hight impact at the end of the 20th century. Moreover, the impact of the words  I and  my seems to grow even further… These and similar examples provide a counterintuitive evidence that a language change might be due to minute differences in the usage of very common words.     Fig. 4: High impact personal pronouns and contractions.     Conclusions In this paper, we used a tailored stylometric method to assess the question of language change over time. Our chosen technique proved to be useful indeed, especially when one focuses on tracing the very linguistic features that were responsible for the observed change. The results were counterintuitive, since the set of strongly discriminative features contained common function words, which formed sinusoidal trajectories of their impact over time. One of the most interesting aspects of language development – overlooked in numerous existing studies – is the question of the dynamics of linguistic changes. Our study corroborated the hypothesis that epochs of substantial stylistic drift are followed by periods of stagnation, rather than forming purely linear trends.   Acknowledgements This research is part of project UMO-2013/11/B/HS2/02795, supported by Poland’s National Science Centre.  ",
       "article_title":" Words that Have Made History, or Modeling the Dynamics of Linguistic Changes  ",
       "authors":[
          {
             "given":"Maciej",
             "family":"Eder",
             "affiliation":[
                {
                   "original_name":"Institute of Polish Language (Polish Academy of Sciences), Poland; Pedagogical University in Kraków, Poland",
                   "normalized_name":"Pedagogical University of Kraków",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/030mz2444",
                      "GRID":"grid.412464.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-20",
       "keywords":[
          "corpora and corpus activities",
          "linguistics",
          "stylistics and stylometry",
          "english studies",
          "text analysis",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital environments now serve as the primary network for academic and non-academic modes of communication, research practices, and knowledge dissemination. This shift has resulted in greater ease of pursuing collaborative modes of engagement. Social knowledge creation, citizen scholarship, interdisciplinary collaborations, and university-community partnerships have become more common and more visible. Engaging with such transformations in knowledge creation has been a significant research focus for the Electronic Textual Cultures Lab (ETCL) at the University of Victoria. This presentation will detail the intellectual foundations of social knowledge creation, as well as the major initiatives undertaken by the ETCL to pursue and enact this research. The ETCL explores these topics via on-campus activities as well as three substantial environmental scans: Social Knowledge Creation: Three Annotated Bibliographies” (Arbuckle, Belojevic, Hiebert, Siemens, et al., 2014), “An Annotated Bibliography on Social Knowledge Creation” (Arbuckle, Belojevic, El Hajj, El Khatib, Seatter, Siemens, et al., 2018), and “Open Social Scholarship Annotated Bibliography” (El-Hajj, El Khatib, Leibel, Seatter, et al., under development). The annotated bibliographies bring together myriad perspectives on how collaborative knowledge creation and engagement practices have been carried out, historically as well as currently. This work suggests how elements of academia might be reimagined in order to effectively integrate collaborative, interdisciplinary, public-minded praxis. Building on field touchstones like Kathleen Fitzpatrick’s  Planned Obsolescence (2011) and John Willinsky’s  The Access Principle (2006), this work proposes that collaboration-driven academic practices in a new media context can create a more critical work environment that integrates creative options for publishing and disseminating research.   “An Annotated Bibliography on Social Knowledge Creation” updates the previously published “Social Knowledge Creation: Three Annotated Bibliographies.” The former version was developed in the ETCL in collaboration with the Implementing New Knowledge Environments (INKE) Research Group, and formulated a snapshot of social knowledge creation scholarship and initiatives up to 2013. The revised document draws on more recent scholarship published in this evolving area of inquiry, and expands the scope to include notable subject additions, including public humanities, crowdsourcing, digital publishing, and open access. In both the 2014 and forthcoming instances, resources are chosen according to their relation to our definition of social knowledge creation: “acts of collaboration in order to engage in or produce shared cultural data and/or knowledge products” (1). Many stress the importance of involving citizen scholars to revitalize research and as a way to respond to a crisis that public humanities draws attention to, namely the ever-expanding gap between the university and the community. The subject additions of the latter document encapsulate the pressing need to create and strengthen community outreach in academic environments.   In 2016, the ETCL team began compiling the “Open Social Scholarship Annotated Bibliography.” According to INKE, open social scholarship involves the creation, dissemination, and engagement of research and research technologies that are accessible and significant to a broad audience. The bibliography draws on research that adopts and propagates these knowledge production ideals that have branched out across movements, including open access, open source, public humanities, citizen scholarship, citizen science, and community outreach, among others. The main trends that are explored include: developing and disseminating research in accessible ways; research that draws on university and community interests and needs; active engagement of community members in academic research practices; and the development of research tools that bring these two communities into productive dialogue and serve their needs. Resources range from traditional, foundational forms of open knowledge and resources to highly praxis-oriented projects. Historical publications, starting with  Philosophical Transactions of the Royal Society of London, exemplify how knowledge was discussed and debated through publication. Advocacy for open access to information is a recurring theme across many of the included works, with a position that publicly funded research should be accessible to the wider public. In addition to the aforementioned discourses, the bibliography addresses the impact of open knowledge on social justice movements through new mediums, and how Internet tools and social networks have been used to mobilize action in activist movements.  These environmental scans lay the foundation for our ETCL-based Open Knowledge Practicum fellowship, launched in January 2017 with two completed rounds, a third one currently running, and more to follow. This initiative puts open social scholarship into action by inviting faculty, staff, students, and members of the community to pursue their own research projects for an academic term in the ETCL. We provide participants with access to resources, library materials, and archives; consultation and guidance from specialists in the field; and other project-specific assistance. The Open Knowledge Practicum is a step toward more publicly engaged scholarship, ranging from discipline-specific foci to research on local public history or the broader community. Practicum findings are published in online, public venues and made discoverable to both general and targeted communities. As a connecting thread, all fellows create, enrich, or revise Wikipedia pages that relate to their topic. This presentation will showcase a number of projects that came out of the Open Knowledge Practicum, available for review at <http://etcl.uvic.ca/?page_id=1919>.  The ETCL also launched Digital Scholarship Fellowships for the 2017-2018 academic year. Digital Scholarship Fellowships support graduate students, postdoctoral fellows, new and visiting scholars, as well as staff, faculty, and librarians making substantial use of digital and/or social knowledge creation methods to carry out humanities or interdisciplinary research. Individuals across disciplines are able to join the ETCL community in this way, and to work alongside the team in the ETCL on relevant projects within their area of research.  We consider Wikipedia to be a prime example of social knowledge creation, as it is an online encyclopedia comprised, maintained, and expanded by thousands of citizen scholars. In partnership with the U Victoria Libraries, the ETCL appointed two Honorary Resident Wikipedians: Dr. Christian Vandendorpe (2014–16) and Dr. Constance Crompton (2017). So far, Wikipedia edit-a-thons have oriented toward social justice themes.  Moreover, the ETCL runs campus-based digital skills training initiatives. DHSI takes place annually at U Victoria and will run for the 18th consecutive year in June 2018. Participants from different fields and locations attend DHSI for two weeks of workshops, seminars, and other conference activities. In 2017 DHSI launched a course stream that brings the various open knowledge oriented research foundations discussed here into a pedagogical setting. Courses include: “Open Access and Open Social Scholarship,” by Arbuckle (U Victoria), “Digital Public Humanities” by Mia Toothill (Cornell U), “Accessibility and Digital Environments,” by Erin E. Templeton (Converse C) and George H. Williams (U South Carolina Upstate), “Ethical Collaboration in the Digital Humanities,” by Daniel Powell (King’s C London), and “Feminist Digital Humanities: Theoretical, Social, and Material Engagements,” by Elizabeth Losh (C William and Mary) and Jessica M. Johnson (John Hopkins U). In DHSI 2018, two new courses will join this stream: “Race, Social Justice, and DH: Applied Theories and Methods” by Dorothy Kim (Vassar C) and David Nieves (Hamilton C), and “Queer Digital Humanities: Intersections, Interrogations, Iterations” by Jason Boyd (Ryerson U) and James Howe (Rutgers U). This course stream addresses the theory, methods, and challenges related to open social scholarship in various settings. The ETCL also hosts training throughout the year, the “Digital Humanities Workshop Series,” launched in partnership with DHSI and U Victoria Libraries and affiliated with Simon Fraser U (DHIL, SFU Library Research Commons) and U British Columbia (UBC Library, UBC Advanced Research Computing), which provides students, faculty, and staff, and members of the community with a wide range of technical skills and relevant theoretical basis in various digital humanities subfields. The ETCL activities and research directions we outline in our paper share a commitment to address and practice scholarship that is responsive to the evolving needs of the university and the larger community.  The ETCL strives to produce relevant and accessible scholarship, while simultaneously thinking about ways of harnessing the digital medium to benefit all. ETCL initiatives also address the potential for creating and fostering university-community partnerships. We seek to highlight the ever-expanding social nature of knowledge production and how scholarship has expanded beyond the academic context, as evident in the vast amount of research produced by citizen scholars and citizen scientists.  ",
       "article_title":"Social Knowledge Creation in Action: Activities in the Electronic Textual Cultures Lab",
       "authors":[
          {
             "given":"Alyssa",
             "family":"Arbuckle",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Randa",
             "family":"El Khatib",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Ray",
             "family":"Siemens",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "digital ecologies and critical infrastructure studies",
          "public humanities and community engaged scholarship",
          "cultural studies",
          "English",
          "interdisciplinary & community collaboration",
          "literary studies",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" John Milton’s  Paradise Lost creates an extraordinarily rich and complex sense of space. The epic poem elegantly captures the cartographical leap of the sixteenth and seventeenth century that owes to advancements in navigation techniques and rapid colonial expansion. The world image was rapidly changing and gaining a more distinct contour as newly colonized lands were becoming better described and known. Maps in this time could often be considered prototypes since cartographers were still experimenting to find a more accurate mimesis of the world. At the same time, the strong foundation of  Paradise Lost and many other retellings of the Genesis captures the saturation of the seventeenth century in religious tradition and references to sacred places. In this way,  Paradise Lost can be seen as a prototype of its own that brings together spatial traditions, new and old, real and imaginary, into a single medium. To date, Milton’s spatial allusions – spanning biblical, classical, and contemporary temporalities – have predominantly been studied in relation to the textual sources that had influenced them. However,  Paradise Lost was written at a time when the visual tradition of mapping places of the bible with cartographic exactitude had reached its peak, seen in the King James Bible, which was also Milton’s family Bible – a tradition that, in retrospect, is an early example of a geospatial, text-to-map project. Milton construed his spatiality on the existing framework of this visual tradition, and consolidated the geographies of classical antiquity and of his contemporary world. These temporalities were conceived to have progressed on a linear spectrum of geographical continuity, according to the prevalent notion of historical sequence of a seventeenth-century audience. By superimposing these layers, Milton uses textual sources to assign moral valence to geographical points; these inform the readers’ understandings of the epic and of the space of human history that it encompasses. The GIS-based digital project, “A Map of the Moralized Geography of Paradise Lost,” explores the multi-temporal complexity of Milton’s spatial allusions through an open access map depicting the moralized geography of  Paradise Lost. These multiple temporalities are delineated by various layers of georectified historical maps, including the map that supplies the visual paratext of the King James Bible, as well as John Speeds map of “The Turkish Empire” (1626). The interactive dimensions of the map permit users to recover and evaluate nuance (by resituating geographical names in their poetic contexts) even as they seek to apprehend and deduce larger patterns.   The most powerfully apparent pattern is the concentration of Milton’s spatial allusions on the Mediterranean world, forming a thick chain around the Mediterranean basin. Sites of biblical or classical significance were, in the seventeenth century, in territories almost entirely controlled by the Ottoman Empire; this superimposition creates a polarized dynamic of moral valence. Additionally, Milton’s map is coordinated with a map based on place names extracted from the Book of Genesis in order to investigate the scope of influences of the biblical book itself on the epic poem. The extraction of geo-coordinates from both works was carried out manually for the sake of accuracy, since the limitations of present geoparsing techniques with variant and historical place names remain a methodological sticking-point. The Genesis map is less complex than the initial one, making it clear that it was literary and exegetical writings, and religious culture more broadly, that built thick association. This condition reinforces the status of geographical references in Milton’s epic as references, as vectors that import or apply associations established through cultural tradition or poetic technique. In this way,  Paradise Lost functions like an early modern chorography that contextualizes place names at use. The fruit of this project is a navigable visual network that invites users to trace contextualized recurring patterns in multiple temporalities.   ",
       "article_title":"The Moral Geography of Milton's Paradise Lost",
       "authors":[
          {
             "given":"Randa",
             "family":"El Khatib",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "modeling",
          "renaissance studies",
          "content analysis",
          "text analysis",
          "visualization",
          "English",
          "literary studies",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper reports on the completion and launch of the locative media app “Go Queer.” Taking the theorization, iteration, and development of “Go Queer” as a model and case study, the paper argues that locative media is uniquely suited to re/mediating queerness. It then proposes that these findings can be used as a framework and set of best practices for developing a variety of queer history applications.  Go Queer is a ludic, locative media experience that occurs on location, in the city, on the playful border between game and story, the present and the past, the queer and the straight, the normative and the  slant. The app takes the city of Edmonton’s queer history as its text, and produces a locative, spatialized narrative of that history by displaying text, images, video and audio in place at the actual locations where they occurred, thus creating what Richardson and Hjorth (2014, 256) call “the hybrid experience of place and presence.” The app invites its users to drift queerly through the city, discovering the hidden histories that always surround us, yet somehow remain just beyond our apprehension. It compiles these traces into a media layer that augments quotidian city space, juxtaposing the past onto the present, creating a deep, queer narrative of place. By bringing together the physical navigation of the contemporary city with the imaginative navigation of its queer past, the app enacts a praxis that I characterize as a  queer ludic traversal, one that renders the navigation itself as queer as the content that it presents. In so doing, the app produces the experience of  place, in Lucy Lippard’s (1997) formulation that  Place is latitudinal and longitudinal within the map of a person’s life. It is temporal and spatial, personal and political. A layered location replete with human histories and memories, place has width as well as depth. It is about connections, what surrounds it, what formed it, what happened there, what will happen there. (7) The app proposes that a productive and underrepresented setting for queer play is the space of the city itself, and that the hybrid reality of locative media provides specific affordances to enable particularly queer navigations, occupations, and constructions of urban space.  The app arises from, and takes shape in relation to, a range of theoretical inspirations. First are the contributions queer theories of space, the urban, and community, such as David Bell’s (2001) observation of “the special relationship between the city and the deviant” (84) and Theories recognizing the very public-ness of the formation, circulation, and inhabiting of queer identities (D’Emilio, 1983; Berlant and Warner, 1998); central here is Sara Ahmed’s theorization of “orientation” and her contention that “orientations are about the directions we take that put some things and not others in our reach” (552). New theorizations of space and place that have come to be called  the spatial turn have similarly mobilized our thinking, challenging us to imagine space as a complex social production (Lefebvre, 1992) and asking us to think through how we move in space as either  tactical or  strategic (deCerteau, 2011). Praxis-based interactivity, which I draw principally from the field of Game Studies, has introduced concepts like the fidelity context (Galloway 2004) and ambient experience (Flanagan 2009). Deep mapping offers new possibilities for modeling space, particularly historical space, by bringing together the explanatory and critical capacities of both narrative and mapmaking (Bodenhamer 2007). These theoretical methods intersect in locative media itself, the vehicle for “Go Queer” and a platform, I argue, that holds significant promise for queer scholarship and expression.  By exploring how each of these theoretical arenas is literalized in the app itself, this paper aims to provide a framework and method for other practitioners interested in deploying locative media technologies to engage queer subjects, histories, and cultural productions. ",
       "article_title":"Locative Media for Queer Histories: Scaling up \"Go Queer\"",
       "authors":[
          {
             "given":"Maureen",
             "family":"Engel",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "queer studies",
          "modeling",
          "public humanities and community engaged scholarship",
          "cultural studies",
          "feminist studies",
          "visualization",
          "English",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The methods of network analysis are becoming increasingly relevant to the digital humanities, particularly in relation to the study of literary characters (Moretti 2011; Pohl, Reitz, and Birke 2008; Park et al. 2013; Trilcke, Fischer, and Kampkaspar 2015; Elson, Dames, and McKeown 2010; Agarwal et al. 2012; Waumans, Nicodème, and Bersini 2015; Xanthos et al. 2016; Choi and Kim 2007; Bollen 2017; Fischer et al. 2017). Several recent studies and presentations have focused on drama. Most studies deal with European and American drama and this is possibly due in part due to easily available data.  However, we believe that network analysis can be used to interrogate interesting features of Javanese theatre as well. Our research focuses on  wayang kulit  (shadow puppetry), one of the oldest and most respected traditions of Southeast Asia. A typical performance lasts all night but usually focuses on a small episode of the Mahabharata, one of the two major Sanskrit epics of Ancient India that provides the narrative material for many traditional theatre forms in Southeast Asia. There are no comprehensive storylines or transcripts available in digital form, so we created our own database by digitizing and annotating the authoritative list of  wayang kulit  storylines compiled by Purwadi (2009).    We used the resulting data to construct a weighted, undirected co-occurence network at the  adegan  (scene) level. Each character is modeled as a node. An edge between two characters means they are present at the same scene, regardless of whether they interact with each other. The weight indicates the number of scenes in which both characters are simultaneously present. While certain favorite characters appear in many stories, many other characters are only present in one storyline. Thus, the network exhibits the ubiquitous characteristics of real-life social networks (Carrington, Scott, and Wasserman 2005; Knoke and Yang 2008). These include a high-level degree of heterogeneity (the number of stories per character in a bipartite projection decreases according to the distribution  P(x)  ≈  x -3.3627 , see Figure 1) and small world properties:   A low clustering coefficient (0.863) A low average shortest path (0.86)      Figure 1 . Log-log plot of stories per character in the wayang network (right). The solid lines represent the actual distribution and the dashed line the theoretical power-law distributions.   Characters in Euro-American theatre usually appear in only one play (or at best in a handful of plays) and the kind of structural analysis we conducted here is only possible in a narrative tradition with recurrent characters. We thus feel this contributes an interesting case study to the burgeoning field on network analysis of theatre characters. Beyond merely revealing some interesting quantitative (small-world) properties of this network, further quantitative analysis enabled us to identify previously unreported features of the  wayang  tradition: 1) a network-theoretical perspective reveals some unexpected insights into how various indigenous Javanese elements were integrated into the \"structure\" of the original Indian epic and 2)  there are significant differences in the network properties of characters that can be represented by \"interchangeable\" puppets and those that can not be changed. To fully appreciate the significance of these findings, a quick overview of the history and performance conventions of  wayang  is needed.   In Java (Indonesia), the recorded history of Mahabharata-derived performances dates back to the 10 th  century CE, but the performances might have an older history (Escobar Varela 2017). In any case, over this one-thousand year period, Javanese artists have invented a number of new characters and local storylines that they have interwoven with the original Sanskrit epics. However, people still readily acknowledge the stories to be mostly Indian in origin. Much to our surprise, we found out that almost half of the characters used in performance today are local in origin. Our initial estimate (and that of people informally interviewed), put the number of local Javanese characters at only 20% to 30%. Why the discrepancy? By applying network theoretical measures, we found significant differences between the Javanese and the Indian characters. Except for the  punokawan  (the clown-servants), which appear in almost every story, all local Javanese characters have low values for network-theoretical measures such as topological degree and weighted degree (Figure 2).  We hypothesize that the reason why people think Javanese characters are less prevalent than they truly are is that, on average, Javanese characters are significantly less \"important' in terms of their network-theoretical measurements.      Figure 2 . Weighted degree and topological degree of Javanese, Javanese  punokawan  (clown-servants) and Indian characters.    Our second finding relates to the usage of the puppets in performance. A complete set of puppets can be expensive and often the the  dalang  (puppeteer) would not have all the needed puppets for a given performance. In this case, they have a choice: they can either borrow a puppet from another  dalang , or they can substitute the required puppet for one that they already posses in their collections. This choice is based on weather the character is considered as \"interchangeable\" or not. We noted the interchangeability of characters based on interviews with professional  dalang.  When comparing the network-theoretical measurements against this table, we also found that all characters deemed interchangeable have low eigenvector centrality and weighted degree in our network (Figure 3). In other words, the more important a character (in network-theoretical terms), the less likely it is to be exchanged for another character.      Figure 3.  Eigenvector centrality and weighted degree of interchangeable and non-interchangeable characters.   Both of this findings seem logical in retrospect, but they were previously unreported. We hope that this application of network analysis can add to the field of network analysis of literary characters and also contribute to the scholarship on Javanese theatre. For this purpose, we are developing an interactive online portal where the contextual information for each character (Figure 4) and the values for network-theoretical measures for each character (Figure 5) and can be consulted in greater detail. All datasets will be openly available for download and we hope this will encourage other research teams to contribute to the quantitative analysis of Javanese theatre.      Figure 4.  Contextual information for a given character (Baladewa in this example), forthcoming online portal.      Figure 5.  Network-theoretical measures for a given character (Baladewa in this example), forthcoming online portal.  ",
       "article_title":"Network Analysis Shows Previously Unreported Features of Javanese Traditional Theatre",
       "authors":[
          {
             "given":"Miguel",
             "family":"Escobar Varela",
             "affiliation":[
                {
                   "original_name":"Department of English Language and Literature, National University of Singapore",
                   "normalized_name":"National University of Singapore",
                   "country":"Singapore",
                   "identifiers":{
                      "ror":"https://ror.org/01tgyzw49",
                      "GRID":"grid.4280.e"
                   }
                }
             ]
          },
          {
             "given":"Andrew",
             "family":"Schauf",
             "affiliation":[
                {
                   "original_name":"Graduate School for Integrative Sciences and Engineering, National University of Singapore",
                   "normalized_name":"National University of Singapore",
                   "country":"Singapore",
                   "identifiers":{
                      "ror":"https://ror.org/01tgyzw49",
                      "GRID":"grid.4280.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "folklore and oral history",
          "graphs",
          "creative and performing arts",
          "networks",
          "relationships",
          "English",
          "asian studies",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Our inquiry considers the speech interactions of characters within plays as a proxy for broad narrative structures. We analyze computationally-generated social networks of 37 plays by Shakespeare to see whether, and how, they can be used to distinguish between Shakespeare’s comedies, tragedies, and histories. Because dramatic performances enact social encounters, social network analysis translates surprisingly well to fictional societies. Stiller et al. have shown that social networks in Shakespeare’s plays mirror those of real human interactions, particularly in size, clustering, and maximum degrees of separation (2003). However, as fictions, these networks are shaped not only by sociological principles, but also by narrative structures. Moretti uses social networks to examine the plots of three Shakespearean tragedies, and to contrast the structure of chapters in English and Chinese novels (2011). Alberich et al. (2002) and Sparavigna (2013) also discuss the interplay between social and narrative constraints on networks. We emphasize this distinction to look for specifically literary features of our networks. Recent papers presented at DH2017 sought ways to richly quantify the details of one or two plays (Fischer et al., 2017; Tonra et al., 2017). At another scale, Algee-Hewitt examined 3,439 plays by looking only at the Gini Coefficient of each play’s eigenvector centrality (2017). With our three dozen plays, we attempt to strike a fruitful middle ground in the inevitable balancing act between detail and scale. Each play is considered individually, but at a level of abstraction which allows rapid and direct comparisons.   Creation of social network graphs Our parser tracks characters present on stage during speech. This approach is highly extensible: it can parse any play that follows TEI P5 guidelines for performance texts. Each speaking character is connected to all characters currently present on stage. These connections are recorded in a network graph, with characters as nodes and shared speech as edges. Edges are directional, and weighted based on the number of lines spoken.  In future, we plan to extend our parser to identify the specific addressees of a character’s speech, allowing us to model more detailed interactions. To verify that our parser is accurate, we compare our generated network of Hamlet to Moretti’s well-known handmade model of that play (2011). Despite some minor differences in peripheral characters like “Servant”, and our less-minor difference of including the play-within-the-play, the two networks are highly similar. Our network graph supports Moretti’s reading. Our tool also improves on Moretti’s model by adding direction and weight to each connection. Although this level of detail turned out not to be necessary for the basic task of using network graphs to distinguish between Shakespeare’s genres, it may be useful in future work examining a less homogenous corpus of plays, or in work asking different questions about this corpus.   Using networks to identify genre We then use our generated network graphs to test our central question: whether the social network enacted by a play’s characters can be used as a proxy for features of the play’s narrative content. More specifically, we ask whether social networks can be used to distinguish between the dramatic genres of tragedy, comedy, and history. Using a support vector machine with fivefold validation, we tested 17 different mathematical features of the networks. No single feature was independently sufficient to identify the genre, though graph density came closest (83% accuracy). However, if features are used in combination, the network graphs can indeed achieve full accuracy. One combination of features which does achieve 100% accuracy is edges, words, and degree. We are currently exploring other combinations that might also be capable of accurately identifying genres.   Discussion  History, comedy, tragedy The potential utility of graph density in distinguishing genres is visually obvious when individual comedy and history networks are compared. Histories feature highly dispersed networks, with large numbers of very minor characters, such as “First,” “Second,” and “Third” members of groups like soldiers and ambassadors, who each interject briefly in a single scene. Connections form chains of acquaintance with little overlap, so even the monarchs have low eigenvector centrality.   Social network graphs of the histories  Henry VI, Part 2 and  Henry V.   Comedies, in contrast, feature networks with far fewer characters, in which nearly everybody speaks to nearly everybody else at some point. Although comedies often have multiple subplots, these separate stories do not result in highly-separated networks. We theorize that comedic networks are strongly shaped by the plays’ final “resolution” scenes, which bring together the full cast. The average eigenvector centrality of the characters in comedies is much higher than in tragedies or histories; this suggests that many more of the characters in a comedy are “important,” reflecting a focus on ensemble stories.   Social network graphs of the comedies  The Comedy of Errors and  A Midsummer Night’s Dream.   Graph density is insufficient, however, to fully distinguish the tragedies, which feature networks somewhere between history and comedy in their density. They often have a dense core with a secondary ring of more peripheral characters. What seems to distinguish them is the existence of the central tragic hero, whose influence directly touches more of the network than the protagonists of histories, but whose connections are less interconnected than the ensembles of comedies. These subtleties are better captured, it seems, by the combined metric of “edges, degree, and words.”   Social network graphs of the tragedies  Othello,  King Lear,  Macbeth, and  Hamlet.     The \"problem plays\" We then use our preliminary identification of each genre’s features to examine Shakespeare’s various contested genres. Training our model only on the plays for which there is strong consensus, we applied it to the “Roman plays,” the “problem plays,” and the “romances” in turn. Of the Roman plays, all but  Antony and Cleopatra are identified as tragedies by every metric;  Antony and Cleopatra is identified by “edges, words, and degree” as a history and by “degree, modularity, and density” as a comedy. Of the problem plays,  All’s Well that Ends Well is always identified as a comedy;  Troilus and Cressida and  Measure for Measure are both identified as a comedy by all metrics except for “edges, criticality, and degree”, which identify them as tragedies. The four romances, despite visually unusual networks which support literary arguments that Shakespeare’s writing had grown more experimental at the end of his career, are identified as comedies by every mathematical metric. We treat none of these identifications as definitive declaration of the plays’ “real” genres, but use them to distinguish between plays whose generic ambiguity lies in their subject matter, and plays whose ambiguity lies in their structure.     Conclusion Our parser successfully and rapidly produces sophisticated social network graphs of TEI plays that can be used to computationally identify theatrical genre in Shakespeare’s plays. Thirty-seven plays is a small scale for this approach: since the parser is highly extensible and can be used with any plays encoded in TEI, future work need not be restricted to the Early Modern period. It need not even be restricted to works written in English. Our networks of the well-studied works of Shakespeare can provide a baseline against which to contextualize analysis of these elements in works for which there is far less critical consensus.  ",
       "article_title":"Analyzing Social Networks of XML Plays: Exploring Shakespeare’s Genres",
       "authors":[
          {
             "given":"Lawrence",
             "family":"Evalyn",
             "affiliation":[
                {
                   "original_name":"University of Toronto, Canada",
                   "normalized_name":"University of Toronto",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03dbr7087",
                      "GRID":"grid.17063.33"
                   }
                }
             ]
          },
          {
             "given":"Susan",
             "family":"Gauch",
             "affiliation":[
                {
                   "original_name":"University of Arkansas, United States",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Manisha",
             "family":"Shukla",
             "affiliation":[
                {
                   "original_name":"University of Arkansas, United States",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "graphs",
          "computer science",
          "corpora and corpus activities",
          "networks",
          "english studies",
          "text analysis",
          "visualisation",
          "relationships",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In working with British colonial records and German church manuscripts of colonized and missionized landscapes in the North American mid-Atlantic, the authors have grappled with the problem of polynymy in their attempt to create a gazetteer of places. As Presner and Shepard (2016) have argued, unlike conventional positivistic approaches to mapping, DH and geohumanities have developed a rich vocabulary with which to describe and analyze the human perception of place. Whether through “deep maps” that recount the stories of place and experience or through the multiple layers of temporally inflected information, the spatial turn has revealed the need to see the practice of mapping as “arguments or propositions that betray a state of knowledge.” (Presner and Shepard 2016, 207). However, whereas there are sophisticated models of temporal-spatial mapping now available to DHers working with historical materials, to date little critical attention has been paid to the place/person variable. The work of Ann Knowles (and her students) has paved the way for sophisticated representations of the experience of place (Knowles 2008; 2015). In her arguments for a nonpositivistic geo-practice within the humanities, Knowles has opened up the field to the “fuzzy data” of critical humanistic inquiry. Privileging design over data, Knowles’ prize-winning visualizations of the Holocaust challenge us to reconsider in sophisticated ways the experience of landscapes. (Knowles 2014 ) On a similar path, as Presner and Shepard conclude, virtual reality and gaming allow for an experiential and avatar-based investigation of dynamic, embodied, albeit presentist, multiple perspectives of place. Students at Bucknell have already produced sophisticated critical cartographical visualizations of the Susquehanna river in the Colonial period that draw in part on Knowles’ perspectives. This paper will explore the problem of creating a gazetteer of colonized landscapes, specifically those of the mid-Atlantic in the 18th century, in which the name of a place (toponym) changes depending on the person or political entity who is describing that place. In colonized landscapes, there can be multiple names for one place. Maps of this period are veritable palimpsests of conquests and defeats; and travel diaries, mission records and letters contain accounts of human experience of places that are multiply identified. The task is made more complicated still when one factors time into the equation: when competing spatial identities persist across generations. Using the case study of the research project “Moravian Lives” we will ask how we can create a gazetteer of places using authority IDs, when that very authority is itself the product of apolitical-historical struggle. “Moravian Lives” is an international collaborative DH project that aims to make available to the scholarly and lay community the vast corpus of life writings of members of the Moravian Church from the mid-18th century to today (http://moravianlives.org). Facing the simultaneity of multiple names for a place, can we create a system of “triples” that satisfactorily reflects the multiple perspectives and presence or absence of agency of those who name place? Drawing on the substantial cultural-historical GIS of the Susquehanna river produced by Faull and a team of Bucknell staff and students that supported the Department of the Interior designation of the Susquehanna River as a National Historic Water Trail in 2012, the Moravian Lives gazetteer aims to provide the most comprehensive place-name resource for researchers in many fields. The construction of an historical gazetteer for Moravian Lives involves complexities that arise from not only the naming of places but also how their spatial identities reflect respective, concurrent relationships to those places by Native American peoples, Moravian missionaries,and colonial representatives. There are multiple names for a single place as well as multiple understandings of place names, and these differences depend on who it was who did the naming. An example of this challenge is 18th-century Shamokin in Pennsylvania. Shamokin was at that point an Iroquois settlement at the confluence of the north and west branches of the Susquehanna River, encompassing the shores of both branches and an island at the river’s fork. To Shikellamy, an Oneida emissary of the Six Nations of the Iroquois or Haudenosaunee, who oversaw the Algonquin-speaking nations of the Lenni Lenape, Shawnee, and Mahican in Iroquoia (present-day Pennsylvania and New York), and who lived in the town in the 1740s, “Shamokin” would have constituted the whole area of the rivers’ confluence. To Count Nikolaus von Zinzendorf, the founder of the Moravian Church who visited Shikellamy in 1742, “Shamokin” represented an opportunity for Moravian missionaries offered to them by Shikellamy in the form of space for a blacksmith’s shop and mission. While the location of that mission was small, it loomed large in Zinzendorf’s interest in founding “Heiden-Collegia”, or colleges of the “heathen”, in Pennsylvania. To Conrad Weiser, a German settler and negotiator between the colonial government in Philadelphia and the Indian nations, and who worked with Shikellamy on several treaties between the Iroquois and the Colonial government, “Shamokin” would have represented a strategic and ultimately military outpost that would become the site of Fort Augusta during the French and Indian War. These “Shamokins” co-existed, with Native American, Moravian, and Colonial inhabitants and visitors relating to it in discrete yet overlapping ways. One byproduct of our work on the gazetteer could thus be the proposition of authority lists to the OCLC’s VIAF council, thereby introducing and linking our information where there is currently no match. In compiling a gazetteer we realize that there is already a VIAF authority ID for Shamokin that is recognized by the Library of Congress/NACO but refers to another (modern) place called Shamokin some 18 miles to the east. (Shamokin, PA VIAF ID: 146606881 (Geographic). We cannot therefore “re-mint” an authority name for these Shamokins. Furthermore, a part of the 18th century Shamokin is now Sunbury (the site of Fort Augusta and Shikellamy’s grave) also has its own VIAF ID, (3 Sunbury, PA VIAF ID: 123181256 (Geographic) but, for the historical and cultural studies scholar, it might be inaccurate, misleading, and in some ways irresponsible to equate Sunbury with or consider it as a variant for the historic Shamokin. How can we recognize spatial multivalence (or “polynymy”) in the Moravian Lives gazetteer? How does the scholar act responsibly while acknowledging their own potential complicity in political-historical renegotiations and multiple cultural understandings of place? In effect, must we not push back at the idea of *an* authority, and work toward a system that recognizes and synchronizes multiple authorities? We propose a two-phased approach to developing the Moravian Lives gazetteer, which will expand geographically to places beyond North America and will need to resolve polynymic complexities in Central Europe, the Arctic areas of Greenland and Newfoundland, the Caribbean, South Africa and Australia. The first phase involves “stabilizing” all of the place names without giving primacy to any one of them. Each would be assigned a unique HTTP URI offering information about each toponym pertinent to its own cultural relationships and link to its siblings. In this way we can push back against the need to choose one authority (whether it be restoring an indigenous name or opting among European ones) and demonstrate that these names are not “same as” or “variants” of the others. This, in turn, allows us to reflect upon colonial places in a much more nuanced way that takes into account geographical features and proximity (viz. ‘Peace huts on the Susquehanna’, ‘an der Höhle bei Bethel’). It also enriches the companion personography under development for Moravian Lives. In the visualization already available through Moravian Lives, each person is associated with place using a single-point Google location (see Figs. 2 and 3); but by integrating the cultural historical mapping already completed for the Susquehanna river project, we can now connect these people with better suited vector data referencing each unique place’s footprint or range at the same time acknowledging that our identification involves a consideration of certainty (or “fuzziness”) by the editor. Through this process, we will strengthen the interlinking of temporo-spatial data within the Moravian Lives project, weaving together the text-based gazetteer with the mapped data. The second phase is to submit our set of authority files to the OCLC and its VIAF council through a member advocate (such as the Moravian Archives). Our work will then be reviewed, assessed against existing identified geographic places in the VIAF database, and where appropriate we hope that new VIAF IDs will be minted. In this way we will make these places discoverable to other researchers considering similarly complex cultural landscapes. ",
       "article_title":"Resolving the Polynymy of Place: or How to Create a Gazetteer of Colonized Landscapes",
       "authors":[
          {
             "given":"Katherine Mary",
             "family":"Faull",
             "affiliation":[
                {
                   "original_name":"Bucknell University, United States of America",
                   "normalized_name":"Bucknell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00fc1qt65",
                      "GRID":"grid.253363.2"
                   }
                }
             ]
          },
          {
             "given":"Diane Katherine",
             "family":"Jakacki",
             "affiliation":[
                {
                   "original_name":"Bucknell University, United States of America",
                   "normalized_name":"Bucknell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00fc1qt65",
                      "GRID":"grid.253363.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "modeling",
          "linking and annotation",
          "cultural studies",
          "indigenous studies",
          "multilingual / multicultural approaches",
          "visualization",
          "English",
          "geography",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction How humanities scholars communicate their research - with one another, with interdisciplinary communities, and with diverse publics - continues to shift with the emergence of new publishing models. We do not understand enough about why scholars choose to publish in different modalities, or what the implications of their choices are for the use, evaluation, and sustainability of research. Thus, publishing systems and services lag behind the advance of digital methods and modes of communication. This paper presents selected results of a multimodal study of humanities scholars' digital publishing needs. Building on national survey of humanities scholars in the United States, initially reported at DH2017 (Senseney et al., 2017), this paper describes preliminary outcomes of a series of interviews with humanities scholars who have a manifest interest in experimental digital publishing. This study seeks to deepen our understanding of scholarly goals for digital publication. Outcomes of this study are guiding the development of a service model for library-based humanities publishing, as part of the Publishing Without Walls (PWW) project (http://publishingwithoutwalls.illinois.edu/). Funded by the Andrew W. Mellon Foundation, the University of Illinois Library is leading the PWW initiative in partnership with the Graduate School of Library and Information Science, the Illinois Program for Research in the Humanities, and the African American Studies Department at the University of Illinois. PWW aims to develop a scalable, shareable model for monograph publishing within libraries, with the goal of bridging gaps in current publishing systems, such as gaps between the complex materials scholars want to publish and what existing systems can accommodate, between scholarly practices and existing publishing tools, and between publishing opportunities at resource-rich and under-resourced institutions. This paper focuses on humanities scholars' motivations for publishing digital, open access, and multimedia monographs. We explore three central motivations for digital publishing: (1) the desire to reach diverse audiences; (2) the desire to integrate interactive, multimedia, and linked evidence; and (3) the desire to publish \"living\" documents. These factors have implications for digital humanities scholars in understanding the impact of different modes of sharing, for libraries seeking to support digital scholarship, for data models underlying enhanced publications, and for publishing service models.   Methods This study comprised a set of semi-structured interviews with humanities scholars. Interview participants were self-selected from among scholars who had already participated in the PWW initiative in some way, whether by attending publishing workshops or submitting to the new series. Nineteen interviews have been conducted to date; more are planned for summer 2018. All interviews are recorded and transcribed, and a formal analysis of resulting transcripts is underway. Participants are all affiliated with academic institutions. They include faculty, postdoctoral research associates, and academic professionals with backgrounds in humanities disciplines, information science, and communications.   Three motivations for enhanced digital publishing  Multiple audiences Scholars turn to open access (OA) monograph publishing to increase impact by reaching more readers, not only within their disciplines but also cross-disciplinary peers and the general public. Visibility and broad dissemination are established motivations for OA book publishing; evidence suggests that these motivations are rewarded, as OA books receive significantly more usage and citation than non-OA counterparts (Emery et al., 2017). Yet, our study indicates that humanities scholars want more than to reach large audiences. They want to reach diverse audiences, ranging from peers in other disciplines to practitioners, policymakers, and the public. Despite potential impact, participants acknowledged that certain prevalent models of OA monograph publishing suffer from a lack of \"institutional weight\" and \"automatic audiences.\" However, participants described leveraging their own social and research networks to promote their work directly.   Interactive, multimedia, and linked evidence Authors pursue opportunities for representing new kinds of evidence in new contexts. The potential benefits of multimedia publishing are largely unrealized in publishing practice due to the challenges of managing complex digital publications (Jankowski et al., 2012). Scholars want to integrate or actionably link to more kinds of evidence, including multimedia sources, interactive visualizations, data sets, and curated collections. They also want to make their sources interactive, to allow readers opportunities to visualize, explore, and assess bodies of evidence while anchoring them to narrative descriptions and interpretations. One participant described his primary goal for multimedia publishing as making evidence \"come alive in a narrative history.\"   Living documents Some humanities scholars want to publish what participants call \"living,\" evolving documents —works-in-progress that are subject to indefinite change. Participants value immediacy of entrance into ongoing scholarly dialogue, both for obtaining rapid feedback from peers and for flag-planting. Some participants see self-publication as a route toward obtaining high-quality peer review more quickly than through the conventional publication; the complexity of peer review in interdisciplinary settings — like the digital humanities — can lead to dilatory, frustrating review processes, which one participant compared to \"the phenomenon of too many cooks in the kitchen,\" and which may yield \"diluted\" end work. The ultimate manifestation of a \"living\" document is a publication that facilitates ongoing co-authorship, annotation, interlinking, and revision. One participant described an ideal publication as an online document that \"people can comment on, that can directly link to its sources and other people can link to it, that has an attached data set of results that other people can make use of and check,\" and which is subject to versioning. He described this as an evolving or living document and noted that, \"at the moment, most of our research papers are dead documents.\"    Future work While openness is a core value of digital humanities scholarship (albeit with qualifications see, e.g., Spiro, 2012), it is not clear how different modes of publication can most effectively open humanities research: to the stratified audiences identified in this study, to deep interaction with sources, and to ongoing evolution. This paper describes outcomes of our study on what humanities scholars need from the next generation of publishing systems and services, and how this study is guiding development of a new model for library-based publishing that can support and sustain highly diverse and broadly impactful research products.  ",
       "article_title":"Audiences, Evidence, and Living Documents: Motivating Factors in Digital Humanities Monograph Publishing",
       "authors":[
          {
             "given":"Katrina",
             "family":"Fenlon",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Megan",
             "family":"Senseney",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Maria",
             "family":"Bonn",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Janet",
             "family":"Swatscheno",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Christopher R.",
             "family":"Maden",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign, United States of America",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "copyright",
          "repositories",
          "Open Access",
          "etc.",
          "sustainability and preservation",
          "licensing",
          "public humanities and community engaged scholarship",
          "library & information science",
          "English",
          "archives",
          "interdisciplinary & community collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Related Studies The idea that \"quantitative dominance relations\" represent an \"important parameter for the central or peripheral position of a character\" in a drama has been established by Pfister in his crucial structuralist monograph on the analysis of drama (Pfister 1997, p. 227). Digital-empirical studies after Pfister have tested different approaches to provide quantitative descriptions of the dramatis personae. Moretti's suggestion to tie the detection of the protagonist to the network analytical criterion of average distance (Moretti 2011, p. 4) was rejected as too simplistic (Trilcke 2013, p. 204), although this network-analytical approach was taken up by numerous studies. In this vein, Jannidis et al. (2016) not only calculated quantitative measures for the frequency of a character's appearance, but also the weighted degree to determine the accuracy of the identification of main characters. Moretti himself has adjusted his approach conceptually, insofar as he has shifted the focus from the 'protagonist' to a relationally defined concept of 'centrality'. He also emphasised the tension between different criteria (like  word space and  character space) not as a deficit, but as a productive factor of a multidimensional quantitative analysis of the dramatis personae (Moretti 2013, pp. 5–9). Moretti's basic ideas – the productive multidimensionality of quantitative analysis and the insight into the relational conceptualisation of quantitative character classification – have been taken up by Algee-Hewitt (2017), who worked with two network-analytical centrality measures (betweenness centrality and eigenvector centrality) and tried to examine the quantitative distribution of the cast of a play.    Goal and Procedure We will conceptually discuss and complement available approaches to the quantitative description of characters in dramatic texts and test them on the basis of a corpus of 465 German-language dramas. The aim in theoretical and conceptual terms is to gain a better understanding of the dimensions of quantitative character analysis and to present diachronic and typological insights into quantitative dominance relations in German-language drama from 1730 to 1930. The subject of the analyses is the DLINA corpus (Fischer & Trilcke 2015). The data is calculated using the Python tool \"dramavis\", which has been supplemented for this purpose with new analysis modules (Kittel & Fischer 2017). In the first step, we examine the multidimensionality of quantitative descriptions as determined by Moretti (chapter 3). In a second step, we will take up Algee-Hewitt's (2017) proposed approach of working with quartiles and discuss it on the basis of the data from the DLINA corpus (chapter 4). In the third step, we present an approach that describes the quantitative distribution of characters in a play (section 5).   The Correlation of Count-Based and Network-Based Rankings of Characters The above-mentioned approaches have made use of various measures for the quantitative description of dramatic characters, which can be divided in two groups: count-based measures, such as the number of words expressed by a character, and network-based measures, mostly centrality measures. According to Moretti 2013 (cf. also Jannidis et al. 2016), these two descriptive 'dimensions' can differ considerably. In order to systematically describe the extent of this deviation, we calculate eight values for each character of the 465 dramas of our corpus, three count-based measures (number of scenes a character appears in, number of speech acts, number of spoken words) and five network-related measures (degree, weighted degree, betweenness centrality, closeness centrality, eigenvector centrality). For each measurement a ranking is created. The rankings are then merged into two meta-rankings: one count-based and one network-based. The two meta-rankings are then combined into an overall ranking. To determine the deviation between the two meta-rankings, we calculate the ranking correlation coefficient Spearmans Rho and check how strongly the two meta-rankings correlate with each other for all dramas of our corpus (fig. 1).    Spearmans Rho for the correlation of count-based and network-based measures.   Complete congruence of the meta-rankings is an exception. In fact, the different measures capture different 'dimensions' of the quantitative character hierarchy. In order to better understand these dimensions, five dramas (marked by the green dots in fig. 1) are examined in more detail and discussed in this paper (see figs. 2, 4, 6, 8, 10 for rankings, figs. 3, 5, 7, 9, 11 for the network graphs of these dramas).    Rankings for Lessing's \"Emilia Galotti\" (1772) – Spearmans Rho: 0,917.      Network graph for Lessing's \"Emilia Galotti\" (1772).      Rankings for Iffland's \"Das Erbtheil des Vaters\" (1802) – Spearmans Rho: 0.806.      Network graph for Iffland's \"Das Erbtheil des Vaters\" (1802).      Rankings for Schiller's \"Die Jungfrau von Orleans\" (1801) – Spearmans Rho: 0.672.      Network graph for Schiller's \"Die Jungfrau von Orleans\" (1801).      Rankings for Anzengruber's \"Der Meineidbauer\" (1871) – Spearmans Rho: 0.442.      Network graph for Anzengruber's \"Der Meineidbauer\" (1871).      Rankings for Wedekind's \"Franziska\" (1912) – Spearmans Rho: -0.222.      Network graph for Wedekind's \"Franziska\" (1912).   As it turns out, the deviations usually affect characters at the bottom of the hierarchy. Here the network-related measures are particularly sensitive to types of clustering in secondary scenes of a drama (figs. 8 and 9), which has even more severe effects if a drama is quantitatively dominated by very few characters (figs. 10 and 11). On the other hand, both meta-rankings are very similar for the quantitatively dominant characters (top 1 and top 1 or 2). These observations can serve as an argument that the multidimensional description is less relevant to discuss protagonists, but rather for the characterisation of quantitative dominance relations within the cast as a whole.   Percentage of Quantitative Dominant Characters Algee-Hewitt 2017 made a suggestion for the characterisation of the quantitative distribution of a cast, albeit with a continuing focus on quantitatively dominant characters. Working with an English-language drama corpus of several thousands of plays, he calculated the eigenvector centrality of the characters and then calculated the percentage of characters located in the upper quartile of the distribution. We have reproduced this test with our corpus – for all measures mentioned above. As an example, the box plots show the values for the eigenvector centrality (fig. 12) as well as for the count-based measure 'number of words' (fig. 13).    Percentage of characters in the upper quartile according to their eigenvector centrality, grouped by decades. Blue line: median.      Percentage of upper quartile characters according to number of words, grouped by decades. Blue line: median.   It is interesting to note that the values we calculated for eigenvector centrality (fig. 12) are significantly higher than the values presented by Algee-Hewitt (for the period of time covered by our corpus the median is lower than 0.15). A comparison of fig. 12 and fig. 13 shows that the network-related measures usually locate a larger percentage of characters in the upper quartile. From a network-analytical point of view, a drama tends to be dominated by several characters. The two curves also tend to follow the same course, especially regarding the big flattening curve in the decades after 1740. What we can see there is the reduction of the percentage of quantitatively dominant characters ('main characters') and the emergence of quantitatively less dominant characters ('secondary characters', 'atmospheric characters').   More Detailed Distribution Analysis With a focus on quantitatively dominant 'main characters', Algee-Hewitt's approach attempts to describe types of distribution of the dramatis personae and thus to identify 'dominance relations' via distribution analyses. Following analyses of the 'small world' phenomenon in drama (Trilcke et al. 2016), we propose to extend this approach and develop a typology of quantitative distribution of characters in dramatic texts. To this end, we take the above-mentioned eight quantitative measures, calculate the distribution of character values across deciles and subject this distribution to a regression analysis (tests on linear, exponential, quadratic and power-law distribution; typologisation according to the regression line with the highest coefficient of determination). Examples in figs. 14 to 19 show the results for three of the dramas discussed above, for a network-related measure (weighted degree) and a count-based measure (number of words).    Lessing's \"Emilia Galotti\" (1772) – decile distribution of weighted degree, y-axis: number of characters.    Lessing's \"Emilia Galotti\" (1772) – decile distribution of number of words, y-axis: number of characters.      Anzengruber's \"Der Meineidbauer\" (1871) – decile distribution of weighted degree, y-axis: number of characters.    Anzengruber's \"Der Meineidbauer\" (1871) – decile distribution of number of words, y-axis: number of characters.      Schiller's \"Die Jungfrau von Orleans\" (1801) – decile distribution of weighted degree, y-axis: number of characters.    Schiller's \"Die Jungfrau von Orleans\" (1801) – decile distribution of number of words, y-axis: number of characters.   These typologies are calculated for all eight values and for all 465 dramas – we will present the results for the corpus as a whole at the conference. With our approach, a more precise, multidimensional description of typical quantitative dominance relations in drama will be possible. The increase in the number of less dominant characters observed on the basis of our quartile analysis (figs. 12–13) will be described with more precision and supplemented by a more differentiated examination of types of 'middle characters'.   Summary This talk brings together several approaches for the quantitative analysis of characters in literary texts, discusses the potential of a multidimensional description beyond top characters (protagonists) and suggests an approach for typologising quantitative dominance relations within the cast of a drama.  ",
       "article_title":" To Catch a Protagonist: Quantitative Dominance Relations in German-Language Drama (1730–1930)  ",
       "authors":[
          {
             "given":"Frank",
             "family":"Fischer",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics, Moscow, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Peer",
             "family":"Trilcke",
             "affiliation":[
                {
                   "original_name":"University of Potsdam",
                   "normalized_name":"University of Potsdam",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03bnmw459",
                      "GRID":"grid.11348.3f"
                   }
                }
             ]
          },
          {
             "given":"Christopher",
             "family":"Kittel",
             "affiliation":[
                {
                   "original_name":"University of Graz",
                   "normalized_name":"University of Graz",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/01faaaf77",
                      "GRID":"grid.5110.5"
                   }
                }
             ]
          },
          {
             "given":"Carsten",
             "family":"Milling",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics, Moscow, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Daniil",
             "family":"Skorinkin",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics, Moscow, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "graphs",
          "german studies",
          "networks",
          "text analysis",
          "relationships",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In recent years, the use and development of Podcasts has significantly grown. Podcasts allow us to listen to topics we are interested in and learn more about an issue or community. Podcasts like  This American Life,  Radio Ambulante and  Latino USA, put at the center of their stories experiences of people and places. Indeed, using audio as a medium to tell the larger stories of our community has proven successful as signaled by all top 10 iTunes podcasts—5 of which are documentary style. Creating university based podcast like,  Ohio Habla, will allow us to connect and learn more the Latin@/Hispanic experiences locally, while amplifying the voices of the community everywhere. Language and cultural studies are in a unique position to utilize this medium to advance the understanding of how culture and language is both transmitted and analyzed.  The  Ohio Habla podcast is primarily produced by students in advanced Spanish language and Latin@ studies classes together with their professor. Each student plans, researches, secures a podcast guest and carries out the interview. Students are able to continue to develop their written, reading, speaking and listening skills and they are responsible to produce one whole 30-45 minute podcast.    Ohio Habla is an extension of the digital oral history project, ONLO (oral narratives of Latinos/as in Ohio), however, it focuses topics, rather than life history. On the other hand, in the case of Latin@ students, they collect family stories, instead of interviewing a member of the community. Podcasting can help document issues that are of interest to our community and potentially be able to share it more widely. Finding new and real ways to use language and storytelling is of great benefit to our students, and podcast in the foreign language classroom can accomplish this. Our own teaching methodology here at Ohio State encourages second language learners to use the language communicatively and in real situations that are as authentic as possible. Podcasting is a great way to use the language in real and creative ways, and most importantly, in community—an element that is often left out the foreign language classroom for various reasons (mainly, time).   Teaching methodologies As a pilot project, this use of podcasting in the classroom may pave the way for further research opportunities about the benefit of podcasting in advanced language courses, service-learning and heritage language learners. This course enhancement will also provide students with a structured opportunity to make deeper connections with Latino/a campus community, to reflect on that experience, and to gain interviewing skills that will serve them in the future. Additionally, students will be instructed in (1) Language and intercultural skills 2) Organizational and professional skills through the research of a topic, securing a guest that can speak about the topic, preparing the guest with agreed up points of conversation, and practicing before recording the interview (3) Technical skills through using recording equipment and editing software. ",
       "article_title":"Latin@ voices in the Midwest: Ohio Habla Podcast",
       "authors":[
          {
             "given":"Elena",
             "family":"Foulis",
             "affiliation":[
                {
                   "original_name":"OSU, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "folklore and oral history",
          "audio",
          "spanish and spanish american studies",
          "digital humanities history",
          "public humanities and community engaged scholarship",
          "cultural studies",
          "video",
          "theory",
          "English",
          "epistemology",
          "interdisciplinary & community collaboration",
          "criticism",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" What is a literary character made of? To this question, a pragmatic answer is to say that it exists as a result of a chain of different linguistic elements, scattered throughout the text. The aim of this paper is to propose a digital method for collecting these elements, so as to analyse their nature, to observe their repartition in texts, and, ultimately, to contribute to a deeper understanding of the functions the literary device called “character” assumes in a text. Projects dedicated to named-entity recognition put a great deal of effort into using Natural Language Processing (NLP) techniques for identifying names of people, places and organisations mentioned in various types of discourses, especially political ones, as well as the co-referential chains built on the basis of these names. However, in spite of important advances in the field, much remains to be done in order to train the computer to link correctly various phrases referring to the same entity, as well the pronouns pointing to it (see Schnedeker and Landragin, 2014). In our case, we are interested in such elements of a co-referential chain that bear characterization features, and this is, inevitably, a supplementary complication. In addition, we are interested in certain elements (eg. “his brother” in the phrase “John is his brother”) that are often left aside in named entity recognition, as performing another functions than strictly pointing towards an entity. Therefore, NLP techniques did not appear adapted to our needs. We will therefore resort to “crowd-reading”, as another means, offered by the explosion of the digital sphere, to make sense from texts. Very similar to the crowdsourcing, the crowd-reading asks to benevolent contributors to annotate a document, bringing in their own view and understanding, instead of transcribing, or adding in information based on a (sometimes external) form of authority. Considering the nature of the work to be done, the crowd-reading appeared as a valid technique in our case.  In a first stage, we submitted a short text (Julio Cortazar “Continuidad de las parques”) to the manual annotation of a hundred students from our universities. This brought to the fore the sheer variety of elements considered to be participating to the characterization of a literary “person” (nouns and adjectives, of course, but verbs and adverbs too), as well as the need to dispose of a controlled vocabulary allowing to understand what kind of characterization each respondent attached to the various strings of characters selected as participating to this function.  In a second phase, we have decided to build an interface, offering a more ergonomic experience to our respondents, and allowing us to extract automatically the linguistic elements selected, as well as to group them by categories. Built with XML Mind, this interface is in fact based on a text lightly encoded with TEI tags, in which our respondents add, every time they select a string of characters, an <rs> tag, bearing in addition two attributes: a @key attribute, defined by each respondent every time he or she encounters a new character. The keys are subsequently available for reuse in the rest of the text. We expect the number of keys to vary considerably from a reader to another. an @ana attribute, with a set of constrained values. Based on another project of character analysis, these values have been defined in Galleron, 2017, and cover aspects such as the ontological type of a character, its sex, age, family position, nationality, occupation, and so on. The text submitted to annotation has been changed for this second experience: it concerns now the “Jardin aux sentiers qui bifurquent” (“Jardín de los senderos que se bifurcan”) by Jorge Luis Borges. At the date of this proposal, the second campaign of crowd-reading has not started yet. We'll have a significant number of answers before the DH conference. Our respondents will be recruited again amongst the students enrolled in literary studies in our universities: while they have a certain level of training in linguistics, literature and poetics, so as to be able to recognise the type of linguistic elements we look for, their reading still remains close of the “non-informed”, “amateur” reading of the \"man in the street\" (see Dufays, 205).  The results will be analysed so as to observe what kind of linguistic units have been identified most often, and what kind of values of the @ana attribute have been mobilised most often. We will further discuss the divergences between the selected elements, and those we were expecting to be selected. This will allow us, on the one hand, to suggest a possible use of our interface as a remediation tool in literary studies, for students with difficulties in extracting pertinent information from a text, so as to respond a specific task. On the other hand, we will advance an hypothesis about the observed distribution of the most frequent elements of characterization, that are far to appear where, intuitively, one would expect them to be grouped together (so as to “introduce” the character) as shown by our first campaign of crowd-reading, and by our own annotation endeavours. ",
       "article_title":"Spotting the Character: How to Collect Elements of Characterisation in Literary Texts?",
       "authors":[
          {
             "given":"Ioana",
             "family":"Galleron",
             "affiliation":[
                {
                   "original_name":"U. Sorbonne Nouvelle Paris 3, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Fatiha",
             "family":"Idmhand",
             "affiliation":[
                {
                   "original_name":"U. de Poitiers",
                   "normalized_name":"University of Poitiers",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/04xhy8q59",
                      "GRID":"grid.11166.31"
                   }
                }
             ]
          },
          {
             "given":"Cécile",
             "family":"Meynard",
             "affiliation":[
                {
                   "original_name":"U. d'Angers",
                   "normalized_name":"University of Angers",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/04yrqp957",
                      "GRID":"grid.7252.2"
                   }
                }
             ]
          },
          {
             "given":"Pierre-Yves",
             "family":"Buard",
             "affiliation":[
                {
                   "original_name":"U. de Caen",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Julia",
             "family":"Roger",
             "affiliation":[
                {
                   "original_name":"U. de Caen",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Anne",
             "family":"Goloubkoff",
             "affiliation":[
                {
                   "original_name":"U. de Caen",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-28",
       "keywords":[
          "and curriculum",
          "spanish and spanish american studies",
          "ontologies",
          "teaching",
          "pedagogy",
          "content analysis",
          "text analysis",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction An understanding of the intellectual and social structures of Digital Humanities (DH) has been sought by many scholars; some have pointed to the potential usefulness of quantitative methods in such analyses (McCarty, 2003; Terras et al., 2013). A few existing studies have applied quantitative methodologies to analyse publication, conference and social media data (e.g. Nyhan and Duke-Williams, 2014; Weingart, 2016; Grandjean, 2016). This study not only incorporates such approaches but extends them by integrating new analysis and visualisation methods into the wider study of DH’s intellectual and social structures. In this paper, we will introduce research on the citation and social network of DH that is giving rise to new understandings of the field’s community structure; scholarly interactions; disciplinary development; and formal/informal communication channels. The citation network of Author Co-Citation Analysis (ACA) comprises 22,321 cited authors across 52,823 cited references from the three core DH journals, while the social network of Twitter Co-Retweet Analysis comprises 3,160 Twitter users and 5,929,609 tweets. To the best of our knowledge, this study is the first to combine bibliometric and social network methods to visualise and compare the DH communities and to uncover their histories. This research contributes to ongoing discussions and debates about the DH knowledge and community structures (Gold and Klein, 2016).   Data Analysis  Citation network The Author Co-Citation network study was presented at DH2017; this paper extends this earlier analysis. For reasons of clarity, we here give a brief overview of this research. The network has been constructed by collecting the citation data of three core DH journals up to December 2016:  Computers and the Humanities ( CHum),  Digital Scholarship in the Humanities ( DSH) and  Digital Humanities Quarterly ( DHQ). 2,582 articles with 52,823 cited references were collected (see Figure 1).     Figure 1. number of articles collected each year (1966-2016) By using  fractional non-self-citation count and  exclusive co-citation count (Zhao and Strotmann, 2008), the weights of nodes and links respectively were calculated for visualisation using the software VOSviewer 1.6.7 (van Eck and Waltman, 2010). An author name disambiguation method (Strotmann et al., 2009) was used, and 22,321 unique cited authors identified. Where possible, other pertinent information was collected (i.e. author full names, gender, country of affiliation, etc.). After counting the occurrences of two authors being cited together, ACA shows the DH intellectual structure and influential topics and scholar groups (Figure 2).     Figure 2. DH Author Co-Citation network   Twitter network Given DH’s early adoption and active use of Twitter (Ross et al., 2011), previous studies have explored the field’s scholarly communications and community on Twitter (e.g. Quan-Haase et al., 2015; Grandjean, 2016). This study introduces a new approach ( co-retweet) to visualise the DH social network.  We have selected all the Twitter users that are followed by the Alliance of Digital Humanities Organisations (ADHO) and its member organisations’ (see  http://adho.org/) Twitter accounts. As dynamic and interdisciplinary as the DH community, it is often difficult and subjective to select the users by their account descriptions. In contrast, the following relationships by the DH organisations indicate more genuine and representative identification. A total of 3,160 unique users have been collected along with all the 6 million tweets from 21/03/2006 when Twitter was created up to 22:00 (GMT) on 5/11/2017 (see Figure 3).     Figure 3. number of tweeting users collected each year Similar to the citation network method, the Twitter user Co-Retweet network has been constructed by calculating the number of non-self-retweets the user received ( non-self-retweet count), and the number of same tweets that two users both retweeted ( co-retweet count) for the weights of the nodes and links respectively. The network resulting visualisations are shown on Figure 4.     Figure 4. DH Twitter Co-Retweet network    Results and comparison In the citation network, the authors identified distinct topic-based clusters of researchers with backgrounds in information studies and historical literature; in linguistics; in statistical text analysis; in early concordance projects; and biotech influenced text analysis. In contrast, the co-retweet network exhibits grouping based on language and region, with clusters related to scholars in North America; in Australia; in the UK; and clusters with Francophonic, Germanophonic and Hispanophonic backgrounds. The Twitter clusters are connected closely whereas clusters on the citation network are more loosely linked. This makes sense, as topics of study are generally more specific and less likely to change, whereas users on social media probably share a wider range of interests. The citation network is based on formal communications and it would take years to get sufficient citations to form links between two scholars. The Twitter network, however, is constructed by more informal interactions between users, and once two users retweeted the same tweet, they immediately build a link on the network. By visualising both networks during different time periods, this study will also present the topics, disciplines, countries that are involved, and how the networks have been developed and formed over time. As shown in Figure 5, we divided the 51 years (1966-2016) of historical citation data into five different periods (Hockey, 2004) for visualisation. The citation clusters experienced isolation (1966-1970); connection (1971-1985); consolidation (1986-1990); sub-fields development (1991-2005); and new specialties expansion (2006-2016). Over time, the most cited topics moved from concordance construction, to computational linguistics, then to information and historical literature studies.    Figure 5. DH Author Co-Citation networks in five periods As shown in Figure 6, DH Twitter users started to have co-retweet connections in 2009; and then they experienced the beginning of connection (2010); multi-region connection (2010); Anglophonic cluster to centre (2011); Francophonic cluster to develop (2012); North America and UK to separate (2013); Germanophonic to come out (2014); Australian cluster to show (2015); Hispanophonic cluster to emerge (2016); Density continue to move to North America cluster (2017). Over time, the network visualisations show that the density is moving from European clusters towards the North American cluster.    Figure 6. DH Twitter Co-Retweet networks in different years   Discussion and conclusion This study is not only the first to contribute to the DH history and community studies by visualising and comparing bibliometric and social networks, but also introduces new network approaches ( co-retweet) to study communications on social media that could support wider social network and data visualisation studies.  As we will discuss, network studies offer powerful but partial ways of studying the aspects of communities that are amenable to quantitative methods. We do not present the visualisations included in this paper as normative representations of the DH “community” or “communities”. Nevertheless, when used with caution, network studies can shed new light on important aspects of the historical formation of DH.  There are methodological limitations exist. For example, because the research subjects (cited authors and retweeting users) are not the same group of people (although with much overlap), obvious differences are expected. Besides, the citation lag time has been considered. Other practical methods to identify and study the DH Twitter communities can also be applied.  ",
       "article_title":"Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network",
       "authors":[
          {
             "given":"Jin",
             "family":"Gao",
             "affiliation":[
                {
                   "original_name":"UCL Centre for Digital Humanities, Department of Information Studies, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Julianne",
             "family":"Nyhan",
             "affiliation":[
                {
                   "original_name":"UCL Centre for Digital Humanities, Department of Information Studies, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Oliver",
             "family":"Duke-Williams",
             "affiliation":[
                {
                   "original_name":"UCL Centre for Digital Humanities, Department of Information Studies, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Simon",
             "family":"Mahony",
             "affiliation":[
                {
                   "original_name":"UCL Centre for Digital Humanities, Department of Information Studies, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "graphs",
          "digital humanities history",
          "networks",
          "library & information science",
          "sociology",
          "theory",
          "social media",
          "visualisation",
          "relationships",
          "English",
          "epistemology",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Research Environment for Ancient Documents (READ) is an integrated Open Source web platform for epigraphical and manuscript research. The original goal of the READ platform was to support scholars in researching and presenting studies of handwritten documents and inscriptions preserved in Gāndhārī language using the Kharoṣṭhī script. Since many of the workflows supported by READ are common to epigraphic and manuscript studies in other textual traditions we wanted to investigate how READ could be generalized to support other writing systems. This presentation will share the results of that investigation with examples from English, Aramaic, Chinese, and Mayan.  Three core components of the READ data model depend on the writing system used by the source material:  The link between physical and textual data The constraint mechanism that allows a user to edit text without disrupting links The sort weight API that allows data in the model to be displayed in an expected sort order  Part One. The database model underlying READ was designed to reflect the separate components and layers of interpretation which manuscript scholars and epigraphers typically use in their research (letter forms => paleography; graphemic units => phonology; inflectional forms => morphology, etc.). Furthermore, the model recognizes a continuum of factual confidence beginning from statements of fact (e.g., the name of a collection in which an item is kept), to data which may have multiple or variant interpretations (e.g., the transcription of a sample of writing). Such variant data is linked back through the model to original facts. At the crux of this system of links are the references between segments on an image each containing an orthographic unit in the writing system and the transcription of that unit. Because READ was originally developed for Kharoṣṭhī, an alphasyllabary or Abugida-type writing system, this link maps image segments to syllable clusters. Other writing systems can be supported by mapping the syllable cluster to the appropriate orthographic units. This has been tested by mapping syllable clusters as follows: English letters, Aramaic syllables, Chinese logographs, and Mayan syllables and logographs. Part Two. READ is intended to be a working environment for born-digital text editions. A critical feature of the model is that links created within the system must be preserved during repeated editing. The editing interface allows users to modify linked syllable clusters. By constraining edits to valid transcriptions of a syllable cluster defined for the language, READ can keep track of user edits and prevent links from being broken. Other writing systems can be supported by defining the valid transcription forms for the orthographic units. In most cases this is less complex than for akṣara-based writing systems. This has been tested by defining valid orthographic units as follows: English – Consonants, Vowels; Aramaic syllables - Consonants, Vowels, Consonant with modifier; Chinese – Logograph; Mayan – Logograph, CV syllable. All systems also permit orthographic units to be Digits and Punctuation signs. Part Three. READ uses custom sort tables to weight the orthographic units and subunits used by the model. Having custom sort tables allows correct sorting of Romanized transcription when the expected sort order is not equal to standard ‘ABC’ order. Other writing systems represented in Romanized transcription with non-standard sorts require dedicated sort tables. Alternatively, writing systems represented in native script via Unicode may be sorted via their Unicode weights. This has been tested using standard ABC weights for English, custom weights for Mayan transcription, Unicode weights for Hebrew transcription of Aramaic, and Pinyin sort weights for Chinese logographs. The outcome of these investigations has been that the READ architecture is generalizable, and that the READ platform could be employed by projects with a focus on documents in any writing system. ",
       "article_title":"Expanding the Research Environment for Ancient Documents (READ) to Any Writing System",
       "authors":[
          {
             "given":"Andrew",
             "family":"Glass",
             "affiliation":[
                {
                   "original_name":"Microsoft Corp., University of Washington",
                   "normalized_name":"University of Washington",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00cvxb145",
                      "GRID":"grid.34477.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "philology",
          "scholarly editing",
          "linguistics",
          "resource creation",
          "text analysis",
          "English",
          "and discovery",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In the past ten years, advancements in computing technology have lent themselves to diverse applications in teaching and learning such as seen with MOOCS, learning managements systems, networked collaborative pedagogy, virtual/augmented reality course modules, and algorithmic-driven approaches to personalized learning. While these engagements represent a variety of exciting (though often controversial) new directions for educational technology, the changing socio-technological conditions of our information landscape call for new critical approaches towards its development and use. Information communication technology (ICT) in educational settings should not only be evaluated according to the way it supports intended learning goals, but also according to the type of technological consciousness it produces in students. In this paper I will draw from methods and values in participatory design (Simonsen), critical pedagogy (Freire; Shor), and the digital humanities (Drucker & Svensson; Rockwell & Sinclair) to outline a way that academic technological practices and infrastructure might be re-engineered to foster more critical and participatory relationships to digital technology within higher education. I will focus specifically on how this approach has particular value for the teaching and use of writing in undergraduate and graduate education in that it enables a praxis-oriented approach to analyzing and designing digitally-mediated rhetorical situations within and beyond academia. I will then describe KNIT, a digital commons at UC San Diego that aims to develop a participatory model of educational technology, and describe the challenges and opportunities experienced in its development. Participatory approaches to ICT  The general user has little expectation or ability of being able to understand or modify the code of ICTs that mediate their everyday communicative activities, such as email, social media, Internet searching, or text editing.  While this lack of critical user participation in software oversight and production may appear as natural, inevitable, and relatively inconsequential, I will argue that it has been normalized through corporate technical policies, cultural myths regarding programming, and the use of technology in educational settings. To demonstrate the range of alternatives to passive relationships to software, I will point to a number of software cultures, projects, and visions in which the everyday user has greater opportunity to democratically participate in shaping the technical functionality and policy of their digital tools. I will argue that examples such as the Free Software community, the Platform Coop movement, and Alan Kay’s 1968 vision for Dynabook represent promising alternative software models that foster participatory design consciousness in the general user that could be fruitfully applied in educational settings. By implementing tools in the classroom that allow for participatory design and oversight, students would have the opportunity to experience greater forms of creative and critical control over ICTs that might lead them to question the lack of similar rights with regard to ICTs in everyday life. In this way, fostering participatory design approaches to digital technology stands as one promising approach to fostering critical and practical resistance in students to exploitative practices inherent in everyday ICTs such as dataveillance and algorithmic influence and manipulation. It also offers the possibility of turning educational technology into a site for producing open source ICT alternatives for general public use.   Techno-rhetorical consciousness Participatory design approaches towards educational technology also have direct application for writing-intensive courses in the humanities in that they can help foster “techno-rhetorical consciousness,” or a sensitive understanding of the way digital technology mediates rhetorical situations. By providing students with the perspective and control over ICTs normally only afforded by corporate or administrative entities, students have the opportunity to study more directly the way ICTs mediate their intellectual activities and communities, and explore how technical modifications might help support personal and collective intellectual goals and values. For example, access to data produced and transmitted through ICTs would enable students to use text analysis techniques from the digital humanities to study patterns in their individual and collective intellectual activities for the purpose of understanding the social dynamics of knowledge production and transmission. It would allow them to gain basic familiarity with algorithmic techniques that have increasing power in everyday life. And it would also provide students with the opportunity to experiment with how different aesthetic and algorithmic design features might better support individual cognitive activities related to writing process or productive intellectual exchange among students. These opportunities would not only have rich potential for the use and development of educational technology itself, but would also help students consider the way digital technology mediates the production and transmission of knowledge and power in everyday life. KNIT, a digital learning commons To explore some of these ideas in practice, we have launched KNIT, a digital commons for UC San Diego and institutions of higher education in the San Diego area. KNIT uses the free and open source software package Commons in A Box and thus, unlike many forms of proprietary software in educational settings, remains open to critical study and modification by the user community. In the final portion of my talk, I will discuss how we are using KNIT to test-drive participatory design practices for educational technology at UC San Diego and how we envision using it to give students a leadership position in its development and governance. I will also discuss the institutional, technical, and educational challenges of this approach and provide recommendations and resources for those interested in experimenting with this method at their home institution.  ",
       "article_title":" Designing writing: Educational technology as a site for fostering participatory, techno-rhetorical consciousness  ",
       "authors":[
          {
             "given":"Erin Rose",
             "family":"Glass",
             "affiliation":[
                {
                   "original_name":"UC San Diego, United States of America",
                   "normalized_name":"University of California, San Diego",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0168r3w48",
                      "GRID":"grid.266100.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-14",
       "keywords":[
          "cultural and/or institutional infrastructure",
          "creative and performing arts",
          "open/libre networks and software",
          "cultural studies",
          "design",
          "English",
          "software studies",
          "including writing",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    Overview:    This short paper looks into the process of developing the Latin American Comics Archive (LACA), a project created by our team at Carnegie Mellon University. LACA combines ongoing research in the Humanities with digital technologies as a tool for enhancing access and analysis capabilities for both scholars and students of these materials. The curated digital archive includes representative samples of Latin American comics digitally encoded in Comic Book Markup Language (CBML), while a technical foundation combining the open source content management system Omeka with TEI Boilerplate offers a customizable front-end for public or restricted access to the individual items and curated collections of the comics. This allows students and researchers access to source materials and possibilities to collaborate in their exploration, definition, tagging, and annotation for the analysis of visual and verbal language, cultural and linguistic characteristics or themes, and a variety of formal categories.     Statement of the problem:  Despite the overdue growing recognition of the genre of comics in academia, the study of foreign/second language comics within the United States has encountered specific obstacles. Primary-source research of Spanish-language comics has often proved to be challenging. Among other difficulties, collections are most often housed in the source countries, and a desired piece of documentation may sometimes be in libraries hundreds or thousands of miles away. Items may be both in public and private hands, and access to certain items is often highly restricted due to their fragility, rarity, and value. Oftentimes, specific documents aren’t cataloged in the archives’ container lists, making the identification, location, and access of relevant materials problematic. When using traditional research methods, these challenges have to be confronted and resolved by the researcher, who works in isolation with the source documents. Many of these issues also generate constraints in the realm of teaching, where the limitations to the access of sources restricts course conceptualization and implementation, and where students don’t usually have much agency or opportunities to engage in larger debates and conversations with other students or scholars of Latin American comics.  Digital tools have the potential to facilitate or solve many of these issues for research and teaching of this important cultural and literary medium. Indeed, they have the ability to address precisely the core values that Spiro (2012) associates with work in the Digital Humanities -- openness, collaboration, diversity, experimentation, collegiality, and connectedness. These tools can, for instance, create optimal opportunities to view and use some of these sources online, thus granting access to an audience who may never have had the chance to see them in the “analog” era, and opening and expanding the possibilities for a richer and deeper type of collaborative research. Our goal is to expand the possibilities of using Spanish-language comics by identifying and piloting the use of digital tools with which digital copies of representative Latin American comics can be made, accessed, and annotated in collaboration with students and scholars. Our focus is on developing an archive of sources that scholars and students can use for analysis, interpretation, and research employing digital tools.    Critical Context LACA seeks to insert itself in the broad scholarly landscape created at the intersection of comics scholarship (e.g. Priego, 2016; Walsh, 2012), visual ontologies and comics (e.g. Bateman et. al., 2017; Turton, 2017), work done to encode comics elements (e.g. Dunst et. al., 2016; Haidar and Ganascia, 2016; Kuboi, 2014), and work on the value of comics as a pedagogical tool (e.g. Brooks, 2017).    Methodology  Given the team’s expertise in Digital Humanities (DH) and Digital Scholarship, and with the support of an institutional Mellon DH seed grant, the project was initiated in the summer 2016. LACA was modeled after existing specialized collections such as MIT’s Comics and Popular Culture archive, UNAM’s specialized online resource  http://www.pepines.unam.mx/, and the Grand Comics Database (GCD) with the purpose of combining the PI’s ongoing research and teaching experience on Spanish-language Latin American comics with the use of DH tools to create an environment enabling students and scholars to have access to and collaborate in the analysis of the digital materials. At the current stage, LACA includes a small digital sample of Latin American comics produced throughout the last century, provided through a combination of previously digitized materials, materials we scanned, and those provided by authors themselves.   The presentation will detail three parts of the project:   Curating the comics and creating the archive;  Creating the online Metamedia platform to house digitized sources for the research and teaching of Spanish-language comics through student/scholar collaboration;  Piloting and implementing the digital archive for research and teaching.       Insights/  Results    LACA was piloted at CMU over the past year as an instrument in courses for undergraduate students of Spanish language and culture. Students and faculty collaborate in the analysis and CBML coding of the comics. In the process, students learn the basics of TEI and CBML, as well as critical approaches to Spanish-language comics, and their work contributes to the availability of comics on the site. Students are also able to develop integrated textual and visual competence, knowledge, and skills. The pilot courses provide initial evidence that coding the comics facilitates students’ attention to details, notice of patterns, and, in general, collaborative advancement in the analysis and understanding of the linguistic and cultural elements contained in the comics. At the same time, it also helps students keep in mind communication to a wide public audience. The PI has benefitted from the additional opportunities afforded to glean information about students’ progress toward cultural, linguistic, visual, and digital literacy. Thus, it is suggested that LACA could be of use and applicable to other courses in Hispanic studies, Modern Languages, and the Humanities.  We intend to make LACA publicly available for use as a hub where students and scholars interested in experimenting with the inquiry of Latin American comics can interact. This would help transform and expand the scale of traditional research methods used, and could open new modes and possibilities for text analysis that can be employed into the realm of student agency and learning. However, as we advance in the process to attain this goal, we acknowledge that IP/copyright permissions remain a challenge. Some creators have granted permission to distribute their works; others will only be used as part of course materials. Despite this, we think it is important to keep in mind Walsh’s (2012) point that “nothing prevents a scholar from applying CBML markup to any text as part of a strategy for reading, interpretation, and analysis. The end goal of markup is not and should not always be publication of a digital surrogate. The encoding of a text may be a rigorous intellectual activity that has great value as process, not just as product.”    ",
       "article_title":"The Latin American Comics Archive: An Online Platform For The Research And Teaching Of Digitized And Encoded Spanish-Language Comic Books Through Scholar/Student Collaboration",
       "authors":[
          {
             "given":"Felipe",
             "family":"Gomez",
             "affiliation":[
                {
                   "original_name":"Carnegie Mellon University, United States of America",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          },
          {
             "given":"Scott",
             "family":"Weingart",
             "affiliation":[
                {
                   "original_name":"Carnegie Mellon University, United States of America",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Evans",
             "affiliation":[
                {
                   "original_name":"Carnegie Mellon University, United States of America",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          },
          {
             "given":"Rikk",
             "family":"Mulligan",
             "affiliation":[
                {
                   "original_name":"Carnegie Mellon University, United States of America",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "and curriculum",
          "cultural and/or institutional infrastructure",
          "spanish and spanish american studies",
          "teaching",
          "cultural studies",
          "library & information science",
          "pedagogy",
          "multilingual / multicultural approaches",
          "English",
          "interdisciplinary & community collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The transcription of handwritten historical documents into machine-encoded text has always been a difficult and time-consuming task. Much work has been done to alleviate some of that burden via software packages aimed at making this task less tedious and more accessible to non-experts. Nonetheless, an automated solution would be a worthwhile pursuit to vastly increase the number of digitized documents. As part of a continuing effort to expand the footprint of digital humanities research at our institution, we have embarked on a project to automatically transcribe and perform automated analysis of Medieval Latin manuscripts of literary and liturgical significance. Optical Character Recognition (OCR) is the process of converting images containing text into a machine encoded document. Recent advances in artificial neural networks have led to software that can transcribe printed documents with near human accuracy (LeCun et al., 2015). However, this level of accuracy breaks down when working with handwritten, and especially cursive, documents except when applied to restrictively specific domains. Neural networks that are trained for this task require thousands of labeled examples so that their millions of parameters can be optimized. While there are thousands of high-quality scans of manuscripts available on the Internet, very few of these documents have been annotated for OCR tasks, and there is only a limited selection of ground-truth data which is annotated and segmented at the word-level (Fischer et al., 2011; Fischer et al., 2012) . There is no data available that provides annotations at the character-level. Normally, machine learning researchers would outsource the production of this ground-truth data to a platform such as Amazon's Mechanical Turk service, which allows crowdsourcing of human intelligence tasks. This is not an option for transcribing Medieval manuscripts, because it requires domain specific expertise. We put together a team of expert Medievalists and Classicists to generate the ground-truth data, and we have been developing a software platform that breaks the tedious task of producing pixel-level training data into more tractable jobs. The goals of this software go beyond just Latin manuscripts: it can be used to generate source data for any machine learning task involving document analysis. We are releasing it publicly, as free and open source software, in hopes that others can also use it to generate data, and help bring further advances in machine learning for handwritten text recognition.   Related work State-of-the-art solutions to handwritten digit recognition on the MNIST dataset have achieved accuracies greater than 99% and have led some to declare handwritten OCR a solved problem (Wan et al., 2013). However, Cohen et al. have shown that adding the English alphabet to the dataset drops accuracy by more than 20% even when using the same methodology (Cohen et al., 2017). Some of the difference can be attributed to the fact that characters like ‘‘I’‘, ‘‘l’‘, and ‘‘1’‘ are often ambiguous without context --- especially when handwritten. To combat this, many handwritten text recognition algorithms will often use recurrent neural networks that look at the whole word and utilize a language model to overcome ambiguities (Fischer et al., 2009; Sánchez et al., 2016). Additionally, Convolutional Neural Networks (CNN) have been shown to have promise in segmenting biomedical images, which are also difficult to ground truth (Ronneberger et al., 2015). A similar approach could be used to segment individual letters in manuscripts. Incorporating human performance information into the machine learning process has been shown to improve the accuracy of tasks like face detection (Scheirer et al., 2014). We hypothesize that incorporating a human weighted loss function will lead to similar improvements in this task as well.   Workflow Currently the software runs in Google App Engine using high-resolution source images. We are in the process of setting up the software to be run in a vagrant environment to make it available for local environments. The vagrant script will provision a Virtual Machine, either locally or to the cloud to serve the software and configure it to work with a user-provided library of documents. In either case, transcribers can access the software via a web browser. The user then proceeds to segment the document by lines and words by drawing bounding boxes, and characters by drawing over them. It also collects text annotations of the text at the word- and character-level. It stores all the information in a MySQL database.   Line and word level  Our process starts by having experts segment the document into lines. Transcribers use a modified version of the Image Citation Tool from the Homer Multitext Project to quickly break the document down into CITE URNs representing each line by drawing boxes around them (Blackwell and Smith, 2014). After all the lines are selected the process is repeated for each word. A screenshot of these processes is shown in Fig. 1.   Figure 1: An example of the interface for selecting lines and words. Manuscript: Einsiedeln, Stiftsbibliothek, Codex 629(258), f. 4r – [Jacobus de Voragine]  Legenda aurea sive lombardica (http://www.e-codices.unifr.ch/en/list/one/sbe/0629)     Pixel level annotation After segmenting the document into words, our software prompts the expert to segment and annotate each word letter by letter. Instead of using a bounding box, we have the user trace over each character in the word using a pen tool. This gives us a pixel-by-pixel segmentation of the image that can be used to train a CNN to segment the characters automatically, much in the same way segmentation models are trained for other computer vision tasks (Ronneberger et al., 2015). At this stage the expert will also select which letter best represents each character from an array of buttons, as shown in Fig. 2.   Figure 2: An example of the tool used to collect pixel level ground-truth at the character level.    Psychophysical measurements The final stage collects psychophysical measurements of the human process of reading. The software brings up individual characters, as shown in Fig. 3, and asks the transcriber to pick an annotation for a character without word context. They will also be asked to select the difficulty of each character. The software also records how long it takes for the user to submit an answer and compares whether the user selected the same character that was selected during the word-level annotation.   Figure 3: A screenshot of the psychometric data collection stage.    Outcomes The software produces a segmented image for each document that can be used as training data for machine learning-based segmentation. Furthermore, it provides the pyschophysical measurements on the reading difficulty of each character. We also designed it to produce word-level segmented data in a similar format to the IAM Historical Document Database (Fischer et al., 2012; Fischer et al., 2011). Finally, the user will be able to export the transcribed document into a standard markup language such as TEI.  ",
       "article_title":"Verba Volant, Scripta Manent: An Open Source Platform for Collecting Data to Train OCR Models for Manuscript Studies",
       "authors":[
          {
             "given":"Samuel",
             "family":"Grieggs",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Bingyu",
             "family":"Shen",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Hildegund",
             "family":"Muller",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Christine",
             "family":"Ascik",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Erik",
             "family":"Ellis",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Mihow",
             "family":"McKenny",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Nikolas",
             "family":"Churik",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Emily",
             "family":"Mahan",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          },
          {
             "given":"Walter",
             "family":"Scheirer",
             "affiliation":[
                {
                   "original_name":"University of Notre Dame, United States of America",
                   "normalized_name":"University of Notre Dame",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00mkhxb43",
                      "GRID":"grid.131063.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "computer science",
          "linking and annotation",
          "resource creation",
          "software design and development",
          "text analysis",
          "English",
          "and discovery",
          "digitisation",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Background The infrastructures that we use to navigate the world often become invisible as they become indispensable (Bowker and Star, 2000). However, critical examination of information systems is necessary to understand their implicit biases, and the ways that they invite some types of engagement and restrict others. Structures of power continue to be replicated in the ways that technologies are deployed in our lives (Noble, 2016; Tufekci, 2016), and the inability to access and assess the standards which make digital communication possible risks the uncritical perpetuation of those power structures (Drabinski, 2013). The moments of rupture, when an established system takes on a new facet with unintended consequences, can be an important moment of visibility, where we are able to reveal its ideological foundations, and the ways that its users adapt their own behaviors to it, or push back against its uncomfortable constraints (Raley, 2006; Marino, 2007). The introduction of emojis to the Unicode Standard, and their widespread adoption over the decade from 2006-2017 is one such moment of transition.  Scholars of standards and standardization argue that the input of users is necessary for a standard to meet the needs of those users (Foray, 1994), and while the process of adding content to the Unicode Standard remains rigid, the unicode.org website provides an explicit record of the development and evolution of the face that Unicode presents to its users, and is able to be read as a text which reveals the contemporary state of Unicode and the cultural ideologies which shape it. Methodology While major language- and script-based additions are made with each update to the Unicode Standard, my analysis focuses on changes to the unicode.org website, and its role as an intermediary document between the Consortium, the Standard itself, and everyday users. The introduction of emojis in various updates to the Standard has resulted in changes to the content and structure of the unicode.org website that reflect an increased engagement with end users, which I argue is the result of increased semantic value of emoji characters for the user   A notable exception to this semantic shift is written Chinese, which is already a semantic-character-based language, as opposed to syllable- or alphabet-based, as are the rest of the world’s major languages. Thomas S. Mullaney gives a thorough historical analysis of the implication of this on text-encoding technologies in  The Chinese Typewriter (MIT Press, 2017).   , as compared to an individual character in a language's written script. It is my intention, through this analysis, to describe the types of changes that happen to the governing body and public documents of Unicode as major changes happen to the Standard itself.  A timeline was created of the dates of major updates to the Unicode Standard since its introduction in 1991, using the official release dates for updates to the Unicode Standard as maintained by the Unicode Consortium. I cross-reference this document with the rollout of each new version by the major platforms   https://unicode.org/emoji/format.html#col-vendor lists the major “vendors” of emojis, or platforms with proprietary visual displays of emojis. These vendors are Apple, Google, Twitter, Facebook, Facebook Messenger, Windows, and Samsung. , with a particular emphasis on updates featuring new emoji characters, beginning with Unicode 6.0 in 2010   While the first major batch of emojis were incorporated into Unicode in 2010, and the first official “Emoji 1.0” release was in 2015, work has been done within the standard since late 2006 to consider the addition and management of emoji-like characters within Unicode— hence the specific 2006-2017 emphasis of this research. (https://www.unicode.org/reports/tr51/#Introduction) .   With this timeline in mind, I scraped the unicode.org domain using Python and the Beautiful Soup   https://www.crummy.com/software/BeautifulSoup/  library to collect the URLS of all the unique pages under the parent domain, as well as a table of links between those pages. This serves as a source-target list for the creation of a network visualization of the unicode.org domain, using the network visualization software Gephi.   http://gephi.io  This process is repeated using archived versions of the unicode.org site, available from the Internet Archive’s Wayback Machine   https://web.archive.org/ , resulting in several structural snapshots of the unicode.org website over time, which can then be overlaid and compared to one another to note particular areas of change within the site.  Additionally, using points of change within the site structure as a guide, I also collect and code page content data to reflect the type of changes made to those pages during each major update. This coding is done on two axes: The first labels each change as being content- or structure-based (eg. adding text or links to a page, respectively), and the second designates which aspect of the Standard and/or Consortium is being addressed by the change. Examples of this second type of labelling would be “Emoji,” “Membership,” “Meta-Documentation,” or “Language Scripts.” This coding is done in two phases— an initial survey of this data in order to formally create labelling categories, and then a closer examination of the updates to apply those labels. Discussion and next steps This research project addresses issues of digital infrastructure from a unique angle: one that considers the socially-constructed nature of technology, as well as the meta-narrative of maintenance and upkeep of a system that has become crucial to our ability to communicate in a digital world. Through analysis of the secondary documents relating to the Unicode Standard, it is possible to gain invaluable insights into the ways that knowledge is organized collectively and continuously, as well as the embedded values that shape who can access and influence that knowledge.  This case study will provide a foundation for more expansive examination of systems of digital infrastructure. It is a beginning point both for further analysis of the adoption and adaptation of Unicode (and emojis in particular), but also as a framework for examining other forms of scaffolding which uphold the content of digital spaces.   ",
       "article_title":"Flexibility and Feedback in Digital Standards-Making: Unicode and the Rise of Emojis",
       "authors":[
          {
             "given":"S. E.",
             "family":"Hackney",
             "affiliation":[
                {
                   "original_name":"University of Pittsburgh, United States of America",
                   "normalized_name":"University of Pittsburgh",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an3r305",
                      "GRID":"grid.21925.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "cultural and/or institutional infrastructure",
          "internet / world wide web",
          "digital ecologies and critical infrastructure studies",
          "film and media studies",
          "cultural studies",
          "library & information science",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    Figure Heather Agyepong, disrupting an ordinary school day in KIT Theatre’s Alfred the Great Time Travel Adventure    Introduction The Digital Ghost Hunt combines coding education, Augmented Reality and live performance into an immersive storytelling experience. S tudents ten to eleven years old (Key Stage 2 in the UK) will explore the haunted Battersea Arts Centre with devices they’ve learned to program themselves. The key objective of The Digital Ghost Hunt is to present technology to students as an empowering tool, where coding emerges as – and fuses with – different forms of storytelling.  It seeks to shift the context in which students see coding and engage groups who may be uninterested in or feel excluded by digital technology, opening up an imaginative space through play for them to discover the creative potential of technology on their own terms.  The Digital Ghost Hunt has been awarded funding through the UK Arts and Humanities Research Council (AHRC) New Generation of Immersive Experiences call, as part of an application led by Mary Krell, Senior Lecturer in Media Practice at the Centre for Material Digital Culture in the University of Sussex.  A ‘scratch’ – a prototype of the experience – will be developed by Elliott Hall of King’s Digital Lab and Tom Bowtell of Kit Theatre.  It will be performed at the historic Battersea Arts Centre with a two-form entry of students from local schools.   Structure of the experience  The Digital Ghost Hunt is split into two parts.  The first part begins with a regular coding class that suddenly goes haywire. While the teacher is trying to restore order, the lesson will be interrupted by Ms. Quill, Deputy Undersecretary of Paranormal Hygiene (Ghost Removal Section).  She will enlist their help in the Ministry’s work as apprentice ghost hunters. Students will use a simplified Python library to program their ghost hunting devices, which are based on two microcomputers: the Raspberry Pi and the BBC Micro:bit.  The coding in the project will focus in particular on two learning goals of the UK’s National Curriculum: “Design, write and debug programs that accomplish specific goals, including controlling or simulating physical systems; solve problems by decomposing them into smaller parts,” and “Use sequence, selection, and repetition in programs; work with variables and various forms of input and output.”  It will teach students to take the overall goal of their devices – detecting ‘paranormal’ phenomena – and break it down into the discrete input, analysis and output tasks required, aided by the project’s abstracted libraries.  How they combine the functions of these libraries will be up to them, and will rely on their understanding of the fundamental logical structures of programming to analyse sensor data, apply it to an algorithm, and debug when things go wrong.  The project will also introduce students to embedded computing through the devices themselves. The emphasis will be on students taking ownership of their devices, deciding which of the ghost detectors they want to build and how it will work. The second part is a ghost hunt, an immersive experience combining Augmented Reality (AR) and live theatre.  Students will work together in small teams, using their devices to find objects and areas touched by the ghost.  These traces will be both virtual objects in Augmented Reality, and actual physical phenomena such as radio waves, ultraviolet paint, and high-frequency sound.  Each device will have different capabilities, forcing the students to work together to get all the clues.  The ghost will in turn communicate with them, given life by actors, practical effects and the poltergeist potential of the Internet of Things.  Only by using the devices they have programmed and working together can students unravel the mystery of why the ghost is haunting the building and set it free.   Coding, play and performance    Figure  A proof of concept ghost hunting device using Lego and the Micro:Bit   Young people’s familiarity with digital products are increasing, but their interest in learning the technology behind it is not, as evidenced in the UK by the low take up of the new GCSE in Computer Science (BCS, 2017).  T eaching coding in schools is promoted by the UK Department of Education (DOE, 2014), but students often experience coding education as a classroom assignment, divorced from their intuitive and creative experiences with commercial digital applications.  There are several applications now using AR as a teaching tool (for example, The Battle of Mount Street Bridge (Schreibmen et al., 2017) and Virtual Roman Frontiers (Wilson at al.)) and initiatives to teach children coding, from commercial apps to coding clubs and the work of the Raspberry Pi and Micro:Bit foundations.  These applications all seek the increase in engagement and experimentation that can occur when ‘work’ is reframed as ‘play.’ (Pellegrini, 2009) However, these applications all take place within a screen, an approach that creates its own problems. A screen can shift a user’s attention to the digital environment to the exclusion of the physical one. (Chrysanthi, 2012) The Ghost Hunt’s approach is to bring AR interaction fully into the physical space without the mediating influence of a screen, reconnecting audiences to the world around them.   The addition of immersive theatre reframes the experience again, from ‘play’ to ‘performance.’ This second shift is important to reach groups not engaging with existing digital resources. In 2016, girls made up just 20% of entrants for the computer science exam, while pupils on free school meals made up just 19% of GCSE entrants even though they are 27% of the population (Cellan-Jones, 2016).  Performance may draw in groups who would otherwise be uninterested in or feel excluded from traditional Computer Science education.   However, the performance should not be seen as secondary in any way to the coding elements of the project. The aim of the project is to expand the imaginative possibilities of digital technology through play; the coding elements are the means to that end, not the other way around.  The Ghost Hunt seeks to shift how the context of computer science is perceived, from a skill intended only for a narrow group to a tool of creativity and play available to all.   As part of its evaluation, the project will use the student’s code and feedback from educators on how the software libraries are used, as well as video, audio and device logging during the experience.  It will be direct engagement with participants through formal and informal methods such as interviews, questionnaires and the creative material they create as part of the experience that will provide the crucial method of evaluation. The only way to assess the pedagogical value of the project in terms of creating a new and sustained interest in the possibilities of digital technology will be if students create new things on their own initiative, independent from the project’s setting and materials.  This metric is beyond the scope of the pilot project but is something the project team are eager to explore in subsequent phases in collaboration with the educational partners.   Beyond the hunt The lessons of the Digital Ghost Hunt scratch funded by the AHRC will direct refinement of the existing tools towards developing a technical and conceptual framework that can be adapted and implemented for different locations, stories and audiences. This short paper aims to present the practice-based collaborative framework of the Digital Ghost Hunt as conceived by its creators in its first funded iteration to elicit feedback from the Digital Humanities 2018 participants and integrate it into future development.   ",
       "article_title":"The Digital Ghost Hunt: A New Approach to Coding Education Through Immersive Theatre",
       "authors":[
          {
             "given":"Elliott",
             "family":"Hall",
             "affiliation":[
                {
                   "original_name":"King's College London, United Kingdom",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "computer science",
          "and curriculum",
          "electronic literature and digital arts",
          "creative and performing arts",
          "virtual and augmented reality",
          "teaching",
          "historical studies",
          "software design and development",
          "pedagogy",
          "English",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Introduction  In Fall 2015, we were approached by author (and prominent DH-skeptic [2012]) Stephen Marche with a request that we help him use computational textual analysis to allow him to write “the perfect science fiction story.” His specific request was for a set of “rules” to follow in composing such a story. On consultation with Marche, we devised an approach that would derive “rules” related to theme (using topic modelling) and style (using a variety of techniques, including our own original work on quantifying style) from a hand-selected corpus of Marche’s 50 favourite science fiction stories. In Fall 2016, we sent Marche a list of 14 thematic rules and created a system to provide real-time stylistic feedback on his efforts to meet a set of 24 stylistic targets. In December 2017, the resulting story, “Twinkle Twinkle,” was published in the popular technology magazine Wired  alongside a set of detailed footnotes explaining and reflecting upon the process of its creation. Whereas Marche and his editors viewed the rule-creating process as “objective” and the publishing professionals interviewed in the piece complained that the resulting story lacked “humanity,” the process in fact blended computational analysis and human interpretation. We regard it as an instance of computer-assisted “creative deformance” rather than “robotic authorship.”   2. Thematic rules  Below is a selection of the 14 rules we sent to Marche:  1.  The story should be set on a planet other than earth.   2.  The story should thus NOT be set in space itself.   3.  On this planet, there should be an existing, non-human civilization. This civilization should have a hierarchical social structure with a powerful ruler. Inhabitants of this alien civilization should be given clearly non-human names. The protagonists of the story should be humans who are directly observing this civilization from a certain distance and do not consider themselves part of it.   4.  The story should be set in a city. The protagonists should be seeing this city for the first time and should be impressed and dazzled by its scale.   5.  Part of the action should unfold at night during an intense storm.   6.  Include a pivotal scene in which a group of people escape from a building at night at high speed in a high-tech vehicle made of metal and glass that is large enough to live in for an extended period (it should have a bed in it, for instance).   9.  Include a scene set on a traditional earth farm, with apple trees and/or corn fields. In this scene, a mother and father are present. (Given the other rules, this is most likely a flashback to a protagonist’s childhood, but I leave the details to you, of course.)   10. Include extended descriptions of intense physical sensations and name the bodily organs that perceive these sensations.   13. DO NOT focus on conventional domestic family life. Marriage should not be a theme. No scenes should depict a conventional bourgeois family (especially a happy bourgeois family) at the dinner table.   These rules were derived as follows. First, we assembled a corpus of some 4,000 texts, of which approximately 3,000 were fiction  other  than science fiction, 1,000 were science fiction, and 50 were the stories hand-selected by Marche. All texts were processed in GutenTag (Hammond and Brooke 2017). Next, we performed topic modelling using Mallet in R (400 topics, 500-word chunks, nouns only). Once this was complete, we examined all the topics that distinguished Marche’s corpus from both comparison sets, positively and negatively, with statistical significance values of p < 0.05. Some of these topics were easily converted into thematic rules, such as topic 199 (Fig. 1), which became the basis of rule 5. Less legible topics required a more creative, less “objective” approach. This was the case for topic 33 (Fig. 2), the topic which most clearly distinguished Marche’s hand-selected corpus from both comparison groups, and which became the basis of rule 6. To preserve the aura of computational objectivity that he craved, Marche was not shown the topic modelling word clouds, and the process by which the rules were devised was not explained to him in detail.      Figure 1:  Topic 199, the basis of rule 5.      Figure 2:  Topic 33, the basis of rule 6.   3. Stylistic rules    Marche began composition once he had received the list of thematic rules. We instructed him to insert his drafts into an online system we had devised, SciFiQ, which would provide stylistic feedback based on 24 criteria:   Literariness Abstractness Objectivity Colloquialness Concreteness Subjectivity Positivity  Text length  Average word length Average sentence length Average paragraph length Average variance in sentence length Average variance in paragraph length Average commas per sentence Percentage of Latinate words Nouns per 100 words Verbs per 100 words Adjectives per 100 words Adverbs per 100 words  Lexical density  Speaking characters Percentage of text which is dialogue  Percentage of dialogue by female characters  Major named locations   All criteria were calculated using GutenTag, which was configured to identify parts of speech, structure, and named locations (using LitNER [Brooke et al. 2016b]); to distinguish narration from dialogue; to identify individual characters and their gender; and to tag each word for stylistic properties (Brooke et al. 2016a) and sentiment polarity. SciFiQ displayed results for each of these criteria, colour-coded to indicate how close Marche was to his targets. Only once the values were within 0.5 standard deviations of the mean value for all stories in the 50-story corpus would the value read green; otherwise they would read purple (too low) or red (too high). Once Marche had all values in the green range (with the exception of story length, which was at the discretion of his editor), composition was considered complete. The story was not modified at all during the editing process for  Wired.      Fig 3:  SciFiQ in composition mode.      Fig 4:  SciFiQ’s analysis report.  4. Discussion  We approached this project with several aims in mind. First, and most prosaically, we regarded it as a means of validating our work on quantifying style (Brooke et al. 2016a). If our system told Marche to make his style less literary, for example, and he made a series of edits based on his intuitive concept of literariness, would our system respond in a way that he regarded as intuitive? In practice, Marche found that our system’s analyses did correspond to his intuitions. We thus consider our approach to style to be further validated. Second, we viewed the project as a way of testing the extent to which a set of tools designed for computational textual  analysis  (some developed by ourselves) could be useful in composition . Following critics like Jerome McGann (2001) and Stephen Ramsay (2011) we were interested in computational tools’ ability to “deform” the literary work in such a way as to model a helpful and non-traditional form of composition. In this, too, we believe the project was successful. In the notes he supplied in  Wired , Marche reported, “the algorithm affected the story much more than I would have thought,” noting in particular the manner in which conflicting rules (1 and 9) presented welcome imaginative challenges, and in which the SciFiQ system demanded a “Rubik’s cube”-like writing process whereby alterations in one part of the story required counterbalancing edits elsewhere.    Of course, we understood that the project would be received in a manner not entirely in keeping with our intentions. Whereas we regarded the project as a somewhat playful creative disruption of the conventional process of composition, Marche presented SciFiQ as “a computational system that [would] optimize [his] prose” and “make [him] better at his job.” In a sidebar,  Wired  asked two prominent literary editors (Andy Ward, editor-in-chief of Random House, and Deborah Treisman, fiction editor of  The New Yorker ) to provide general feedback on Marche’s story, “without knowing who (or, more specifically, what) wrote it.” The implication that the story was  written by  an algorithm is inaccurate (indeed,  Wired  credited it to Stephen Marche, not to SciFiQ). Further, the claim that opinions were solicited “blind” appears disingenuous given that Ward’s primary complaint was that the story “doesn’t sound human” — an unusual comment unless one has been told that computers were somehow involved in the story’s composition. From the perspective of  Wired , then, the story’s appeal as well as its danger lay in the notion of computers taking over the essentially human act of storytelling.    5. Conclusion  The “Twinkle, Twinkle” project will likely be received by the public as another instance in the longstanding debate about the proper role of machines in storytelling. Yet for us, its significance is rather different. It served as an opportunity to use tools and approaches designed for the analysis of literature in creative production; to validate our 6-dimensional approach to style; and to reach across the divide that continues to separate academic work on literary analysis both from contemporary creative writers and their popular audiences. ",
       "article_title":"SciFiQ and “Twinkle, Twinkle”: A Computational Approach to Creating “the Perfect Science Fiction Story”",
       "authors":[
          {
             "given":"Adam",
             "family":"Hammond",
             "affiliation":[
                {
                   "original_name":"University of Toronto, Canada",
                   "normalized_name":"University of Toronto",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03dbr7087",
                      "GRID":"grid.17063.33"
                   }
                }
             ]
          },
          {
             "given":"Julian",
             "family":"Brooke",
             "affiliation":[
                {
                   "original_name":"Thomson Reuters, Canada",
                   "normalized_name":"Thomson Reuters (Canada)",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01r4zz038",
                      "GRID":"grid.497213.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "knowledge mobilization",
          "computer science",
          "electronic literature and digital arts",
          "corpora and corpus activities",
          "english studies",
          "text analysis",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In the last decade, crowdsourcing has become a major technique for transcribing a large volume of historical manuscripts. The volunteers of Transcribe Bentham   http://www.transcribe-bentham.da.ulcc.ac.uk/.   have transcribed more than 19,000 pages of manuscripts written by Jeremy Bentham (Causer and Wallace 2012). More than 480,000 pages of weather observations from the US Government Arctic logbooks written in the 19th century were transcribed by 4,730 people through the Old Weather   https://www.oldweather.org/.   project (Eveleigh et al. 2013).  However, managing a crowdsourcing project remains a big challenge for humanities scholars. The following practical difficulties are encountered:  The need to draw public attention to the project successfully. The need to encourage participants’ long-term involvement. The tasks requiring crowdsourcing in humanities studies (e.g. transcribing ancient handwritten manuscripts) are often difficult for non-trained participants.  In case of Japanese Studies, the last difficulty is particularly crucial; due to the drastic change in the writing system that occurred at the end of 19th century, 99% of modern Japanese people are unable to read  kuzushiji, classical calligraphic renderings of Japanese characters that were common for both publishing and handwriting. Therefore, the crowdsourcing technique has never been successfully applied to pre-modern Japanese materials.  However, humanities scholars can use education to draw the attention of a large number of people, promote their long-term participation, and train them to tackle difficult tasks. The fundamental idea in this paper is to develop a crowdsourcing system embedded in a collaborative learning environment that enables learners to conduct crowdsourced tasks as a part of their learning with their peers. Minna de Honkoku   The literal translation of Minna de Honkoku in English is “Transcribe with everyone.” Also, the video tutorial of Minna de Honkoku in English is available at:  https://www.youtube.com/watch?v=iX5xN4vZeao.   ( https://honkoku.org/) is a crowdsourced transcription project of pre-modern Japanese earthquake records, developed by the members of the Historical Earthquake Study Group (HESG) at Kyoto University based on this idea. In this paper, we will briefly describe the aim, materials, approach, and results of Minna de Honkoku.    The Background and aim of the project HESG is a joint group of seismologists and historians including the authors at Kyoto University who have been studying pre-modern earthquake records for seismic research and disaster prevention. Since instrumental observation of earthquakes in Japan began only after the end of 19th century, transcribing written records are required for studying past earthquakes. Therefore, Japanese seismologists have developed an extensive collaboration with historians and archivists. However, the number of records to be transcribed is vast and cannot be handled by a small group of scholars. This prompted the members of HESG to think of using crowdsourcing for transcribing historical earthquake records. We have set the first goal of our project, Minna de Honkoku, to transcribe all the 114 books from the Ishimoto Collection, which is composed of historical earthquake records collected by a seismologist Mishio Ishimoto (1893-1940) and digitized by Earthquake Research Institute (ERI), Tokyo University. The number of pages in the books ranges from 14 to 268. The total number of pages across the 114 books is 6,386. Each digital image in the collection contains two pages, as presented in Figure. 1.   An example of two digitized pages in a book from the Ishimoto Collection  The challenge and our approach The biggest challenge of our project is to crowdsource the reading of  kuzushiji, which is illegible for most modern Japanese people except trained experts. Our approach to this challenge is to design our crowdsourcing system as an online learning environment where participants can learn  kuzushiji by transcribing the earthquake records in a collaborative manner.  More specifically, Minna de Honkoku integrates crowdsourcing with online learning in the following two ways:   Collaboration with a mobile learning app: Minna de Honkoku collaborates with KuLA   Android version:  https://play.google.com/store/apps/details?id=yuta.hashimoto.kula and  iOS version:  https://itunes.apple.com/jp/app/id1076911000.   (Kuzushiji Learning App), a mobile learning app for reading  kuzushiji that was developed by one of the authors (Hashimoto 2017) and has been downloaded 85,000 times since its release in 2016 (see Figure. 2). After completing a set of basic lessons for reading  kuzushiji, the users of KuLA are invited to Minna de Honkoku as an opportunity to acquire more practical training by transcribing actual materials from pre-modern Japan. They can thus begin participating in the project as a continuation of their learning.   Collaborative learning through distributed proofreading: Transcribing  kuzushiji correctly is quite difficult, and beginners usually make a lot of mistakes. For quality control of transcriptions, Minna de Honkoku uses “distributed proofreading” adopted by Project Gutenberg (Newby 2003) but with an educational purpose; when you finish transcribing an image from a book on the transcription editor of Minna de Honkoku (see Figure. 3), your transcription will be shared and reviewed by other participants on the timeline that shows user activities in real-time (see Figure. 4). When another participant makes corrections on your transcription, you will receive a notification with the feedback, informing you of the mistakes you made and the corrections (see Figure. 5, 6).     Screenshots of KuLA    Transcription editor of Minna de Honkoku    The timeline view of user activities    The notification panel    Corrections made by another participant (added texts are colored in green and deleted texts in red)    The results The website of Minna de Honkoku was launched on January 10, 2017. The transcription of 114 books (6,386 pages) from the Ishimoto Collection was completed on May 31, 2017. Thus, our initial goal was completed in less than five months since the project launch. We extended our goal and added another 223 books stored in ERI. As of November 2017, 271 books out of 337 (9,254 pages out of 9,716) including those from the Ishimoto Collection have been transcribed by volunteers. A total number of 3.12 million characters have been transcribed. A total of 3,457 people have registered an account, and 285 of them have transcribed at least one character on the website. While we were unable to include all registered users in the transcription process, a small number of regular volunteers have eagerly contributed to the project: 35 users have transcribed more than 10,000 characters, and 6 of them more than 100,000.   The background and motivations of the participants In order to understand the backgrounds and motivations of the participants, we administered an online questionnaire to them via Google Form between March 8 to May 13, 2017. We obtained responses from 64 participants. The following is a brief summary of the questionnaire results:  70% of respondents (45 people) are KuLA users. We asked the respondents to choose the reasons of their participations from 12 pre-defined choices (multiple choices up to three are allowed). The most selected reasons are as follows:  “Transcribing historical manuscripts is fun” (70%, 45 choices).  “I can learn from other participants’ transcriptions and reviews” (50%, 32 choices). “I can contribute to seismic research and disaster prevention through the project” (44%, 28 choices).    The results above suggest the following: (1) KuLA works effectively as an “entrance” to Minna de Honkoku, and (2) the possibilities of collaborative learning greatly motivate the participants, although the most powerful motivation is the enjoyment gained from transcribing.   Conclusion In this paper, we have described the background, aim, approach, and results of Minna de Honkoku, a crowdsourced transcription of historical earthquake records of pre-modern Japan. It had been often said that crowdsourced transcription of pre-modern Japanese materials is not possible because reading  kuzushiji is too difficult for non-trained volunteers. However, our learning-centered approach appears to have achieved considerable success. The same approach may also be used in many other countries that are facing difficulties in reading historical manuscripts due to changes in writing systems.  Lastly, desire to learn is one of the most fundamental characteristics of human beings, fulfilling which is one of the important roles of a scholar as a teacher. We therefore believe that considering academic crowdsourcing in the context of education will bring beneficial outcomes.  ",
       "article_title":"Minna de Honkoku: Learning-driven Crowdsourced Transcription of Pre-modern Japanese Earthquake Records",
       "authors":[
          {
             "given":"Yuta",
             "family":"Hashimoto",
             "affiliation":[
                {
                   "original_name":"National Museum of Japanese History, Japan",
                   "normalized_name":"National Museum of Japanese History",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/008yfsh88",
                      "GRID":"grid.471895.6"
                   }
                }
             ]
          },
          {
             "given":"Yasuyuki",
             "family":"Kano",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Ichiro",
             "family":"Nakasnishi",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Junzo",
             "family":"Ohmura",
             "affiliation":[
                {
                   "original_name":"Bukkyo University, Japan",
                   "normalized_name":"Bukkyo University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/04629sk87",
                      "GRID":"grid.444208.e"
                   }
                }
             ]
          },
          {
             "given":"Yoko",
             "family":"Odagi",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Kentaro",
             "family":"Hattori",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Tama",
             "family":"Amano",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Tomoyo",
             "family":"Kuba",
             "affiliation":[
                {
                   "original_name":"Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Haruno",
             "family":"Sakai",
             "affiliation":[
                {
                   "original_name":"Tokyo Metropolitan Library",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "classical studies",
          "project design",
          "organization",
          "management",
          "public humanities and community engaged scholarship",
          "historical studies",
          "English",
          "asian studies",
          "crowdsourcing",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background, aims, and hypotheses In 19th century Spanish American novels, the expression of emotionality is an essential characteristic of the texts belonging to different subgenres. This contribution is concerned with the linguistic manifestation of emotions in literary texts on the textual surface. See Winko, 2003 for a discussion of how emotional meaning and literary texts are related. Especially during the Romantic period in the first half of the century, many sentimental novels have been written (Zó, 2015). But emotions also play an important role in other types of novels: a love story is often a basic plot element for example in historical or costumbrista novels. Also, there are novels characterized more by negative emotions, like Cuban anti-slavery novels (Rivas, 1990), Argentine anti-Rosas novels (Molina, 2011: 285-312, García Ardeo, 2006), or sociopolitical novels in general.  In text mining, a common method to analyze emotions is Sentiment Analysis (Pang and Lee, 2008). Sentiment Analysis is the computational treatment of sentiment, opinion, or emotion in text. Sentiments are usually modelled in terms of polarity values (positive, negative, neutral) or emotion values (such as trust, fear, joy, etc.). The aim of this proposal is to test several hypotheses about sentiments in subgenres with an explorative analysis of a corpus of Spanish American novels. To this end, sentiment values are used as features in a text classification task. A secondary objective of this contribution is to compare the results of two different sentiment lexica for Spanish to see how well they perform. The first hypothesis of this proposal is that the degree and kind of emotionality in the novels differs for different subgenres. The second hypothesis here is that not just emotions in general matter, but also whether they are expressed in the direct speech of the characters of the novels or in narrated text. The anti-slavery novel, for example, has been defined in terms of its atmosphere of fear, but also by vigorous interferences of the narrator. Cf. Rivas, 1990.    State of the Art Two recent examples for the usage of Sentiment Analysis with literary texts are Zehe et al., 2016 for the prediction of happy endings in German novels and Kim et al., 2017 for the analysis of prototypical emotion developments in literary genres with English texts. Sentiment Analysis has been used with Spanish texts, as well, mainly for the analysis of reviews and tweets (see Henríquez Miranda and Guzmán, 2017 for an overview). To the best of my knowledge, there are no applications of Sentiment Analysis on Spanish novels yet, and the distinction of direct speech and narrated text has not previously been used in combination with the analysis of sentiments in literary texts.   Data For this analysis, a corpus of 30 Spanish American novels has been selected. The collection has the following characteristics: The novels have been published between 1840 and 1910 (13 before 1880 and 17 after 1880), are from three countries (Argentina: 16, Cuba: 9, Mexico: 5), and have been written by 16 different authors. This is a subcollection of a larger corpus of Spanish American novels being prepared in the context of the junior research group  Computation Literary Genre Stylistics (CLiGS), see  .  Fig. 1 shows the distribution of novels per decade and subgenre:     Distribution of novels per decade and subgenre As the texts at hand are not easily distinguishable genre fiction but more general literary fiction, the assignment of subgenre labels is a non-trivial task. For the assignment of subgenre labels to the novels, the subgenres as given in titles and subtitles of the novels were collected and subgenre assignments made in secondary literature were considered. Both types of information were used to derive four kinds of interpretive Because the many variations found had to be normalized for this computational analysis, an interpretive step was unavoidable. subgenre labels corresponding to four broad types of novels: costumbrista (6 novels) Novels of manners in the context of the Costumbrismo movement., historical (8), sentimental (9), and sociopolitical (7) novels. The distribution of novels shows that there is a tendency for sentimental novels to belong to the first half and for non-sentimental novels to the second half of the century. This observation may be relevant for future tasks with a bigger corpus and interested in the development of genres over time. More detailed metadata for the novels can be found at  .     Methods In general, Sentiment Analysis can be done with a machine learning approach and a lexicon-based approach. Here, two sentiment lexica were used: (1) SentiWordNet 3.0, an adaptation of WordNet 3.0 for sentiment analysis (Miller, 1995, Baccianella et al., 2010) and (2) the NRC Emotion Lexicon (Saif and Turney, 2013). The two lexica differ in how sentiments are modelled and also in their volume. SentiWordNet has polarity values (positivity, negativity, neutrality) for WordNet synsets which range between 0 and 1 and sum up to 1. The NRC lexicon, in contrast, has only binary values (0 or 1), but those are provided for positivity and negativity as well as eight basic emotions (Trust, Fear, Joy, Sadness, Anger, Disgust, Anticipation, Surprise). SentiWordNet contains 117,653 entries, the NRC lexicon just 14,182. SentiWordNet can be used for Spanish because the synset IDs can be mapped to the Spanish version of WordNet. The NRC lexicon has been translated into Spanish automatically. See Baccianella et al., 2010 for evaluation reports for SentiWordNet. The authors of the NRC lexicon state that the translated versions may contain errors. An orthographic check on the NRC lexicon returned 409 entries that were not recognized as Spanish words. A further evaluation and improvement of the translated lexica is desirable.  In order to use the sentiment lexica, the texts had to be lemmatized (for NRC) and annotated with WordNet synsets (for SentiWordNet) which was done with the NLP library FreeLing (Padró and Stanislovsky, 2012). To be able to use the distinction between direct speech and narrated text as a feature, the texts were annotated semi-automatically in their TEI master files (see Fig. 2):    Example of a paragraph with annotated direct speech, from \"Camila o la virtud triunfante\" (1856) by Estanislao del Campo Each paragraph was split into sentences. Each sentence was annotated with FreeLing and the words with sentiment values were determined using the lexicons. The sentiment values for the words were summed up for each sentence. The Sentiment Analysis could be refined further by considering the sentence structure (and negation), which is a future task. For the eight basic emotions of the NRC (Trust, Fear, etc.), a sentence is assigned the emotion with a highest value in the sentence. Besides the sentiment features that come directly from the lexicons, the following features were determined for each sentence:      Additional features for the Sentiment Analysis  A Decision Tree classifier was used for the classification of the novels by subgenre, using the above-mentioned features (see Manning and Schütze, 1999: 578-589 on this method). The advantage of Decision Trees is that they can be interpreted. This is desirable in an explorative analysis interested in the kind of sentiment-based features that are relevant to differentiate novels of different subgenres. When compared to other types of classifiers, Decision Trees do not necessarily yield the best results in terms of accuracy, but their interpretability is valued higher here in order to gain insight into how sentiments, the opposition of direct speech vs. narrated text, and subgenres are related.  To generate data forthe machine learning task, the values of the single sentences were aggregated into five sections and divided by the section length (number of sentences contained in the section), resulting in150 data points for the 30 novels. 60 different experiments were run, varying the sentiment features and lexicon used, and the depth of the decision tree. A 5-fold cross-validation was applied.   Results and Discussion The results of the experiments are summarized in Fig. 4 below. The depth of the Decision Tree was varied between 1 and 15. Restricting the tree depth helps to prevent overfitting and usually leads to a better performance of the classifier on the test set. The accuracy is given as the mean F1 score obtained from the cross-validation. Four different sets of sentiment features were used: Features from the SentiWordNet lexicon (WN) and from the NRC lexicon (NRC), both without differentiating between direct speech and narrated text, as well as WN- and NRC-features with separate sentiment values for direct speech and narrated text (WN speech and NRC speech). The results of all experiments are compared to the “most frequent”-baseline and to a baseline obtained with an SVM classifier, using the 5,000 most frequent words.      Results for subgenre classification with sentiment features  Although the F1 scores are not very high (the highest mean value being at 0.52), almost all of them outperform the “most frequent”-baseline (0.3) which confirms that sentiment features are relevant for subgenre classification. Still, the results do not reach the best mean score of the MFW classification (0.57). See Hettinger et al., 2016 for a discussion of various types of features (MFW, topics, networks) for subgenre classification, stating that genre classification in general works best with most frequent words, all words, and the like. In terms of classification accuracy, a next step will be to combine both sentiment features and MFW to see if the sentiment features can contribute to improve the overall results.  When comparing the results for the two different sentiment lexica, the NRC lexicon performs better than SentiWordNet, although the latter covers almost ten times as many words as the first one. A look into the feature importance shows that the eight basic emotions, which are only present in the NRC lexicon, are crucial (see Fig. 5 and 6).     Feature importance for a tree with depth 3, using NRC and speech vs. narrated text        Feature importance for a tree with depth 6, using NRC and speech vs. narrated text    Regarding the difference between speech and narration, Fig. 4 above also shows that the highest values for both WN and NRC are reached when the sentiment values are calculated separately for direct speech and narrated text. The best scores are obtained for the feature set “NRC speech”. The most important feature in both example trees is positive speech, followed by narrated fear. Fig. 7 shows the Decision Tree corresponding to the feature importance in Fig. 5 above.     A Decision Tree for the classification of subgenres, based on the best parameters  The tree shows that novels with higher values of positive speech are more likely to be sentimental novels. Other features that contribute to the distinction of sentimental novels are lower values of trust and higher values of anticipation in narrated text. The path for historical novels includes less positive speech and more fear and anger in narrated text. Costumbrista novels are characterized by less sadness in narrated text than sociopolitical novels and by more trust in narrated text than sentimental novels. Sociopolitical novels differ from historical novels in that they have a lower value of fear and anger in narrated text. The results of all experiments can be found at  .     Conclusion and Future Work This exploration of sentiments in Spanish American Novels showed that Sentiment Analysis can be used as a basis for subgenre classification tasks. It has been shown that the distinction between emotions in direct speech and emotions in narrated improves the classification results considerably. Regarding the two sentiment lexica that were tested, the NRC Emotion Lexicon performs better than SentiWordNet. The Decision Trees resulting from the classification give much insight into how sentiments in general, in direct speech and in narrated text are related to different types of novels. That the features can be interpreted easily contributes to a better understanding of what textual features are connected to the subgenres, but the classification results themselves can still be improved. Other classifiers, for example Random Forest trees or an SVM, might yield better results but will also be less interpretable. Another important next step is to increase the corpus size to make the results more stable.  ",
       "article_title":" Exploration of Sentiments and Genre in Spanish American Novels  ",
       "authors":[
          {
             "given":"Ulrike Edith Gerda",
             "family":"Henny-Krahmer",
             "affiliation":[
                {
                   "original_name":"Universität Würzburg, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "literary studies",
          "computer science",
          "corpora and corpus activities",
          "spanish and spanish american studies",
          "text analysis",
          "data mining / text mining",
          "English",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction For quite some time, humanities scholars have been using digital tools in addition to their established methodology to try and make sense of large and expanding data sources that cannot be handled with traditional methods alone. The digital methods have computer science aspects that may be combined with but do not readily fit into humanities methodology; an issue which is still too implicit in scholarly debate. This gives rise to a need for methodological consolidation to structure the combination of digital and established humanities methods. In this paper, we propose an approach to such consolidation, that we call  data scopes (also see Graham, Milligan, Weingart 2016).   In principle, the methodology is relevant for many humanities disciplines, especially those that deal with large-scale heterogeneous data sets and sources. We think that digital methods should extend, not replace established methodology. Digital tools are now often employed in a methodological vacuum as if they would yield results all by themselves, but we propose that they should always be embedded in broader research methods.    Digital Data and the Research Process  Data and data sets are often presented as external to research, as the data are the sources upon which the research draws. However, all research considers datasets from the vantage point of research questions. Data and questions shape and transform each other in cycles of searching, selecting, close and distant reading, and extending the data with other data sets and annotations. In this process, the scope of the data and the scope of the research questions are aligned so the latter can be addressed. Preparing data for analysis requires interpretation and is therefore inseparably part of research and should be incorporated into the disciplinary methodology. This calls for an extension of usual source criticism with more specifically digital source criticism. In a typical research project, involving digital data, they are processed with a variety of tools that change them in many ways, making tool results and data at times inseparable. Tool and data criticism are therefore intertwined.  We will illustrate our argument with an example from research on the change in discourse regarding migration in Western Europe from 1913-2013 (van Faassen and Hoekstra, 2017). That study focuses on a ‘scientization’ of the migration debate and how the scientists and politicians in the debate were connected to each other. The overall research question can be addressed in many ways, but not straightforwardly answered from a single data set as the discourse spans a very long period and a lot of different media. Therefore the question was split into several specific questions that can be more directly operationalized as analyses of a combination of two digital sources, the  Publications of the Research Group for European Migration Problems (REMP) and the online  International Migration bulletins of the Intergovernmental Committee for European Migration (ICEM), which merged with the  REMP-bulletin. For the first dataset, it was possible to identify key actors and their roles and to address specific questions: “who were writing forewords, prefaces or introductions to each other’s work; Who ordered the research? Who financed it? etc.” (van Faassen and Hoekstra, 2017, p. 7). This requires modelling of actors (persons and organisations), their roles and the relationships between them, normalizing names of persons and mapping changing roles and names of roles over time, and linking them across publications. This in turn requires interpretations relying on domain knowledge that need to be argued for. For the long term trends in the migration discourse, the frequency in the occurrence of key terms was analysed using the other dataset (and a control set) consisting of series of article titles in  International Migration and  International Migration Review, two important long running journals supplemented by topic overviews from WorldCat. This required not only the use of weighted frequency analysis, but also a critical assessment of the value of the various series. Consequently, preparing data sets and analyzing them tends to happen in iterations, where initial analyses inform a next iteration of selecting, modelling, normalizing and linking, and data and research scopes are brought ever closer together.      Conceptual model   Researchers start a research project with a research question, that may be adjusted and expanded in the course of the research process. From the onset, these questions determine the research scope and therefore the scope of which data are relevant. As the research proceeds, partial questions are either answered, or prove to be unanswerable with the available data, because of their form or because of the nature and extent of the data. Researchers then interact with their data, to annotate them in such a way that it enables them to answer questions. They may also pull in other data sets to expand the existing cluster of data, so that the scope of the data will fit the research scope better.  These dynamics of the research process transform the cluster of data into a research instrument, bringing certain aspects of the data into focus, but thereby pushing others to the background or even completely out of view. Making this process transparent allows better reflection on its consequences for interpretation and shaping meaning. Digital sources and data clusters are not just ‘raw material’ that are the object of study, but points of departure that these iterative research interactions change (Boonstra et al. 2006, 23). It is the research process that turns a data cluster into a data scope:  We may discern a limited number of separate activities working that are part of this research process that produces the data scope: ·  Modelling represents the data in such a way that it will fit the research scope  ·  Selection chooses datasets and parts of datasets that are relevant for the research questions  ·  Normalizing structures data and reduces data variation so that they may be queried more easily and they can be used for comparisons, classifications or calculations  ·  Linking data connects previously unconnected data, providing them with context from other data sets. Researchers should be aware that the validity or relevance of links can be context-dependent (person X and Y are linked for a specific question because they played the same role, but the link may be invalid for other questions, see Brenninkmeijer et al 2012).  ·  Classifying data groups them in in order to reduce complexity. This adds a level of abstraction to the data  Because these activities all have the potential to transform the original data, the result is a data scope that is particular to and formed by the research process. This is related to the data life cycle (Boonstra et al. 2006), but the latter focuses on data as the product of research, while data scopes focus on the impact of transformations during the research process. As the research process transforms a cluster of data into a data scope, many levels of annotations are added to the original data sets. For the purpose of our analysis these annotations comprise all types of data enrichment, that range from structuring (for instance by adding markup to a text), to identifying named entities and keywords as structured metadata to adding explanatory notes and everything in between. It is easy to lose track of the changes, as enriching and transforming often goes in small steps and using many different manual and automatic procedures, and because transformations are often cumulative. In light of the incremental transformative effect of the research process upon the data, it is important to keep track of these changes, so that researchers can communicate about and account for their data scopes. Documenting the data changes makes both the research process and the resulting data scope transparent. In this way the research process also becomes reproducible and transferable to other research data clusters. (Groth et al. 2012; Ockeloen et al. 2013)    Discussion  The data scope concept is not a strictly theoretical model, but it is rooted in an experience of many years of empirical research with a lot of different research projects. In the presentation we will illustrate the concept with an example of a research project that combines a number of data sources to analyze the a long-term perspectives on discursive cycles relating to migration and migration policy.  Data scopes are not just a plea to work interdisciplinary and collaboratively, but in a number of research processes they are necessary. While our examples are mostly drawn from the historical sciences, the value of the concept is by no means confined to that disciplinary field. In our view, such a methodological approach is always indispensable when there are large amounts of data available for research and when the data are of heterogeneous in nature. Transparency and transferability is also important when researchers from different disciplines collaborate or when there is a collaboration between humanities researchers and more technologically oriented partners. In those cases, researchers, who have often mostly separate tasks, have to make sure that they understand each other to prevent misunderstandings and waste of resources. Many humanities researchers have already adopted a part of the methodology, but unfortunately they often just use a number of tools, without acknowledging the cumulative transformative effects these have. The value of the proposed model is in the emphasis on a coherent methodological approach to doing research with (large scale) data.   ",
       "article_title":"Data Scopes: towards Transparent Data Research in Digital Humanities",
       "authors":[
          {
             "given":"Rik",
             "family":"Hoekstra",
             "affiliation":[
                {
                   "original_name":"Huygens ING, KNAW, Netherlands, Netherlands, The",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marijn",
             "family":"Koolen",
             "affiliation":[
                {
                   "original_name":"Huygens ING, KNAW, Netherlands, Netherlands, The",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marijke",
             "family":"van Faassen",
             "affiliation":[
                {
                   "original_name":"Huygens ING, KNAW, Netherlands, Netherlands, The",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "digital humanities history",
          "historical studies",
          "library & information science",
          "theory",
          "epistemology",
          "English",
          "criticism",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital archivists tend to disagree about the place of paratexts. Whereas  Google Books often scans texts at such a low resolution that anything but printed words are difficult to discern, Andrew Stauffer’s   Book Traces  project and Steven Olsen-Smith and Peter Norberg's   The Melville Marginalia Project  aims to identify individual copies of nineteenth- and early-twentieth-century books in libraries by highlighting their unique marginalia and inserts. Illustrations, advertisements, marginalia, boards, and decorative initials—the effluvium of the print form—does not digitize easily. Moreover, in terms of library and information science, paratexts resist standard means of categorization. Paratexts are problematic because they offer an exception rather than a type. To scholars, they often seem extraneous or even detrimental to the written texts they accompany. Marginalia, for instance, simultaneously defaces and compliments a text. Advertisements are a distracting and commercial accretion to an artwork. And yet, all paratexts provide necessary context for understanding the complexity and fullness of print history. The question I will address in this paper is how archivists ought broadly to understand paratexts, and how specifically should they treat nineteenth-century illustrations.  Numerous digital archives have taken on the task of scanning, categorizing, and tagging illustrations ( e.g. the  William Blake Archive, the  Cervantes Project, Cardiff University’s  Illustration Archive), and yet the purpose and constraints of this task remain unfixed. In fact, Julia Thomas notes in her recent  Nineteenth-Century Illustration and the Digital (2016), that owing to the uniquely important role of context for these paratexts—usually the book or periodical—\"the digital might appear an alien environment for historic illustrations.\" While the role of the digital image archive concerned with illustrations remains unsettled, recent scholars have used the affordances of the digital archive to open up new avenues for curation and exploration. Using as a case study a digital archive that I direct and edit titled  Visual Haggard, a  NINES indexed and peer reviewed archive that contextualizes and improves access to the illustrations of Victorian novelist H. Rider Haggard (1856 - 1925), I argue that digitizing illustrations must be inclusive.   I will consider the problem of inclusion and exclusion in digital archive curation. As paratexts, illustrations are lumped together with a number of visual objects that initially accompanied fictions. For this reason I explain the necessity of using metadata to differentiate illustration types. The large decorative initials which appear in many nineteenth-century texts, but originated in medieval manuscripts, are less illustrations of the text than embellishments. However, their ideological function is significant and multifold. Similarly, advertisements were often in conversation with serialized fictions—whether thematically or stylistically. In this paper I discuss strategies to enable digital image archivists committed to creating an authentic encounter with the history of print to avoid ignoring or marginalizing these types of unique and difficult paratexts.  ",
       "article_title":"Digitizing Paratexts",
       "authors":[
          {
             "given":"Kate",
             "family":"Holterhoff",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "art and art history",
          "digitisation - theory and practice",
          "digital humanities history",
          "resource creation",
          "cultural studies",
          "theory",
          "English",
          "archives",
          "and discovery",
          "digitisation",
          "epistemology",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  As anyone, who has worked with medieval manuscripts, will know, sometimes more than half of the words are abbreviated. For example, in a forthcoming paper on Middle English and Latin manuscripts of the  Polychronicon, we found that in the most heavily abbreviated Latin sections as many as 59 percent of the words could be abbreviated, while the number for Middle English was 21 per cent (Honkapohja and Liira, in preparation). Studies comparining Latin and Romance have met with similar results (Hasenohr, 1997; Careri et al., 2011). Nevertheless, in digital scholarship, abbreviations are typically seen as something to get rid of rather than useful data to mine.   A major reason for lack of attention given to manuscript abbreviations can be found in editorial practices inherited from printed editions. It is a standard practice for editors to expand abbreviations as “a service to the reader” (cf. Driscoll, 2009). Twentieth-century editorial theory often treats abbreviations as scribal variation as “accidentals” (see e.g. W. W. Greg, 1950), not relevant for the authorial “work” contained in the manuscripts, as much scholarship focuses either directly on the work or uncovering the work under layers of scribal copying and errors. The outcome is an editorial tradition in which silently expanding abbreviations is very much the norm.  Digital approaches for making use of abbreviations as data are available, but are often not used. TEI P5 guidelines introduced the possibility of encoding both the abbreviations and their expansions using the <choice> elements with <abbr> and <expan> (cf. Driscoll, 2006, 2009; Honkapohja, 2013). Still, many digital resources continue the practice of silently expanding abbreviations. Reasons may range from considering encoding abbreviations to be too labour intensive to basing the digital resources on printed editions which expand the abbreviations (cf. Honkapohja et al., 2009). Moreover, text retrieval systems are typically unable to recognize different forms of the same word and the problem is usually solved by normalisation (cf. Kestemont, 2015: 160). Furthermore, some research questions, including investigations into syntax or stemmatology, also require normalisation. However, while normalisation may be necessary for some research questions, it also discards large amount of potentially useful data, which makes other types of research impossible.  The fairly few scholars who do work with abbreviations have identified a number of potentially interesting lines of enquiry. Abbreviations can be used, for example, for identifying change of scribe in the text (cf. Kestemont, 2015) or in historical dialectology for identifying regional characteristics in scribal language (see e.g. Smith, 2016), or studying the effect of right-margin justification on scribal spelling (Shute, 2017), or hiding endings in multilingual business writing (Wright, 2011). Consequently, the practice of expanding abbreviations is discussed and criticised by a number of scholars (Driscoll, 2006; Kyt ö et al., 2011; Rogos 2011, 2012; Stutzman, 2014, Lass, 2004).    Even though the problems related to the prevailing practice of silently expanding are well known, and some resources such as the  Medieval Nordic Text Archive (MENOTA) do encode them, there have been relatively few studies which would have attempted to use them as data (e.g. Camps, 2016; Honkapohja, 2018; Kestemont, 2015; Rogos, 2012; Smith, 2016; Shute, 2017), especially in comparison to fields such as stemmatology and stylometry.  My proposal for short paper presents project plan and early results for a project, called  Corpus Approaches to Manuscript Abbreviations  (CAMA), funded for September 2017- February 2020.    The current project focuses on applying methodologies developed for corpus linguistics on abbreviations in the spelling system of Early Middle English, 1150-1350. The period is of interest as it was a formative one for the writing systems of English. Linguistic situation in England changed dramatically after the Norman Conquest of 1066, which introduced a new ruling class and relegated English to a tertiary role after Latin and Anglo-Norman French. When Middle English texts become more numerous in the 13 th century, we find a very diverse dialect landscape in which the lack of a prestigious vernacular has led to the proliferation of local varieties, with almost every text appearing to represent a separate linguistic system.   Within the Early Middle English period, my project focuses on four research questions:  (Q1) Does each scribe have an individual scribal profile of abbreviations? (Q2) Are some abbreviation usages connected to certain geographic areas? (Q3) How are Latin and Old English abbreviations distributed in Germanic and Romance vocabulary? (Q4) What is the function of abbreviations in the spelling system(s) of Middle English? The data comes from the  Linguistic Atlas of Early Middle English (LAEME), a corpus of ca. 650,000 divided into scribal samples of localised Middle English. Each text in LAEME is based on a diplomatic transcription from manuscript facsimiles, not editions, and using a mark-up system that encodes the expansions of abbreviations, but in a way which makes identifying the abbreviation easy and workable (LAEME: 3.3.1). Consequently, it can be used to compile a dataset, which can be analysed quantitatively.   The methodology is based on corpus linguistics, statistical analysis and historical dialectology. I will use corpus enquiries to compile a dataset of the findings, then subject the dataset to statistical analysis using R and tried and tested techniques such as linear regression, linear correlation, principal component analysis, chi square test and cluster analysis which have yielded results in previous studies of abbreviations and spelling variation (cf. Kestemont, 2015; Smith, 2016).   Compiling the dataset consists of three steps:  Corpus enquiries, using the web interface and scripts of LAEME.  Corpus enquiries for unabbreviated forms of the abbreviated words found in stage 1 in each text, in which a particular abbreviation is used. These can be localised, using the lemmas tagged in the LAEME (see 2.3.2: E). Compiling a dataset the results, which will include  a) results of the corpus enquiries, i.e. the abbreviation type, the abbreviated word, non-abbreviated variant(s), frequencies,  b) information included in the LAEME metadata, i.e. text, lemma, grammatical tag, manuscript, date, script, place, co-ordinates in the LAEME localisation grid, and  c) additional variables needed for research questions Q1 and Q3, i.e. word origin: Germanic/Romance/Latin (12), content vs. function word (13).    The dataset will be subjected to further analysis, using:  The inbuilt mapping function in LAEME, which allows dynamically creating feature maps, based on the distribution of any form, its lemma, or grammatical tag. Statistical analysis,   linear correlation and linear regression, using the form of the abbreviation as the dependant variable, and the results encoded in the dataset (2.3.3: 3) as independent variables, calculating which of them interact with the type of the abbreviation in a certain specimen (cf. Smith, 2016), Principal component analysis common in stylometry (cf. Kestemont, 2015: 168-70).    As I am giving the presentation fairly early in the funding period, I hope to receive valuable feedback on the methodology and also to build a bridge between corpus linguistics and stylometry, creating discussion on the value and potential of scribal 'accidentals' as data.  ",
       "article_title":"A Corpus Approach to Manuscript Abbreviations (CAMA)",
       "authors":[
          {
             "given":"Alpo",
             "family":"Honkapohja",
             "affiliation":[
                {
                   "original_name":"University of Edinburgh, United Kingdom",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "philology",
          "scholarly editing",
          "corpora and corpus activities",
          "linguistics",
          "stylistics and stylometry",
          "data mining / text mining",
          "English",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Introduction For nearly a decade, Brian Vickers has been championing a method of authorship attribution for Early Modern Drama based on the number of rare 3-6-word Ngram matches between an authorial corpus and an anonymous play that are found nowhere else in a reference corpus (Vickers, 2008, 2009, 2010, 2011, 2012). He has consistently argued that word-Ngrams are more appropriate than simple words because of the idiomatic nature of language, but he has recently intensified his attack in “The Misuse of Function Words in Shakespeare Authorship Studies” (2016). Although Vickers claims conclusive results on plays and even parts of plays, his method has been challenged (Burrows, 2012, Craig and Kinney, 2009, Hoover, 2011, 2012, Jackson, 2008, 2010). Three more recent challenges are especially significant.   Antonia, Craig, and Elliott assess “the quality and quantity of authorial markers . . . , rather than success in classification” (2014: 152). They show that, in spite of the inherent sequentiality of language, single words are sometimes the most powerful variables, including in a test on Early Modern Drama. They test only words and word-Ngrams, but their study confirms the effectiveness of frequent words for authorship attribution of drama and offers no significant support for the effectiveness of rare Ngrams.   Jackson accepts the potential usefulness of rare Ngrams while criticizing Vickers’s method (2014). He shows that the presence of many rare word-Ngram matches between Kyd’s corpus and  Arden of Faversham that are found nowhere in other plays of the period does not provide conclusive evidence of Kyd’s authorship because Vickers has not compared the numbers of such matches in plays by other authors. He reports that two Shakespeare plays each “afford considerably more unique matches than any of the three canonical Kyd plays” (2014: 54). Jackson’s work is impressive, but the Early Modern Drama corpus is so problematic that it cannot provide clarity on the question of Vickers’s method. Early Modern spelling variability makes fully automated methods impossible, many of the plays are anonymous, many are not reliably dated, and the majority have been lost entirely (Vickers, 2008: 13).   Finally, Hoover’s tests of the Vickers method on Victorian drama show that some tests fail and that strong false positives can occur (2015). These results are similar to his earlier results on narrative fiction and Modern American poetry (2010, 2011). Unfortunately, he does not include testing with frequent words, frequent character-Ngrams, or frequent word-Ngrams, and crucially does not test the methods head-to-head on exactly the same texts. 2. The Vickers attack on frequent function words Vickers’s recent attack on function words adds somewhat contradictory arguments to his usual argument based on the idiomatic nature of language. First, contrary to the widely-shared view that function words are appropriate variables because their relatively unconscious makes intentional manipulation unlikely, he argues that they are not necessarily used unconsciously (2016, 6-9). Second, he argues that “they are minutiae of usage, of no appreciable significance; and . . . , they exist below the threshold of our unaided perception” (18). He ignores the fact that many analysts use longer word lists including many lexical words. He also argues that authors of drama create distinct idioms for their characters, so that word frequencies cannot capture the author’s own idiom (16). This criticism applies to most genres, and seems to apply to his own method as well, as he accepts rare Ngram matches with any character in a play as evidence of authorial identity. Its chief weakness, however, is that the effectiveness of (function) words as variables for the attribution of drama can be tested empirically, so that this argument, like his other a priori arguments, seems largely irrelevant. It is to such testing that I now turn. 3. Frequent words, frequent word-ngrams, and frequent character-ngrams and Victorian drama I began with a corpus of 125 Victorian plays and 2,600-word sections of plays that mirrors Vickers’s corpus–with Hoover’s extensions (2015)–as closely as possible. I selected 6 authors with corpora of 7 or more plays and treated 4 plays and a 2600-word section from each as if they were anonymous. Using the remaining plays as knowns, I tested these 30 test texts using words, character-2grams, -3grams, and -4grams, and word-2grams, -3grams, and -4grams, using 6 of the methods implemented in JGAAP (Juola, 2009): Burrows’s Delta, Linear Discriminant Analysis, Nearest Neighbor Driver with metric Kendall Correlation TauB, WEKA Linear Regression, WEKA Multilayer Perceptron, and WEKA SMO r: Polynomial. JGAAP was chosen because of its wide variety of methods and variables; testing with Stylo (Eder et. al., 2016) and other methods gave similar results. The best results were 93.3% correct for the 300 most frequent words and the 300 most frequent character-4grams, though words performed better overall. Character-2grams and 3grams were weaker, and word-Ngrams were weaker still (4grams weaker than 3grams, which were weaker than 2grams).   The best results for 7 authors with smaller corpora were 95% correct, but this was achieved just once, based on the 300 most frequent words, and results were weaker overall, presumably because of the smaller corpora (2-4 plays) and because a higher proportion of the test texts were 2600-word sections. Words again gave the best results, then character-2grams, character-4grams, and word-2grams. Longer word-Ngrams gave much weaker results.  These results clearly refute Vickers’s arguments that frequent words are inappropriate for the attribution of drama. Although character-Ngrams are effective for these texts, words alone are even more effective, and increasingly longer word-Ngrams are increasingly ineffective. These results are quite strong, especially considering that 6 of the 30 test texts in the first test and 9 of the 20 in the second are 2,600-word sections. 4. Rare ngrams and Victorian drama How well does Vickers’s rare Ngram method perform on these same texts? I collected all the 3- to 6-word Ngrams that occur at least twice in the 125-text corpus. I established a reference corpus of 86 plays by 14 authors (minimum 3 plays each), and a set of 14 plays and 8 sections of 2,600 words by authors outside the reference corpus, and 2 additional plays and 15 sections of 2,600 words by reference set authors. These 39 additional texts allow a comparison of the frequencies of Ngram matches between the authorial corpus and the author’s test texts and between the authorial corpus and texts by other authors. If Vickers is right, the test texts should contain many more rare Ngram matches with the authorial corpus than texts by other authors do.  To test an author from the reference corpus, I remove that author’s plays and delete all the 3- to 6-grams that occur in the remaining reference corpus. I then create an authorial corpus that matches the known texts tested in JGAAP and delete all the 3- to 6-grams that are absent from it. Because the texts differ greatly in length, I divide the total number of occurrences of each Ngram by the length of the text in words to give a measure of frequency relative to text-length, and then sort the test texts and sections on this frequency. (Vickers gives raw numbers, ignoring the effect of the lengths of the plays; Jackson uses matches per 1,000 lines as a measure of relative frequency [2008: 123-25].) The most favorable measure of success is to count just one error for each of the author’s test texts that is outscored by any text by another author. By this measure, the overall success of Vickers’s method for the 30 test texts 63.3%. All 5 texts by Byron and Phillips are correctly attributed, showing that the test is sometimes effective. Unfortunately, the four other authors show errors for 1, 2, 3, and all 5 test texts. For the 7 authors with smaller corpora, the results are much worse: no author’s texts outscore all texts by other authors, and the overall success rate is just 45%. (Many of these small corpora are the same size as Vickers’s Kyd corpus.) Yet, even these poor results underestimate just how badly the method fails: 20 or more plays or sections by other authors (more than half) outscore 3 of the test texts in the 2 sets, and 10 or more outscore another 5 of the test texts. 5. Conclusion As attractive as Vickers’s rare Ngram method initially seems, and in spite of its apparent effectiveness for some authors, it cannot offer the conclusive proof of authorship that Vickers claims. Frequent words, contrary to Vickers’s a priori arguments, are quite effective in attributing plays and even short sections of plays to their authors, and very much more effective than rare Ngrams (as are frequent character-Ngrams). It is presumably possible that Early Modern Drama is different enough from Victorian drama that the method works better there. However, the results presented here, combined with those for narrative fiction and modern American poetry (Hoover, 2011, 2012), strongly suggest that rare Ngram matching is not a sound method of authorship attribution. ",
       "article_title":"Authorship Attribution Variables and Victorian Drama: Words, Word-Ngrams, and Character-Ngrams",
       "authors":[
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "literary studies",
          "stylistics and stylometry",
          "english studies",
          "text analysis",
          "English",
          "authorship attribution / authority"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Abstract This paper describes a study which analyzes natural disasters described in the Chinese Standard Histories. We first define the scope and nature of disasters as presented in the Standard Histories. The records, in plain text but usually contain the dates, locations, type, and severity of the natural disasters, are then extracted. The extracted records are further annotated with metadata so as to meet the needs of the studies on the history of disasters. In order to ensure flexibility and extensibility, we have designed a markup language, WXML, to tag the information. A search/retrieval system with GIS is then developed to provide visualization of the distribution of time, space, and type of disasters of the search result.  We have made some preliminary observations. For instance, the number of disasters recorded during the Yuan Dynasty is significantly higher than the other dynasties (both in absolute number and on average). As another example, disasters seem to disproportionately concentrate around urban centers, in particular the capital of the time. This shows that the records in the Standard Histories may not accurately reflect the actual events, but rather how they were documented by the officials.   Natural Disasters described in the Chinese Standard Histories   Chinese Standard Histories (正史), 24 in total, are the official histories of the Chinese Dynasties. A Standard History is usually written during the succeeding dynasty, based on existing, often meticulously kept, records of the previous dynasty. These tomes start from  Shiji (史記), written by Sima Qian (司馬遷) in the Han Dynasty around 90 BCE, and ends with  Ming Shi (明史), the Standard History of Ming Dynasty (1368-1644). Together they cover about 2,500 years of China’s written history. Fourteen of the standard Histories contain volumes of  Wuxingzhi (Book of the Five Elements, 五行志), which record natural disasters and mysterious phenomena. Disasters are also documented in the  Benji (Chronical of an Emperor, 本紀), another port of a Standard History. These records document the nature of disaster, time, location, and severity; thus serve as important source for modern studies of the history of disasters in China.  In this paper, we focus on the natural disasters recorded in the  Benji’s and  Wuxingzhi’s in the Standard Histories.   We have also included the  Book of Signs (靈徵志) of  Weishu (魏書), which also contains a fair amount of natural disasters.   We exclude the human-caused and unexplainable phenomena described in the  Wuxingzhi.   The name  Wuxingzhi indicates a view of the world in which the five elements, metal, wood, water, fire, and earth interact with each other. Thus certain phenomena were interpreted as signals of the missing of balance. However, the portion of this type of writing diminished significantly after the 10th century (You , 2007).   After analyzing the formation of the  wuxingzhi’s and other studies of natural disasters, we classified the natural disasters into 14 categories: flood, rain, frost, hail, famine, drought, cold, snow, wind, locust, borer, plague, earthquake, and landslide.   Fire is not considered a natural disaster. Although some fire might be due to natural reasons such as forest fire caused by lightning, researchers of natural disasters usually regard fire, as a general category, a manmade disaster since it is often hard to identify the cause (Zhang, 2012)).      Processing the Records and Markup   We have designed an XML format (Wuxing Markup Language, or WXML) to tag the texts.  A  record is a writing of natural disaster indicated in the text. A record contains the following elements:  event,  time period,  area,  severity, and  frequency. A record may describe several  events. For instance, a record of drought often also mentions famine. In this case, both events are tagged.  Time period (written using dynasty, era, year, month, day) has three subtags: starting time, ending time, and duration. If only a date is indicated, that date is considered the starting date. If there’s no mentioning of duration or ending date, then the ending date is the same as the starting date. If duration is vague (such as “it rained for some 30 days”), then the ending date tag will not be filled. The element  area contains two subjects: location and range. Since one or several administrative regions, a river or a mountain range may be indicated in a disaster, the location tag may have multiple values. The range tag could also be an administrative region or a geographical entity. When a record describes the area as “capitol and its surrounding prefectures”, the location will be the capitol of the time, and the range will be the “surrounding prefectures”.  Severity includes the effect, the damages, and the reactions that followed. For example, a flood may include the effect of the breaking of the embankment which results in flooding of the farms and houses (damages), which leads to the reduction in taxes in the following year (reaction).  Frequency is less complicated, although not entirely trivial. A record may mention several earthquakes, without indicating the exact number. In this case, it will simply be tagged as “several”.    Producing and Counting the events   We first use the 14 keywords of disasters to extract descriptions mentioning the disasters. The paragraphs are then parsed automatically to identify the records and their time, event, area, etc. We remark that each description may contain several events, several locations, or even several time periods. We then tag the events, time periods, and locations automatically from the descriptions. The dates are standardized using the Buddhist Studies Time Authority Databases developed at Dharma Drum College ( http://authority.dila.edu.tw/time/). Geographic coordinates are provided using the Chinese Civilization in Time and Space developed at the Academia Sinica ( http://ccts.ascc.net/). An expert is then asked to go through the result to correct manually.  Several ways have been used in the literature to count the number of events. A record may involve multiple locations, different years, and multiple disasters. The same disaster may also appear in different books. A simple way that counts only the appearance of a type of disaster was used in (Deng , 1973) (regardless of the frequency, locations and severity, it is counted as 1 if it appeared in China during that year at least once. Otherwise it is 0 for that year). This method was adopted later by other researchers (Luo, 2005). At the other extreme, each tuple of time, disaster, location is recorded as one event (Yuan , 2008). A third option is to specify a tuple of time and location as an event without consider the other attributes (Wang, 2005; Zhang, 2007). By using tags, our approach provides the flexibility of being able to adjust to any of these counting methods, without being forced to pre-select one, by simply turning on or off an attribute.   Using single time and type as the event unit (while counting multiple locations as one), we tabulated a total of 9,717 events of natural disasters mentioned in Chinese Standard Histories, after removing duplicates from 6,653 events mentioned in  Wuxingzhi and 3,848 in  Benji. (We also removed 489 duplicate events between  Yuanshi and  New Yuanshi, and 79 duplicate events between  Old Tangshu and  New Tangshu.) The time distribution is as follows:     Figure 1 Distribution of natural disasters (X-axis: 5-year as a unit; Y-axis: frequency) Note that the number of natural disasters recorded reached a peak during the Yuan Dynasty (1271-1368 BCE). ( Yuanshi, 元史, only documented events occurred in China proper, not the Mongolian empire that ruled most part of the known world at the time.)    The system and some observations   We have built a system using the events of natural disasters mentioned above. Our interface allows one to specify one or several types of disasters, the era, and/or the areas and show the resulting data in number (or in graphs), on map, and also the texts of the events and their sources. The following is an example of disasters in the Guanzhong (關中) area.    Figure 2 The number of disasters in Guanzhong area    Figure 3 The percentage of disasters in Guanzhong area vs the country The x-axis in both figures are years (in 5 years) in western calendar, while the y-axis of Figure 2 is the absolute number of disasters and the y-axis of Figure 3 is the  percentage of  all natural disasters recorded in the entire China during that time period. Note that although the number of disasters peaked around the year 1300, the percentage was dramatically high during the early Tang dynasty (618-907 BCE), when Changan (長安), a city in Guanzhong (關中), was the capital at the time. After the demise of Tang, the attention of latter empires gradually shifted to the northeast and south, and the percentage of disasters trailed off significantly, as Guanzhong gradually became irrelevant.  There are other interesting phenomena. For instance, there seemed to be more natural disasters during prosper periods. This may indicate that when the country was going through great turbulence such as foreign invasion or peasant revolt, the local officials simply did not bother to report natural disasters.  Concluding Remarks  In this paper we described a study on the natural disasters documented in the Chinese Standard Histories. We analyzed previous work on natural disasters and classified the events into 14 categories. We extracted texts of the records from  Wuxhingzhi and  Benji, and developed a markup language WXML to tag the events. We then build a system which is flexible in that one can use any of the measures mentioned above to show the results. Since the records are time-standardized and geo-referenced, our system also allows one to specify the type of disasters, time period, and locations and present the results either as charts or geographically. We are currently developing our system to allow full-text search to add flexibility.  We presented some preliminary observations. They seem to show that the natural disasters documented in the Standard Histories may not truthfully reflect the actual natural disasters that occurred. In other words, the records may reflect more on the circumstances under which the books were produced rather than the actual disasters that occurred. To more accurately capture natural disasters in Chinese history, one should at least also consult the local gazetteers (difangzhi, 地方志) (Chen, 2016). The WXML that we have designed is sufficiently flexible to incorporate those records as well. ",
       "article_title":"On Natural Disasters In Chinese Standard Histories",
       "authors":[
          {
             "given":"Hong-Ting",
             "family":"Su",
             "affiliation":[
                {
                   "original_name":"National Taiwan University, Taiwan",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Jieh",
             "family":"Hsiang",
             "affiliation":[
                {
                   "original_name":"National Taiwan University, Taiwan",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Nungyao",
             "family":"Lin",
             "affiliation":[
                {
                   "original_name":"National Taiwan University, Taiwan",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "digitisation - theory and practice",
          "historical studies",
          "text analysis",
          "English",
          "asian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" My research is multi- and interdisciplinary focusing on electronic literature and cybercultures in/of Latin America. My latest articles and book manuscript explore the divide and convergence in literature and technology. This project lends itself well to the application of those theories and the evaluation of how they can best be implemented in classroom practices and complemented with co-curricular modules. I will therefore present my research findings on the use of Digital Humanities components specifically for the teaching of Latin American Studies. The presentation would thus serve as a report of: 1) initial research findings and best practices found at other institutions; 2) work accomplished at the DHSI 2018 Workshop (Victoria, Canada) “Critical Pedagogy and Digital Praxis in the Humanities”; 3) feedback gained from presentation at the DHSI 2018 Conference & Colloquium; and, 4) samples of syllabi to foster a lively discussion on the application of such a course with co-curricular components for Latin American Studies programs. The goal of this project is to do a detailed study of program and curriculum design at other institutions on the use of DH modules specifically for Latin America/US Latino culture with a focus on pedagogical methodologies that engage critically about the problems that DH platforms do and do not resolve in Latin American Studies. The course design and the co-curricular components complement and intersect each other. This project will facilitate the assessment of various curriculums and specialized courses for the digital humanities and would ultimately lead to develop a course for all students interested in DH Latin American Studies.   Students in this course would include (but not limited to) those in the Latin American Studies Minor Program, International Business, International Studies (BAIS and GPIS), Humanities, Political Science, Spanish majors and Minors, World Cultural Studies majors and minors.  Course components will include developing language proficiency, learning and using DH tools, and analyzing the effectiveness and drawbacks of such technologies specifically to Latin American Studies.  The interactive, systematic, and innovative features of digital humanities have been proven to advance language learning both in and outside of the classroom. Through exploring different forms of digital humanities, including multi-media, online archives, as well as existing web tools like Google Earth and Twitter, instructors and scholars of foreign languages not only facilitate collective and immersive language learning, but also broaden and deepen students’ exposure and knowledge of foreign culture. These projects break the traditional geographical and cultural boundaries in learning a foreign culture and/or language. Therefore, it is essential for instructors to reflect on how best to incorporate digital humanities in language/culture learning, and to determine to what extent digital learning complements and even replaces traditional ways of teaching and learning.  Students will be encouraged to adapt these new tools of analysis to their own future career objectives. The field of Digital Humanities is collaborative and very interdisciplinary as it produces new scales of analysis with varying modules (texts, maps, audio-mapping and networks) which may include experiments across modalities with: distant reading alongside close reading techniques, programming language, audio creation, geotagging, speech recognition encoding documents in TEI (Text Encoding Initiative   Text Encoding Initiative Markup Language at the University of Virginia,  https://dh.virginia.edu/tool/text-encoding-initiative-markup-language-tei (for my future reference)  ), learning the basics of computational text analysis, programming chatbots using the Python programming language, etc. The course will also note the drawbacks or pitfalls of the use of technology.  However, the skills needed in DH have less to do with a particular hardware or commercial software and more about engaging in digital literacy (train interpretative methods necessary for critical analysis), and showcase how digital humanities is valuable to better understand Latin America’s transformations in the production, circulation and reception as well as its impact on culture, politics, history, literature, music, etc. The course will encourage students to develop more analytical projects from the use of such modalities. The focus will also be to analyze and address  why this method of learning is complementary or even superior to traditional methods, specifically addressing the impact and implications that technology involved on ideologies, ethics and ideas. For example, a more involved topic would approach the idea of “mapping” as interpretation of geospatial data in GIS, georectify historical maps in  Map Warper, manage digital archival objects in  Omeka, and use  Neatline to build “deep maps” of particular neighborhoods or landmarks in a city, layering historical photographs, maps, geospatial data, literary texts, and other elements to build analysis about their city.   Additionally, the course will attempt to link to public libraries (Slover in Norfolk), museums (Chrysler, Mariners, Living Museum), research centers, community groups (Norfolk Chamber of Commerce, Hispanic Chamber of Commerce, Hispanic Community Dialogue) or other campus-level initiatives (ODU’s Institute of Humanities “Mapping Lambert’s Point Project,” for instance). The goal is to build projects that make use of the University and community’s collections. These public projects can energize students to work that much harder, as they can create materials with a chance of life beyond the classroom itself. The course will draw on resources from, participate in and continue their learning with the Regional, National and International Network   To be featured in the Latin American Studies Program website  aimed to promote digital humanities initiatives to Old Dominion University faculty and to learn from and collaborate with external groups.   I already have established contacts and am in current collaborations with: Centro de Cultura Electrónica in Mexico City; the project Cultura Digital Chile (Universidad Diego Portales, Chile); the Latin American and Digital Humanities/Cybercultures at University of Georgia; the Digital Latin American Cultures Network: Researching the Cultural Dimensions of New Media in the United Kingdom; I am also a board member of the organization Lit-e-Lat: Red de Literatura Electrónica.  This network would be dedicated to exploring, analyzing, and sharing the cultural and visual modalities of digital humanities in the research and teaching of Latin America. The network would engage in these discussions through symposia for faculty and students with guest speakers or virtual conferences, virtual exhibitions, and online or hybrid workshops.   For example, “Tecnoestética y sensorium contemporáneo: arte, literatura, diseño y tecnología” in September 2017 in Córdoba Argentina;   The network and initiatives that I foresee fostering and/or facilitating may include:    K-12 Service Education: Working with the College of Education and the Licensure Students in the World Languages and Cultures Department to: Expand on its longstanding educational outreach commitments with K-12 educators and students at the local and state level; and, serve as a resource to K-12 educators working to meet Virginia Performance Standards as they relate to Latin American content in the social, natural, and life sciences by  Language Without Borders Initiative: Create the next generation of global professionals through innovative language education, with Superior level proficiency in Spanish and overseas internship experience. DH and Latin/o American Cybercultures Initiative: Exposure to the digital culture of Latin America through seminars, symposia, courses, exhibitions, and workshops.  ",
       "article_title":"Digital Humanities in Latin American Studies: Cybercultures Initiative",
       "authors":[
          {
             "given":"Angelica J.",
             "family":"Huizar",
             "affiliation":[
                {
                   "original_name":"Old Dominion University, United States of America",
                   "normalized_name":"Old Dominion University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04zjtrb98",
                      "GRID":"grid.261368.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "and curriculum",
          "globalization & digital divides",
          "knowledge representation",
          "spanish and spanish american studies",
          "teaching",
          "cultural studies",
          "pedagogy",
          "English",
          "interdisciplinary & community collaboration",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Alan​ ​Liu​ ​has​ ​called​ ​upon​ ​digital​ ​humanists​ ​to​ ​think​ ​more​ ​critically​ ​about​ ​infrastructure​ ​-​ ​the​ ​“social cum​ ​technological​ ​milieu​ ​that​ ​at​ ​once​ ​enables​ ​the​ ​fulfillment​ ​of​ ​human​ ​experience​ ​and​ ​enforces constraints​ ​on​ ​that​ ​experience” (Liu, 2017).​ ​Liu’s​ ​invitation​ ​comes​ ​at​ ​the​ ​moment​ ​when​ ​researchers​ ​involved​ ​in large-scale,​ ​long-term​ ​projects​ ​are​ ​shifting​ ​focus​ ​from​ ​remediation​ ​and​ ​the​ ​creation​ ​of​ ​digital incunabula​ ​to​ ​transmediation​ ​and​ ​the​ ​development​ ​of​ ​systems​ ​that​ ​support​ ​sustained​ ​discourse across​ ​ever-morphing​ ​digital​ ​networks,​ ​when​ ​we​ ​are​ ​recognizing​ ​the​ ​potential​ ​for​ ​“dynamism​ ​of​ ​the base​ ​or​ ​serialized​ ​form​ ​of​ ​the​ ​text—the​ ​state​ ​in​ ​which​ ​it​ ​is​ ​stored—as​ ​opposed​ ​to​ ​dynamic​ ​modes​ ​of presentation” (Brown, 2016: 288)​. ​REED​ ​London​ ​is​ ​one​ ​such​ ​project​ ​with​ ​a​ ​polyvalent​ ​dataset​ ​that​ ​spans​ ​over​ ​500 years’​ ​worth​ ​of​ ​archival​ records, ​embracing​ ​from​ ​the​ ​start​ ​the​ ​need​ ​to​ ​establish​ ​a​ ​stable, ​responsive production​ ​and​ ​presentation​ ​environment​ ​primed​ ​for​ ​use​ ​by​ ​a​ ​wide​ ​range​ ​of​ ​scholarly​ ​audiences. Thus​ ​we​ ​find​ ​that​ ​we​ ​are​ ​immediately​ ​testing​ ​those​ ​infrastructural​ constraints. ​In​ ​this​ ​paper, members​ ​of​ ​the​ ​REED​ ​London​ ​project​ ​team​ ​will​ ​address​ ​the​ ​challenges​ ​we​ ​face​ ​as​ ​we​ ​develop​ ​and implement​ ​a​ ​framework​ ​that​ ​trains​ ​us​ ​to​ ​think​ ​about​ ​our​ ​collected​ ​data​ ​in​ ​relation​ ​to​ ​much​ ​larger networks​ ​of​ ​disparate​ ​resources​ ​and​ ​user​ ​needs. REED​ ​London​ ​develops​ ​from​ ​a​ ​partnership​ ​between​ ​the​ ​Records​ ​of​ ​Early​ ​English​ ​Drama​ ​(REED) and​ ​the​ ​Canadian​ ​Writing​ ​Research​ ​Collaboratory​ ​(CWRC). ​Together​ ​we​ ​are​ ​establishing​ ​an​ ​openly accessible​ ​online​ ​scholarly​ ​and​ ​pedagogical​ ​resource​ ​of​ ​London-centric​ documentary, editorial, and bibliographic​ ​materials​ ​related​ ​to​ ​performance,​ ​theatre,​ ​and​ ​music​ ​spanning​ ​the​ ​period​ ​1100-1642. With​ ​support​ ​from​ ​the​ ​Andrew​ ​W.​ ​Mellon​ ​Foundation​ ​and​ ​a​ ​CANARIE​ ​Research​ ​Software​ ​Program grant,​ ​a​ ​team​ ​of​ ​researchers​ ​in​ ​the​ ​digital​ ​humanities​ ​and​ ​performance​ ​history​ ​from​ ​the​ ​U.S., Canada,​ ​and​ ​the​ ​U.K.​ ​are​ ​building​ ​a​ ​stable,​ ​extensible​ ​editorial​ ​production​ ​and​ ​publication environment​ ​that​ ​will​ ​create​ ​new​ ​possibilities​ ​for​ ​scholarly​ ​presentation​ ​of​ ​archival​ ​materials​ ​gathered from​ ​legal,​ ​ecclesiastical,​ ​civic,​ ​political,​ ​and​ ​personal​ ​archival​ ​sources​ ​in​ ​and​ ​around​ ​London.​ ​The REED​ ​London​ ​project​ ​combines​ ​materials​ ​from​ ​three​ ​printed​ ​REED​ ​collections​ ( Inns ​ of​ ​ Court,  Ecclesiastical​ ​ London​,​ ​and​  Civic​ ​ London​ ​ to​  1558)​,​ ​the​ ​prosopographical​ ​material​ ​from​ ​REED’s  Patrons​ ​ &​ ​ Performances​ ​ (P&P)​,​ ​the​ ​bibliographical​ ​materials​ ​of​ ​the​​  Early​ ​ Modern​ ​ London​ Theatres (EMLoT)​ ​​database,​ ​and​ ​in-progress​ ​and​ ​planned​ ​digital​ ​collections​ ​focusing​ ​on​ ​London​ ​area performance​ ​spaces,​ ​most​ ​notably​ ​the​ ​Globe,​ ​Rose,​ ​and​ ​Curtain​ ​theatres​ ​and​ ​Civic​ ​London 1559-1642.  REED​ ​is​ ​an​ ​internationally​ ​renowned​ ​scholarly​ ​project​ ​that​ ​has​ ​worked​ ​to​ ​locate,​ ​transcribe,​ ​and​ ​edit evidence​ ​of​ ​drama,​ ​secular​ ​music,​ ​and​ ​other​ ​communal​ ​entertainment​ ​in​ ​Britain​ ​from​ ​the​ ​Middle Ages​ ​until​ ​1642.​ ​Since​ ​1979​ ​REED​ ​has​ ​published​ ​twenty-seven​ ​printed​ ​collections​ ​of​ ​transcribed records​ ​plus​ ​contextual​ ​materials.​ ​REED​ ​has​ ​long​ ​recognized​ ​the​ ​importance​ ​of​ ​online​ ​access​ ​to​ ​its resources,​​first​​ with  P&P​​​ and​​​  EMLoT​, ​​and​​ more​​ recently​​ with ​​the ​​born-digital​​ collection​  Staffordshire​. REED​ ​has​ ​wrestled​ ​with​ ​the​ ​balance​ ​between​ ​what​ ​was​ ​once​ ​considered​ ​its​ ​“core”​ ​print​ ​publication activities​ ​and​ ​“adjunct”​ ​digital​ ​efforts,​ ​in​ ​the​ ​process​ ​migrating​ ​its​ ​data​ ​across​ ​a​ ​succession​ ​of programs​ ​and​ ​formats​ ​from​ ​Basic​ ​and​ ​dBASE​ ​to​ ​TEI​ ​P5​ ​XML​ ​and​ ​MySQL (Hagen,​ ​MacLean,​ ​and​ ​Pasin, 2014).​ ​REED​ ​has​ ​developed​ ​its digital​​ resources ​​in​​ ways ​​that​​ complicate ​​integration ( P​ &P ​​​exists ​​in​​ a ​​Drupal​​instance;​​​  EMLoT ​​​was built​ ​in​ ​a​ ​version​ ​of​ ​Django​ ​that​ ​is​ ​now​ ​out-of-date;​ ​ REED​ ​Staffordshire​ ​was​ ​lightly​ ​tagged​ ​in​ ​TEI​ ​and relies​ ​on​ ​EATSML for entity management, an​ ​XML​ ​format​ ​used​ ​by​ ​the​ ​Entity​ ​Authority​ ​Tool​ ​Set​ ​(EATS)​ ​for​ ​serialisation​ ​of​ ​its​ ​data).​ ​The​ ​components​ ​of​ ​REED​ ​London​ ​must​ ​therefore​ ​first be​ ​made​ ​intra-operable​ ​before​ ​they​ ​can​ ​become​ ​interoperable (Jakacki, 2016).​ ​The​ ​partnership​ ​with​ ​CWRC supports​ ​broader​ ​adoption​ ​of​ ​standards​ ​for​ ​TEI​ ​text​ ​markup,​ ​RDF​ ​metadata​ ​specifications,​ ​and named​​ entity ​​aggregation,​​ most ​​immediately​​ with ​​the ​​ingestion​​ of ​ EMLoT ​​​and ​​the​​ printed ​​​ Inns ​​ of Court​​ ​collection.  CWRC​ ​is​ ​an​ ​online​ ​infrastructure​ ​project​ ​designed​ ​to​ ​enable​ ​unprecedented​ ​avenues​ ​for​ ​studying the​ ​words​ ​that​ ​most​ ​move​ ​people​ ​in​ ​and​ ​about​ ​Canada.​ ​Built​ ​with​ ​funding​ ​from​ ​the​ ​Canada Foundation​ ​for​ ​Innovation,​ ​the​ ​CWRC​ ​platform​ ​supports​ ​best​ ​practices​ ​in​ ​the​ ​production​ ​of​ ​online collections,​ ​editions,​ ​born-digital​ ​essays,​ ​anthologies,​ ​collections,​ ​monographs,​ ​articles,​ ​or bibliographies,​ ​and​ ​supports​ ​the​ ​inclusion​ ​of​ ​visual,​ ​audio,​ ​and​ ​video​ ​sources (​About​ ​CWRC/CSÉC).​ ​It​ ​supports collaboration​ ​through​ ​the​ ​use​ ​of​ ​interoperable​ ​data​ ​formats​ ​and​ ​interlinking​ ​of​ ​materials,​ ​and​ ​for teams​ ​like​ ​REED​ ​London​ ​provides​ ​invaluable​ ​tools​ ​for​ ​communicating,​ ​tracking​ ​activity,​ ​and workflow.​ ​We​ ​envision​ ​that​ ​as​ ​the​ ​partnership​ ​develops​ ​and​ ​as​ ​REED​ ​London​ ​advances​ ​through production​ ​toward​ ​publication​ ​we​ ​will​ ​take​ ​full​ ​advantage​ ​of​ ​CWRC’s​ ​functionality.​ ​From​ ​the​ ​start​ ​we have​ ​worked​ ​directly​ ​in​ ​CWRC’s​ ​unique​ ​editor,​ ​CWRC-Writer,​ ​which​ ​allows​ ​us​ ​to​ ​edit​ ​REED​ ​London records,​ ​essays,​ ​and​ ​bibliographical​ ​material​ ​using​ ​more​ ​diplomatic​ ​and​ ​critical​ ​TEI​ ​P5​ ​XML​ ​markup and​ ​at​ ​the​ ​same​ ​time​ ​creating​ ​semantic​ ​web​ ​annotations​ ​with​ ​RDF​ ​to​ ​identify,​ ​manage,​ ​and​ ​interlink entities​ ​contained​ ​within.​ ​The​ ​platform​ ​is​ ​also​ ​helping​ ​us​ ​to​ ​develop​ ​a​ ​better​ ​editorial​ ​workflow through​ ​management​ ​of​ ​access​ ​to​ ​data​ ​and​ ​editing​ ​by​ ​role,​ ​team​ ​communications,​ ​tracking​ ​and reporting​ ​of​ ​team​ ​activities. To​ ​ensure​ ​REED​ ​London’s​ ​stability​ ​and​ ​sustainability​ ​while​ ​extending​ ​its​ ​content​ ​and​ ​value​ ​to​ ​new generations​ ​of​ scholars​ ​the​ ​project​ ​is​ ​being​ ​built​ ​within​ ​the​ ​CWRC​ environment. ​The​ ​scope​ ​of​ ​REED London​ ​would​ ​not​ ​be​ ​possible​ ​without​ ​the​ ​sophisticated,​ ​integrated​ ​platform​ ​that​ ​CWRC​ ​provides. The​ ​focus​ ​of​ ​our​ ​first​ ​year​ ​is​ ​the​ ​design​ ​and​ ​construction​ ​of​ ​a​ ​collaborative​ ​online​ ​production​ ​and publication​ ​environment.​ ​Extending​ ​from​ ​CWRC’s​ ​existing​ ​integrated​ ​content​ ​management​ ​and preservation​ ​system,​ ​the​ ​enhanced​ ​environment​ ​will​ ​accommodate​ ​the​ ​range​ ​of​ ​record​ ​texts, editorial​ ​and​ ​bibliographical​ ​content​ ​from​ ​the​ ​source​ ​materials,​ ​while​ ​a​ ​customized​ ​browser-based CWRC-Writer​ ​platform​ ​will​ ​support​ ​the​ ​team’s​ ​goal​ ​of​ ​developing​ ​online​ ​editorial​ ​collaboration​ ​and review.​ ​The​ ​resulting​ ​streamlined​ ​production​ ​and​ ​publication​ ​environment​ ​will​ ​yield​ ​multi-faceted user-centered​ ​editions,​ ​meaning​ ​that​ ​agile​ ​component​ ​archival​ ​and​ ​editorial​ ​parts​ ​can​ ​cohere according​ ​to​ ​various​ ​criteria​ ​in​ ​response​ ​to​ ​scholars’​ ​research​ ​and​ ​teaching​ ​needs.​ ​In​ ​this​ ​way​ ​we are​ ​establishing​ ​a​ ​platform​ ​that​ ​produces​ ​new​ ​forms​ ​of​ ​“edition”​ ​that​ ​combine​ ​customized​ ​textual​ ​and contextual​ ​materials,​ ​exportable​ ​customized​ ​datasets​ ​and​ ​dynamic​ ​data​ ​visualizations.​ ​It​ ​also​ ​means that​ ​we​ ​will​ ​be​ ​able​ ​to​ ​realize​ ​the​ ​promise​ ​of​ ​extending​ ​the​ ​value​ ​of​ ​these​ ​materials​ ​to​ ​colleagues​ ​in fields​ ​beyond​ ​performance​ ​history,​ ​including​ ​political,​ ​religious,​ ​and​ ​cultural​ ​studies,​ ​and​ ​linguistics.  The​ ​partnership​ ​between​ ​CWRC​ ​and​ ​REED​ ​allows​ ​us​ ​to​ ​explore​ ​the​ ​potential​ ​for​ ​new​ ​research applications​ ​associated​ ​with​ ​prosopography,​ ​networks,​ ​and​ ​deep​ ​contextualization.​ ​REED​ ​London’s wealth​ ​of​ ​references​ ​to​ ​very​ ​itinerant​ ​individuals​ ​across​ ​contemporaneous​ ​records​ ​means​ ​that​ ​we will​ ​be​ ​able​ ​to​ ​discern​ ​patterns​ ​through​ ​linking,​ ​analysis,​ ​and​ ​visualization.​ ​We​ ​will​ ​leverage​ ​REED’s named​ ​entities​ ​for​ ​linking​ ​people,​ ​places,​ ​events,​ ​and​ ​organizations.​ ​Our​ ​team​ ​has​ ​healthy​ ​debates about​ ​the​ ​problematic​ ​present​ ​of​ ​linked​ ​data.​ ​Brown​ ​has​ ​stated​ ​that,​ ​“linking​ ​up​ ​with​ ​other​ ​data means​ ​connecting​ ​one​ ​ontology​ ​to​ ​another,​ ​and​ ​this​ ​brings​ ​with​ ​it​ ​a​ ​pressure​ ​toward​ ​generalization rather​ ​than​ ​specificity” (Brown,​ ​Simpson,​ ​et.​ ​al.,​ ​2015).​ ​Cummings​ ​has​ ​posited​ ​that​ ​“being​ ​able​ ​to​ ​seamlessly​ ​integrate​ ​highly complex​ ​and​ ​changing​ ​digital​ ​structures​ ​from​ ​a​ ​variety​ ​of​ ​heterogeneous​ ​sources​ ​through interoperable​ ​methods​ ​without​ ​either​ ​significant​ ​conditions​ ​or​ ​intermediary​ ​agents​ ​is​ ​a​ ​deluded fantasy” (Cummings​ ​2014).​ ​Still,​ ​as​ ​a​ ​group​ ​we​ ​hope​ ​that​ ​by​ ​publishing​ ​our​ ​ontologies​ ​as​ ​a​ ​means​ ​of​ ​relating​ ​these entities​ ​as​ ​linked​ ​open​ ​data,​ ​we​ ​will​ ​be​ ​able​ ​to​ ​contribute​ ​to​ ​larger​ ​dialogues​ ​about​ ​class​ ​and​ ​society in​ ​Britain​ ​-​ ​certainly​ ​over​ ​the​ ​500​ ​years​ ​covered​ ​by​ ​REED​ ​London,​ ​but​ ​also​ ​about​ ​the​ ​development​ ​of Britain​ ​and​ ​Europe.​ ​CWRC​ ​content​ ​will​ ​be​ ​aggregated​ ​by​ ​the​ ​Advanced​ ​Research​ ​Consortium (ARC),​ ​and​ ​REED​ ​London​ ​will​ ​benefit​ ​from​ ​that​ ​aggregation,​ ​as​ ​we​ ​anticipate​ ​that​ ​people​ ​who​ ​figure in​ ​the​ ​REED​ ​London​ ​corpus,​ ​such​ ​as​ ​Elizabeth​ ​I,​ ​Francis​ ​Bacon,​ ​and​ ​Inigo​ ​Jones​ ​will​ ​be discoverable​ ​by​ ​scholars​ ​searching​ ​for​ ​these​ ​known​ ​figures​ ​across​ ​other​ ​linked​ ​resources.​ ​Perhaps more​ ​important,​ ​REED​ ​London​ ​records​ ​include​ ​extended​ ​references​ ​to​ ​thousands​ ​of​ ​Londoners​ ​who were​ ​in​ ​some​ ​way​ ​connected​ ​to​ ​performance,​ ​but​ ​who​ ​were​ ​not​ ​defined​ ​by​ ​that​ ​connection:​ ​civic officials,​ ​guild​ ​members,​ ​lawyers,​ ​clerks,​ ​priests,​ ​etc.​ ​The​ ​work​ ​of​ ​this​ ​project​ ​thus​ ​holds​ ​as​ ​yet unrealized​ ​value​ ​for​ ​a​ ​much​ ​broader​ ​understanding​ ​of​ ​British​ ​historical​ ​subjects. Working​ ​within​ ​CWRC’s​ ​platform​ ​and​ ​optimizing​ ​CWRC-Writer​ ​has​ ​allowed​ ​the​ ​core​ ​REED​ ​London team​ ​to​ ​move​ ​efficiently​ ​to​ ​an​ ​advanced​ ​planning​ ​phase.​ ​By​ ​the​ ​end​ ​of​ ​2017​ ​we​ ​will​ ​have designed​ ​templates​ ​for​ ​all​ ​record​ ​formats​ ​from​​  Inns of Court​ ​​and​ ​mapped​ ​database​ ​fields​ ​from  EMLoT​​ ​to​ ​align​ ​with​ ​the​ ​record​ ​parts​ ​from​ ​the​ ​print​ ​collections.​ ​We​ ​will​ ​have​ ​harvested​ ​a​ ​preliminary “white​ ​list”​ ​of​ ​named​ ​entities​ ​(people,​ ​places,​ ​organizations)​ ​from​ ​all​ ​three​ ​print​ ​​​collection​ ​indexes, P&P,​ ​and​ ​Staffordshire.​ ​Because​ ​of​ ​this​ ​efficient​ ​onramp​ ​we​ ​will​ ​be​ ​able​ ​to​ ​focus​ ​in​ ​the​ ​first​ ​half​ ​of 2018​ ​on​ ​ingesting​ ​data,​ ​records,​ ​and​ ​contextual​ ​materials​ ​from​ ​Inns​ ​of​ ​Court​ ​and​ ​EMLoT.​ ​We​ ​will test​ ​the​ ​REED-specific​ ​entity​ ​list​ ​on​ ​ingested​ ​materials.​ ​We​ ​will​ ​also​ ​begin​ ​to​ ​user-test​ ​the​ ​editorial workflow​ ​system​ ​with​ ​the​ ​larger​ ​project​ ​team​ ​of​ ​REED​ ​editors​ ​and​ ​staff.​ ​By​ ​June​ ​2018​ ​we​ ​will​ ​have begun​ ​semantic​ ​tagging​ ​and​ ​experimentation​ ​with​ ​the​ ​CWRC​ ​HuViz​ ​semantic​ ​web​ ​visualization​ ​tool. At​ ​the​ ​DH​ ​2018​ ​conference​ ​we​ ​will​ ​report​ ​on​ ​further​ ​customization​ ​of​ ​the​ ​CWRC​ interface,​ our​ ​plans for​ ​data​ ​discovery​ ​and​ ​research​ ​collaboration,​ ​and​ ​present​ ​preliminary​ ​plans​ ​for​ ​user-responsive editions​ ​and​ ​data​ ​linkage.  ",
       "article_title":"REED London and the Promise of Critical Infrastructure",
       "authors":[
          {
             "given":"Diane Katherine",
             "family":"Jakacki",
             "affiliation":[
                {
                   "original_name":"Bucknell University, United States of America",
                   "normalized_name":"Bucknell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00fc1qt65",
                      "GRID":"grid.253363.2"
                   }
                }
             ]
          },
          {
             "given":"Susan Irene",
             "family":"Brown",
             "affiliation":[
                {
                   "original_name":"University of Guelph, Canada",
                   "normalized_name":"University of Guelph",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01r7awg59",
                      "GRID":"grid.34429.38"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Cummings",
             "affiliation":[
                {
                   "original_name":"Newcastle University, United Kingdom",
                   "normalized_name":"University of Newcastle Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00eae9z71",
                      "GRID":"grid.266842.c"
                   }
                }
             ]
          },
          {
             "given":"Kimberly",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"University of Guelph, Canada",
                   "normalized_name":"University of Guelph",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01r7awg59",
                      "GRID":"grid.34429.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "repositories",
          "cultural and/or institutional infrastructure",
          "sustainability and preservation",
          "digital ecologies and critical infrastructure studies",
          "ontologies",
          "historical studies",
          "cultural studies",
          "english studies",
          "archives",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Thanks to recent advances in scanning technologies there has been an increase in the number of methods developed for digitizing cultural heritage objects. Many of the resulting 3D models are used for visualization or archiving purposes. Unfortunately, there are still few projects oriented to gain archaeological knowledge from point clouds and triangular meshes. In this paper we present some results of an ongoing project that applies machine learning and computer vision techniques for recognizing, retrieving and classifying cultural heritage objects in an automatic way (Jiménez-Badillo, et al. 2010, 2013; Jiménez-Badillo and Román-Rangel, 2016, 2017; Roman Rangel and Jiménez-Badillo, 2015, Román-Rangel et al., 2014, 2016a, 2016b; Jiménez-Badillo and Ruiz-Correa, 2017). The presentation focuses specifically on a method to analyze style variations of archaeological artefacts with minimal human intervention. This is based on a 3D morphing algorithm proposed by Shelton (2000). Our implementation allows analyzing pairs of objects whose shapes represent the canonical extremes of a continuum, that is, objects that belong to two different “styles” within a cultural tradition. The purpose of the algorithm is taking two extreme shapes (i.e. 3D point-clouds, surface meshes or 3D digital models) as input in order to extract several 3D virtual models whose shape or “style” lies “in-between” the two extremes. This is useful in situations where archaeologists need to decide to which extreme a real artefact is more similar. Archaeologists can also apply the algorithm to compare each object of a collection against all the other members of the set. This would produce an “atlas” of the shape variations expected for such collection, which in turns would facilitate the application of a classification method based on machine learning. The formal mathematical details of this approach can be found in the original paper by Shelton (2000). During the presentation, we plan to offer an intuitive explanation of the algorithm for the benefit of those Humanists who are not experts in mathematics. This can be summarized as follows:  Mathematically speaking, the problem consists in finding correspondences between two 3D point-clouds or surface meshes. This means finding points in surface “A” that match corresponding points in surface “B” with minimal user intervention. The challenge is how to make that the algorithm recognizes meaningful geometric correspondences between models “A” and “B”. In other words, how to find structural geometric correspondences between the points that define, for example, the nose of model “A” with the nose of model “B”, and the same for all other features of the masks. The solution proposed by Shelton is a mathematical equation that fits three criteria:    Similarity. For each point a on surface A, the function  C(a) must find a point close or on the surface B.   Structure. Function C must produce the least possible distortion in the transition from A to B. In other words, the result of function C(a) must have a geometric structure similar to A.   Plausibility. Function C must represent a realistic model derived from surface A.  The first property establishes that C must find real points in A that match points in B. The second condition establishes that the correspondences found between A and B must not be arbitrary. On the contrary, there must be matching substructures of A present in B (e.g. the matching of the nose in A must have some correspondence with the nose in B), so that the deformation makes sense (figure 1). The last condition guarantees that the deformation includes the previous knowledge of the user in terms of which forms are acceptable for the deformation, because it makes no sense to transform a face mask into an airplane, for example. The idea for this project came from the need to rank shape similarities in a collection of archaeological stone masks from Mexico. This includes masks belonging to several well-defined styles, but it also includes many others that cannot be clearly positioned within a specific class because they share features of two or more canonic styles (figure 2).  These masks were found in the Sacred Precinct of Tenochtitlan, the main ceremonial Aztec complex, located in Mexico City. The schematic features of these objects set them apart from other artifacts with more naturalistic style. This has attracted the attention of many specialists and during the last three decades these items have been the subject of intense debate for two main reasons: First, the 162 masks were located in 14 Aztec offerings dating from 1390 to 1469 A.C., yet they do not show typical “Aztec” features. Indeed, their appearance resembles artifacts from the southern State of Guerrero, particularly from the Mezcala region, which is hundreds of kilometers away from the ancient Tenochtitlan.  Such origin would not be rare, as it was common for the Aztecs to import goods from other regions either by trade or by extracting tribute from conquered towns. The style of the masks and figurines, however, is more difficult to explain. It is similar, if not identical, to the style of objects produced in Mezcala and other places of Guerrero during much earlier times, probably during Classic (200 to 1000 A.C.) or even Preclassic times (2000 B.C. to 200 A.C.), while the offerings are Late Postclassic contexts. This leads to the question: Did the Aztecs collected “antique” objects to re-use them in their own offerings?, or the Guerrero/Mezcala styles survived till the late Postclassic period and therefore the offering objects were produced during Aztecs times? It is worth noticing that before the Aztec offering findings very few Mezcala style artifacts had been found in Postclassic contexts. Unfortunately, not enough stratigraphc information is available for collections from Guerrero, so specialists rely purely on stylistic considerations to explain the chronology of these artifacts. Second, it is not clear how many Guerrero/Mezcala styles exist. Some specialists believe there are at least five different traditions (Covarrubias, 1948, 1961; Olmedo and González, 1986; González and Olmedo, 1990), while others recognize only four (Gay, 1967) or two (Serra Puche, 1975). The diversity of views is due in part to a lack of contextual information available for the majority of artifacts found in Guerrero, but it also reflects the subjective criteria used to classify such artifacts.  Clearly, more objective methods are needed to answer questions such as: how many styles were developed in the Guerrero/Mezcala regions?; which specific styles are represented among the offering objects found in the Sacred Precinct of Tenochtitlan?; and more importantly for the purposes of this paper: To which style each mask belongs? Previous studies have tried to solve some of these questions by analyzing object shapes with clustering methods (Olmedo and González, 1986 , González and Olmedo, 1990, Jiménez-Badillo and Ruiz-Correa, 2017), but we believe that the application of morphing algorithms could produce a more objective assessment to solve the problem of style attribution in this and other archaeological collections.  Our application takes examples of two canonical styles and applies the deformation algorithm in order to produce a hundred virtual 3D models whose shapes go from one to the other extreme (figure 3). The virtual models produced in this way represent intermediate steps from style “A” to “B”. Each virtual model has associated a number that indicates its degree of similarity to style “A” or “B”. We can then examine a real archaeological object “c” to determine if its shape is closest to “A” or “B” and by how much. During the presentation we demonstrate a piece of software that implements the morphing algorithm and show, in a visual way, which parts of a 3D model suffer more deformation while transitioning from style “A” to “B” (figure 4). As this is a work in progress, we are interested in receiving feedback from the audience about the relevance of our tools to resolve similar or new archaeological questions and welcome collaboration with other research projects willing to try this generic software for new applications.    Figure 1. The morphing algorithm proposed by Shelton (2000) seeks to generate a sequence of intermediate virtual models from “A” to “B”. To do that, it needs to identify correspondences in geometric substructures (i.e. noses, mouths, etc.) in both models     Figure 2. The first three columns from left to right show nine masks belonging to three different styles from Guerrero, Mexico. The fourth column on the extreme right shows three masks that cannot easily be attributed to the Sultepec, Chonatl or Mezcala styles    Figure 3. A sequence of 3D virtual models produced with Shelton´s algorithm (Shelton, 2000). Notice that each model represents a transition between the shape of mask “A” and the shape of masks “B”    Figure. 4. A snapshot of the morphing software implemented as part of the research project ",
       "article_title":"A machine learning methodology to analyze 3D digital models of cultural heritage objects",
       "authors":[
          {
             "given":"Diego",
             "family":"Jimenez-Badillo",
             "affiliation":[
                {
                   "original_name":"Instituto Nacional de Antropología e Historia, Mexico",
                   "normalized_name":"National Institute of Anthropology and History",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/0509e3289",
                      "GRID":"grid.462439.e"
                   }
                }
             ]
          },
          {
             "given":"Salvador",
             "family":"Ruiz-Correa",
             "affiliation":[
                {
                   "original_name":"Instituto Potosino de Investigación en Ciencia y Tecnología",
                   "normalized_name":"Institute for Scientific and Technological Research",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/03sbzv212",
                      "GRID":"grid.419262.a"
                   }
                }
             ]
          },
          {
             "given":"Mario",
             "family":"Canul-Ku",
             "affiliation":[
                {
                   "original_name":"Centro de Investigación en Matemáticas, A.C. Guanajuato",
                   "normalized_name":"Mathematics Research Center",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/02nhmp827",
                      "GRID":"grid.454267.6"
                   }
                }
             ]
          },
          {
             "given":"Rogelio",
             "family":"Hasimoto",
             "affiliation":[
                {
                   "original_name":"Centro de Investigación en Matemáticas, A.C. Guanajuato",
                   "normalized_name":"Mathematics Research Center",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/02nhmp827",
                      "GRID":"grid.454267.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "archaeology",
          "computer science",
          "repositories",
          "sustainability and preservation",
          "art and art history",
          "databases & dbms",
          "artificial intelligence and machine learning",
          "resource creation",
          "English",
          "archives",
          "and discovery",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Authorship attribution, the analysis of a document’s contents to determine its author, is an important issue in the digital humanities. An accurate answer to this question is important, as not only do scholars rely on this type of analysis, but they are also used, for example, to help settle real disputes in the court system (Solan, 2012). It is thus important both to have analyses that are as accuracy, and to know what the expected accuracy levels are.  In keeping with good forensic practice, scholars such as Juola (2015) have proposed formal protocols for addressing authorship questions such as “were these two documents written by the same person?” Juola (2015) described a simple and understandable protocol based on a relatively small number of distractor authors, multiple independent analyses (e.g, separate analyses based on character n-grams, on word lengths, and on distributions of function words), and a data fusion step based on the assumption that the analyses were biased towards giving correct answers. Juola (2016) proposed minor revisions using Fisher’s exact test to formalize the probability of a spurious match. The revised protocol has been formalized into a software-as-a-service product called Envelope to provide a standard (and low cost) authorship verification service. We reimplemented Juola’s (2016) protocol on a corpus of blog posts to determine whether, in fact, the protocol yields acceptable accuracy rates. Our reimplementation used the JGAAP open-source software package, an ad-hoc distractor set of ten authors (plus the author of interest), and the five analyses listed in Juola (2016): Vocabulary overlap, word lengths, character 4-grams, 50 MFW, and punctuation. Blog data was taken from the Blog Authorship Corpus [Schler et al. (2006)] a collection of collected roughly 140 million words of blog text from 20,000 bloggers collected in August 2004. From this collection, we gathered 4000 examples of authors who had written 300 or more sentences. Ten of these authors were reserved, following Juola (2015;2016) as fixed distractor authors, while the others were randomly paired to create wrong-author test sets. To test same-author accuracy, the first hundred sentences of each of the remaining 3990 blogs were used as “known documents” in the Envelope protocol, while the last hundred sentences of that author were used as “unknown documents.” Perhaps obviously, the correct answer for these tests is that the documents should verify as the same author. To test different-author accuracy, the first hundred sentences of every author in the set was used as a “known document” and compared to the last hundred sentences of the other, paired, author. This procedure generated nearly four thousand test cases of both same and different authors. Each test case was analyzed five times and the rank sum of the known document within the eleven candidate authors calculated as an overall similarity measure from 5..55. This was converted to a  p-value using Fisher’s exact test.  Juola (2016) recommends a seven-point evaluative scale, as follows:  p < 0.05 (Strong indications of same authorship) p < 0.10 p < 0.20 p < 0.80 (Inconclusive) p < 0.90 p < 0.95 p >= 0.95 (Strong indications of different authorship)  The results of these experiments are presented in table 1. The final column indicates the odds ratio; the likelihood that any particular finding at that level corresponds to an actual correct author.   p-value Same Author Different author Odds   < 0.05 2948 748 3.941   < 0.10 246 359 0.686   < 0.20 195 396 0.492   < 0.80 409 1390 0.294   < 0.90 54 234 0.231   < 0.95 47 230 0.204   > 0.95 91 663 0.137   These results show that, in the same-author case, the proposed protocol is very good at identifying same-authors; roughly 3/4 of the actual same-author cases tested at the 0.05 level or better. Because of this, any result less stringent than “strong indications of same authorship” is actually evidence  against same-authorship. The different-author case is more problematic; in theory, if there is no relationship between the known and questioned documents, the p-value should be uniformly distributed, representing a variety of chance relationships. However, the 0.20 < p < 0.80 range (“inconclusive”) contains 60% of the probability space, but only 1390/3990 = 35% of the different-author analyses. By contrast, the 0 < p < 0.05 contains 19% of the analyses, while 0.95 < p < 1.00 contains 17% of the different-author analyses. The observed distribution is thus highly weighted to the extremes of the probability space.  These results indicate that the underlying independence assumptions -- that (e.g.) similarity measured by analysis of word lengths is independent of similarity derived from the most common (function) words -- are not held generally. If a set of genuinely independent analyses could be found, the accuracy of this protocol would be greatly enhanced. Assuming the same distribution for the same author case, the odds ratio for the “strongly indications of same authorship” would be closer to 15:1 rather than 4:1. Nevertheless, these results do show that, suitably interpreted, Juola’s proposed protocol yields accurate results in a high proportion of test cases. We continue to work both on the development of a better analysis suite (with better independence properties) as well as continuing to replicate this experiment to obtain more accurate estimates. ",
       "article_title":"Large-Scale Accuracy Benchmark Results for Juola's Authorship Verification Protocols",
       "authors":[
          {
             "given":"Patrick",
             "family":"Juola",
             "affiliation":[
                {
                   "original_name":"Duquesne University, United States of America",
                   "normalized_name":"Duquesne University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02336z538",
                      "GRID":"grid.255272.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "artificial intelligence and machine learning",
          "linguistics",
          "stylistics and stylometry",
          "library & information science",
          "English",
          "authorship attribution / authority",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Context  One of the bigger problems in comparing historic Dutch texts is wildly differing spelling of the same word. Seventeenth century Dutch did not have standardized spelling. Many spelling variants of the same word coexisted, making it very difficult to use any language processing tools on such texts because they depend on the same word being spelled the same way. So, for example basic algorithms like named entity recognition to recognize place or personal names, or even just part-of-speech tagging to find the grammatical context of words to analyze, for example, changing meanings of words of phrases work less well on older texts. Other languages, of course, have the same problem.   The Dutch digital research platform  Nederlab aims to provide researchers with as many current and historic Dutch text and a toolset to do research on them. As such, spelling normalization would be an important addition to their tools. This project is a collaboration between the CREATE-project of the University of Amsterdam and  Nederlab to tackle that problem. To deal with the problem, rather than developing a tool from scratch, we chose to adapt an existing tool to this situation: VARD2.   VARD2  VARD2   http://ucrel.lancs.ac.uk/vard    Baron, A. and Rayson, P. (2008). VARD 2:  A tool for dealing with spelling variation in historical corpora. Proceedings of the Postgraduate Conference in Corpus Linguistics, Aston University, Birmingham, UK, 22 May 2008.   (an acronym of VARiant Detector) is a Java tool developed by Alistair Baron. It uses two lists (a normalized word list and a variant list) to suggest or replace variant words with their normalized counterparts. The normalization suggestions using a combination of four different methods: 1. known variant replacements; 2. character edit distance; 3. letter rules and 4. phonetic distance. Not all of these were useful for Dutch: the phonetic matching algorithm for example is based on English phonemes and hence did not work on these texts, but the re-spelling rules and the known word replacements worked very well.    VARD2 was designed to normalize Early Modern English, but is modifiable for other languages with a custom configuration. To create a configuration we used the modifiable parts of VARD2: the letter rules, the variant list and the normalized word list.  Corpus We used the 1657 edition of the Dutch translation of the bible as a training set. Not only because there was a modernized version of it available that stuck rather closely to the original word order, but also because it would make it possible to later include another edition of the same book printed in 1637 to easily find more spelling variants for the words we had manually respelled or checked in the 1637 edition. We were able to make a golden standard of modernized spelling for the books Genesis and Exodus. Choices We chose to only do orthographic respelling, in order to preserve grammatical relevant elements of the texts as those may be relevant to research using natural language processing. One problem were words that did not follow Dutch re-spelling rules or did not have a clear Dutch respelling: foreign words, particularly place names and personal names, We chose to ignore such words as they would taint re-spelling rules for Dutch. Problems & solutions  The first problem we encountered was the lack of any usable existing word list of all possible conjugations in modern Dutch. To get as many possible conjugations of every Dutch word that occurs in the  Woordenboek der Nederlandse Taal   http://wnt.inl.nl   (WNT) a two-pronged approach was necessary. A set of algorithms, one per word class provided possible conjugations for each word in the  WNT. First approach: for some word classes we were able to check the conjugations manually, but the large numbers of nomina and verbs made that impossible to do in this project. Second approach: for those the resulting word lists were checked automatically against the occurrences of those words in the  Corpus of Spoken Dutch 1,  Dutch Wikipedia 2 and  Verbix 3 ,.   Another problem, there was no set of respelling rules available that was effective for respelling Early Modern Dutch - the rule sets available did correct some spellings but caused mistakes in others. Extracting re-spelling rules from patterns in our golden standard provided an effective set of rules, especially when we generalized the rules where possible to catch similar instances. Third, VARD2 could not handle word variations where two words should be re-spelled to a single word. Our solution was to pre-process texts with a script to remove spaces from such words.  The fourth problem was that some homonyms had overlapping spelling variations but needed to be re-spelled to different spellings in modern Dutch. An example is the word 'nog': spelling variations 'nog' and 'noch' were used interchangeably, but in modern spelling those two spellings denote differences in meaning. The only way to determine the correct modernization is to take the grammatical context of the word into account, which VARD2 does not do. This necessitated a second pre-processing step: we were only able to run a few tests, but part of speech tagging the original text and (manually) selecting a few patterns that marked one meaning or the other seemed to provide enough information to deduce the correct re-spelling.  Results  All in all, with a few additions and modifications a tool like VARD2 can be successfully converted to work on a Early Modern Dutch. Tests on other types of texts (a treatise on mathematics from 1605, the description of a beached whale from 1599, a description of the New World from 1770, a poetry book from 1637 etc) show promising results, indicating that a little extra training can make this configuration work well for different genres. Automatic respelling of the entire 1657 bible at a 95% confidence level resulted in automatic re-spelling of 62% of 340,000 variants. For the earlier edition (1637), automatically correcting at 95% confidence corrects 60% of just short of 350.000 unknown words, at 75% confidence 84% of the variants were corrected. The paper will show the results of automatically re-spelling 17 th century texts using a VARD2 trained on just the first two chapters of the bible.   ",
       "article_title":"Adapting a Spelling Normalization Tool Designed for English to 17th Century Dutch",
       "authors":[
          {
             "given":"Ivan",
             "family":"Kisjes",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Wijckmans",
             "family":"Tessa",
             "affiliation":[
                {
                   "original_name":"Huygens ING/Nederlab",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "philology",
          "artificial intelligence and machine learning",
          "linguistics",
          "natural language processing",
          "historical studies",
          "English",
          "content analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Japanese books in the Edo period (1603-1868) were mainly published by woodblock print. Their caligraphic writing style using differnt characters prevents native Japanese people to read and understand the content, and the knowledge of the past has been buried in libraries. To change this situation, NIJL-NW project started a ten-year mass digitization program to create the open dataset of 300,000 old Japanese books [7]. To take advantage of emerging big data of Japanese culture, we are working on the development of “deep access” technology to make the content of books accessible by structuring the content by either manually or automatically.  This paper focuses on a series of old Japanse books called “Bukan” [6]. Bukan offers the directory of families of the state king (Daimyo) and bureaucrats of the central government (Bakufu) in the Edo period. Bukan has a unique history. It had been a best seller book for as long as 100 to 200 years, had been updated and published frequently with a peak frequency of a few times in a month, and had been the battle field of two commercial publishers competing each other to improve the quality of their own Bukan editions. Because of good coverage and quality of Bukan, the comprehensive analysis of Bukan is expected to improve our understanding on the political, administrative, and cultural structure in the Edo period.  Comprehensive analysis cannot be achieved, however, without a solution to the problem of multiple versions. Bukan had been published for a long period with high frequency, and it is not known how many versions had been published, or how to decide the proper ordering of existing versions. Moreover, the complete transcription of Bukan is not realistic due to a large amount of text across multipe versions. In short, two major problems, management of versions and reduction of transcription, need to be solved for comprehensive analysis of Bukan   Method  We first propose the concept of “differential reading,” which refers to the mode of reading books, such as close reading and distant reading. It is a reading focusing only on changes between different versions with support from digital tools. Algorithms to detect changes in different versions are two-fold; namely text-based and image-based approaches.  Text-based change detection is effective for manuscripts. Many tools, such as CollateX [2] and ViTA [9], have been developed for text comparison, or Versioning Machine [8], for structured text or TEI (Text Encoding Initiative). In the case of woodblock print, however, image-based change detection has a number of advantages. In the terminology of old Japanese bibliography, versions can be further classified into “publication” and “correction,” where the former refers to the complete re-creation of the woodblock, while the latter refers to the application of small patches to the woodblock. Change detection on publication is an easy problem for image processing, and change detection on correction is also feasible by image matching because only a small part is corrected and other parts remain the same. Other advantages of image-based change detection include transcription-less change detection and non-textual change detection.  By taking advantage of image-based change detection, we formulate differential reading as a two-step process; namely machines work first to detect changes, and humans work next to read changes.                     Figure 1: Comparison of two different versions of Bukan. Left: Kansei Bukan (1789); middle: Kansei Bukan (1791); right; the result of change detection, where red color represents regions present only on the 1789 version, and blue, the 1791 version.   Results An image-based change detection algorithm was implemented on image processing library OpenCV 2.4 with a combination of algorithms such as FAST for feature detection, BRIEF for feature description, and Hamming distance for feature matching. In addition, RANSAC was used for estimating homography matrix for matching two images. Changes are then emphasized using a coloring scheme by assigning red and blue for large difference in pixel values and white for small difference in pixel values. We compared two different versions of Bukan, Kansei Bukan (1789) [3] and Kansei Bukan (1791) [4] to check if the image-based change detection algorithm can identify changes between versions two years apart. Figure 1 shows the result of image-based change detection. It is clear that a part of the page, such as the genealogy of the family, has been changed from the 1789 version to the 1791 version. In the workflow of differential transcription, machine gernerated change infromation will be transferred to planned differential reading interface so that humans can focus only on a part of the image.  Differential transcription needs base transcription, on which transcription of subsequent versions depend. Initially the database of “Bukan Complete Collection” [1] uses Kansei Bukan (1789) as the base transcription. The database not only contains basic information about Daimyo, but also offers visualization about “Sankin Kotai,” which is a required travel for Daimyo between their states and Edo city to meet Shogun (the national leader) in every two years or more often. Animated visualization in Figure 2 shows spatio-temporal and seasonal patterns of their trips coordinated by Bakufu. The database also offers the graphic design collection of Daimyo, such as family emblem, costumes, and tools they used for official activities. We found one important missing element in creating the database; namely the standard ID system agreed within the community. Bukan is a collection of entities, such as people and political organization that changes over time. To uniquely identify entities appearing in different sources and to create a time-series database of linked entities, we need the standard ID system in the Edo period through collaboration with historians. With a proper ID system, this system may evolve into the information infrastructure of people and political entities for the historical studies of the Edo period.               Figure 2: Bukan Complete Collection website. Left: the list of Daimyo family emblems; right: animated visualization of spatio-temporal patterns of Daimyo trips. Only Japanese website is available at this moment    Discussion and Conclusion The advantage of differential reading is two-fold. First, when reading two similar versions, differential reading has advantage over close reading by reducing the burden of human attention. A traditional approach of side-by-side comparison is error-prone, and machines can be optimized for pixel-level comparison without loss of attention by fatigue. For this type of task, human-machine collaboration should evolve into a combination that machines are in charge of low-level change detection while humans are in charge of high-level interpretation. Second, differential reading can be used as a component for differential transcription. The base transcription is required in any case, but the amount of transcription for subsequent versions is significantly reduced. A version management system may play an important role to optimize the transcription workflow, which is left for future work. A proposed approach of differential transcription by human-machine collaboration is not only effective for Bukan, but also applicable to other woodblock print books with different versions. Our tools have been developed on IIIF (International Image Interoperability Framework), which allows us to apply our tools not only on NIJL-NW datasets but other datasets in the same manner. In the future, we plan to make a user interface on top of our IIIF Curation Viewer [5] and combine it with a workflow management tool to support efficient work of transcriptors.   ",
       "article_title":"Differential Reading by Image-based Change Detection and Prospect for Human-Machine Collaboration for Differential Transcription",
       "authors":[
          {
             "given":"Asanobu",
             "family":"Kitamoto",
             "affiliation":[
                {
                   "original_name":"Center for Open Data in the Humanities, Joint Support-Center for Data Science Research, Research Organization of Information and Systems; National Institute of Informatics",
                   "normalized_name":"National Institute of Informatics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/04ksd4g47",
                      "GRID":"grid.250343.3"
                   }
                }
             ]
          },
          {
             "given":"Hiroshi",
             "family":"Horii",
             "affiliation":[
                {
                   "original_name":"AMANE LLC.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Misato",
             "family":"Horii",
             "affiliation":[
                {
                   "original_name":"AMANE LLC.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Chikahiko",
             "family":"Suzuki",
             "affiliation":[
                {
                   "original_name":"Center for Open Data in the Humanities, Joint Support-Center for Data Science Research, Research Organization of Information and Systems",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Kazuaki",
             "family":"Yamamoto",
             "affiliation":[
                {
                   "original_name":"National Institute of Japanese Literature",
                   "normalized_name":"National Institute of Japanese Literature",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/01464wm64",
                      "GRID":"grid.471866.a"
                   }
                }
             ]
          },
          {
             "given":"Kumiko",
             "family":"Fujizane",
             "affiliation":[
                {
                   "original_name":"Notre Dame Seishin University",
                   "normalized_name":"Notre Dame Seishin University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/04t4jzh38",
                      "GRID":"grid.412289.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "historical studies",
          "image processing",
          "content analysis",
          "English",
          "asian studies",
          "encoding - theory and practice"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The history and context of the development of Digital Humanities in Russia as outlined in this paper shows that there are various influences at play which have led to the forming of the Russian DH field. We link the quantitative methods used to previous trends in scholarship, including mathematics, Russian editorial practices, and the development of museum computing in the country. By doing so we can consider the individual societal contexts which encourage a field to emerge, and although that field may look similar to outsiders, identify the lineage of intellectual approaches which still influence methods and cultures within the discipline.  The connection between Russian Formalism and the Digital Humanities (Allison et al., 2011; Moretti, 2013; Jockers, 2013; Stanford University, 2015) relates to the tradition that originated following the strengthening of Russian mathematics at the turn of the nineteenth century after the Moscow Mathematical Society was established in 1864. The influence of this school on literary studies can be traced through the twentieth century from Andrey Bely’s experiments at the threshold of mathematics and poetry (Akimova, Shapir, 2006; Giansiracusa and Vasilieva, 2017) to the Moscow Linguistic Circle with Roman Jacobson as its chair (Akimova, Shapir, 2006; Pil’shchikov, 2015), to the Prague Linguistic Circle and further to the Tartu-Moscow School (Uspensky, 1998). Boris Jarkho’s ‘Research Methods for Literary Studies’ written in 1936 anticipated the approach of Stanford Literary Lab not only in its ‘quantitative interpreting’ (Underwood, 2017) but also in a skill of a scholar able to see wider contexts and make bridges across disciplines. The traditions are currently developed at the Centre for Digital Humanities at the Higher School of Economics in Moscow via digital tools (Skorinkin, 2017; Bonch-Osmolovskaya and Skorinkin, 2016; Orekhov and Tolstoy, 2017; Kuzmenko and Orekhov, 2016; Fischer et al, 2017). Another tradition related to building the National Corpus of the Russian Language  The National Corpus of the Russian Language (http://www.ruscorpora.ru) includes over 600 million words. It was published online in 2004 and developed by the linguists from the Russian Academy of Sciences (Sitchinawa, 2006).  can be traced back to Alexei Lyapunov (Sitchinawa, 2006), another famous Russian mathematician. The point here is not that mathematics sustained and influenced all the Russian humanities (Bakhtin’s famous studies can provide an opposite example  See, for example (Gasparov, 2002; Sedakova, 1992), for the discussion of the difference between Bakhtin and the Russian Formalism. ) or that quantitative approach as a trendy international methodology was also present in this part of the world in the 1960s-1970s but that it provided the rigor and method to the field which was disconnected from the international research methods and standards. This disconnection resulted in a dramatic difference in academic cultures.  A recent paper (Underwood 2017) discusses distant reading as a part of the digital humanities project aimed at coping with confirmation bias. Underwood shows that (social) sciences provide the ‘experimental structure’ and help us build research design around hypothesis, samples and results. A specific Russian feature was that research methodology of this type was provided via mathematics, linguistics, and sciences. Social science and anthropology played a minor role in the interplay of influences (Gasparov, 2016). A major part of the current Russian digital humanities project is connected to linguistics. However, linguistics did not only provide a set of formal features and a methodology to trace a formal technique in a literary work. It was an important initial influence, a novel method to do literary studies as a part of a new scientific perspective (Tynjanov, 1971; Jarkho, 2006) in the early twentieth century. The Moscow Linguistic Circle active from 1915 to 1924 held its meetings in Roman Jacobson’s flat in Moscow and its members were over 60 linguists and scholars working in text analysis and literary studies  Tynaianov and Schklovsky, famous for their contribution to Russian Formalism, were members of the Moscow Linguistic Circle (Shapir, 1996). . Apart from its significant international influence, the society had an important impact on how Russian scholarship developed (Akimova, Shapir, 2006; Shapir, 1996; Pil’shchikov, 2015). Its traditions were continued in applying quantitative methods to studying poetry in the second half of the twentieth century (Akimova, Shapir, 2006; Bodrova, 2017). Its influence can be traced in a highly influential approach of applying structural linguistics to interdisciplinary cultural studies at Tartu University  Tartu University in Estonia, a part of Russia at that time, was home for the literary studies done in the tradition of the methodology looking at formal structural features.  also in the second half of the twentieth century (Gasparov, 2016).  A part of current projects in Russian digital humanities are connected to this tradition. The project of creating a semantic edition of Leo Tolstoy’s complete works  A project that is currently developed at the Higher School of Economics and Leo Tolstoy museum. Apart from using a representational mark-up in TEI standards, the project includes experiments towards an interpretive component (Bonch-Osmolovskaya, 2016; Bonch-Osmolovskaya and Skorinkin, 2016). (Bonch-Osmolovskaya, 2016) includes representative and interpretive components. The edition’s interpretive part works with a humanistic data model of the characters’ roles in  War and Peace validated through the digital tools of natural language processing and extracting semantic roles (Bonch-Osmolovskaya and Skorinkin, 2016), this approach also includes a classification of characters using character networks (Skorinkin, 2017). The connection of digital approaches to the previous trends of scholarship (Russian Formalism and structural interdisciplinary studies initiated by scholars from Tartu and Moscow) is explicitly proposed and maintained through the Moscow-Tartu Summer School annually organized at the Higher School of Economics in Moscow.  Quantitative approaches to studying poetry has been a path traditionally pursued by Russian mathematicians or people related to mathematics. Andrey Bely who was closely related to Nikolai Bugaev  Boris Bugaev’s (Andrey Bely’s) relations with his father and the influence of the academic environment on Bely’s development have been widely discussed in literature (see, for example, Janecek, 2015 and Giansiracusa and Vasilieva, 2017)., one of the first chairs of the Moscow Mathematical Society, developed a quantitative approach to studying poetic rhythm in 1910 and initiated a society where scholars were taught to use statistics to study poetry (Semyonov, 2009). Andrei Kolmogorov, a famous Russian mathematician, organized a seminar and published several papers in this field in the early 1960s (Semeyonov, 2009; Kolmogorov, 2015).  The tradition has been continued via digital tools where the authors show the limitations of digital analysis (Orekhov, 2014) or integrate mapping poetry in interdisciplinary cultural studies following the Tartu tradition (Kuzmenko and Orekhov, 2016).  Russian editorial practices in the second half of the twentieth century were focused on publishing complete works of the authors from the canon of the time. Thorough editorial work was limited by the editors’ attempts to combine international standards of scholarly apparatus and the requirements of the moment. Twentieth century’s attempts to create scholarly editions using interpretive practices of the time (Bonch-Osmolovskaya, 2016) resulted in a current need to build new epistemological foundations for contemporary scholarly editions. Digital methods and digital scholarly standards are probably the best possible option to cope with epistemological difficulties in the field.  While editing textual materials was complicated by interpretive practices, visual editions in the 1970s, 1980s and early 1990s were introducing new standards of metadata and data models. Their editors made an important step towards digital practices and museum computing. The editorial practices of printed visual editions of artworks related to the standards of publishing museum images (Kizhner et al, forthcoming), the quality of images and the scholarly apparatus accompanying visual editions in the 1970s and 1980s prepared the anticipations of standards for digital publishing and placing images in a wider context via digital tools (Polulyakh, 2009; Sher, 2006).  A specific Russian feature was looking for formal (structural) components to interpret a literary work, bringing a wide interdisciplinary context to interpretation. The tradition was sustained during the twentieth century before Russian scholars turned to digital humanities. The influence of social science, gender and race studies, enlarging or changing a canon did not leave significant traces even if (when) the ideas reached the community of scholars. A current exception are projects aimed at studying the nineteenth century literary canon and future developments seeking to compare it with contemporary canons (Vdovin and Leibov, 2013). The authors propose to build a canonical corpus and study the changes using a mark-up. The idea relates to Moretty’s evolutionary theories (ibid) and the Russian traditions of observing the dynamics of a formal feature that can be traced back to Boris Jarcho’s papers written in the 1930s.  The paper will demonstrate, using evidence from various sources that Russian traditions of quantitative interpreting, the influence of strong mathematics and a trend of placing cultural objects within a broader context were crucial for our understanding of how digital humanities, as a quantitative methodology, developed in the country, in a different way than it did elsewhere. Understanding these alternative histories will help us understand the range of activities taking place in Digital Humanities worldwide, by looking at the social, scholarly, and cultural contexts, helping the community to navigate and bridge differences.  ",
       "article_title":" The History and Context of the Digital Humanities in Russia  ",
       "authors":[
          {
             "given":"Inna",
             "family":"Kizhner",
             "affiliation":[
                {
                   "original_name":"Siberian Federal University, Russian Federation",
                   "normalized_name":"Siberian Federal University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/05fw97k56",
                      "GRID":"grid.412592.9"
                   }
                }
             ]
          },
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":"University of Edinburgh, UK",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          },
          {
             "given":"Lev",
             "family":"Manovich",
             "affiliation":[
                {
                   "original_name":"City University of New York",
                   "normalized_name":"City University of New York",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00453a208",
                      "GRID":"grid.212340.6"
                   }
                }
             ]
          },
          {
             "given":"Boris",
             "family":"Orekhov",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Anastasia",
             "family":"Bonch-Osmolovskaya",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Maxim",
             "family":"Rumyantsev",
             "affiliation":[
                {
                   "original_name":"Siberian Federal University, Russian Federation",
                   "normalized_name":"Siberian Federal University",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/05fw97k56",
                      "GRID":"grid.412592.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "digital humanities history",
          "cultural studies",
          "theory",
          "epistemology",
          "English",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Books written by and marketed towards women have been analyzed mostly in the context of popular culture ( Radway, 1987; Hollows, 2000; Modleski, 2008). In literary criticism however, fictional work by women is regularly held up to such ‘women’s novels’ to measure the quality  (van Boven, 1992; Vogel, 2001; Groos, 2011). This connection made between female author gender and popular feminine novels is likely based on bias, but it is not yet well-researched in computational stylistics. In this paper we present a pilot study for examining this potential bias, through the combination of a reader survey and text analysis.     Related work Although computational stylistics is now quite common in analysis of fiction  (i.e. Semino and Short, 2004), ‘women’s’ genres are not researched often in relation to literature.  Jautze et al. (2013) focuses on differences between the syntactic make-up of sentences in literary novels and so-called ‘chick lit’ (cf. Ferriss and Young, 2013);  Montoro (2012) performs computational-linguistic analysis on chick lit as opposed to a BNC sampler corpus – but not to literary fiction specifically.     Women’s books  What is the relationship between books by women and ‘women’s books’ according to readers? We examine this through results of the National Reader Survey  (2013). Respondents were supplied with a list of 401 recent Dutch-language novels (translated and originally Dutch, published between 2007-2012) that were most often loaned from libraries and bought from bookstores between 2009-2012 (Koolen et al., in preparation).   Note that the Riddle corpus’ novels show the one-sidedness of the market: it consists of few genres, there are very few novels by people of color, it contains mostly European and North-American novels.  ,   The factor of translation will be taken into account in further development of this pilot, for information on effects within the larger project, see van Dalen-Oskam, 2016.   Respondents supplied ratings of literary quality on books they had read (on a scale of 1-7) and were allowed to motivate one of their scores.    Overall, works by female authors are judged to have lower literary quality (M=3.92, SD=0.81) than those by male authors (M=4.73, SD=1.04); t(344)=-8.34, p < 0.01. This is partially caused by romantic novels, which are mainly written by women (M=3.02, SD=0.60).   To distinguish genres, we roughly base ourselves on Dutch publishers’ assignments of genre, which is done through a uniform classification system in the Netherlands.   More surprisingly, within general fiction female authors’ works scores’ (M=4.55, SD=0.84) are significantly lower than for male’s (M=5.53, SD=0.73); t(120)=-7.60, p<0.01.   An analysis of the motivations shows that the concept of the ‘women’s book’ (‘vrouwenboek’) and similar gendered terms are used dozens of times to explain what literary quality is  not; a male equivalent is mentioned twice (‘men’s book’, ‘boy’s book’). Examples of novels referred to as ‘women’s’ book’ are translations of  Eat, Pray, Love by Gilbert (general fiction),  Remember Me? by Kinsella (romantic fiction) and  The Ice Princess by Låckberg (suspense). Thus, works by female authors are equated with ‘women’s books’ regardless of the novel’s own genre. Perceived connections that respondents provide are: bad story (about love), a simple style, no deeper layers, etc.. But how much do ‘women’s books’ differ from novels that are perceived as literary? And are they more strongly connected to other female-authored novels than to male-authored ones?    Text analysis We perform two experiments as a first exploration. We compare present-day romantic novels by female authors (R), predominantly chick lit, to general fiction by women (GF) and general fiction by men (GM). We select the lowest scoring novels in the romantic genre and the highest in the general fiction genre (i.e. the most ‘literary’ ones according to our respondents), to find the clearest contrast (cf. Table 1). We use only one novel per author, unless the author uses a different pen name (Kinsella/Wickham).   Table 1. Division of books in the sub-corpus   Genre / gender author (av. rating literariness)   Transl. from English  Originally Dutch   Romantic / female (2.8) 10 2   General fiction / female (5.2) 10 2   General fiction / male (5.9) 10 2    Experiment 1: style As we have shown, the style of ‘women’s books’ is seen as inferior. We use stylometric analysis to explore this notion, adding Gilbert’s  Eat, Pray, Love to this experiment (cf. Section 3); a hybrid of general fiction and romance. Stylometric analysis is most often used to perform authorship recognition, but has been successfully applied to identify gender  (Rybicki, 2015) and fictional genres (Allison et al., 2011). We apply the method detailed in Eder (2017). First, with R-package Stylo (Eder et al., 2016), we construct a bootstrap consensus tree based on the 100 through 1,000 most frequent words with 100-word intervals, using Classic Delta to calculate stylistic similarity (cf. Eder, 2017). Second, we use network analysis and visualization tool Gephi to visualize the novels’ connectedness (Bastian et al., 2009). Color-codes are based on modularity, which visualizes groupings of greater inner coherence (Blondel et al., 2018). Finally, we apply the ForceAtlas2 algorithm to make groupings more visually distinct.     Network visualization of the novels’ stylistic proximity (R = romantic, GF = general fiction/female author, GM = general fiction/male author). Colors indicate groupings based on modularity  Fig. 1 shows six clusters. Part of the romantic novels (blue, soft pink) are indeed separated from the general fiction (other colors); Stockett’s  The Help is stylistically connected strongest to romantic novels. General fiction by female and male authors hardly form clusters of their own. Except for one ‘male’ cluster which contains a Barnes’ novel and an outlier: Gilbert’s novel – which is seen as a ‘women’s novel’ by our respondents. Weiner, known for opposing the ‘chick lit’ label to her work  (Mead, 2014) has a stronger connection to general fiction. In other words, stylistically seen, part of the romantic novels appear to have a specific signature, but most novels by female authors are not obviously stylistically connected to them.    Experiment 2: sentiment  We now use Linguistic Inquiry and Word Count (LIWC), a word list analysis tool, which has a dictionary for Dutch (Boot et al., 2017) and has been applied to literary fiction in genre analysis (Nichols et al., 2014). LIWC contains a number of content and sentiment-related categories that are of interest. Attention to physical appearance, a (heterosexual) love story, work and friendship and have been identified as themes of chick lit novels  (Gill and Herdieckerhoff, 2006), which are the main component of the romantic genre in this corpus. We report significant differences on salient categories in an independent  t-test between averages of groups (p < 0.01).    Table 2. Significant differences (p < 0.01) between groups   LIWC category   Romantic-Gen. Female   Romantic-Gen. Male   Gen. Female-Gen. Male    Articles   X    Prepositions   X    Affect  X  X    Posemo  X  X    Negemo      Social   X    Communication  X  X    Friends  X  X    Job  X     Swearwords  X     Table 2 shows that romantic novels differ from general fiction in some ways: more positive emotions, but no significant difference in negative emotions, more words pertaining to friendship. The romantic novels differ in other ways from either the female or the male-authored literary novels: there are more job-related words in the romantic novels than in female-authored general fiction; less articles and prepositions than male-authored general fiction. Female-authored literary novels and male-authored ones do not significantly differ on any category. This might indicate that when comparing literary fiction to romantic novels, readers choose to focus on commonalities with female authors and differences with male authors, whereas differences between female authors and commonalities with male authors are overlooked. However, we need to be careful with interpretations of  t-tests in LIWC  (cf. Koolen and van Cranenburgh, 2017). Additional analysis will need to be performed to identify within-group differences. Finally, physicality and the body do not appear to be specific to romantic novels. This finding corroborates earlier research, see Montoro (2012) and  Koolen (2018).     Conclusion Romantic novels appear to be more different from all general fiction than the general fiction differs among authors of female and male gender. They contain signature elements, albeit not all the expected ones (positive emotions and friends, not attention to appearance). Part of the romantic novels are clearly different from general fiction stylistically, but a number of them cluster with male-authored general fiction; most notably work by Gilbert and Weiner. Although further testing is needed, they show that computational stylistic analysis might be used to paint a more objective picture of the actual style of contemporary novels by female authors and the relationships between them. We offer a speculation: if we consider the romantic novels in this corpus to be ‘women’s novels’, there are a several indications that commonalties between female-authored general fiction and romantic novels are stressed heavily and this might be a reason female authors’ novels are judged to have less literary quality. Nevertheless, we do not aim to assert ‘low’ literary quality of the romantic novels, either. To examine gendered quality perceptions further, we will include other fictional genres in future research.  ",
       "article_title":"Women’s Books versus Books by Women",
       "authors":[
          {
             "given":"Corina",
             "family":"Koolen",
             "affiliation":[
                {
                   "original_name":"The Huygens Institute for Netherlands History, Amsterdam",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "graphs",
          "networks",
          "stylistics and stylometry",
          "sociology",
          "feminist studies",
          "text analysis",
          "relationships",
          "English",
          "literary studies",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Recent results of computer-aided research suggest that characters in novels – measured by their character speech – can be laid out stylistically distinct from other characters of the same novel (Hoover, 2017; Fields, Bassist, Roper, 2017). Thus, experienced authors are able to create characters with ‘distinctive voices’ which can be identified by word frequencies. Unlike stylometrically determined signals in respect to author, genre or period, it is then an intratextual criterion for similarity and disparity. The study’s subject is therefore not a large text corpus of different authors and periods, but a single literary text that comes into analytical focus. This approach to text selection is oftentimes called ‘microanalysis’ (Hoover, 2017). The term does not only differ from buzzwords such as ‘big data’, it also emphasizes the differences to concepts such as ‘macroanalysis’ (Jockers, 2013) and ‘distant reading’ (Moretti, 2000; 2005) despite their comparable quantitative techniques. Surprisingly, studies on the stylistic differentiation of character speech are mostly limited to novels even though the structure of dramatic texts makes a quantitative examination of dramatic character speech easier: The speech is neither sorted nor commented nor framed by a narrator. By consequence and in contrast to narrative texts, the character speech can be isolated automatically. Initial approaches are already available: E.g., John Burrows and Hugh Craig show that individual drama characters can indeed be successfully assigned to an author’s signal (Burrows, Craig, 2012). Both argue against critics who question a successful attribution of dramatic texts to an author, as Masten (1997) does who claims that the lack of narrators would lead to many indistinguishable voices.   Distinctive Character Speech in Dramatic Texts?  Figure 1   Figure 1,  2 and  6 were generated using the ‘stylo’ package for R.  Figure 3,  4 and  5 were created using the ‘DramaAnalysis’ package for R (Nils Reiter, Marcus Willand). https://github.com/quadrama/DramaAnalysis. The visualization of  Figure 2 was done in Gephi.   is based on David Hoover’s approach in  The Microanalysis of Style Variation (2017) but is applied to the genre of drama. The hierarchical cluster analysis in  Figure 1 illustrates the various characters of Gotthold Ephraim Lessing’s  Minna von Barnhelm,  oder das Soldatenglück (1767) in regard to their similarity. As one of the plays of “Lessing’s maturity” (Worvill, 2005: 177)  Minna von Barnhelm seems to be an appropriate drama to discuss its characters and their speech. Michael Metzger, e.g., argues that Lessing created “a characteristic pattern of language for each of the various roles he has written” (Metzger, 1966: 196; see also Worvill, 2005; Asmuth, 2009).  The stylometric analysis is based on word frequency lists which are extracted from the individual characters’ utterances. With the help of ‘Cosine Delta’, that is claimed to achieve more reliable results than ‘Burrows’s’ or Argamon’s Delta’ (Evert et al., 2017), the speeches’ relative stylistic similarity is calculated by means of word frequencies. Contrary to Hoover’s approach, the character speech is not divided into artificial segments of 1500 words each but by its ‘naturally given’ act boundaries.   The act boundaries are marked with underscores in the illustration.   This is helpful for the interpretation of the stylometric results based on the conditions of their emergence, such as the co-presence of characters. The procedure’s disadvantages are the speech segments’ inconsistencies: Some segments fall below a length of 700 words and must be excluded.   To compare: Fields, Bassist and Roper use segments of only 200 words each.  It also eliminates the so-called possibility of ‘randomization’, as it is practiced by Hoover: the individual character speeches’ word distribution is randomly assigned to the segments in order to ‘normalize’ outliers. However, one should be cautious regarding the random distribution of words since potentially better results can only be measured by the underlying hypothesis.      Figure 1: Dendrogram of  Minna von Barnhelm, 1000 MFW, no culling, Cosine Delta, Ward Clustering.   Although some speech segments fall below a length of 1000 words, it should still be feasible to use a vector length of 1000 MFW (Eder, 2017b). The results of  Figure 2 support this hypothesis, but a larger scale study on this topic is a future task.     Minna von Barnhelm’s stylometric analysis indicates certain signs of stylistically distinctive character speech: E.g., Tellheim’s speech – he is the male protagonist of the play – from Act 1 and 3 is grouped in immediate vicinity. The same holds true for the speech in Act 3 and 5 taken from Paul Werner. However, most of the speech segments seem to follow a different criterion. This is particularly evident for the uppermost section of the chart: The speeches by Major Tellheim, Minna von Barnhelm, Franziska (Minna’s chambermaid) and the landlord (Wirt) are grouped on a contiguous branch, i.e. they resemble the other segments stylistically. Those four segments of speech belong to the drama’s second act. There are other examples that seem to confirm act boundaries as an important factor for the analysis’ results. The most striking ones are those of Tellheim and Minna in both Act 4 and 5. The analysis shows that the results by Hoover, Fields, Bassist and Roper cannot be transferred to Lessing’s dramatic text directly.  A single dendrogram, however, must not be more than a first indication for the assessment of the hypothesis. To avoid a potential ‘cherry picking’ problem at this point, further stylometric analyses on an expanded corpus were conducted.   I analyzed 13 texts – three by Lessing, four by Friedrich Schiller, three by Johann Wolfgang Goethe and three by Friedrich Hebbel – with a total of 175 speech segments. Parameters used: 1000 MFW, no culling, Cosine Delta, Ward Clustering. The visualization is not shown in the paper.   Both, the author’s signal (175 of 175 segments matching) and the text unity (171 of 175 segments matching) can be clearly identified. Thus, the cluster analysis does not seem to be influenced negatively by the relatively small sizes of the speech segments.  Figure 2, a network plot that uses the same corpus, consolidates this finding.   See Eder (2017a) for advantages of stylometrics visualized by network plots.        Figure 2: Stylometric network of 13 dramas. 500–1500 MFW, no culling, Cosine Delta, three nearest neighbors. Node sizes represent average degree, node colors represent modularity rank.    Co-presence and Character Semantics Stylometrics are not the only method to determine relative similarities within a text corpus. The extent to which they are suitable to discuss open questions – in contrast to, e.g., author attribution – remains to be examined. If parameters such as distance measures, word size or culling must be redefined with respect to the text corpus, ‘cherry picking’ would then become inherent to the method (Schöch, 2014; Jannidis 2014; Eder, 2013). It is therefore necessary to compare the established observations to other quantitative methods. This is done by means of analyzing co-presence and semantics of character speech.  Figure 3 illustrates the speech parts of the six most important characters in Lessing’s  Minna von Barnhelm. The following investigation focuses on the protagonists Tellheim and Minna. In the second, but especially in the fourth and fifth act, Tellheim and Minna are mainly co-present. This structural data correlates with the observations in  Figure 1. The speech segments of those acts are grouped closely together, while Tellheim’s speech in Act 1 and 3 is clearly separated. In these two acts Tellheim and Minna are not co-present.      Figure 3: Co-presence in  Minna von Barnhelm.  The observation that stylistic similarities of the character speech is related to structural characteristics challenges earlier research and demands further investigation: Is it possible to expand or specify this finding? A semantic word field analysis, as used by Willand and Reiter (2017), serves to operationalize the thematic conception of character speeches.   For this purpose, five dictionaries on the topics of family, war, love, ratio and religion were created, enlisting 65 to 110 words each. The words were used in dramas between 1770 and 1830 (Willand, Reiter 2017).  Figure 4 illustrates two diagrams that compare different segments of Tellheim’s and Minna’s speech. The figure on the left compares Tellheim’s speech in Act 1 and 5. It indicates significant semantic differences in those segments that also showed little similarity in terms of style. The themes ‘love’ and ‘ratio’ are given greater weight in Act 5, while the context of ‘family’ is invoked less frequently. All in all, one can clearly detect a discrepancy in the semantic fields’ word frequencies.      Figure 4: Semantic fields in  Minna von Barnhelm.  The diagram on the right shows the semantic fields of Minna and Tellheim in the fifth act. Compared to the diagram on the left, the two speeches of Tellheim and Minna seem to correlate better with each other, especially considering the word fields ‘love’ and ‘ratio’. None of the word fields is conspicuous due to extreme differences. Whether this observation can actually be used as a marker for similar topics or not has to be proofed within a larger text corpus. By consequence, this would be useful to determine a threshold value to mark similarity and disparity. I started this task using the Euclidean distance to measure the similarity between different segments of character speech in  Minna von Barnhelm. It results in the following values of similarity:      Figure 5: *denotes co-presence, yellow colored values are nearest neighbor segments as taken from the stylometric analysis ( Figure 1), lower values display a higher similarity.  Average of the four nearest neighbor segments: 0,008703 Average of Tellheim’s segments (without nearest neighbors): 0,012026667  The difference of the two groups’ average margin is a value of 0,00332, or 38,2 percent. Although the sample size is still small, one dramatic text only, this seems to be quite a significant result. Thus, the word field semantics do at least provide an indication that style, theme and presence of characters are related to some extent.   Conclusion A closer examination of the character speech in  Minna von Barnhelm has shown that it is plausible to combine different analytical methods. Thus, the investigation benefits from their respective strengths. Herein, results can be validated and opened for broader questions. In the chosen dramas, co-presence seems to have an impact not only on style but also on the semantics of character speech. The segments spoken by the two protagonists in Act 5 of  Minna von Barnhelm exemplify this thesis. These results differ from Hoover’s and suggest having a closer look on co-presence and its influence on the distinctiveness of character speeches in dramas as well as in novels. The absence of a narrator in dramatic texts is one possible starting point to explain the differences outlined in this paper.    ",
       "article_title":"Quantitative microanalysis? Different methods of digital drama analysis in comparison.",
       "authors":[
          {
             "given":"Benjamin",
             "family":"Krautter",
             "affiliation":[
                {
                   "original_name":"QuaDramA / University of Stuttgart, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "philology",
          "german studies",
          "stylistics and stylometry",
          "text analysis",
          "English",
          "semantic analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In the frame of the research project  The Sphere. Knowledge System Evolution and the Shared Scientific Identity of Europe we investigate the knowledge tradition that is interwoven with the history of one text: the  Tractatus De Sphaera by Johannes de Sacrobosco. This 13th century treatise on cosmology has been published as part of university textbooks up until the 17th century. We have identified a corpus of more than 300 printed books related to Sacrobosco’s text and obtained digital copies – a process that took three years to complete. These textbooks, which were part of the mandatory curriculum in most European universities at that time, contain Sacrobosco’s text in its original version, as well as in translated, annotated or commented form. In addition, publishers included other texts that were seen as relevant for the study of cosmology from fields such as medicine, astronomy or mathematics (Valleriani, 2017).  Based on this corpus we seek to study how knowledge innovations have proliferated through the dissemination of texts, and identify the structural and social factors that contribute to or hinder the spread of certain kinds of knowledge. We do so by making use of methods from the area of network analysis which we apply on a dataset that we derived from our literary corpus.  This paper presents the foundational work that enables this kind of research with immediate application for similar projects concerned with editorial histories and structural analyses of corpora. We demonstrate the practical application of linked semantic data and the CIDOC-CRM model for shaping and addressing research questions in the humanities (Crofts et al., 2011).   Challenges Our main challenge for this part of the project is the digital representation of the structure of the books and relevant contextual data.  The data model needs to be detailed. Individual texts can be derived from and include other texts. This genealogy of a text needs to be represented. We also require a suitable way of inputting complex data in a user friendly way. We need to be able to query and extend the data in a flexible manner. The data needs to support not only our initial research questions, but also future ones and those by other researchers. We need to be able to maintain an audit trail and trace occurrences that appear as a result of a network analysis to the original source. Last but not least we want to be able to publish our data in an understandable and reusable format. We meet those challenges by modelling our data in adherence to the formal ontology CIDOC-CRM (Crofts et al., 2011) and the FRBRoo extension for bibliographic records (Bekiari et al., 2015), by storing our data in RDF and according to the 5-star deployment scheme for Linked Open Data (Berners-Lee, 2006), and by making use of the Metaphactory (Metaphacts, n.d.) and ResearchSpace platform for semantic data creation (Oldman, 2016). The next challenge is the development of a mathematical model that allows us to analyse the evolution of knowledge innovations – initially based on the textual sources and social structures, and later including other kinds of evidence such as book illustrations, family and business relationships, etc.   Related work Our project builds on previous work in the area of semantic data, specifically CIDOC-CRM, and network analysis for research in the humanities.  Historical research that makes use of network modelling and analysis is increasingly relevant (Renn et al., 2016). A recent example is the establishment of the Journal for Historical Network Analysis (Rollinger et al., 2017). The evolution of scientific ideas in particular lends itself to be studied through networks (Lalli and Wintergrün, 2016) as well as how academic funding structures are of influence (Bellotti, 2012). CIDOC-CRM (Crofts et al., 2011) has been developed and successfully used as a way of reconciling and connecting sources coming from different cultural and technical contexts. Examples include CLAROS (Kurtz et al., 2009), which brings together classical art research databases, PHAROS (Reist et al., 2015), which provides consolidated access to photo archives, or the reconciliation of the Arachne database of the German Archaeological Institute (Krummer, 2006). A RDF implementation of CIDOC-CRM and FRBRoo has been developed at the University of Erlangen (Goerz et al., 2008). The team is also involved in Wiss-Ki (Goerz et al., 2009), along with ResearchSpace (Oldman, 2016) one of few tools that support data creation in CIDOC-CRM compatible RDF (CIDOC/RDF).   Our approach  CorpusTracer To address the outlined challenges we developed CorpusTracer. CorpusTracer is our front-end for creating and querying the dataset (Figure 1). It is a custom configuration of the Metaphactory semantic data platform and relies on modules developed as part of the ResearchSpace initiative. ResearchSpace is a cultural heritage research platform that builds on Metaphactory as a middleware and introduces modules for CIDOC-CRM compatible data creation and access. It allows to write data directly in CIDOC/RDF to a Blazegraph triple store. Crucially, it is possible to harvest the expressivity of CIDOC-CRM while not having to expose users of the tool to its complexity. We will demonstrate the tool, which is available open-source for download and use.     Figure 1. The home screen of CorpusTracer, featuring a search field and recently edited book and person records, with images and biographical data of persons drawn from Wikidata    Data model Our data model (Figure 2) relies on generic concepts defined in CIDOC-CRM and FRBRoo, making it understandable and reusable outside the scope of our project. We have earlier described the model in more detail (Kräutli and Valleriani, 2017). Since then, we have slightly expanded the model to account for more complex derivations of texts, and for illustrations. The FRBRoo approach, which separates the concept of a book into several layers of physical and conceptual abstractions, fits well to the research framework.   FRBR introduces the concepts of Item, the material book, Manifestation, the prototypical book, Expression, the text of a book, and Work, the overall work conveyed by the book.  It allows us to accurately capture the composition of each book: the texts it contains and, for each text, whether it is an original text or how it derives from existing texts.  We employ a strict separation between the data that is based on our corpus and data that provides context, such as biographical details or location data. We achieve this by linking relevant entities to external sources from Wikidata and the CERL thesaurus. Researchers are able to search for and link to resources on Wikidata directly within the CorpusTracer user interface.     Figure 2. A graphical representation of our CIDOC-CRM/FRBRoo data model    Discussion The technical foundation provided through Metaphactory and ResearchSpace allowed us to develop the data model and implement a version of CorpusTracer ready for inputting data within a few months. Our team could then start with inputting the bibliographic data while scholars simultaneously performed the structural analysis of the publications. Changes on both the model and the interface were implemented as we gained a better understanding of the material at hand.  Although we use the platform primarily for data creation, we designed it in a way that will also allow the general public to access and navigate the dataset – which ultimately also benefits expert users. The structured search component of the Metaphacts platform is implemented to allow querying the graph database without having to know the underlying data model (Figure 3). Queries can be made for different entities (books, texts, persons, etc.) and the relationships between them. Data can be downloaded in CSV format on different pages of the interface as well as by using the structured search. In order to extract the network data required for our analysis we however rely on custom SPARQL queries.  To construct the queries a good knowledge of the data model, the SPARQL syntax and the architecture of graph databases is required. While we found the data created through the platform to be reliable, one has to be careful not to introduce errors when querying the data manually. Unlike in relational databases, where one row in a table corresponds to one item of data, the boundaries of individual entities are not strictly defined in the Blazegraph triple store. We often found errors in our own custom queries that produced a higher number of results than we would have expected. Despite the above reservations we find it preferable to not to rely on the graphical interface and CSV downloads to access the data, but to use custom SPARQL queries: for reasons of transparency, for maintaining an audit trail between original and extracted data, and for better reproducibility when the dataset changes.     Figure 3. The Structured Search interface of the Metaphacts platform allows  also non-expert users to formulate complex query on the graph database      Future We have now completed the work on the dataset for the structural analysis of the corpus. The dataset can be accessed and downloaded, along with CorpusTracer, via our website (sphaera.mpiwg-berlin.mpg.de).  We continue to extend the dataset, particularly with regards to other forms of evidence to study exchange of knowledge. CorpusTracer implements an annotation tool which we use to mark illustrations in the digitised pages of the books (Figure 4). By employing an image hashing algorithm we identify shared illustrations across books that indicate relationships between printers.  Currently we are working on a mathematical model that enables us to identify the contributing structural and social factors that lead to the successful proliferation of particular knowledge innovation.     Figure 4. ResearchSpace provides a Mirador IIIF Viewer with annotation functionality,  which we use to mark illustrations within pages of the books    ",
       "article_title":"Digital Modelling of Knowledge Innovations In Sacrobosco's Sphere: A Practical Application Of CIDOC-CRM And Linked Open Data With CorpusTracer",
       "authors":[
          {
             "given":"Florian",
             "family":"Kräutli",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Matteo",
             "family":"Valleriani",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Esther",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Christoph",
             "family":"Sander",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Dirk",
             "family":"Wintergrün",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Sabine",
             "family":"Bertram",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Gesa",
             "family":"Funke",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Chantal",
             "family":"Wahbi",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Manon",
             "family":"Gumpert",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Victoria",
             "family":"Beyer",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Nana",
             "family":"Citron",
             "affiliation":[
                {
                   "original_name":"Max Planck Institute for the History of Science, Germany",
                   "normalized_name":"Max Planck Institute for the History of Science",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0492sjc74",
                      "GRID":"grid.419556.a"
                   }
                }
             ]
          },
          {
             "given":"Guillaume",
             "family":"Ducoffe",
             "affiliation":[
                {
                   "original_name":"ICI Bucharest, Romania",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-23",
       "keywords":[
          "graphs",
          "corpora and corpus activities",
          "networks",
          "historical studies",
          "library & information science",
          "relationships",
          "design",
          "English",
          "semantic web",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Summary  The paper presents how digital photographs of street art and graffiti writing [1]   are analyzed with computational methods by the Computer Vision group of Heidelberg University, where an interdisciplinary collaboration between art history and computer vision is embedded since 2009. The project on urban art started in November 2017 and has the following aims: It studies the effect of digital possibilities on street art and graffiti writing regarding access, dissemination and mobility. Per definition urban art is strongly attached to a street environment, which is canvas and frame at the same time. This resulting immobility of urban art is in contrast with traditional art, where the materiality simplifies a display at alternating locations. Eventually, the paper highlights why urban art can only endure within a digital context. An example of Bristol-born artist Banksy (*1974) illustrates this: In 2015, he put up a stencil on a wall in Calais, depicting  The Raft of the Medusa   (Fig.1); only two years later, workers painted over the wall and covered Banksy’s work (Samuel, 2017). The project also establishes a data collection of urban art, consisting of reproductions from  Google Arts and Culture , other image archives and a private collection by art historian and street art scholar Ulrich Blanché. Lastly, it demonstrates how computer-based tools are used to study images with regards to form and content. In this way, patterns over time and space or artistic networks are revealed and relations between artwork and urban environment can be evaluated. Therefore, the project team utilizes an interface, which was developed within the group and allows for a visual search based on multiple image regions in large image sets.  2. Evaluating street art and graffiti writing In 2009, a collaboration between art history and computer vision was established within the Computer Vision group. Thus building a bridge between the two disciplines, which resulted in the realization of works, including the creation of an interface (Bell et al., 2014), reconstruction of drawing processes (Monroy et al., 2011) or the detection and analysis of gestures in medieval manuscripts (Yarlagadda et al., 2013), (Schlecht et al., 2011), (Yarlagadda et al., 2010). The group uses deep learning algorithms and unsupervised approaches to study visual similarities on image level (Bautista et al., 2017), (Bautista et al., 2016) and whole sequences (Milbich et al., 2017). The current project utilizes existing methods to study urban art. The presence of digital image collections of urban art and computational approaches enable both large-scale evaluations and detailed studies, which has not been done by scholars so far. Previous work mainly concentrated on terminology (Blanché, 2015), social aspects (Ross, 2016) or individual artists (Blanché, 2012), (Blanché, 2010), highlighted its mediality (Glaser, 2017) and generally justified its study in art history.  The presentation highlights the influence of digitization on urban art, describes the building of a suitable dataset and its evaluation through computational methods. (1) Digital possibilities have influenced all of humanities; for urban art, however, the effect is even more profound. Most traditional artworks are mobile; artists paint on canvas or paper, which allows for easy transportation and public display at various places. In this way, styles, content, or individual motifs spread and art reveals itself to be less bound to a specific place. On the contrary,   urban art is per definition tied to the street; its meaning only fully unfolds on site. The street not only provides a canvas, but also imposes form and additional meaning. As a result, urban art is greatly ephemeral: Works are being over-painted by authorities and artists (Samuel, 2017) – as the example of Banksy showed – or buildings are torn down. In reaction, works are increasingly documented and made available online.   Since its start in November 2017, the project has studied the presence of urban art on the  Internet. Its visibility on different websites has impacted the community and visuality of urban art: Communication between artists and fans has increased and is simpler, motifs are disseminated faster and wider, breaking national borders and indicating a tight network.   It is only through digital possibilities that urban art can be preserved and disseminated – this distinguishes it from traditional art.    (2) In order to study form and content of images with computer-based tools, the project gathers a dataset of urban art, providing metadata if available. Images are taken from  Google Arts and Culture  or  Facebook’s Global Street Art . However, the project team also received a comprehensive set of photographs, capturing urban art in various cities worldwide between 2007 and 2017. All images were taken by art historian Ulrich Blanché; this unique data enables to address new questions regarding the capturing process: How did the photographic perspective and thematic focus change over time? Does it vary for different locations? Is there a correlation between alternating perspectives and Blanché’s social role? Eventually, he captured urban art first as a simple admirer, then as a student and finally as a scholar – although the first role persisted throughout time. The final image collection, including metadata, will be published and can be used by other scholars. A large number of images contain large context regions and objects, including buildings or cars. To improve performance and detection, the data was pre-processed: Around 200 images from the Blanché-dataset were annotated with bounding boxes marking artwork or context.  (3) The project studies the visuality of urban art on the basis of this image collection using computer-based methods. It aims to find recurrences and variances of a motif, ultimately not only pointing to the same but different artists. On a smaller scale, the example of Cologne street artist ‘kurznachzehn’ illustrates this: She uses old family photographs to create paste-ups, which she attaches to walls in various German cities. Her most recognized motif is a young girl – the artist’s mother as a child. The girl appears throughout her oeuvre in a similar pose but in varying scenarios: while picking up a dandelion (Fig.2), painting or feeding a little bird (Glaser, 2017), (‘kurznachzehn’, 2017). In order to study image collections, the project team utilizes unsupervised methods, which have been successfully applied on other tasks and do not rely on labeled data. This is valuable, since digital reproductions of urban art rarely have information regarding artist, title or creation date – this is mainly due to the anonymity of artists and legal reasons. Reproductions are evaluated on an interface, which not only allows to search for individual but also multiple regions and thus to consider geometrical relations between artworks and urban environment. The example of the dandelion-picking girl (Fig.2) by ‘kurznachzehn’ illustrates this: True to the nature of her gesture, she always appears close to the ground. Underlying algorithms use a SVM-classifier trained with one positive against many negative examples. While other retrieval systems require manual tagging, the algorithm purely operates on visual qualities. Currently state-of-the-art methods are being implemented, using CNN instead of HOG-features to train the classifier. Eventually, the interface not only detects identical motifs but also variations. First tests showed promising results; the project team studied images of artworks by Brazilian street artists OsGemeos. The user was interested in a figure seen from behind and a text region to its right; (Fig.3) shows the search results for the given queries after the second training round; the bottom row includes all correctly retrieved images as selected by the user. Results can now be analyzed regarding formal and semantic similarities or variances; also, it allows to evaluate the position of the motif in relation to the urban context. Future work should study the motif of the figure seen from behind also in the context of its general appearance in art history. Applying computational methods to urban art data has emphasized chances and benefits, not only for art history but also for computer vision. Existing algorithms have been tested on challenging data and proofed their efficiency. However, working with urban art data has also highlighted some challenges: Collections are biased towards certain time periods, nationalities and dominated by works of popular artists. The new dataset, established within the project, is therefore extremely valuable. First tests, although overall successful, showed that algorithms are challenged by a dominating background, imaging mode (perspective) and the size of artworks. To remedy the latter, the project team decided to annotate part of the Blanché dataset with bounding boxes, which improved detection. 3. Conclusion  The presentation consists of two parts: A theoretical basis will be established in the first, discussing the influence of digital possibilities on aspects, such as mobility, access or dissemination, while the dataset will be introduced in the second half, which also includes presentations of search results on the interface. The project team aims to further establish urban art as a profound research topic in academia, point to new research questions and possible challenges when working with urban art data. Most importantly, the presentation emphasizes the chances offered by computer-based methods to study urban art in detail and on large-scale. (Words: 1485) 4. List of Illustrations      Fig.1: Banksy,  The Raft of the Medusa , Calais, 2015      Fig.2: ‘kurznachzehn’,  Girl picking Dandelion , Dusseldorf, 2013     Fig.3: Search results for figure and text to its right on interface; image collection of Brazilian street artists OsGemeos  ",
       "article_title":" Urban Art in a Digital Context: A Computer-Based Evaluation of Street Art and Graffiti Writing  ",
       "authors":[
          {
             "given":"Sabine",
             "family":"Lang",
             "affiliation":[
                {
                   "original_name":"Heidelberg Collaboratory for Image Processing",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Björn",
             "family":"Ommer",
             "affiliation":[
                {
                   "original_name":"Heidelberg Collaboratory for Image Processing",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "computer science",
          "art and art history",
          "artificial intelligence and machine learning",
          "resource creation",
          "film and media studies",
          "English",
          "and discovery",
          "digitisation",
          "content analysis",
          "interdisciplinary & community collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  Stylometry is a very successful application area of the digital humanities (e.g., Juola, 2006). However, to date it is mainly confined to the study of linguistic style, perhaps reflecting a general focus of the digital humanities on text. Stylometric tools for  visual material are not yet as well-established, despite recent advances in digital art history  (Saleh and Elgammal, 2015; Manovich, 2015). Part of this deficit may originate in the challenging aspects of traditional computational image analysis, which requires deep expert knowledge for hand-crafting engineered features applicable to a problem domain. Recent advances in artificial intelligence together with the availability of large corpora of annotated images have partly ameliorated the situation. Now we can delegate to the machine the task of discovering the features relevant for classification. In particular, deep convolutional neural networks (CNNs; LeCun et al., 2015) have been very successful in many image classification tasks, using a feature hierarchy akin to levels of processing in the human visual system. Here we propose a method for a visual stylometry of comics based on CNN features. We test transfer to comics by using a large corpus of graphic narratives. We further show how CNN features can be used to predict readers’ attention. In closing we explore how the approach might be used for tasks such as locating text or finding characters, as well as in other domains such as art history.    Convolutional Neural Networks (CNNs)  Convolutional Neural Networks (CNNs) are a class of neural networks specialized in analyzing data with an implicit spatial layout, such as the stack of three 2D matrices commonly used to represent RGB images in the computer. CNNs are characterized by local connections, shared weights, pooling, and the use of many layers. Within each convolutional layer, a stack of different filters (feature maps) is trained. Each unit is connected to local patches in the feature maps of the previous layer through a set of learned weights. These are called a filter kernel, and learned by backpropagation. A local weighted sum computed by applying the filter kernel to the image is passed through a non-linearity   This nonlinearity is needed because neural networks with just linear activation functions are effectively only one layer deep, and therefore cannot be used to model the full range of real world problems, many of which are nonlinear. , often a rectified linear unit (ReLU). All units in a feature map share the same filter kernel; feature maps in a layer differ by using different kernels. The receptive size of each filter (i.e. the region of the image it responds to) is small at the lower layers, and becomes progressively larger at higher layers. Conversely, the higher the layer, the more complex the features encoded by the filters. Pooling layers typically replacing a local patch by its maximum value are added to further reduce the number of parameters and to provide a more coarse-grained and robust description.   Lower-level filters often respond well to edges and boundaries and thus resemble simple cells in human visual cortex. Higher-level features, in contrast, can code for complex stimuli like textures or facial parts. Just like the visual system, CNNs compose objects out of simple features by using compositional feature hierarchies. Edges combine into motifs, motifs into parts, and parts into objects. CNNs pre-trained on large-scale image classification tasks like ImageNet (14 million images with over 1,000 classes) can be adapted to specific material by re-training just a few layers, assuming that basic features at the lower level are more or less generic. Therefore, we expected transfer to comics drawings, even for networks that had been pre-trained on photographic images. Note that all of the networks we use have been trained on photographs, i.e., they have never seen graphic novels. However, since they have learned filters and filter combinations that are useful for the interpretation of our environment, we hypothesized that they might also be useful for the analysis of drawings. Drawings are abstractions, but as such they do have a relationship to our environmental reality.  Method  The material we used is the Graphic Narrative Corpus (GNC; Dunst et al., 2017). The GNC is a representative collection of graphic novels, i.e., book-length comics that tell continuous stories and are aimed at an adult readership. The stratified monitor corpus currently includes 209 graphic narratives amounting to nearly 50,000 digitized pages. A subset of the first chapter of these works is annotated by human annotators with respect to the location and identity of panels, main characters, character relations, captions, speech bubbles, onomatopoeia, and the respective text. Furthermore, eye movement data is collected for these pages to measure readers’ attention. At the time of writing we had available the first 10 pages of 95 works by 87 authors. In order to test generalization of the features and their transfer to graphic illustrations, we describe material from the GNC using a specific CNN, Inception V3 (Szegedy et al., 2015) using pre-trained weights from ImageNet. We chose Inception V3 for stylometry and artist attribution due to its state-of-the art performance, economic parameterization, and relative independence of input sizes. Because of the small amount of training data, we trained a support vector machine (SVN) rather than a neural network to classify drawing style, based on a description of 9 pages of each of the works in terms of the visual features coded in each of the main (mixed) layers of Inception V3. One randomly determined page per comic was held out to evaluate performance. Second, we were interested in which features of the material were guiding visual attention of the reader. For the prediction of the distribution of attention we used DeepGaze II (Kümmerer et al., 2016), currently the leading entry in the MIT saliency benchmark. DeepGaze II uses features from several layers of VGG-19 (Simonyan and Zisserman, 2014) to predict “empirical saliency”, i.e., where people look or where they move the mouse to un-blur an image   We are convinced that in principle, the DeepGaze architecture could also use features of a different CNN such as Inception as predictors. However, it is currently only available based on VGG features. . We were again interested in determining how good the transfer from photographs to graphic illustrations is. We compared DeepGaze II predictions to empirical gaze locations of 100 readers reading a subset of 105 pages from 6 graphic novels using the metric of information gain explained (Kümmerer et al., 2015).    Analysis and Results  Overall, the top-1 classification accuracy in the artist attribution analysis, based on the highest vote, was 93%. That is, the artist of 93% of the hold-out pages was correctly classified based on Inception features. It is instructive to further inspect the few misclassifications. For example, a page of “Batman: The Long Halloween” drawn by Tim Sale was mistakenly attributed to David Mazzuchelli, the artist responsible for “Batman: Year One”, which was also part of the training corpus. Tim Sale got only the second highest vote for this particular page. Our analysis illustrates that stylistic classification is generally possible using out-of-the box features of a pre-trained neural network and given only very limited amount of training material. We expect the results to yet improve given more training material, and possibly using a neural network classifier rather than an SVM. An in-depth analysis and the corresponding visualizations of which features are the most discriminative and most strongly associated with a given artist are underway and will be presented at DH. The empirical fixation distribution is very convincingly reproduced using DeepGaze II, even without additional training (Figure 1). Overall, for all 105 pages the match of empirical fixation distribution by the model predictions was quite high (Figure 2). CNN features can thus be used to predict which image regions will attract attention. Most likely this is due to their encoding of image properties that combine into objects such as text boxes, faces, and characters, on which most of the empirical fixations are concentrated.   Figure 1. Empirical fixation distribution of 100 readers (dots) and DeepGaze II predictions (contour lines) on a page from the German translation of Daniel Clowes’ (1997/2000) graphic novel Ghost World     Figure 2. Distribution of the measure  information gain explained, comparing theoretical Deep Gaze II predictions with empirical fixation locations obtained by measuring eye movements of 100 readers on 105 pages from six graphic novels    Outlook  We obtained very promising results of a stylometric analyses of comics artist based on CNN features trained on photographs. Given this successful transfer, we suspect that such features are general enough to be applied in a wide variety of other domains in which a visual stylometry may be useful. For example, arts historians may be interested in combining the method with nearest neighbor search to describe how close different artist are in feature space (cf. Saleh and Elgammal for a similar approach based on classic features). Historians may find visual feature based analyses useful for annotation of documents containing images along with text.  If stylometry works so well, CNN features can probably be used for detecting visual elements such as speech bubbles or characters within panels. We currently experiment with YOLO 9000 (Redmond and Farhadi, 2017), which does a very good job at locating objects and persons in panels, and is likely to also function well as a speech bubble locator with additional training. If such object classes can be located automatically, implementation into an annotation tool might make the tedious task of annotation significantly easier, so that annotators have more time to concentrate on work at the narratological level. ",
       "article_title":"Computational Analysis and Visual Stylometry of Comics using Convolutional Neural Networks",
       "authors":[
          {
             "given":"Jochen",
             "family":"Laubrock",
             "affiliation":[
                {
                   "original_name":"University of Potsdam, Germany, Germany",
                   "normalized_name":"University of Potsdam",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03bnmw459",
                      "GRID":"grid.11348.3f"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Dubray",
             "affiliation":[
                {
                   "original_name":"University of Potsdam, Germany, Germany",
                   "normalized_name":"University of Potsdam",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03bnmw459",
                      "GRID":"grid.11348.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "corpora and corpus activities",
          "art and art history",
          "artificial intelligence and machine learning",
          "resource creation",
          "image processing",
          "English",
          "and discovery",
          "digitisation",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction The spring of 2016 has become known as the “Primavera Violeta” (“Purple Spring”), a period that saw the emergence of new digital activist networks tackling gendered and sexual violence in Latin America. Of the hashtags generated by these movements, few gained the public recognition and “celebrity status” of #MiPrimerAcoso (“My First Harassment” or “My First Abuse”), a hashtag that asked users to publically share their first experiences of sexual violence. On April 23, 2016, women in Mexico and across Latin America shared their stories via their personal Twitter accounts in response to a request tweeted by journalist Catalina Ruiz Navarro of the pop-feminism collective (e)stereotipas: “¿Cuándo y cómo fue tu primer acoso? Hoy a partir de las 2pmMX usando el hashtag #MiPrimerAcoso. Todas tenemos una historia, ¡levanta la voz!”  (When and how did your first acoso  happen? Today from 2pm on, use the hashtag #MiPrimerAcoso. We all have a story, raise your voice!)    Figure 1: A typical #MiPrimerAcoso tweet. In English: I was eleven years old and a man passed on a bicycle and grabbed my breast. A woman in the street blamed me for wearing that blouse.   After its initial launch, #MiPrimerAcoso spread rapidly throughout Mexico and quickly became a trending topic across Latin America. This analysis investigates the ways that Twitter users– activists, laypersons, public figures– use hashtags to talk about trauma, paying special attention to the ways that quantifiable modes of Twitter engagement point to more complex affective experiences.  Methods This project undertakes both qualitative and quantitative analyses of tweets posted using #MiPrimerAcoso in order to examine the key actors, contexts, and conditions that emerged from the hashtag’s narrative premise. For the initial assessments, this analysis uses the #MiPrimerAcoso corpus collected and published by media company Lo Que Sigue. To provide a point of comparison, this project also analyzes a collection of tweets posted using another Primavera Violeta hashtag– #VivasNosQueremos (“We Want to Live”)– whose corpus was collected and published by Lo Que Sigue at the same time as the #MiPrimerAcoso corpus.   Affective (and Effective) Tweeting  Hashtag dialogues serve to construct and re-construct bridges between different streams of dialogue within movements, between movement collaborators and stakeholders, and between activists, political powers, and the general public. To illustrate some of the preliminary findings of this exploration, I evaluate the prevalence of retweets and multiple-hashtag use (or “co-tagging”) in the #MiPrimerAcoso corpus and another corpus published by #LoQueSigue of tweets posted using #VivasNosQueremos. Throughout this paper, I call upon Papacharissi’s (2015) work on the affective properties of Twitter dialogues to further illustrate the forms of personal and political affect that drove the trans-national trajectory of #MiPrimerAcoso.     Although #MiPrimerAcoso is entangled with other Twitter dialogues on gender violence, it “stands alone” more often than one of its closest peers, and is less frequently retweeted and co-tagged. Here, I find that these concrete metrics summarize diverse modes of engagement: retweeting another user’s personal story of violence is necessarily a different act than retweeting a popular news story about the hashtag. However, these metrics do demonstrate the ways in which use characteristics reflect the discursive mandate of a hashtag. Engagement with #MiPrimerAcoso might include reading, listening, creating original content, rebroadcasting, or responding to the content of other users within the affective public generated by the hashtag. This diverse set of practices allows Twitter users to “tune into an issue or a particular problem of the times but also to affectively attune with it, that is, to develop a sense for their own place within this particular structure of feeling” (Papacharissi 118).  The Twitter users who tweeted their experiences of violence undertook a delegated task of content creation in response to the prompts posted by Ruíz Navarro. This guiding of the discussion allowed Twitter users to act and to  feel using a pre-constructed response frame. By asking users to share the how and when of their first acoso, users tasked with personifying the political and  making it about themselves. By focusing on a tweet structure that outlines an individually expressive personal action frame through the medium of shared experience, #MiPrimerAcoso allows its users to make \"small and fitful contributions\" (Bennett and Segerberg 2011) to a cause while feeling a profound sense of identification with the movement.   If we want to understand what it is people want from digital activism, #MiPrimerAcoso offers captivating insights regarding our need to see ourselves within online political movements. The secret of #MiPrimerAcoso’s handling of collective and individual resonance lies in its personalization and generalizability: although the hashtag calls on a specific category of experience, it is sufficiently broad that many interpretations of  acoso fit the bill, and many users were able to affiliate with the hashtag without necessarily sharing a personal story of sexual violence. As Papacharissi (2015) notes, the use of hashtags as “open” signifiers allows various publics to affiliate with a movement and “fill in” the open hashtag with their own desired meanings. Women were able to link their own experiences of sexual violence to the individual narratives that had already been shared using the hashtag #MiPrimerAcoso. What, then, of those who did not contribute their original narratives to the library of primer acosos, but instead chose to respond or rebroadcast existing #MiPrimerAcoso content? In responding to a tweet, users may amplify, stifle, or otherwise alter the public life of the digital  acoso. Although Papacharissi and others have linked the act of retweeting to the expression of solidarity with a movement, this conclusion may prove reductive in the context of #MiPrimerAcoso. However, solidarity does not adequately summarize the act of rebroadcasting another person’s  acoso: it is an expansion of the tweet’s intangible audience of ethical witnesses to the tweeted acoso, a “re-telling” of scene of violence. Like any other hashtag, #MiPrimerAcoso needed to meet specific communicative and technical (in the case of Twitter) requirements in order to maximize its “reach” and extend beyond the core audience of (e)stereotipas. Referring to the act of retweeting, Papacharissi argues that refrains reinforce affect (Papacharissi 2015). By posting tweets tagged #MiPrimerAcoso, users spread the affective and contextual implications of the hashtag to their own Twitter audiences: those in digital “earshot” of their tweets. Similarly, the authors of original #MiPrimerAcoso tweets were also invited to act as amplifiers of the larger movement by adding their story to a collaborative, polyvocal narrative of lived violence.   Conclusions   In our study of digital movements, the use of the hashtag is the tip of the iceberg in comparison to the forms of knowledge, feeling, and understanding that emerge from these affective discourses. The results of this research have also suggested that conventional Twitter analysis methods may not adequately assess the affective clout of digital dialogues. For this reason, this analysis has strived to use the concrete metrics of the #MiPrimerAcoso data as guide to direct a “closer” reading of the narrative attributes of the tweets. When examining Twitter data, we must strive to expand the possibilities behind a simple, quantifiable act such as a retweet, and understand the hashtag as a point of contact between the user and digito-phenomenological processes of which we are largely unaware. Of course, there are key characteristics of the hashtag itself that are crucial to our understanding: its connectivity, for example, or its capacity to understand individual content as part of a larger dialogue. The hashtag is a departure point: an entity that gives rise to visible manifestations of trauma, digital acts of vulnerability and moments of personal catharsis, responses of support, condemnation, or indifference.  We should consider the tweet, then, as the execution of a series of digital actions, but also as the manifestation of a confluence of contacts between the ontological and phenomenological worlds of Twitter. To better assess these intangible qualities of Twitter data, we can listen to the testimonies of #MiPrimerAcoso authors, and pay attention to the strategies they employ to construct the  acoso in relation to their present selves, the ways in which they reflect on the act of tweeting the  acoso in front of an intangible digital audience. Here, I want to emphasize the diversity of experiences that users bring to the discursive space of Twitter, and the need to pay attention to the varied motivations that drive Twitter users to participate in social campaigns. These experiences do not easily reduce themselves to quantitative metrics, but we can search for their traces in the textual manifestations of our digital activity: the stories we tell, the words we use, the affective investments that we make as observers and participants.  ",
       "article_title":"Hashtags Contra El Acoso: The dynamics of gender violence discourse on Twitter",
       "authors":[
          {
             "given":"Rhian Elizabeth",
             "family":"Lewis",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "internet / world wide web",
          "spanish and spanish american studies",
          "feminist studies",
          "social media",
          "anthropology",
          "English",
          "gender studies",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Historians grapple with missing information constantly. While there are many statistical tools for gauging the impact of missing source data on quantitative results and conclusions, DH researchers have rarely deployed these tools in their work. This paper presents one implementation of data imputation used in the study of the New York City art dealer M. Knoedler & Co. Demonstrating the significant contribution imputation had on our study and its conclusions, this paper will discuss specific, practical rhetorical strategies, including static and interactive visualization, for explaining this methodology to an audience that does not specialize in quantitative methods.  Missing Data in the Digital Humanities Miriam Posner has argued that both data structures and rhetorical conventions for computing with missing information, uncertainty, and highly subjective/viewpoint-contingent knowledge remains a key desideratum of DH scholarship. (Posner, 2015) Several attempts have been made by the information science community to express uncertainty in a structured format, ranging from generalized ontologies for reasoning in a networked world (Laskey et al., 2008), as well as more specific projects such as the  Topotime library for reasoning about temporal uncertainty. (Grossner and Meeks, 2013)  However, many DH projects have sidestepped these approaches. Matthew Jockers, for example, has asserted that the availability of full text is becoming such that literary historians will no longer have to be concerned about drawing a representative sample. (Jockers, 2013: 7–8) More commonly, though, scholars have attempted to carefully constrain their conclusions based on what they know to be missing from their data. Theorizing and documenting the difference between one’s data set and one’s subject has become a genre of DH work unto itself. Katherine Bode has argued that such documented datasets should be understood as  the object of DH inquiry. (Bode, 2017)  While statistical literature on the problem of missing-data imputation is quite mature (see Gelman and Hill, 2006 for a valuable review), few DH research projects have openly explored the use of statistical procedures for reckoning with missing data, nor have they grappled with how to theorize and present such imputation in the context of their home disciplines. (An important exception includes Brosens et al., 2016) Bode, for one, has explicitly rejected such approaches, arguing (without specific evidence) that quantitative error assessment cannot be usefully performed in historical analysis. (Bode, 2017: 101) We argue that such methods should be  central to data-based digital humanities practice. Simulation and imputation allow us to realize multiple, sometimes conflicting assumptions about the nature of missing data. In doing so, these affordances allow us to evaluate how certain assertions may propagate their assumptions through the transformations we perform on our sources.    Case Study: Modeling M. Knoedler & Co.’s Business from Sparse Stock Books As part of a research initiative into data-based approaches to the study of the art market, we are investigating the changing strategies of the New York City art dealer M. Knoedler & Co., whose stock books have been encoded by the Getty Research Institute ( http://www.getty.edu/research/tools/provenance/search.html). Based on these transaction data, we have built a predictive model that classifies whether a given artwork would result in a profit or a loss, using a host of variables such as how much money the work of art originally cost, the genre and size of the work, their prior relationships with buyers and sellers, and the time the work remained in stock before it was sold, to name but a few. Predictive modeling illuminates complex relationships between these variables and highlights unusual sales for further archival research.  As informative as these stock books are, however, many of their notations are partial: Knoedler’s staff may have neglected to record the date of sale; there may be a listed purchase without a description of the type of work (i.e. portrait, landscape, etc.); the identity of the buyer, and whether they were a first-time customer or a well-known shopper, may also have gone unrecorded. Because our random-forest-based model (Liaw and Wiener, 2002) does not allow missing values, we must either discard incomplete records (and thus eliminate nearly half of the records from consideration), or we must find ways to impute values for our predictor variables.   Figure 1 Schematic workflow for imputing missing data, producing derivative features, building models, and then aggregating statistics from the multiple models produced.  While it is impossible to perfectly reconstruct these missing records, it  is possible to operationalize educated guesses about their possible values. (Figure 1) Purchase and sale dates for artworks, for example, can be predicted with some accuracy based on their location in the roughly-chronological series of stock books. Likewise, unknown genres can also be imputed as a function of the existing distribution of genres across stock books, with, e.g. abstract paintings being far less common in the pre-20th c. books than in the later ones. By defining an informed range of possibilities for these missing data, and then sampling from that range, we can produce ensemble models and results that provide a more nuanced representation.    Figure 2 Partial dependence plots illustrating the marginal effect of artwork genre on Knoedler’s chance at profitability.  Figure 2(a) shows the marginal effect of artwork genre on Knoedler’s chance of turning a profit across three periods of their business, only considering around 20,000 “complete” cases from the Knoedler transaction records (approximately 60% of the known transactions they made.) A first glance suggests that history paintings were markedly less profitable after 1935, while still lifes became comparatively more profitable after 1935. However, 2(b) shows the results not from 1 model, but from 500 models, each trained on a slightly different set of stochastically-imputed data. By visualizing one bar for each model, this plot drives home the effect of increased uncertainty on these measurements, while visually foregrounding the crucial methodological decision - 500 models instead of 1 - in a way that a box plot or other summary visualization method does not (at least, not in the eyes of a reader unused to reading such idioms.) The apparent advantage of still life in Knoedler’s post-1935 business has evaporated, although the notably-lower value of history paintings between 1900-1935 may have withstood this simulation of uncertainty. While this model affirms that genre is largely an anachronistic construct that has little effect on prices, these results complicate a simplistic reading by indicating that, in some cases, there  is a significant relationship that must be reckoned with.    Figure 3 Partial dependence plots illustrating the marginal effect of time in stock on Knoedler’s chance at profitability.  Figure 3 shows a similar comparison of complete case vs. imputed data for a continuous variable: the time a painting spent in stock. Both 3(a) and 3(b) support the conclusion that not only did a longer time in stock contribute to lower chances of turning a profit, but that Knoedler’s window for making a profitable sale grew throughout the lifetime of the firm, from around 2 years before 1900, to more than 5 years after 1935. The increased uncertainty added by the multiplicity of models in 3(b) discourages the kind of over-interpretation that the seeming-precision of 3(a) allows. However, it also demonstrates that, even in the face of so many missing or imprecise dates in the Knoedler stock books, we can still recover meaningful quantitative conclusions.   Figure 4 Screenshot of an interactive application allowing users to modify imputation assumptions and see the effect on modeling and analysis results.  These static visualizations are easily enhanced through animation that shows the buildup of individual model characteristics into aggregate confidence intervals. (Lincoln, 2015) We have also experimented with interactive applications (Figure 4) that allow the user to specify different imputation assumptions, and then immediately see the downstream results on our predictive models, reinforcing the close relationship between starting assumptions and modeled conclusions. (An early demo of this work:  https://mdlincoln.shinyapps.io/missingness/)  Computationally, these imputations are simple, perhaps even simplistic. More complex approaches, such as iteratively modeling every missing variable (Buuren and Groothuis-Oudshoorn, 2011), might lead to more accurate modeling. However, these less parsimonious methods are more opaque to humanities scholars. Operationalizing the historian’s habit of educated guessing and thoughtful assumptions, and visualizing those operations straightforwardly, may allow missing data imputation to work its way into the accepted suite of DH methodologies.  ",
       "article_title":"Modeling the Fragmented Archive: A Missing Data Case Study from Provenance Research",
       "authors":[
          {
             "given":"Matthew",
             "family":"Lincoln",
             "affiliation":[
                {
                   "original_name":"Getty Research Institute, United States of America",
                   "normalized_name":"Getty Research Institute",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00qb4ds53",
                      "GRID":"grid.446893.4"
                   }
                }
             ]
          },
          {
             "given":"Sandra",
             "family":"van Ginhoven",
             "affiliation":[
                {
                   "original_name":"Getty Research Institute, United States of America",
                   "normalized_name":"Getty Research Institute",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00qb4ds53",
                      "GRID":"grid.446893.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "art and art history",
          "agent modeling and simulation",
          "historical studies",
          "visualisation",
          "English",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Figure 1 shows a slab of tomb biography of the Tang dynasty.   This image was downloaded from <http://www.lyqtzz.com/uploadfile/20110817165325665.jpg>. The Tang dynasty existed between 688CE and 907CE. More images of tomb biographies are available at <http://goo.gl/XHCL9P>.  Researchers can copy the words on such slabs to produce a collection of tomb biographies for research. A typical tomb biography contains various types of information about the deceased and their families and, sometimes, a rhyming passage of admiration. Employing software tools to analyze the texts, one can extract useful information from the collections of tomb biographies to enrich databases like the China Biographical Database (CBDB) to support further Chinese studies.   The China Biographical Database (https://projects.iq.harvard.edu/cbdb/home) is a free and open database for Chinese studies.       Figure 1. A slab of tomb biography of the Tang dynasty   It is well known that modern Chinese texts do not include delimiters like spaces to separate words. Hence, researchers design algorithms for segmenting Chinese character strings into words (Sun et al., 2004; Shao et al., 2017).  In contrast, it is not as well known that, in classical Chinese, there were no markers for the separation of sentences. The characters in Figure 1 simply connect to each other. In modern Chinese, texts are punctuated for pauses in sentences and ends of sentences. The research about algorithmically inserting these syntactic markers into classical Chinese is receiving more attention along with the growth of digital humanities in recent years. The needs of segmenting ancient texts for humanities studies are not unique to Chinese studies, interested readers can find examples for German texts (Petran, 2012) and Swedish texts (Bouma and Adesam, 2013). Huang et al.  (2010) employed the techniques of conditional random fields (CRFs) to segment texts of literature and history. They achieved 0.7899 and 0.9179 in F 1 ,   The  precision rate,  recall rate, and  F measure are designed for evaluating the effectiveness of information retrieval and extraction. F 1 is a popular choice of the F measure.   respectively, for segmenting the texts in Shiji and Zuozhuan.   Shiji (史記) and Zuozhuan (左傳) are two very important sources about Chinese history.  Wang et al. (2016; 2017) applied recurrent neural networks to segment texts in a diverse collection of classical Chinese sources. They achieved F 1 measures that are close to 0.75, and item accuracies that are near 0.91.   The  item accuracy evaluates the labeling judgments including both punctuated and non-punctuated items. In a typical sentence segmentation task, there are many more non-punctuated items than punctuated items, so it is relatively easier to achieve attractive figures for the item accuracy than for the F measure.   The researchers achieved different segmentation results for different corpora even when they applied the same techniques and procedures. It is thus inappropriate to just compare the numbers for ranking because the nature of the corpora varies widely.  In this proposal, we report our attempts to segment texts in tomb biographies with CRF models (Lafferty, 2001). We studied the effects of considering different types of lexical information in the models, and achieved 0.853 in precision, 0.807 in recall, 0.829 in F 1, and 0.940 in item accuracy .   We interviewed Hongsu Wang (王宏甦), the project manager of the China Biographical Database Project at Harvard University, about his preferences in post-checking the segmentation results that are produced by software. He suggests that higher precision rates are preferred. When seeking higher recall rates (often sacrificing the precision rates), the false-positive recommendations for punctuation are annoying to the researchers.  Better results were accomplished when we employed deep learning techniques, including applicaitons of long short-term memory networks and sequence-to-sequence networks, for segmenting our tomb biographies.    Data Sources We obtained digitized texts for three books of tomb biographies of the Tang dynasty (Zhou and Zhao, 1992; 2001). The collection consists of 5119 biographies which contain 423,922 periods, commas, and semicolons. There are 5505 distinct types of characters and a total of more than 2461 thousand of characters in the collection.   In terms of Linguistics, we have 5505 character types and 2,461,000 character tokens.  When counting these statistics, we ignored a very small portion of characters that cannot be shown without special fonts. Hence, these statistics are not perfectly precise, but they are accurate within a reasonable range. On average, a biography has about 480 characters. Some of them are very short and have many missing characters. Hence, we exclude biographies that have no more than 30 characters in our experiments.   30 is an arbitrary choice, and can be changed easily.     Training and Testing CRF Models We consider the segmentation task as a classification problem. Let C i denote an individual character in the texts. We categorize each character as either  M (for “followed by a punctuation mark”) and  O (for “an ordinary character”). Assume that we should add only a punctuation mark between C 3 and C 4 for a string “C 1 C 2 C 3 C 4 C 5”. A correct labeling for this string will be “O O M O O”.   Due to the constraint on the word count in DH 2018 proposals, we can only briefly outline the steps for training and testing CRF models. More details can be provided in the presentation and in an extended report.   We can convert each character in the texts into an  instance, which may be used for training or testing. We provide with each instance a group of contextual  features that may be relevant to the judgment of whether or not a punctuation mark is needed. For instance, we may use one character surrounding a character X and itself as the group of features. The following are two instances that we create for C 3 and C 4. The instance for C 3 is (1), and the leftmost item is the correct label for C 3, and the rest are the features for C 3.   Here, we adopt typical notations for CRF-based applications. w[0] is the current word, w[-1] is the neighbor word to the left of the current word, w[1] is the neighbor word to the right of the current word. Two actual instances that are produced from “孝敬天啟，動必以禮” for character-based segmentations will look like the following.  O w[-1]=敬,w[0]=天,w[1]=啟 M w[-1]=天,w[0]=啟,w[1]=動 Two instances that are produced from “母子 忠孝 ， 天下 榮 之” for the word-based segmentations will look like the following. M w[-1]=母子,w[0]=忠孝,w[1]=天下  O w[-1]=忠孝,w[0]=天下,w[1]=榮   M w[0]=C 3,w[-1]=C 2,w[1]=C 4 (1)  O w[0]=C 4,w[-1]=C 3,w[1]=C 5 (2)  We can train a CRF model with a selected portion of the instances (called  training data), and test the resulting model with the remaining instances (called  test data). The instances in the training and the test data are mutually exclusive.   We employ a machine-learning tool that learns from the training data to build a CRF model.   CRFSuite: <http://www.chokkan.org/software/crfsuite/>  We then apply the learned model to predict the classes of the instances in the test data. The labels of the instances in the test data are temporarily concealed when the learned model makes predictions.   Thus, the instances for testing CRFs look like (1) and (2) that do not carry the correct labels, M and O, respectively..  The precision rate and recall rate of the learned model are calculated with the correct and the predicted labels.  We report four sets of basic experiments next, each investigating an important aspect for analyzing Chinese texts. The 5119 biographies were resampled and randomly assigned to the training (70%) and test (30%) sets for every experiment.   Recall that we used only those biographies that have no less than 30 characters.  We repeated every experiment three times, and report the averages of the precision and recall rates.   Changing the Size of the Context We certainly can and should consider more than one character around the current character as the context. Figure 2 shows the test results of using different sizes of contexts for the instances. The horizontal axis shows the sizes, e.g., when  k=2, the feature set includes information about two characters on both sides of the current character. P1 and R1 are the average precision and recall rates, respectively.       Figure 2. Effects of varying context sizes  We expected to improve the precision and recall rates by expanding the width of the context. The margin of improvements gradually decreased, and the curves level off after the window sizes reached six. The recall rises sharply when we add the immediate neighbor word into the features, emphasizing the predicting power of the immediate neighbor character. When  k=10, the precision and recall are 0.765 and 0.729, respectively, and the item accuracy exceeds 0.91.    Adding Bigrams We added bigrams that were formed by consecutive characters into the features. The following instance shows the result of adding bigrams to the features in (1).   Here, w[-1_0] is the bigram on the left side of the current word, and w[0_1] is the bigram to the right of the current word. When we consider bigrams for a wider context, we may consider bigrams like w[-2_-1] and w[1_2].    M w[0]=C 3,w[-1]=C 2,w[1]=C 4, w[-1_0]=C 2 C 3 ,w[0_1]=C 3 C 4 (3)  Figure 3 shows the test results of adding bigrams while we also tried different sizes of context. The curves named P1 and R1 are from Figure 2, and P2 and R2 are results achieved by adding bigrams to the features. Both rates are improved, and the gains are remarkable.      Figure 3. Adding bigrams improves the results.    Effects of Pronunciation Information Using the characters and their bigrams in the features is an obvious requirement. Since the tomb biographies may contain rhyming parts, it is also intriguing to investigate whether adding pronunciation information may improve the overall quality of the segmentation task.  We considered two major sources of the pronunciation information for Chinese characters in the Tang dynasty:  Guangyun and  Pingshuiyun.   Guangyun and  Pingshuiyun are《廣韻》and《平水韻》, respectively   The statistics in Table 1 show that adding pronunciation information into the features did not improve the overall performance for the segmentation task significantly.   This does not suggest that using the pronunciation information alone was not useful. We have conducted more experiments to evaluate the effectiveness of using the pronunciation information for the segmentation tasks, and will provide more details in the presentation and in an extended report.  The results suggest that, given the characters and their bigrams, adding pronunciation did not contribute much more information. Huang et al. (2010) reported similar observations when they used  Guangyun in their work. Relatively,  Guangyun is more informative than  Pingshuiyun for the segmentation tasks.     Features Width of Context = 1 Width of Context = 2    Precision Recall F 1  Precision Recall F 1    Characters 0.652 0.535 0.588 0.695 0.620 0.655   Characters+Bigrams 0.743 0.654 0.696 0.802 0.736 0.768   Characters+Bigrams+Guangyun 0.748 0.671 0.707 0.781 0.707 0.742   Characters+Bigrams+Pingshuiyun 0.737 0.659 0.696 0.763 0.698 0.729    Table 1. Contributions of pronunciation information    Adding Word-Level Information We can obtain information about the reign periods, location names, and office names in the Tang dynasty from CBDB. By segmenting characters for these special words and adding appropriate type information, we added word-level information into the features. The statistics in Table 2 show that the word-level information did not raise the performance very much.   In Table 2, WOC stands for “Width of Context”, “P” stands for precision, “R” stands for recall, “C+B” stand for “Characters and Bigrams” and “C+B+W” stands for “Characters, Bigrams, and Words”.     Features WOC = 1 WOC = 2 WOC =3 WOC = 4    P R P R P R P R F 1    C+B 0.743 0.654 0.802 0.736 0.823 0.766 0.839 0.790 0.814   C+B+W 0.747 0.671 0.800 0.741 0.818 0.767 0.832 0.787 0.809   C+B+PMI 0.748 0.661 0.804 0.740 0.824 0.769 0.839 0.791 0.814    Table 2. Adding word-level information  We examined the training and test data, and found that, although we gathered the special terms for the Tang dynasty, those words were not used in the biographies often. As a consequence, we did not add a lot of word-level information in the features in reality.  We have also adopted pointwise mutual information (PMI) of bigrams as features, but the net contributions are not significant.     Discussions We have consulted historians, 6,   In addition to Hongsu Wang of Harvard University, we also consulted Professor Zhaoquan He (何兆泉) of the China Jiliang University. They use tomb biographies of the Tang and the Song dynasties in their research.   and learned that our current results are useful in practice. The best precision rates and F measures are better than 0.8 in Figure 3 and Table 2. The best item accuracy is better than 0.94.  In fact, we have designed an advanced mechanism to further improve our results.   Again, we could not provide details about more experiments because of the word limit for DH 2018 submissions.  The new approach employs a second level learning step that learns from the errors of the current classifiers.  One may plan to consider more linguistic information in the segmentation tasks. If appropriate corpora or sources are available, it is worthwhile to explore the effects of adding part-of-speech information in the task (Chiu, 2015; Lee, 2012). We have applied deep learning techniques for the segmentation tasks, and achieved better results. Although we look for methods to reproduce the segmentations in the given texts, we understand that not all experts will agree upon “the” segmentations for a corpus. Different segmentations may correspond to different interpretations of the texts, especially for the classical Chinese. The results of asking two persons to segment Chinese texts may not match perfectly either (Huang and Chen, 2011).  ",
       "article_title":"Classical Chinese Sentence Segmentation for Tomb Biographies of Tang Dynasty",
       "authors":[
          {
             "given":"Chao-Lin",
             "family":"Liu",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Yi",
             "family":"Chang",
             "affiliation":[
                {
                   "original_name":"National Chengchi University, Taiwan",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-18",
       "keywords":[
          "historical studies",
          "text analysis",
          "data mining / text mining",
          "English",
          "asian studies",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Humanities data and data in our daily lives As our world becomes increasingly data-driven, data skills and literacies (including the ability to assess data gaps and coverage, misleading visualizations, and the ethics surrounding data collection, usage, and sharing) are becoming crucial tools to our lives, both inside and outside of higher education. Scholarship across disciples is moving towards more data-intensive work, and scholars are increasingly expected to include open access to the data collected and used. At the same time, devices and software we use, the platforms we use to communicate, and the places we shop are increasingly enabled by the collection of data about our purchasing habits, web history, and contents of our email inboxes. Governments at all levels are increasingly collecting and using data to alter policies and direct day-to-day activities, ranging from transportation infrastructure to policing. While much (though certainly not all) data-driven scholarship may seem significantly different from third-party data collection and data-driven policing, the former provides an opportunity to prepare students to understand, critique and improve the latter. Learning about the accurate and ethical and collection and usage of data and algorithms is a crucial part of liberal education that can help students better understand the processes around them, and better prepare them to apply those ethics and practices in the workplace and civic realm after graduation. While many may think of data literacy as being the work of Computer Science departments, or perhaps library workshops targeted at researchers, the author argues that teaching these skills in the humanities classroom is fruitful for both the development of disciplinary knowledge and for developing crucial skills for use outside of the humanities classroom. Humanities data provides an excellent space to think critically about how people, ideas, and culture can and cannot be captured and analyzed through data. Comparing the data structures of colonial record keeping with the structures communities develop to document themselves provides clear lessons in the power of determining who and what gets documented, in the values that each community holds, and in privacy, ethics, and consent. Text mining novels, government records, or newspapers facilitates critical thinking about the value of metadata, the ability (or lack thereof) to derive meaning from large collections of text, and the use of different algorithms and approaches to ask different questions. At the same time, the ability to think about humanities sources as data, and to properly curate and analyze them as such, provides a productive way to engage more with the way we conceptualize the sources, the way disciplinary knowledge is constructed and practiced, and the affordances provided by digitized and born-digital resources.   Data Challenges in Higher Education The process of gathering, \"cleaning,\" and organizing data can be incredibly time-consuming and difficult to prepare for. It can be tempting (and in many cases, required), to provide students with pre-prepared data to for analysis. Allotting time, either as in-class instruction or independent, project-based work, can take up weeks of time and can be a grueling disincentive for engagement. However, working critically with data rather than working with pre-packaged, pre-prepared datasets also aides us in the integration of digital humanities methods into the classroom, and better enables us to teach students emerging research methods through the full course of humanities research. Students can get a glimpse of the intellectual labor that goes into data collection, organization, and curation; not just in the final analysis. There are several data literacy models that have shown success in other contexts. Data curation training often occurs in university-wide workshops or seminars, and are often brief and necessarily divorced from content and community practices (Carlson and Johnston 2015 p. 2-3). The Data Information Literacy (DIL) initiative, led by Jake Carlson and Lisa R. Johnson, is an extension of the ACRL Information Literacy Framework that focuses on both the creation and consumption of data (ibid.). DIL is designed to be integrated into courses and research labs in the context of subject-specific data and domain-based community practices, but is primarily intended for faculty, staff, and graduate students working on peer-reviewed publication (ibid., p. 2-3). The Library-Led DH Pedagogy: Modeling Paths Toward Information and Data Literacy symposium facilitated productive conversations about the topic of data and information literacy in the digital humanities, but has not produced significant scholarship, models, or frameworks (Padilla et al. 2015). In addition to making the case for teaching critical data literacy in the digital humanities classroom, the author will discuss both practical and theoretical approaches to data literacy in the undergraduate classroom that speak to the impetus behind teaching data literacy in the humanities: for greater disciplinary knowledge and understanding, to better facilitate digital scholarship and knowledge production, and to prepare students to better grasp, interrogate, and work with data in the public and private sector as citizens, employees, and employers.  ",
       "article_title":"Critical Data Literacy in the Humanities Classroom",
       "authors":[
          {
             "given":"Brandon T.",
             "family":"Locke",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "and curriculum",
          "teaching",
          "public humanities and community engaged scholarship",
          "library & information science",
          "pedagogy",
          "English",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  First published in 1771,  Encyclopedia Britannica continues in publication today and is the only encyclopedia in any language to survive that 250-year period. Historical editions of the Encyclopedia offer scholars a unique means of examining the evolution of ideas and beliefs about sensitive cultural topics – such as suicide, race, and hysteria – by studying their treatment in different editions. But what can this curated dataset as a whole can tell us about larger patterns in the social construction of knowledge in the nineteenth-century English-speaking world?   We are creating a data set of all text from these historic editions for use in text mining. The corpus will include over 100,000 entries, all of which need to be tagged with essential metadata fields. How do we identify the different subject areas in this body of knowledge? This article briefly discusses the use of an automatic-metadata-generating algorithm, HIVE, created by the Metadata Research Center at Drexel University. But the central issue it addresses is the theoretical problem encountered in defining a subject vocabulary for this corpus. The  Encyclopedia claims to represent the “Sum of Human Knowledge,” and while we can dispute this claim, it nonetheless represents the existence of older knowledge taxonomies used in its creation. How do we construct a subject vocabulary without distorting this older organizational scheme for subject categories? Those older vocabularies were clearly biased. For example, the decision to include or exclude entries, as well as the size assigned to entries, were all based on assumptions about what mattered as “legitimate” knowledge. Many of these are assumptions we no longer share; the editors excluded forms of knowledge rooted in folk and tribal cultures, and female authors were wholly absent until 1889. Racism and the perspective of British Imperialism are evident in many entries. These prejudices reflect the social beliefs of the writers and editors, of course, and as such, they illustrate the degree to which knowledge in the nineteenth century was clearly socially constructed. And the invented nature of that taxonomy needs to be captured accurately. The value of the curated content of Britannica to researchers today is that is the most comprehensive representation we have of that older knowledge system in its totality, and so it makes it possible to study that system as a structure and to observe how it changed over time.     The problems in tagging this biased dataset take three forms. First is the danger of historical anachronism. Applying a C21-century ontology, like Library of Congress Subject Areas, to C18 and C19 editions makes it accessible to modern researchers, but it also misrepresents the older system of knowledge. For example, the entries on “History” from the important 3rd (1797) and 7th (1842) editions present authoritative accounts of human prehistory. While we might tag them under “anthropology,” that field of knowledge was not recognized by the Royal Academy of Sciences until the 1880s (as a subset of Biology) and does not appear in the Encyclopedia itself until 1889. In fact, the older references cite the Book of Genesis as their authority, and a tag on applications of scripture to the interpretation of external reality might better represent the entry than an anachronistic “Anthropology, history of” tag could do.  The second difficulty is encountered when trying to reconstruct the older ontology used by the Encyclopedia, because it was a moving target. Subject categories changed over the first 150 years, with new categories added, others (like human prehistory) moving from one field to another, and still others disappearing. While we might construct a stable ontology for one edition, any historically-accurate ontology will have to become a system of multiple ontologies, whose relationships with one another need to be explained at the very least.  Third is the question of how to treat the built-in biases within the corpus. Older ontologies of knowledge are rife with bias, often through omission. Historically-accurate subject terms duplicate that bias. Information on attitudes toward women and national minorities, for example, exists within multiple entries, but there are no subject terms for minorities and no entries for women as such, making that data largely invisible without some form of intervention.  We are in the process of creating this new dataset and by summer of 2018 we will be completing preliminary tests on tagging systems, so the final paper will share preliminary results. ",
       "article_title":"Ontological Challenges in Editing Historic Editions of the Encyclopedia Britannica",
       "authors":[
          {
             "given":"Peter M",
             "family":"Logan",
             "affiliation":[
                {
                   "original_name":"Temple University, United States of America",
                   "normalized_name":"Temple University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00kx1jb78",
                      "GRID":"grid.264727.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "scholarly editing",
          "digitisation - theory and practice",
          "ontologies",
          "historical studies",
          "cultural studies",
          "library & information science",
          "metadata",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In his essay “How Not to Teach Digital Humanities,” Ryan Cordell outlines some of the pedagogical and institutional challenges of integrating DH into larger humanities curricula. Importantly, Cordell argues that successful Digital Humanities pedagogy must always take into account local institutional and infrastructural contexts, and notes his structuring of previous classes in order to afford students’ leveraging of campus archival collections. Cordell’s focus on material and institutional infrastructure as the “context” of Digital Humanities work dovetails with other scholars’ calls to productive engage with the wider “structures” that enable DH work,    Kirschenbaum, Matthew. (2012). “Digital Humanities Is/As a Tactical Term.”  Debates in the Digital Humanities, ed. Matthew Gold, University of Minnesota Press    and, more recently, invocations of “critical infrastructure studies,    Parks, Lisa and Starosielski, Nicole. (2015).  Signal Traffic. University of Illinois Press    . While the highlighting of the local and distributed material systems and institutions that underpin digital technologies (and therefore DH practices) can provide crucial insights into the hidden labor and material translations that shape DH institutions, this highlighting can serve to flatten and neuter the ideological-epistemological structures that  also undergird digital practices.  In the context of the STEM educational apparatus, where I find myself enmeshed as a member of the Science and Technology Studies (STS) Department at an Engineering-Centered Institution, these epistemological frameworks can be especially influential, and are often cast as emphasis of “technical expertise” at the cost of the kinds of critical knowledge work that humanities faculty claim to encourage in our students. At face value, this may not be particularly surprising to other humanities scholars. In advocating for the need for DH faculty to resist overplaying “the digital” card in our classrooms, Cordell describes the orientation of the kinds of students we find enrolled in Humanities majors:  Many of our students honestly, truly, really choose literature or history or art history or religious studies because they wanted to read and think deeply rather than follow what they perceive as a more instrumentalist education in business or technical fields. To do so they often resist substantial pressure from family and friends pushing them toward “more practical” majors, which are often perceived to be more technical majors.    Cordell, 2016     Cordell’s characterization fits the standard understanding of where DH takes place—in English departments    Kirschenbaum, Matthew. (2012). “What is Digital Humanities and What’s it Doing in English Departments?”  Debates in the Digital Humanities, ed. Matthew Gold, University of Minnesota Press    and classrooms where computational methods are being used to augment “traditional” humanities education. These students are of a different sort than students in more “instrumentalist” programs and majors—usually stereotyped in DH scholarship as STEM students interested in quantification, technology, and the ability to get a job.    Cordell 2016   However, if DH is to truly operate not as an “interdisciplinary bridge,”    Chris Biemann, Gregory R. Crane, Christiane D. Fellbaum, and Alexander Mehler. (2014). “Computational Humanities – Bridging the Gap Between Computer Science and Digital Humanities”,  Dagstuhl Reports, Vol. 4, Issue 7, pp. 80–111    but rather as a force to resolve and heal the divides between computational/technical practices and interpretive/critical scholarship, we must begin to take seriously the kinds of epistemic-infrastructural contexts STEM disciplines are embedded in, as well as the understand the ideological histories that have shaped those contexts. We must attend to students and scholars in educational contexts  the opposite of which Cordell outlines above: in Engineering-Centered Institutions, Polytechnics, and other  instrumentalist    Nieusma, Dean. (2015). “Conducting the Instrumentalists: a Framework for Engineering Liberal Education.”  Engineering Studies. Vol. 7, 2-3, pp. 159-163     educational contexts.   In this essay, I want to talk about instrumentalism not as pragmatic practice, but as ideological-epistemological apparatus. Instrumentalism not only resists the kinds of non-deterministic scholarship practiced in many humanities spaces, but it is also explicitly designed to account for, consume, and subvert the impacts of critical perspectives on technological systems. I thus want to inflect the concept of “infrastructure” differently than Alan Liu, who defines infrastructure as “the social-cum-technological milieu that at once enables the fulfillment of human experience and enforces constraints on that experience.”    Liu 2016   Rather than enabling and constraining the activities of users, I argue that infrastructures operate epistemically, as “machineries of knowledge,”    Knorr Cetina, Karin. (1999).  Epistemic Cultures: How the Sciences Make Knowledge. Harvard University Press    to  produce those users themselves.    Knorr Cetina, 1999   I borrow from STS scholar Karin Knorr Cetina in arguing that infrastructures of scientific and technical production—including those relevant to the digital humanities—should be understood less as “knowledge infrastructures” and more as “epistemic infrastructures.” For Knorr Cetina, the term “knowledge structures” implies that material-social systems work to produce  what we know. The term “epistemic structures,” in contrast, highlights how those systems work instead to produce  how we know, by producing the practices, tools, spaces, and boundaries of “knowing” and of knowable objects.    Knorr Cetina, Karin (2007). “Culture in global knowledge societies: knowledge cultures and epistemic cultures”  Interdisciplinary Science Reviews. Vol. 32, 4, pp. 361-375    Machineries of knowledge thus produce “epistemic subjects” and “epistemic objects:” practitioners and their always-in-negotiation objects of study.    Knorr Cetina, 1999    If we take seriously the epistemic infrastructures of STEM education, it would be wrong to think of engineering students as instrumentalist persons who enter STEM in order to “be filled” with narrow technical expertise, or of engineering instructors as conspiratorial anti-political agents. Rather, the instrumentalist epistemic structures of engineering education  produce students and teachers as technical practitioners; experts who, through their mastery of the fundamentals of math and physics, practice the production of “non-political” material systems. Simultaneously, though engineering students generally understand that technology “in the world” has social dimensions, engineering’s epistemic infrastructures produce technology as an  epistemic object—“Technology” as abstract and ideal, methodological and  apolitical—and define the boundaries of STEM’s knowledge domain as the exploration of that epistemic object of Technology.   Instrumentalist epistemic infrastructure is frighteningly effective at producing anti-political practices. Erin Cech’s longitudinal study of engineering students at four different universities shows that engineering students’ interest in public welfare, social concerns, and the political impacts of technological systems steadily  declines over the course of their education.    Cech, Erin. (2014). “Culture of Disengagement in Engineering Education?”  Science, Technology, and Human Values. Vol. 39, 1, pp. 42-72    This is despite the fact that, in most engineering programs, what little hands-on design, making, and human-interaction work that students do engage in almost always occurs towards the end of their coursework. This heavy declination of interest in social and political good should be especially concerning given that early outreach programs, particularly at the grade school level, combine building activities with “use technology to change the world” rhetoric to recruit students into STEM career paths. These programs, which include activities like  Lego Mindstorms workshops and hands-on hackathons—and are not altogether unlike celebrated “making” pedagogies in the digital humanities--even consciously recruit women and underrepresented minorities, ostensibly in an effort to diversify the STEM workforce. Upon entrance into STEM higher education, however, students are subjected to a double “bait-and-switch:”    Lachney, Michael and Nieusma, Dean. (2015). “Engineering Bait-and-Switch: K-12 Recruitment Strategies Meet University Curricula and Culture.”  Proceedings of the American Society for Engineering Education.    as making and building activities are immediately sidelined in favor of math and science foundations courses, so too are political and ideological concerns systematically excised from the epistemic object of engineering. This double bait-and-switch is coupled with a systemic administrative devaluing of interpretive humanities and social science courses. While engineering students in the U.S. are required (for now) to take “broad educational” courses, in my experience engineering students are often encouraged by their academic advisors to take “easy” humanities courses that they can mostly ignore in order to concentrate on their core educational work and simultaneously boost their GPA. Instrumentalist infrastructures thus practice the double-move of simultaneously  accounting for and  defanging the political ramifications of humanities scholarship.   Unlike Cordell’s students who, for various reasons, approach technologically-centered humanities classes with reticence and suspicion, engineering and STEM students interested in taking seriously their humanities classes are often attracted to elective classes that are viewed as fitting in with or dovetailing with their technical education, such as economics or philosophy of technology classes, or that allow them to apply their technical skills in the hands-on, self-directed ways that they are unable to pursue in their core coursework, such as digital arts classes. The technological inflection of the digital humanities thus offers a unique incentive for STEM students, as well as pathway for critical humanities and social sciences faculty to productively engage with those students. Ideally, the digital humanities can even begin subverting the instrumentalist epistemic infrastructures of Engineering-Centered Institutions (and the neoliberal university in general).  However, digital humanities pedagogy is also in a unique position to  reinforce instrumentalist epistemological infrastructure, as well. Partly, this comes from the difficulty of teaching technical skills and critical thought to undergraduates at the same time, due in no small part to the epistemic infrastructures erected in the university post-instrumental turn. Ian Bogost has opined that humanists have to bracket criticality in order to get our grounding in technical skills.    McPherson, 2014   I certainly sympathize with the pragmatic difficulties of teaching undergraduates code and close reading at the same time, particularly in our contemporary instrumental episteme. But bracketing technological practice as apolitical skills with potential social impacts—even in the context of a humanities course—only continues to produce Technology as apolitical epistemic object, as something that can be learned apart from the social and political world. As Tara McPherson suggests, the ontology of brackets is particularly pervasive in digital culture, and can actively undermine critical perspectives on technology and ontologies of difference that emerge from feminist, queer, and postcolonial positions.    McPherson, 2014   Thus, DH’s relative lack of attention to the epistemic practices of Technology can  encourage students to assume the instrumentalist stance, and, worse, to pre-tune them to the rejection of politics of difference.   Digital humanities  can provide a model of transformational resistance to technocratic culture. Rita Raley argues that “the digital humanities should not, and cannot, bear the burden of transforming the technocratic knowledge economy.”    Raley, Rita. 2014. “Digital Humanities for the Next Five Minutes.”  differences: a Journal of Feminist Cultural Studies. Vol 25, No. 1, pp. 26-45    But if not us, then whom? And who better to build material-epistemic infrastructures that subvert the bracketing of critical thought and technical practice, that challenge the very ideological tenets of instrumentalism, than digital humanists? By entangling ourselves in the apparatuses of STEM education, and by building frameworks for STEM students—especially those in engineering and computer science—to ideologically contextualize their own educational experiences in their technical majors, digital humanities pedagogy can make inroads into dismantling technocratic culture by allying with the very persons in the best position to reproduce it.  Acknowledgements: Activities described in this paper were made possible in part by the National Endowment for the Humanities. ",
       "article_title":"Epistemic Infrastructures: Digital Humanities in/as Instrumentalist Context",
       "authors":[
          {
             "given":"James W.",
             "family":"Malazita",
             "affiliation":[
                {
                   "original_name":"Rensselaer Polytechnic Institute, United States of America",
                   "normalized_name":"Rensselaer Polytechnic Institute",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01rtyzb94",
                      "GRID":"grid.33647.35"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "and curriculum",
          "cultural and/or institutional infrastructure",
          "digital ecologies and critical infrastructure studies",
          "digital humanities history",
          "teaching",
          "cultural studies",
          "sociology",
          "pedagogy",
          "theory",
          "epistemology",
          "English",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Recently, text miners have analyzed gendered discourse based on a binary opposition, male/female (M/F), trying to determine distinctively ‘female writing style,’ ‘female keywords,’ or ‘female themes’ (Rybicki 2015; Jockers 2011, 2013). The terms ‘male’ and ‘female’ suggest biology and hence were abandoned by literary critics during the big feminist recovery projects of ‘women writers’: “women’ was used in preference to ‘female’ despite the fact that the former is a noun, not an adjective, indicating that gender was a cultural formation rather than a biological one. More theoretically enlightened text miners have used the tools of data analytics to trace changes through time in those attributes assigned to women and those assigned to men, examining how notions of gender change over time (Garg, Scheibinger, Jurafksy, Zou 2017; Underwood, Bamman, and Lee 2018; Olsen 1992). There is another way to analyze gender historically using digital tools without assuming a biological basis for differences between men and women that involves searching for gender categories beyond the binary opposition M/F. In her book  Gender Trouble, Judith Butler encourages undermining the M/F binary opposition by ‘proliferating’ identity categories (Butler 1990, pp. 17, 146). Butler says that ‘the very notion of the subject, intelligible only through its appearance as gendered, admits of possibilities that have been forcibly foreclosed by . . . various reifications of gender’ into the M/F binary (Butler 1990, p. 33).   The result of Butler’s call to multiply gender categories has been the creation of what Bowker and Star call “boundary objects” (1999): “cisgender” and “transgender” have expanded the binary while still relying on the underlying classification of m/f. But to apply these categories on historical documents is anachronistic: there are historically accurate gender categories that have been identified by others for eighteenth-century such as “molly” (Alan Bray 1988; Randolph Trumbach 1991) and “sapphist” (Lisa Lynn Moore 1997; Yopie Prins 1999). But what about others that have not yet been identified by readers? The Feminist Controversy in England project tries to find foreclosed identity categories, to uncover historically specific gender designations in novels, pamphlets, and essays written by women between 1788 and 1810 in England. In 1974, Garland Publishing (now no longer in existence) published a collection of 44 treatises by women authors published on topics related to emerging feminism, edited by Gina Luria Walker ( https://books.google.com/books/about/The_Feminist_Controversy_in_England_1788.html?id=j1pqMwEACAAJ ). Mary Wollstonecraft’s ground-breaking  Vindication of the Rights of Women (1792) was among them. They were facsimile editions. We have used Optical Character Recognition software (Tesseract 3, trained for 18 th-century typefaces by the Early Modern OCR Project,  http://emop.tamu.edu), corrected the OCR using TypeWright ( http://www.18thconnect.org/typewright/documents), run through Named Entity Recognition software to identify character names, and uploaded into the Catma.de interface where they have been tagged by three different teams: undergraduate students, graduate students, and the Professor who is the Principal Investigator on the project. Each team used its own taxonomy explicitly defined in Catma.de except the PI who derives a set of tags from the texts themselves.  The first, most basic taxonomy according to which the texts were tagged by undergraduat students identifies personality traits and activities of characters. The second more interpretive set of tags, encoded by graduate students, involves formal features of novels and essays--protagonists, narrators, and other character types. After these two procedures, each text’s tags are clustered by character in a graph, an interactive d3.js interface that allows a third round of tagging by the PI: the personality traits and activities (character attributes) are tagged either as ‘gender-normative’ or ‘different,’ and the different categories are given what Johnny Saldaña calls ‘In Vivo’ codes, short phrases that come from the language of the text itself (Saldaña 2009, 2016). Afterwards, these In Vivo codes are regularized across the whole set of documents. A second visualization interface provides a network view of all the characters grouped by their connections to tagged attributes, both gender normative and different. The goal has been to discover characters clustering around a set of non-normative character attributes--that is, to find personality traits and activities that are both different and shared, which is to say not merely a matter of any specific character’s personality. We argue that such clusters present alternative gender categories, based of course upon m/f norms (as are ‘cisgender’ and ‘transgender’) but contesting those norms nonetheless.  At DH2018, we present preliminary findings using our prototype visualization interfaces. As we have discovered so far, many characters share attributes in common with Harriet Freke, a character in Maria Edgeworth’s  Belinda. Thus we argue that ‘freke’ represents a specific gender category found in many of the transcribed texts. The goal is to postulate non-binary gender terms that have been derived from the texts themselves, and to demonstrate how this procedure offers an alternative method for historicizing gender.  ",
       "article_title":"Visualizing the Feminist Controversy in England, 1788-1810",
       "authors":[
          {
             "given":"Laura C",
             "family":"Mandell",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Megan",
             "family":"Pearson",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Rebecca",
             "family":"Kempe",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Steve",
             "family":"Dezort",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "queer studies",
          "feminist studies",
          "english studies",
          "visualisation",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The point of departure for our paper is the statement that “The computer is not a tool to help us do whatever we do, it  iswhat we do, it is the medium on which we work” (Dene Grigar, Electronic Literature Organization), and that the Platform Studies approach is essential in the Digital Humanities field to better understand rules of the contemporary digital world. Our goal is to present an output of our 2-years research project devoted to the 8-bit computer ZX Spectrum (especially the ZX Spectrum 16/48K and 128K models). According to Nick Montfort and Ian Bogost: “Platform Studies investigates the relationships between the hardware and software design of computing systems and the creative works produced on those systems. Particular platform studies may emphasize different technical or cultural aspects and draw on different critical and theoretical approaches, but they will be united in being technically rigorous and in deeply investigating computing systems in their interactions with creativity, expression, and culture.” (http://platformstudies.com/)   One could imagine a narration about the ZX Spectrum platform as the official history of the British company Sinclair Research Ltd., in which the official and copyrighted market products of the company would be presented (both the hardware and the software, like games, word processors, graphics processing programs). Sir Clive Marles Sinclair created the ZX Spectrum at the beginning of the 80s as a machine that first and foremost was meant to serve educational purposes. As is the case with the inventions of many creators, Sinclair’s broke away and began a life of its own. This unofficial grassroots and human story is the one we wish to tell.  The starting point for our narrative is the belief that the ZX Spectrum platform is unique as compared to other 8-bit machines. Its uniqueness lies in the reception of the platform by users on a scale which is incomparable to that of any other platform. The traditional way of using platforms (not only the 8-bit) is based on their consumption, or the use of the official equipment, as well as programing, delivered by the manufacturer. And although the stories about platforms such as the C-64 or Atari are no strangers to creative and bottom-up approaches, these are based on the creation of independent programs. Besides the ZX Spectrum, none of these platforms generated the same hardware systems or clones on such scale and creative level. This is related to the simplicity of the computer’s construction and the cheap cost of the accessories as well as the geopolitical conditions in the world in the period of the platform’s popularity, the 80s and 90s. It should also be added that the UK platform was popular mainly in Europe (despite attempts, the platform was never popularized in the United States).  One of the novel aspects of the output of our project is the attempt to compare the East and the West - two worlds with different approaches to the same platform. The story of the Spectrum is used as a focal point that will enable us to describe the differences between the two sides of the Iron Curtain, also after its fall, in the period of political transformation in the countries of the formerEastern Bloc. Briefly, in one of these worlds software and platforms were easily accessible commercial goods, while in the other they were coveted symbols of a different reality in a situation where legal software was almost inaccessible and the platforms were sold only in special stores with foreign goods, or distributed illegally. Both financial and political matters, and the aforementioned simplicity, decided that the platform was cloned en masse. Creativity in both naming the clones (ZX Evolution, Didaktik, Scorpion – just to mention a few), as well as ways of tuning the equipment, is the subject of our study and description. It is also worth noting that no other platform inspired as many equipment parties, bringing together fans of platforms that to this day create clones of the hardware, or magazines devoted to it. Currently, clones of this platform can still be purchased.  By describing the ZX Spectrum platform, we try to tackle trends that are relevant to contemporary studies on digital media, taking into account and affirming the local perspective, different from the dominant one. We are interested in the aspect of creation in the field of digital media, as well as the use of computers for artistic purposes or programing for fun. During the several decades of the existence of digital media, a number of creative fields and worlds bringing together users of different platforms, used for their creative purposes, have flourished. Alongside the fields of electronic music, video games, new media art and electronic literature, there is the demoscene, separate from and not having many links with the rest of the digital world.  What is the demoscene? This phenomenon is apparent to those with advanced understanding of digital media. In the book Freax. The Brief History of Computer Demosceneit is stated that “almost all modern art genres have an underground stream that can not be found anywhere, or bought in shops, and only insiders know of its existence.” (Polgár, 2005: 6) Adjectives such as illegal, grassroots, independent are often related with this field and practice. The term itself is derived from the word “demonstration” and refers to the demonstration of the capabilities of a platform and the skills of a programmer. A basic understanding of the demoscene will treat it as “a subculture in the computer underground culture universe, dealing with the creative and constructive side of technology\" (Demoscene FAQ).   The reasons, however, for telling the story of this particular platform through this perspective are several. Among the platforms, the 8-bit Spectrum is widely considered to be the cheapest(this aspect is important to taking up the issues of accessibility, universality and democratic nature of the platform). This is a very important factor considering the fact that the prices of other platforms could be added up to a number of monthly salaries. One of the objectives of the demosceners is to circumvent the technical limitations of the platform. Demosceners fell in love with the ZX Spectrum, more or less because it is recognized as a platform characterized by the simplest technical solutions, so it was natural to perform impossible operations on it.  Our proposal can be compared above all to recent approaches from thebook series on platforms studies. To indicate the most recently published, among them are  Now the Chips Are Down: The BBC Micro  Alison Gazzard(2016, MIT Press, due to the British local context) and  The Future Was Here: The Commodore Amiga(2012, MIT Press) by Jimmy Maher. Another work that addresses the issues of a local approach to personal computer is an  Electronic Dreams: How 1980s Britain Learned to Love the Computerby Tom Lean (Bloomsbury Sigma, 2016). The book by Tom Boellstorff, Bonnie Nardi, Celia Pearce & T. L. Taylor,  Ethnography and Virtual Worlds: A Handbook of Method(Princeton University Press, 2012), mainly the ethnographic research by T. L. Taylor, is a reference point for our ways of working with the community that uses digital media.   Our paper will present the findings of a two year research project devoted to the platform. The research work will involve semi-structured interviews (with 20 demosceners from Russia, the Czech Republic, Poland, Slovakia), centered around the creative possibilities of the platform.  ",
       "article_title":" ZX Spectrum, or Decentering Digital Media Platform Studies approach as a tool to investigate the cultural differences through computing systems in their interactions with creativity and expression  ",
       "authors":[
          {
             "given":"Piotr",
             "family":"Marecki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Michał",
             "family":"Bukowski",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Robert",
             "family":"Straky",
             "affiliation":[
                {
                   "original_name":"GH University of Science and Technology, Poland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "hacker culture",
          "computer science",
          "creative and performing arts",
          "media archaeology",
          "cultural studies",
          "diversity",
          "English",
          "including writing",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The purpose of this ten-minute presentation is to present this thematic site which we have constructed in 2016 (and is bilingual) and is of use for professors, students, and general public. The object this academic web page (the first of its kind in this specific field) is to provide a large amount of documents (including links to over 200 working papers) historical statistics (over 600 Excel charts and graphs), bibliographies, guides to archival sources and short historical summaries of the banking histories of many countries in Latin America as well as Spain. Both El Colegio de México and the University of Cantabria (Spain) have collaborated in this project under the direction of Dr. Carlos Marichal. The site will soon be transferred to the electronic resources of the Libraries of both academic institutions.   I argue that this thematic webpage corresponds to an increasing trend in contemporary academics to  combine  concrete and deep research results in  subdisciplines  with complementary resources of a varied nature, including historical statistical series, reference texts, images (photos and engravings), timelines, and resources for teaching.    Such resources are especially useful for consultation on line by professors and students in local universities, many of which – in Mexico and Latina America- do not have really rich library/digital resources In addition I might remark that there is a demand from schools and universities for advanced online courses in humanities and social sciences that can be especially useful for updating university professors, especially in the provinces, where there is urgent need for support to achieve a substantial improvement in teaching and research in humanities and social sciences in Mexico or other countries in the region.    To accomplish this, a multidisciplinary working group of academics has been set up to gather pertinent information from the various humanities and social sciences in the field of banking and financial history of Latin America and Spain, which explains the international consortium engaged. The interest of the project lies in the pioneering projects in this field in the humanities and social sciences both in academia in Mexico and elsewhere. The site can be consulted at    http://codexvirtual.com/hbancaria/  ",
       "article_title":"Presentation Of Web Site On The Banking And Financial History Of Spain And Latin America",
       "authors":[
          {
             "given":"Carlos",
             "family":"Marichal",
             "affiliation":[
                {
                   "original_name":"El Colegio de Mexico, Mexico",
                   "normalized_name":"College of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01vp99c97",
                      "GRID":"grid.462201.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "project design",
          "organization",
          "databases & dbms",
          "spanish and spanish american studies",
          "management",
          "public humanities and community engaged scholarship",
          "historical studies",
          "resource creation",
          "English",
          "and discovery",
          "digitisation",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Accurate information about the human population distribution is essential for formulating informed hypothesis in the context of several social, economic, and environmental issues. Government instigated national censuses are authoritative sources of population data, subdividing space into discrete areas (e.g., fixed administrative units) and providing multiple snapshots of society at regular intervals, typically every 10 years. Many research institutions or national statistical offices have developed historical Geographical Information Systems (GIS), containing statistical data from previous censuses together with the administrative boundaries (i.e., records of administrative boundary changes) used to publish them over long periods of time. However, using these data can still be quite challenging, particularly when looking at changes over time. There are multiple reasons why population data aggregated to administrative units is not an ideal form of information about population counts and/or density. First, these representations suffer from the modifiable areal unit problem (Lloyd, 2014), which states that the results of an analysis that is based on data aggregated by administrative units may depend on the shape and arrangement of the units, rather than capturing the theoretically continuous variation in the underlying population. Second, the spatial detail of aggregated data is variable and usually low, particularly in the context of historical data. In a highly aggregated form these data are useful for broad-scale assessments, but using aggregated data has the danger of masking important local hotspots, and overall tends to smooth out spatial variations. Third, there is often a spatial mismatch between census areal units and the user-desired units required for particular types of analysis. Finally, the boundaries of census aggregation units may change over time from one census to another, making the analysis of population change, in the context of longitudinal studies dealing with high spatial resolutions, difficult. Given the aforementioned limitations, high-resolution population grids (i.e., geographically referenced lattices of square cells, with each cell carrying a population count or the value of population density at its location) are often used as an alternative format to deliver population data. All cells in a population grid have the same size and the cells are stable in time. There is no spatial mismatch problem as any partition of a given study area can be rasterized to be co- registered with a population grid. Population grids can be built from census data through the application of spatial disaggregation methods (Monteiro et al., 2014), which range in complexity from simple mass- preserving areal weighting, to intelligent dasymetric weighting schemes that leverage regression analysis to combine multiple sources of ancillary data. Nowadays, there are for instance many well-known gridded datasets that describe the modern population distribution, created using a variety of disaggregation techniques (e.g., the Gridded Population of the World (Doxsey-Whitfield et al., 2015) or the WorldPop databases (Tatem, 2017)). However, despite the rapid progress in terms of disaggregation techniques, population grids have not been widely adopted in the context of historical data. We argue that the availability of high-resolution population grids within historical GIS has the potential to improve the analysis of long-term geographical population changes. Perhaps more importantly, this can also facilitate the combination of population data with other GIS layers to perform analyses on a wide range of topics, such as the development of the transport network, historical epidemiology, the formation of urban agglomerations, or climate changes. This work reports on experiments with a hybrid disaggregation technique that combines the ideas of dasymetric mapping and pycnophylactic interpolation (Monteiro et al., 2014), using machine learning methods (e.g., linear regression models, ensembles of decision trees, or deep learning approaches based on convolutional neural networks, which previously have only seldom been used for spatial disaggregation (Robinson et al., 2017)) to combine different types of ancillary data (e.g., historical land-coverage data from the HILDA project (Fuchs et al., 2015), together with modern information that we argue can correlate with historical population), in order to disaggregate historical census data into a 200 meter resolution grid. Apart from few exceptions related to the use of areal interpolation for integrating historical census data, most previous related studies have focused on modern datasets. We specifically report on experiments related to the disaggregation of historical population counts from three different national censuses which took place around 1900, respectively in Great Britain, Belgium, and the Netherlands. All three statistical datasets, together with the corresponding boundaries for the regions at which the data were collected (i.e., parishes or municipalities), are presently available in digital formats within national historical GIS projects. The obtained results indicate that the proposed method is indeed accurate, outperforming simpler schemes based on mass-preserving areal weighting or pycnophylactic interpolation. Moreover, the obtained results also show that modern data, particularly pre-existing gridded datasets that describe the modern population distribution (i.e., data from the Gridded Population of the World (Doxsey-Whitfield et al., 2015) project), are particularly useful as features for supporting the disaggregation of historical population counts. The best results were obtained with regression models leveraging multiple features (i.e., different models attained the best results in each of the three national territories that were considered), although a simple dasymetric technlque, leveraging the modern population gridded data to define the disaggregation weights, achieved very competitive results. Acknowledgements This research was partially supported by the Trans-Atlantic Platform for the Social Sciences and Humanities, through the Digging into Data project with reference HJ-253525. The researchers from INESC-ID also had financial support from Fundação para a Ciência e Tecnologia (FCT), through the project grants with references PTDC/EEI-SCR/1743/2014 (Saturn) and CMUPERI/TIC/0046/2014 (GoLocal), as well as through the INESC-ID multi-annual funding from the PIDDAC program, which has the reference UID/CEC/50021/2013. ",
       "article_title":"Spatial Disaggregation of Historical Census Data Leveraging Multiple Sources of Ancillary Data",
       "authors":[
          {
             "given":"João Miguel",
             "family":"Monteiro",
             "affiliation":[
                {
                   "original_name":"University of Lisbon, IST and INESC-ID, Portugal",
                   "normalized_name":"University of Lisbon",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/01c27hj86",
                      "GRID":"grid.9983.b"
                   }
                }
             ]
          },
          {
             "given":"Bruno Emanuel",
             "family":"Martins",
             "affiliation":[
                {
                   "original_name":"University of Lisbon, IST and INESC-ID, Portugal",
                   "normalized_name":"University of Lisbon",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/01c27hj86",
                      "GRID":"grid.9983.b"
                   }
                }
             ]
          },
          {
             "given":"Patricia",
             "family":"Murrieta-Flores",
             "affiliation":[
                {
                   "original_name":"University of Lancaster, United Kingdom",
                   "normalized_name":"Lancaster University Ghana",
                   "country":"Ghana",
                   "identifiers":{
                      "ror":"https://ror.org/01g3dya06",
                      "GRID":"grid.472314.7"
                   }
                }
             ]
          },
          {
             "given":"João Moura",
             "family":"Pires",
             "affiliation":[
                {
                   "original_name":"Universidade NOVA de Lisboa, FCT / NOVA LINCS, Portugal",
                   "normalized_name":"Universidade Nova de Lisboa",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02xankh89",
                      "GRID":"grid.10772.33"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "technologies",
          "computer science",
          "geohumanities; spatial & spatio-temporal analysis",
          "artificial intelligence and machine learning",
          "modeling",
          "visualization",
          "English",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Our project will make freely available a searchable webapp, built in eXist-db (  http://exist-db.org/exist/apps/homepage/index.html ), containing a database of poems responding to Lancashire Cotton Famine of 1861-65, along with audio recitations and musical performances drawing directly on these poems (Rennie, 2017). This poetic response is important in that it often represents labouring-class voices from the mid-nineteenth century, which, in spite of renewed academic interest in such material, remain underappreciated (Goodridge et al, 2012 provides a useful introduction and bibliography). The study of this material and its digital publication will significantly enrich literary scholarship and historical perspectives of this economic crisis, and provides the opportunity to draw public attention to an episode of history that is little known beyond the scholarly sphere. The project seeks to establish a much more detailed understanding of the nature of Lancashire Cotton Famine poetry: its extent, its intents, and its functions. To date, there is no critical literature specifically addressing the poetry written and published during the Cotton Famine, though the period is touched upon in Brian Hollingworth’s anthology  Songs of the People: Lancashire dialect poetry of the industrial revolution (1972: 98-113).  The project draws predominantly on the local newspaper collections of Lancashire’s various civic archives and local studies centres for material. Though some of these newspapers have been digitised in collections such as the British Newspaper Archive and Gale Historical Newspapers, the majority are in hard-copy or microfilm format, so that users attempting to access the poetry encounter practical obstacles relating to both geography and the preservation of materials. Additionally, as the archives and local studies collections in Lancashire are significantly under-resourced, there are long-term concerns about maintenance of the equipment and staffing levels required to ensure that access to these significant historical collections is sustainable. The recovery element of our project therefore aims to ensure long-term, free-of-charge access to a near-complete repository of Cotton Famine Poetry without the requirement to visit multiple archives or local studies centres. As the recovered poems have been transcribed by hand, we are also avoiding replicating the Optimal Character Recognition errors which have been incurred by some of the existing newspaper databases (Joulain-Jay, 2016). Alongside the vital recovery and collation of this material, the experience of the investigators in the field of labouring-class poetry enables a simultaneous critical analysis of the poetry as it emerges, focussing on local, regional, national, and international fields of interest. We encounter a wide range of poetic styles, written both in Lancashire dialect and standard English, which demonstrate the sophisticated literary engagements of their authors. In terms of subject matter, the poems describe not only the direct, local experience of the Cotton Famine, but also offer more abstract reflections on issues including work, poverty, war, slavery and abolition. We want to determine the extent to which political dissent is present in the poetry, and to what degree opposing discourses relating to slavery and the American Civil War were articulated through literature of this type. We are already beginning to establish that a significant proportion of Cotton Famine poetry represents a labouring-class address to a regional and national middle-class readership, and part of our analysis will involve mapping a transatlantic discourse between the Lancastrian labouring classes and writers on both sides of the American conflict. The popular narrative of the Cotton Famine has Lancashire textile workers staunchly supporting the North in spite of the deprivation caused by the war, because of their strong support for abolition. Early analysis of poetic responses suggests a more complicated engagement with the American conflict, including some elements of support for the south (see also Ellison, 1972). We hope that by making the poetry freely available online, we encourage its further use as an important historical and literary resource for understanding some of these complex themes.  The digitisation of a large and varied body of work such as this presents both challenges and possibilities, and this paper will reflect upon the difference that digital methods make to our interpretation of the material and the ways in which it can be used. The process of marking up text for the database enables us to make our editorial decisions in presenting this body of work transparent, and to group similar poems and themes for the reader’s ease of analysis. Nonetheless, in so doing, we impose our own interpretations upon what was a fluid form - often orally transmitted, and published in different versions across different media. A key concern in presenting this material has been the desire to ensure its usefulness for scholars who might have different approaches and questions to our own. In forcing us to grapple with these challenges, the use of digital methods has encouraged us to make explicit our own methodologies and thought processes, and enabled the creation of a resource that could be considered more intellectually ‘open’ than the traditional analogue anthology.  The design of our webapp therefore reflects our desire for a flexibility which in turn offers better representation of a literature which was originally available in multiple, sometimes changing forms. Some poems were written to be read or sung aloud, while others endeavoured to capture in writing the transient forms of local dialect, and different variations of the same poem appear across the newspapers. The use of XML enables us to continuously add layers of interpretation as they occur in the data for macro analysis, while marking up at word-by-word level enables a careful close reading in which we are forced to be conscious of the decisions we are making about the presentation of material. An important part of the project is its public-facing element, including the involvement of school groups in finding and transcribing the poetry. We also welcome submissions of potential Cotton Famine poetry from members of the public, local historical societies, and educational projects with an interest in this material. Managing the upload and editing of these submissions, and ensuring that appropriate credit is given, is one of the tasks that the project team has taken on, and it is likely to present its own set of challenges: gathering this data provides an opportunity to involve the public in undertaking research and giving them insights into this process, but we also need to ensure that the results are useful and worthwhile for the project’s outputs. At present we are not planning to train people beyond the team in how to encode in TEI, but giving contributors a ‘behind the scenes’ tour of the database and offering an introduction to how we create and structure our digital materials (and why) has the potential to enable further discussions and may encourage contributors to get involved with digital humanities activity beyond our immediate project. We feel that this is an important step in ensuring that contributors gain an understanding of what happens to their data once they submit it, and how it is transformed into what they see in the final digital publication. We will discuss these challenges and how we intend to maintain interest and engagement amongst our contributors.     At this stage of the project, in which we are making key decisions about how to manage our own data and that from our external contributors, we would welcome discussion and comments from the wider digital humanities community on how we can ensure that our resource is an effective tool for both research and teaching. We hope, too, that the challenges that we are encountering and some of our proposed solutions might prove helpful for others working with comparable datasets or audiences. ",
       "article_title":"The Poetry Of The Lancashire Cotton Famine (1861-65): Tracing Poetic Responses To Economic Disaster",
       "authors":[
          {
             "given":"Ruth",
             "family":"Mather",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "digital humanities history",
          "historical studies",
          "resource creation",
          "english studies",
          "English",
          "archives",
          "and discovery",
          "digitisation",
          "theory",
          "epistemology",
          "literary studies",
          "criticism",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   What can be said, at an empirical level, about the internal structure of literary narratives? Can we model the “shape” of a plot? In recent years, there has been a surge of interest in what might be thought of as a computational form of narratology. Instead of flattening out the text into an unordered bag of words, a series of studies have looked at the fluctuation of different types of literary signals across “novel time,” the linear space between the beginning and end of a text. Most well-known is probably Matt Jockers' work with Syuzhet, an R package that calculates the dispersion of positive and negative sentiment across novels. Ben Schmidt, working with a corpus of movie and TV scripts, tracked the distribution of topics across the screenplays, finding a footprint of the prototypical cop drama, with a crime at the beginning and a trial at the end. Andrew Piper, writing in New Literary History, identifies a signature for the “conversional” narrative, based on Augustine's  Confessions , and then traces this signal forward in literary history. At the Stanford Literary Lab, Holst Katsma tracked the “loudness” of speech utterances across chapters in Dostoevsky and Austen. And Mark Algee-Hewitt, working with collaborators on the Suspense project, has trained a classifier that can score the “suspensefulness” of passages of text at different regions across narrative time.  Each of these signals -- sentiment, topic, suspense, loudness -- is a fascinating object of study in its own right. But what are they signals of? How do they interact with one another? Do they track distinct literary phenomena, or are they moved by the same underlying forces? Are they hierarchically related to one another -- is “conversion” a subset of “topic”? Does “sentiment” encompass “suspense”? From among the infinite proliferation of threads that weave through the text, why should we select these? Are they the most explanatory, the most cross-cutting, the most fundamental? Or are there any fundamental, cross-cutting aspects to narrative at all? Far from trying to offer definitive answers to these questions, we explore a minimalist, bottom-up approach, with the goal of providing a basic corpus-linguistic survey of the internal structure of English language novels -- we are interested in a very simple treatment of the question, with the aim of providing framing and context for higher-level studies. Instead of starting with a relatively complex signal like sentiment or suspense, we just look at the distributions of individual words across narrative time in a corpus of ~50,000 novels, and then systematically identify the words that have the most non-uniform distributions across narrative time when averaged across the entire corpus -- words that are most skewed, the most distinctive of beginnings, middles, ends, and anything in between. This is extremely simple, essentially just a particular way of counting words. Working with Gale’s American Fiction corpus (~18k novels from 1820-1940), a subset of cleaned texts from the Chicago Novel Corpus (~7k American novels from 1880-2000), and a subset of HathiTrust (a sample of 20k works identified as fiction by Underwood et al.), we split each text into a set of N equally-sized chunks and then count the total number of times that each word appears in each of these chunks across all novels. For example -- the word “love” appears 9,418 times in the first 1/100th of novels in the corpus, compared to 25,132 in the last percentile. With this, we can represent each word as a distribution across narrative time and compare the variance to what would be expected, given the frequency of the word, under a uniform distribution -- the baseline variation that we would expect if the frequency of the word had no significant relationship with the position inside the narrative. This gives a simple way to score each word, to quantify the degree to which it tends to cluster in some regions of the narrative at the expense of others. With content words, many of these results confirm basic intuitions about genre conventions and the pragmatic requirements of storytelling. For example, beginnings are filled with descriptions of people, places, and things; birth, youth, education; and enumerations of family relationships. Guns, death, war, and criminal justice peak right around 95%, the moment of climax and peak action. Endings are marked by marriage, death, and expressions of emotion, both happy and sad.     Education, guns, and marriage across ~30k novels. (Gale American Fiction, Chicago Novel Corpus)   Other words have patterns that are somewhat less self-evident. For example -- words related to food, eating, talking, and female characters peak strongly around the 10-20% marker in the novel, as if -- once the cast of characters is introduced at the start, it's common novelistic practice to sit them down and put them in conversation together over a meal, as an early set-piece. Or, at the 50% marker -- novelistic middles seem to be dominated by speaking and thought, words related to dialogue and psychological experience.     Food and conversation at ~20%; dialogue at ~50%. (Gale American Fiction, Chicago Novel Corpus)   More interesting, though, it turns out that function words -- including many of the most frequent words in English -- also have very irregular distributions across narratives when averaged across tens of thousands of texts, often in ways that seem to suggest the presence of low-level (and highly consistent) narratological tendencies that sit well below the conventions of genre or plot. For example, the indefinite articles “a” and “an” fall off dramatically across narrative time -- they are overrepresented at the start, fall off quickly in the first 20%, decline more slowly across the middle, and then fall quickly again at the end.     “A” and “an” across ~30k novels. (Gale American Fiction, Chicago Novel Corpus)   Since “a” carries very little semantic content, the interpretation of this is more complex. Perhaps this is related to the fact that “a” is used when an entity is referred to for the first time, when it is “unfamiliar” in the narrative frame? For instance, to use an example from Abbot (2006), we might first say -- “Mary saw a movie last week” -- before then switching to the definite “the,” once the entity has been introduced -- “The movie was not very interesting.” It seems plausible, then, that “a” would be in higher demand at the beginning, when everything is unfamiliar and the fictional world needs to be described for the first time. If this is the narratological role of “a” -- can we treat it as a marker for a general notion of “speed” or “motion” in narrative, the degree to which the text is moving into new fictive contexts that need to be described for the first time? The distribution of “the,” though, is neither the same nor precisely the opposite of “a,” and seems to be marking some different configuration of narrative pressures. “The” is high at the start, like “a,” but with a much faster falloff; then flat across the middle, and with a significant rise at the end.     Indefinite vs. definite articles. (Gale American Fiction, Chicago Novel Corpus)   This seems to muddy the picture. “A” seems to be marking something about how beginnings and ends are different, whereas “the” is marking something about how they are similar. But how precisely, and why? Why do “a” and “the” diverge at the end? These trends are highly variable across the ~50 most frequent words in English, often in ways that seem to suggest a kind of basic taxonomy of narrative variation, a set of lenses for thinking about the ways in which narratives can change across the axis of the text. For example, “and” and “or” also separate cleanly at the end:     “And” vs. “or.” (Gale American Fiction, Chicago Novel Corpus)    Where, perhaps, “or” tends to introduce a state of indeterminacy, a potential fork, and thus falls off as the text approaches the end, as the “circle” of the plot comes to a close, as James would say? Or, less intuitive -- personal pronouns tend to increase across the narrative. But, the object pronouns “him” and “her” rise more steeply than the subject pronouns “he” and “she” -- so, as the narrative progresses, people increasingly become grammatical  objects ? People do things at the beginning of stories, and increasingly have things done to them at the end?      Object pronouns rise more steeply across the narrative than subject pronouns. (Gale American Fiction, Chicago Novel Corpus)   This paper will investigate these patterns and attempt to provide linguistic and literary explanations for why they look the way they do, with a focus on the differences between “a” and “the.” Overall, we find that the distributions are highly consistent across corpora (Gale, Chicago, HathiTrust), date of publication (1850-2000), and available metadata for canonicity and author identity:     The distribution of “a,” sliced by corpus, publication date, author gender, and canonicity. (Gale American Fiction, Chicago Novel Corpus)       The distribution of “the,” sliced by corpus, publication date, author gender, and canonicity. (Gale American Fiction, Chicago Novel Corpus)   We also find that the patterns are significantly different than trends observed in a corpus of ~20k nonfiction volumes from HathiTrust (Underwood et al., 2015), which suggests that they mark something specific about the structure of (fictional) narratives, and not just something that arises generally in long documents. Beyond the aggregate trends -- we explore the degree to which individual texts do and don’t conform to the corpus-level averages, with a focus on what can be learned from the extreme examples that most strongly exemplify and resist the overall trends -- for example, the ~1% of novels for which “a” increases consistently across the entire text at a statistically significant level. ",
       "article_title":" Distributions of Function Words Across Narrative Time in 50,000 Novels  ",
       "authors":[
          {
             "given":"David William",
             "family":"McClure",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Scott",
             "family":"Enderle",
             "affiliation":[
                {
                   "original_name":"University of Pennsylvania",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "computer science",
          "linguistics",
          "text analysis",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The Research Environment for Ancient Documents (READ) project commenced in 2013 with development support from a consortium of institutions (University of Washington in Seattle, Ludwig Maximillian University in Munich, University of Lausanne, University of Sydney and Prakaś Foundation) involved in the study and publication of ancient Buddhist documents preserved in Gāndhārī. READ has been developed as a comprehensive multi-user platform for the transcription, translation and analysis of ancient Sanskrit, Gāndhārī, Pali and other Prakrit texts: manuscripts, inscriptions, coins and other documents. It is based on open source software (Postgres, PHP and JQuery), supports the TEI standard and provides an API for integration with related systems. READ is positioned as a research environment, complementary to existing textual repositories and integrated with existing dictionaries. Existing transcriptions can be imported, elaborated upon, analyzed, and then published as research output in standards-based formats. The defining innovation of READ is atomization to a semantically linked network of objects; a paradigm shift in data structure from strings of marked-up text to sequences of linked objects. The underlying design and entity model was presented at both the Digital Humanities conference in 2015 and the 2016 Australian Digital Humanities Conference. READ has been publicly released and is supporting a wide range of corpus development projects. Whilst this presentation will follow on to briefly precis the range of research projects currently supported by READ, the focus will be on a related platform. READ Workbench (Workbench) is a server portal hosted at the University of Sydney since 2016 to ‘harness’ READ. Developed using the same technology stack as READ, it is a comprehensive management framework to support the integration of people and processes in the collaborative development, maintenance and publishing of textual corpora. The design of Workbench evolved organically as the implementation requirements of READ expanded from a single researcher working on a single text, to the capacity to support multiple projects, each with a team of researchers collaborating on the development of an integrated corpora, with widely divergent research objectives. The fundamental objective was to design a supporting framework with which manage the balance between autonomy and collaboration in large scale projects. The approach adopted was to implement strategies, models and workflow patterns consistent with those applied in the IT consulting industry to digital content design, development and migration projects. Workbench is a software as a service (SaaS) platform managing multiple READ installations, each with project and language specific configurations, supporting researcher collaborations across multiple institutions. It provides a self-service portal for researchers to develop, maintain, manage and publish texts without requiring technical support or the mediation of a database administrator; critical to the longer-term sustainability of corpora projects. Workbench’s three facets (configuration management, database management and corpus workflows) provide a scalable implementation architecture for READ and instantiate a comprehensive corpus development methodology. Whilst the configuration management services might be bracketed as conventional for a SaaS platform, database management is predicated upon an architectural innovation that enables researchers to build, share, manage, maintain and publish their own texts. The adoption of a single text/single database (TextBase) as the fundamental object of development, collaboration and portability is quite a departure from conventional models where a centralized administrator manages a single monolithic corpus database.  This TextBase architecture underpins a corpus development, analysis and publishing methodology that provides significant flexibility in terms of the iteration and synchronization of three fundamental workflows: text alignment, analysis registration and text aggregation. The text alignment process integrates image, text and model configuration data to automatically generate a database. This approach allows for the distribution of responsibility to specialists and integration of their research output to align the image and the transcription at their most atomized to generate a ‘substrate’. Rather than requiring researchers to command exacting mark-up schemas, substrate databases can be automatically generated from Word processing and Spreadsheet inputs. Workbench enables each of the specialist roles to work independently and their contributions be separately managed and integrated, ameliorating project risk by minimizing dependencies and bottlenecks. The analysis registration process synchronizes analysis data with an existing substrate. This approach allows a researcher to work independently and externally to READ in developing their own analysis ‘strata’ and then register that strata on a substrate. Grammatical analysis, translation, semantic, syntactic and structural analysis can all be independently developed and iteratively registered. Researchers from other disciplines can develop and register their own analysis (archaeological, historical etc.). Each stratum is registered on a particular substrate (an edition) of the text within a TextBase, is separately owned and attributed, and its visibility is controlled by the researcher registering it. The text aggregation process allows individual researchers to work and innovate in private to the point where they elect to participate in research collaborations or their text is ready for publishing. A TextBase might be aggregated with others to form a ‘sequenced’ collection, a ‘mapped’ collection or a ‘merged’ corpus; a continuum expressing an increasing degree of synthesis and harmonization of analysis ontologies and methodological standards. Researchers may contribute their TextBase to any number of aggregates. This approach allows a researcher to align a TextBase with the analysis standards of an established corpus as a predicate to participation as a constituent of that merged aggregate. In parallel, that same TextBase might be mapped to the analysis ontology of an entirely different collection. The potential exists for the same TextBase substrate to manifest as a constituent of separate aggregates, with alternative configurations of registered analysis strata, supporting widely divergent (aggregate specific) research objectives; the emanation of multiple TextBase avatars. The strategy adopted with Workbench was to design a solution architecture within which to reframe some of the ubiquitous issues in the conventional corpus development model; ownership, control, confidentiality, innovation, standardization, portability, resourcing and support. The critical innovation in maximizing development flexibility and in balancing autonomy and collaboration across the range of individual, collection and corpora development projects is the TextBase; the target of text alignment, the substrate for registration of analysis and the object aggregated. ",
       "article_title":"READ Workbench – Corpus Collaboration and TextBase Avatars",
       "authors":[
          {
             "given":"Ian",
             "family":"McCrabb",
             "affiliation":[
                {
                   "original_name":"University Of Sydney, Australia",
                   "normalized_name":"University of Sydney",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/0384j8v12",
                      "GRID":"grid.1013.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "philology",
          "project design",
          "corpora and corpus activities",
          "organization",
          "scholarly editing",
          "management",
          "text analysis",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The nascent field of queer game studies has expanded exponentially in recent years thanks to the work of scholars such as Adrienne Shaw, Bonnie Ruberg, and Edmond Chang. This growth in scholarship has paralleled a significant rise in LGBTQ representation in games, including games such as  Gone Home,  The Vanishing of Ethan Carter,  Dream Daddy, and others. Yet, despite growing representation and scholarly attention to queer characters and players, queer game studies continues to face the multivalent marginalizations of queer folks and their experiences in gaming. A prime example of this marginalization is the difficulty of preserving queer gaming cultures: queer representations and gaming communities are recorded largely in ephemeral, unofficial digital forms such as wikis, blogs, and fan-made websites due to a lack of access to mainstream platforms that often minimize and reject queer perspectives and desires. There is some advantage to these forms in that they allow queer gamers to create online communities as “counterpublics,” which are communities defined against normative rules and expectations, but this means that queer gaming cultures are also in constant danger of being ignored, becoming outdated, or disappearing suddenly due to lack of resources (Warner, 2002).    A case in point is   GayGamer.net , a website dedicated to game news, commentary, and community for LGBTQ gamers that went dark without notice in May 2016.   GayGamer.net  was a valuable resource for documenting LGBTQ game characters and communities, and while parts of it were captured by the Internet Archive, much of the site is no longer accessible outside of an old Facebook page (  GayGamer.net ). While many digital objects face similar issues of compatibility and archiving, queer game artifacts and documentation are especially endangered because of the marginalized status of queer gamers and characters in gaming culture. With fewer individuals (almost all volunteers) and institutional resources to support them, these sources must be actively preserved now before they—and crucial LGBTQ cultural heritage with them—are lost.   The LGBTQ Video Game Archive, founded by Adrienne Shaw at Temple University, was created to address these issues by collecting LGBTQ representations from the 1980s to the present in order to “offer a record of how characters are explicitly coded, what creators have said about these characters, as well as how fans have interpreted these characters” (Shaw). The archive aims to allow easy, comprehensive access to queer gaming sources for queer game scholars, queer gamers, and the interested public, and further to ensure that these sources remain available in the future. To this end, one of the archive’s current projects is an ongoing preservation effort in association with the Strong National Museum of Play that seeks to save copies of the many online media artifacts that document queer gaming cultures. The archive’s preservation project demonstrates how archiving can be used to further social justice projects in digital spaces by safeguarding the cultural productions (including personal blogs, community forums, wikis, etc.) of marginalized peoples. As part of this presentation, I will share the process I developed for collecting and storing the websites, images, and videos referenced in the archive, and then transferring these materials to the Strong for permanent storage and public access. Using a combination of browser plugins and command line tools such as wkhtmltopdf and youtube-dl, the sources are saved as common file types that are entered into an Omeka database. The database allows the Strong to make the files publicly accessible, and the common file types allow for easier maintenance and curation of the collection. This process could be of use to other scholars and activists working to collect, curate, and sustain digital cultural resources, especially those significant to marginalized communities.   By preserving this cultural heritage, the LGBTQ Video Game Archive allows for new analyses of queer gaming culture and representation that highlight ongoing issues and emerging possibilities in games. For example, Utsch et al. used the archive to create data visualizations of queer representation throughout video game history, and revealed several trends such as a predominance of gay men in LGBTQ representation and an exponential growth in overall number of representations (Utsch et al., 2017: 7). To date, however, an intersectional analysis of the archive that addresses sexuality alongside identity categories of race, class, or disability has not been attempted, and this paper presentation will address these intersections using new interactive data visualizations. Completing these visualizations required revisiting each representation in the archive and recording additional data about the character’s identity. The visualizations are interactive in order to make them more fluid and dynamic: in other words, to make them better representations of identity than the static categorizations that intersectionality has sometimes been accused of (Puar, 2005: 125). This intersectional analysis of the archive is only the beginning of the archive’s potential, and it has a number of limitations. For example, it only includes games currently in the archive, and only what is observable and documented about each representation. Future work will add more games to the analysis, and provide more granular analysis of particular genres, developers, and intersectional identities in games. Together, preservation and critical analysis are essential tools for developing archival practices that support social justice in digital humanities, and both are much needed forms of public, academic, community-oriented activism.   In sharing the LGBTQ Video Game Archive’s ongoing efforts to preserve and visualize queer representation in games, this paper presentation calls for increased attention in digital humanities to the needs of marginalized groups such as queer gaming communities. Concepts and design practices such as imagining a QueerOS can help guide the field’s attempts at better inclusion, but we as digital humanities scholars can and must do more (Barnett et al., 2016). As we build and make with our digital tools, we must constantly confront the question of who we are building and making for. I argue that digital humanities should be the digital theories and practices of social justice, and it should do the crucial work of engaging with communities and supporting their efforts to make and shape themselves. Representation in queer games and queer gaming communities provides some practical methods for doing so, and contributes to ongoing discourse of what digital humanities can be. ",
       "article_title":"Preserving and Visualizing Queer Representation in Video Games",
       "authors":[
          {
             "given":"Cody Jay",
             "family":"Mejeur",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "audio",
          "repositories",
          "sustainability and preservation",
          "queer studies",
          "film and media studies",
          "video",
          "cultural studies",
          "english studies",
          "archives",
          "English",
          "multimedia",
          "games and meaningful play"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Providing scholarly access to large collections that are distributed across various content providers, and to create user-friendly applications to work with that data in a diversity of scholarly projects is the underlying goal of the development of the Common Lab Research Infrastructure for the Arts and Humanities (CLARIAH [1]).  Big-scale infrastructure projects in the humanities and social sciences such as the Digital Research Infrastructure for the Arts and Humanities (DARIAH) (Edmond et al., 2017), or the Common Language Resources and Technology Infrastructure (CLARIN) (Hinrichs and Krauwer, 2014) aim to provide solutions for both preservation and access to collections and data necessary for scholarly research  (Zundert, 2012). Some infrastructure projects build decentralized “atomic” software services, e.g., as in the LLS infrastructure project (Buchler et al., 2016), while others prefer to build more centralized virtual research environments, as in the European Holocaust Research Infrastructure (EHRI) (Lauer, 2014). Also, even within a single infrastructure project, these two models can coexist. This is the case of the CLARIAH infrastructure, where different approaches have been taken to date for serving different user groups, i.e., several specialized tools for linguists (Odijk, Broeder & Barbiers, 2015), or a research environment (the  Media Suite) that serves the scholarly needs for working with audiovisual data collections and related mixed-media contextual sources that are maintained at cultural heritage and knowledge institutions. This paper discusses the rationale and challenges behind the development of the  Media Suite.    The CLARIAH Media Suite  Whereas in some domains of scholarly research the focus is on the creation of private data collections, in other domains scholarly research focuses on already  established data collections maintained at heritage and knowledge institutions. Access to and use of these latter collections is often restricted, especially when they concern audiovisual media, due to intellectual property rights (IPR) or privacy issues (e.g., with respect to recorded interviews). Therefore, many scholars end up using collections that are openly available. Or, they spend considerable amounts of time in doing on-site visits to archives where data are available for consultation. Data collections at these institutes are “locked,” scattered, or at least hard to use for scholarly research.  To open up these collections for research and let scholars take advantage of the sheer quantity and richness of these data sets, we developed the  Media Suite (Figure 1)  [2] , a research environment or high-level tool that works as a data aggregator where the metadata and the media content can be explored, browsed, analyzed, stored in personal collections, annotated manually, enriched automatically, and visualized, and where the user annotations can be exported.     Figure 1. The CLARIAH Media Suite’s homepage ( , version 2)  The ultimate goal of the  Media Suite is to: (i) enable distant reading (Schulz, 2011), that is,  identifying patterns or new research questions in all aggregated collections; (ii) facilitate close reading: the  detailed examination of individual items (e.g., videos) in a collection or parts of these items (e.g., video segments) during search and scholarly interpretation, and (iii) make sure that the “scholarly primitives” (Unsworth, 2000, also described as an infrastructure framework in Blanke and Hedges, 2013) are well supported.  Even though these are accepted scholarly approaches that should be taken into account by infrastructure projects in the humanities nowadays, the question is: How to facilitate “close reading” when the media objects cannot be accessed because of copyright issues? How to enable “distant reading” when the content cannot be fully automatically processed or when their metadata is diverse and incomplete? How to cater to the needs of scholars with specific research questions and methods in the context of an infrastructure that has to be generic enough to be feasible?    Challenges and solutions  The approach of the CLARIAH Media Suite to tackle these challenges is: (i) to organise and implement a federated authentication mechanism to overcome access barriers (Figure 2, number 5), and (ii) to provide mechanisms that enable researchers to work with tools and aggregated data  within the infrastructure. We refer to this approach as to “bringing the tools to the data”, as opposed to “bringing the data to the tools”.     Figure 2. The building blocks of the CLARIAH Media Suite  Figure 2 shows the main elements that constitute the  Media Suite research environment. We explain below to which challenge is each element an answer to:   Data Sources -- Data Governance Institutional collection maintainers have internal data governance processes to ensure that data assets are formally managed. One important aspect covered by governance processes is licensing: who has permission to access the data. However, data governance with respect to external processes --loosely defined as being part of an 'infrastructure'-- is typically not accounted for. This means that key data governance areas such as availability (e.g., metadata can be harvested), usability (e.g., source data can be viewed), integrity (e.g., protocols are in place to handle duplication and enrichment) and security (e.g., provenance information is maintained), need to be (re)organized or (re)considered, formalized and supported by the  Media Suite and the emerging infrastructure in which it is embedded.    APIs -- Sustainable development A digital infrastructure should use existing protocols, conventions, and standards. Besides obtaining data by harvesting using the OAI-PMH protocol, or using APIs, the functionalities have been organised in a modular approach, which includes (Matínez et al., 2017): ●Components: which use the API’s to perform specific tasks. ●Tools: which incorporate a number of components in a tool. Moreover, all components and tools developed in the project are open source. In addition, the Media Suite offers public APIs, which provide mechanisms for software programmers to create functionalities using API building blocks or components. We offer a Collection API, a Search API, and an Annotation API, which provides functionality for adding data annotating existing data, using the W3C Web Annotation data model (Sanderson et al., 2017).   Components/Tools -- User friendly interaction design Developing new tools “from scratch” would be a very inefficient (and costly!) endeavour. The digital infrastructure should provide tools that are suitable both for common scholarly tasks, and for specific tasks required by each discipline. However, the digital humanities community incorporates a wide diversity of scholars with different research questions, methods, and levels of expertise in working with information processing techniques and technologies. As every infrastructure, we also have to tackle “the generalization paradox” (Zundert, 2012). We address this challenge by (i) focussing on the similarities in research methods from different disciplines (e.g., De Jong, Ordelman, Scagliola, 2011; Melgar et al., 2017), (ii) analyzing tools that support qualitative methods (Melgar & Koolen, 2018), and (ii) working with scholars as co-developers in the process [3]. The resulting functionalities are built in a modular (lego) approach that supports both flexible software development of components and user friendly interaction with assembled tools. A current challenge is to provide entity-based browsing (Verhoeven and Burrows, 2017) of both linked open data collections (RDF) and tabular data via an exploratory browser (see De Boer et al., 2017).    Enrichment services and Work Space -- Working with audio-visual content and private data In addition to IPR and privacy restrictions, access to the audiovisual content in the  Media Suite is also limited due to its nature; consisting of pixels (video) and samples (audio) and some manually generated metadata or subtitles (text). Typically, scholars want to search audiovisual data using (key)words that may be ‘hidden’ (encoded) in the pixels or the samples. This is called the semantic gap (Smeulders, 2000) that needs to be “bridged” by decoding the information in the pixels and the samples to semantic representations, e.g., a verbatim transcription of the speech or labels of visual concepts in the video (a car, a face, the Eiffel Tower), that can be matched with the keywords from the scholars. These semantic representations can be generated manually or, especially when data collections are large, automatically using automatic speech recognition (ASR) or computer vision technology.  The generation of semantic representations is addressed in different ways. On the one hand, we are currently developing an ASR service that resides within the CLARIAH infrastructure that can handle requests from the infrastructure itself (e.g., to process a data set that exists within the infrastructure), but also request from individual scholars that want to process their private collections. On the other hand, supporting manual annotation is key for interpretation in scholarly contexts (Melgar et al., 2017). The  Media Suite aims to support the generation of both ways of semantic representations in complementary ways via information workflows centered around a “Work Space” (see Figure 3) which stores private session data and enables collaboration .     Figure 3. The CLARIAH Media Suite’s Workspace      Conclusions and Future work  The paper describes the challenges found in building a sustainable, dynamic, multi-institutional infrastructure that can properly serve media scholars and digital humanists in general. We choose the approach of building a research environment that adheres to infrastructural requirements while at the same time being flexible and user-friendly. In order to ensure its used and further development after the project's lifetime, we need to carefully align the requirements of scholars with the context of the ecosystem the  Media Suite needs to live in: an ICT infrastructure hosted and maintained by multiple institutions that in turn, adheres to a diverse set of institutional requirements with respect to, for instance, data access permissions and software development and maintenance. The  Media Suite is currently functional and used by scholars doing actual research projects and will be developed further, e.g., by incorporating additional data sources (e.g., social media data), increasing metadata granularity (e.g., adding computer vision or emotion recognition), adding advanced annotation tools, and supporting missing data visualization (data critique) for heterogeneous datasets.   [1]    [2] The first of a four release versions was introduced in April 4, 2017.   [3] Indeed, an adopted strategy at the CLARIAH project level, has been to offer grants to scholars to conduct small scale research pilot projects using the CLARIAH infrastructure. In the media studies focus that we describe in this paper, almost ten scholars participate as co-developers. We follow an Agile methodology for implementation, which despite criticisms has proved to be useful for this type of projects (van Zundert, 2012)   ",
       "article_title":" Challenges in Enabling Mixed Media Scholarly Research with Multi-media Data in a Sustainable Infrastructure  ",
       "authors":[
          {
             "given":"Roeland",
             "family":"Ordelman",
             "affiliation":[
                {
                   "original_name":"University of Twente, Netherlands Institute for Sound and Vision",
                   "normalized_name":"University of Twente",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/006hf6230",
                      "GRID":"grid.6214.1"
                   }
                }
             ]
          },
          {
             "given":"Carlos",
             "family":"Martínez Ortíz",
             "affiliation":[
                {
                   "original_name":"Netherlands Escience Center",
                   "normalized_name":"Netherlands eScience Center",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/00rbjv475",
                      "GRID":"grid.454309.f"
                   }
                }
             ]
          },
          {
             "given":"Liliana",
             "family":"Melgar Estrada",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands Institute for Sound and Vision",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Marijn",
             "family":"Koolen",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for Netherlands History",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jaap",
             "family":"Blom",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          },
          {
             "given":"Willem",
             "family":"Melder",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          },
          {
             "given":"Jasmijn",
             "family":"Van Gorp",
             "affiliation":[
                {
                   "original_name":"Utrecht University",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          },
          {
             "given":"Victor",
             "family":"De Boer",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          },
          {
             "given":"Themistoklis",
             "family":"Karavellas",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          },
          {
             "given":"Lora",
             "family":"Aroyo",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"Poell",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Norah",
             "family":"Karrouche",
             "affiliation":[
                {
                   "original_name":"Erasmus University Rotterdam",
                   "normalized_name":"Erasmus University Rotterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/057w15z03",
                      "GRID":"grid.6906.9"
                   }
                }
             ]
          },
          {
             "given":"Eva",
             "family":"Baaren",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          },
          {
             "given":"Johannes",
             "family":"Wassenaar",
             "affiliation":[
                {
                   "original_name":"Netherlands Institute for Sound and Vision",
                   "normalized_name":"Netherlands Institute for Sound and Vision",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/025ae8628",
                      "GRID":"grid.425952.d"
                   }
                }
             ]
          },
          {
             "given":"Julia",
             "family":"Noordegraaf",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Oana",
             "family":"Inel",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "computer science",
          "audio",
          "repositories",
          "sustainability and preservation",
          "film and media studies",
          "video",
          "archives",
          "English",
          "interface & user experience design/publishing & delivery systems/user studies/user needs",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Building online research components for projects in the digital humanities is a common practice. However, not many researchers have a plan for these online components once the project halts or comes to an end. Consequently, many of these projects become abandoned and slowly degrade over time –some more gracefully than others. Additionally, there is a certain inherent fragility associated with software and our online research tools. In turn, this fragility threatens the completeness and the sustainability of our work over time. Previous studies have attempted to harness and manage the fragility of online resources. Studies have been carried out to address their potential reconstruction  (Klein et al., 2011), the overall decay of websites  (Bar-Yossef et al., 2004) and the decomposition of their shared resources  (SalahEldeen and Nelson, 2012). Recently, our research has been focusing on analyzing the perceptions of change in distributed collections  (Meneses et al., 2016). However, we believe that the inherent characteristics of online digital humanities projects present an interesting (and unique) area for inquiry for two reasons. First, the research aspect of digital humanities projects hinders previous approaches –as the methods for identifying change in the Web do not fully apply. And second, digital humanities projects have a limited useful life –which is accompanied by research from primary investigator, which may or may not be indicated by updates in the project’s content and tools.  We presented a paper in Digital Humanities 2017 that explored the abandonment and the average lifespan of online projects in the digital humanities  (Meneses and Furuta, 2017). However, we believe that managing and characterizing the online degradation of digital humanities projects is a complex problem that demands further analysis. In this abstract, we propose to explore further the distinctive signs of abandonment of online digital humanities projects. For this second instalment of our study we took a different direction: we departed from strictly using retrieved HTTP response codes and incorporated additional metrics such as number of redirects, DNS metadata and a detailed analysis of content features.   This study aims to answer four questions. First, can we identify abandoned projects using computational methods? Second, can the degree of abandonment be quantified? Third, what features are more relevant than others when identifying instances of abandonment? Our final question is philosophical: can an abandoned project still be considered a digital humanities project?   Methodology A complete listing of research projects in the Digital Humanities does not exist. However, the Alliance of Digital Humanities Organizations publishes a Book of Abstracts after each Digital Humanities conference as a PDF. Each one of these volumes can be treated as a compendium of the research that is carried out in the field. To create a dataset, we downloaded the Books of Abstracts corresponding from 2006 to 2016 –except for 2015 which was not available for download. We must thank and acknowledge Dr. Jason Ensor from Western Sidney University for providing us the abstracts for the 2015 Digital Humanities conference –which completes our dataset of conference abstracts. We obtained these abstracts after we had carried out our preliminary analysis. Therefore, we will present our findings using the complete dataset of abstracts in the presentation of our paper. Then we proceeded to extract the text from these documents using Apache Tika and parse the 5845 unique URLs that we found using regular expressions. Then we used Python’s Requests Library to retrieve the HTTP response codes and headers corresponding to the URLs, which we used to classify the websites into two groups depending on their correctness: valid (correct) and decayed (showing signs of degradation). Figure 1 shows the distribution of decay for each year. Based on our preliminary findings we approximate the average lifespan of a research project to 5 years, which aligns with reports from previous work  (Goh and Ng, 2007). The average time in years since the last modification of the websites in the study is shown in figure 2.     Figure 1: URL decay by year.    Figure 2: Average time in years since last modification.   Developing classifiers To develop classifiers for the degradation identified in the previous section, we considered features computed based on DNS metadata, the initial HTTP request, number of redirects, and the contents and links returned by traversing the base node. The features we included are divided into topology, content-type, anchor-text and child node features. These features stem from concepts we used in our previous work  (Meneses et al., 2016).   The text associated with resources is the most obvious feature for determining the topics. Given that we are dealing with a very specialized domain, we developed a domain-oriented expectation model. In particular, we generated topic and term frequency models to examine the similarity among the documents in a given project (the contents of the base node and the metadata and the contents of the child nodes). We used Latent Dirichlet Allocation to model the content of the text  (Blei et al., 2003)  and a simple Tf-Idf ranking function to measure and compare them. This ranking function is based on adding the Tf-Idf values for the documents, which were calculated using the terms from the topic modelling as a vocabulary. We will present a detailed version of our results on the longer version of our paper.    Discussion This study is an attempt to categorize change in a very specific domain. More so, this study constitutes one step towards addressing potential strategies for the archival and the long-term preservation of abandoned digital projects. It is important to highlight that not all projects are equal and thus require different approaches towards long-term preservation. In the case of dynamically generated projects, a common approach nowadays is to produce a static set of HTML files which are easier to store. However, this approach assumes the backwards compatibility of Web browsers over time –something that has not always been the case.  To summarize, in this study we aim to computationally identify the indicators of the abandonment of digital humanities projects –as well as quantify their degrees of neglect. To address our philosophical question, we believe that an abandoned project can still be considered a valid digital humanities project depending on its audience. However, this has several nuances that should be considered. Digital online projects in the humanities have unique characteristics that make them impervious to the metrics that used in the Web as a whole –which make them worthy of study. In the end, we intend this study to be a step forward towards better preservation strategies and for the planned obsolesce of digital humanities projects.  ",
       "article_title":"Part Deux: Exploring the Signs of Abandonment of Online Digital Humanities Projects",
       "authors":[
          {
             "given":"Luis",
             "family":"Meneses",
             "affiliation":[
                {
                   "original_name":"Electronic Textual Cultures Laboratory - University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Jonathan",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"King’s College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Furuta",
             "affiliation":[
                {
                   "original_name":"Center for the Study of Digital Libraries, Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ray",
             "family":"Siemens",
             "affiliation":[
                {
                   "original_name":"Electronic Textual Cultures Laboratory - University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-19",
       "keywords":[
          "computer science",
          "repositories",
          "sustainability and preservation",
          "internet / world wide web",
          "library & information science",
          "text analysis",
          "archives",
          "data mining / text mining",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In this short paper I will explore some of the problems—particularly those having to do with power and access—inherent in collaboratively produced community digital history projects. I will focus on two projects currently in development in which I (working from within an academic institution and digital media lab) have partnered with people from marginalized populations located in geographic areas that have been given relatively little attention. One of my goals in initiating these projects has been to explore how to use institutional resources, including grants and IT support, to work outside of institutional structures. In each instance, my community partners and I have created projects centered on individual, personal narratives as they relate to place. Our objective has been to develop a kind of “people’s history,” giving voice to those who have traditionally been excluded from historical research and writing. In the course of conceptualizing and beginning to make these projects, however, we’ve encountered a number of thorny questions about notions of community, access, and narrative form and content.  Conceptually, these projects have been fundamentally enabled by digital technologies that allow new makers to produce historical narratives. Indeed, digital media has fed an emerging industry in small-scale, creative historical projects, many of which academic historians would term “micro histories.” Explored perhaps most famously by Carlo Ginzburg, micro histories can be viewed as correctives to “great man” theories of history or macro narratives that are easily undermined when challenged by specific circumstances. Focusing on seemingly “small” events—a day in an individual’s life, for example—micro histories often make transparent the point of view of the researcher, thus destabilizing hegemonic forms of historical writing. Micro histories can also bring attention to, or use, lacunae in the historical record, as well as offer narrative forms for “regular” people to engage in the construction of history. Working with digital tools and a loose concept of micro history, last spring I founded Bard College’s “Mobile History Van,” which operates under the umbrella of Bard’s Digital History Lab ( http://eh.bard.edu/dhl/). Both are funded by a major grant from the Mellon Foundation. The Mobile History Van uses digital technology to record and publish local history, and has worked closely with a local library and museum on digitizing archival materials and recording community history. While these projects have excavated important aspects of the historical record, they were executed with institutional partners—a college and historical society—and are thus still inscribed in easily recognized power structures.   In pursuit of developing projects outside of institutional structures, I approached students from Bard’s Clemente program ( https://www.clementecourse.org), a college credit granting year-long course for people who earn below a certain income level. Many of these students are struggling with substance abuse issues, criminal records, and post-traumatic stress disorders, but they wish to find ways to engage in the world. My intention: to work together with them and develop digital projects from the ground up, including working with other people who might not consider themselves part of the community described by the local historical society.   The first part of this talk will briefly introduce the audience to the genesis of the projects, and to their current state. By the time of the conference, both projects will be near completion, one as a series of personal narratives mapped onto the city of Kingston NY, the other in the form of a podcast about storytelling. The second half of this talk will examine a series of thorny questions we have encountered in the process of our work: Who controls the projects? What role does the institution play in supporting the projects, and how does this institutional affiliation shape the outcome in each case? How do we develop the projects for maximum input from collaborative partners? As we are working outside of an institutional structure, how do we define “community”? When should we, or is it necessary to, protect the storyteller? And finally, what does this type of project reveal about access to digital media?  ",
       "article_title":"A People's History? Developing Digital Humanities Projects with the Public",
       "authors":[
          {
             "given":"Susan Michelle",
             "family":"Merriam",
             "affiliation":[
                {
                   "original_name":"Bard College, United States of America",
                   "normalized_name":"Bard College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04yrgt058",
                      "GRID":"grid.252838.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "folklore and oral history",
          "digital humanities history",
          "epistemologies",
          "public humanities and community engaged scholarship",
          "historical studies",
          "cultural studies",
          "diversity",
          "theory",
          "epistemology",
          "English",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" PoéticaSonora is a research group interested in the study of sound, listening, and legibility at the intersection of art and literary studies. The group has worked together for two years, organizing several events and projects that operate under two main axes, activation and preservation. The most important project on the preservation axis is dedicated to the design, creation, and development of a digital audio repository (DAR) for sound art and sound poetry in Latin America. During the first phase of the project to gather audio files for the repository, we have conducted fieldwork and archival research in different public centers and private collections, mostly in Mexico City. The DAR prototype was designed and developed by graduate students at Concordia University, in Montreal, with an initial sample of 317 audio tracks, performed or composed by around 180 artists, most of them from Mexico but also Argentina, Brazil, and the US. These tracks have previously been classified as sound art, sound poetry, radioart, experimental music, spoken word, poetry slam, performance, hip hop, indigenous language poetry, among many other terms.  The interest of PoéticaSonora members has focused on studying audio recordings of poetry readings, as well as organizing and curating sound art and experimental music events. This presentation, however, studies how musical instruments and other sound-generating devices accompany, even modify the human voice, and how the DAR contributes to understand the understanding of these works and the context where they are developed. Texts by scholars who have taught, studied, and/or conducted fieldwork in Montreal, such as Mark J. Butler, Jeremy Wade Morris, Tara Rodgers, and Jonathan Sterne will serve as a theoretical framework to discuss some artistic practices by Mexican women vocal artists who participate in collaborative creative networks (sometimes called “bands,” “collectives,” “jams,” among other labels) and use sound-generating devices as a fundamental element in their performance. As a case study, I will focus on the path of two such artists, Edmée García and Leika Mochán, who combine spoken word and singing with the use of loop pedals. For several years they collaborated on the LP  Frágil, along with jazz songwriter Iraida Noriega, and have ever since worked in different creative projects, both solo and with other artists. Some of the pieces where they use loop pedals are also analyzed here, such as García's  Chilanga habla, described by herself as a “piece for poet and Line6,” and Mochán's “Kaleidocycle,” consisting of an amplifier and a Line6 DL4 attached to a customized bicycle.    The work of García and Mochán contributes to the discussion about what is intuitive and what is not in the use and adaptation of digital devices to produce sound with artistic or aesthetic purposes. These artists generate their own learning networks, transmitting to each other the empirical knowledge they acquire from free experimentation with a device. García calls Mochán “the loop pedal guru,” and learned from her and Noriega how to use it during the creation of  Frágil. This experience completely shaped the way García would perform her next poetry book,  Chilanga habla, up to the point of deciding not to publish the text-based version, as it did not portray the project's whole scope and shape. As for the “Kaleidocycle,” it allows Mochán to interact with the audience in a direct way, and posits questions about the false distinction between liveness and recording, particularly at the moment of performance. The different paths followed by García and Mochán after  Frágil are a good example of how knowledge is not always prefigurative (from an elder to a youngling), but also configurative (among peers) and sometimes even postfigurative (from a youngling to an elder). This presentation sheds light on how Mexican artists face a device's intended use and how their actual uses diverge and become mainstreamed within certain collaborative networks.   Frágil and some works by García have already been integrated to PoéticaSonora's DAR. The presentation will start with a brief showcase of how their collaborative networks are illustrated in the prototype, as well as the roles and instruments each participant plays in a particular composition. It will then discuss how to integrate new works by García and Mochán, how to possibly solve some of the prototype's limitations, and reflect upon the next steps in the project, considering the implications it may have on the prototype's data schema. As it stands, does the DAR help us visualize these collaborative networks? Is it necessary to have an entity for groups and collectives, or can it be inferred from other categories in the database? It will finally discuss a brief genealogy of loop pedals to understand how such a marginal guitar effects unit (like the Boss RC 20 or 30, Line6 DL4) evolved into a device for singers (Boss RC 500, but more specifically the TC Helicon series), and in so doing re-purposed this device. With these case studies I will explain how the functions delegated to the loop pedal allow these artists to overcome the fact of not being a “musician,” even though both have a strong musical background, and to perform “solo” despite holding a creative relation with the loop pedal.   ",
       "article_title":"Peer Learning and Collaborative Networks: On the Use of Loop Pedals by Women Vocal Artists in Mexico",
       "authors":[
          {
             "given":"Aurelio",
             "family":"Meza",
             "affiliation":[
                {
                   "original_name":"Concordia University, Canada",
                   "normalized_name":"Concordia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01qxhf360",
                      "GRID":"grid.448967.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "audio",
          "databases & dbms",
          "creative and performing arts",
          "spanish and spanish american studies",
          "video",
          "feminist studies",
          "English",
          "interdisciplinary & community collaboration",
          "including writing",
          "multimedia",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  INTRODUCTION Integrating the digital humanities (DH) into undergraduate level higher education programs has often been a difficult and ambiguous process. Faculty sometimes struggle to create syllabi that incorporate technologies but that do not require constant redesign as technologies evolve. Institutions may lack systems to connect students with faculty and staff who are interested in collaborative research, and collaboration beyond one’s own institution can be complicated or inaccessible for students. These are real challenges; as institutions increasingly develop DH courses and degrees, the impact on undergraduate students is diverse, ranging in minimal involvement, to career-altering. So, what should the role of the undergraduate in DH be, and how can we address these challenges? For the past three years I have explored these questions. This exploration has led to helping redesign and teach the foundational seminar for Hope College’s Mellon Scholars DH Program, as well as co-founding and chairing the Undergraduate Network for Research in the Humanities (UNRH), an undergraduate-led organization with the mission of reimagining the undergraduate role in DH through the establishment of a network of digital humanists who present research, collaborate, and share ideas. On the basis of these experiences as an alumna of Hope’s DH Program and UNRH Chair, I have been considering the ways in which faculty, staff, and institutions might support undergraduate DH researchers. My work has culminated in a series of models, programs, and initiatives that address the need for fostering the next generation of digital humanists in the classroom, at the institution, and beyond.   METHOD  2.1-CLASSROOM The first challenge I consistently identified in DH courses was an incohesive structure that treated the digital and the humanities as separate units rather than an interconnected academic space. Secondly, seminar themes grounded in particular technologies had to be redesigned frequently as these technologies evolved or became outdated. This was the case for the year-long introductory seminar for Hope’s DH Program. Each year students felt that the seminar was two unrelated courses, one focusing on a particular area in the humanities, the other, teaching technologies like GitHub and data analysis. The course was a noble attempt but ultimately inconsistent, incohesive, and not a truly interdisciplinary approach to DH. I set about designing a seminar model that was adaptable to new technologies yet still focused on an intersectional theme. I consulted with educators at conferences and researched seminar formats at other institutions, but unsurprisingly there was a wide range of approaches that seldom emphasized independent research quite like Hope’s program. Thus, I grounded the seminar model in that very aspect: a chronological approach to independent research in the humanities. Over course of four units students engage with the evolution of humanities-based research and with the research process from beginning to end. During the first unit, students work in the archives, practice cataloging primary sources with tools like Zotero, develop strong but focused research questions, and discuss literature to answer the ever-present question “What is DH?” The second unit follows the progression in humanities-based research, moving from sources like libraries and datasets into the first examples of DH: text analysis. Students curate their own text-based datasets, analyze and visualize them, present them with Omeka, and discuss research project methodologies of source compilation and argumentation. The third unit it titled: CCP-Collaboration, Communication, & Presentation. It involves group research collaboration and finalizing research projects through effective communication and presentation. Students complete writing workshops in which they must adapt a piece of writing for different audiences and styles, from conference abstracts to blogs and tweets; they also practice oral and web presentation skills. The final unit addresses advanced topics and tools which require students to focus on race, gender, sexuality, politics, and socioeconomic status. Students learn that equity and accessibility are paramount when creating public scholarship, digital or otherwise, and they are exposed to a survey of technologies in efforts to broaden their concept of what form research can take. The outcome of this course should be a comprehensive and diverse approach to humanities-based research projects through the chronological progression that research in the humanities has followed.   2.2-INSTITUTION For collaborative research, students and faculty alike find it challenging to make necessary connections with one another in the four short years that students have on campus. My solution is Bin(d)r: the Baccalaureate Interdisciplinary Network for (Digital) Research. Stemming from an initial idea of a physical binder with pages featuring the profiles of faculty, staff, and students interested in collaborative research, Bin(d)r: is ideally implemented as a searchable database of anyone on campus with research interests and skills. It is like Tinder for academics. All faculty and staff interested in collaborating simply create a profile on a site with tools like WordPress’s “Ultimate Member” Plugin. Students are invited to create profiles if they are interested in research. By including specific research interests and skills, faculty and students can get “matched” in a timely manner. Bin(d)r: has parentheses around “digital” because this tool does not have to be exclusively for digital projects, but it would provide an extra level of support for digital projects, connecting computer science students with humanities faculty, for example. Bin(d)r: is capable of being entirely free, low maintenance, highly interdisciplinary, and ultimately a tool for encouraging undergraduate research. Furthermore, if the digital Bin(d)r: takes off at numerous institutions, searching others’ databases would foster cross-institutional collaboration.  While considering the institutional level, I would also argue that institutions must make space to hear the voices of their students. I propose that institutions establish a quarterly forum for undergraduates, faculty, and administrators to gather and discuss how the institution can better support students. Academic institutions are designed first and foremost to educate their students, so I assert that students have the right to tell institutions how they can improve, and institutions have the responsibility to listen. Simply creating space for dialogue is empowering.  2.3-BEYOND I also argue that empowering undergraduate researchers means providing agency, accreditation, and opportunities to join a community. Because DH is emerging at different rates across the globe, many students never meet other students engaging in their work. Furthermore, exposure to different methodologies, technologies, and project ideas has a profound impact. Faculty and staff gain this exposure at academic conferences and within their departments. UNRH aims to give this space and community to students, too.  Our method of creating UNRH relied heavily upon initial organization, forming a Steering Committee, review system, and website. The format of our conference was meticulously designed. We created a “speed-dating” session for rapid introductions and elevator pitch practice, a formal project presentation session, informal poster-style presentation sessions, a keynote address, and workshop sessions. These workshops include technology tutorials, panel discussions about different students’ roles and experiences at their institutions, and design-thinking sessions to address the needs and concerns of students striving to develop DH projects. Beyond the conference we have been developing an online network space in which students create profiles and can share project updates, articles, conference opportunities, and requests for peer review. In essence, each of our decisions was an effort to create space and flexibility for students to answer for themselves the question of what the undergraduate role in DH can be.   RESULTS  3.1-CLASSROOM The feedback from my students who experienced my seminar model have been positive. The survey results indicate that the seminar has largely met the learning outcome goals, and students indicated increases in confidence and preparedness in conducting independent research (approximately 30% average increase) and using new technologies (approximately 37% average increase) according to a seven-point scale. Those who indicated having less prior experience (1-4) had an average increase of about 33% in independent research and about 39% in technology use. I plan to track program retention rates in the coming years to hopefully see improvements as the sophomore students navigate from the structured seminar into the independent research spaces of their junior and senior years.   3.2-INSTITUTION Bin(d)r: has not yet been implemented but is in development for implementation at Hope College in the coming year.   3.3-BEYOND The results of our efforts exceeded expectations. Since our first conference in 2015, we have accepted over 50 projects, involving over 80 undergraduates from 31 institutions all across the United States, Canada, Nigeria, and Pakistan. According to in-person comments and our post-conference evaluations, students have felt empowered, encouraged, and independent in their research. Moreover, students were amazed at what they learned and accomplished by interacting with undergraduates from other institutions.  Through our initial design and modifications over the years, we feel confident in the model for an organization and conference that grants agency to undergraduates, and space to understand their own roles. Now in my third year as Project Manager/Chair, when I consider again the undergraduate role in DH, I think of students as connected learners and independent researchers pursuing their own interests while learning from peers and mentors alike. Within and beyond this space, each student must determine her role for herself. Instructors, institutions, and organizations, invest in these students, for they are the next generation of digital humanists.  ",
       "article_title":"Next Generation Digital humanities: A Response To The Need For Empowering Undergraduate Researchers",
       "authors":[
          {
             "given":"Taylor Elyse",
             "family":"Mills",
             "affiliation":null
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "and curriculum",
          "cultural and/or institutional infrastructure",
          "teaching",
          "public humanities and community engaged scholarship",
          "cultural studies",
          "sociology",
          "pedagogy",
          "English",
          "interdisciplinary & community collaboration",
          "philosophy"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The Hebrew Bible (the Tanakh) is the most ancient and sacred collection of Jewish texts. Throughout the history, additional religious Jewish texts have been written such as the Mishna, the Babylonian Talmud, and many more. These additional texts are often related to (or inspired by) the Bible. As such, many of them quote verses   The Bible is divided into basic text elements called  verses.   from the Bible (as in Figure 1). Depending mostly on their frequency and location within the text, the quotations may indicate a weak or strong semantic relation between a given text and a specific portion of the Bible. Knowing these semantic relations may be beneficial for those interested in studying or investigating the Bible.  Nowadays, a variety of Jewish texts are publicly available over the Internet, yet the identification of Bible quotations within them is often sparse and sometimes entirely absent. Moreover, the existing identification lacks a rigorous representation, which makes it difficult to automatically infer semantic correspondence and to develop supporting software applications. We report an ongoing project that aims to establish the machinery for the automatic detection and rigorous representation of quotations of Bible verses within Jewish texts. The project consists of three interleaving components. In the first component, an algorithm for identifying Bible quotations in text is developed. In the second, the results of executing the algorithm on a large and open text corpus are represented as a  Linked Data graph (RDF dataset). In the third component, we develop a web frontend for making the dataset accessible to end users. Exposing the data to end users may also engage their participation in data-driven crowdsourcing (Ched et al, 2015), and hence, will serve to collectively help in improving the dataset quality. In what follows, we elaborate on each of the project components.    Algorithm Quotation detection is gaining popularity in fields such as copyright enforcing and political analysis, and within ancient texts (Ernst-Gerlach and Crane, 2008; Gesche et al, 2016). The algorithms in use share common characteristics, yet each domain brings its own specificities and challenges. Given an input text, our algorithm first matches maximal n-grams   An  n-gram is a contiguous sequence of n words from a text.   in the text to candidate Bible verses. For example, the green bigram (2-gram) in the first line of Figure 1 will have one matching verse, since its text ('לך לך') appears in exactly one Bible verse. This matching is maximal, since the words that appear before and after the bigram are not part of the quoted verse.    A portion of ancient Jewish Text (from Midrash Raba), that quotes two Bibles verses. Quotations to the same verse are marked in a similar color. Note that each quotation refers only to a part of the verse (1-4 words of it).  A first challenge that we face is related to variations found between the quoting text and the original Bible text, mostly related to the omission (or inclusion) of Hebrew vowel letters. As an example, consider the red quotation in the second line of the figure, that contains the word 'המוריה', where in the original Bible source the 'ו' (vav) vowel is omitted. We have implemented two alternative solutions, one is based on  fuzzy search (Levenshtein distance), and the other on  exact search performed simultaneously on two versions of the Bible, with and without vowels.  Not all verse candidates are valid quotations of Bible verses in the text. For instance, the phrase 'בית אביך' in the third line of the figure (underlined) is a common phrase that appears in eleven different Bible verses. Nevertheless, the phrase is mentioned in a different context, which is not related to any of them. False candidates occur mostly in bigrams and trigrams (3-grams), and the algorithm makes an effort to filter them out. One approach is to keep a candidate if a matching candidate appears in a larger n-gram in the same text. For instance, the green bigrams and trigram shown in the figure are reported as valid quotations since there is a 4-gram that quotes the same verse in the text ('אל הארץ אשר אראך', line 3). We are considering additional filtering approaches related to statistical data inference and machine learning. We are also creating collections of labeled data for a systematic evaluation of the algorithm.   Linked Data The detected quotations are represented as RDF Linked Data, making them accessible to machines for standard consumption and integration. We use a lightweight ontology that we have defined, augmented with standard properties taken from known ontologies such as RDF, RDFS, and Dublin Core (DC). We are working on the integration of additional ontologies such as CIDOC-CRM, FRBR, and SPAR. Key ontology classes are  Book,  Section,  Text, and  Quotation. A  Book is composed of  Sections, that may be composed of other  Sections, and eventually of  Text elements. Each Bible verse is a node of type  Text in the RDF graph. To date, our graph contains a total of 23,206  Text nodes of Bibles verses. Additional 355,181  Text nodes represent text elements within other Jewish books (where quotations are searched for). An edge from a  Text node of the latter kind to one of the former kind indicates a 'quotes' relationship. Nodes of class  Quotation hold additional details such as the exact location wherein a quotation appears in the text.    A SPARQL query that retrieves all text elements quoting the first verse of the Bible.  A Linked Data graph may be accessed by expert users using the SPARQL query language. An example SPARQL query is shown in Figure 2. To make our data widely accessible, we have implemented a graphical web frontend that acts like a search engine for Bible verses. A user selects a set of verses from the Bible, and then being presented with all text elements that quote one or more verses from the set. (The elements are retrieved from the RDF graph.) The results are sorted by significance, and may be filtered using predefined categories. We plan to enhance the web interface with data-driven crowdsourcing support, where the crowd will help in improving the accuracy of the algorithm by marking false negatives (places in the text that the algorithm has missed), as well as false positives (incorrect detections). The web tool, as well as the detection algorithm and related artifacts, are accessible via our main  GitHub repository.   ",
       "article_title":"Towards Linked Data of Bible Quotations in Jewish Texts",
       "authors":[
          {
             "given":"Oren",
             "family":"Mishali",
             "affiliation":[
                {
                   "original_name":"Technion, Israel Institute of Technology, Israel",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Benny",
             "family":"Kimelfeld",
             "affiliation":[
                {
                   "original_name":"Technion, Israel Institute of Technology, Israel",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "near eastern studies",
          "computer science",
          "knowledge representation",
          "library & information science",
          "text analysis",
          "English",
          "semantic web"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Clarifying the genesis of a passed down text is of outmost importance for many scholarly disciplines within the humanities such as history, literary studies, and Bible studies. Often, historical text sources have been copied over and over for hundreds or even thousands of years, thus being subjected to paraphrasing and other kinds of modifications, repeatedly. Despite the significance of source criticism for the humanities as a whole, algorithmic support in this matter is still limited. While current approaches are able to tell  if and  how frequent a text has been modified—to the best of our knowledge—there has been no work on determining the  degree of paraphrastic modification. To a human reader, the introduction of, say, spelling variations is indubitably a minor modification compared to substituting entire words. Yet, how can the different “degrees” of alterations, which are intuitively clear to scholars, be captured in an algorithmic way?   To this end, we present a first approach for designing a metric for paraphrastic modification in text (henceforth paraphrasticality). Based on an English Bible corpus (three literal Hebrew and Greek translations and three standard translations) we measure the frequency of different classes of textual variations between each pair of Bibles. We then use the probability of these variations in a machine learning experiment to derive weights for these classes of modifications. Ultimately, this allows us to define a metric for paraphrasticality which we validated with promising results.   Related work Measuring the  similarity or  distance between two spans of text is relevant to many areas in and related to natural language processing (NLP). One of the earliest approaches is Levenshtein’s (Jurafsky and Martin, 2009) edit distance which is based on character-level removal, insertion, and replacement operations. BLEU (Papineni, 2002) is the most common evaluation metric in machine translation, capturing the difference between gold and automatic translations based on (word-level) n-gram overlap. In  stylometry, different kinds of delta metrics are used to compute the difference between the writing style of authors or texts (Jannidis et al., 2015). These are typically based on the frequency distribution of the most frequent words. These first three approaches have in common that they rely on surface features (token and character-level) alone and do not incorporate semantic proximity. In contrast to that, computing the  semantic similarity between two sentences is a popular task within NLP (Xu et al., 2015). However, approaches in this field are typically not indented for manual inspection and are thus less suited for applications in the humanities. Lastly, Moritz et al. (2016) quantify modification operations on a parallel Bible corpus yet do not present a way to aggregate these counts into a distance metric. In contrast to these related contribution, here, we aim to develop a metric which is both semantically informed as well as human interpretable.    Data  We use a parallel corpus of the Old Testaments of six English Bible translations   Note that our approach is not limited to applications on historical text and that our choice of textual material is based on technical reasons only. In fact, any paraphrastic, parallel corpus would work equally well for our proposed method.  from the 19 th century, half of them being literal translations that closely follow the primary source texts’ language and syntax while the other half are standard translations (see Table 1).      Table 1: Bible editions used for investigation. Sources: bst:  https://www.biblestudytools.com/; mys:  https://www.mysword.info; ptp: Parallel Text Project (Mayer and Cysouw, 2014)   Literal translations: Robert Young, the translator of  YLT, created a highly literal translation of the original Hebrew and Greek texts. Thus, Young tried to be as consistent as possible in representing Greek tenses with English ones, e.g., he used present tense where other translations used past tense (see Young,  1898a; Young, 1898b) as in: ‘In the beginning of God's preparing the heavens and the earth —’ (Genesis 1:1).  SLT: Upon publication, Julia Smith’s Bible translation was considered the only one directly translating the historical source texts to contemporary English. She aimed at complete literalness and tried to translate each original word with the same English word, consistently, and tended to translate the Hebrew imperfect to English future tense (Malone, 2010).  LXXE by Sir Lancelot Charles Lee Brenton is an English translation from the Codex Vaticanus version of the Greek Old Testament, which itself is a translation of the Hebrew Old Testament (Roger, 1958).   Standard translations: WBT by Noah Webster is a revision of the King James Bible mainly eliminating archaic words and simplifying Grammar (Marlowe,  2005 ).  ERV is today’s only officially authorized revised version of the King James Bible in Britain (no author , 1989). The most recent edition in our study is  DBY, Darby’s translation of the Bible. The Old Testament was published by his students in  1890 and is based on Darby’s German and French versions (Marlowe, 2017).     Methods  Preprocessing and alignment: We use MorphAdorner (Burns, 2013), a specialized toolkit for early modern and modern English, to tokenize and lemmatize the Bibles. After removing punctuation and verse identifiers, we pair up our six Bibles in every possible combination (15 in total). Since the different Bible versions are inherently aligned on the verse-level (by their verse identifier), our next step builds up a statistical alignment at the token level for each pair of bibles using the Berkeley Word Aligner (De Nero and Klein, 2007), a tool originally designed for machine translation.    Counting modification operations: Building on these word-aligned pairs of Bibles, we can describe the divergence between a pair of verses in terms of the  modification operations—such as replacing a word by its synonym—which would be necessary to convert one version into another. We automatically apply and count the modification classes introduced by Moritz et al. (2016) for each verse and Bible pair (see Table 2). Synonyms, hypernyms, hyponyms and co-hyponyms, are identified based on BabelNet (Navigli and Ponzetto, 2012).      Table 2: Operations used as features together with normalized estimated weights (coefficients) of the fitted model   Weight identification: By counting modification operations, we gain a fine-grained description of the  exact differences between two spans of text. However, to construct a metric, we had to find a way to condense these modification frequencies down to a single number. For that we exploit the fact that we deal with two classes of Bible translations, literal and standard ones. Thus, to estimate a human judgment of deviation, we assume that standard translations are more homogenous to each other than literal translations (since the latter demand for more creative language use; see Section 3). Hence, we can train a classifier to distinguish whether a pair of Bible verses is from the same class (both Bibles being standard or literal translations, respectively) or from different classes. For this task, we train a maximum entropy classifier   Using the scikit-learn.org implementation. Training for this binary classification task was done using 10-fold cross-validation achieving an accuracy of .68.  where we use the relative frequencies of the modification operations as features. Now, the key part of our contribution is that we can exploit the coefficients of our fitted model as the first ever presented empirical estimate of the relative importance of these modification operations for paraphrasticality.    Results   Feature weights: Table 2 lists the final, normalized (summing up to 1) feature weights of our fitted model. Lemmatization, hyponym and synonym relations turn out to be especially important for the classification task.   Metric: Based on these coefficients, we define the paraphrasticality metric  par between two word-aligned text spans  a and  b as      where 𝑛 is the total number of features (or classes of operations), 𝜃 𝑖 is the absolute weight for feature 𝑖 determined via the classifcation experiment and 𝒳 𝑖 a,b is the relative frequency of the respective operation. In order to gain face validity for this newly defined metric, we compute the paraphrasticality score for each one of the 15 Bible pairs in our corpus (as average of their verse paraphrasticality). The results are presented in Table 3.      Table 3: Deviation between each pair of Bibles in terms of our newly developed paraphrasticality metric; higher values indicate higher distance       Table 4: Top 3 most frequent operations (without fallback) per Bible pair   Qualitative validation: We can identify three regions in the plot. The upper left triangle shows that our standard translations do not differ much from each other (as expected), especially since WBT and ERV are revisions of the same Bible. The 3x3 rectangle in the upper right corner represents pairs of one literal and one standard translation, respectively. We can see that the distance between those is about 0.3 thus displaying increasing paraphrasticality compared to pairs of  only standard translations. The highest deviation however is between the literal translations by Smith (SLT) and Young (YLT) compared to the English Septuagint (LXXE). This can be explained by the choice of vocabulary by each translator and by the purpose they follow in their translations. For example, SLT and LXXE use “firmament” when YLT uses “expanse”, SLT and YLT use “rule” when LXXE uses “regulating”. We thus conclude that our metric yields valid and—perhaps even more important for applications in the humanities—interpretable results.  Our approach also enables to judge distance on a fine-grained level based on pure operation counts. In Table 4 we show the top 3 operations for each Bible pair. As we can see, most of the top 3 operations per Bible pair relate to semantic relations between the aligned word pairs (matching lemma, synonymy, or co-hyponomy) underscoring the advantage that our metric has as opposed to more surface feature-dependent approaches (to textual similarity) such as Levenshtein distance or delta measures.   Conclusion We presented the first study on designing a metric for paraphrasticality. Different from existing approaches on measuring distance or similarity between texts, we describe paraphrasticality as frequency of specific modification operations for which we tried to find empirically adequate weights via a machine learning experiment. As demonstrated, our approach is specifically useful for applications in the humanities as operation frequencies, and feature weights, as well as paraphrasticality scores are open to manual inspection. A more comprehensive comparison against existing similarity metrics and a human judgment is left for future work.  ",
       "article_title":"Towards a Metric for Paraphrastic Modification",
       "authors":[
          {
             "given":"Maria",
             "family":"Moritz",
             "affiliation":[
                {
                   "original_name":"University of Goettingen, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Johannes",
             "family":"Hellrich",
             "affiliation":[
                {
                   "original_name":"Graduate School “The Romantic Model”, Friedrich-Schiller-Universität Jena, Germany",
                   "normalized_name":"Friedrich Schiller University Jena",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05qpz1x62",
                      "GRID":"grid.9613.d"
                   }
                }
             ]
          },
          {
             "given":"Sven",
             "family":"Buechel",
             "affiliation":[
                {
                   "original_name":"JULIE Lab, Friedrich-Schiller-Universität Jena, Germany",
                   "normalized_name":"Friedrich Schiller University Jena",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05qpz1x62",
                      "GRID":"grid.9613.d"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-23",
       "keywords":[
          "computer science",
          "artificial intelligence and machine learning",
          "linguistics",
          "stylistics and stylometry",
          "machine translation",
          "English",
          "natural language processing",
          "translation studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Introduction What, we may ask, is Latin America’s contribution to global art? The answer assumes special importance in the context of the twentieth century when pioneers in the intersection of art and technology were creating new precepts, like in the works of the ‘Fluxus’ group in USA. If abstraction and penetration through formal stasis were acknowledged as the basic style in the art world, then pioneers in Latin America were also in pursuit of a set of most innovative possibilities for their art. We may say that Latin American artists through the sixties and seventies created such kinetic artworks that no group of artists, joined together either by contiguity or ideology, had yet achieved anywhere else in the world. What Latin American artists did were to reinvent kinetic possibilities in their most entropic and unprognosticated formats, denying subjectivity and creating ooportunities of looking at movement in art, not movement as a vector but as a function with an unknown trajectory. Our ongoing archival project on Digital Art in Latin America is especially oriented toward a perception of a digital art prototypes that evolved as a response to infiltrations of technology into Latin America.  2. Thematic Content  We should first emphasize on the rise of pre-electronic art in the middle of the twentieth century, which was initially represented in structural invariances of optical and sometimes pre-digital templates. Julio Le Parc’s metallic illuminations, and Martinoya and Joel’s Abstratoscopio Cromático are exemplars of this new beginning with the experimental media, a new search for entropy in kinetic objects (Fariat, 2015). Leading artists like LeParc were experimenting with projection of light on reflective material. In Brazil, Abraham Palatnik tried trans-positioning colors through mechanical movements. Waldemar Cordeiro was a similar innovator with punch cards. Chileans Carlos Martinoya and Naum Joel created the Abstratoscopio Cromático, an installation which anticipates an entirely new artistic usage of polarized light effects, something the world never witnessed before (Martinoya and Joël, N. 1968). In Mexico, Manuel Felguérez, produced innovative pictorial compositions using paleo-computational programming in an age when the PC was nonexistent, and the Mexican artist Pola Weiss embraced video art. The archival project on digital heritage preservation could be an attempt to save the history of this transformation in the arts and to restore the place of Latin-American artists in the trajectory.  3. History of Digital Art in Latin America The beginning may be marked from the late 1940s and early 1950s, especially in countries such as Argentina and Brazil where some of the most innovative artists began experimenting with new tools and technologies. Unfortunately, these pioneering Latin-American artists have neither been recognized nor absorbed in mainstream literature or the history of art. The initiative is specially oriented to create an appropriate perception of a digital art prototype that evolved as a response to infiltrations of technology in the Ibero-American world. Our objective is to show that Latin American artists in the new media evolved a peculiar style which manifests itself in the intelligent use of kinetic actions in an art work, to create effects which supersede categorization. They were able to explore movement and its entropic combinations. The moment could no longer be predicted, and the teleological design would lie outside subjective and interventionist approach. Perhaps Julio Le Parc or Abraham Palatnik, the agents of creative deconstruction, saw the possibility of having an art independent of intentions, one couldn’t predict outcomes, an in an interation of lights and angles, or as in an interactive program with random inputs. In Le Parc’s kinetic sculptures and in Palatnik’s optical moments we see this first evidence of play and disruption of linear structure, later so enormously amplified in the kinetic sculptures of Mexican artists like Rafael Lozano Hemmer or Gilberto Esparza (Thompson and Mukhopadhyay, 2015). The parallels to such art installations in the north are in the K 456 of Nam Jun Paik, and the robots of Norman White, but the peculiarity of Latin American artists lie in their minimal use of technology and the potentially rich suggestivity with more formless, thermodynamic movements.  And also from Brazil, Waldemar Cordeiro was a similar innovator in the 60s who used punch card applications to manipulate images within a prehistoric computer: some of his works were recently restored by the ITAU Cultural in Brazil. In Argentina, back in the 50s Julio LeParc started experimenting with projection of light and only years later his works were shown at the Venice Biennial (1966). Also in Argentina Marta Minujin together with Wolf Vostell in Cologne, Germany and Allan Kaprow in New York, were transmitting the actions of events to artists in Cologne and New York by means of a satellite: this artwork was known as Simultaneidad en Simultaneidad (1966). In Chile, Carlos Martinoya and Naum Joel created the Abstractoscopio Cromático (1960), to anticipate an entirely new artistic object with polarized light effects, something the world had never witnessed before. In Mexico, Lorraine Pinto combined light with the music of Stockhausen in La Quinta Dimensión (1968), an artwork shown in the same year as that of the Olympic Games in Mexico City. Years later Manuel Felguérez, produced innovative pictorial compositions using paleo-computational programming in an age when the PC was nonexistent. He named his project La Máquina Estética (1975-1977). Women like Pola Weiss embraced video-art: her video Flor Cósmica was produced in 1977 and few years earlier in 1973 in Brazil, Analivia Cordeiro created the video-dance installation performance M3x3.  4. Geo-cultural Markers We see that many of the artists that contributed to the development of new pathways in art and technology were ones living in other countries, especially Europe or the United States; some others came to Latin America from other regions of the planet and settled and gradually established themselves there. In Argentina for instance a fertile ground for innovation in the arts was created by the Instituto Di Tella that was bringing some of the most important and revolutionary creative minds of the world to work and exchange ideas with Argentinian artists in Argentina (Plotkin and Neiburg 2014). The Instituto Di Tella as such became during the sixties a deservedly international reference for the arts. Some of those important figures showed their work for first time at the Di Tella museum in Buenos Aires. Julio Le Parc and Martha Minujín were among them.  5. Archival Tools Any archive could be created with adequate programming for a template that creates a space for storing information on these important art works. The survey for this project would have to be long drawn, with attempts of collecting information on individual works displayed in museums and galleries, and often during important events like biennials and electronic festivals of art. Simultaneously, it is necessary to obtain, wherever available, existing videos or photographs of electronics installations and designs (Gumbrecht and Marrinan, 2003). There is no comprehensive digital catalogue of digital art. Archival efforts would have to be directed with the aim of creating a virtual space in which visitors to the archive have an opportunity to share a video or visual image of the art works that have been created as part of a tradition. 6. Additional Material 6.1 Links http://www.digitalmeetsculture.net/article/digital-latin-america-aims-to-show-latin-potential-for-digital-artistic-creation/  6.2 Websites Archive of Digital Art The Google Art Project / Digital meets Culture An introduction to the booming world of Latin American digital arts 6.3 Articles Davis, D. (1995). The work of art in the age of digital reproduction (An evolving thesis: 1991-1995). Leonardo, 381-386. Bertacchini, E., & Morando, F. (2013). The future of museums in the digital age: New models for access to and use of digital collections. International Journal of Arts Management, 15(2), 60. Turnbull, D., & Connell, M. (2014). Curating digital public art. In Interactive Experience in the Digital Age (pp. 221-241). Springer, Cham. 6.4 Books Hilbert, M. R. (2001). Latin America on its path into the digital age: where are we?. United Nations Publications. ",
       "article_title":"The Search for Entropy: Latin America’s Contribution to Digital Art Practice",
       "authors":[
          {
             "given":"Tirtha Prasad",
             "family":"Mukhopadhyay",
             "affiliation":[
                {
                   "original_name":"Universidad de Guanajuato, Mexico",
                   "normalized_name":"Universidad de Guanajuato",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/058cjye32",
                      "GRID":"grid.412891.7"
                   }
                }
             ]
          },
          {
             "given":"Reynaldo",
             "family":"Thompson",
             "affiliation":[
                {
                   "original_name":"Universidad de Guanajuato, Mexico",
                   "normalized_name":"Universidad de Guanajuato",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/058cjye32",
                      "GRID":"grid.412891.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "repositories",
          "audio",
          "sustainability and preservation",
          "art and art history",
          "spanish and spanish american studies",
          "digital humanities history",
          "cultural studies",
          "video",
          "theory",
          "epistemology",
          "archives",
          "English",
          "criticism",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In this exploratory research, we sought to investigate how we might identify and quantify the contextual shift surrounding significant entities in news based corpora. For example, might we be able to see changing public opinion such as that experienced by George W. Bush Jr. after the events of 9/11 and thus note how a population can rally behind their leader in the face of cultural trauma? Our method of identifying these changes has its roots in the field of distributional semantics and the measurement of semantic shift. A typical approach to solving this problem involves building multiple word models across subsets of the sample corpus which are organized by date. By comparing the outputs of the different models we can see how the definitions of words have evolved. We adopt Temporal Random Indexing (TRI) (Basile et al., 2014) as our method of measuring semantic shift over time as it allows for a direct comparison between word representations on the basis of simple cosine similarities.   Method In order to apply our method of measuring contextual shift in relation to entities we require a consistent representation of each entity that will span the entire collection e.g. the algorithm will need to know that “President Bush”, “G.W.” and “Dubyah” all refer to the same individual. In order to achieve this, an Entity Disambiguation process is applied to the source text prior to building the semantic space. This step substitutes mentions of each entity with a URI obtained from DBpedia, allowing the algorithm to track an individual through the collection irrespective of how they are referenced. We use CogComp NER    (Ratinov and Roth, 2009) for entity recognition and AGDISTIS    (Usbeck et al., 2014) for disambiguation.  Given the output from the disambiguation tools, a different semantic space for each year in the collection’s timespan is built using the TRI implementation by Basile    (Basile et al., 2014). Each space provides a semantic representation of words and Named Entites (NE) in terms of their proximity in space, which reflects their semantic relatedness. A time series for each NE is extracted by computing the cosine similarity between two consecutive semantic spaces (e.g. 2001 and 2002). Finally, candidate dates for the shift in meaning are extracted using the Change Point Detection algorithm as implemented by Kulkarni    (Kulkarni et al., 2015).    Evaluation For test data we utilized the New York Times collection curated by LDC    (Sandhaus, 2008) which spans 20 years of American news from 1987 to 2007. While methods which measure semantic shift in word sense typically require collections which span hundreds of years, because circumstances evolve more quickly than language, we believe that a 20 year span is more than enough to produce interesting results when the same methods are applied to the examination of entities.  The collection was preprocessed and analysed using the method described in Section 2. This yielded a series of 20 language models which provided semantic representations for each entity identified and linked by CogComp NER and AGDISTIS. We computed the temporal shift for all the entities in the corpus and ranked them by the magnitude of this shift (p-value from the Change Point Detection algorithm). We selected the top 100 entities from this ranking (i.e. those with the greatest semantic shift) and selected the largest group of entities which underwent a semantic shift in the same year from within that group.   Results The evaluation methodology described in Section 3 yielded a shortlist of 12 entities which undergo a sizeable semantic shift in 2001: Federal_Bureau_of_Investigation, Pentagon, White_House, New_York, Congress, Department_of_Justice, George_H._W._Bush, Texas, West, Saddam_Hussein, Republican_Party_(United_States), and American_Motors. Almost all of them are related to politics and have strong connections with the happenings of 9/11. Notably, while a member of the Bush family is connected with these events and does indeed undergo a shift in semantic representation, it is the wrong individual - the father rather than the son. This assignment of a semantic shift to George_H._W._Bush in 2001 is certainly due to the disambiguation process. While we believe the inclusion of the entity disambiguation step is an interesting contribution of this work, we observed a number of problems with the process. The contents of the knowledge base, which informs the disambiguation software, has a dramatic impact on the quality of the results obtained. So too does the nature of the entities being disambiguated. One notable example of this was our results with regards to mentions of “the Internet”. Our method showed a dramatic increase in discourse surrounding the Internet from the mid 90s up into the second millennium. However, while the representation was consistent, the referent chosen by the disambiguation software was an American band known as “The Internet”, rather than the network of computers we use today. While the error with the Internet is obvious, more challenging was distinguishing between mentions of George W. Bush Jr. and George H. W. Bush Sr. The former’s role in the events post 9/11 (reports of which were included in our corpus) made him an important entity for the disambiguation software to correctly annotate. However, in many cases this proved to be extremely difficult. This is understandable given the similarity in context surrounding both Bush Jr. and Bush Sr., We can work with an incorrect annotation provided it is consistently incorrect. However the unpredictability surrounding the name “Bush” presents a difficult problem when this information is used as part of the Random Indexing process.   Conclusion We have presented a preliminary case study, which although not robust enough to infer any conclusions, highlights the potential of this type of analysis. We conducted our preliminary investigation guided by a major cultural trauma that occurred between 1987 and 2007, and which caused a sudden reaction and change in the public discourse. It is clear that a weakness in the method is the disambiguation process. Future work will focus on improving the quality of disambiguation as well as investigating the possibility of building time series models over shorter spans of time e.g. months or weeks.  ",
       "article_title":" Temporal Entity Random Indexing  ",
       "authors":[
          {
             "given":"Annalina",
             "family":"Caputo",
             "affiliation":[
                {
                   "original_name":"Adapt Centre, Trinity College Dublin, Ireland",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          },
          {
             "given":"Gary",
             "family":"Munnelly",
             "affiliation":[
                {
                   "original_name":"Adapt Centre, Trinity College Dublin, Ireland",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          },
          {
             "given":"Seamus",
             "family":"Lawless",
             "affiliation":[
                {
                   "original_name":"Adapt Centre, Trinity College Dublin, Ireland",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "computer science",
          "linguistics",
          "natural language processing",
          "linking and annotation",
          "text analysis",
          "English",
          "content analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Can data-capture be a tool for feminist historiography? Can contemporary frameworks for understanding networks—actor-network theory, linked open data standards—help to shift our understanding of cultural production and literary history? This paper argues that data capture modelled on the principles of the “ego-network” is rich in its potential to address persistent problems in the recovery of non-canonical literary history. An ego-network is a data representation of the way that individuals are “embedded in local social structures” (Hanneman). Women’s cultural production has frequently been that of secondary cultural labours like editorship. Literary scholarship has struggled with appraising this secondary labour since at least 1986, with the publication of Shari Benstock’s foundational  Women of the Left Bank, in which she points out that  women were frequently the “midwives of modernism,” editors and caregivers performing who supported the construction singular male author. Likewise, scholars like Jack Stillinger (1991) have tried to break apart the “myth of solitary genius” which perpetuate the burial of these secondary labours. Despite the efforts of traditional scholarship to unsettle these myths and valorize the literary labours performed largely by women, the field has not yet succeeded in turning critical attention away from canonical, singular authors and towards what I call a practice of  distributed authorship . I contend that this is a problem of methodology, and that the careful creation of network graphs to represent distributed authorship may assist in correcting this persistent literary historical sticking point.  This paper emerges from the early stages of “Modernism, Feminism, and the Ego-Network,” a postdoctoral research project with the major linked open data modelling project, Linked Modernisms. It concentrates on the archival collections of British activist, author, anthologist, and editor, Nancy Cunard. Cunard’s  archives at the Harry Ransom Center (University of Texas at Austin) represent a who’s-who of literary and historical figures from the modernist period, paper remnants of a professional and personal network. Multiplicity and polyvocality are the hallmarks of her oeuvre, and her texts demonstrate networked connections amongst modernist writers, ideas, and events. In an echo of the scholarly trend at large, periodic attempts to recover Cunard’s work and legacy (Chisholm, 1979; Marcus, 1995; Moynagh, 2002; Gordon, 2007) have not taken root, despite her deep connectedness. In DH, feminist digital scholarship has revealed the way that histories of literature and histories of DH have been obscured in the wake of canonical digital archival projects (Mandell, Earhart), and so the problems of archival recovery affect print and digital scholarship alike. This paper will present visualizations and theoretical concerns that emerge from the on-going building and modelling of a prototype of an ego-network for feminist archival recovery.  This project takes up a relatively simple example as a prototype for feminist data collection: Cunard’s conventional anthology  Poems for France (1944), published in the last years of the Second World War as a tribute to newly fallen and occupied France. The process of its publication is well represented by the archival collection, in which Cunard has meticulously preserved received correspondence in response to calls for contributions in the  Times Literary Supplement and individual solicitations. Cunard clearly drew upon the breadth of her literary network, as letters from well-known figures like T.S. Eliot, Cecil Day-Lewis, and Vita Sackville-West show up, whether or not they ultimately contributed poems to be anthologized. However, from the point of view of conventional literary studies, this collection offers little in the way of telling examples or golden anecdotes. Cunard has little to no editorial hand over the text of the submitted poems, many of which had been published elsewhere. She rejects few contributions, and those who decline her invitation are brief and polite. Studies of editorship in the early-twentieth century have by and large looked to the depth of involvement of individual editors like Ezra Pound in the writing of authors like Hilda Doolittle or T.S. Eliot or concentrated on authors’ own processes of revision (Sullivan 2013). Even in these studies of editorship, an impulse towards individual authorship persists. The work of the cultural contributor is contained in the perceivable strong hand of the individual, and collaboration is reduced to direct editorial intervention amongst canonical figures. The version of cultural contribution that Cunard undertakes in this anthology must be read differently to be read as an instance of cultural contribution.  My methodological approach to modelling this anthology has been to build a small dataset of the poems and letters relating to the collection. The dataset is currently in the form of a relational database. I begin from the position that Cunard represents the central node in an ego-network, and that the anthology can represent the immediate social structure in which she is embedded. I made a few key decisions in modelling my data. First, I have expanded the dataset’s definition of  publication. Inclusion in the published anthology and publication elsewhere are recorded as an instance of a poem’s publication. In addition, a poem’s inclusion in manuscript form in the Collection, whether that manuscript is received from a contributor or transcribed by Cunard is also recorded as an instance of publication. This decision aims to give similar weight to the work of solicitation and curation as it does to the instance of publication. Second, I created unique rows for each mention of a poem or of the work as a whole in the letters held in the Collection. This additional data has allowed me to sketch the shape of individual relationships across a social network that emerges in the anthology tethered to discussions of the anthology. The current dataset contains over 600 data points relating to one small anthology. This dataset is an ego-network in the sense that it take a single node in a social network—Cunard—as its tether. Rather than expanding to the whole network of modernist literary production, it is interested in the relationships between people and publications (in the expanded sense) in context of the event of the anthology.  In the current phase of the project, I am cleaning and refining my data model. I am incorporating name authority files provided to me by the HRC and maintained by standards like VIAF. I am also incorporating node type descriptions drawn from Linked Open Data ontology developed by Linked Modernisms. This phase of the project aligns with what Laura Mandell calls “guerilla coding”—in which projects that attempt to make a cultural critical interventions into technologies and standards must also make themselves legible to those same existing, problematic technologies and standards. As “Modernism, Feminism, and the Ego-Network” emerges from Linked Modernisms, it is already in conversation with the dynamics of canon creation in digital space. LiMo, already more comprehensive in its scope than many canonical DH archival projects, has made admirable attempts to redress a persistent scholarly bias against women’s cultural contributions by partnering with the Orlando project to address the equitable representation of women’s histories. But equitable representation is only one part of the on-going project of re-evaluating and re-narrating women’s historical experiences. A feminist digital humanities approach also requires that we examine the data structures in which we perform this work. In the “guerilla coding” phase of this project, I hope that my ego-network may shift the way that major digital projects construct whole networks. The lessons in feminist data capture and modelling that emerge from this prototype dataset are laying the groundwork for modelling data in relation to the work and communities of women’s cultural production. The immediate next step in this project will be to expand the data model refined in this prototype to the study of Cunard’s other works. As the data model argues that inclusion in an archives has equal weight to traditional publication, it leaves open the possibility of treating private documents like scrapbooks as a work of cultural production. This is particularly fitting for Cunard’s private work: Cunard owned a printing press, and frequently mocked up her scrapbooks to look like volumes for publication, blurring the line between private and public work. Cunard, of course, is not the only cultural producer whose study might benefit from the creation of an ego-network, and later stages of this project will experiment with building overlapping ego-networks in line with the models developed in the prototype. ",
       "article_title":"Ego-Networks: Building Data for Feminist Archival Recovery",
       "authors":[
          {
             "given":"Emily Christina",
             "family":"Murphy",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Canada",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "graphs",
          "repositories",
          "sustainability and preservation",
          "networks",
          "feminist studies",
          "english studies",
          "archives",
          "relationships",
          "English",
          "literary studies",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Open research data is facilitating broader ways of using, reusing, enriching, and linking research results. Many services use metadata to bring the information of different repositories together. Europeana, for example, links material from various thematic focal points with diverse origins and makes a wide range of collections, archives and source objects searchable. Other platforms interlink and aggregate material for one distinct discipline or thematic interest. To connect musicological collections and repositories, we created a metasearch that builds up on annotated music. IncipitSearch is a tool and a service specifically tailored for research on music incipits, the initial sequences of notes that characterise a work. It is simultaneously a centralised data endpoint, where multiple aggregated catalogues can be accessed and searched by their music incipits, as well as a decentralised software and data cluster.  Open Data and Meta Search Engines: Perspectives for Digital Musicology? Open research data is facilitating broader ways of using, reusing, enriching, and linking research results. Many services use metadata to bring the information of different repositories together. Europeana ( https://europeana.eu), for example, links material from various thematic focal points with diverse origins and makes a wide range of collections, archives and source objects searchable. Other platforms interlink and aggregate material for one distinct thematic interest such as Ariadne ( http://ariadne-infrastructure.eu), which makes manifold archaeological contents accessible, or correspSearch ( http://correspsearch.net), which enables to search through collections of editions of letters.  Meanwhile, musicological projects do not only often have digital components, too. Ambitious global catalogue projects like the Répertoire International des Sources Musicales (RISM,  https://opac.rism.info) or national library services such as the catalogues of the Italy's Servizio Bibliotecario Nazionale (SBN,  http://opac.sbn.it) or the Deutsche Nationalbibliothek, (DNB,  https://portal.dnb.de) substantially rely more and more on the digital representation of their data. In addition, overall demand of digital research platforms has lead to born digital editorial projects, e.g. Freischütz Digital, a genuinely digital edition of Carl Maria von Weber’s Freischütz ( http://freischuetz-digital.de) exploring the possibilities of multimedial digital musicological work editions, or the digital thematic work catalogue of the complete edition of Gluck’s works ( http://gluck-gesamtausgabe.de). The researcher’s stronger trust and belief in the benefits of open and accessible research data has led to a stronger emergence of open data policies in musicological projects. In order to interlink existing data repositories and encourage new proposals, a digital data hub is needed. But how can musicological data collections be connected and linked together? In our approach, we concentrated on musical incipits, the initial sequences of notes, that function as identifier for works, and created IncipitSearch, a metasearch for musical incipits.     Encoding Music Incipits One of the main goals of musicological catalogues is making musical works findable and researchable. The main problem that often occurs, especially for music composed before 1800, is that it originally was composed for a singular religious or secular cultural event, e.g. at an aristocratic court to be performed only once or just a few times. Music was additionally bound to formalised genre standards and therefore unambiguous titles were not required. But how to search for a Sonata in D of a composer who has composed 20 sonatas in D? As early as the 1960s, Music librarians introduced the idea to generate a human and machine readable standardised format to identify music by its melodic beginning. For that purpose, Barrey S. Brook and Murray Gold developed the Plaine & Easie Code that allows the transcription of the beginning notes of a musical piece into a combination of numbers and letters. What Brook and Gould pointed out in 1964 was already a distinct definition of and  guide to the Plaine & Easie code system. They introduced it as “an accurate shorthand for musical notation, especially useful for incipits and excerpts.” With some foresight they also stated that “it must be so devised to be readily transferable to electronic data-processing equipment for key transposition, fact-finding, tabulaturing and other research purposes.” (Brook and Gold, 1964)  Plaine & Easie Code is a simple to parse plaintext format and therefore suitable to deliver important metadata for manifold musicological interests. IncipitSearch adopts this standard and at the moment is purely concentrated on Plaine & Easie. However, the future goal is to be capable of reading incipits notated in other formats as well, e.g. MEI ( http://music-encoding.org) or abc notation ( http://abcnotation.com).    Searching Music Incipits  Musc information retrieval systems either build up on audio or symbolic music notation. In digital musicology, that deals with notation and critical digital edition of works, the search in notated music is widely used (Typke et. al. 2005). RISM is undoubtedly the most established repository for musical data. It contains over one million records of historic music materials and over 1,7 million musical incipits (for manuscripts only), which can be accessed using an incipit search ( RISM search). Further incipit search engines build up on the RISM datasets. For example, Utrecht University has developed an extended and experimental search approach offering extended functionalities for user input as well as using sophisticated matching and ranking methods (Van Nuss et. al. 2017).  But other musical incipits exist which cannot be accessed via RISM because they either have not been implemented as data yet or because they are not a type of resource the RISM collection is focusing on and will not be added to the catalogue, such as work catalogues.    IncipitSearch  Scope and Functionalities The efforts to implement incipits in the digital work catalogue of the complete edition of Gluck’s works and to make them searchable have led to the idea of connecting this research data with other repositories and creating even easier ways to instantiate new machine readable incipit repositories. Both digital and analogue catalogues, editions and collections which provide their data in a standardised format can be interlinked with IncipitSearch.  IncipitSearch addresses music that can be displayed in common western music notation. Its main focus lies on music composed prior to 1800. Nevertheless, through its openness it can be furthermore used as a platform to explore challenges in researching culturally and historically different forms of musical notation. IncipitSearch is a tool and a service specifically tailored for research on music incipits. It is simultaneously a centralised data endpoint where multiple aggregated catalogues of incipits can be accessed as well as a decentralised open source software that can be integrated as stand-alone search in other platforms. A microservice based software architecture allows high flexibility in usage and extension of individual components (Haft et. al. 2015).  IncipitSearch enables users to enter search queries in the search field by playing them on a virtual piano keyboard while Plaine & Easie Code can also be directly entered into the search field. Search with transposition or with exact matching can be selected ( https://incipitsearch.adwmainz.net). Next to the found concordant incipts, the result list displays backlinks to the entry in the respective catalogue.     Screenshot of the search interface of IncipitSearch.    Metadata Schema To enable a standard suitable for the different types of musicological repositories such as digital and analogue catalogues, editions and collections and to provide an output of the collected data, we have developed an easy to understand RDF schema using the schema.org vocabulary. Besides being recommended by the W3C, cross-linking possibilities for data and the possibility to rely on various vocabularies for specific topics, the interoperability and the multiple serialisation formats for RDF are advantageous.  Schema.org provides a vocabulary for the description of web pages. The initiative of several major search engine companies aims to develop a simple vocabulary to add semantic information to webpages. These vocabularies were designed in collaboration with domain experts. For the markup of music information, the data type MusicComposition ( http://schema.org/MusicComposition) supplies most elements to describe a work and its parts. To add the possibility of describing music incipits, we have expanded the vocabulary with further elements. The format can be used directly for data interchange - a feature request for the extension of schema.org with incipit declaration is planned.  The metadata format functions as an acquisition format for the repositories. It can be used to add information to the catalogue by adding music incipits to existing resource as well as a schema for the annotation and digital publication of analogue catalogues. Moreover, it will provide the aggregated data in a standardised format to enable further usage.     Conclusion At the moment, IncipitSearch aggregates the incipit data of the catalogue of Gluck’s works, the SBN OPAC, the RISM OPAC and includes a sample data set of the thematic Breitkopf Catalogo delle Sinfonie 1762. IncipitSearch builds on the potential of open musical data and provides the possibility to interlink musicological repositories of various types. This is accomplished by concentrating on musical incipits and using a standardised data interface, a straightforward metadata schema and encapsulated software components. Through consistent usage of authority control and metadata standards, IncipitSearch is an open source tool and service warranting sustainability, transparency, and accessibility of research data.   External Links  Europeana:  https://europeana.eu  correspSearch : http://correspsearch.net  Deutsche Nationalbibliothek (DNB):  https://portal.dnb.de  Freischütz Digital:  http://freischuetz-digital.de  IncipitSearch : https://incipitsearch.adwmainz.net  Répertoire International des Sources Musicales:  https://opac.rism.info  schema.org: http://schema.org Servizio Bibliotecario Nazionale (SBN):  http://opac.sbn.it  Work catalogue of the complete edition of Gluck’s works (GluckWV):  http://gluck-gesamtausgabe.de    ",
       "article_title":"IncipitSearch - Interlinking Musicological Repositories",
       "authors":[
          {
             "given":"Anna",
             "family":"Neovesky",
             "affiliation":null
          },
          {
             "given":"Frederic",
             "family":"von Vlahovits",
             "affiliation":null
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "repositories",
          "sustainability and preservation",
          "information retrieval",
          "music",
          "archives",
          "English",
          "semantic web",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper discusses the endeavours of the research project  MIMEHIST: Annotating EYE’s Jean Desmet Collection  (2017-2018) - funded by the Netherlands Scientific Research Organisation - to do optical character recognition (OCR) and apply various computer vision techniques on the business archive of film distributor and exhibitor Jean Desmet  (1875-1956) .   The Desmet collection consists of approximately 950 films produced between 1907 and 1916, a business archive containing around 127.000 documents, some 1050 posters and around 1500 photos. The Collection is unique because of its large amount of rare films from the transitional years of silent cinema, and because of the richness of its business archive which holds extensive documentation of early film exhibition and distribution practices in the 1910s. These features contribute to its immense historical value which was one of the main reasons why it was inscribed on UNESCO’s Memory of the World Register in 2011. By OCRing and classifying Jean Desmet’s business archive, MIMEHIST will allow scholars to browse and annotate its documents - all scanned in high resolution - in the new ‘Media Suite’ of the Dutch national research infrastructure (CLARIAH). The results will be integrated in a search interface enabling media historians to identify word frequencies and topics as a basis for research on early film distribution and exhibition and, the paper argues, open for media historical research which productively builds on and expands the collection’s use in previous scholarship.  Throughout the past decades, Desmet’s business documents have offered a rich source for socio-economic cinema history. Media historians such as Karel Dibbets and Rixt Jonkman have studied parts of the collection’s (related) data by manually transcribing and organising it into databases (Jonkman, 2007; Dibbets, 2010). This work produced an empirical, quantitative foundation for network analysis of Dutch film distribution and exhibition in cinema’s earliest years. However, this research also made evident that the archive is too large and diverse to organise and transcribe manually. A particular challenge is that collection contains many different kinds of documents: personal letters, business letters, records of film rentals, postcards, newspaper clippings, telegrams, scraps of paper with notes, photographs etc. Furthermore, some documents are typewritten, others handwritten.    To allow scholars to research and annotate larger amounts of the archival documents’ data in CLARIAH’s Media Suite , automated information extraction from the documents seemed challenging yet promising.  MIMEHIST took up this challenge by trying OCR, document classification, topic modelling, named entity recognition and other visual and linguistic tools on the set of scans in order to extract as much data and metadata from the individual documents as possible. Different document types required different treatment. For instance, we quickly determined that it did not make much sense to do OCR on a  tiny handwritten note, while handwriting detection on the other hand would be possible and could yield productive results on such an item.   Experiments were conducted in visual document classification, visual document analysis and distant reading. Visual document classification was performed by clustering a combination of color and texture histograms derived from the scans. This step was taken mostly because the existing index of the archive is incomplete: it has information on the folders in the archive, which contain the documents, but not the documents themselves. The Media Suite works with individual documents, not folders, so it became necessary to, for instance, discern sub-folder covers from the documents inside.  A second reason to do classification was that each type of document needs a different kind of processing - typed letters can be OCR-ed, but not photos, while handwritten letters could be classified by comparing handwriting styles. By separating different document types it became possible to employ the most effective information extraction tools on them. This procedure also allowed for finding visually similar documents, making it possible for researchers to look for similarities in for instance texture or color.  The typewritten documents were OCR-ed, then classified on the basis of the recognized text in order to differentiate e.g. personal letters from business correspondence. Named entity recognition on the texts provided us with a network of people and places, with links to the letters. Attempts at handwriting recognition on the basis of ‘image texture’ histogram comparisons provided mixed results, - for the instances where larger samples of a single person’s handwriting were available it worked reasonably well, but for handwriting types occurring only a few times the confidence of the classifier was too low and such documents were classified as one of the more frequently occurring types. The results of these steps, in combination with the existing index’s metadata, provided a rich enough metadata structure for the use of individual documents in the tool.  In addition to a discussion of these steps, our paper reflects on the results’ epistemological implications for future research, by discussing them in relation to previous quantitative approaches to the Desmet Collection. From this vantage point, our paper argues that while previous quantitative studies of Desmet’s business documents were premised in the coding and transcription procedures of Cliometrics and  Annales  historiography, MIMEHIST’s results nurture exploratory and qualitative research coupled with serendipitous search and annotation procedures focusing also on visual features. Consequently, the paper argues, researchers may to a greater degree than hitherto highlight data contingencies and multiplicity of viewpoints in the Desmet business archive.  ",
       "article_title":"OCR’ing and classifying Jean Desmet’s business archive: methodological implications and new directions for media historical research",
       "authors":[
          {
             "given":"Christian Gosvig",
             "family":"Olesen",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, The Netherlands",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Ivan",
             "family":"Kisjes",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, The Netherlands",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "computer science",
          "audio",
          "cultural and/or institutional infrastructure",
          "creative and performing arts",
          "digital humanities history",
          "film and media studies",
          "video",
          "theory",
          "epistemology",
          "data mining / text mining",
          "English",
          "including writing",
          "criticism",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The collected works of Leo Tolstoy were printed and published in 90 volumes of some 46,000 pages between 1928 and 1958. The visibility and usability of these volumes were increased by the project \"Tolstoy Digital\", a TEI-encoded version of this vast resource (Skorinkin & Mozhaev 2016). This talk, however, is not about the 90 volumes themselves, but about the 91st volume of this edition, a supplement volume containing indexes of works and proper names, from both the fictional works and the many volumes containing Tolstoy's letters. \"The 91st Volume\" is a web application based on the digitised index of proper names for the 90-volume collection of Tolstoy's collected works ( ). The digitised data features additional properties, which can be explored by the enthusiast as well as the specialist.  This talk tries not just to present a new tool for literary scholars, but tries to generalise how this kind of resources can be used to gain new insights into larger text collections.    Level 1: Enhanced Searches  First and foremost, the index retains its original functionality, which is to map names to volumes and pages. Collected works of a canonical writer are not primarily meant to be read one by one, line by line. A 90-volume collection of books does not only contain entertaining narratives, but it can also be viewed as a set of facts, dates, names, mentions, etc. An index is the key to this data, and it was the only means to gain some orientation in the pre-digital age. In the web app version of the \"91st Volume\", the index is even more convenient to use than in the paper version, as it allows \"fuzzy\" searches. By entering \"ava\" it will list among the results terms like \"Poltava\", \"Bavariâ\", or \"Abdulla-al'-Mamun Zuravardi\". The higher the frequency of a name within the whole collection, the higher up it will be displayed in the results. These types of searches are already an enhancement over the traditional index search. If we cannot define in advance what we are looking for, we still have the lists of all names in the index (which sum up to more than 16,000 entries). Once we've found what we were looking for, we don't need to remove any book from its shelf and open the right page, but can jump directly to the corresponding page. A graphical word-cloud representation is also featured and conveys a first idea about the most frequent words in the corpus.    Level 2: Studying Life and Works of Leo Tolstoy by Means of Network Analysis  Turning an index of names into a network is a new approach to facilitate the study of contexts. The co-occurrence of names in the same environment (on the same page, in the same chapter, etc.) reveals similarities and relations between different entities, which on the scale of 90 volumes, helps us to understand larger contexts. \"The 91st Volume\" unfolds a rather unconventional social network of Leo Tolstoy. It shows not only Tolstoy's connections with other people (e.g., his pen pals), but also the connections of people from the point of view of Tolstoy. The co-occurrence of proper names on the same page within the 90 volumes establishes an edge of the emerging network as it creates a link between two entities. For example, the Hindu scripture \"Bhagavat-gita\" can be found five times on the pages of the Complete Works, and it shares these five pages with a total of 43 other names mentioned. The proximity of these mentionings is not accidental, of course, in our example they form some kind of \"Indian cluster\" containing works like \"Gitopadeša\", \"Dhammapada\", \"Vamana Purana\", or names like Ramakrišna Šri Paramagamza. For Tolstoy, the mentioned texts are part of a set of carriers of philosophical knowledge, and are associated with names like Xenophon, Montaigne, Montesquieu, Pascal, Skovoroda, Socrates. These networks provide great opportunities for understanding the whole range of Tolstoy's interests and ideas. It presents a panoramic picture revealing general trends and larger thematic clusters. For each individual name there is also a small graph showing the most significant names associated with it. Another new kind of access to the 90 volumes is a heat map that shows the density of proper names used in each of them (the more names mentioned, the warmer the colouring). In the first volume of the collection containing youth experiments, a red splash suddenly appears in the middle of a rather calm blue background on page 269. You can view this page and will find that it contains a list of European cities: Rome, Naples, Dresden, Berlin.    Level 3: Editorial Evolution of the \"Complete Works\"  The index also allows scholars to study the coming into life of the \"Complete Works of Leo Tolstoy\", i.e., the difficulties that had to be overcome when working on this edition (as they are laid out in Osterman 2002). The \"91st Volume\" allows us to understand how editorial principles have changed over time, especially as regards the depth of commenting. For example, the 13th volume, with draft editions of \"War and Peace\", has a weak commentary, and the 47th volume (diaries and notebooks) features such detailed comments that it is the most detailed in the entire 90-volume edition. Quantifications like this allow us to draw conclusions to the process of editing the Complete Works over three decades. Like mentioned above, the web app retains all the capabilities of the traditional index, and at the same time extends its potential through computer-based information management, a multi-purpose search engine and different kinds of visualisations. The app is to be understood as a suggestion to apply the newly developed methods to the Collected Works of other authors.  ",
       "article_title":" The 91st Volume — How the Digitised Index for the Collected Works of Leo Tolstoy Adds A New Angle for Research  ",
       "authors":[
          {
             "given":"Boris V.",
             "family":"Orekhov",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Frank",
             "family":"Fischer",
             "affiliation":[
                {
                   "original_name":"National Research University Higher School of Economics, Russian Federation",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "philology",
          "linguistics",
          "text analysis",
          "visualisation",
          "English",
          "literary studies",
          "semantic analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" At the beginning of a research project, every scholar in the humanities asks a question: what should I read? This paper presents a new search engine that sifts through large corpora of unstructured text in order to find particular passages that deal with a concept of interest. Its underlying algorithm is based on the practice of concept search, which was originally developed in legal practice to efficiently automate the process of document review (Blair, 1985; Bai, 2005; Algee-Hewitt, 2008; Zhu, 2009; King, 2017). Our search engine builds upon that technique by applying it to large corpora relevant to humanistic scholarship and, crucially, dividing each text into passages of a standard size, improving the specificity of results. In order to demonstrate the fullest potential of this technique, our paper will focus upon its application to a specific problem in eighteenth-century intellectual history, while also discussing its most significant theoretical implications, including a reevaluation of the tight connection that has developed between the computational evaluation of the great unread through distant reading (Cohen, 1999; Moretti, 2013). Concept search for large text corpora  Our team has expanded the paradigm of text discovery by developing a search engine that sifts through large digitized corpora to identify passages that deal with particular concepts. The algorithm itself is simple. After reading deeply in a subject, a researcher gathers together a number of passages from various sources that focus on a particular concept of interest. The researcher then uses a word-frequency analyzer to judge which of the remaining terms are most important to describing his or her concept. That keyword set is used to perform a search. The search uses a number of statistical measures to determine which items are likely to be most relevant, and produces results either as a downloadable csv spreadsheet or a GUI. The researcher can then read through the results in a preliminary fashion to judge whether they are satisfactory. If they are not, the process can be iterated.  Concept search is a statistically-based method of information retrieval that has been adopted widely in legal practice and the business world, but that is only rarely used in the humanities. Concept search and keyword search are not the same. Classical Boolean keyword search queries are predicated on single terms and phrases. Concept search, on the other hand, searches for a cluster of terms drawn from a loading set of passages identified by a researcher, which potentially encode the underlying semantic quality of the passages in that set. (It should be said, this sense of “concept” is statistical and highly artefactual, quite different from the sense of the term in psychology, linguistics, or philosophy.) Keyword search simply looks for matches between query terms and the documents in a corpus. Concept search, on the other hand, does not require the occurrence of any one of its search terms, instead relying on a variety of statistical measures to judge which passages in the search corpus are most likely to be useful. Keyword search often returns non-relevant items because of the problems of synonymy and polysemy. Concept search attempts to overcome these problems, as well as that of OCR error, by representing a concept as the statistical likelihood of the occurrence of the cluster of terms in its query set.   What makes this search especially useful is that results are ordered, not by  volume, but by which  passage is most likely to be relevant to a particular concept. We originally developed our search engine to search through Cengage-Gale’s Eighteenth Century Collections Online database. In doing so, we divided its volumes into 16 million passages, each of 1,000 words. Compare this technique to the traditional method of identifying which volumes to consult. It is akin to asking a librarian for material relevant to a research topic, and having that librarian not only identify which books are likely to be of interest, but also opening each volume to the particular page that most clearly deals with that topic.   Each set of results is essentially an index of hundreds of thousands of passages, sequentially ordered by which are most likely to be relevant. Instead of only displaying page after page of search results that must be consulted one by one, it offers researchers the option of downloading a single spreadsheet that is easy to filter by author, work, and date, and that allows for a quick, global view of which texts are relevant. In order to make sense of this data, one has to sort and filter judiciously. We have developed a number of standard search filters that can be used repeatedly to select for literary works, canonical works across disciplines, and an author’s gender. Or, should a researcher prefer, she can quickly select for one, two, or fifty authors that she would like to examine. Sorting is equally important. The most basic statistical measure is term frequency, which counts each time a keyword appears in a particular passage. The search engine also allows for sorting by other statistical measures, including the proportion of keywords (useful if there is a great deal of error in a passage due to the process of digitization, or if there is a relatively high number of stopwords), tf–idf. Searching for eighteenth-century principles and theoretical implications In order to demonstrate the fullest potential of this technique, we present a use case that concerns a specific problem in intellectual history. In the eighteenth century, many of the most famous authors obsessed over the possibility of encapsulating a whole book, or even an entire discipline, in a single proposition called a principle. Among these are the best-known statements of the Enlightenment, including Newton’s inverse-square law of gravity and Kant’s categorical imperative. David Hume put it succinctly. A principle, he said, offers “a whole science in a single theorem” (Hume, 1987). By promising to encapsulate and disseminate an author’s most fundamental ideas, the principle became the preeminent intellectual device of the Enlightenment. In some cases, however, less famous principles might lie buried in books, some obscure. The most common research tools and methods – long reading, critical bibliographies, and Library of Congress subject headings – would be of only limited help in discovering these. Our search engine provides an efficient means of discovering passages in which authors frame principles and reflect upon the consequences of this rhetorical obsession. In accounting for the eighteenth-century enthusiasm for principles, one important question that has been difficult to answer using traditional research methods is what women thought of this rhetorical habit. The principle is undeniably masculinist (etymologically, the word itself is tied to seeds and semen), and the prevailing assumption in the period was that men framed principles, so much so that Rousseau claimed in his pedagogical treatise  Émile that women lacked the ability to generalize their ideas. By applying standard filters to our search results, we are able to efficiently identify important passages in works by female authors, in which they chafed against the pervasive insistence that women were shut out from the culture of the principle.  The final section of our paper concerns the theoretical implications of the tool we have developed. We interrogate the common denigration of search that many digital humanists have voiced (Moretti, 2007; Berry, 2012; Jockers, 2013), while also questioning the tight connection that has developed between the critical concepts of “the great unread” and “distant reading” (Moretti, 2000; Sculley and Pasanek, 2008; Trumpener, 2009). Our position is that many of the strongest examples of digital scholarship treat the great unread as a textual noumenon that can only be approached obliquely and in its totality, through the analysis of minimal textual features. The search engine we have developed allows computational methods to invigorate more traditional modes of reading by helping researchers to quickly draw together a project-specific list of works that comprise canonical and non-canonical material. As such, it promises to open new avenues for research, both for those scholars committed to historicist methods, those exploring alternatives to critical modes of reading (Best and Marcus, 2009; Shore, 2010; Pasanek, 2015), and those who wish to rethink scholar’s pervasive recourse to context, in an effort to trace how ideas move across history (Felski, 2015). ",
       "article_title":"Searching for Concepts in Large Text Corpora: The Case of Principles in the Enlightenment",
       "authors":[
          {
             "given":"Stephen",
             "family":"Osadetz",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          },
          {
             "given":"Kyle",
             "family":"Courtney",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"DeMarco",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          },
          {
             "given":"Cole",
             "family":"Crawford",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          },
          {
             "given":"Christine Fernsebner",
             "family":"Eslao",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "corpora and corpus activities",
          "information retrieval",
          "digital humanities history",
          "resource creation",
          "library & information science",
          "english studies",
          "English",
          "theory",
          "and discovery",
          "digitisation",
          "epistemology",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In 2015, our team began work to get the Mayan hieroglyphs into the international standard Unicode, so Mayan text can be reliably interchanged on computers and other devices. Thanks to collaboration with standards experts and recent advances in computer technology and Mayan decipherment, the work to encode Mayan in Unicode has progressed significantly from the state we reported at DH2016 [1]. This paper will describe the challenges that prevented scholars from encoding Mayan in the past, and the strategies we used to overcome these hurdles. We will also give examples of the rapidly expanding repository of digitally encoded, machine-readable Mayan texts, and report on the implications for future research.             Figure 1. The first two Mayan texts selected for encoding in the Unicode format: parallel “cognate” almanacs on the Dresden (D38b-D41b) and Madrid Codices (M10b-M13a). Cf. Bricker and Bricker 1988; Aveni et al. 1996: Fig. 10; Aveni 2004:158-159       Past Problems and Current Approaches  Early attempts to apply computational approaches to Mayan decipherment from the late 1960s and 1970s proved premature, given the technological limitations of the time [4; 8]. In addition, the stage of decipherment then allowed only a fraction of Mayan syllabic and logographic signs to be read with any certainty (probably less than 30%), compared to what is possible today. Subsequently, a number of Mayan database projects attempted to cover the full corpus of Mayan texts [2; 11; 15], encompassing thousands of inscriptions and spanning almost 2,000 years across multiple regions. Given the wide range of scribal practices across such a broad spectrum of space and time, it has proven difficult to identify a core set of characters.   Our project, on the other hand, decided to focus on the extant Mayan codices, three of which are preserved in libraries in Dresden, Madrid, and Paris (see figure 1). These Codices are widely accepted to originate in Pre-Columbian Yucatan, Mexico, during the late Postclassic period (ca. AD 1250-1519). This strategy proved fruitful, as these ancient documents attest to much greater consistency in the range of scribal practices, and make use of a relatively limited, time-specific, common repertoire of signs. Ultimately, inscriptions and earlier Classic-period texts can be added, building upon the base repertoire of the Codices.              Figure 2. Examples of our workflow relying on glyph-block cluster arrangements or “quadrats” for rendering complex Mayan signs from the Dresden Codex.                Figure 3.  Database screenshot from our new semi-automated Mayan transliteration and translation functionality      Format Because most database projects operated under largely non-standardized formats, Mayan textual data could not be widely shared, but was limited to those institutions that shared the same (non-standard) formats. In contrast, we centered our efforts on getting the script into the international character encoding standard Unicode. The advantages of Unicode include:   wider accessibility, since Unicode is an open-source standard that is freely available to all users and developers. This would facilitate scholars and humanities students to contribute to improving and expanding the ongoing textual repository of encoded, machine-readable Mayan hieroglyphic texts.  reliable communication, ensuring the recipient of a text will receive the text as originally sent.  enhanced searching and querying capabilities, as well as advanced text mining, in ways that are not possible with annotated collections of pictures, drawings, or scans of ancient inscriptions.  greater compatibility and discoverability with/through existing online resources, such as graphemica.com, and other aids.  Improved long-term archiving and preservation of textual records, given the stability of the standard and the ease of depositing multiple copies.   Technical Issues and Solutions    The degree of visual complexity prevalent in the Mayan script has posed unique challenges, both in describing the data in a standardized way, and being able to accurately render signs with standard font protocols. This complexity includes ligation of signs, horizontal and vertical joining, truncation, and infixation (figure 2), as well as identification of cluster arrangements (“quadrats”) and the placement of signs in the clusters. To our knowledge, no other projects beside ours have focused on identifying all attested cluster arrangements in the codices consistently under the Unicode Standard [ 12] while describing sign-positioning in ways supported by new font and software upgrades.     To overcome the technical challenges involved, we are working directly with Unicode specialists and implementers, whose experience in encoding other writing systems has informed our methodology, specifically in describing the cluster arrangements and the database structure. Thus, our description of the cluster arrangements drew from work on Egyptian hieroglyph quadrats by Andrew Glass [ 3]. Based on input from collaborators, the database has been set up so it can generate real-time semi-automated transliterations and translations of Mayan hieroglyphs into English and Spanish (figs. 3-4). This system is also capable of breaking down visually complex glyph-block cluster arrangements (“quadrats”) into their constituent individual signs and displaying them in linear fashion.            Figure 4.  Database screenshot from our new semi-automated Mayan transliteration and translation functionality (multi-record table view).    Research Results Our analysis of the main textual contents of the Mayan Codices has enabled us to attain a full analysis of all the extant hieroglyphic inscriptions in the Mayan Codices   Carlos: Were you going to include Table 1?   (Table 1). It also resulted in the identification of the full range of permutations by which individual signs (graphemes) conform into glyph-block arrangements or specific cluster-configurations (i.e. “quadrats). These quadrats unfold into 167 different types [ 12], which we ordered into classes, ranging from one up to six signs per glyph-block (Figs. 3-4). We have also developed a “mapping engine” able to segment the Mayan Codices into a meaningful, hierarchical arrangement of their constitutent levels and segments. This tool can help to identify the underlying thematic composition and structure of the Codices and other complex texts at different levels (i.e. glyph-block, phrases, frames, almanacs, pages, sections/chapters, volume), in much the same way as scholars of Western literary tradition have been able to identify medieval text structures [14]. For instance, this engine allows for laying out the structure of the Dresden Codex as a document composed of 74 pages, plus four blank pages, arranged into 22 sections, 96 almanacs and tables, and 575 frames.    “Paramount to” doesn’t quite work here in English; I suggest alternate wording that I think captures the idea.  A key component of our efforts has been the creation of a new Unicode-based “glyphary” tool, a comprehensive digital catalogue of characters (graphemes) exclusively occurring in the Codices, which substantially departs from previous efforts [4;11;16;19] by its underlying methodology and novel taxonomy. In this methodology   The sentence was getting too long (for English readers!), so I’d suggest breaking it up. You can insert a more appropriate term in place of “methodology”, if needed.   alphanumeric codes are tied together to code points assigned by the Unicode Standard. In developing this glyphary tool, we relied on previous collaborations between one of us (Pallan) and teams at IDIAP and UniGe (Switzerland) for developing digital multimedia resources and machine vision algorithms suitable for Mayan [6;7].  Implications of Research  Based on the above results, our paper provides a critical look at the implications for Mayan scholarship and the humanities, including the degree to which the codical sign-set compares to the earlier sign-sets from the Classic-period monumental inscriptions. We also explore specific idiosyncrasies and the global patterns that can be identified within codical texts and datasets, partly by programming highly specific SQL (Standard Query Language) queries for addressing these and other culturally significant questions. For example, which lexical terms occur with greater frequency, to which semantic and grammatical categories do these terms belong? Which major languages are represented and what is the affiliation of the lexical terms attested [9; 17;18]? We also develop indicators that permit approaching complex scribal features and practices within the codices, such as the degree of phoneticism and the ratio of individual signs per glyph-block (see Table 1 below). Our system also allows precise mapping of undeciphered and problematic signs and identifies the contexts in which they occur.    Codex: DRESDEN MADRID PARIS   Number of extant pages 74 pages 112 pages 24 pages   Number of almanacs 96 (75 almanacs + 21 tables) 237 (almanacs & tables) 18 (almanacs & tables)   Number of frames 575 / 1659 total 889 / 1659 total 192 / 1659 total   Number of glyph-blocks  2951 / 7122 41.43%   3340 / 7122 total 46.89%   831 / 7122 total 11.66%    Blocks per frame ratio 5.13 blocks per frame 3.75 blocks per frame 4.32 blocks per frame   Number of graphemes (main-text signs, not counting calendric grid)  7208 / 17147 total 42.03%   7913 / 17147 total 46.14%   2026 / 17147 total 11.81%    Signs per glyph-block ratio: 2.442 signs per glyph-block 2.369 signs per glyph-block 2.438 signs per glyph-block    Table 1. Comparative statistics derived from analysis of three extant Mayan Codices  Future Work and Goals Plans for future work include further development of advanced OpenType Mayan font —in close collaboration with Andrew Glass— providing more accurate rendering of linear signs into glyph-blocks. We are also planning to expand our resources into the realm of the monumental stone inscriptions, in collaboration with Dr. Gabrielle Vail and other researchers, with the aim of generating robust, representative new datasets of texts from all the major Classic and Terminal Classic scribal traditions. thereby substantially increasing the range of chronological/regional variability of our textual repositories.  To facilitate collaborative team data editing and analysis, we are currently migrating some of our core database functionalities into  a MySQL-based server that offers greater compatibility with widely used open source solutions such as SQLite and MariaDB. On the longer term, we are collaborating with the READ ( Research Environment for Ancient Documents ) co-creators Andew Glass and Stephen White [ 13 ] to adapt and expand this powerful engine into a “Mayan-READ”. This tool would provide scholars and humanities students with the full-range of our open-access online resources, allowing them greater access and interactivity with our datasets, plus the ability to contribute in expanding a Unicode-based repository of digitally encoded, machine-readable Mayan hieroglyphic texts. In so doing, we are also seeking to establish innovative collaborations with cultural institutions and research groups (such as INAH in Mexico). Part of this effort involves organizing workshops, where our workflow and methodologies can be learned by other teams  in Mexico and other locations, and ultimately put into practice in ways that can have greater impact to benefit the humanities research community as a whole.   ",
       "article_title":"Achieving Machine-Readable Mayan Text via Unicode: Blending “Old World” script-encoding with novel digital approaches",
       "authors":[
          {
             "given":"Carlos",
             "family":"Pallan Gayol",
             "affiliation":[
                {
                   "original_name":"University of Bonn",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Deborah",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"University of California at Berkeley",
                   "normalized_name":"University of California, Berkeley",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an7q238",
                      "GRID":"grid.47840.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "archaeology",
          "computer science",
          "repositories",
          "corpora and corpus activities",
          "sustainability and preservation",
          "linguistics",
          "machine translation",
          "archives",
          "data mining / text mining",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    This paper is based on our commitment to the possibilities of re-thinking the processes of digitization such that digitization does not end with the uploading the scanned object and archivally-mandated metadata. Rather, that point is merely the beginning of the life of any particular digital collection. The ways that any collection is used by academic researchers, community groups, and members of the public should contribute to the processes of digitization. These collections live when they are used and these uses should be reflected in the collection so that other researchers can see and build on this work. Every collection that has been digitized has an afterlife. But how can we use new technologies – in particular, the International Image Interoperability Framework (IIIF) and linked data –  in order to make these afterlives visible and usable? How can we develop infrastructure and protocols so that the metadata  lives ?   Our project focuses on building a platform for annotations based around a specific collection of images: the Chinese Immigration records, initially captured by Library and Archives Canada (LAC), and subsequently digitized, preserved and made accessible online by Canadiana. There are approximately 41,000 images in this particular set of archival images that we work with. Canadiana has recently completed the digitization of this collection. Because it is comprised of a nearly complete set of immigration certificates for individual Chinese migrants collected between 1910 and 1953, the collection is particularly rich for researchers working in the area of race, immigration history, and citizenship.  In working with these materials, Lily Cho’s research team has identified several layers of annotations that would be pertinent to this material. For example, the research team has transcribed names of each immigrant on the record. Each image contains two names: the anglicized Chinese name written down by an immigration agent, and a name in Chinese script written by the migrants themselves. In our transcriptions of the first several hundred images, there is no correspondence between the name written by the immigration agent and the name in Chinese script. Because these records were used to identify individual immigrants for the purposes of allowing them to exit and enter Canada (and thus functioning much like a passport for Chinese immigrants who were, during this period, denied the rights of citizenship), this finding radically changes our understanding of how Chinese immigrants navigated racist immigration controls during this historical period. However, there is currently no way for her research team to contribute to the metadata already attached to this collection.   Such contributions to the metadata already in place function as annotations in this project. Working in partnership, Cho and Julienne Pascoe, who has been the Lead Metadata Architect for Canadiana and is now serving as a Digital Archivist at LAC, are developing a platform for supporting annotations for this archive using the Web Annotations standard, the IIIF, and linked data. Canadiana is currently in the process of implementing IIIF as well as the initial stages of developing a data model that would provide the foundation for such a partnership. In short, this project  uses IIIF as a framework for enabling open standards for annotations that can then be reused as linked data - all three areas coming together to support the linking, sharing and re-use of metadata.    This paper reports on the progress we have made in developing this platform, and will also briefly outline the possibilities for the use of this platform beyond this specific collection of images.  Although museums and archives are under enormous pressure to digitize their collections, and are rapidly in the process of doing so, these digitization initiatives are rarely undertaken in conversation with some of the primary users of these digitized texts and objects: academic researchers. For example, metadata that meets archiving standards is not necessarily useful for researchers, and is often based on hegemonic archival practices that reinforce colonial structures and narratives. At the same time, academic researchers often have resources to contribute to, and enrich, the digitization that has been accomplished as well as facilitate postcolonial interpretations of the archive. This project brings academics and digital archivists together in order to develop protocols so that digitized collections can be dynamically connected to the communities using them. Once digitized, a collection does not need to remain static. It can respond to, and include, the findings of researchers in the community; and these findings could and should be made available to other users of the collection. However, protocols for curating, organizing, and disseminating this information must be developed. This project will use one specific collection, an archive of approximately 40, 000 head tax certificates held by LAC and digitized by Canadiana, as a test case for developing precisely the kinds of protocols that would allow a digitized collection of materials to leverage the findings emerging from people using these materials.   ",
       "article_title":" Afterlives of Digitization  ",
       "authors":[
          {
             "given":"Lily",
             "family":"Cho",
             "affiliation":[
                {
                   "original_name":"York University",
                   "normalized_name":"York University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/05fq50484",
                      "GRID":"grid.21100.32"
                   }
                }
             ]
          },
          {
             "given":"Julienne",
             "family":"Pascoe",
             "affiliation":[
                {
                   "original_name":"Library and Archives Canada; Canadiana.org",
                   "normalized_name":"Library and Archives Canada",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/00a25bg52",
                      "GRID":"grid.431349.8"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-14",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "linking and annotation",
          "historical studies",
          "resource creation",
          "and discovery",
          "sociology",
          "English",
          "archives",
          "metadata",
          "digitisation",
          "asian studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper presents a practical approach to building digital humanities (DH) at a university, across disciplines with diverse requirements, starting without institutional support, with scarce staff on a low budget. Examples are provided from the Centre For 21 st Century Humanities (C21CH) at University of Newcastle, Australia (UON).  Digital humanities (DH) requires expertise that crosses many fields from specific humanities disciplines to software development and production management. DH has a broad range in scale – from a scholar learning basic programming to hack a Python script, to multi-institutional collaborations on neural network learning. Few people are experts in all these fields meaning DH is often a collaboration. The requirements for any individual DH project can differ greatly also requiring IT skill sets that may not be easy to find in any one individual. This makes it difficult for university humanities departments with no spare cash, and often reluctant to invest heavily in IT, to successfully support DH, yet DH projects present problems beyond standard service offerings and provisioning and different to STEM. The Digital Lab of C21CH at UON has evolved an approach, here called ‘rapid bricolage’, that has successfully delivered a range of sustained internationally recognised DH projects influencing national debate. Some comparison will be made with other approaches, and while not necessarily suiting all circumstances, ‘rapid bricolage’ has proved an effective approach catering to characteristic issues in DH research, drawing from but differing to established IT practice.  This ‘rapid bricolage’ approach draws on ‘rapid’ software development and ‘bricolage’, both common practice in software development and humanities, but modifies them to meet the unique needs of Digital Humanities. These modifications are epistemic, structural, methodological and a matter of degree. It has also crucially involved consultative processes to identify and Pareto prioritise inter-disciplinary interests and achievable, feasible, high impact projects. The success of these feeds back to build interest and support for DH towards funding and growth, and results in project driven infrastructure, bridging the gap between projects without infrastructure and infrastructure without projects by beginning with demonstrable utility and developing with shared human and technical resources.  C21CH projects include (http://c21ch.newcastle.edu.au):   Colonial Frontier Massacres (v1.2) – map of massacres in Australia. EMWRN archive (v2) – innovative archive of material cultures of early modern women’s writing. Intelligent Archive (v3beta) – stylometry software. ELDTA site (v1alpha) – linguistics web player for media with tiered glosses, translations etc. Text To Map (prototype) – online automatic recognition and mapping of places in texts, linking to and from the text and the map. Scriptopict (v1) – annotations for images eg:  Battle of Kurukshetra Mural and  Mixtec Glyph.   Rapid Rapid application development is an established methodology for software development focusing on getting a prototype working as early as possible followed by regular review with clients and incremental feature additions and bug fixes. For humanities departments this approach ensures that at least some software exists as an outcome of initial spending when the budget is tenuous and provides an encouraging proof of concept. For the cost of a meeting with several professors or executives a working prototype can be developed, making it worth simply trying it out rather than lengthy discussions about the value of proceeding. A rapid approach also helps greatly when the client is unclear of what is needed or has little understanding of IT. An early prototype establishes confidence and commitment early. Gaps in desired functionality immediately become clear through interaction. In particular because research is heuristic and highly changeable it allows for speculative requirements to change as the project progresses. Because of this, an even more rapid than usual approach is suited to humanities research because, as research, not all requirements cannot be known in advance. The process necessarily involves taking some action with ongoing revision, addition and enhancement. Not all aspects of humanities research activity, such as thorough rumination on a nuanced argument on a complicated problem, fit this ‘rapid’ model, but: - the speculative, heuristic activities necessary to research are enhanced;  - some slower methodical activities essential for rigour and completeness can be sped up, sometimes making research possible that otherwise would not have been, or improved through the need for clear structures and definitions;  - the 'slow' process of rumination, of considering complex problems and developing arguments, while irreplaceable, can be augmented. Bricolage Bricolage is a well-established approach in software development. Software is typically put together from pre-existing libraries, frameworks and cut and pasted code that is modified and added to, to produce something that works in ways that conventional intellectual property and copyright are not practically applicable to. This approach is in sympathy with developments in critical theory in the late 20 th century and after, with ‘bricolage’ and the problematization of authorship being major themes in describing postmodernity and in contemporary humanities methodology. Just as a very rapid approach suits humanities research so too is bricolage especially suitable for DH.   As research, DH typically requires constant and regular modification and adjustment, rather than delivery of a working system according to contracted specification. Much software is developed for a STEM or commercial purpose, or has a STEM like approach to problem solving. STEM and the commercial sector have larger budgets and devote larger budgets to software. This means that humanists are often in a pragmatic situation of re-using software from different disciplines despite having divergent requirements. Humanities often focus on complexity, exceptions, structural change and highly contingent historical (not repeated) events, while STEM and commerce focus on systemisation, normalisation, ceteris paribus and repeatability, for example. If humanities researchers are to avoid fitting research to the software limitations this means constantly adapting systems to their own different epistemic, ontological and methodological paradigm, ie: bricolage.  The DH research need for these two approaches, rapid application development and bricolage, combined in extremis presents challenges to established IT practice. These challenges can be met with appropriate staffing, strategy and a ‘rapid bricolage’ approach to build DH at a University despite diverse demands and resourcing adversity. ",
       "article_title":" Rapid Bricolage Implementing Digital Humanities ",
       "authors":[
          {
             "given":"William Dudley",
             "family":"Pascoe",
             "affiliation":[
                {
                   "original_name":"University of Newcastle, Australia, Australia",
                   "normalized_name":"University of Newcastle Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00eae9z71",
                      "GRID":"grid.266842.c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "project design",
          "cultural and/or institutional infrastructure",
          "organization",
          "digital humanities history",
          "management",
          "indigenous studies",
          "theory",
          "English",
          "epistemology",
          "interdisciplinary & community collaboration",
          "literary studies",
          "criticism",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The role of women in industrial development is now largely recognized in sociological and economic studies on developing countries during the first industrial revolution in Europe. Yet data on their remuneration, schedules and domestic work, and that of men working in the same sectors, remains deficient for many regions, especially for France. The Time-Us project aims to reconstruct the remuneration and time budgets of women and men working in the textile trades in four French industrial regions (Lille, Paris, Lyon, Marseille) in a long-term perspective, by bringing together a multidisciplinary team of historians, natural language processing (NLP) experts and sociologists. It will create comparable series on the remuneration and time allocation of employed men and women (i) through classical sources and company and trade association archives, and (ii) by piecing together a series of qualitative sources identifying words and actions associated with work in both domestic and non-domestic activities. The project will provide keys to understanding the gender gap by analyzing changes in work and time uses during the first industrialization process. The Time-Us team works on a heterogenous corpus of French handwritten and printed sources spanning from the seventeenth to the twentieth century. These documents are mainly preserved in French local archives, from the four industrial regions that have been mentioned above (for instance, Archives municipales de Lyon, Bibliothèque municipale de Lyon, or Bibliothèque nationale de France in Paris, etc.). The analyzed corpus brings together numerous historical sources, and includes court decisions, petitions, police reports and files, and sociological surveys on living conditions of the working class (especially  Les monographies de famille de l’École de Le Play or Le Play’s families’ budgets (Hincker, 2011)). Many of these documents are manuscripts, written by various hands over long periods of time (more than a hundred years for the “Registre de contraventions aux règlements des métiers\" that begins in 1670 and ends in 1781 (Lyon, Archives municipales).   This unpublished set of documents constitutes an important corpus of historical sources that is well-suited for applying computational analysis. In this paper, we will present the approach adopted by the Time-Us team to analyze this corpus. We will also discuss the prospects opened up by this project for historical research in terms of digital research workflow. Our goal consists in applying NLP methods to heterogeneous historical documents, in order to identify and analyze the relevant semantic or syntactic patterns that describe work, remuneration and time budgets. The application of such methods, mainly parsing, will facilitate the analysis of the corpus by creating series of comparable quantitative and qualitative data: Quantitative data on remunerations, household budgets and time spent for domestic (or unpaid) and non-domestic (or paid) work by women and men. Qualitative data on paid and unpaid tasks realized by women at home and at work, namely information on the type of the task, its description, its duration and its results. Computational methods will also be used to extract statements describing the women performing these tasks (occupation, social status, age, marital status, family composition), and the relationships between the actors involved in these tasks, especially between men and women (family relationships such as husband and wife, brothers and sisters mothers and sons, or working relationships such as employers and employees). The analyzed sources can take a number of varied forms. Thus, we chose to work closely with economic and labour historians in the data modelling process. As the corpus gathers together diverse historical sources, the definition of a light and flexible annotation schema, bringing together the history and language processing experts, is a major step to create “gold data” to train parsing models. This gold data take the form of annotated texts encoded in TEI ( Text Encoding Initiative). TEI can be seen as a bridge representation for historians and NLP experts: in this approach, historians annotate a first set of documents in TEI, in order to create training data that can be easily processed and analyzed by NLP experts. Besides, the choice of the TEI allows for the creation of sustainable data, that can be reused in the long term by other projects and researchers. Our aim consists also in creating a flexible TEI data model that will be relevant to modelize differents types of data, and that will enable NLP experts to extract comparable information such as quantitative data (amounts of money, period of time…). In this way, this model could be reused by other research projects especially, but not only, projects of economic and labor history.   A first step is the transcription of the manuscripts into a simple TEI representation, covering the text and a set of metadata. This task is nothing but trivial, due to the diversity of sources mentioned above, but it is not the scope of this paper. Then, the representation is enriched by annotation layers. The first annotation layer is the recognition of tasks and occupations, linked to their associated amounts of money, and the actors of the transaction. The extraction of Named Entities such as person and place names is also necessary in order to properly analyse how gender and localization influence remuneration. The annotation process will start as a collaborative effort, in order to get a first dataset that could possibly be used to train/configure NLP tools, but also to help designing a precise annotation guide between the NLP people and historians. At a later stage, we will progressively deploy more automatic NLP tools to create these annotations. In this regard, we plan to identify the elements of vocabulary (tasks, products) and the interesting phrases (e.g. “ someone was paid (this amount) for (this product) for a (given amount of time)”), using knowledge acquisition techniques based on the distributional hypothesis and syntactic analysis of the corpus. The knowledge of the domain will allow us to define syntactic extraction pattern to be applied on the corpus to detect and annotate specific instances of tasks, products, money, people and relationships between these pieces of information. Some human validation will still be needed to filter the vocabulary, refine the patterns, and propose missing elements (vocabulary and patterns). Language processing will be conducted with the French processing chain developed by the INRIA Almanach team, and in particular with the FRMG parser (Morardo and de La Clergerie 2014). Parsing produces dependencies between words, allowing us to identify who does what, when, how for some event. The processing chain has already by used several times for knowledge acquisition over specific domains (legal, medical). In our case, specific issues may arise because of the quality of the transcriptions and the peculiarities of the language used, which contains archaic constructions, whereas our parser was designed for contemporary French.     Example of a parse for one sentence of the corpus The annotation task is therefore mainly collaborative, so the need for a shared framework has emerged. Several digital projects have already taken into account the specific needs of historians in terms of image visualization, transcription and collaboration. For instance, the  Transkribus interface enables Humanities scholars to transcribe handwritten and printed historical sources, and offers a very powerful Handwritten Text Recognition engine. The project  Transcribe Bentham takes account the collaborative dimension in transcribing historical documents. The  Old Bailey transcription project uses a combination of hand encoding an automatic recognition and extraction systems. Nevertheless, they do not address all the requirements of Humanities scholars working on primary sources, and the need of comprehensive Digital Humanities-based publishing systems is emerging. We have chosen to setup a specific digital workflow enabling historians and NLP experts to work together. We will present the solution that has been put in place, and especially a customized wiki with:  the Transcribe Bentham transcription desk, adapted to our needs, and a TEI toolbar, specifically customized for tagging named entities and measures.    Customization of the TEI toolbar ",
       "article_title":"The Time-Us project. Creating gold data to understand the gender gap in the French textile trades (17th–20th century)",
       "authors":[
          {
             "given":"Eric",
             "family":"de La Clergerie",
             "affiliation":[
                {
                   "original_name":"ALMAnaCH, Inria Paris",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Manuela",
             "family":"Martini",
             "affiliation":[
                {
                   "original_name":"LARHRA, Université Lyon 2",
                   "normalized_name":"Claude Bernard University Lyon 1",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/029brtt94",
                      "GRID":"grid.7849.2"
                   }
                }
             ]
          },
          {
             "given":"Marie",
             "family":"Puren",
             "affiliation":[
                {
                   "original_name":"ALMAnaCH, Inria Paris",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Charles",
             "family":"Riondet",
             "affiliation":[
                {
                   "original_name":"ALMAnaCH, Inria Paris",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Alix",
             "family":"Chagué",
             "affiliation":[
                {
                   "original_name":"ALMAnaCH, Inria Paris",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "corpora and corpus activities",
          "french studies",
          "historical studies",
          "English",
          "natural language processing",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In this paper, we present the collaboration between the pilot project  Kalīla wa-Dimna – Wisdom encoded  E-Learning/E-Research project, located at and funded by Freie Universität Berlin, homepage       and LERA. Information and a demonstrator can be found at   https://lera.uzi.uni-halle.de   In the project’s first phase, devoted to experimenting with existing tools and identifying necessary adjustments, we adopted and generalized LERA for the classical Arabic language. This modification worked well and thus will become a cornerstone for future research within the ERC-Advanced Grant Project “AnonymClassic” (Gruendler 2017). No. 742635, “The Arabic Anonymous in a World Classic,” PI Beatrice Gruendler, Freie Universität Berlin, see      The benefit is already apparent, yielding first observations of the text’s development, and the tool was successfully applied in an undergraduate academic course at the Seminar for Semitic and Arabic Studies of FU Berlin.     Kalīla wa-Dimna  – Wisdom encoded  Using Sanscrit sources,  Kalīla wa-Dimna was composed in Middle Persian (version lost) in the 6 th century CE and ultimately translated into in forty languages worldwide. Its key phase is the Arabic translation from the 8 th century, from which all later translations derive. But this version has proliferated in many variants, which prevents a conventional critical edition by stemma (Gruendler 2013). This project seeks to assess via a (partial) synoptic critical edition the range of variation between selected Arabic manuscripts of this work. These derive from the 13 th to the 19 th century CE.     LERA  –  Locate, Explore, Retrace and Apprehend complex text variants  LERA is an interactive, digital tool for analyzing variations between multiple versions of a text in a synoptic mannerwith several differences to other well known collation tools (Schütz and Pöckelmann 2016). It was first developed for printed texts of the French Enlightenment (Bremer et al. 2015) within the SaDA-project See the project’s homepage:   https://sada.uzi.uni-halle.de   at Martin Luther University Halle-Wittenberg and since then adopted to other texts and languages.  The tool follows three major steps for generating the synopsis. The first is a segmentation of the given manuscripts. In the case of  Kalīla wa-Dimna, the text-units are narrative steps. Second, an automatic alignment of these segments is doneby the built-in algorithm. The researcher can intervene afterwards by moving, cutting or merging the segments if necessary. The final step is the detailed comparison of the aligned segments’ words by the system. The identified variants are highlighted with colors depending on the kind of difference. Besides, a variety of filters are available, e.g., hiding all changes that purely concern punctuation. On this basis, a comprehensive and easily readable apparatus is generated. The proto-edition thus produced can be downloaded in several formats.  Moreover, LERA provides further assistance for the analysis of the variants. Most prominent is CATview (Pöckelmann et al. 2015), a graphical representation of the alignment that facilitates overviewing and navigating within the synopsis. Further information on CATview is also available at  https://catview.uzi.uni-halle.de  It is also associated with the word clouds and search functions of LERA.     Modification of LERA for  Kalīla wa-Dimna  In this project, LERA made its debut in classical Arabic, which has required some language-specific adoptions. Processing the Arabic alphabet was rather uncomplicated, because the system already uses Unicode. Regarding the backend, the processes for tokenizing, indexing (for search) and language recognition were extended. On the other hand, modifications for the frontend included adding a font for the alphabet, displaying the correct writing direction, and revising the download function. More important, some specific needs for the  Kalīla wa-Dimna project have already been implemented. LERA now allows the manual alignment by experts using unique segment identifiers, which are encoded within the manuscripts’ XML/TEI files. On this basis, we also added identifiers for the segments that can be edited and displayed in the synoptic view. On major improvement is the visualization of transposed segments. They occur if the order of the segments within one manuscript was changed or when similar segments appear somewhat distant to each other, but aligning them is blocked due to other aligned segments. We included an option to display copies of them in the synopsis that will be shown grayed and are linked to their actual position. They will also appear in CATview.  In respect to the project’s goal to investigate the interrelation of the manuscripts of  Kalīla wa-Dimna, we developed two new modes for coloring the variants. The first one only highlights passages that are unique to one manuscript, which points to some independence of the copyist-redactor regarding the other manuscripts. The second mode only highlights those passages that appear in exactly two manuscripts. Finding such pairs is important evidence that suggests that both manuscripts are related to each other.  Another helpful extension is the so called 80%-filter, which leads to treating words as identical if they share at least 80% of their letters. This approximating similarity measure is grounded in the property of the Arabic language that words with identical roots tend belong to one semantic field.   Benefits LERA could be adjusted for the first phase of this interdisciplinary collaboration. Based on this, we already made interesting observations: against our assumption, the first analysis shows that there are no distinct groups of manuscripts. Instead, variations fluctuate, forming continua in which some manuscripts cumulatively assemble reformulations that appear scattered among others. Furthermore, in the summer semester of 2017 the project was integrated into an  undergraduate  academic course on  Kalīla wa-Dimna at Freie Universität Berlin. The students used the synopsis to explore the variants of five aligned manuscripts in class and wrote papers applying this method individually.    Conclusion With the work presented here, we established a foundation for a comprehensive analysis of  Kalīla wa-Dimna. Owed to the text’s complex history and manifold variants, this ambitious project is planned for a timespan of ten years. With the ongoing research, more features of analysis will be needed. This includes an advanced utilization of language specific information for the comparison, e.g., a root extraction for Arabic words. Moreover, the comparison and visualization of manuscripts of  Kalīla wa-Dimna in other language is being considered. Finally, the functionality to comment on the identified variants is crucial for their scientific investigation.   ",
       "article_title":" Adjusting LERA For The Comparison Of Arabic Manuscripts Of _Kalīla wa-Dimna_  ",
       "authors":[
          {
             "given":"Beatrice",
             "family":"Gründler",
             "affiliation":[
                {
                   "original_name":"Freie Universität Berlin",
                   "normalized_name":"Freie Universität Berlin",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/046ak2485",
                      "GRID":"grid.14095.39"
                   }
                }
             ]
          },
          {
             "given":"Marcus",
             "family":"Pöckelmann",
             "affiliation":[
                {
                   "original_name":"Martin Luther University Halle-Wittenberg",
                   "normalized_name":"Luther University",
                   "country":"South Korea",
                   "identifiers":{
                      "ror":"https://ror.org/015dhy417",
                      "GRID":"grid.444157.7"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-17",
       "keywords":[
          "computer science",
          "and curriculum",
          "scholarly editing",
          "teaching",
          "pedagogy",
          "text analysis",
          "English",
          "interface & user experience design/publishing & delivery systems/user studies/user needs",
          "near eastern studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction This paper discusses the promises and pitfalls of linking historical data on cultural events. Quite a few datasets on historical European music, theatre and film are now publicly available online (Baptist 2017). The ones that contain programming information are, at least to some extent, already event-based. However, they are highly heterogeneous in scale and scope, and they generally do not use the same definitions for, for example, venues, events, or companies. Conceptualizing and embedding cultural events such as concerts or theatrical performances in a linked data framework helps to overcome such issues without forcing an overarching ontology, and it enables researchers to acknowledge the performative and interactive nature of cultural expressions within their (local) societal context (Nijboer and Rasterhoff 2018).  By linking event data internally as well as to external knowledge bases such as DBpedia and Wikidata by means of shared vocabularies, researchers are invited to systematically analyse cultural life cross-sectorally (i.e. theatre, music), internationally (European comparisons and connections), and contextually (in relation to local social, economic, political and cultural features) (cf.  EPAD: European Performing Arts Dataverse). In this paper we discuss the conceptual and practical requirements for such a linked-data approach on the basis of a series of research projects on historical cinema, musical, and theatrical events in the research program  Creative Amsterdam: An E-Humanities Perspective (CREATE).  Cultural events Events play a key role in historical scholarship, and have gained even more urgency with the increasing importance of digital resources in humanities research. Many projects on historical events, however, employ them as devices to structure data collections and do not explicitly aim to develop analytical frameworks in relation to event data collection and data modeling (De Boer et al. 2015; Van Hage et al. 2011; Shaw 2013. An exception can be found in a statistical method known as event history analysis, which treats events as dependent variables, seeking to statistically describe, explain, or predict their occurrence (Allison 2004). Most research on (urban) arts and culture, however, does not try to statistically identify variables that predict or explain an event, for example the staging of the opera Norma or the screening of the movie Casablanca. Rather, historians may seek to identify (series of) events that have contributed to, for example, the canonical status of specific expressions or genres, to the shaping of local and international cultural taste cultures, or to the emergence of some places as particularly creative and cultural.  We therefore emphasize that (networks and series of) events should also be considered as independent variables that can help us identify and disentangle processes of cultural change and continuity. Central in this view is the assumption that 1) events can be seen as units of analysis with structural properties (notably, a time and place) with links to, for example, actors, institutions, other events, and local properties, and 2) that these interlinkages are key to analysing their role in shaping, for instance, local cultural or social life (Tilly 2002). Turning individual event datasets into linked data versions would provide instantaneous insight into how much performing arts datasets overlaps, ontologically, with any of the others. This provides a roadmap for integrating these still scattered data and studying them in conjunction. A systematic analysis of cultural events therefore requires a data structure which allows for querying connections. Linking cultural event data A first analysis of performing arts datasets demonstrated that normalizing even the most basic data across datasets is tricky and that trying to completely harmonize and link all the relevant datasets is futile (Baptist 2017). Fortunately, the structure of linked data provides a way to transparently query heterogeneous data, without enforcing an overarching ontology. Breaking events down into variables such as ‘people’, ‘venues’, ‘place’, and ‘time’, for instance, circumvents the issue of formally defining a ‘performance’. Linked data also allows researchers to test various different link-ups of two data sets so they can evaluate the results when they adjust their queries. In the case of cinemas, for example, one of the problems is that the typology of cinemas differs across countries and periods. In the Netherlands cinemas are divided into types ‘A’ and ‘B’ according to frequency of screenings; in Flanders the cinemas are classified according to how soon they tend to new films after their premiere. If the data was put into a relational database it would be necessary to ‘reconstruct’ either of the classifications for the other dataset. But linked data, because of its model of loose connections, allows querying both datasets, defining a classification only during the query. For the datasets on cultural events such as historical musical and theatrical performances we build on a rigorous relational data model by Karel Dibbets et al. for the  Cinema Context database (Van Vliet et al. 2009). All movies (often circulating under various titles), persons and companies in in this dataset have been identified and aligned to a master record, and where possible linked to the well known and well maintained Internet Movie Database (IMDb). We develop this approach for other datasets and by linking data on cultural events to other datasets and to other knowledge bases using shared vocabularies such as   schema.org  and  Vocabulary of a Friend (VOAF). In this paper we illustrate research potential, but also practical issues by discussing a recent project on the establishment of movie theatres in the city of Amsterdam in the early twentieth century. By linking data on the history of cinema and movie-going to local contextual data (e.g. census data, municipal election data), we assess how linked data might be used to analyse how specific local historical characteristics shaped form and function of urban cultural life.  ",
       "article_title":"Modeling Linked Cultural Events: Design and Application",
       "authors":[
          {
             "given":"Kaspar",
             "family":"Beelen",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Ivan",
             "family":"Kisjes",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Julia",
             "family":"Noordegraaf",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Harm",
             "family":"Nijboer",
             "affiliation":[
                {
                   "original_name":"Huygens Institute for the History of the Netherlands, Netherlands, The",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Thunnis",
             "family":"van Oort",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Claartje",
             "family":"Rasterhoff",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-18",
       "keywords":[
          "databases & dbms",
          "ontologies",
          "linking and annotation",
          "historical studies",
          "English",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Calha Norte is the northernmost region of the Brazilian Amazon, and constitutes the largest mosaic of protected areas in the world, encompassing nearly 14 million hectares of land. It stretches from the Amazon river in the south to the frontier regions between Brazil and the Guianese and Surinamese borders in the north, encompassing half of Pará and the state of Amapá in its entirety. Due to the vastness of the area, enforcement of protected areas can be poor, and far too little is done to involve local communities in the decision-making processes that inform conservation policy. Increasingly, digital technologies are helping to overcome some of the challenges to conservation.  In the past ten years the Amazonian Institute for Man and the Environment (Imazon), an environmental NGO based in Belém, has made a name for itself by developing a “logging feasibility map” (Souza et al., 2010). The feasibility project uses GIS mapping software to chart data on transportation networks (including existing roads and navigable rivers), topography, biodiversity figures, deforestation areas, conservation zones and timber processing facilities. The combination of this data allows for reliable predictions of at-risk areas for deforestation, which has led to better zoning of different areas for community use, conservation and even tourism. By charting the data in a format easily interpreted by stakeholders, the feasibility maps produced by Imazon have become invaluable tools for raising public awareness and building consensus between enforcement officials, regulatory agencies, loggers, conservation groups, and local communities. Given the immensity of the Calha Norte region and the isolation of many of its inhabitants, communication and monitoring are pressing challenges for authorities charged with the enforcement and protection of conservation zones. Initiatives like Imazon’s feasibility map offer innovative solutions at a relatively low cost. Digital tools such as GIS mapping, data analytics software and tele-communications technologies are helping to bridge gaps in the knowledge and understanding of the ecological, political, and social realities of the region, thereby improving predictors of at-risk zones and response times to threats. This short paper presentation will discuss some of the ways in which digital technologies are helping environmentalists overcome the challenges of conservation policy in the Amazon region of Brazil, especially in Calha Norte. During the presentation, I will present my own personal project, the Calha Norte Portal, a website which features an interactive data visualization map of the protected areas in Calha Norte mosaic as well as a blog on conservation issues and the history of the region. In doing so, I hope to demonstrate the potential of the tool to contribute to ongoing research on conservation policy in the Amazon.       Ex. 1 & 2: Screenshots from CalhaNortePortal.com. Top: The online version of the map application, located on the “Discover” page. Bottom: The CalhaNortePortal.com homepage  The data used for Calha Norte Portal was gathered during my work with the Social Policy department at Imazon. In accordance with the department’s focus on communities in the Calha Norte region, I compiled data from various sources about the region’s history, cultural diversity, transportation networks, governing bodies, development indices, demographics, economic activities, protected area implementation, and accessibility. This data was then plotted in the creation of the Calha Norte Portal.     Ex. 3: Calha Norte in the Google Earth map. This application allows the user to navigate through the region by clicking on protected areas, indigenous territories, maroon communities and municipal capitals to access historical information, demographic statistics, economic and political data, photos, deforestation figures and an implementation index for protected areas. Users can also look back in time at satellite images from 1960 to the present and visualize patterns of deforestation, and urban sprawl over time.    Ex. 4: Example of the pop-up window for the municipality of Santana, Amapá. The Calha Norte map focuses mostly on political, economic, historical, cultural and social data for populations in protected areas and municipalities. As an anthropologist, I am particularly interested in dispelling the myth of Amazonia as an uninhabited biological entity. Advocating for conservation policy which involves the participation of local communities has been the focal point of recent critical reports from the International Union for the Conservation of Nature (IUCN) (Cisneros and Orellana, 2017; Orellana, 2017). These reports reflect a general shift in attitudes on the management of protected areas away from traditional approaches which sought to isolate designated conservation zones and often neglected the history of interactions between local populations and the land in question.  My motivation in developing the Calha Norte map was to bridge some of the divides between disciplines in conservation studies. Inspired by Susanna Hecht’s political ecology approach to the study of the Amazon (Hecht and Cockburn, 2010; Hecht, 2013), I wanted to create an interdisciplinary platform which would incorporate elements from the fields of anthropology, political science, history, sociology and geography. By breaking down the data I collected on the Calha Norte region in a visual, interactive format I hoped to facilitate an engagement with the region’s political, social and cultural characteristics, and emphasize the importance of tailoring environmental policy to the realities of the region’s inhabitants. The stand-alone map in Google Earth is meant to be played with, manipulated and explored, in ways that dismantle the linear narrative format of most textual information on the area and incorporate elements of critical cartography studies. Crampton and Krygier state that critical cartography demonstrates its political nature by “linking geographic knowledge with power” (2005: 1), suggesting that a democratization of mapping tools through digital technologies can also result in new avenues for democratizing political power. My project on the Calha Norte region aims to engage with this idea by reformulating knowledge of the region with a focus on mapping its social, cultural and historical characteristics. By advancing a more holistic vision of human interactions with the environment, I hope to make an argument for conservation policy which advocates for greater local-level management of natural resources by the inhabitants of designated protected areas.  Beyond this engagement with the critical cartography literature, my short paper presentation will also raise questions on open source access to information and digital technologies related to environmental issues in the Amazon. Particularly, my paper focuses its commentary on the complexities of conservation in the region, and the importance of greater transparency in the creation and management of protected areas, indigenous territories and traditional community lands. In relation to the over-arching theme of the conference, I hope that my presentation will demonstrate the immense potential for digital technologies to bridge divides of communication and understanding between institutional bodies, environmentalists, policymakers and the communities they serve, as well as bridging gaps in knowledge for scholars and researchers of the Amazon region. ",
       "article_title":"Bridging Divides for Conservation in the Amazon: Digital Technologies & The Calha Norte Portal",
       "authors":[
          {
             "given":"Hannah Mabel",
             "family":"Reardon",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "repositories",
          "eco-criticism",
          "sustainability and preservation",
          "modeling",
          "cultural studies",
          "anthropology",
          "English",
          "archives",
          "visualization",
          "interdisciplinary & community collaboration",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  State of the art and experimental design  Robert Musil, one of the most important authors of twentieth-century German-written literature, fought in the Austrian army at the Italian front. During WWI, between 1916 and 1917, Musil was chief editor of the  Tiroler Soldaten-Zeitung (TSZ) in Bozen. This activity has posed a philological problem to Musil scholars, who have not been able to attribute with certainty a range of texts to the author. However, solving the riddle of authorship for this particular set of texts promises a great advancement in the study of Musil’s political thinking. With this paper, we present a new approach that combines historical and philological research with stylometric methods.  The determination of possible authorship starts with reviewing the literature for previous attempts. There are 38 articles in the TSZ for which Musil’s authorship has been proposed at least once (see Table 1).   Text # Title Date of publication Attributed by   Excl_1 Der Weg zu den Sternen 08.07.1916 C, FL   Excl_2 Aus der Geschichte eines Regiments 26.07.1916 C, FL   1 Kameraden arbeitet mit! 06.08.1916 A, FL   2 Bin ich ein Österreicher?  20.08.1916 A, FL   3 Herr Tüchtig und Herr Wichtig 27.08.1916 C, FL   4 Das Schlagwort 27.08.1916 A, FL   5 Die Erziehung zum Staat 03.09.1916 A, FL   6 Bauernleben 01.10.1916 C   Excl_3 Kunst hinter der Front 08.10.1916 C   7 Sonderbare Patrioten 15.10.1916 A, FL   8 Noch einmal Bauernleben 29.10.1916 C   9 Opportunität 12.11.1916 FL   Excl_4 Kannst Du deutsch [III] 12.11.1916 A, FL   10 Eine gute persönliche Beziehung 26.11.1916 A, FL   11 Eine österreichische Kultur 10.12.1916 R, A, FL   12 Der Nörgler und der neue Österreicher 17.12.1916 A, FL   13 Das Kompromiß 24.12.1916 A, FL   Excl_5 Der Augenzeuge 24.12.1916 C   14 Heilige Zeit 31.12.1916 A, FL   15 Zentralismus und Föderalismus 07.01.1917 FL   16 Föderalismus oder Zentralismus 14.01.1917 FL   Excl_6 Kannst Du Deutsch [V] 21.01.1917 A, FL   Excl_7 Vorpolitische Reinigung 04.02.1917 A, FL   Excl_8 Kannst Du Deutsch [VI] 04.02.1917 A, FL   17 Zu Milde und zu Wilde 11.02.1917 A, FL   Excl_9 Aus einer öffentlichen Schwulstfabrik 18.02.1917 A, FL   Excl_10 Schnucki in der „großen Zeit“ 18.02.1917 A, FL   18 Neu-Altösterreichisches 25.02.1917 A, FL   19 Ist die »österreichische Frage« schwierig? 04.03.1917 FL   20 Seiner Hochwohlgeboren! 04.03.1917 D, A, FL   21 Luxussteuern 04.03.1917 A, FL   22 Positive Ziele 11.03.1917 FL   23 Der Frieden versprochen! 18.03.1917 FL   24 Das Staatsprogramm der Deutschen 18.03.1917 A, FL   25 Wehe dem Staatsmann! 25.03.1917 FL   26 Der Frieden und die Zukunft 01.04.1917 FL   27 Presse und Krieg 08.04.1917 FL   28 Vermächtnis 15.04.1917 D, R, C, A, FL   Table 1. TSZ articles attributed to Musil, derived from (Schaunig, 2014). D = (Dinklage, 1960); R = (Roth, 1972); C = (Corino, 1973, 2003, and 2010); A = (Arntzen, 1980); FL = (Fontanari and Libardi, 1987). The major problem for carrying out a stylometric analysis on the texts published in the TSZ is their shortness. As demonstrated by (Eder, 2015), the minimum length for a reliable authorship attribution is around 5,000 words. However, the average length of the 38 disputed TSZ articles is slightly below 1,000 words (see Figure 1).    Figure 1. TSZ articles’ lengths As a possible solution for this issue, we developed a combinatory design that analyzes longer chunks composed by the juxtaposition of single texts. To reduce the number of combinations, we excluded the 9 shortest texts (below 500 words), together with the only text (Excl_2 in Table 1) that has been solidly attributed to Musil on philological grounds (Corino, 1973). This leaves us with a corpus of 28 texts, already digitized by (Amann et al., 2009). The optimal configuration was obtained by combining groups of 6 texts. This permutation generated 376,740 text chunks with an average length of N=6,963 words and a standard deviation of 909 words. As for the composition of the training set, we  combined the stylometric “impostors method” (Koppel and Winter, 2014) with historiographical research. Following (Juola, 2015), we thus fixed the number of “impostors” to a minimum of three, identifying as likely candidates Franz Blei, Franz Kafka, and Stefan Zweig. In addition, we selected three possible TSZ collaborators: Marie delle Grazie, Hugo Salus, and Albert Ritter (cf. Urbaner, 2001). While all others were digitally available, we manually retrodigitized Ritter’s texts. The training set was completed by a selection of articles authored by Musil in various journals between 1911 and 1919. For each author, the retrieved material was subdivided in three text chunks (length ranging 6,000–8,000 words): the training set was thus composed of 21 text chunks.    The Experiment  Validation and experimentation were carried out using the R package  Stylo (see Eder et al., 2016). A 20-fold stratified validation had the following results: (1) distance measures (with the exception of Cosine) work slightly better than machine learning algorithms; (2) word-based analysis outperforms 10-character n-grams  (cf. Halvani et al., 2016: 39) ; (3) Fig. 2 shows that accuracy levels fluctuate substantially between 10 and 400 MFWs.      Mean accuracy (with 10-char. n-grams)   Delta 99.16 98.96   Eder 98.58 98.57   Canberra 99.37 99.24   Cosine 95.03 95.40   Cosine Delta 98.90 98.79   SVM 98.56 98.46   k-NN 95.28 94.95   NSC 95.55 95.34    Table 2. Stratified validation results      Figure 2. Stratified validation results   For these reasons, we limited feature selection to altogether 16 combinations of: (1) the distance measures Burrows’s Delta, Eder’s Delta, Cosine Delta, and Canberra; (2) the frequency strata 10–100 MFWs, 20–200 MFWs, 50–500 MFWs, and 100–1,000 MFWs.  For each iteration, the distances between test set and training set were saved into a matrix. At the end of the process, mean values were calculated. In all 16 configurations, Ritter and Musil are the only candidates for authorship of the TSZ articles. This evidence has been corroborated by the discovery of a document in the  Kriegsarchiv in Wien, which confirms that Ritter was part of the TSZ editorial team (see Fig. 3).     Figure 3. Ritter in the TSZ team. Source: Kriegsarchiv, Wien The stylometric results are synthetized by Fig. 4, which represents only the distances between Musil’s and Ritter’s signals. For highlighting the distinctions, measures were normalized to a range between –1 and +1. A general trend is evident: while, for the articles published in 1916 (articles 1–14), figures point quite clearly to Musil’s authorship, the picture is less clear for the articles published in 1917 (articles 15–28). In no case, however, Ritter’s signal is clearly dominant. Musil thus appears as the most likely author, with the following caveats: First, the combinatory design, while having shown the dominance of Musil’s signal, may have suppressed different, minor signals. Second, Musil, in the role of chief editor, may have altered many articles in the journal, thus intermixing his authorial signal with those of others. By consequence, results that question Musil’s authorship are as a tendency more substantial than those that corroborate it.    Figure 4. Experiment results (full test set) In a second experiment, we split the corpus in two, applying the same experimental set-up. The first sample just contained those texts that were  not attributed to Musil by at least two distance measures (N=14). Here, Ritter appeared as dominant throughout the whole selection. The second sample contained the texts that had  been relatively clearly attributed to Musil in the first round (N=14): results show that here, Musil’s signal was even more dominant, with all values close to +1 (see Fig. 5).      Figure 5. Experiment results (split test set) When further reducing the selection to 9 texts (those for which all classifiers scored less than –0.5 in the previous round, see Fig. 5), all texts were attributed to Ritter with a stronger probability, while the graph generated by the remaining 19 texts was still confined to Musil’s region (see Fig. 6). In answer to our research question, results suggest that Musil attribution may be disproved with a high level of confidence for texts No. 15, 16, 18, 19, 22, 23, 25, 26, and 27 (see Table 1 for details). At the same time, our analysis proposes that Ritter may be the author of these 9 articles.    Figure 6. Experiment results (split test set)   Future research Future expansion of our research should define new training sets to validate the results and increase the test set. Both, however, will require an extensive digitization effort: most of the useful texts (i.e., propagandistic WWI writings) are not available in a clean plain-text format. In addition, other software should be tested on the already defined corpus, e.g., JGAAP (Juola et al., 2008) and CLEF/PAN (Stamatatos et al., 2015), as these consider features and methodologies excluded from the present experiment, such as character n-grams and machine learning.  With our study, we hope to have laid the groundwork for a research that can have long-lasting consequences on the historiography of German literature, evidencing at the same time how quantitative methods are not in opposition, but complementary to the qualitative strands (Herrmann, 2017) of literary history.  ",
       "article_title":"Whose Signal Is It Anyway? A Case Study on Musil for Short Texts in Authorship Attribution",
       "authors":[
          {
             "given":"Simone",
             "family":"Rebora",
             "affiliation":[
                {
                   "original_name":"University of Verona, Italy",
                   "normalized_name":"University of Verona",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/039bp8j42",
                      "GRID":"grid.5611.3"
                   }
                }
             ]
          },
          {
             "given":"J. Berenike",
             "family":"Herrmann",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Gerhard",
             "family":"Lauer",
             "affiliation":[
                {
                   "original_name":"University of Basel, Switzerland",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Massimo",
             "family":"Salgaro",
             "affiliation":[
                {
                   "original_name":"University of Verona, Italy",
                   "normalized_name":"University of Verona",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/039bp8j42",
                      "GRID":"grid.5611.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "german studies",
          "stylistics and stylometry",
          "historical studies",
          "authorship attribution / authority",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Introduction “Anger is loaded with information and energy,” says Audre Lorde in a 1981 speech on its political uses—but the nature of this affective information, sparked by a given political present, becomes highly vexed when articulated through literary objects (Lorde, 1997: 280). On the one hand, the cool detachment of aesthetic mediation keeps the politics of experimental works from being seen as mere propaganda, but runs the risk of appearing elitist or self-indulgent. On the other hand, the red-hot political outrage of a protest poem by Amiri Baraka or Sonia Sanchez grounds itself in the present, but may be attacked for subordinating aesthetic sophistication to political agendas. Building on recent scholarship (like the work of Lauren Berlant and Sianne Ngai) suggesting that feeling gives structure to cultural formations, my research investigates the provocation and articulation of emotions like frustration, anger, and discontentment within recent US literary history as they relate to systemic injustice. An agitprop play that ends with shouts for workers to unite in class revolution; a poetic broadside that vents frustrations against white supremacy in America; a novel that indulges in a revenge fantasy against America’s colonial history. Unlike plays, poems, or novels that seem to obscure, submerge, or confound their own political dimensions, these works wear their hearts on their sleeves: they are frustrated, fed up with how things are, and unafraid to speak truth to power in a direct, seemingly “un-literary” way. “Measured Unrest in the Poetry of the Black Arts Movement” offers a proof-of-concept for performing sentiment analysis on some of the most politically and affectively charged poetry of the 20 th century in America, that of the Black Arts Movement of the 1960s and 1970s. The BAM first took shape at the height of the Black Power Movement with the foundation of the Revolutionary Theatre by Amiri Baraka in 1965. As Larry Neal—one of BAM’s principal theorists—says in a 1969 manifesto, the “Black Arts movement seeks to link, in a highly conscious manner, art and politics” toward “the liberation of Black people” (Neal, 1969: 54). Moreover, what Neal calls the movement’s “black esthetic” is famous for its affective dimensions, often exploring the limits and political uses of anger, frustration, and militant poetic rage. But while BAM writers sought to link art and politics through explicitly racial terms, many—though by no means all—were marked by a failure to attend to the intersections of gender with racial injustice.  In this project I ask two questions in particular: first, how are the feelings associated with injustice in this corpus coded in terms of race and gender? And second, what can natural language processing techniques like sentiment analysis show us about the relations between different dimensions of poetry—like affect and gender—given that poetry is highly figurative and notoriously difficult to quantify in terms of sentiment or opinion? Method In addressing both these questions, this project uses a small corpus of poetry—currently 26 books—from prominent BAM authors. I employ both close reading as well as machine reading techniques, combining the powerful scale of sentiment analysis with the granularity of traditional literary analysis in an effort to explore the intersections of feeling, gender, race, and injustice in the radical poetry of this period. My goal in this project is not to develop a sentiment classifier that works on experimental poetry in English. Rather, it is to see what existing classifiers can show us about a specific corpus of poetry.  In this sense, I use pre-existing sentiment classifiers like VADER and Pattern (via TextBlob) to perform a kind of exploratory computational analysis on my corpus (Hutto and Gilbert, 2014; De Smedt, and Daelemans, 2012). Rather than use these tools to make general claims about this incredibly diverse body of poetry, I test, experiment, and make targeted use of sentiment analysis techniques to pursue research questions already present in existing scholarly conversations—for example, how poets might tie heightened affects to an explicitly political quest for racial justice in America. The insights I draw from my computational analyses, then, go hand in hand with more traditional literary practices. Moreover, my methodology aims to acknowledge the fact this poetry was written in the shadow of government surveillance programs, active FBI counterintelligence operations, and a larger culture fearful of radical thought. Because of this, my project explores the fraught methodological implications of using distanced, potentially decontextualizing computational text analysis techniques to think through BAM poetry, and how these methods might best be used to pursue questions, problems, and lines of inquiry centered around black thought and experience. The already vibrant conversations on sentiment analysis and natural language processing more generally have been illuminating in forming these thoughts and questions. The discussion between Matthew Jockers and Annie Swafford on the  Syuzhet package and “archetypal plot shapes” has helped me not only to consider the current possibilities and limitations of sentiment analysis as applied to literary corpora, but also to think through the kinds of results we expect from digital projects and how we verify those results as an academic community (Swafford, 2015; Jockers, 2015). With regards to poetry and NLP more specifically, Lisa Rhody’s topic modeling of highly figurative ekphrastic poetry is a great model for how unexpected failures in textual analysis can also be productive, prompting us towards new questions as well as new understandings of familiar methods like close reading (Rhody, 2012).  Results I have implemented NLP techniques with NLTK and TextBlob, a text-processing Python library, on my collection of 26 books of poetry. I have also used two sentiment classifiers—Pattern (via TextBlob) and VADER—to evaluate my corpus for sentiment and interpret my results. While this work is ongoing, so far my work comprises explorations and experiments in the smaller-scale uses of sentiment analysis in the study of poetry and affect.  For example, Pattern considers Quincy Troupe’s “Come Sing a Song”—from his 1972 collection  Embryo Poems, 1967-1971—to be the most negative poem in my entire corpus. In a corpus of poetry containing direct attacks, extreme invective, and explicit takedowns of individuals, groups, and institutions, I did not find this poem to contain an exceptional amount of negative sentiment. On the contrary, I found “Come Sing a Song” to be positive and celebratory with regards to black life and black artistic expression. VADER, meanwhile, considers Nikki Giovanni’s “The True Import of the Present Dialogue, Black vs. Negro”—from her 1968  Black Feeling, Black Talk—to have the most negative sentiment in the corpus. These results are very much in keeping with other human readers of this poem: critics consider it to be one of the most significant and famous examples of a certain type of angry, militant, even aggressive poem. Where Pattern and I disagree strongly over the feel of Troupe’s “Come Sing a Song,” critics and VADER seem to agree that Giovanni’s “The True Import” has, on the surface, an exceptional amount of negative sentiment compared with its contemporaries.   Among other things, my project analyzes discrepancies and correspondences such as those described above. Already, my findings have revealed an interpretive disjoint between the denotative affective impact of words—what might be called their surface sentiment—and their more nuanced affective import as shaped by poetic, literary, social, and political contexts. A sentiment classifier like VADER, for example, highlights the intensity of negative sentiment in a poem according to the words and phrases it contains without the literary and historical context of their use. This kind of surface reading, attuned specifically to words’ immediate affective impact, anticipates the space between a surface anger that can spark feelings regardless of context and a poetic form that, in the case of Giovanni’s “The True Import,” leverages negative sentiment to address meaningful social issues in a productive, ultimately positive way. By investigating these poems through conventional literary methods (i.e., historical contextualization, close reading, consideration of relevant scholarship) and computational methods (in this case Pattern and VADER), while also investigating the histories, intended use contexts, and potential biases of the chosen computational methods, this project provides an opportunity to examine what it is, exactly, that provides a book, poem, or poetic line with its emotional charge. ",
       "article_title":"Measured Unrest In The Poetry Of The Black Arts Movement",
       "authors":[
          {
             "given":"Ethan",
             "family":"Reed",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "literary studies",
          "corpora and corpus activities",
          "text analysis",
          "English",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Does “late style” exist? That is, do novelists exhibit a well-defined and distinctive stylistic shift as they reach old age, artistic maturity, or both? Edward Said’s  On Late Style: Music and Literature Against the Grain argues not only that such a style does exist, but that it has well-defined characteristics. Said describes late style as, somewhat paradoxically, involving “a nonharmonious, nonserene tension, and above all, a sort of deliberately unproductive productiveness going  against” (Said 22). The term “late style,” derived from Thodor Adorno’s concept of Beethoven’s  Spästil, is one which Adorno conceives of as “catastrophic” (Adorno 567). As Adorno puts it, “the maturity of the late works of significant artists does not resemble the kind one finds in fruit. They are, for the most part, not round, but furrowed, even ravaged. Devoid of sweetness, bitter and spiny, they do not surrender themselves to mere delectation” (564). To determine whether this claim is more than just anecdotally true, it deserves to be experimentally tested. Using new techniques of computational stylometric analysis, I test whether a writer’s late works are statistically dissimilar to the rest of their corpus. I find that late style is not a statistically quantifiable phenomenon. Instead, the opposite is true: the novelists tested exhibit very distinctive early styles.  Twelve single-author corpora were prepared for this study. These include three novelists Said cites at length: Marcel Proust, Thomas Mann, and Jean Genet, as well as nine novelists from the 19th and 20th centuries, chosen for their prolificacy and electronic availability: Charles Dickens, Joseph Conrad, Ernest Hemingway, Henry James, Walter Scott, George Meredith, Willa Cather, Arnold Bennett, and Mary Augusta Ward. Two samples were taken from each novel in these corpora, so that the internal stylistic similarity of the samples serve as a metric check for the validity of the method. These samples were randomly chosen, to ensure that no text is longer than the shortest text in each corpus, and that that the analysis will compare equal amounts of text. Each of these samples was then vectorized to 500-dimensional vectors, according to their top 500 word frequencies. These samples were then reduced to five dimensions using principal component analysis (PCA). Five dimensions were used here, instead of the usual two, since a cross-validated grid search in a previous study determined this value to be the most effective at clustering documents according to voice and style. This study also introduces two new metrics for stylistic difference. First, the “distinctiveness score” of a novel sample is calculated by determining the distance of the vector from the mean in five-dimensional space, using the Pythogorean theorem. A late novel that shows a high distinctiveness score, therefore, could correctly be called an instance of “late style.” Second, I introduce a metric representing the “periodicity” of the writer’s style. This is calculated by first inferring prior category labels of early, middle, and late using publication years. Then, the novel’s reduced vectors are clustered using a Baysian Gaussian mixture model, which probabilistically infers three or fewer clusters. These assignments are finally compared using a mutual information score, which calculates the similarity of these clusters with the prior inferences, regardless of label. A high periodicity score indicates that a novelist exhibits distinct stylistic periods, whereas a low score indicates that a novelist has a relatively unchanging or unpredictable stylistic progression.      Figure 1: Thomas Mann Figure 1 shows a projection of the first two dimensions of the vectors generated from Thomas Mann novels. The sizes of the points represent their relative publication years: small circles are early works, and large circles are late works. The colors represent the clusters predicted from the Bayesian Gaussian mixture model. The samples with the highest distinctiveness scores are from his first work  Der Kleine Herr Friedemann and his early work  Tristan. The samples showing the least distinctiveness, are from  Doktor Faustus, the very work Said cites as an example of a distinctive late style.       Figure 2: Marcel Proust Figure 2 shows the same projection for samples from the works of Marcel Proust. Proust’s first work,  Du côté du chez Swann, is the most distinctive. Proust’s last published work,  Le temps retrouvé, which Said cites as an example of late style, is in fact very non-distinctive. Proust’s middle works, however,  La prisonnière and  Albertine disparue, are only intermediary with respect to publication dates, since they were the final novels he wrote. Here, Said is somewhat correct that Proust has a late style, but misidentifies the works that exemplify it. Again, however, Proust’s early style shows a stronger signal than his late.       Figure 3: Charles Dickens Figure 3 shows vectors generated from Charles Dickens novels. Here again, the early work  The Pickwick Papers has the highest distinctiveness score, followed by  David Copperfield. Late works like  Our Mutual Friend are among the least distinctive. As the alignment of the point colors and sizes here suggests, Dickens shows a strong periodicity. At 0.469, his is the second-highest periodicity score.    Author Periodicity Score   Proust 0.023   Meredith 0.028   Ward 0.166   Cather 0.177   Conrad 0.177   Bennett 0.220   Hemingway 0.326   Scott 0.360   Mann 0.367   Genet 0.457   Dickens 0.469   James 0.472   Table 1 Table 1 shows the periodicity scores of all the novelists studied here. Those novelists with well-known early and late styles, such as James and Dickens, have high periodicity scores. Writers like Proust, on the other hand, whose novels all form part of the series  À la recherche du temps perdu, and were all published within about a decade, show the lowest periodicity scores.  This study, beyond simply testing and ultimately disproving the claims of Said and Adorno, provides a framework for stylometric analysis of textual difference, one which could be used to enhance authorship detection techniques and the techniques of forensic text analysis more generally. More experiments are needed, of course, to test the validity of these techniques beyond the domain of literature. ",
       "article_title":" Does \"Late Style\" Exist? New Stylometric Approaches to Variation in Single-Author Corpora  ",
       "authors":[
          {
             "given":"Jonathan Pearce",
             "family":"Reeve",
             "affiliation":[
                {
                   "original_name":"Columbia University, United States of America",
                   "normalized_name":"Columbia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj8s172",
                      "GRID":"grid.21729.3f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "corpora and corpus activities",
          "stylistics and stylometry",
          "english studies",
          "text analysis",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital data preservation is complex and multi-layered. The digital humanities brings unique challenges and opportunities to \"keeping data alive\" that are leading to innovative cross- disciplinary solutions. Data preservation involves standards, guidelines, open-source vs. proprietary software, accessibility, and much more. While establishing best practices, cultivating a community of experts, and developing infrastructure for 3D data used in cultural heritage has been the focus of several coordinated efforts in Europe over the past decade (Campana and Remondino 2014; Fresa and Prandoni 2015; Vecchio et al. 2015), efforts have been less systematic in the United States. Recently, however, digital humanities practitioners have spearheaded 3D data preservation and sharing in the United States. While scholars working with 3D data must deal with management and sustainability issues (Galeazzi 2016; Richards-Rissetto and von Schwerin 2017), endeavors are typically tailored to individual projects. To broaden and coordinate efforts, the Community Standards for 3D Data Preservation (CS3DP) project is bringing together librarians, curators, technical specialists, and scholars to begin the process of developing standards for preservation and sharing of digital 3D data. While long-term archival of these data, for example, in a dark archive, is integral to our research (Koller et al. 2010), the MayaCityBuilder project is contributing to “keeping data alive” by developing workflows to supporting reuse and repurposing of procedurally-generated 3D data in the humanities.  While many types of 3D models are being used in humanities scholarship, the case study focuses on 3D models of ancient Maya architecture generated from multiple data sources including architectural drawings, excavation reports, Geographic Information Systems (GIS), and airborne LiDAR. To contribute to 3D data preservation efforts, while maintaining realistic goals, the MayaCityBuilder Project focuses on procedural modeling—rapid prototyping of 3D models from a set of rules. Procedural modeling is ideally suited for the development of 3D modeling standards that promote data interoperability, dissemination, and reuse because they bring with them the underlying metadata, paradata (information about modeling choices) (Bentkowska-Kafel et al. 2016), and descriptive data (e.g., data sources, textures, building type). Within these circumstances, the two objectives of the “keeping data alive” component of the MayaCityBuilder Project, supported by a Tier I Research and Development Grant from the Division of Preservation and Access of the National Endowment for the Humanities (NEH), are to develop  workflows: (1) to generate, store, and make accessible 3D models of ancient architecture in open-source and proprietary software to foster data (re)use and (2) to host, deliver, and visualize these 3D models, linked to metadata, paradata, and descriptive data, in 3D visualization environments. These objectives are part of a larger goal to contribute to  innovative methods of materials analysis and new modes of discourse using interactive 3D web visualizations. To achieve this goal requires not only data accessibility but also data compatibility—scholars must also be able to combine and recombine data for reuse and repurposing.  Building on previous research and development and lessons learned from the MayaArch3D Project (von Schwerin et al. 2013), Gabii Goes Digital (Opitz et al. 2016), and the Archaeological Data Service (ADS) (Galeazzi et al. 2016), we present technical workflows to dynamically host, deliver, and visualize 3D models that are linked to metadata, paradata, and descriptive data in two 3D environments: (1) an open source 3D web-based environment based on 3DHOP (3D Heritage Online Presenter—an open-source software package for hosting interactive, high-resolution 3D models on the web that uses HTML, JavaScript, and WebGL (Web Graphics Library) (2) Unity—a proprietary and widely-used gaming engine that offers free access to many of its powerful tools. We present an overview of the workflows we have developed explaining how the steps serve our objective of data reuse and more broadly access and preservation of 3D data. Additionally, we discuss how these workflows relate to the next phase of the project, i.e., prototype development. The prototype will take advantage of recent developments in web technology, namely the adoption of WebGL that renders interactive 2D and 3D computer graphics in browsers without plugins. The ability to efficiently generate, store, deliver, and visualize models in an interactive 3D web- based environment will help keep data alive by fostering collaborative and comparative humanities research. We focus on procedural models because they can be quickly generated and are directly linked to metadata and paradata. 3D models allow scholars to test architectural reconstructions and situate them within landscapes to investigate spatial relationships at multiple scales while providing a sense of embodiment (Barcelo et al. 2000; Dylla et al. 2010; Frischer and Dakouri-Hild 2008; Richards-Rissetto and Plessing 2015; Saldana 2015). However, the diversity of 3D data types, tools, and technologies in combination with a lack of standards requires workflows to promote reuse and repurposing of 3D data to contribute to long-term access and preservation of 3D data. ",
       "article_title":"Keeping 3D data alive: Developments in the MayaCityBuilder Project",
       "authors":[
          {
             "given":"Heather",
             "family":"Richards-Rissetto",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Rachel",
             "family":"Optiz",
             "affiliation":[
                {
                   "original_name":"University of Glasgow, Scotland, UK",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Fabrizio",
             "family":"Galeazzi",
             "affiliation":[
                {
                   "original_name":"University of York, England, UK",
                   "normalized_name":"University of York",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04m01e293",
                      "GRID":"grid.5685.e"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "technologies",
          "archaeology",
          "computer science",
          "repositories",
          "geohumanities; spatial & spatio-temporal analysis",
          "information architecture and modeling",
          "sustainability and preservation",
          "databases & dbms",
          "modeling",
          "archives",
          "visualization",
          "English",
          "geography"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  PI and Presenter: Brad Rittenhouse   Others involved: Taha Merghani, Sudeep Agarwal, Vidya Iyer, Madison McRoy, Sidharth Potdar, Nate Knauf, and Kevin Kusuma  In this short paper, I will discuss an ongoing text analysis project, which applies NLP, topic modeling, mapping, and other methodologies to the Wright American Fiction corpus. From a theoretical standpoint, the project is an extension of my qualitative work, which tracks a notable historical shift in literary data management strategies through the works of two canonical American writers: Herman Melville and Walt Whitman. Both wrote in New York as it grew from a small market town of around 60,000 residents to a global metropolis of nearly 1,000,000 and had to imagine strategies of data management to integrate newly urban, consumerist surroundings into their writings in an effective, efficient manner. Translating increasingly crowded material realities—populated by people, products, and print—into literary data, these writers illustrate an important ontological shift from the positivist data strategies of the Enlightenment to digital logics of aggregation, organization, and metonymic indexing that increasingly address the impossible scale of modern infosopheres.  As relatively privileged subjects, however, these writers’ very ability to integrate and innovate with this information was largely based upon a free access to information (and indeed information overload) that many contemporaries did not enjoy. In short, critics have historically apportioned literary status upon hegemonic standards of information, with prestigious genres like “encyclopedic writing” preferring masculinist topics and knowledge bases such as ballistics (Pynchon), cetology (Melville), violence (Bolaño) over spheres of knowledge historically more accessible and immediate to women and people of color.  My quantitative work looks to sidestep these biases, using an assortment of natural language processing techniques to recover works from the archive that may be performing similarly impressive literary acts of aggregation, but which critics may have overlooked because the works exist in alternative thematic and affective registers. By measuring the accretion of material information across the corpus, and identifying areas of relative density, my process points to writing which humans readers have overlooked but which machines are able to see as substantially similar to canonical encyclopedic works.  We intentionally made a very broad measurement of the text to identify a broader range of artistic expression. The process itself involves chunking all the texts into 500-word segments, performing a parts-of-speech tag with OpenNLP, then rendering these tags in “baby binary”: a “0” for all non-nouns, a “1” for all nouns. We then summed the segments and divided by the total length of each to obtain a noun density measurement, which generally indicates an aggregation of material information. Though it is possible to use more specific grammatical measures (subjects, objects, etc.), we used nouns at-large so as to capture a fuller spectrum of thought, sentiment, and other immaterial objects that accompany the human masses of urbanization.  We also assembled a fair amount of demographic metadata for the corpus, which has allowed us retrieve relatively forgotten works from the archive. After identifying the densest chunks of text, we attempted to identify author gender with the use of the machine learning platform SexMachine. We cross-referenced these results with those derived from the noun-density analysis to pinpoint female authors of interest. To conduct this analysis, we first performed exploratory data analysis to understand the underlying distribution of noun ratios across the corpus, which appeared to be normally distributed, although with a slight right skew. Then we compared this distribution with that of the noun ratios identified for authors of each gender. The distributions seem to be largely similar. This naturally led to an outlier analysis within each gender, which identifies outliers as works with a noun ratio 1.5 interquartile ranges either above or below the median, yielding 71 outliers for male authors and 47 outliers for female authors (43 and 26 on the high-end, respectively). We then performed additional analyses on these outliers to get a better understanding of what differentiated them from the rest of the corpus.  One case study I will present from among these outliers is that of Emma Wellmont, a nineteenth-century temperance writer who the academy has largely ignored, I suspect because of the emotional, sensationalist overtones of her chosen genre. Nonetheless, her work is quantitatively similar to Walt Whitman’s, with many extracts in the highest quadrant of noun density across the corpus and packed with what the latter evocatively refers to as “stuff.” Unlike Whitman, however, her densest passages are often emotional, pathetic scenes of death and suffering. Critics, if they read Wellmont’s work (and most do not), would likely label it sensationalistic or melodramatic, and therefore, unserious, writing. My methodology, on the other hand, makes an argument for her as an important encyclopedist, albeit of canonically unlikely subject matter. I will present the case study through a prototype interactive visualization that allows users to explore the corpus at-large, all the way down to significant passages within individual works (Figure 1).   Figure   This curatorial process builds upon the methods described by Long and So in their recent article “Literary Pattern Recognition: Modernism between Close Reading and Machine Learning,” using high-powered computing and statistical analysis on a corpus scale to identify information-dense passages for later close reading and analysis. Reading literature as information, the methodology is flexible in not only illuminating macro-scale trends, but also identifying human-readable works and passages for literary critics who also value critical reading practices. The project also runs in parallel to Dennis Yi Tenen’s recent work in its “articulation of ‘effect spaces’ via material density,” though it pulls from a broader range of quantitative, grammatical measures in its attempt to broaden the generic construct of encyclopedic writing.  ",
       "article_title":"Finding Data in a Literary Corpus: A Curatorial Approach",
       "authors":[
          {
             "given":"Brad",
             "family":"Rittenhouse",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          },
          {
             "given":"Sudeep",
             "family":"Agarwal",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "computer science",
          "natural language processing",
          "library & information science",
          "text analysis",
          "visualisation",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Outline The application of computing methods to scholarly editing was one of the first major areas to be explored in the nascent discipline of “Humanities Computing”: the direct ancestor of what we now know as “Digital Humanities”. The highly-structured nature of scholarly editions, with their formal links between text and carefully crafted apparatus, and the promise that complex patterns in historical texts might be usefully explored by computer methods, made them obvious targets for the early application of computer methods to humanities materials (Hockey; see also early work by Dearing and Petty and Gibson). The first formal attempt at systematic computer representation of texts, the Text Encoding Initiative, analyzed these structures into a formal encoding scheme, itself building on the principles set out in De Rose et al’s landmark 1990 publication, “What is Text, Really”. While the TEI encodings created from 1990 onward proved a solid foundation for many scholarly editions in digital form, from the very beginning scholars recognized a fundamental problem in the TEI encodings when applied to scholarly editions. At its most basic: one wants to see the text of a manuscript on screen as it appears in the manuscript: page by page, line by line. But also one wants to see that text not as it appears in the manuscript, but according to its logical structure as an act of communication: that is, as composed of segments (Acts and scenes; or stanzas and lines; or chapters, paragraphs and sentences). Because these two views almost never correspond, we have what is usually termed the problem of “overlapping hierarchies”: paragraphs cross page boundaries; manuscripts contain multiple works, distributed in complex ways across the pages.  Many papers have addressed this issue of the “overlapping hierarchy” (De Rose; Sperberg-McQueen and Huitfeldt), and this author has wrestled with this issue across multiple editions and operating systems. In 2010, the author commenced work on a new system for collaborative online scholarly editing, “Textual Communities”. A key aim was that this system would seek a robust and fundamental solution to the problem described as “overlapping hierarchies”. Accordingly, the first task was to rethink exactly what we mean by the terms “document”, “work” and “text”. For this, the author went to textual scholarship, which has been considering the meaning of these terms for centuries. In a series of articles (2013a, 2013b, 2017) the author has explored their meaning, with the 2013a article most clearly anchoring his perceptions in the traditions of textual scholarship. In summary, these terms are defined as follows:  A text is an act of communication instanced in a document The act of communication is composed of an ordered hierarchy of objects (Acts and scenes; or stanzas and lines; or chapters, paragraphs and sentences): hence, a tree The document is composed of an ordered hierarchy of objects: the volume, divided into quires, divided into leaves, divided into recto and verso pages, divided into columns, divided into lines (or, surfaces, divided into zones, etc): hence, a tree  In this analysis, every text is composed of two distinct and independent hierarchies: one tree for the document, and one for the act of communication. Both trees are essential. An act of communication cannot exist unless it is physically instantiated in a document. If the document does not present an act of communication, then it is simply marks on paper, without lexical meaning. Textual communities formalized these definitions in an ontology. The naming system used by this ontology is based on the well-known Kahn-Wilensky system (1995), commencing with a naming authority (in this example, TC:CTP) and then using a sequence of property/value pairs to specify each object. In this case, we are describing that part of paragraph 291 of the Parson’s Tale (“PA”) which appears in line 40 of folio 232v of the Hengwrt manuscript of the Canterbury Tales:  The document hierarchy: TC:CTP/Document=Hg:Page=232v:Line=40 The act of communication hierarchy: TC:CTP/Entity=CTP:Part=PA:ab=291 The text, combining both hierarchies: TC:CTP/Entity=CTP:Part=PA:ab=291: Document=Hg:Page=232v:Line=40  In this formulation, every text is composed of a sequence of leaves, with every leaf shared by two distinct trees. Thus the “leaf” of text contained in line 40 of folio 232v occupies TC:CTP/Document=Hg:Page=232v:Line=40; that same text is part of TC:CTP/Entity=CTP:Part=PA:ab=291. The power of the system can be readily appreciated. First, one may travel through the document hierarchy to show the text page by page, line by line. Second, one may travel through the act of communication (“entity” in our system) hierarchy to find the different versions of paragraph 291 in multiple manuscripts and compare them. In this analysis, what we term “overlapping hierarchies” is a symptom, a result of the underlying system of distinct trees sharing leaves. Theory is one thing; implementation is another. We wanted a system that could be updated in real time. (Here, I speak of “we” as the progressing work became more and more a collaborative project). That is: a manuscript page could be transcribed, the order and structure of the text on the page rearranged, deleted, replaced, and the results written near-instantly to a storage system and available immediately to others. Over a long text (20,000 lines of the Canterbury Tales) in many manuscripts (88 for the Tales, some 30,000 pages in total) this is rather challenging. One may compare this with removing leaves from the trees, rebuilding the branches to which they were attached, and then reattaching the leaves, all in a howling gale. A brief attempt to use an XML database (in this case, XML DB, now maintained by Oracle) revealed substantial performance problems. For several years, we used a MySQL relational database. But the tables linking the distinct trees rapidly became so complex, and the queries required to manipulate them so unwieldy, that we abandoned it. Finally we moved to representing all documents in JSON form, and then storing and retrieving them through a JSON document store (MungoDB). This has proved complex, but very fast and effective. We are able to represent the two hierarchies precisely, in a manner which permits realtime updating and retrieval, within the JSON store. Indeed, one could extend the model we apply beyond two hierarchies: a text could be composed of as many hierarchies as one likes. The first public version of Textual Communities (after seven years of work) will be released in the first half of 2018, and the author will propose a workshop on the system at the conference. This paper will show the full system briefly. It is designed to be easy to use, to the point that a textual scholar with no special computer expertise will be able to use it to create an edition. Further, the implementation of the underlying database in JSON, and of javascript throughout the system, should make it possible for computer programmers expert in Javascript (and with no expertise in XML) to make complex critical editons. The system also contains tools to allow management of a large collaborative edition, with management of transcription page by page. The sophisticated Collation Editor, developed by the New Testament Greek edition projects in Birmingham, England and Munster, Germany, itself built on CollateX, is also integrated. This work raises many questions. XML is currently used for basic document input, and for transcription page-by-page. However, the inability of XML to fully represent more than a single hierarchy in a single document is a serious impediment to Textual Communities. In essence, the textual model we implement in Textual Communities is more powerful than XML can provide. Our hope is that others will take up this challenge, to find ways to move past this weakness in XML. Indeed, we offer Textual Communities not as, in any sense, a definitive system. It is a first attempt to implement the ontology of text and document upon which it is built. We hope and expect others will do better than this system. ",
       "article_title":"Creating and Implementing an Ontology of Documents and Texts",
       "authors":[
          {
             "given":"Peter",
             "family":"Robinson",
             "affiliation":[
                {
                   "original_name":"University of Saskatchewan, Canada",
                   "normalized_name":"University of Saskatchewan",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/010x8gc63",
                      "GRID":"grid.25152.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "philology",
          "graphs",
          "information architecture and modeling",
          "scholarly editing",
          "ontologies",
          "networks",
          "english studies",
          "relationships",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Engaging the “bridges/puentes” theme central to the conference, this paper presents first-hand knowledge and practical insights garnered from a collaborative digital mapping project between North/South academics, students, and community activists engaged in community-based social justice activism in Cali, Colombia. A foundational goal of this Digital Humanities project is thus to create intercultural and communicative bridges between not only the academic communities of Gonzaga University and Pontifica Universidad Javeriana, but also to provide a platform by which Colombian community organizers shape their presence in local as well as digital communities.    The paper discusses our goals and methods, and also the roadblocks we encountered, in establishing collaborative pathways to embed Digital Humanities mapping tools as central elements within a field-based Communication and Community Development course. The Digital Humanities project at the heart of this course aligns with pedagogy as well as practical fieldwork in the area of Development Communication, which holds that communication processes and projects that support or foster the growth of grassroots civil society are essential elements of community development and empowerment. In this vein, Digital Humanities perspectives and methodologies that privilege the bottom-up democratization of access and information inform course content assigned to students from the Global North, who come to Cali, Colombia as part of an intensive immersion.    As such, the course invites students from Cali and the United States to engage, accompany, and shadow community-based organization that work in areas such as citizens’ radio; street theatre and community-based performances; and grassroots documentary production. The work undertaken by the community-based organizations seeks to displace hegemonic media and dominant culture imaginaries, which routinely render these resource-deprived communities as being inherently abject, dangerous, chaotic, and pathological.  T he community organizations with whom we partner engage community problems by creating and claiming spaces for public expression that amplify popular voices within their own communities and beyond. The Digital Humanities mapping project developed for this course responds to these community initiatives, in that it serves as a community-academy collaborative space. The digital map produced collaboratively,  provides a platform that presents, promotes, captures, and renders visible popular or grassroots  media, communication activities, and products through which “citizens can learn to manipulate their own languages, codes, signs, and symbols, empowering them to name the world in their own terms” (Rodriguez 2011). The paper documents a central element of our work, the mutual efforts engaged in creating active and equitable roles for each party involved in the digital product’s production, whether those groups come from the academy, the community, the Global North or the Global South.    Beyond discussing the compatibility and fit of Digital Humanities tools with the articulation of community-based approaches to citizens’ media and popular communication, this paper also discusses the significant ways in which Digital Humanities mapping tools can be mobilized to foster or promote community-based experiential learning experiences in intercultural contexts bridging the global North and South. Experiential learning emphasizes the acquisition of knowledge through interactive processes of action and reflection; where students can take an active part in the creation of knowledge (Hale 1999). We contend that producing Digital Humanities projects in the contexts of an international immersion and hand-in-hand with local partners whose voices, perspectives, and needs drive project conceptualization and the mapping process, presents a unique opportunity for experiential learning that extends well beyond the classroom and into the lived realities of all the parties involved. In this vein, the experiential learning opportunities developed in such an environment embrace the broader humanistic agenda of Digital Humanities as a field, where people come together through and with technology to “produce a collaborative, connected, and relational knowledge production, of making and learning and learning through making” (Goldberg 2015). Accordingly, our project seeks to facilitate an experiential learning opportunity for our students, but in doing so we also seek to diminish the sometimes too rigid boundaries that privilege academic institutions as the sole purveyors and producers of knowledge. By collaboratively creating a digital map with and for local community members while in their communities, our project aims to decentralize knowledge production and encourage our students to become conscious of diverse forms of knowledge and authority.    Furthermore, our experience also suggests that with effective planning and development, community-based Digital Humanities mapping projects can productively alleviate issues and problems that commonly arise in the context of experiential- or service-learning courses taking place in intercultural contexts across the North/South boundary. It is well known that “service-learning can reinforce stereotypes and paternalism among students. Some scholars argue that many applications of service learning do little to question the role of students as providers of resources…” (Chupp & Joseph 2010). Additionally, service- or experiential-learning is “often implemented with a sole focus on the potential beneficial impact on the student, with little or no emphasis on the possible longer-term beneficial impact on those served by the activity and their broader community” (Chupp & Joseph 2010). The collaborative mapping project we have developed engages Digital Humanities approaches within an embedded community context, with the explicit intention of addressing potential problems linked to the implementation of experiential service learning project in partnership between the North and South.    In sum, the Digital Humanities mapping project nested within this Communication and Community Development course remains an experimental and open collaboration. Well-established and emergent issues and challenges continue to exist. With that caveat in mind, experience and evidence also suggests that digital technology mapping tools provide a set of ready enhancements to experiential learning, study abroad, and Communication and Community Development courses. These features begin to realize the promise and purpose of Digital Humanities by creating bridges that foster global collaboration, create open access platforms, and generate academy-community, North/South collaborations that equalize access to the generation and circulation of knowledge locally and globally.  KEYWORDS: Global South/North, Experiential Learning, Mapping, Community Development, Citizens’ Media ",
       "article_title":"Mapping And Making Community: Collaborative DH Approaches, Experiential Learning, And Citizens’ Media In Cali, Colombia",
       "authors":[
          {
             "given":"Katey",
             "family":"Roden",
             "affiliation":[
                {
                   "original_name":"Gonzaga University, United States of America",
                   "normalized_name":"Gonzaga University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03ze70h02",
                      "GRID":"grid.256410.4"
                   }
                }
             ]
          },
          {
             "given":"Pavel",
             "family":"Shlossberg",
             "affiliation":[
                {
                   "original_name":"Gonzaga University, United States of America",
                   "normalized_name":"Gonzaga University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03ze70h02",
                      "GRID":"grid.256410.4"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "and curriculum",
          "globalization & digital divides",
          "teaching",
          "film and media studies",
          "cultural studies",
          "pedagogy",
          "multilingual / multicultural approaches",
          "English",
          "anthropology",
          "interdisciplinary & community collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Digital resources in poetry in Spanish are scarce, particularly for certain periods. This poses difficulties for Digital Humanities studies in Spanish.  Some digital editions of medieval poetry exist, e.g. BiDTEA (Gago Jover et al, 2015), ADMYTE (Marcos Marin and Faulhaber, 1992), PoeMetCa (Escribano et al, 2016), besides resources containing partial editions like ReMetCa (González-Blanco and Rodríguez, 2014). For the Golden Age, Navarro-Colorado et al. (2015) presented the  Corpus of Spanish Golden-Age Sonnets. For later periods, we are not aware of poetry collections, although other genres are covered in Textbox (Schöch et al, 2017), BETTE (Santa María Fernández et al, 2017), Aracne (Álvarez and Martín, 2015) or Revistas Culturales 2.0 (Ehrlicher and Rißler-Pipka, 2015).  This paper describes the DISCO corpus and how it complements available digital materials for poetry in Spanish in several respects: First, the author and period range. Second, metadata concerning the authors and their works expressed in TEI-RDFa, given the importance of interoperability between literary datasets and the advantages of Linked Open Data as a paradigm. Finally, example findings that can be obtained with our corpus are provided, regarding metrical patterns diachronically.  The corpus is available on GitHub    https://github.com/postdataproject/disco /   and Zenodo.    https://doi.org/10.5281/zenodo.1012567       Corpus description The corpus contains 4087 sonnets in Spanish by 1204 authors (15th to 19th century),   About 125 sonnets by approx. 20 authors whose production took place in the early 20th century (with date of death prior to 1936) are also included in the corpus; the documentation on the GitHub repository (footnote 1 above) provides more details.   extracted from HTML sources at Biblioteca Virtual Cervantes (García, 2005, 2006a, 2006b) and Wikisource. Sonnets were chosen given the form’s importance in European poetry, where it is even considered as its own genre. The form’s clear restrictions make it easily amenable to computational treatment, facilitating meaningful comparison across poems. Several computational linguistics studies on the sonnet exist (Navarro-Colorado et al., 2015, 2016, 2017a, 2017b; Agirrezabal, 2017). A new sonnet corpus complements earlier work on both traditional and computational poetry analyses.   We focused on canonical and non-canonical authors, from different Spanish-speaking countries (Figure 1).    Sonnet and author distribution per period, including the number of female and male authors, and the continent where they developed their literary activity. Numbers in parentheses indicate authors which were probably active in Europe.   Encoding Paradigms: TEI and Linked Open Data The poems are encoded in XML-TEI P5. A plain-text version is also provided. Together with the TEI-semantics, this corpus provides a layer of Linked Open Data (LOD) expressed in RDFa (Herman et al., 2015). To our knowledge, no out-of-the-box tools exist for publishing literary TEI corpora as LOD.   Whereas the publication of literary corpora in Linked Open Data formats is not widespread, inspiration could be drawn from the linguistics community, which has been especially successful in building the means to convert resources with linguistic annotations to the Resource Description Framework model (see McCrae et al., 2011; Chiarcos and Ejavec, 2011). In addition, more general projects, not limited to linguistic analysis, are being developed as well: see work on building a TEI ontology in Ciotti et al (2016).   In this context, the enrichment of TEI with RDFa attributes is a solid approach to translate TEI semantics to the web (see precedents like Jewell, 2010) and benefit from the wide range of possibilities of the Semantic Web: First, we enrich our dataset by linking to third-party ones (as DBpedia), providing additional resources to complement the corpus. Second, we publish our data openly using standard schemas, thus supplying semantic interoperability that allows third-party applications to automatically use our data.      Author metadata  Author metadata were extracted or inferred from unstructured source content, and specified in the teiHeader: Year, place of birth and death, and gender. Two versions of the texts are available: one collecting every sonnet per author, the other with a single sonnet per file.  For the current corpus release we augmented the TEI annotation with URIs and class/property information, expressing them in RDFa. The most straightforward information concerns authors and their works, and the DCMI Metadata Terms (DCMI Usage Board, 2012) provides an appropriate scheme. Most features regarding authors’ biographical data were formalised with the FOAF vocabulary (Brickley and Miller, 2014). Links to other resources were supplied. For instance, authors were assigned  Virtual International Authority File (VIAF) identifiers, by querying VIAF’s API supplemented with manual validation. Since the corpus includes non-canonical authors, LOD is an important asset to share their work thanks to the enhanced display of this type of data implemented by search engines.  Our documentation 1 provides further details.      Metrical encoding and enjambment  Using the  met attribute, each line was annotated for scansion (strong and weak syllables) with the ADSO tool    https://github.com/bncolorado/adsoScansionSystem    (Navarro-Colorado, 2017), which specializes in Spanish fixed-meter forms, attaining a performance of 0.95 F1. A heuristic was used to automatically annotate the quatrains’ rhyme-scheme, i.e. enclosed (ABBA) or alternate (ABAB).  Using an  enjamb attribute, lines were annotated for enjambment   The tool detects different types of enjambment (i.e. a mismatch between syntactic and metrical structure) as characterized by Quilis (1964). The tool also detect’s Spang’s (1983) concept of  enlace, which takes place when a subject or direct object occur in a line adjacent to their governing verb’s line, and which triggers a less noticeable effect than the enjambment types defined by Quilis   with the ANJA tool   See   https://sites.google.com/site/spanishenjambment/  for details   (Ruiz-Fabo et al., 2017). The tool’s performance at detecting enjambment is above 0.8 F1, and its efficacy at classifying enjambment types varies across periods and types. A  cert attribute specifies the expected certitude for each enjambment type annotated.   The corpus documentation 1 provides more details.    How’s this corpus different? The metadata mentioned in 2.2. were unavailable in structured, machine-readable format in the corpus sources, or in other sonnet collections, like Sonnet-Archiv (Elf Edition). Regarding coverage, the corpus complements Navarro-Colorado et al’s (2015) Golden Age Sonnet corpus, by including minor Golden Age authors. For later periods, we cover more poems and authors than existing digital corpora, up to the 19th century. Our corpus integrates RDFa annotations, which in a second version will be fully compliant with the POSTDATA model.   See Bermudez-Sabel et al. (2017).  Version 0.2 of the POSTDATA model is available at   https://doi.org/10.5281/zenodo.832906    This is a pioneering model that will provide means to publish European poetry materials as Linked Open Data. Finally, combining the annotation of metrical patterns, stanza types and enjambment is not offered by prior corpora.       Some metrical findings  Corpus data on stress patterns (Figure 2) agree with existing descriptions   See Domínguez Caparrós (2014: 143) or Henríquez Ureña (1919: 132) for details on  a maiori and  a minori patterns. The main  a maiori variants as described in previous literature are 2 6 10 and 3 6 10; this is confirmed in our data. Patterns are formalized as a series of numbers indicating stressed syllables, e.g. 2 6 10 for the second, sixth and tenth syllables. Note that 10th-syllable stress is mandatory in all patterns.    of the Spanish hendecasyllable based on small-sample analyses: A maiori patterns (with 6th-syllable stress) predominate, and a minori patterns (with 4th-syllable stress) follow. However, our data show an increase of a minori patterns in the 19th century, which might suggest an interest in metrical variety in that period.  Regarding diachronic data on the number of stressed positions (Figure 2), patterns with three stresses are highly used across periods. However, most a maiori patterns with four stresses decrease in the 19th century. This might indicate a 19th-century preference for “lighter” patterns, with stresses further apart from each other. Whereas the predominant meter for sonnets is naturally the hendecasyllable, alexandrines   In Spanish, the alexandrine has 14 metrical syllables. In sonnets, the hendecasyllable predominates almost exclusively. However, particularly since the 19th century, alexandrine sonnets have been written.   are attested, mostly in the 19th century, preferentially used by American authors. The alexandrine sonnet uses an alternate rhyme scheme (ABAB) more often than the usual enclosed scheme (ABBA). See Figure 4.     Distribution of stress patterns per period (percentage of lines for each pattern) for the 10 most frequent patterns in the corpus, sorted by decreasing percentage of occurrence in the complete corpus. Pattern classes are also provided ( mai:  a maiori, i.e. stress on 6th syllable,  min:  a minori, i.e. stress on 4th and 8th syllable). Rows for  a minori patterns are in italics.  Stress count refers to the number of stresses in the pattern. Patterns with three stresses are widely used in any period. Most  a maiori patterns with 4 stresses decrease in the 19th century, whereas  a minori patterns increase in that century.         Distribution of stress patterns per period (percentage of lines for each pattern) for the 11 most frequent patterns in the corpus.       Count of hendecasyllable vs. alexandrine sonnets according to the authors’ continent of production, in the  19th century (alexandrine sonnets are very rare before). The type of rhyme scheme in the quatrains (enclosed or alternate) is also specified. The alexandrine sonnet is preferentially used by American authors, and there’s a preference for alternate rhyme for this meter length.   Acknowledgements Supported by the project ‘Poetry Standardization and Linked Open Data: POSTDATA’ (ERC-2015-STG-679528), funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme, and led as a Principal Investigator by Dr. Elena González-Blanco, LINHD-UNED ( http://postdata.linhd.es/).   ",
       "article_title":"The Diachronic Spanish Sonnet Corpus (DISCO): TEI and Linked Open Data Encoding, Data Distribution and Metrical Findings",
       "authors":[
          {
             "given":"Pablo",
             "family":"Ruiz Fabo",
             "affiliation":[
                {
                   "original_name":"UNED (Universidad Nacional de Educación a Distancia), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Helena",
             "family":"Bermúdez Sabel",
             "affiliation":[
                {
                   "original_name":"UNED (Universidad Nacional de Educación a Distancia), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Clara",
             "family":"Martínez Cantón",
             "affiliation":[
                {
                   "original_name":"UNED (Universidad Nacional de Educación a Distancia), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Elena",
             "family":"González-Blanco",
             "affiliation":[
                {
                   "original_name":"UNED (Universidad Nacional de Educación a Distancia), Spain",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Borja",
             "family":"Navarro Colorado",
             "affiliation":[
                {
                   "original_name":"Universidad de Alicante, Spain",
                   "normalized_name":"University of Alicante",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/05t8bcz72",
                      "GRID":"grid.5268.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-21",
       "keywords":[
          "computer science",
          "corpora and corpus activities",
          "spanish and spanish american studies",
          "linking and annotation",
          "resource creation",
          "English",
          "and discovery",
          "digitisation",
          "literary studies",
          "standards and interoperability"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Even-Zohar's polysystem theory is a well-established approach to understanding how entire translated literatures interact (or not) with the body of the receiving native literary culture. Even-Zohar identifies a number of possible interactions depending on the relative \"strength\" and \"age\" of the two (or more) literatures, and translated literatures may assume \"peripheral\" or \"central\" positions within the target literary polysystem. According to this scholar, translations are usually peripheral to native literature; but he also cites examples where a given literary polysystem places some imported subsystems in a central position, while other “foreign imports” remain in the periphery (Even-Zohar 1990). Even-Zohar thus deals with literary creation en masse rather than, as is often the case in academic approaches to literary translation, on single books original and translated. The obvious parallel to Distant Reading has already been drawn (Helgesson and Vermeulen 2015, 25-26); but it might also be tempting to do the same for a related approach, macroanalysis, if we are to follow the distinction made by the exponent of the latter term (Jockers 2013, 48). Both bring together investigations into masses of literary material unattainable by traditional close reading; yet macroanalysis looks inside many books at once using quantitative methods applied to their lexical layers that have been called “stylometry” well before both Moretti and Jockers.   Material From our personal mixed Polish-Italian perspective, few cases could serve as a better pretext to try to negotiate this marriage between polysystem theory and computational stylistics than that of  Quo vadis (1896), the historical romance by Henryk Sienkiewicz, Poland’s first literary Nobel Prize winner of 1905. Its international success – long gone with the wind but unparalleled by any other Polish novel to this day – resulted in a veritable explosion in terms of numbers of translations into various languages. In many countries, several different translations simultaneously vied for the public’s attention. Yet “several” does not even begin to describe the situation in Italy, where at least three hundred different editions can be still found today (Woźniak 2016). In the first two years of the existence of  Quo vadis on the Italian market (1899-1900), as many as eight different translations were already available to the readers (Berti and Gagetti 2016).   No wonder: not only was the novel set in the Italian capital and not only did it deal with a subject already very present in Italian culture old and new; the book’s (and its author’s) brand of conservative Catholicism must have appealed to some of the most influential circles of the country. Yet the novel was also praised by some of Italy’s progressive critics, who saw, in Sienkiewicz’s persecuted Christians, the struggle of their contemporary revolutionary movements, and who liked to read his depiction of Imperial Rome’s decadence as a diatribe against the existing power structure (Marinelli 1984).  This profusion of Italian renderings is also the reason why building their representative selection was no easy task. Only a single translation was available online; a search in Polish and Italian libraries provided almost seventy candidate texts: signed or unsigned by a translator, published by a variety of publishers, often in several somewhat different editions. In the end, twenty-four translations produced until mid-20th c. have been identified as more or less independent of each other, although some of these still share over 50% of material, as evidenced by comparison of texts for identical 5- or longer word clusters with  WCopyFind (Bloomfield 2011-2016). When applied to genuinely different translations, the similarity ratio is of the order of 5-7%.  The natively Italian literary polysystem was represented by close to 1300 different literary texts, mostly selected and adapted from  Progetto Manuzio, one of the most comprehensive Internet collections of electronic texts in Italian. To include as many texts as possible, this set of Italian writing included dramas, epic poems and opera libretti as well as novels and novellas from the 15th to the 21st century. Several translations of other novels by Sienkiewicz were also added to the collection, and another big body of translations of a single author, Shakespeare, was included as well.    Methods The stylometric method applied has been described by Eder (2017) and applied to other literary corpora by Rybicki (2014, 2016). Basing on Burrows’s Delta procedure (2002), a list of most-frequent words (MFWs) is produced for the entire corpus. These words are then counted in the individual texts, and their frequencies are compared in text pairs to produce a matrix of distance measures; in this study, the distances were established by means of the modified Cosine Delta (Smith and Aldridge 2011), which is now seen as the most reliable version (Evert et al. 2017). The distance matrix then undergoes Cluster Analysis (Ward’s hierarchical clustering), resulting in grouping the texts into “clusters” of greatest similarity; this is repeated for reiterations from 100 to 2000 MFWs at 100-word increments, and a consensus between the individual iterations is produced to show each text’s most consistent nearest neighbors, next-to-nearest neighbors and next-to-next-to nearest neighbors. The procedure is performed by means of  stylo (Eder et al. 2016), a stylometric package for  R (R Core Team 2016). The results are visualized by means of network analysis, applying the “Force Atlas 2” gravitational algorithm (Jacomy et al. 2008) in  Gephi (Bastian et al. 2009) to the above-mentioned scores. Instead of applying a “human-made” classification of the resulting network of nodes and edges (i.e. identifying authors, genres and literary periods based on external and traditional literary history), the task of dividing the network into groups of greatest internal similarity was entrusted to  Gephi’s modularity function, which finds communities within a weighted network (Blondel et al. 2008). The main experiment was conducted by successively increasing the number of communities shown until the expected separate cluster of translations of  Quo vadis became a separate entity in the network, and the degree of its discreteness could thus be assessed.    Results Dividing the network into just two modularity groups failed to isolate Sienkiewicz from the main Italian community. Instead, the main division was that between 19 th/20 th-century novels, translated or originally Italian, and everything else – the one notable exception to this rule was the prose of Pirandello, classified with the earlier texts. At three modularity groups, Italian drama detached itself from early prose. At four, the first writer became a separate community, but this was the native Deledda rather than the alien Sienkiewicz. At five, 19 th- and 20 th/21 st-century novels became two distinct groups; at six, another native Italian, Salgari, received his own class; at seven, pre-19 th-century works detached themselves from later prose. It is only at ten communities that a translated rather than an Italian author became a separate subsystem (to use Even-Zohar’s term) – in fact, not one but two: Sienkiewicz (not just his  Quo vadis) and Shakespeare (Fig. 1).      Figure 1. Network analysis of distances between most-frequent-word usage. Thick and short lines (edges) denote small distance (or high similarity). For simplicity, only the final 10-community modularity is shown.     Discussion It seems too much of a coincidence that two major subsystems (translations of Shakespeare and translations of Sienkiewicz) become separated from the main body of literature in Italian at the same time, and that this happens only after two native authors receive their own subsystems. If such a mechanism were to be observed in even more extensive collection of texts (when they finally become available), Even-Zohar’s hypothesis of the usually peripheral position of translated literature could find its stylometric illustration. At the same time, this experiment confirms not only that original novels are more similar to translated ones than the former to original drama; but also that certain original authors are more different from other original authors than those translated from another language. Obviously, this hypothesis must be tested in the future in other literary polysystems to claim that the affinity between polysystem theory and macroanalysis is anything more than metaphorical. Even-Zohar speaks of reception of literary works within a broader national culture; macroanalysis counts context-free words. Still, in its attempts to bring distant and close reading together, stylometry has been clutching at even weaker straws. Stylometrists continue to make similar leaps (of faith?) between their graphs and trees and networks on the one hand, and traditional literary history on the other. They usually believe that frequencies of very frequent words provide insights into more abstract characteristics of texts than their mere lexical or even grammatical difference: and these abstracts so far include authorship, genre, chronology, or gender. This study might just have added a new one. At the very least, it is an invitation to apply Even-Zohar’s concepts in various “distant” approaches to literature.   Acknowledgements This research was made as part of the project: “Miejsce  Quo vadis? w kulturze włoskiej. Przekłady, adaptacje, kultura popularna” (0136/ NPRH4/H2b/83/2016), funded by Poland’s National Program for Advances in the Humanities (NPRH).   ",
       "article_title":"Polysystem Theory and Macroanalysis. A Case Study of Sienkiewicz in Italian.",
       "authors":[
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow, Poland, Poland",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Katarzyna",
             "family":"Biernacka-Licznar",
             "affiliation":[
                {
                   "original_name":"Uniwersytet Wrocławski, Wrocław, Poland",
                   "normalized_name":"University of Wrocław",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/00yae6e25",
                      "GRID":"grid.8505.8"
                   }
                }
             ]
          },
          {
             "given":"Monika",
             "family":"Woźniak",
             "affiliation":[
                {
                   "original_name":"Università degli Studi di Roma \"La Sapienza\", Italia",
                   "normalized_name":"Sapienza University of Rome",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/02be6w209",
                      "GRID":"grid.7841.a"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-29",
       "keywords":[
          "graphs",
          "italian studies",
          "networks",
          "stylistics and stylometry",
          "relationships",
          "English",
          "literary studies",
          "translation studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Even as the United States fought for independence in the American Revolution, it was already in the process of becoming a settler colonial power in its own right. This short paper interrogates the origins of American settler colonialism through text mining three corpora of personal and official documents. In order to understand and address present structural inequity in the United States, scholars, policy-makers, educators, and the public need to examine the country’s long history as a settler colonial society.  Through topic modeling and text mining methods, my research highlights the underlying goals and desires that prompted land acquisition, settlement, and cycles of violence between Euro-American settlers and Native Americans in the trans-Appalachian west between 1776 and 1820. This project explores three collections, or corpora, of documents, separated by the positions of the historical authors and document type: settler correspondence and records; official government documents; and writings of political elites in the eastern United States. The first corpus for this study consists of correspondence, journals, and memorials from settlers, colonial officials and military leaders in the territories (colonies) between 1776 and 1820. This is the smallest corpus of the three, at two million words. Few documents from representative settlers have been transcribed and published, so the corpus over-represents leaders in the settler communities, however the petitions from the settlers to Congress give voice to the most pressing challenges, needs, and hopes of the settlers themselves. The documents included in each corpus were transcribed and published in bound volumes during the nineteenth century and are now in the public domain. A second corpus, of approximately four million words, consists of official government records, including treaties with Native American communities, military records, documents related to public lands and governance of the territories, as well as pension and other petitions submitted to Congress in the late eighteenth and early nineteenth centuries. The third corpus is, by far, the largest of the three, at approximately 39 million words, and consists of the papers of the foremost political leaders in the eastern United States. The letters of the members of the Continental Congress are included, as are the writings of George Washington, James Madison, Thomas Jefferson, Benjamin Franklin, and John Adams. Not surprisingly, these statesmen wrote far more than settlers, who were primarily concerned with agricultural cultivation, hunting, and defending their families on the frontier. The aforementioned sources form the corpora for text mining and analysis experiments. My study extracts and compares American settler, administrator, and political leaders’ perspectives on significant topics in the study of settler colonialism, such as land value; property acquisition and sales; as well as the presence, actions, and views of Native Americans. Early experiments using the LDA algorithm in MALLET to topic model the corpora and Lexos to visualize the topic clouds have already revealed significant patterns (Blei, 2012). While recognizing that topic models are more effective with large corpora, my research began with a small experiment. Using MALLET, I created a topic model of ten topics of the twenty-five published petitions from settlers to Congress (1787-1798) from the  Territorial Papers of the United States. This model suggests that one of the primary motivations for Euro-American emigrants to move to the western territories was to achieve what they described as a competency, or the means to rear their children “in a comfortable manner” and “raise a subsistence by their [own] industry” (Petition from the Inhabitants of Vincennes to Congress, 1787). The topic related to land reveals the dominant concerns that settlers expressed. They implored Congress to recognize their existing land claims, ensure reasonable land prices, provide military protection from Native American raids, and ensure justice through the provision of judges. These measures, they believed, would foster access to land, enable trade, establish legitimacy, and provide settlers with the means to achieve their modest goals.   Even though their objectives differed from those of the settlers, government officials both in the east and on the ground, in the western territories, were equally motivated to acquire land beyond the Appalachian Mountains.  In the aftermath of the American Revolution, the government was in dire financial straits. Political leaders urged agents to obtain western lands from Native communities so that the territory could be sold to pay off the burdensome war debts. Consequently, backcountry government officials decried settler violence against neighboring Indigenous communities, even as they took advantage of the unruly settlers’ actions to compel land cessions that the United States government desperately needed.     Figure 1: Topics related to land in the Continental Congress members’ correspondence There was a high price to be paid for white American independence though, as is demonstrated in the topics generated from the Continental Congress members’ correspondence records (Figure 1). The words “transmitted, negotiations, ceding, extinguishment, extinguishing,” and, ominously, “funeral” stand out among the more benign “northwest, lands, and western.” Most of these words are more or less neutral when considered out of context, but, given their use in relation to the settler colonial endeavor, they evidence the brutal effects of American land acquisition and expropriation from Native communities.  These topics and the related documents both direct attention to specific sources for close reading, but also yield new terms of interest to explore at a distance and in a broad comparative framework. In addition to the results of topic modeling the aforementioned corpora, this presentation will also share experiments using part-of-speech tagging and collocations to explore concepts, such as land, family, independence, competency, and war to understand the ways in which settlers, and political and military leaders conceived each of these topics.  This talk offers an initial glimpse into the early stages of a much larger project that seeks to create an interactive interface for documents from the first four decades of the United States’ formation as a nation and nascent empire based on topic models and text mining approaches, such named entity recognition, and collocates. The interface will eventually allow users to drill down into documents that contain specific sought-after features, such as individuals’ names, gender identity, topics of interest, etc. This interface, it is hoped, will enable historians, students, genealogists, and interested members of the public to explore some of the most important documents related to the complicated, conflicting, and, occasionally, complementary objectives of American settlers and other political actors. The policies these agents developed between 1776 and 1820 not only shaped American settler colonialism in the eighteenth and nineteenth centuries, but they continue to reverberate more than two centuries later.  ",
       "article_title":"Interrogating the Roots of American Settler Colonialism: Experiments in Network Analysis and Text Mining",
       "authors":[
          {
             "given":"Ashley",
             "family":"Sanders Garcia",
             "affiliation":[
                {
                   "original_name":"The Claremont Colleges, United States of America",
                   "normalized_name":"Claremont Colleges",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03xaz7s88",
                      "GRID":"grid.431610.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "graphs",
          "corpora and corpus activities",
          "networks",
          "historical studies",
          "text analysis",
          "relationships",
          "data mining / text mining",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This project looks at the work of American-Chicana poet and fiction writer Gloria E. Anzaldúa, author of  This Bridge we Call Home (2002). My research proposes a digital representation of Gloria Anzaldúa's Frontier theory as part of my scholarly investigation. This study will include the creation of a mapping tool that will reflect the rhizomatic spaces analyzed by the author, raising awareness about the multiple cultural identities found in the United States. Through personal narratives, theoretical essays, poetry, letters, works of art and fiction,  This Bridge we Call Home examines issues such as classism, homophobia, racism, political identity, native sovereignty, lesbian pregnancy and motherhood, transgender issues, Arab-American stereotypes, Jewish identities and spiritual activism. These stories are written by women and men, both of color and white, and motivated by a desire for social justice.  This Bridge We Call Home invites feminists of all colors and genres to develop new forms of transcultural dialogues, practices, and alliances.  The anthology, object of study is the last work produced by the author before she passed away and undertakes a more inclusive essence compared with her earlier writings. The book includes women and men of different classes, nationalities, races, ages and sexual orientations, reflecting the desire of inclusivity and dialogue promoted by the author and editor. This project also attempts to bring together multiethnic voices and promotes a interdisciplinary resource that interest not only the literature and culture discipline, but also other humanities fields, such as history, anthropology, sociology and gender studies.   The result of this project will be a powerful new online education and research tool for undergraduate and graduate students as well as the world community at all levels of expertise. To create this public resource I will use the mapping tool “Google Lit Trips”, a site affiliated with Google. Normally this tool is used to recreate and mark the journeys of fictional characters from famous literature works. In my case I will use the various sections of Gloria Anzaldúa’s anthology that reflect real life experiences of the writers. I will then provide geospatial representations of the true stories narrated by the authors that live some kind of political, racial, sexual or class struggle in the United States. In the book 87 writers are given a space to celebrate their diversity. In the mapping tool,  at each location along the journey there will be placemarks with pop-up windows containing a variety of resources including relevant media, thought provoking discussion starters, and links to supplementary information about 'real world' references made in that particular portion of the text. The author voice herself emerges beyond the limits of either American or Mexican culture and provides a voice to the people of the borderlands. Her work is based on multiple experiences to create a universal history that transcends the social barriers that connect us collectively with each other. While the politics of identities requires subjecting ourselves to specific categories of identity, spiritual activism requires that we get rid of all these barriers.   This project has the objective to put the reader inside the stories, provoking reflections and awareness about contemporary social, political, sexual and racial issue that affect our modern society. The reader will travel alongside with the protagonists of the autobiographical stories through the recreation of 3D geographic tours of the narratives that have been described. At the same time the mapping tool creates an engaging and  relevant literary experiences for students. At each location I will be able to include web links, videos, audios, images, annotations and critical activities related with the different sections of the anthology. The experience of the pop up windows provide a range of supplementary information, such as links that give additional information about the 87 authors or cultural traditions that have been mentioned by the characters. The students find themselves seeing the settings almost how they were there. The pop up windows provide engaging content, such as audios, videos or activities related to the story line. These activities are designed to help readers discover connections between their culture and the different cultures that have been described in the story.   One of the primary goals of this project is to emphasize the relevance of cultural diversity in the University environment in the context of the Hispanic world. My objective is to initiate contemporary debates over themes such as immigration, globalization, discrimination, acceptance and inclusion. The mapping tool will explore ways of bringing its unique materials to a wider audience inside and outside the United States. The contribution of this project is not only to continue expressing a dialogue within and between women, women of color, and among people that live in the borderlands, but also to expand visions and theoretical spaces in general. The different stories told in the anthology explore the different shades of the mixed-race identity of women and men that are often perceived as outsiders within their own country.  The digital representation of the anthology and its multiple resources proposes a new attitude towards the learning process of college students and the public sensitivity outside the academia. One of the primary intentions is to dismantle traditional forms of identity, and destroy social boundaries, by embracing difference and otherness as a unique component of every single individual part of our society. The focus on themes such as the effects of migration and globalization are evident in the transnational, transcultural and transgender identities represented though the voices of the 87 writers. The external links provided as resources bring the readers beyond the stories. The students become travelers discovering the similarities and qualities of the characters from cultures beyond their own. This could be an effective way to make students feel part of the stories and hopefully inspire them to fight against the different levels of discrimination that the writers are describing. The final goal will be to include this online platform as an integrative portion of a culture and literature class at the university level.   ",
       "article_title":"Cultural Awareness & Mapping Pedagogical Tool: A Digital Representation of Gloria Anzaldúa's Frontier Theory.",
       "authors":[
          {
             "given":"Rosita",
             "family":"Scerbo",
             "affiliation":[
                {
                   "original_name":"Arizona State University, United States of America",
                   "normalized_name":"Arizona State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03efmqc40",
                      "GRID":"grid.215654.1"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "audio",
          "electronic literature and digital arts",
          "cultural and/or institutional infrastructure",
          "spanish and spanish american studies",
          "cultural studies",
          "video",
          "feminist studies",
          "English",
          "multimedia",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Giacomo Leopardi (1798-1837) was a significant Italian romantic poet best known for  a volume of poetry,  Canti , published in 1835 in Naples, one copy of which he manually corrected shortly before his death.  Among his poems, the series entitled  Idilli  ‘Idylls’ (1819-1821) comprises  Alla Luna, L’infinito, Lo spavento notturno, La sera del giorno festivo, Il sogno  and  La vita solitaria . In the earliest extant form in  the Naples Notebook the poems were  written in three separate phases (Idylls 1-3, 4-5 and 6), then corrected  by the author in identifiably different pens , before being copied into the Visso manuscript, where they were again revised. Their first publication was in the review ‘Nuovo Ricoglitore’ in 1825/1826, then in the Bologna edition of  Versi  ( 1826). Except for the third Idyll:  Lo spavento notturno , the other poems were collected in the Florence edition of the  Canti  ( 1831), then reprinted in the Naples edition (1835), in which  Lo spavento notturno  appears as one of the  Fragments  (n. XXXVII).    This textual history of Leopardi’s Idilli  is interesting because much of Italian Philology of the Author ( Variantistica ) has been based upon it. After Francesco Moroncini’s first critical edition of the  Idilli  (1927), three other editions have appeared: Peruzzi (1981), De Robertis (1984) and the latest by Franco Gavazzeni, printed by Accademia della Crusca  ( Leopardi , 2009). This is both a critical edition of the manuscripts and the print-versions. It contains detailed textual notes recording all handwritten changes including marginal alternative readings, which are characteristic of Leopardi’s method of correction. Four different pens have been identified in ‘Naples notebook’ of the  Idilli, before their copying into the Visso manuscript: A, B, C and D, which were used both for writing the new texts and for correcting the earlier idylls. For example, Leopardi wrote idylls 4 and 5, then corrected idylls 1, 2 and 3. With pen C he then wrote idyll 6, and then finally corrected all the preceding ones.   A wiki edition of Leopardi was also produced as part of an advanced course in Italian Literature in  2016-2017 at the University of Rome La Sapienza (Giuffrida and Nieddu, 2017; Caterino and Nieddu, 2017) and was based on Gavazzeni’s critical print edition of the  Canti.   The technical limitations of the wiki software used (MediaWiki), led the group of editors to seek better ways to encode and present the text. They were also interested in producing editions of other authors, and to have the capacity to expand the Leopardi edition in future. As a result of a lecture given by Paul Eggert in October 2017 at the University of Bologna, Eggert suggested the use of the Ecdosis editing system which had already been developed for the Australian romantic poet Charles Harpur. Other tools, such as Tapas (Bauman et al., 2017), EVT (Rosselli Del Turco et al., 2014), CollateX (Dekker and Middell, 2017) and Juxta (n.d.) had also been considered but in spite of their individual strengths none provided the comprehensive web-based editing system for modern manuscripts that the editors were seeking.  Leopardi’s Idylls were chosen as the basis for the pilot project. Although it was designed as a general set of web-tools, this was the first time that Ecdosis had been applied to another editorial project. Conversion of the print and wiki editions into Ecdosis took only 12 days of part-time work. Since Ecdosis’s own WYSIWYG editor was still incomplete it was decided to encode the text in XML and import it. The XML files were created by copying from the wiki edition and from a PDF of the print edition (Italia, 2018). The detailed textual notes of the print edition were used to create separate files for each of the identifiable versions. Within each version changes were encoded with the usual <add> and <del> codes and our own <undeleted> code for earlier alternatives that were not cancelled. The importation process split the corrections and their contexts into separate layers, amalgamating local levels of correction into coherent sub-versions. Although these layers were never written by the author they are still useful as a storage mechanism to record local changes within a version. In this way individual layers remain simple, needing only a few codes to denote changes in format, such as lines, headings and stanzas, since all deletions and insertions have already been converted into layers. Figure 1 shows an example of how this process works for a segment of Idyll 3, La Luna, from the Naples notebook. Finally, the separated versions and layers of each poem were stripped of their remaining markup, which in Ecdosis is stored separately and only recombined with the text for display, so reducing each version/layer to a readable plain text file. This greatly simplifies all subsequent text processing such as searching, comparing and hyphenating when compared to the complexity of the original XML.  Hyphenation of XML encoded texts is quite difficult due to the variety of tags that may occur between two halves of a word. (Bauman, 2016).Our tests of search engines on major digital scholarly editions revealed that literal searches (often more useful than keyword searches) do not work across internal variant boundaries (<add>,<del>,<rdg> etc). Comparison between TEI-XML files containing inline variants is still an open problem that requires human intervention (Bleeker, 2017).      Figure 1: Falsely colourised portion of Naples notebook showing pen A (red) and pen B (blue) During importation the page-images were also linked to the text. The manuscript images are copyright to the National Library, Naples, but it is anticipated that permission to publish these with the edition will be obtained soon. For the moment the site is protected by the same passsword as for the wiki. The linking produces a list of images which scrolls in sync with the text to keep the top and bottom of each page-image aligned with its corresponding position in the text. To provide a smooth transition and accurate alignment between text and image when scrolling all other possible alignment positions are calculated in proportion to these fixed points. Changing the version loads the relevant images on the left hand side of the screen and the corresponding text on the right. Layers within a version can be changed by clicking on a tab above the text, and changes between layers are highlighted. This removes altogether the need for a ‘diplomatic’ display where changes are displayed awkwardly above or below the line using inline formats.  In Compare View differences between versions and layers are shown at the character level: deletions on the left-hand version/layer in red and additions on the right-hand version/layer in blue. When displaying a layer the invariant text is shown in grey to indicate that this is not a true version, but true versions and final layers are displayed in black. Scrolling is also synchronous and aligned left to right, regardless of differences in length between the two currently displayed versions.  Table View resembles the traditional critical apparatus. Differences and similarities between versions/layers are arranged in columns. Versions/layers can be excluded from comparison and the table rebuilt. Also versions can be moved up or down the display to explore specific clusters of variation for editorial purposes. The establishment of a reading text from this information could be encoded as another version and added to each poem as a default text.  The pilot edition has mostly been a success (Leopardi, 2018). There is still some difficulties with the encoding of marginal alternatives which cannot be placed with certainty in the text. These will probably be encoded as annotations instead. Another problem is that the modules in Ecdosis and the website itself currently resemble too closely those of the Charles Harpur Critical Archive (Eggert, 2018) and will therefore need significant customisation. The possibility to export the entire contents of the edition to a simple collection of files in nested folders, encoded in standard HTML and plain text, has mitigated initial concerns about ‘lock-in’. It is hoped to use other Ecdosis tools to expand the edition in future by increasing the number of poems and placing them in the context of research into the issues and people of Leopardi’s day. ",
       "article_title":" From print to digital: A web-edition of Giacomo Leopardi’s Idilli  ",
       "authors":[
          {
             "given":"Desmond",
             "family":"Schmidt",
             "affiliation":[
                {
                   "original_name":"Queensland University of Technology, Australia",
                   "normalized_name":"Queensland University of Technology",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/03pnv4752",
                      "GRID":"grid.1024.7"
                   }
                }
             ]
          },
          {
             "given":"Paola",
             "family":"Italia",
             "affiliation":[
                {
                   "original_name":"Università di Bologna, Italy",
                   "normalized_name":"University of Bologna",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01111rn36",
                      "GRID":"grid.6292.f"
                   }
                }
             ]
          },
          {
             "given":"Milena",
             "family":"Giuffrida",
             "affiliation":[
                {
                   "original_name":"Università degli studi di Catania, Italy",
                   "normalized_name":"University of Catania",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03a64bh57",
                      "GRID":"grid.8158.4"
                   }
                }
             ]
          },
          {
             "given":"Simone",
             "family":"Nieddu",
             "affiliation":[
                {
                   "original_name":"La Sapienza, Università di Roma, Italy",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-29",
       "keywords":[
          "philology",
          "scholarly editing",
          "digitisation - theory and practice",
          "italian studies",
          "library & information science",
          "English",
          "encoding - theory and practice"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital Humanities, and by extension digital humanists, tend towards a culture of open access, interdisciplinary collaboration, and a maker ethos. These disciplinary values position the digital humanities for high impact reaching beyond disciplinary boundaries into more public fora. One might argue that this public-facing ethos is a natural extension of web-based scholarship.    Yet, simply putting resources on the web does not necessarily engage the public or publics they wish to reach. With institutions, research bodies, and funding agencies expecting greater impact from research (see, for example,  Watermeyer, Ozanne, Reale), digital humanities scholarship is increasingly being viewed as an answer to that perpetually thorny crisis in the humanities. In his 2010 article, ‘The Engaged Humanities: Principles and Practices of Public Scholarship and Teaching’, Gregory Jay believes that public scholarship and community engagement will become central to revitalizing the humanities in the 21st century. He argues that the future of the humanities depends upon two interrelated innovations: the organized implementation of project based engaged learning and scholarship, and the continued advancement of digital and new media learning and scholarship (51)  Further, he writes, ‘efforts to connect humanities research and teaching with projects to advance democracy, social justice, and the public good might take advantage of the latest episode of the crisis in the humanities and even represent a new direction for revival (51). A means for advancing new values within our teaching and research is through the development of projects of social relevance which engage the public in their design and implementation. These go by various names: crowdsourcing, participatory engagement, and social engagement.  Crowdsourcing is a popular term that has been used for over a decade. Defined by Mia Ridge in  Crowdsourcing our Cultural Heriage as ‘…the act of taking work once performed within an organisation and outsourcing it to the general public through an open call for participants’ (1). Crowdsourcing structures public participation more as what the public can do for my project as opposed to understanding why the public might choose to spend their creative capital on my research. This may lead to a framing of public participation as project work that gets outsourced to those with lesser skills than individuals within the academy and/or on the project team. This can lead to a bifurcation of a them vs us mentality, with the ‘them’ (the public) not as educated, talented, or resourceful as the ‘us’ within the academy.   This very point was articulated in a thread on the Text Encoding Initiative list in February 2016. After some 15 positive responses about successful crowdsourced projects, one respondent asked why were we not hearing about failures. This quickly morphed into another thread with the subject line ‘Crowdsourcing Transcription Failures’. In this thread respondents posted a number of issues and challenges in carrying out these projects (as well as some advice by others on the list in how to address these). One respondent, however, indicated that it was not feasible to think about public participation at the same level as that from those on the project team. And while in principle, this is a reasonable assumption, the articulation of this particular post starkly drew the us vs them line with the us not having the ‘patience’ to clean up the mess left by the them:  what we call ‘failure’ may simply be a matter of impatience. If we expect to do the equivalent of a ‘barn-raising’ in digital humanities, where a large number of people come together and do a lot of tedious work quickly, we have to expect a lot of ‘mopping up’ to do afterwards. And we're limited by our level of patience, in how far we want to go to train people to mark texts in thoughtful, observant, well-informed ways. (Beshero-Bondar).  This attitude reinforces public perceptions of academics existing in an ivory tower in which research can only be undertaken by a scarce, specialised work force. Participatory engagement projects, on the other hand, begin to debunk notions of us (the experts) against them (the amateurs [or the public]). Participatory engagement, on the other hand, frames involvement on the project differently. It is a political and a design issue, in addition to being a research decision. The participatory process is one that generates new thinking about the research process, audience, and value while a vehicle to challenge limiting beliefs of who our scholarship is for and the role of the humanities as a public good in society. The National Centre for Public Engagement identifies the public as not simply an extended work force, but active players in the design process:    Public engagement describes the myriad of ways in which the activity and benefits of higher education and research can be shared with the public. Engagement is by definition a two-way process, involving interaction and listening, with the goal of generating mutual benefit.    https://www.publicengagement.ac.uk/explore-it/what-public-engagement   In his 2010 book  Cognitive Surplus:  Creativity and Generosity in a Connected Age Clay Shirky observes how the Internet changes the way we spend our spare time. The so-called ‘cognitive surplus’ that used to be spent on passive activities (notably watching television) can now be used in a profoundly different way, for new kinds of creativity and problem-solving. He writes, ‘the wiring of humanity lets us treat free time as a shared global resource, and lets us design new kinds of participation and sharing that can take advantage of that resource’. (39)  Participatory Engagement projects provide us with opportunities to rethink our roles as researchers and as teachers, about our obligations to those in society who have not had the same opportunities as we have, and last, but not least, how to build meaningful, socially relevant, digital collections for our own and future generations. Yet, these types of projects have different lifecycles, require different staffing and skills, and come with different obligations than more traditional DH projects, and many project teams are not prepared for this. This talk will explore the motivations of the public in participating in our scholarship,  our responsibilities in inviting them as collaborators, and new ways to think about the goals, motivations, and audiences for our research.  ",
       "article_title":"Designing Digital Collections for Social Relevance",
       "authors":[
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":"Maynooth University, Ireland",
                   "normalized_name":"National University of Ireland, Maynooth",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/048nfjm95",
                      "GRID":"grid.95004.38"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "art and art history",
          "cultural studies",
          "english studies",
          "English",
          "archives",
          "interdisciplinary & community collaboration",
          "crowdsourcing",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The Coptic language is the last phase of the Egyptian language family, descending ultimately from the ancient hieroglyphs. Coptic Scriptorium has developed a multidisciplinary research platform using core Corpus Linguistics tools and methods in collaboration with other disciplinary methods. This paper will argue that this collaborative, interdisciplinary approach allows for the creation of research resources that enrich even  disciplinary work.  Coptic Scriptorium has created the first open source natural language processing tools for any phase of the Egyptian language family, including a tokenizer, normalizer, part of speech tagger, language of origin tagger (for loan words from Greek, Latin, and other languages), and lemmatizer. We have also contributed annotated data to the universal dependency Treebank project. A fully searchable corpus annotated with these tools is available online at copticscriptorium.org, and all tools and corpora can be downloaded from our GitHub repositories. This paper will argue that multidisciplinary collaboration improves even disciplinary research. Three examples are provided here; these and others will be demonstrated live in the short paper.  Collaboration with Egyptologists creating a TEI Coptic lexicon file enabled the creation of an online Coptic Dictionary, in which words in our searchable database are hyperlinked to the dictionary entries. The dictionary entries likewise show frequency statistics for the terms in our database. This collaboration benefits Egyptology, by providing an open source corpus for teaching and research linked to a dictionary, and it benefits corpus linguistics, by providing clear frequency data and lexical resources for linguists.  Collaboration with Religious Studies scholars has enabled including in our corpora transcriptions of Coptic manuscripts that have never before been published in print. Scholars in Religious Studies have provided transcriptions of texts to the project, enabling scholars in other disciplines, such as Linguistics, to conduct computational corpus research on important, previously inaccessible texts. Likewise Religious Studies scholars can use the database to conduct philological and historical research on religious texts. Coptic Scriptorium also annotates manuscript information of interest to archivists, philologists, and codicologists within a multilayer annotation model. This enables codicologists, philologists, and archivists to use the query syntax of our corpus linguistics database (ANNIS) to investigate research questions about scribal practices, spelling and morphology, and other manuscript-related issues over multiple manuscripts, including utilizing metadata such as repository information, dates and locations of the original manuscripts, etc. We presented the very beginnings of the Coptic Scriptorium project at DH 2014 in Switzerland. This short paper will demonstrate the extensive progress made as a result of collaboration and interdisciplinary partnerships. ",
       "article_title":"Corpus Linguistics for Multidisciplinary Research: Coptic Scriptorium as Case Study",
       "authors":[
          {
             "given":"Caroline T.",
             "family":"Schroeder",
             "affiliation":[
                {
                   "original_name":"University of the Pacific, United States of America",
                   "normalized_name":"University of the Pacific",
                   "country":"Chile",
                   "identifiers":{
                      "ror":"https://ror.org/005gk0s47",
                      "GRID":"grid.441817.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "classical studies",
          "project design",
          "corpora and corpus activities",
          "organization",
          "management",
          "linguistics",
          "linking and annotation",
          "English",
          "theology",
          "natural language processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The past decade has witnessed a wave of manuscript digitization projects initiated by museums, libraries, and individual scholars. This paper will address digitization of some primary sources essential for the study of late antiquity and Byzantine history and religion. Many of these initiatives will advance the study of Greek and Latin texts, as well as Hebrew—the primary languages of the Christian canon and the early to medieval Christian tradition in the West. Research in Syriac, Coptic, and Christian Arabic, however, are essential for understanding the development of religion in the late antique and early Medieval or Byzantine periods. The digitization of their sources has lagged behind. Focusing specifically on Coptic manuscripts—the texts of early and medieval Christian Egypt—this paper will explore the role of colonialism in in the history of Coptic archives and how to resist reinscribing both colonial epistemologies and traditional notions of “canon” after the “digital turn” in archival studies.  Digitization has been heralded as a means of increasing access and availability of texts that may be inaccessible for various reasons, including the dispersal or dismemberment of the original archives or repository. Technology is seen as a possible means to reassemble these dismembered texts and archives, to reunite fragments of papyri and codices virtually online. It is also heralded as a way to save texts that still reside in the Middle East, in zones of political, military, or cultural conflict. Finally some scholars hope it will bring more exposure to traditions that up until now have been seen as marginal to the dominant Greek and Latin traditions. This paper will interrogate two premises: first, that digitization can “recover” or “reconstruct” an original, now dismembered ancient or medieval archive; second, that current digitization efforts are disrupting the dominant canonical paradigms in the study of late antique, Byzantine, and Medieval religious history. The paper will argue that digitization cannot fully repatriate, reconstruct, or save damaged or dispersed physical archives. But the digital can transform our relationships with the sources of early Christanities if we pay critical attention to the limits of the digital, so as not to reify colonial archaeological, archival, and canonical practices in the digital realm.  This paper will first discuss the original collection of Coptic manuscripts in the context of colonial occupation of Egypt, excavations in Egypt, and the antiquities trade. It will then examine the progress, possibilities, and potential problems of digitization initiatives at specific libraries and museums with significant Coptic collections: British Library, Vatican, Bibliothèque Nationale, Österreichische Nationalbibliothek, etc. The paper will also analyze the work of specific digital humanities projects in Coptic (particularly Coptic Scriptorium at the University of Pacific and Georgetown University, PATHs at Sapienza University in Rome, and the virtual Hill Museum and Manuscript Library) as well as the efforts of the Coptic cultural heritage organization the St. Shenouda Foundation to collect microfilms and digital images for diasporic Coptic cultural heritage preservation. The paper draws on insights from post-colonial digital humanities, Native American digital humanities (especially regarding issues of repatriation and digitization of cultural heritage), archival theory, Coptic Studies, and manuscript studies. ",
       "article_title":"The Digitization of “Oriental” Manuscripts: Resisting the Reinscribing of Canon and Colonialism",
       "authors":[
          {
             "given":"Caroline T.",
             "family":"Schroeder",
             "affiliation":[
                {
                   "original_name":"University of the Pacific, United States of America",
                   "normalized_name":"University of the Pacific",
                   "country":"Chile",
                   "identifiers":{
                      "ror":"https://ror.org/005gk0s47",
                      "GRID":"grid.441817.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "classical studies",
          "repositories",
          "corpora and corpus activities",
          "sustainability and preservation",
          "digitisation - theory and practice",
          "globalization & digital divides",
          "archives",
          "English",
          "theology",
          "near eastern studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The research presented here concerns methodological issues surrounding Zeta, a measure of distinctiveness or keyness initially proposed by John Burrows (2007). Such measures are used to identify features (e.g. words) that are characteristic of one group of texts in comparison to another (Scott 1997), a fundamental task that many standard tools support, e.g. WordCruncher (Scott 1997), AntConc (Anthony 2005), TXM (Heiden et al. 2012) or stylo (Eder et al. 2016). Widely used methods include the log-likelihood ratio (where observed frequencies are compared to expected frequencies; see Rayson and Garside 2000) and Welch's t-test (where two frequency distributions are compared; see Ruxton 2006). Zeta, by contrast, is based on a comparison of the degrees of dispersion of features (see Lyne 1985, Gries 2008). Zeta appeals to the Digital Literary Studies community because it is mathematically simple and has a built-in preference for highly interpretable content words. Indeed, Zeta has been successfully applied to various issues in literary history (e.g. Craig & Kinney 2009, Hoover 2010, Schöch 2018). However, its statistical properties are not well understood, as important work on evaluating measures of distinctiveness (Kilgariff 2004, Lijfijt et al. 2014) has not included Zeta. Therefore, we submit two key aspects of Zeta to exploration and evaluation: (a) variations in the way Zeta is calculated and (b) variations of key parameters. We gain a more precise understanding of how Zeta works and propose a new variant, “log2-Zeta”, that shows more stable behavior for different parameters than Burrows’ Zeta.   Deriving Zeta variants and key parameters Zeta is calculated by comparing two groups of texts (G1 and G2). From each text in each group, a sample of n segments of fixed size with m word tokens is taken. For each term (t) in the vocabulary (e.g., consisting of lemmatized words), the segment proportions (sp) in each group are calculated, i.e. the proportion of segments in which this term appears at least once (binarization). Zeta of t results from subtracting the two segment proportions:     From this formalization, we can derive several variants of Zeta: applying division instead of subtraction; using relative frequencies (rf) instead of segment proportions (sp); and applying a log-transformation to the values rf and sp instead of using them directly. This results in eight variants of Zeta (Table 1).     segment proportions relative frequencies   normal log2 normal log2   subtraction sd0 ds2 sr0 sr2   division dd0 dd2 dr0 dr2   Table 1: The eight variants of Zeta with their labels; “sd0” corresponds to Burrows’ Zeta. The formalization also points to two major parameters of Zeta: segment sampling strategy (using all possible consecutive segments, or sampling n segments per text to overcome text length imbalances) and segment size (segments with m tokens, influencing the granularity of the dispersion measure). We expect the segment size to be of particular importance, as choosing extreme values affects the calculations very strongly: using a segment size of 1 token is equivalent to relative term frequencies; using unsegmented texts is equivalent to document frequencies. Because Burrows (2007) gives no theoretical justification for his particular formulation of Zeta, a systematic exploration and evaluation is called for.   Text collection, code and raw data Experiments have been performed using two very different text collections:  A collection of French Classical and Enlightenment Drama (1630-1788): 150 comedies and 189 tragedies (from the Théâtre classique collection; Fièvre 2007-2017). A collection of Spanish novels (1880-1940): 24 novels from Spain and 24 from Latin America (from the CLiGS textbox: Henny 2017 and Calvo Tello 2017).   For reasons of space, we only report results for the Spanish novels. Texts, metadata, code, results and figures are available on Github:  .    Methods and hypotheses To obtain a better understanding of Zeta and its variants, we first visually explore the relation between segment size and the resulting zeta scores. We expect both Zeta variants and segment size to have visible consequences in this setting. Secondly, we evaluate the distinctiveness of words selected by different Zeta variants by using the highest ranked words as features in a classification task for distinguishing texts into two previously defined classes. This captures the degree to which the different Zeta variants and parameters identify words distinctive of these two classes. Note that we calculate Zeta scores from the complete set of documents. While this is not valid for a real-world classification task, it allows us to better judge the level of distinctiveness of the selected words. We expect better performance in the classification task with some of the new variants, compared to the classic “Burrows Zeta” (sd0). We also expect extreme segment lengths to significantly impact classification performance. We primarily aim at a methodological contribution here, so we do not attempt include a discussion of our results from a literary perspective (but see Schöch 2018 for such a contribution).   Exploratory approaches to Zeta variants and parameter variation First, we take a closer look at the relationship between overall frequency and Zeta scores as it evolves with increasing segment size.     Distribution of Burrows Zeta (sd0) scores depending on segment size. Each dot is one word, ordered by descending frequency. The x-axis is log-scaled.    Figure 1 shows that when using very short segments, only highly frequent words (such as function words) can get high Zeta scores. With longer segments, words that are somewhat less frequent overall (well-interpretable content words) can also reach high Zeta scores; a desirable effect. Additionally, we explore the influence of segment length and Zeta variant on the relation between segment proportions and zeta scores.      Segment proportions and Zeta scores for two Zeta variants (left: Burrows Zeta; right: log2-Zeta) and three segment sizes (100, 1000, 5000). Each dot is one word, 500 top Zeta words are shown. Colors indicate the words with the 40 highest (green) and lowest (blue) Zeta scores.    Figure 2 shows that with increasing segment size, Zeta scores generally increase because segment proportions increase. It also shows that in Burrows’ Zeta (left), terms with low segment proportions can never gain high Zeta scores. This limitation motivates the log2 and division variants that alleviate this effect: here, words to the bottom and left of the plots can also obtain extreme Zeta scores.    Evaluation of Zeta variants and parameters Evaluating the performance of Zeta variants and different parameters is non-trivial, because it is impossible to define a human-annotated gold standard for distinctive words. Therefore, we use a classification task (with a Linear-SVM classifier and 3-fold cross-validation) for evaluation (the Spanish novels have to be classified by their continent of origin: America and Europe). The baseline of classification performance is F1=0.49 on average across all conditions and has been obtained using the top-80 most frequent words weighted with TF-IDF (see Robertson 2004).     Classification performance depending on Zeta variant, segment size and number of segment-samples. We report mean F1-score over 15x3 folds.    Figure 3 shows that, as expected, most Zeta variants outperform the baseline. Segment size also influences performance: Burrows Zeta (“sd0”) performs particularly poorly with small segments (50, 100) and particularly well with large segments (>10000). Contrary to our expectation, large segment sizes do not generally have a negative impact on performance. The log2-Zeta variant (“sd2”) performs better than Burrows’ Zeta and is more robust with respect to segment size. In addition, we evaluate the parameter sampling size (number of segments randomly sampled for each document). For Burrows’ Zeta (“sd0”), we observe a better classification performance for small samples.   Discussion: Interpretability While improved performance and robustness are welcome, another important characteristic of Burrows’ Zeta should not be forgotten, namely the high interpretability of the most distinctive words it identifies. The question is whether the gain in performance obtained with log2-Zeta comes at the expense of interpretability of the most distinctive words. Currently, we can merely offer some preliminary observations on this issue: First, the interpretability of distinctive words could be operationalized in a first approximation as the proportion of content words (nouns, verbs and adjectives) in the list of the most distinctive words, as opposed to function words and named entities. Second, segment size and Zeta variant both appear to influence the types of words Zeta that determines to be distinctive: for example, very small segment sizes favor highly frequent function words, while very large segment sizes lead to place and person names taking up a considerable space in the word list. Also, some Zeta variants, including log2-Zeta, produce lists of words containing high proportions of place and person names even at intermediate segment lengths, that is wordlists that are less interpretable (see annex). These preliminary observations point to a possible trade-off between performance and interpretability that requires further, systematic investigation.   Conclusions and Future Work Our experiments have allowed us to gain a much more detailed understanding of how Zeta works, mathematically and empirically. Additionally, we have identified at least one Zeta variant (“log2-Zeta”) that selects more distinctive words with regard to our classification task and is more robust against variation in segment length than Burrows Zeta.  As future work, we plan to conduct an investigation into the notion of “interpretability” and its relation to classification performance. Also, we plan to build an interactive visualization for our results to support a dynamic exploration of Zeta variants, key parameters and their influence on classification accuracy and distinctive words obtained. A larger agenda item is the evaluation of a substantial number of measures of distinctiveness, including Zeta, in a common framework.   Annex For reasons of space, the wordlist annex can be found at:  .    ",
       "article_title":" Burrows’ Zeta: Exploring and Evaluating Variants and Parameters  ",
       "authors":[
          {
             "given":"Christof",
             "family":"Schöch",
             "affiliation":[
                {
                   "original_name":"University of Trier, Germany",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Schlör",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Albin",
             "family":"Zehe",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Henning",
             "family":"Gebhard",
             "affiliation":[
                {
                   "original_name":"University of Trier, Germany",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Becker",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Hotho",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "computer science",
          "spanish and spanish american studies",
          "stylistics and stylometry",
          "text analysis",
          "data mining / text mining",
          "English",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The largest collections of art historical images are not found online but are safeguarded by museums and other cultural institutions in photographic libraries. These collections can encompass millions of reproductions of paintings, drawings, engravings and sculptures. The 14 largest institutions hold together an estimated 31 million images (Pharos). Manual digitization and extraction of image metadata undertaken over the years has succeeded in placing less than 100,000 of these items for search online. Given the sheer size of the corpus, it is pressing to devise new ways for the automatic digitization of these art historical archives and the extraction of their descriptive information (metadata which can contain artist names, image titles, and holding collection). This paper focuses on the crucial pre-processing steps that permit the extraction of information directly from scans of a digitized photo collection. Taking the photographic library of the Giorgio Cini Foundation in Venice as a case study, this paper presents a technical pipeline which can be employed in the automatic digitization and information extraction of large collections of art historical images. In particular, it details the automatic extraction and alignment of artist names to known databases, which opens a window into a collection whose contents are unknown. Numbering nearing one million images, the art history library of the Cini Foundation was established in the mid-twentieth century to collect and record the history of Venetian art. The current study examines the corpus of the 330’000+ digitized images.  Image Processing Pipeline  Photo/Cardboard Extraction  The records in the Cini Foundation consist of a photographic reproduction mounted on a cardboard card onto which metadata information is recorded. The initial scan of these records is a 300 dpi picture produced on a scanning table, and includes the digitized cardboard and color balance markers. The first task consists in separating the cardboard backing and the photographic reproduction from the raw scanned image. Despite the apparent simplicity of such a task, it proved challenging on account of the multiple layouts of the metadata information on the cardboard cards, and the variations in the sizes and positions of the attached images. In the end, what proved most effective in the extraction of the image was a Convolutionnal Neural Network (CNN) architecture designed for semantic segmentation (Ronneberger, O. et al 2015). For this, an accurate model was trained on scans which had been annotated in the course of 2 hours. The details oft he approach are part of another study (Ares Oliveira, S. and Seguin, B. 2018).   Figure Left: original scan with the extracted areas highlighted with red and blue rectangles. Right: the prediction mask generated by the neural network.     Text Extraction The second part of the pipeline consists of extracting and reading the metadata. For this task, the open-source Tesseract toolkit and the commercial Google Vision API were tested, with the latter having better performance. The OCR system provided a list of words and their positions, which were then clustered into blocks of text representing the different metadata fields (authorship, title of painting, location etc.). A layout model was used to represent the expected positions of these different fields. This allowed the assignment of each block of text to its corresponding metadata field. A precise analysis of the performance of this step is presented in another publication (Seguin, B. 2018).   Figure Illustration of the OCR process. The extracted words (top-left) are clustered into blocks of metadata (top-right) and then assigned to their corresponding label (bottom).     Automatic Alignment of Artist Names In order to leverage the extracted metadata to get insights into a collection, it is important to link them to a knowledge database. This can allow, for example, city names to be placed geographically on a map. Here, we focus on aligning artist names with a knowledge database: the Union List of Artist Names (ULAN), managed by the Getty. This opens up a wealth of new information for the contextual understanding of the artwork’s creation. The alignment process is depicted on Figure 3, it is a complex two-pass process that integrates automatic matching with collection specific knowledge in an efficient manner. The first pass tries to perform an exact match with a large name dictionary. For the second pass, a list of candidates are generated from the correctly matched elements of the first pass, and approximate matching is used to correct small OCR errors.   Figure Alignment process. The parts in color correspond to collection-specific knowledge.  There are three challenges that needed to be tackled during this alignment process :   Names variation : one major issue that arises is that a given artist may be called by different names, depending on regional variations and pseudonyms. Many variations are recorded in ULAN (i.e “ Tiepolo Gianbattista” and “ Tiepolo Giovanni Battista” both corresponding to the same artist), although some have to be added to the name dictionary. Furthermore, the naming conventions for elements whose dating or provenance is known but not authorship, which may be specific to a collection, can be added to the dictionary.   Implicit knowledge : one related challenge is linked with the pragmatics of the annotation process. Understanding that if one archivist writes “ Leonardo” on a file, he or she is referring to  Leonardo da Vinci implies modeling a series of implicit assumptions which are changing depending on the evolution of local cataloging practices and that of the art historical field itself. In our case, we tackle this by disambiguating unclear names. For instance “ Tiziano Vecellio” could technically refer to the well-known “ Tiziano”, or his relative “ Tizianello”, but the first is much more prominent than the second.   Compositional structure : the last challenge is linked with the practice of archivists to describe particular unknown authors using specific syntactic process like (“ Tiziano (bottega di-)”, “ Tintoretto (Maestro di)” or “ Michelangelo (copia da-)”), referring to workshop productions or copies. Understanding and modeling this “grammar” permits to generate, in a compositional manner, potential matching strings to be considered when looking for possible alignments. Such strings do not only give a link to an artist but also qualify relationships (how strongly an artist was involved in the creation process of a painting, whether the piece is an original or a copy, etc.).     Results   Figure Left : Distribution of number of artworks assigned for each artist. Right : Proportion of images assigned with respect to the most common artists. The 200 most represented artists represent 43% of the collection.  Of the 330,078 scans composing the corpus of study, 14.6% had an empty author field, mostly because the photographs represented architecture or aerial city views. Out of the remaining 85.4% with an authorship field, 73.8% were automatically matched to an author (61.6% after the first pass), with an additional 1.4% representing ambiguous situations which could be resolved. This accounts for 208'510 elements automatically matched. At the end of pre-processing, the potential author names can be divided into three categories :  (A) Author names which have been matched with a reference record of another database (B) Author names which may have been matched if the algorithm were to be improved (e.g. in terms of author name variation or possible compositional structure) (C) Authors undocumented in standard databases of artists.  Figure 5 shows the global matching results for category A. The geographical composition of aligned authors is dominated by Venetian artists (Tiepolo, Tintoretto, Palladio, Tiziano, Veronese, etc.) showing the rationale behind the creation of the collection. In terms of chronology, the collection is focused on the sixteenth century, as shown by the distribution of year of death of the aligned artists. This is in line with the period referred to as the “Venetian Golden Age”. Figure 4 shows the very uneven representation of artists, with only 346 having more than 100 images, representing more than 50% of the whole collection.   Figure Spatial (right) and temporal (left) distribution of the 1’746 artists with at least 10 images assigned.   Category B is predominant in the elements that were not matched. Apart from OCR errors, the most typical unmatched string corresponds to collective works in which several authors are named. For instance, the string “ Bassano Jacopo e Francesco” (his son) corresponds to 134 records. Adding additional parsing capabilities to the system could enable the resolution of such cases in the future.  Names in category C, which were not matched with ULAN, are in fact not a product of misalignment but represent new discoveries in the collection. In the present study, a number of artists who do not feature in ULAN were uncovered in the Cini archive. These include, Augusto Caratti, a minor artist from nineteenth-century Padua, who is represented by 65 images in the Cini collection, and Natale Melchiori an early eighteenth-century painter from Castelfranco, Veneto, represented by 39 images. Another artist who does not feature in the ULAN database but nevertheless has a significant presence in the Cini archive with 106 drawing, is Antonio Contestabile, an eighteenth-century draftsman from Piacenza.   Conclusion These early results show the potential of the systematic processing of a large number of art historical records, leading to the mapping of unknown collections, and to new discoveries. It also highlights for the first time the challenges inherent in the process. Such challenges, it is important to note, are not purely technical but rather linked with the complexity of modeling local archiving traditions and the historical practices of art history.  ",
       "article_title":"Extracting and Aligning Artist Names in Digitized Art Historical Archives",
       "authors":[
          {
             "given":"Benoit",
             "family":"Seguin",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lia",
             "family":"Costiner",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Isabella",
             "family":"di Lenardo",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "computer science",
          "art and art history",
          "linking and annotation",
          "resource creation",
          "library & information science",
          "image processing",
          "English",
          "and discovery",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" “All of us, even when we think we have noted every tiny detail, resort to set pieces which have already been staged often enough by others.” — W. G. Sebald,  Austerlitz  Periods are the set pieces of history, and their staging is a strategy for making change over time meaningful and understandable. Periodization structures not only histories themselves, but also the ways those histories are organized in libraries, the ways teachers of history organize syllabi and textbooks, and the ways historians organize themselves in academic institutions. Like the histories they structure, periodizations are also imposed on the conquered by their conquerors. Periodization itself is a legacy of colonialism, grounded in a linear ontology of time that has forced aside indigenous understandings of temporality. Periodization is also a perennial topic for reflection in the humanities, as scholars cast a critical eye on the categories that organize their work. But like a linear conception of time, periodization is both easily critiqued and difficult to relinquish. Critique is not the only response to periodization: several scholars have suggested alternative approaches to conceptualizing historical temporality. Wishart (2004: 313), responding to histories of the Plains Indians that “fold their ethnographies into periods that are derived from American, not indigenous, realities,” suggests as alternatives periodizations grounded in economic cycles or patterns of population change. Dimock (2001: 758) proposes abandoning the “decades and centuries” scale of conventional literary periods in favor of a “deep time” of “extended and nonstandardized duration.” Others explicitly consider the role of the digital humanities in realizing alternatives to periodization. Brooks (2012) claims that “the digital world is moving in concert with Indigenous literary traditions” (312) and foresees that, as scholars embrace digital media, “the measuring tape of time will become decreasingly useful and, perhaps, increasingly (self)destructive” (309). Underwood (2013) argues that the penchant for periodization among literary scholars stems not from a desire to neatly sort history into standardized bins, but from a disciplinary identity rooted in theories of discontinuity and rupture. He sees the digital humanities as challenging that identity by providing tools and vocabulary for describing gradual, continuous change. Besides critique and the imagining of alternatives, a third response to periodization is to document it. This is the motivation behind PeriodO ( http://perio.do), a gazetteer of scholarly definitions of historical periods. Gazetteers are typically directories of place names, but understood broadly, any reference tool documenting named concepts that can be spatiotemporally located is a gazetteer (Shaw, 2016: 58). The PeriodO gazetteer documents specific published assertions about periods, including their names, their extent in space and time, and when, where, and by whom these assertions were made. Unlike gazetteers focused primarily on standardization, PeriodO is a  deep gazetteer which attempts to document a range of perspectives taken and judgments made (Shaw, 2016: 58–60). Hence there is not a single “Bronze Age” in PeriodO, but hundreds (Figure 1).   Figure 1. Visualizing the temporal extents of hundreds of different “Bronze Age” concepts  PeriodO is published as “linked data,” providing for the documented concepts stable identifiers in the form of URLs, which can then be resolved into sets of “triples”—subject-predicate-object structures representing assertions about those concepts. As of November 2017, there are over five thousand periods documented in PeriodO, from more than one hundred sources in over twenty languages. For each of these periods, the assertions documented include structured bibliographic data describing the source, temporal extent as delimited by up to four points in time, and spatial coverage via links to places in other linked data gazetteers. PeriodO has been designed to be collaboratively edited by a community of scholars, regardless of whether they have any knowledge of or experience with linked data technologies (Shaw et al., 2015). Anyone with a free ORCID personal identifier (Haak et al., 2005) can immediately submit proposed additions to the gazetteer, without any additional barriers to contribution. Through its public domain identification and documentation of period concepts, PeriodO provides a means by which curators of periodized data can resolve ambiguous period terms and bridge datasets employing different interpretations of the same period term. This is much like the service provided by the Pleiades gazetteer of ancient places (Elliott and Gillies, 2008). Pleiades uses PeriodO URLs to identify its period concepts, just as PeriodO uses URLs from place gazetteers like Pleiades to identify its place concepts. Pleiades plays an integral role in an increasingly fecund ecology of linked scholarly data projects, many of them incubated by the community-driven Pelagios initiative (Isaksen et al., 2014). The Peripleo spatiotemporal search and visualization tool, built to search over and visualize data produced by the projects participating in this initiative, indexes PeriodO URLs and can use PeriodO data to translate between period terms and spatiotemporal locations (Simon et al. 2016). PeriodO is also used by the ARIADNE archaeological research data infrastructure to document the more than 600 period concepts it employs (Niccolucci and Richards, 2013). The PeriodO team would like to engage those gathered at the 2018 Digital Humanities conference in Mexico City for several reasons. First, we would like to present the results of four years’ iterative development of the PeriodO dataset and tools, funded by consecutive grants from the National Endowment for the Humanities and Institute of Museum and Library Services. This will include the outcomes of two workshops focused on periodization and spatiotemporal knowledge organization, held in August 2016 and December 2017. We hope that an overview of PeriodO’s design and implementation will be of interest not only to those working with periodized data, but anyone interested in the architecture of scholarly infrastructure. Second, we hope to inspire others to use the PeriodO data for purposed other than data curation. Visualizations of PeriodO data could be used to help students understand the nature and politics of periodization, or to make arguments about the history of historiography. Advocates of alternatives to periodization may find PeriodO's documentation useful in the spirit of “know your enemy.” A large collection of multilingual descriptions of temporal extent and their corresponding interpretations as numerical ranges may be useful for natural language processing. There are undoubtedly other possibilities, and if the data in its current state is not adequate for exploring them, we’d like to figure out how we can make it so. Most importantly, we hope to catalyze collaborations with a broader range of scholars interested in documenting periodizations. The majority of period concepts documented in PeriodO originated in archaeology, art history, and the authority files of libraries and museums. We would like to have far more documentation of periodizations from areas such as literary studies, social history, and intellectual history—areas with far less consensus on periodization than archaeology and art history. And while PeriodO documents period concepts associated with places around the world, the majority of its scholarly sources are still American and European—another defect we'd like to correct. A primary goal of PeriodO is to enable contrast of and comparison between different interpretations of the past, and this requires broad collaboration. As broad as that collaboration may become, PeriodO will always be limited by the framework of linear time that it employs as a means of making temporal extents comparable. Still, there is no reason that PeriodO could not connect with other projects exploring alternative temporalities, in the vein of Drucker’s (2009) experiments with relational temporal modeling, Brooks’ “spiral” time, or even Underwood’s probabilistic, gradient time. Though it may not be possible to directly compare the temporal entities or processes registered by these various alternative conceptualizations, they might still be interlinked and hence more readily brought into dialogue with one another. We hope that our colleagues at DH 2018 will have some ideas about how that could happen, or insights into why it might not. ",
       "article_title":"A Deep Gazetteer of Time Periods",
       "authors":[
          {
             "given":"Ryan",
             "family":"Shaw",
             "affiliation":[
                {
                   "original_name":"University of North Carolina at Chapel Hill, United States of America",
                   "normalized_name":"University of North Carolina at Chapel Hill",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0130frc33",
                      "GRID":"grid.10698.36"
                   }
                }
             ]
          },
          {
             "given":"Adam",
             "family":"Rabinowitz",
             "affiliation":[
                {
                   "original_name":"University of Texas at Austin, United States of America",
                   "normalized_name":"The University of Texas at Austin",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj54h04",
                      "GRID":"grid.89336.37"
                   }
                }
             ]
          },
          {
             "given":"Patrick",
             "family":"Golden",
             "affiliation":[
                {
                   "original_name":"University of North Carolina at Chapel Hill, United States of America",
                   "normalized_name":"University of North Carolina at Chapel Hill",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0130frc33",
                      "GRID":"grid.10698.36"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "information architecture and modeling",
          "cultural and/or institutional infrastructure",
          "modeling",
          "historical studies",
          "linking and annotation",
          "library & information science",
          "visualization",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Even as the scholarly communications field pursues the opportunities presented by digital technology, its routine operations remain anchored in print-centric regimens. For those working to evolve scholarly communications in the Internet age, particularly as it bears upon long-form scholarship, there is compelling need to productively disrupt and reconfigure the workflows and work cultures that have naturalized around the production of printed products. It is precisely this complex, systemic issue that Greenhouse Studios | Scholarly Communications Design at  the University of Connecticut (UConn) addresses with its design-based, collaboration-first model of scholarly production.     With funding from the Andrew W. Mellon Foundation, the Digital Media & Design Department at UConn, the University Library and UConn Humanities Institute launched Greenhouse Studios in 2017. As a transdisciplinary collective, Greenhouse Studios employs design-thinking methodology to long-form digital scholarship.  With its first two cohorts of collaborative projects, the Studios implemented an inquiry-driven approach that addresses the divided workflows and counter-productive labor arrangements that have complicated scholarly communications in the digital age.  While the introduction of digital tools across the “information chain” model of scholarly communications has altered activities from research and writing through to preservation and reading, it has not reconfigured the larger workflow in which the various actors remain interlinked but largely independent save for key transactional, or “handoff,” moments (CNI, 2016).  Simply put, the “information chain” of scholarship begins with a knowledge creator, passes through to a publisher and culminates with accessibility secured by libraries and use by readers (Owen, 2002: 275-88) . This transactional model has contributed to the persistence of an increasingly detrimental division of activities into those of the knowledge creation, or “domain,” side and those of the production, or “build,” side (Sosin, 2016).   By disrupting and reconfiguring divided workflows that have naturalized around the production of printed products, Greenhouse Studios brings together project teams on the “domain” side versus the “build” side. Each year, a new theme or problematic frames the work of the project teams, and diverse groups of collaborators are brought together, including designers, developers, editors, faculty and librarians. Starting with a problematic or issue rather than a faculty interest flattens counterproductive hierarchies and bringing in partners early in the process lends itself to the collaboration-first approach of the creation and expression of knowledge. Digital formats for the projects are not presupposed, as the format—digital or analog—that best represents the long-form scholarly work is taken under consideration. The first cohort of Greenhouse Studios teams developed projects in diverse formats including a documentary film, a virtual reality environment and an electronic decision-making novel. Guiding the work of the teams, the Greenhouse Studios design process model provides a workflow for each project  through five major sprints or phases. The design process model was developed through a series of exercises to elicit individual mental models of the scholarly design process from the perspective of a project manager, scholar, designer, repository manager, digital scholarship librarian, developmental editor and MFA student/research assistant. Comparisons of the mental models highlighted similar project phases for each participant, although the points of intersection were often differently identified. In looking at these points of overlap,  neutral descriptors for shared activities were adopted, both for mutual intelligibility and to eliminate the kinds of value judgments that domain-specific terms may inscribe.   Figure 1. Greenhouse Studios Design Process Model   Being mindful of the Greenhouse Studios goals for workflow and work culture, the design process model adopts elements from the long tradition of design thinking as applicable across diverse fields. Design thinking as taught, practiced and disseminated by its most well-known and long-standing academic and corporate proponents, Stanford University’s Hasso Plattner Institute of Design (aka the  d.school ) and the design firm IDEO, traces its roots to the 1960s’ merger of a Stanford program that joined arts and mechanical engineering (Miller, 2015). Today, it has extended to endeavors as far afield as finance, films, museum exhibition, journalistic communications, education, and critical making in the digital humanities. Across various incarnations, design thinking processes typically involve a series of iterative discovery and development cycles, each characterized by a subset of activities designed to facilitate that cycle’s goal.   Work through the Greenhouse Studios design process model begins with an inquiry or prompt and brings together team members in response to a central problematic. During the catalyst phase of  Assemble, team members gather, meet fellow participants and review the guidelines for project teams. The relevant human talents and other resources are defined during the first full sprint, or the  Understand phase, which produces a  project brief framing the project’s aims and audiences. During the subsequent  Identify phase, relevant sources of knowledge and inspiration are researched and synthesized. The resulting  creative brief outlines the media formats of the project, as well as the formal peer review and assessment plans for the work. Iterative prototyping and refining of a project takes place during the following  Build phase, producing a  media manuscript, which could be a website, book manuscript, documentary film, exhibition or other format. During the  Review phase, the project is revised, edited and submitted for peer review. The final phase is the  Release or launch of the project, as well as the longer-term work of dissemination, assessment and preservation. Adjacent to this phase, there may also be  other publications produced by individual project team members.  This design process model guides each of the Greenhouse Studios inquiry-driven, collaboration-first projects. The implementation of the process began before the launch of the first cohort of projects, and the model has undergone subsequent iterations  across several development cycles. The team participants and an inquiry prompt act as catalysts for the workflow, which places collaboration at the center of the process, rather than an individual scholar’s research goals. The emphasis on the “collaboration-first” nature of the process allows participants to collectively imagine scholarly projects from the outset and serves as a corrective to divided workflows, even digital-centric ones, where collaborators are only brought on board for the final implementation of projects.  ",
       "article_title":"A Design Process Model for Inquiry-driven, Collaboration-first Scholarly Communications",
       "authors":[
          {
             "given":"Sara B.",
             "family":"Sikes",
             "affiliation":[
                {
                   "original_name":"University of Connecticut, United States of America",
                   "normalized_name":"University of Connecticut",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02der9h97",
                      "GRID":"grid.63054.34"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-03",
       "keywords":[
          "scholarly editing",
          "project design",
          "organization",
          "cultural and/or institutional infrastructure",
          "management",
          "library & information science",
          "English",
          "design",
          "interdisciplinary & community collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The semantics and logic of transcription have received attention from a number of digital humanists, some starting from the practice of digital editions [Pierazzo 2011, 2015], some from a consideration of markup languages [Robinson 1994, Huitfeldt 1995], some from a critical examination of the foundations of digital humanities [Caton 2009, 2013a, 2013b, 2014]. Attempts to describe how transcripts provide information about their exemplars [Huitfeldt/Sperberg-McQueen 2014, Sperberg-McQueen et al. 2017] have focused on individual transcripts, not multiple divergent transcripts of the same exemplar. Here we describe ways in which transcripts of the same exemplar can differ and we sketch a model of transcription which accounts for such differences.   Examples Our catalog of ways in which transcripts differ and disagree takes the form of examples, many illustrating exceptions to the general rule that a transcript reflects “the exemplar, the whole exemplar, and nothing but the exemplar” and that competent transcribers will agree on the reading of the exemplar [Sperberg-McQueen et al. 2014]. For brevity, examples often consider only single words; discussions refer to the first of several transcripts as A, the second as B, an arbitrary transcript as T, and the exemplar as E. Some transcripts were constructed for this paper.  Transcripts which differ and disagree  Example LW: what type does this token instantiate?    E is a word from Ludwig Wittgenstein's notebook 117 (p. 269), written in a simple substitution code [Wittgenstein, n.d.]. A and B, ignorant of the cipher, transcribe it as “munonyqi” and “wunouyqi”, respectively. C, better informed, has “muuvnyzi” (“offenbar”).     The transcripts reflect contradictory readings of the token in E; at most one can be correct.  Here all transcripts agree on which marks in E are tokens, but disagree on the types they instantiate. We infer that a transcript's mapping from tokens in E to types is a salient feature for modeling.   Example MCN: which marks are tokens? E is a tombstone from northwestern Britain [Collingwood/Wright 1965-1990, no. 932].      A [Lafleur 2010 p. 28f.] reads the mark between some word pairs as a punctum: DIS MANIB · M · COCCEI NONNI · ANNOR · VI HIC SITVS EST B is similar except for the last line: ‘HIC · SITVS · EST’. Here A and B do not disagree over the reading of any tokens; they disagree on what marks in E are tokens. A formal account must distinguish the identification of tokens from the mapping of tokens to types.   Example TE: what is the structure of this text? At the eastern end of Magdeburg cathedral lies the Tumba Edithae (tomb of Edith), with an inscription part of which is shown here.      A [Neugebauer/Brandl 2012] begins reading on the south:    B begins reading on the north with “INPVLSV” but otherwise resembles A.  A and B agree in their readings of each individual character and word and also in identifying which marks in E are writing; they differ only in the higher-level structure(s) compounded from words and characters. A model of transcription must include such higher-level structural organization as a substantive part of transcription; so similarly [Huitfeldt et al. 2010].    Transcripts which differ without disagreeing That some differences between transcripts do not signal disagreements about E goes (almost) without saying. A and B can differ in pagination and running heads without disagreeing on how to read E: page furniture is normally an exception to the general rule that everything in T transcribes something in E.  Example JA: literal transcription and marked corrections      E is a word from a letter of Jane Addams [Hajo et al. 2015-].    A writes “altho”, B “altho[ ugh]” (bracketed italics mark editorial additions), and C “[although]” (brackets mark editorial interventions). A and B thus differ in content but agree that E has “altho”. B and C provide the same normalized spelling but provide different (albeit compatible) information about E.  B and C assign special meaning to brackets and bracketed material: unlike other characters, they transcribe nothing in E. An account of transcription must specify which tokens in the transcript are to be interpreted as transcribing tokens in E and which not.   Example SJ: long and short s      E is one word from a sonnet by Sor Juana Inés de la Cruz [Sor Juana 1700 p. 163]. A (a web site presenting Sor Juana's work) writes “vista”, B “viſta”. A and B differ but do not disagree. Both identify the third character of this word as a lower-case S; B further specifies a long S. If we take A to be ambiguous (the S could be long or short), then A subsumes B: B provides additional but not contradictory information. In E, however, long and short S are allographs in complementary distribution; typographic context determines which appears. In vi_ta, S will always be long not short. So in reality A provides the same information as B, not less. Many differences without disagreement arise where one transcript preserves allographic differences and the other preserves graphemes. Arguments on the topic involve no disagreements about E, only about the choice of type system. A model should clarify the role of type systems in transcription.   Example TJ: word-level and character-level fidelity    E is from Thomas Jefferson's draft of the U.S. Declaration of Independence.       A: the laws of nature & of nature's god B: the laws of nature and of nature's God A [Boyd 1950- p. 1:423] preserves and reinstantiates the type of each character, while B [Harrison 1993 p. 39] preserves and reinstantiates the type of each word but not each character. (That is, it normalizes the spelling of words.) Here the type systems of A and B diverge even more sharply than above. A formal account must be able to describe type-system differences of this kind.   Example FK: typographic rendering of inscription details      E is the words “para siempre” in a letter from Frida Kahlo to Diego Rivera [Kahlo]. A and B differ only in rendering the underscoring in E as italics or underscoring. Typographic features of T often convey information about E, but different transcribers use different conventions. A formalization must account for such conventions.   Example FE: completeness and incompleteness      E is a grave marker (Naples, fourth/fifth century CE) now in the Jewish Museum, New York (JM3-50). A:    B:     B transcribes all of E, A [Lafleur 2010 p. 144]] only the Latin text. Any model must describe how we know which material in E is transcribed and which (if any) is not.    JLM: Transcripts which disagree without differing It is hard to find plausible examples of this class of phenomena. But an imaginary example may illustrate it. If A uses italics to mark editorial insertions, and B to represent underlining in E, then John  loves Mary.  will mean different and contradictory things in A and B. Differences in typographic conventions and type system can lead to conflicting interpretations of T. A model must describe how such conflicts arise.     Formal model Space constraints limit us to a sketch. We assume the concepts  type,  token, and  document. Types and tokens are not limited to graphemes or words but include larger structures. The document itself is typically a compound token, and its text a compound type.  A set of mutually exclusive types we call a type inventory. Tokens instantiate exactly one type in an inventory: a letter is an I or a J, but not both. Transcriptions commonly involve not one type inventory but several. (“I” is both a letter and a word.) A set of type inventories is a type system. A reading of a token k with respect to a type inventory I maps k to a type p in I; we write (k, I, p) for such a reading. A reading of a document D identifies a set K of tokens in D and maps them to types. We write R = (D, K, P, M), where P is a type system and M a set of triples (k, I, p) where k ∈ K, I ∈ P, p ∈ I. Every k in K maps to at least one type; none maps to two types in the same inventory. Examples MCN and TE illustrate differences in K, examples SJ and TJ differences in P, example LW differences in M. Transcription policies determine which tokens in E are transcribed (normal) and which not (special); similarly which tokens in T transcribe E (normal) and which do not (special). They also constrain the type system by distinguishing some types and equating some with each other. A transcription policy is thus a triple (SE, ST, Q), where SE and SE are predicates true of all and only the special tokens in E and T respectively, and Q is a set of type equivalences. Examples FE and JLM illustrate differences in SE, example JA a difference in ST, and example FK a difference in Q. From a reading of T we can reconstruct a reading of E based on an assumed transcription policy; this allows readers of T to have information about E without examining E directly.  ",
       "article_title":" Interpreting Difference among Transcripts  ",
       "authors":[
          {
             "given":"Michael",
             "family":"Sperberg-McQueen",
             "affiliation":[
                {
                   "original_name":"Black Mesa Technologies LLC, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Claus",
             "family":"Huitfeldt",
             "affiliation":[
                {
                   "original_name":"University of Bergen",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "philology",
          "scholarly editing",
          "digitisation - theory and practice",
          "historical studies",
          "English",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Digital approaches to palaeography – the study of historical or ancient handwriting – have received significant attention in recent years. Projects like ORIFLAMMS (Stutzmann, 2016) and DigiPal (Brookes, 2015) have focussed on this, as well as projects aimed more at the book or written object in general such as HisDoc and Diva DIA, work on the Cairo Genizah (e.g. Wolf et al., 2011), work at the Centre for the Study of Manuscript Cultures (CSMC) in Hamburg; and many more. Although taking different approaches and addressing different aspects of palaeography, these projects have all made significant and important advances. However, relatively few have addressed explicit semantic modelling of handwriting itself. One such model was developed for the DigiPal project (Stokes, 2012) and has since been implemented in open-access and freely-available software called Archetype (2017). The model was developed initially for the Latin alphabet, but it has proven to be much more versatile than anticipated, with application to Hebrew and decoration (Brookes et al., 2015), bilingual Greek-Latin inscriptions, and experiments with Chinese, Cuneiform, Mayan, and others (see Figure 1 and Figure 2).     Figure 1: Archetype applied to Chinese script, showing search results for characters (graphs) containing the component 可      Figure 2: Demonstration of Archetype applied to Mayan hieroglyphics  Although applied successfully to different scripts, the DigiPal model (like most palaeographical method) assumes a homogeneous corpus comprising samples of the same alphabet and writing-style. Although convenient, this homogeneity is in fact very limiting, as throughout history many, perhaps even most, societies employed a range of different writing styles and systems. In Western Europe, for example, the best-known example is probably the Gothic script-system, comprising  Textura,  cursiva and so on; but similar patterns can be observed elsewhere in Hebrew, Arabic, Chinese, Tibetan and beyond. Furthermore, different alphabets or writing systems were often used together. Egyptian scribes use hieroglyphic, Hieratic and Demotic contemporaneously; Greek and Latin inscriptions are found across much of the Roman Empire, sometimes with third languages and alphabets; the Dunhuang materials contain a wide range of languages and scripts including Chinese, Tibetan, Sanskrit, Arabic and Sogdian; materials in four writing-systems survive from medieval Sicily; and so on. All of this suggests very strongly that people often – perhaps usually – could and did write in different alphabets or writing-systems. Identifying such cases would give us much important information about areas like education (how many Arab scribes also learned Hebrew?), cultural influence (were Sogdian annotations written at the same time and place as the main text in Chinese?) and so on. However, this requires an approach that allows for comparison across different scripts, discussion of which has really only just begun in both \"digital\" and \"non-digital\" palaeography (Stokes, 2017).  In principle, the DigiPal model provides such a framework. It specifies that characters are made up of components, defined as structural elements which recur across different letters (such as the ascenders in  b,  h,  l and so on: Stokes, 2012). If one can map between components of different writing-systems then it is relatively easy to search for graphs (i.e. instances of letters written on the page) which share those components, and this allows for comparison. A proof-of-concept is illustrated in Figure 3, where six instances of the Archetype system are searched simultaneously via the software’s web API; a further example is given by Stokes (2017).       Figure 3: Screenshot from proof-of-concept cross search for example letters (graphs) with ascenders  However, multigraphic contexts require revising some basic assumptions of the model. For instance, DigiPal (and much palaeographical discussion) assumes that \"etically\" same is also \"emically\" same: that things that look the same mean the same.  H and  H look identical and therefore are normally assumed to represent the same letter: this is normally valid and indeed essential for communication in a monographic context. However, it may not hold in general: if we write  HELLO and I HΣOΥΣ then it becomes clear that the first is the Latin capital H (Unicode U+0048) and the second a Greek capital Eta (Unicode U+0397: cf. Bugarski, 1993). In context one can categorise these as separate characters, but a user searching the database for palaeographically comparable forms would presumably want to find both. Similar are apparently unambiguous characters used for different functions, perhaps deliberately to echo a different writing system. For example, in \"pseudo-fonts\" like GRΣΣK, the linguistic context shows that the two central letters function as the English grapheme E, but they are represented by the Greek capital Sigma. Comparable examples are widespread, such as the use of Greek letters in the Latin text of the ninth-century Lindisfarne Gospels illustrated in Figure 4. In a monographic context this could be addressed as grapheme E with allograph \"sigmoid\" or something similar, but in a multigraphic context a palaeographer would presumably want to be able to find examples of both capital Sigma and \"sigmoid\" E with a single search and without necessarily anticipating the coincidence of forms in advance.      Figure 4: Detail from the Lindisfarne Gospels (London, British Library, Cotton Nero D.iv), showing use of Greek letters for writing Latin  In order to address this, some changes to the DigiPal model are proposed. The first is to change the central hierarchy of Character-Allograph-Idiograph-Graph presented by Stokes (2012), separating the linguistic/emic from the graphic/etic in a many-to-many relationship as shown in Figure 5 and Figure 6. This allows users to search by form, component or linguistic function.   Figure 5: Revised (extract of) the DigiPal model of script  Figure 6: Example of the model applied to the form Φ used for Latin  F as shown in Figure 4   A further more practical change is to add sub-components. The use of components works well for alphabets and abugidas but much less so for more ideographic or hieroglyphic writing systems. In these cases, characters are made up of components which can themselves be further characters which contain further components, and so on. With sub-components, a component may be a discrete entity in its own right, or it may be made up of a set of further components. One might then describe the Korean glyph 곥 as a character having ㅈ as a component which has フ as a sub-component; this would allow retrieval of all instances of any graphs (character blocks) containing ㅈ, or idiographs containing フ, and so on (cf. Stokes, 2014). The same approach can be used for writing systems with subscript letters or character stacks, such as Myanmar or Tibetan (e.g. the second element in བསྒྲུབས་ which comprises four distinct characters: Flynn, 2015), or even ligatures and conjoined letters such as ﬅ. This then allows searching for more complex components that reoccur across writing systems, such as the Korean sub-component ㅈ (U+110C) which also appears in Japanese  katakana as a distinct character (ス, U+30B9) and as a sub-component in numerous Chinese ideographs.   One limitation of this model is the lack of linguistic context. DigiPal treated characters as distinct entities with no direct relationship between each other, but in practice letters normally appear in a broader linguistic context, namely the text, and this becomes essential in a multigraphic environment. Archetype goes some way towards addressing this, as it allows for including the text on an image, and the text itself can be marked up in XML and then linked in turn to sections of the image (Figure 7 and Figure 8). From this it is relatively trivial to detect the section of text in which a given graph is found, and if the XML markup specifies the language then this would allow one to find (for example) occurrences of a given grapheme or allograph within a specific linguistic context without needing to specify the language of each individual graph.     Figure 7: Screenshot of Archetype used to provide Hebrew transcription and English translation linked to manuscript image, courtesy of Stewart Brookes      Figure 8: Screenshot of Archetype implemented for the Models of Authority project, showing text and image isolated by XML markup (here for  salutatio clauses in medieval charters)  These extensions and changes to the model are not sufficient in themselves to fully address the challenges raised here. For instance, they still assume that different scripts can be reduced to common \"atomic\" units but it is not clear that this always holds, particularly for scripts that have entirely different genealogies such as Chinese and Sogdian, or when different writing implements and different directionality of script is involved (but see Stirnemann and Oszlowy-Schlanger, 2012). It also assumes that researchers can agree on what these \"atoms\" might be, and in principle link between them using the Semantic Web or similar. In some cases these units are evident, such as ascenders or descenders in many scripts, or the radical in Chinese and so on, but as Petrucci in particular has pointed out different scholarly viewpoints will necessarily produce different descriptions, and each of these different views is potentially valid and important (2001: 70–1), but it is not evident that they can necessarily be compared. This problem that extends well beyond the present discussion to encompass data interchange in general, though in practice it may be that if different scholars have such different approaches then perhaps uniting them is not meaningful, and an \"ecosystem\" of alternative viewpoints may be more appropriate. Nevertheless, work to date suggests that the approach described here can provide a useful entry into the problems of multigraphism, particularly when combined with further refinements such as the distinction between components of allographs as envisaged by the scribe \"in mind’s eye\" and traces of graphs actually executed on the page, as well as the ability to compare \"essential elements\" like components as well as \"elements of style\" in DigiPal’s Features (Stokes, 2017 and Parkes, 2008). By building further in this direction, and most likely also adding machine vision and other approaches to searching, it seems likely that good further progress can be made. ",
       "article_title":"Modelling Multigraphism: The Digital Representation of Multiple Scripts and Alphabets",
       "authors":[
          {
             "given":"Peter Anthony",
             "family":"Stokes",
             "affiliation":[
                {
                   "original_name":"École Pratique des Hautes Études – Université PSL",
                   "normalized_name":"École Pratique des Hautes Études",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/046b3cj80",
                      "GRID":"grid.424469.9"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "information architecture and modeling",
          "linking and annotation",
          "historical studies",
          "English",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Traditional full-text digital libraries, including those in the field of pre-modern Chinese, have typically followed top-down, centralized, and static models of content creation and curation. In this type of model, written materials are scanned, transcribed by manual effort and/or Optical Character Recognition (OCR), then corrected manually, reviewed, annotated, and finally imported into a system in their final, usable form. This is a natural and well-grounded strategy for design and implementation of such systems, with strong roots in traditional academic publishing models, and offering greatly reduced technical complexity over alternative approaches. This strategy, however, is unable to adequately meet the challenges of increasingly large-scale digitization and the resulting rapid growth in potential corpus size as ever larger volumes of historical materials are digitized by libraries around the world. The Chinese Text Project (https://ctext.org) is a full-text digital library of pre-modern Chinese written materials which implements an alternative model for creation and curation of full-text materials, adapting methodologies from crowdsourcing projects such as Wikipedia and Distributed Proofreaders (Newby and Franks 2003) while also integrating them with full-text database functionality. In contrast to the traditional linear approach, in which all stages of processing including correction and review must be completed before transcribed material is ingested into a database system, this approach works by immediately ingesting unreviewed materials into a publicly available, managed system, within which these materials can be navigated and used, as well as improved through an ongoing collaborative correction and annotation process. From a user perspective, this has the consequence that the utility of the system does not rest upon prior expert review of materials, but instead derives from provision to individual users of the ability to interact directly and effectively with primary source materials and verify accuracy of transcription and annotation for themselves. Combined with specialized Optical Character Recognition techniques leveraging features common to pre-modern Chinese written works (Sturgeon 2017a), this has enabled the creation of a scalable system providing access to a long tail of historical works which would otherwise not be available in transcribed form. The system is highly scalable and currently contains over 25 million pages of primary source material while being used by over 25,000 users around the world every day.   Creating transcriptions The most fundamental type of material contained in the Chinese Text Project consists of digital facsimiles of pre-modern published works. These are typically ingested in bulk through collaboration with university libraries which have created high quality digital images of works in their collections. After ingestion, the next step in making these materials more useful to users is creation of approximate transcriptions from page images using OCR. Producing accurate OCR results for historical materials is challenging due to a number of issues, including variation in handwriting and printing styles, varying degrees of contrast between text and paper, bleed-through from reverse sheets, complex and unusual layouts, and physical, water or insect damage to the materials themselves prior to digitization. In addition to these challenges which are common to OCR of historical documents generally, OCR for premodern Chinese works faces additional difficulties in extracting training data due to the large number of distinct character types in the Chinese language. Most OCR techniques apply machine learning to infer from an image of a character which character type it is that the image represents, and these techniques require comprehensive training data in the form of clear and correctly labeled images in the same writing style for every possible character. This is challenging for Chinese due to the large number of character types needed for useful OCR (on the order of 5000); unlike historical OCR of writing systems with much smaller character sets, it is not feasible to simply create this data manually. Instead, training data is extracted through an automated procedure (Sturgeon 2017a) which leverages knowledge about existing transcriptions of other texts to assemble clean labeled character images extracted from historical works for every character to be recognized (Figure 1). Together with image processing and language modeling tailored to pre-modern Chinese, this significantly reduces the error rate in comparison with off-the-shelf OCR software.    Figure 1. OCR training data is extracted automatically from handwritten and block-printed primary source texts.   Navigating texts and page images Once transcriptions of page images have been created, they are directly imported into the public database system. The system represents textual transcriptions as sequences of XML fragments, in which markup is used to express both the relationship between transcribed text and the page image to which it corresponds, as well as the logical structure of the document as a whole. This facilitates two distinct methods of interacting with the transcribed material: firstly, as a single document consisting of the transcribed contents of each page concatenated in sequence to give readable plain-text with logical structure (divisions into chapters, sections, and paragraphs); secondly, as a sequence of page-wise transcriptions, in which a direct visual comparison can be made between the transcription and the image from which it is derived (Figure 2). In both cases, an important contribution of the transcription is that it enables full-text search; the primary utility of the page-wise view is that it enables efficient comparison of transcribed material with the facsimile of the primary source itself. As these two views are linked to one another and created from the same underlying data, this makes it feasible to read and navigate a text according to its logical structure, and at any stage of the process jump to the corresponding location in the sequence of page images to confirm accuracy of the transcription.      Figure 2. Full-text search results can be displayed in context in a logical transcription view (left), as well as aligned directly together with the source image in an image and transcription view (right).    Crowdsourced editing and curation As initial transcriptions are created using OCR, they inevitably contain mistakes. Users of the system have the option to correct mistakes they identify, as well as to annotate texts in a number of ways. Two distinct editing interfaces are provided: a direct editor, which enables direct editing of the underlying XML representation, and a visual editor allowing simplified editing of page-level transcriptions, which edits the same underlying content but does not require direct understanding or modification of XML. Regardless of which mechanism is used to submit an edit, all edits are committed immediately to the public system. Edits are versioned, allowing visualization of changes between versions and simple reversion of a text to its state at an earlier point in time. At present, the system receives on the order of 100 edits each day, representing much larger numbers of corrections, as editors frequently choose to correct multiple errors and sometimes entire pages in a single operation. Further visual editing tools supplement these mechanisms to enable crowdsourcing of more complex information. Illustrations are entered by the user drawing a rectangular box on the page image to indicate the location of the illustration, then filling in a simple form describing various aspects of it (Figure 3). This results in an XML fragment describing the illustration, which can simply be inserted into the text at the appropriate location to represent it. This allows the illustration to be extracted from its context on the page and represented in the full-text transcription view as well as in the page-wise view. It also facilitates illustration search functionality, where illustrations can be searched by caption across all materials contained in the system (Figure 4). A similar visual editing interface is used to enable the inputting of rare and variant characters which do not yet exist in Unicode. These characters are no longer in common use, but occur in many historical documents. The visual editing interface for rare character input also uses metadata provided by the user to identify whether a given character is the same as any existing character known to the system, and if so, assigns a common identifier so that data about these characters can be aggregated, and text containing such characters searched.    Figure 3. Identification and markup of illustrations within source materials are crowdsourced using purpose-designed visual editing tools which convert user input into XML.     Figure 4. Image search: individual images are extracted from (and linked to) the precise locations at which they occur in source materials, and can be searched by caption.    Exporting data and integrating with external systems In addition to the main user interface, a web-based Application Programming Interface (API) provides machine-readable access to data and metadata stored in the system. This facilitates text mining applications, as well as integration with other online systems. An example of the latter is the MARKUS textual markup system (De Weerdt et al. 2016), which can use the API to search for texts and load their transcriptions directly into this externally developed and maintained tool. An XML-based plugin system for the Chinese Text Project user interface also enables users to define and share extensions to the web interface which can be used to create connections to external projects and resources. This allows third-party tools such as MARKUS to integrate directly into the web interface, facilitating seamless connections between separately developed online projects. Text mining access is further facilitated by the provision of a Python module capable of accessing the API (Sturgeon 2017c), which is already in use in teaching and research (Sturgeon 2017b).  ",
       "article_title":" Chinese Text Project A Dynamic Digital Library of Pre-modern Chinese ",
       "authors":[
          {
             "given":"Donald",
             "family":"Sturgeon",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "philology",
          "public humanities and community engaged scholarship",
          "resource creation",
          "historical studies",
          "image processing",
          "English",
          "and discovery",
          "digitisation",
          "asian studies",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Artificial Intelligence has unlocked the access to the text of medieval manuscripts! The partners of the European research project HIMANIS implemented, for the first time, the indexing and plain text querying of thousands of pages of medieval manuscripts. The large scale of the corpus and the possibility to search in plain text for handwritten sources are unheard of in medieval studies, so that the results present a major shift for historians. The challenge of multilingualism, script variation and abbreviations, which are crucial for HTR on medieval sources, has been successfully met.   Context Millions of medieval manuscripts, charters and archival documents are preserved worldwide, and centuries of scholarship and text editions could naturally not exhaust the wealth of these resources. Digital libraries ( BVMM,  Gallica,  e-Codices,  Manuscripta Mediaevalia, etc.) and archives ( Monasterium) are amassing  reproductions of medieval manuscripts and archives, often with scarce metadata. However, while Optical Character Recognition technologies allow to easily “distant read” several millions of books (Moretti, 2013; Crane, 2006; Clement et al., 2008; GDELT Project, 2015),  medieval manuscripts and archives remain difficult to access, read and understand. Handwritten Text Recogni tion (HTR) systems cannot offer sufficiently accurate transcripts on historical documents. Therefore  continuous efforts are made in Europe. After  tranScriptorium  (McNicholl and Miles-Board, 2015) , the EU has funded HIMANIS and also funds  Recognition and Enrichment of Archival Documents  (READ, 2016-2019) under the H2020 program, with few medieval sources.  MONK , developed by the University of Groningen, is also a well-known infrastructure, including some medieval resources as those from Stadsarchiev Leuven.    HIMANIS: consortium, corpus, method, and evaluation Handwritten Text Recognition (HTR)  is the focus of the European cross-disciplinary research project HIMANIS (Historical MANuscript Indexing for user-controlled Search), funded by the JPI Cultural Heritage. The partners applied HTR technologies for  multilingual medieval manuscripts and demonstrated the feasibility of an accurate and meaningful automated text indexing of large collections of hitherto untranscribed text images .   The partners build a  cross-disciplinary consortium. The principal investigator is a researcher in the Humanities (Institut de Recherche et d’Histoire des Textes, CNRS) and the project gathered several research teams in engineering sciences, both in the private and public sectors : A2iA (France), University of Groningen (The Netherlands) and Polytechnic University of Valencia (UPVLC, Spain). Cultural Heritage institutions provided support and datasets: Archives Nationales (France), Bibliothèque nationale de France. UPVLC and University of Groningen are partners in or host institutions for the above mentioned READ and MONK developments.   As a challenging  and particularly interesting corpus, the partners chose the large collection of registers and formularies produced by the French royal chancery in the 14 th and 15 th c., encompassing  199 volumes, representing 83’000 pages, with 64’830 royal charters in 175 registers, and 24   formularies and related resources. This large and iconic collection bears witness to the rationalization of late medieval administration and is a key source to our understanding of medieval Europe and the rise of centralized nation states on the continent as consequence of the long lasting wars between France and England. While HTR on medieval sources is  notoriously highly difficult given the greatly variable handwriting styles, this corpus is even more challenging because of its multilingual content and the large number of abbreviated words.    A first work package consisted in creating the corpus, formatting available metadata, and authority data. The Archives Nationales digitized the corpus in several batches during the project. The metadata on French chancery registers are diverse. In increasing order of information quality, there are: (1.) medieval tables of content copied in autonomous inventories in the 18 th and 19 th c.; (2.) index cards with reference to shelfmarks (and rarely folio number) containing person and place names; (3a.) printed systematic inventories, including some already converted to EAD without their indexes, as well as (3b.) handwritten systematic inventories which were only accessible  in situ, and (3c.) printed geographic or thematic inventories; (4.) partial, rarely scholarly, editions. The partners devised an integrated TEI format to accommodate all four types of metadata, and converted all metadata to this format, including the handwritten inventories on which the partners applied HTR-technologies to recognize the text abstracts and the index entries  (Stutzmann et al., 2017) . In total, after more than 150 years of systematic research, inventories only covered 28’000 charters, that is ca. 43% of the register corpus and only one formulary was edited (Odart Morchesne, 2005; Guyotjeannin and Lusignan, 2011) . Authority data encompass linguistic dictionaries and gazetteers, which were used to produce a lemmatized search engine.    A second work package consisted in training and applying a robust “optical model”, capable of dealing with the variability and abbreviations in medieval, multilingual handwritings. The existing editions were first “aligned” on the available “text images” at a line level, applying techniques developed by the partners in the project Oriflamms (Leydier et al., 2014; Stutzmann et al., 2015; Bluche et al., 2016; Oriflamms, 2017) .  Basing on this alignment and using deep neural networks (CNN/RNN), machines could learn to “read”  (Bluche et al., 2017) . Learning on the monumental, modernizing, and very regularizing edition by P. Guérin (normalized punctuation, expanded abbreviations) (Guérin and Celier, 1881) , the system created so-called “character lattices” which included abbreviations, so that the system was also able to read and expand abbreviations (fig. 1).     Figure 1: Text indexing. The query “[Saint Omer]” retrieves both abbreviated and unabbreviated strings in different volumes and different handwritings. Images: Paris, Archives Nationales, JJ 35 and JJ 164.   The decoding process produces different “hypotheses” for each spot on the image (typically from one to ten variant readings) and rated them according to their confidence levels according to inner statistical models and linguistic ones. The index has been “pruned” (i.e. reduced by removing) from the most unlikely readings, but still contains more than 28 bn index entries, 3 bn lines, 44 bn “words” and “pseudo-words”.   The search engine was published online as a beta version ( http://prhlt-kws.prhlt.upv.es/himanis/ ) and is being transferred to  https://himanis.org/  where images and texts are accessible through the IIIF protocol and as IIIF annotations (fig. 2).    Figure 2: Indexed words as IIIF annotation in the IIIF compliant viewer Mirador (image: Paris, Archives Nationales, JJ 137, page 14)  Like in the tranScriptorium model, the users can set the confidence level for the search to reduce the noise or maximize the hit list value, a functionality that is the information science equivalent to the performance measure in computer science through “precision” (number of correct occurrences in the hitlist) and “recall” (number of correct occurrences compared to an error-free edition).   Qualitative and quantitative measures demonstrates that the HIMANIS system has obtained a very high level of precision, more than 85%, even increased to 99% through lemmatization (Stutzmann, 2017a-c)  (fig. 3).    Figure 3: Measuring the performance of text indexing (Recall/Precision Evaluation)    Additional challenges: writer identification, granularity, crowdsourcing…  In parallel to HTR, another focus was automated writer identification.  It allows a preliminary, nonetheless novel, analysis on the organization of the French chancery. These are among the first measured and convincing results produced for medieval handwritings, not represented in international competitions (Fiel et al., 2017; Andreu Sánchez et al., 2017) . Based on the Quill feature (Brink et al., 2012)  and validated on a partial ground-truth established by a paleographer, the system clustered hitherto unstudied page images, attributing them to 204 hypothetical writers. Fig. 4 illustrates the calculated presence of writers in each volume, giving a first insight into possible collaborations between scribes within the chancery across time.    Figure 4: Writer Identification: Visualization of the different “hands” and their likeliness in the different volumes (representation of Gaussian standard deviation for style clustering, with 204 clusters)  The partners tackle additional challenges. The text and structure of registers containing multiple charters impose to combine different granularities and intertwined both physical (page) and intellectual levels (one/several charter(s) on one/several page(s)). The integration of authority data and gazetteers allows new access, but with possible errors in text indexing and identification of named entities, measuring the applicability and usefulness of text indexing is an important methodological new task. Crowdsourcing results are currently negatively biased, because of the implemented ergonomics and users’ strategies. They tend to “suggest corrections” in order to improve their future search experience, rather than to validate correct spots (fig. 4). Nevertheless, it helps measuring impact, adequacy, precision, and usefulness.    Figure 5: Crowdsourcing as biased feedback    Perspectives  The results of HTR are setting a new standard and make the digitized images a new source. Yet, they must obviously not be mistaken for a scholarly edition. HIMANIS participated in the current trend of Digital Humanities and uses images as data, both for text and for writer identification (Kestemont et al., 2017) . Deep indexing represents new challenge for “distant reading” addressing topics and charter contents, because there is not an even number of hypotheses for all image spots. In new funded projects HOME (History Of Medieval Europe) and HORAE (Hours: Recognition Analysis, Editions), the partners are working on a methodology to create truthful and trustworthy results from uncertain and uneven, automatically generated data.   ",
       "article_title":"Handwritten Text Recognition, Keyword Indexing, and Plain Text Search in Medieval Manuscripts",
       "authors":[
          {
             "given":"Dominique",
             "family":"Stutzmann",
             "affiliation":[
                {
                   "original_name":"Institut de Recherche et d'Histoire des Textes (CNRS), France",
                   "normalized_name":"Institut de Recherche et d'Histoire des Textes",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05evznf71",
                      "GRID":"grid.450127.5"
                   }
                }
             ]
          },
          {
             "given":"Christopher",
             "family":"Kermorvant",
             "affiliation":[
                {
                   "original_name":"Teklia, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Enrique",
             "family":"Vidal",
             "affiliation":[
                {
                   "original_name":"Universitat Politècnica de València, (Spain)",
                   "normalized_name":"Universitat Politècnica de València",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/01460j859",
                      "GRID":"grid.157927.f"
                   }
                }
             ]
          },
          {
             "given":"Sukalpa",
             "family":"Chanda",
             "affiliation":[
                {
                   "original_name":"Rijksuniversiteit Groningen (The Netherlands)",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          },
          {
             "given":"Sébastien",
             "family":"Hamel",
             "affiliation":[
                {
                   "original_name":"Institut de Recherche et d'Histoire des Textes (CNRS), France",
                   "normalized_name":"Institut de Recherche et d'Histoire des Textes",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05evznf71",
                      "GRID":"grid.450127.5"
                   }
                }
             ]
          },
          {
             "given":"Joan",
             "family":"Puigcerver Pérez",
             "affiliation":[
                {
                   "original_name":"Universitat Politècnica de València, (Spain)",
                   "normalized_name":"Universitat Politècnica de València",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/01460j859",
                      "GRID":"grid.157927.f"
                   }
                }
             ]
          },
          {
             "given":"Lambert",
             "family":"Schomaker",
             "affiliation":[
                {
                   "original_name":"Rijksuniversiteit Groningen (The Netherlands)",
                   "normalized_name":"University of Groningen",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/012p63287",
                      "GRID":"grid.4830.f"
                   }
                }
             ]
          },
          {
             "given":"Alejandro H.",
             "family":"Toselli",
             "affiliation":[
                {
                   "original_name":"Universitat Politècnica de València, (Spain)",
                   "normalized_name":"Universitat Politècnica de València",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/01460j859",
                      "GRID":"grid.157927.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-30",
       "keywords":[
          "computer science",
          "information retrieval",
          "historical studies",
          "image processing",
          "text analysis",
          "English",
          "medieval studies",
          "content analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The Linked Open Data (LOD) community is growing In Digital Humanities (DH). Important datasets are being published in RDF. SPARQL endpoints have been progressively created in many cultural heritage organizations (Edelstein et al., 2013). However, the use of those datasets in real research is still not prevalent. Although there are several DH projects (Boer, V. de et al., 2016), SPARQL query exploitation is often limited within small technology-savvy communities (Lincoln, 2017). The situation is better for less-complicated Application Programming Interfaces (APIs) (XML and JSON). However, Sugimoto (2017b) suggests the needs of API standardization and ease of data reuse for ordinary users. In a broader context, the underuse of data, tools, and infrastructures seems to be a common phenomenon in DH. For example, the use of the Virtual Language Observatory in CLARIN is rather low (Sugimoto, 2017a). In case of the limited use of SPARQL endpoints, there could be different reasons for this: • Lack of awareness of existence • Lack of skills to use SPARQL • Opened data is too narrow in scope • Lack of computing performance to be usable • Interdisciplinary research is not widely exercised It is a pity that the benefit of Open Data is only partially spread, although data is available. To this end, the author has experimented with Wikipedia/DBpedia to explore the potential use of and/or the revitalization of Open Data in and outside research community.   Revitalization of Wikipedia/DBpedia by gamification The choice of Wikipedia/DBpedia is rationalized by taking into account the above-mentioned issues. The broad scope of their datasets would solve the problem of datasets in DH being too specific to be used by third party researchers (or the researchers do not know how to use data and/or what to do with them (Edmond and Garnett, 2014; Orgel et al., 2015). In addition, interdisciplinary research could be more easily adopted, using a more comprehensive yet relatively detailed level of knowledge. The keyword of the approach of this project is  gamification. In order to showcase a social benefit of Open Data and DH, gamification would be a catalyst to connect the scholars and the increasingly greedy public consumers. Kelly and Bowan (2014) stated that limited attention has been paid to digital games until recently, although this is changing rapidly (see Hacker, 2015). Although there are a few projects such as Cross Cult which uses elaborate semantic technologies (Daif et al., 2017), this article contributes to this discourse from a web innovation perspective in a simplified DIY project environment.  The game developed for the project is quite simple. It is a quiz that requires users to guess the age of a randomly selected person by looking at a portrait of the person (born between 1700 and 2002) (Figure 1). Apparently, the age of a person in a particular image is provided neither by Wikipedia, nor by DBpedia. It is, in fact, calculated programmatically by comparing the birthdate and the date of image. The random selection of data is sometimes costly for data processing, but it is the key to developing a game application. The application is intended for fun, thus, includes all types of contemporary persons such as politicians, sport athletes, musicians, actors, and businesspersons. In addition, the inclusion of historical figures is very important in DH in that the user would learn history.    Figure 1 Quiz to guess the age of a person found in a Wikipedia article  When the user cannot guess the age, there is a help function. A hint section is equipped with a face detection API of IBM Watson, suggesting the estimate age and gender of the person in the image by machine learning. Finally, this game is extended into another quiz to guess the nationality of a person. Indeed, any interesting data of Wikipedia/DBpedia can be used for gamification, and the method is easily adoptable.   Potential for Citizen Science As a reflection of critics of Linked Data quality, Daif et al. (2017) reckon that human supervision is needed to manage the data. In our case, the application is sometimes not able to calculate the age of a person, due to several reasons of metadata quality. For instance, data may be not numeric (“16th century”) (Figure 2), malformed (not ISO compliant: “05/11/88”), confusing (the creation date of digital image is used instead of that of analogue image), inaccurate, wrong, or missing, resulting in an error message. This is normally regarded as an optimization problem of the code. However, it is possible to take advantage of this error. When it occurs, it is a sign of data quality problem. Therefore, users are persuaded to follow the provided links to Wikipedia/DBpedia and able to double-check the original data (Figure 3). This scenario creates a dual possibility. In other words, the application can be used as: • A curation tool of Wikipedia/DBpedia for existing active editors of Wikipedia. • A tool to transform normal users into new curators of Wikipedia Although this scenario has not happened due to the project setting, if the users are able to correct data, the impact for data curation could be considerable. Not only is it to the benefit of correcting and/or adding data in Wikipedia, but DBpedia will also be improved, leading to the higher quality of datasets of this LOD magnet, affecting hundreds of applications worldwide. In this way, this application opens up the potential to  crowdsource the curation of Wikipedia/DBpedia. The success of the crowd data curation has been proven in DH (see Brinkerink, (2010) and NYPL Labs).     Figure 2 Wikimedia metadata displaying “16 th century”     Figure 3 The game persuades users to improve Wikipedia   Conclusion In conclusion, this article demonstrates an experimental case study of mixing gamification (entertainment) with data-driven research (education) and the possibility for data curation (crowdsourcing), showcasing cutting-edge technologies such as SPARQL and Deep Learning API, with the help of Open Data in the framework of DH. It also displays a potential for a new digital research ecosystem among humanities research and digital technologies, connecting various stakeholders including humanities researchers and the public.  ",
       "article_title":"Revitalizing Wikipedia/DBpedia Open Data by Gamification -SPARQL and API Experiment for Edutainment in Digital Humanities",
       "authors":[
          {
             "given":"Go",
             "family":"Sugimoto",
             "affiliation":[
                {
                   "original_name":"Austrian Academy of Sciences, Austria",
                   "normalized_name":"Austrian Academy of Sciences",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/03anc3s24",
                      "GRID":"grid.4299.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "historical studies",
          "cultural studies",
          "library & information science",
          "metadata",
          "English",
          "semantic web",
          "crowdsourcing",
          "games and meaningful play"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What is the purpose of higher education? In the United States, this question dates back to at least the nineteenth century with the passage of the Morrill Acts of 1862 and 1890, and has taken on new urgency in an era of manufactured austerity and neoliberal crisis. In particular, scholars of critical university studies such as Christopher Newfield, Fred Moten and Stefano Harney, Sara Ahmed, Craig Steven Wilder and Roderick Ferguson critique the ways higher education often reproduces the very conditions of inequality it claims to challenge. Often, these compelling analyses are based on the investigation of conditions at a few representative universities, but through leveraging digital methodologies we can gain a wider perspective that enables a more comprehensive analysis of what universities put forward as their purpose.  Our research advances these conversations through large-scale textual analyses of two data sets: university mission statements included in the U.S. Department of Education’s Database of Accredited Postsecondary Institutions and Programs and recent demand statements put forth by activist students. Mission statements offer a public-facing proclamation that bridge universities to larger communities and educational contexts. Often, they present idealized claims that reflect the university’s marketed brand. We use “university” broadly; our data set includes community colleges, public universities, private universities, research institutions, teaching-focused institutions, for-profit and nonprofit schools, and our analysis highlights the variation in their commitments to education. The second data set is a collection of student demands compiled by WeTheProtestors and the Black Liberation Collective, two social justice groups that are working to address institutional inequality across U.S. universities. In many cases, these demands are written to address the institutions of the official university mission statements we are working with, and range from private institutions like Yale University and Ithaca College, to public universities such as Iowa State and UCLA. These demands challenge existing institutional language and require analysis in their own right. With this research, we seek to answer two questions: 1) What do contemporary U.S. universities claim as their mission and vision? 2) How do these stated aims of education intersect or diverge with the demands of activist students calling for pedagogical, institutional, and social change? In analyzing this data, we draw from the insights of critical race, gender, and sexuality studies, which have long been sites of institutional critique. Coupling digital tools with a theoretical lens informed by activist pedagogy enables us to better apprehend the power structures and social dynamics at play in public-facing institutional documents and how those interface with the communities they are tasked with serving. By better understanding the professed commitments of academic institutions, we aim to contribute to the project of making education more just, equitable, and inclusive. This work is carried out through the web scraping of data, topic modeling, and statistical analysis in Python. Once analyzed, the raw data and findings are also rendered as interactive web-based data visualizations in JavaScript to make the research more accessible to the public and available for refactoring. Initial statistical textual analysis and data visualization that we have conducted has revealed interesting trends among public universities in contrast to demands put forth by students. Mission statements from state universities emphasize a commitment to the objectives of research, knowledge, and professionalism, and the endeavors of providing and serving, and learning and teaching. However, student demand statements have a more expansive understanding of education that stresses inclusivity and community, while also voicing concerns about race, gender, workers, and resources. By comparing and contrasting across data sets, we examine what each type of institution and group is seeking to achieve, and work to determine whether universities are serving the needs of student populations. When universities are more concerned with vocational skills training rather than challenging power hierarchies, structural inequalities, and the distribution of resources along embodied axes of race and gender, there is a clear disconnect between what institutions are offering students and what students in turn demand. If universities aren’t serving their students and communities, who are they serving? Our data set and programming files will be made publicly available in a code repository so that others who investigate higher education can perform their own research. While our focus is grounded in the specific histories of higher education in the United States, we hope that sharing this research at an international conference will encourage others to perform similar analysis of institutional and popular discourse in their countries, thus allowing for a more vibrant understanding of how higher education functions in different contexts. By inviting others to add data to our public repository from international institutions, we can begin to consider how globalization impacts learning institutions. In an effort to advance intercultural scholarly exchange, a Spanish translation of this research will be available online.  ",
       "article_title":" The Purpose of Education: A Large-Scale Text Analysis of University Mission Statements  ",
       "authors":[
          {
             "given":"Danica",
             "family":"Savonick",
             "affiliation":[
                {
                   "original_name":"The Graduate Center of the City University of New York",
                   "normalized_name":"City University of New York",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00453a208",
                      "GRID":"grid.212340.6"
                   }
                }
             ]
          },
          {
             "given":"Lisa",
             "family":"Tagliaferri",
             "affiliation":[
                {
                   "original_name":"Fordham University, DigitalOcean",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-24",
       "keywords":[
          "cultural studies",
          "diversity",
          "english studies",
          "text analysis",
          "black studies",
          "English",
          "gender studies",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The experiences of six million victims of the Holocaust perished with them. This paper will discuss the ways text and data mining technology has helped to recover fragments of lost experiences out of oral history interviews with survivors. The paper will also present how a data-driven anthology of these fragments has been built. The first part situates the challenge of uncovering experiences of the voiceless in historiography. The second part shows how text and data mining techniques have been applied to recover fragments of lost experiences from a big corpus of English language interview transcripts in the collection of the United States Holocaust Memorial Museum (USHMM). The third part demonstrates how web technology and visualization are used to render these fragments in a digital anthology.  The ethical and theoretical problem of narrating the experience of those who did not survive the Holocaust has been often addressed. Primo Levi has argued that survivors cannot tell the experience of those who did not survive because the Saved and the Drowned are “two particularly well differentiated categories among men.” The Saved lived in a morally questionable “grey zone” that compromises their testimony (Levi, 2018). Others have pointed out how trauma inhibits survivors from recalling their own experiences (Felman, Laub 2013; Lacapra, 2014; Hartman, 2015). Others have argued that testimonies are shaped by narrative and discoursive processes (Bernard-Donals, Glejzer 2001; Rosen, 2009). Survivors’ testimonies are therefore often used to study memory, and the underlying mediative processes (Langer, 2007). In short, there are gaps between the experience of the Saved and the Drowned, and between experiences recalled in a testimony and the original experience in the past.  This paper argues that despite these gaps, in testimonies there is a set of rudimentary experiences that are shared by both the Saved and the Drowned. They are basic physical and emotional states, as well as actions, that are cross-cultural; they are not the expression of post-traumatic states or any discursive, narrative, and linguistic mediation, but the very original experience. “Children crying for their parents“ or “feeling ashamed at the moment of being forced to undress” are examples of these rudimentary experiences. Beyond their rudimentary nature, experiences shared by the Drowned and the Saved have another feature: given a reasonably large collection of testimonies, they recur in narration of victims who had very different fates. Epistemologically, the recurrent rudimentary experiences in testimonies by the Saved are the likely experiences of the Drowned. This however overlooks - on purpose - the realm of suppressed memories. The first computational goal of this work was to retrieve textual fragments expressing similar rudimentary experiences in a corpus of 1571 randomly selected interview transcripts (approximately 27 million tokens) in the USHMM. The retrieval of textual fragments expressing similar experiences is a text mining task that has two differences from text reuse and plagiarism detection (Alzahrani et al, 2012; Büchler et al., 2014). First, non–native speakers are likely to use different vocabulary, as well as different grammatical constructions, to describe the same experience. Second, while plagiarism and text reuse detection aim to discover any repeating sequence in a text, this project has sought to discover only rudimentary experiences. In addition to the fact that plagiarism and text reuse detection tools could not offer solutions to the problems above, the project had to face another core difficulty: inference of meaning from longer sequences of words requires substantial further research in Text Mining. In order to retrieve fragments describing experiences that are recurrent and rudimentary, a specific pipeline involving both algorithmic and human supervised stages has been designed by the author. Prior to the implementation of the pipeline, the data underwent a standard linguistic pre-processing, including detection of multiword expressions. The document frequency of all verbs in the corpus was computed, and verbs with document frequencies above the median (0.14) were labelled as “recurrent” and were investigated by a human agent. From this list of recurrent verbs, those expressing rudimentary physical and emotional experiences (for instance, “cry”, “yell”, “fear”) were selected. Focus on verbs is explained by the fact that they are the most natural form to express experience. As a second step, a word embedding model was trained on the data, and synonyms of the pre-selected verbs were identified. The word embedding model broadened the initial focus on verbs since less frequently used adverbial and adjectival expressions were also identified (for instance, “undress”, “barefooted” and “naked”). This resulted in an array of recurrent synonym sets. As a third step, from all textual contexts in which members of a given synonym set occur, document collections were constructed, and trained with a TF-IDF based LDA (Blei et al, 2003). The LDA model resulted in groups of words, also known as topic words, that tend to co-occur in a collection of textual contexts, as well as those textual contexts that are the most likely to be close to the group. As a short evaluation, the context based application of LDA was efficient to analyze the tendency of larger unit of words to co-occur. Traditional metrics to measure strength of association give less efficient results with units longer than bigrams. Furthermore, they cannot capture synonymy while LDA can do in certain cases. The last stage of the pipeline was the analysis of groups of words, and the textual contexts close to them, by a human agent. This was meant to investigate whether a given word combination, uncovered by LDA, actually referred to an experience, and to capture complete phrases, or “fragments,” that express the experience. The result of the modelling process was a collection of approximately 200 fragments expressing 30 rudimentary experiences, though the model continues to identify additional experiences. In short, the pipeline has helped to detect sets of “sub-experiences” associated with a given rudimentary experience (shooting as sub-experience in the domain of nakedness), as well as textual fragments expressing them. At the same time, the model features limitations: it cannot for instance detect metaphorical expressions. The second computational task was to find prototypical episodes in the domain of a rudimentary experience without supervision. For this purpose, the document collection of textual contexts underlying a given synonym set was trained with paragraph vector model (Le and Mikolov, 2014), and clustered with affinity propagation (Frey et al, 2007). This produced not only clusters but specific contexts that are the centers or the prototypical members of clusters. These prototypes are seen as typical episodes in the domain of a rudimentary experience. Using these findings, a digital anthology that renders fragments expressing rudimentary experiences, prototypical instances of rudimentary experiences along with transcripts and audio / video recordings is currently being developed. This anthology will support a hierarchical tree visualization in which branches represent core rudimentary experiences and leaves represent either prototypical instances or sub-experiences in the domain of rudimentary experiences. It will answer three important requirements of scholarship. First, it will uncover the multiplicity of contexts and ways in which the very same rudimentary experiences could take shape. Second, it will enable the investigation of a testimony both as a text and as an audio / video record. Third, the anthology enables the reading, listening or watching of experiences, which were retrieved and selected not by drawing on a historical preconception. Instead, a “let the data speak” approach was implemented in the pipeline described above. The retrieval and selection process was guided by features (recurrence, prototypically, characteristic word combinations) inherent in the data set, which gives rise to a data-driven anthology. As a whole, the anthology does not aim to present hitherto unknown or surprising experiences. Instead, the goal is to challenge the implicit banality of experiences such as “children crying for their parents” by letting survivors talk about them (where and how they happened; most importantly what and how they felt). The contribution of the anthology is the offering of a wide-scale overview of a large variety of experiences - narrated by victims themselves and retrieved with a bottom-up approach - which would not be accessible by reading individual testimonies. The goal of this work can be summarized with an analogy. Original works of Pre-Socratic philosophers vanished forever; nonetheless, their intellectual world have remained accessible and investigable through hundreds of fragments recovered from later works (Kirk et al., 1957). Individual experiences of millions perished, but their likely experiences continue to live through fragments in testimonies. Our contemporary understanding of the Holocaust is by large based on archival sources produced by perpetrators. These sources can help to investigate the process through which victims went through, but not the way victims experienced the process. The anthology of recovered fragments wants to impact scholarship by presenting the perspective of the victim from less studied angles. The overall goal is to let those who did not survive speak through recovered fragments. ",
       "article_title":"In Search of the Drowned in the Words of the Saved: Mining and Anthologizing Oral History Interviews of Holocaust Survivors",
       "authors":[
          {
             "given":"Gabor",
             "family":"Toth",
             "affiliation":[
                {
                   "original_name":"Yale University, United States of America",
                   "normalized_name":"Yale University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03v76x132",
                      "GRID":"grid.47100.32"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "scholarly editing",
          "digital humanities history",
          "linguistics",
          "historical studies",
          "cultural studies",
          "theory",
          "epistemology",
          "data mining / text mining",
          "English",
          "natural language processing",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Nondestructive Imaging of Egyptian Mummy Papyrus Cartonnage This rapid development and testing project brought together international partners, scholars and collections in an exploratory, pilot effort from November 2015 to March 2017. The international, multidisciplinary team demonstrated that some nondestructive digital imaging techniques and technologies (Fig. 1) have potential to make texts visible in Egyptian Ptolemaic papyrus mummy mask cartonnages. A major challenge in working across the different technologies, disciplines and institutions was integrating data from diverse technical imaging systems and work processes, requiring new and proven digital humanities data management capabilities.      Figure 1. Multispectral Imaging of Mummy Mask at UCL Centre for Digital Humanities, one of the advanced imaging techniques researched during this project.  Before this project, other scholars destroyed the masks to access the papyri, denying future researcher access to the primary historical artefacts (Mazza, 2014). This project capitalized on digital humanities skills and data management techniques in assessing the integration of non-destructive digital imaging technologies to make texts visible in layers of papyrus in mummy cartonnages for open research and analysis. Intermediate goals, such as detecting the presence of text, also proved valuable in highlighting the destructive techniques used to study mummy masks and offering scientifically valid approaches for documenting the initial state of objects and their production for future research. A global team pulled together expertise from science and the humanities, including: digital humanities, Egyptology and papyrology, medicine, dentistry, particle physics, imaging science, data and project management, and systems engineering. Team members rapidly implemented a phased and agile approach at multiple institutions to develop and apply increasingly complex imaging, processing and data integration techniques to penetrate the paint and papyrus layers in mummy cartonnage and host all data online (Fig. 2).      Figure 2. Mummy cartonnage advanced imaging process flow  Data Integration  Project data integration was dependent on common data and metadata standards for ease of image correlation and integration, as well as effective data and project management across disciplines, technologies and institutions. All the different imaging modalities (Multispectral imaging, X-ray fluorescence, Optical Coherence Tomography, X-ray microCT, Terahertz and others) yielded very different data sets from each technology and institution. Integration of images from multiple imaging sources offered potential to apply the strengths of multiple imaging techniques for ease of visualization by scholars and curators. Integrating data from a variety of equipment required significant planning and collaboration across institutions and disciplines (Fig. 3). This required streamlined standardization processes and/or more time and resources to devote to this part of a program.    Imaging Technology Imaging Institutions Contributing Objects  Principal Investigators     Multispectral Imaging    UCL, Manchester, Duke, UC Berkeley,    RB Toth Associates     UCL Petrie,    UC Berkeley, Duke, UCL*     Melissa Terras,  Adam Gibson,   Bill Christens-Barry, Michael Toth      Spectral Domain Optical Coherence Tomography   Duke University   Duke, UCL*    Sina Farsiu,   Adam Wax, Cynthia Toth     X-ray Fluorescence Scanning   SLAC SSRL   Berkeley † , UCL*  Uwe Bergmann   X-ray Micro-Computer Tomography University of California at Berkeley  Berkeley † , UCL*   Dula Parkinson    X-ray Micro-Computer Tomography Queen Mary University of London   Berkeley † , UCL*    David Mills,  Graham Davis    Terahertz Imaging University of Western Australia  UCL*   Vincent Wallace, Shuting Fan, Anthony Fitzgerald     XRF Analyzer   Bruker Scientific   Petrie, Berkeley, Duke, UCL*    Lee Drake,  Adam Gibson     Fiber optic Reflectance Spectroscopy  Equipoise Imaging  Berkeley † , UCL*  Bill Christens-Barry   *UCL Phantom surrogate papyrus samples  †UC Berkeley Tebtunis Center s.n. cartonnage fragment  Table 1. Participating institutions and imaging techniques used during cartonnage imaging. The integration of data and work processes from a variety of scientific tools, disciplines and institutions required storage, dissemination, and searchable access to data from instruments that provide output in different formats, some of which were unique to the research methods and disciplines (Emery et. al., 2004). While common standards and processes across institutions were encouraged, this was difficult with data and standards from technologies as diverse as nuclear synchrotrons and optical cameras. In addition, many contributors to this project volunteered their time and equipment for imaging and basic processing, but had limited time to spare from their day-to-day responsibilities – ranging from medical personnel preventing blindness to particle physicists studying elemental changes in bone formation. Data Storage and Management The approximately 300 Gb of data products– including images, individual reports, captured and processed data sets, analytical data and metadata– are now freely available online at  https://www.ucl.ac.uk/dh/projects/deepimaging/data. This data set comprises a core content set of digital images, analytical data and technical reports on the imaging and analysis of mummy mask cartonnage and modern surrogates from the multiple imaging institutions. UCLDH established this project website to host the project information and data at the same for scholars, scientists and the public (UCL, 2017).  Collecting, organizing and hosting data with appropriate metadata from multiple institutions and systems around the globe proved to be a complex problem. This included providing access to and sharing of timely, complete, and relevant data during the project. This was due to both different data collection standards and the wide range of output from proprietary equipment. A key strength of this program was all institutions agreed to make all data freely available under Creative Commons license. This allowed the free exchange of all data for digital processing, analysis and research.  The data structures of the Archimedes and Galen Palimpsests and the University of Pennsylvania’s OPenn served as models, but had to be adapted to include the various types of data sets for each image and data collection modality. To support scientific data integration, the team also used the Library of Congress CLASS-D data model. Some adjustments were needed to previous flat file access protocols to make the data product more accessible to users and future researchers. As an example, the large captured multispectral data sets were put in separate folders from the processed images, with the former available for follow-on digital processing and research, and the latter available for immediate visualization of our findings produced with current processing tools. The need for quality assurance to verify and validate the data proved important. Once the data was integrated, some type of feedback mechanism was needed to validate and check the data against other data in collaboration with the collector as part of collaborative research. This highlighted the value of the data in conjunction with other data, with feedback on the efficiency and quality of the data and its reproducibility as initially structured and standardized. This significantly improved data sharing and preservation across the research team. Conclusions Effective data management, integration and technical support are critical enablers in any broad digital research program to ensure data availability for follow-on research, even those (like this one) with a limited budget. The ability of imaging equipment to produce a standard data output with relative ease of use by the operator and researcher is important to the visualization, storage of and access to the data. Standardized procedures and data output better allow independent imaging of the same object with multiple technologies, with subsequent integration of data to leverage the strengths of each technology and technique. ",
       "article_title":"Digital Humanities Integration and Management Challenges in Advanced Imaging Across Institutions and Technologies",
       "authors":[
          {
             "given":"Michael B.",
             "family":"Toth",
             "affiliation":[
                {
                   "original_name":"UCL, University College London, UK; R.B. Toth Associates, United States of America",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":"UCL, University College London, UK; University of Edinborough, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Adam",
             "family":"Gibson",
             "affiliation":[
                {
                   "original_name":"UCL, University College London, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Cerys",
             "family":"Jones",
             "affiliation":[
                {
                   "original_name":"UCL, University College London, UK",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-01",
       "keywords":[
          "archaeology",
          "databases & dbms",
          "resource creation",
          "library & information science",
          "image processing",
          "English",
          "metadata",
          "and discovery",
          "digitisation",
          "near eastern studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This work-in-progress paper offers for critical review the current challenges of an ambitious project to create a digital framework for interpreting the dissolution of monasteries in Europe. The most dramatic episode of the European Reformation ( c.1517-  c.1648), the state suppression of monasteries, the dispersal of their populations, the re-distribution of their property and the re-deployment of their infrastructure, represented the largest and furthest-reaching re-ordering of society, economy and culture before the Industrial Revolution (Chadwick, 2001; Youings, 1971). The scale, scope, pace and reach of the process make it perhaps the most formidable of all pre-modern territories for the data-driven researcher, and have ensured that narrative histories founded on conventional methods of data analysis have consistently failed to provide perspectives of adequate breadth, depth, and accuracy.   In respect of research data, the medieval monastery presents both the best and worst of all prospects. A world in microcosm, possessed of its own demographic, economic, social, cultural and environmental imprint, in principle there are multiple layers to its source-base. It also runs deep through time, passing any polity, dynasty, and even place of settlement to reach back to the remote beginnings of Christian-occupied Europe. Yet for these same reasons, the sources of the medieval monastery are also uniquely unstable. The self-containment of the monastery was such that while the form and function of its documentary record might be comparable one to another, it is never quite the same. A durable - but not always enduring - presence in a world that was chronically disturbed, the record underwent repeated and extended interruptions. The monastery invited the manipulation of those in power, and its records are susceptible to conscious distortion. Even well-preserved monastic records can confound the researcher. The closure and re-constitution of these centuries-old institutions brings these data complexities into collision with the records of the state, city, commune and of private individuals at a moment when these constituencies were in transition to a post-medieval world. The bare historical record may give the impression of the dissolution as an event bounded by the dates of specific acts of state, but in fact its course and consequences were a collective experience which unfolded over several generations. This means that for effective interpretation, datasets should be defined not by the intrinsic criteria of a particular monastery but rather by those that can be related to the contexts in which it was situated, relating to a range of organisational and social networks and to physical place and space. This requires drawing from the monastery’s records data that they were not originally created to document. For example, a contextualised approach to data on the monastery’s population profile demands not only a raw numeral but also a measure of its geographical origin, social status and generational mix, each in relation to other neighbourhood constituencies. Because the dissolution was experienced over  la longue durée, a wide chronological frame is needed: only by capturing data from 1450 to 1650 can the process of dissolution be traced in real time. Given the inherent characteristics of the records, this can be no conventional time-frame bringing a strict linear order to each dataset. With unequal interruptions in every category of record, instead the timeline must be drawn between irregular census points derived from individual documents.  Presently, we are applying these principles to a single case-study, the English Benedictine abbey of Battle, in the county of Sussex, dissolved in 1538. A substantial foundation, holding territory across seven counties of England and Wales, overseeing diverse agricultural, commercial and industrial interests and governing a network of satellite churches and communities, Battle presents sufficient scale and complexity to guide, and test, our emerging methodology (Evans, 1941-2; Searle, 1974). We are creating datasets which aim to measure (1) every aspect of the monastery’s presence in and imprint upon its neighbourhood in the period before its dissolution and (2) the pattern and pace of change in that presence and imprint as the monastery was suppressed. We have defined data categories to evaluate its dynamic role in its neighbourhood, providing a series of key performance indicators at those census points which can be established. Although these do not always directly reflect the categories of the monastic records, generally it has been possible for data to be anchored by a specific documentary reference. However, it is sometimes necessary to make use of proxies. For example, because the family origins of monks are rarely documented, surnames are taken as an index of origin and social position, and because the precise site and proportions of monastic buildings are not consistently documented we have adopted a ‘best-guess’ principle, utilising historic mapping, field-, excavation and environment surveys, realising the benefits of the Archaeology Data Service (  www.archaeologydataservice.ac.uk ) and Heritage Gateway (  www.heritagegateway.org.uk ).  Our paper explores how we are addressing these complexities and challenges in our sources by combining a webapp built on an open source XML database (xQuery-based eXist-db,   http://exist-db.org/ ) with highly customisable mapping using jQuery and GoogleMaps API to create a digital framework for analysing the process of dissolution across Europe. The framework allows researchers to interpret its events and sources at levels from regional to site-specific, utilising a comparative approach to reveal and visualise patterns that have been largely obscured within this often chaotic set of sources. We are building the webapp to be redeployed by others: its source code and documentation will be released freely on GitHub, enabling others to reuse it for their own research aims. The complexity of the process of dissolution might suggest that certainty in interpretation is an impossible goal, but our work to date suggests that far from this being a deterrent to digital approaches, it instead raises important questions about how we describe our datasets and how we can represent with honesty and clarity the uncertainty, inconsistency and gaps in our sources. These are questions with which every digital humanities scholar must grapple, and having set out the solutions we have identified so far, we are keen to invite discussion on how we might resolve or improve our approach for the benefit of current and future projects encountering similar issues.  ",
       "article_title":"Towards A Digital Dissolution: The Challenges Of Mapping Revolutionary Change In Pre-modern Europe",
       "authors":[
          {
             "given":"Charlotte",
             "family":"Tupman",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Clark",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Holding",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-27",
       "keywords":[
          "technologies",
          "geohumanities; spatial & spatio-temporal analysis",
          "modeling",
          "historical studies",
          "resource creation",
          "visualisation",
          "visualization",
          "and discovery",
          "digitisation",
          "medieval studies",
          "interface & user experience design/publishing & delivery systems/user studies/user needs",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Abstract We present LitViz, a webbased tool for visualizing literary data which utilizes the text2voronoi algorithm to map natural language texts onto voronoi diagrams. These diagrams can be used, for example, to visually differentiate between (groups of) authors. Text2voronoi utilizes the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. This means that, in contrast to conventional approaches to text classifiction, we do not directly use linguistic features, but explore visual features derived from the texts' visualizations to perform operations on texts. We illustrate LitViz by means of 18 authors, each of whom is represented by 5 literary works. Introduction In this paper we present a new tool, called LitViz, for the visual depiction of literary works. To this end, we utilize the text2voronoi algorithm (see Mehler et al. (2016b)) which maps natural language texts to image representations. The idea is to generate images of texts which can be used instead of these texts’ symbolic information to characterize them, for example, in terms of authorship, topic or genre. Text2voronoi is in line with the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. In contrast to conventional approaches to text classification, we therefore do not directly use linguistic features, but explore visual features derived from the texts’ visualizations in order to identify, for example, their authors. We exemplify LitViz by means of 18 authors each of whom is represent by 5 literary works. LitViz allows for interacting with the visualizations of these works in two modes: two- and three-dimensionally (see Figure 1 and 2).   Figure 1: Visual depiction of E.T.A. Hoffmann’s Das steinerne Herz   Related Work The idea of visualizing literature was inspired by Martin Wattenberg’s The Shape of Song1 (Wattenberg, 2001; Wattenberg, 2002). Wattenberg explores identical or otherwise repetitive passages of a composition to visually depict them. This is done by means of semicircles, which combine repeated and repetitive positions in such a way that the micro- and macro-structure of a composition becomes visible. Our idea is to transpose this idea to the visualization of literary data.  Kucher and Kerren (2015) give an overview of state-of-theart techniques of text visualization and present a website that allows for differentiating between these techniques. Cao and Cui (2016) provide a systematic review of many advanced visualization techniques and discuss the fundamental notion of information visualization.  Mehler et al. (2016a) present a web tool called Wikidition which allows for automatically generating large-scale editions of text corpora. This is done by using multiple text mining tools for automatically linking lexical, sentential and textual data. The output is stored and visualized using a MediaWiki. Thus, any Wikidition is extensible by its readers based on the wiki principle.  Rockwell and Sinclair (2016) present a detailed web tool, called Voyant tools, for visualizing texts. Unlike Voyant, our focus is on non-standard techniques of visualizing textual data that go beyond histograms, scatterplots, line charts and related tools.  Generally speaking, text visualization supports distant reading as introduced and exemplified by Moretti (2013), Rule et al. (2015) and Michel et al. (2011). These approaches show how visualizations that support distant reading may look like to get overviews of documents by just looking at the final visualizations. LitViz is a tool following this tradition: it utilizes text2voronoi to extend the set of techniques mapping textual data. In this way, it combines Wattenberg’s approach with distant reading techniques from the point of view of text visualization.     Figure 2: 3D visualization of Franz Kafka’s Der Kübelreiter.  Model Our goal is to generate images from literary works in a way that text classifiers can be fed by the features of these iconic representations in order to perform classification experiments, for which usually linguistic features are explored. This is the task of the text2voronoi algorithm, which calculates image representations of texts in four steps Mehler et al. (2016b): In the first step, the input text is analyzed by means of TextImager Hemati et al. (2016) to extract linguistic features in the usual way, that is, features, spanning a vector space of linguistic data. In the second step, the resulting vector space is used to compute embeddings for each of the extracted linguistic features. Embeddings are produced by means of word2vec (Mikolov et al., 2013). In the third step, a voronoi tessellation of the embedded features is computed. As a result, each lexical feature is mapped onto a separate voronoi cell whose neighborhood reflects the feature’s syntagmatic and paradigmatic associations with other features of the same space. The topology of the voronoi cells spans a voronoi diagram that visually represents the input text. Each of these cells is characterized by its filling level, transparency and height (third dimension) thereby reflecting its co-occurrence statistics within the input text, while the position and size of a cell is determined by the embedding of the corresponding feature – for the mathematical details of this algorithm see Mehler et al. (2016b). Finally, the text2voronoi algorithm extracts visual features from the voronoi diagrams to feed classifiers performing classifications of the input texts.  LitViz utilizes the first three steps of this algorithm. Unlike the classical text2voronoi procedure, it does not address the final step of classification. Rather, it gives access to voronoi diagrams of input texts via a two-dimensional graphical interface, which can be transformed into a three-dimensional one by means of user interaction. These two- and threedimensional text representations can be used by the user of LitViz to interact with the underlying input texts in order to highlight single voronoi cells, to change her or his reading perspective or to visually compare voronoi diagrams of different texts. In this way, LitViz paves the way to a kind of a comparative distant reading by making accessible the visual depictions of different texts in an interactive manner. The LitViz Tool We have selected 18 authors of German literature each of whom is represent by 5 literary works. The works are taken from the Project Gutenberg (https://www.gutenberg.org/) and visualized by means of the text2voronoi algorithm. Any of these examples is made accessible by the front page of LitViz (see Figure 3). When hovering over a voronoi cell of the voronoi diagram of a sample work, information about the underlying linguistic feature represented by this cell is displayed. According to Mehler et al. (2016b), we call these images VoTes: Voronoi diagram of a Text. LitViz presents VoTes via a graphical user interface for two- and three-dimensional interactive graphics. In this way, we go beyond Wattenberg’s 2D depictions of musical pieces.      Figure 3: Front page of LitViz.   The second page (tab) of LitViz gives access to the comparison tool. Here the user first selects the number of VoTes to be compared. Then the user selects a subset of works of the authors to be compared. In the example in Figure 4, we compare four VoTes of two authors: two VoTes of two works of Heinrich Heine (top) and two VoTes of Heinrich Mann (bottom). It is easy to see that these VoTes fall into two classes, depending on the underlying authorship. Heinrich Mann’s two VoTes are organized around a center that is composed of many small cells, while there is a small subgroup of peripheral cells that are large. In contrast to this, the two VoTes of Heinrich Heine do not display such a center and are more evenly distributed in terms of their size. It is a main task of LitViz to allow for such comparisons. In this way, that is, by interacting with the texts’ image representations and by using the mouse-over technique, the user can study single features and how they are related to other features of the same representational space.      Figure 4: Comparison tool: Heinrich Heine (top) in comparison to Heinrich Mann (bottom).   Last but not least, LitViz provides a so-called custom tab. Here, the user can upload and visualize its own texts. It is then possible to set filter options using an option tool (see Figure 5) in order to further restrict the visualization.    Figure 5: Custom VoTe with filter options.  Conclusion We introduced a novel web tool, called LitViz, for visually depicting natural language texts based on the text2voronoi algorithm. LitViz enables the comparison of the visualizations of different texts. This allows, for example, for comparing the styles of the underlying authors visually. In this way, we extend the existing tool palette of distant reading. LitViz can be accessed via:  http://alba.hucompute.org/text2voronoi  ",
       "article_title":"LitViz: Visualizing Literary Data by Means of text2voronoi",
       "authors":[
          {
             "given":"Tolga",
             "family":"Uslu",
             "affiliation":[
                {
                   "original_name":"Goethe University of Frankfurt, Germany",
                   "normalized_name":"Goethe University Frankfurt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04cvxnb49",
                      "GRID":"grid.7839.5"
                   }
                }
             ]
          },
          {
             "given":"Alexander",
             "family":"Mehler",
             "affiliation":[
                {
                   "original_name":"Goethe University of Frankfurt, Germany",
                   "normalized_name":"Goethe University Frankfurt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04cvxnb49",
                      "GRID":"grid.7839.5"
                   }
                }
             ]
          },
          {
             "given":"Dirk",
             "family":"Meyer",
             "affiliation":[
                {
                   "original_name":"Goethe University of Frankfurt, Germany",
                   "normalized_name":"Goethe University Frankfurt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/04cvxnb49",
                      "GRID":"grid.7839.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-19",
       "keywords":[
          "computer science",
          "creative and performing arts",
          "internet / world wide web",
          "linguistics",
          "stylistics and stylometry",
          "visualisation",
          "English",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This long paper will offer an archeology of the Gale database  Sabin Americana, 1500-1926, tracing its origins through an earlier microfilming project to Joseph Sabin’s  Bibliotheca Americana, a monumental 29-volume “Dictionary of works related to America” begun in 1868 and completed in 1937. While Bonnie Mak, Ian Gadd, and others have explored the bibliographic roots of much-used digital resources like the ESTC and EBBO, the category of Americana has a distinct bibliographic tradition whose digital implications have not been examined. While many contemporary databases derive from earlier bibliographic projects organized by language or nation, “Americana” was for Sabin and his contemporaries a transnational and multilingual category that understood “America” as the entire Western Hemisphere. Sabin and other nineteenth-century bibliographers of “Americana” ultimately produced works with an implied teleological view of a New World history that began with “discovery” and culminated in the emergence of the United States; nevertheless, they conceived of the early history of the hemisphere as a shared one, and their work emerged from an extended scholarly network that encompassed not only the Anglophone but also the Hispanophone world.  While Gale’s database borrows Sabin’s name and title, it is otherwise strikingly vague on the exact nature of its relationship to the original print bibliography. A close examination reveals that, although the structuring logic of the database is not dissimilar to Sabin’s alphabetic schema and indexing, its selection principles and framing radically redefine America as the United States. Unlike the original bibliography, the vast majority of the works included are in English, with few in Spanish and even fewer in indigenous languages. The search interface offers \"subject\" options that uncritically sort the entire span of New World history into U.S.-based periodizations: colonial era, early republic, antebellum, postbellum, and so on. These silent omissions both assume and reinforce the conflation of \"America\" and \"United States.” When a database that claims to be “drawn from Joseph Sabin’s famed bibliography” and, like it, to “cove[r] four centuries of life in North, Central, and South America, and the West Indies,” returns overwhelmingly English-language sources from the “colonial era,” or fails to produce a single hit for one of the most prominent Mexican historians of the nineteenth century while returning dozens for his U.S. counterpart, the effect is not just inaccurate but deeply pernicious. I will argue that this dramatic shift is not so much a function of digital remediation as of a changed scholarly infrastructure that cannot accommodate the capaciousness of “Americana” in its earlier bibliographic sense. The logic of nineteenth-century Bibliotheca Americanas, I suggest, invites us to think otherwise, offering an alternate bibliographic framework that might inform the development of non-proprietary digital systems for bibliographic control.  I will conclude by considering my own work towards this end in the context of the Digital Bibliotheca Americana project. It assembles a freely-available dataset that re-centers indigenous and Spanish-language texts, offers insight into the contours of Americana at scale, and enables computational analysis of the material and conceptual relocation of “Americana” to the United States over the course of the nineteenth century. ",
       "article_title":"An Archaeology of Americana: Recovering the Hemispheric Origins of Sabin’s Bibliotheca Americana to Contest the Database’s (National) Limits",
       "authors":[
          {
             "given":"Mary Lindsay",
             "family":"Van Tine",
             "affiliation":[
                {
                   "original_name":"University of Pennsylvania, United States of America",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "globalization & digital divides",
          "digital ecologies and critical infrastructure studies",
          "media archaeology",
          "historical studies",
          "film and media studies",
          "library & information science",
          "multilingual / multicultural approaches",
          "English"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Statistics describing the inequitable conditions for women in global film industries have been gathered and circulated for more than 30 years. These statistics have barely deviated despite the development and application of a range of equity policies. In some instances the participation of women has become marginally worse. Furthermore, the repeated release of poor equity data has given the industry’s structural misogyny an air of inevitability. This situation is not unique to the film industry.  Our project uses newly available forms of data and data analysis to proose innovative strategies for redressing the systemic and frequently personal bias against women in two different “merit based” industries – the film industry and academic grants schemes. Using data derived from the Australian, Swedish and German film industries as well as data from two different Australian research grant schemes, we propose, compare and evaluate several approaches to controlling collaborative network evolution in order to increase network openness. Our approach is informed by the findings of a major longitudinal study which found that “female actors have a higher risk of career failure than do their male colleagues when affiliated in cohesive networks, but women have better survival chances when embedded in open, diverse structures.” (Lutter 2015)  This project rests on two inter-related manoeuvres then. Firstly, it flips the object of analysis. If we are going to make these industries a better place for women and other minorities then we need to understand the specific operations of gatekeeping that maintain the dominance of white, cis men. The second aspect of the project is to use the data we have collected about specific collaboration networks to propose an innovative course of action to change male dominated, exclusionary environments. This data, on creative roles in films and on researchers receiving grants, contains not only information about the characteristics of projects and all the people involved but also, equally importantly, relational data that enables us to look into the connections within and across teams working on films or research projects respectively. Social network analysis (SNA) provides methods for visualising these group relationships, and through quantitative measures that characterise network structure, provides methods for identifying strategically important components and participants in the network. It also therefore points to ways in which these networks can be most effectively “dismantled” or opened up.  Network visualizations are useful for observing the implicit structure in the collaboration data, for understanding the scale of the problem and for identifying the key connected players. By adding the dimension of gender to these network visualizations we can clearly see the influence of gender on patterns of domination. In addition to making the existing network patterns visible our further concern was to see beyond these patterns and look for ways in which the data could suggest the most effective interventions for challenging and changing the status quo.  In this regard, network visualizations enable us to quickly identify outliers, and easily demonstrate the discrepancies between a given network and the more open reference network we would like to achieve. By depicting changes in both the network structure and its components, visualizations can facilitate the process of testing different policy proposals for achieving social change in organisational or industrial settings and can be used to monitor the emergence of new patterns (especially unwanted ones). There is some precedent in approaching network visualizations in this way. Crime experts and counter terrorist specialists have used “criminal network analysis” for example to identify opportunities to undermine the coherence of dominant groups. Drawing on the literature on the use of social network analysis to characterize criminal networks and identify key nodes whose removal would disrupt the network (i.e., Borgatti, 2006; Rostami & Mondani, 2015; Schwartz & Rouselle, 2009; Réka A., Hawoong J., & Barabási A-L., 2000), we investigated the network of male-only producers and other creatives in the film industry and male-only networks of researchers in the university sector. We investigated the impact of key players in these networks, and the hypothetical impact of removing different key players. Specifically, we used Borgatti’s network fragmentation factor (F) (equation 4 in Borgatti, 2006) as a quantitative measure of network disruption. In this equation, the F value is 0 when there is no fragmentation in a network (all nodes connected in a single component), and is 1 when all nodes in a network are isolated. Using an iterative script, F was calculated for the initial network, a node was removed, and then F was recalculated to assess the impact of the node removal on network fragmentation.  At each iteration, the increase in F obtained from removing a range of male producer or male researcher nodes from the initial networks was calculated and compared. The large(est) male producer/researcher node in the centre of a given network suggests itself as a node whose removal would significantly increase the network fragmentation, and it was indeed the case that this change yielded the largest increase in network fragmentation. Those male producer or researcher nodes whose removal from the initial network yielded relatively large increases in network fragmentation were also observed to have relatively high values of ‘betweenness centrality’, as computed for the initial network. Node betweenness centrality measures how often a node appears on shortest paths between nodes in the network. A high betweenness centrality in the initial network provides a heuristic for identifying candidate nodes for removal that would significantly increase the network fragmentation. This paper will present the project’s findings on the best strategies for dismantling domination patterns and behaviours in collaborative networks, one of them being removing the nodes with the highest betweenness centrality, or in the case of male dominated collaboration networks, removal of the men we call the “gender offenders”. ",
       "article_title":"Solving the Problem of the “Gender Offenders”: Using Criminal Network Analysis to Optimize Openness in Male Dominated Collaborative Networks",
       "authors":[
          {
             "given":"Deb",
             "family":"Verhoeven",
             "affiliation":[
                {
                   "original_name":"University of Technology Sydney, Australia",
                   "normalized_name":"University of Technology Sydney",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/03f0f6041",
                      "GRID":"grid.117476.2"
                   }
                }
             ]
          },
          {
             "given":"Katarzyna",
             "family":"Musial",
             "affiliation":[
                {
                   "original_name":"University of Technology Sydney, Australia",
                   "normalized_name":"University of Technology Sydney",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/03f0f6041",
                      "GRID":"grid.117476.2"
                   }
                }
             ]
          },
          {
             "given":"Stuart",
             "family":"Palmer",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Sarah",
             "family":"Taylor",
             "affiliation":[
                {
                   "original_name":"RMIT Univerrsity",
                   "normalized_name":"RMIT University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/04ttjf776",
                      "GRID":"grid.1017.7"
                   }
                }
             ]
          },
          {
             "given":"Lachlan",
             "family":"Simpson",
             "affiliation":[
                {
                   "original_name":"Independent Researcher",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Vejune",
             "family":"Zemaityte",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Shaukat",
             "family":"Abidi",
             "affiliation":[
                {
                   "original_name":"University of Technology Sydney, Australia",
                   "normalized_name":"University of Technology Sydney",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/03f0f6041",
                      "GRID":"grid.117476.2"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "graphs",
          "creative and performing arts",
          "networks",
          "film and media studies",
          "feminist studies",
          "relationships",
          "English",
          "including writing",
          "gender studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Concert goers are growing accustomed to performers, particularly small ensembles, who bring a tablet computer on stage and place it on the music stand before they begin to play. Perhaps less noticed, a silent tap on the side of a screen, or the nimble pressing of a foot pedal has replaced the rustle of page turning. Notwithstanding an observable increase in the use of digital devices to study and perform music, the transactions between score and performers are only mildly affected: the digital score primarily acts like its paper counterpart: a static, largely reliable instrument for learning and performing a given work of music. The flexibility of the digital medium, as opposed to something fixed on paper, calls for a more modern concept of the score, one that undermines its prescriptiveness by making room for material and features targeted at supporting performers in their interpretation and advocacy of a musical work. What would such a truly digital score look like? And more importantly, what are the critical instruments necessary to understand its impact on performance and music making at large?  Scholarly digital editions, through carefully encoded music notation, are leading the re-definition of the digital music score and are tackling some of these questions, but these publications often take the form of rich websites apt to careful study, but less for performance practice. The role of dynamic digital scores for performance still demands investigation. This paper will present three experiments based on the same technical application, two of which involve newly composed music, that require a performer to use a digital score as it changes its shape based on factors out of the performer’s control. The experiments In these experiments, we use location-based weather data obtained from the web to introduce variation; as a factor that neither the composer or the performer can control, weather data provides a mechanism for driving change in the music notation.   Meteomozart , the earliest of our experiments, is a dynamic score of Mozart’s Piano Sonata No.13 in B♭ major, K.333/315c. At the time of writing, the score only includes the first theme of the first movement (about 60 measures).  Meteomozart adjusts the score based on the weather at the performer’s location (or at a location set by the user). Different slurs and dynamics are shown, taken from four sources: Mozart’s 1783 manuscript, the first printed edition (1784) and two performing editions by Bartók (1911) and Saint-Saëns (1915). This sonata is often discussed as an example of the lack of clarity in Mozart’s slurring, George Barth, for instance discusses how slurs in the sonata were changed substantially from the autograph manuscript to the first edition, and more dynamics were introduced. In  Meteomozart, certain weather condition (e.g. sunny, cloudy, rainy, etc.), obtained via the Dark Sky API, will change the score to show slurs and dynamics from one of the four sources. For example, Mozart’s autograph comes up in clear weather, while Bartók will show up in stormy weather. While there is no scholarly reason to associate weather conditions with specific editions, the driving idea behind the experiment is to take away some control from the performer, or rather, to make it more obvious that some control is always taken away in a printed edition. Editors typically build an understanding of the historical contingencies that makes one version better than the other; also, it makes sense to provide a clean text that performers can pick up and play. But often there are very good reasons for not making decisions for the performer (Chopin, for example, published versions of his works in three countries all with minor differences, all published around the same time; which one is “correct”?).  Meteomozart presents a slightly unpredictable text instead of a clear one. It is obvious from audio recordings that the same work of music can be performed in more than one way; likewise, this experiment tries to make it obvious that the score may have more than just one “text”. What would happen if performers took control of the variants as opposed to having the weather determining them?   Meteomozart’s software was re-purposed for a collaboration between composer Joseph L. Arkfeld and the digital humanities scholar Raffaele Viglianti. The first experimental composition resulting from this collaboration was a piano piece,   Chance of Weather  , a modern take on a piano program piece that takes inspiration from weather conditions. Rather than focusing on a specific condition or setting, such as Debussy’s  Jardins sous la pluie, Chance of Weather engages with the weather that is currently affecting the performance environment. The piece invites the outside world into the performance space, which is usually sterile to the elements. If the audience had to walk through a windy and rainy evening to get to a windowless, temperature controlled performance space, they will find that Chance of Weather evokes the gusts and the dampness of their day. Likewise, if they came on a pleasant warm afternoon, the piece will reflect their recent experience.   A second composition,  Blue Bird, deploys this technique with new parameters by setting to music a fragmentary and variant-rich poem by Emily Dickinson. The unfinished and fragmentary state of Dickinson’s late poetry itself provides a point of departure for dynamic notation. Text-setting for a digital dynamic score presents its own unique challenges apart from instrumental music that must be addressed in order for the music to be singable. One of the most directly accessible permutations would be for the text to change between iterations, which would demand a non-traditional text. Dickinson’s text at the heart of  Blue Bird was located on Marta L. Werner’s database  Radical Scatters: Emily Dickinson’s Late Fragments and Related Texts. Spanning Dickinson’s final years, this collection includes facsimiles and transcriptions of letters, full compositions, drafts, and manuscript copies found after her death. Werner identifies one of these documents as a “trace fragment,” and, akin to leitmotifs, these fragments exist both autonomously and as parts of larger compositions. Like many of her published works, the constellation of fragments selected for this piece was not titled by Dickinson;  Blue Bird is titled after the longest fragment’s subject out of 6 total. These fragments lend themselves naturally to a musical style that kaleidoscopically hovers around a sound space, given their contradictory instability and tight interrelation.  Blue Bird not only sets to music Dickinson’s text, but also its textual condition.  To determine what texts will be shown for the performers, the score will use the past 24 hours’ apparent temperature and cloud cover data from the location of the performance obtained via the Dark Sky API. The musical setting communicates the relationship between weather and the chosen fragment of text. While it would be possible to use more data points from the API, Using more data points with relatively limited text would produce different settings of the same text, rathern than the more tightly through-composed aleatoric composition that is  Blue Bird.  The dynamic scores: encoding and presentation The scores of the experiments described above are encoded with the Music Encoding Initiative XML format, which provides a number of strategies for encoding textual variance and ambiguity. Specifically, the encoding uses elements defined for encoding variants across textual sources and critical apparatus. We define a list of “sources” that correspond to predetermined weather patterns; these correspond to <rdg> elements throughout the text that are grouped within the <app> element when certain musical text changes depending on the weather condition.    Ex. 1: Example use of <app> and <rdg> in  Chance of Weather.  Besides including text in direct alternation, variation is also included by translating musical ideas to different locations in the piece. In order to do this effectively and avoid encoding the same music notation multiple times, these units will be encoded in separate files and included via XInclude operations. The dynamic scores are published as a dedicated website by using Verovio, an engraving engine for MEI. By producing SVG output that maps directly to the underlying MEI encoding, Verovio makes it possible to locate the pre-composed variants in the text and manipulate the score according to weather data ",
       "article_title":"“Fortitude Flanked with Melody:” Experiments in Music Composition and Performance with Digital Scores",
       "authors":[
          {
             "given":"Raffaele",
             "family":"Viglianti",
             "affiliation":[
                {
                   "original_name":"University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Joseph",
             "family":"Arkfeld",
             "affiliation":[
                {
                   "original_name":"University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "electronic literature and digital arts",
          "creative and performing arts",
          "hypertext",
          "music",
          "including writing",
          "English",
          "interface & user experience design/publishing & delivery systems/user studies/user needs",
          "encoding - theory and practice"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" In the wake of Michael Brown’s murder in Ferguson, Missouri, on August 9, 2014, and the non-indictment of police officer Darren Wilson on November 25, 2014, backlashing protests and riots took to the streets of Ferguson and to other major American cities across the country. They also took to the Twittersphere. A national conversation about police brutality and the American criminal justice system exploded on Twitter during this time period, eventually elevating the hashtag #Ferguson, tweeted over 27 million times, to the most frequent in Twitter’s ten-year history, and the hashtag #BlackLivesMatter, tweeted over 12 million times, to third place (Sichynsky, 2016). First coined by Alicia Garza, Patrisse Cullors, and Opal Tometi in July 2013, the hashtag #BlackLivesMatter became a banner for a national protest movement and an index for conversations about the systematic devaluing and elimination of black life. Over the last five years, literary scholars and historians have noted that, within this massive social media movement, the novelist, essayist, and civil rights literary icon James Baldwin seemed to be often and increasingly invoked (Maxwell, 2016). The perceived frequency of Baldwin-related tweets has been pointed to by many as evidence of the Harlem-born author’s 21 st-century resurrection and recent political resonance (Glaude Jr., 2016; Robinson, 2017). Because tweets can be digitally archived and made computationally tractable, they can be collected, measured, and analyzed at scale, and they can offer a picture of Baldwin’s social media reception that goes beyond perception and anecdotal evidence. This talk will share work-in-progress from my project  Tweets of a Native Son ( http://www.tweetsofanativeson.com/), which brings large-scale social media data and computational methods to bear on Baldwin’s 21 st-century remediation, recirculation, and reimagination. This talk will discuss the methods and progress made in the project thus far, argue that social media analysis might usefully contribute to a growing body of computationally-assisted scholarship focused on readership, reception, and textual circulation, and finally gesture to how such an approach might change our understanding of how texts are shared between communities of people, namely through its emphasis on networks.  Methods, Analysis, Initial Findings First I “hydrated,” that is, retrieved the full JSON information for, an archive of over 32 million tweets that were sent between June 1, 2014 and May 31, 2015 and that mentioned Ferguson, Black Lives Matter, and 20 other black individuals who were killed by the police during this time period, which was first purchased from Twitter and shared by Deen Freelon, Charlton McIlwain, and Meredith D. Clark (Freelon, McIlwain, Clark, 2016). I next searched for all the tweets that mentioned “James Baldwin” by his first and last names using the Python and command-line tool “twarc” and the command-line JSON processor “jq,” which returned 7,326 tweets and retweets. By using twarc utilities, a k-means clustering algorithm, and manual tagging, I then identified the most retweeted tweets in the archive and the text that appeared most often across all tweets in the archive, which revealed that the most frequent appeal to Baldwin during this time period was through quotation and overwhelmingly through the quotation of Baldwin’s 1960s-era essays, radio interviews, and television appearances.  By studying the text of the most retweeted and most frequently cited tweets, and by tracing tweeted Baldwin quotations back to their literary and historical origins, my project argues that Baldwin’s appeal as a #BlackLivesMatter muse comes, at least in part, from the remediation of much of his non-fictional work into YouTube videos and free online essays; from his aphorisms with deep roots in African American written and oral traditions; and from his sympathetic proximity to but never full embrace of black radicalism. Another goal of  Tweets of a Native Son, however, is to let others explore, hypothesize, and learn about Baldwin’s #BlackLivesMatter-related social media reception through a series of interactive data visualizations on the project’s website. These interactive visualizations are meant to provide a perspective on Baldwin’s living legacy, a refracted vision of Baldwin’s life and career through those who actively called upon him in a moment of political and emotional urgency, a means by which others can come to their own conclusions about Baldwin’s resurrection.  DH Reception Studies and Networked Reading  Tweets of a Native Son most broadly hopes to join and affirm recent digital humanities work that is trained on readership, reception, and textual circulation, such as Lincoln Mullen’s  American’s Public Bible and Ryan Cordell and David Smith’s  Viral Texts, and to amplify Katherine Bode’s call that the digital humanities better attend to and account for the ways in which literary texts “circulated and generated meaning together at particular times and places” (Mullen, 2016; Cordell and Smith, 2017; Bode, 2017). Like the 19 th-century newspaper archives used by Mullen, Cordell, and Smith, social media archives offer a window into how texts travel, how texts are used and changed by individuals, and what these texts mean in context. Social media archives additionally offer massive amounts of (relatively) clean, recent data. Though of course with these advantages, they also present more ethical challenges, since this data is often tied to corporations and produced by still-living human beings whose consent, possible harm, and creative attribution must always be considered.  Finally, however, I believe that social media data might help us better theorize and make visible the networked structures of readership, reception, and textual circulation, because social media data, such as Twitter data, is often inherently networked in structure, recording retweets, replies, follower communities, hashtag communities, and more. This networked structure emphasizes the way that texts are not only engaged with by individuals but are shared between individuals, taking on social and communal meanings. For the particular case of Baldwin and #BlackLivesMatter in 2014-2015, the quotations of Baldwin’s words were often recirculated as coalition- and community-building material, helping to forge connections between individuals across space, time, and American history. During the future stages of this project, I hope to employ network science and network visualization to better understand Baldwin’s significance within #BlackLivesMatter.  ",
       "article_title":"Tweets of a Native Son: James Baldwin, #BlackLivesMatter, and Networks of Textual Recirculation",
       "authors":[
          {
             "given":"Melanie",
             "family":"Walsh",
             "affiliation":[
                {
                   "original_name":"Washington University in St. Louis, United States of America",
                   "normalized_name":"Washington University in St. Louis",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01yc7t268",
                      "GRID":"grid.4367.6"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "graphs",
          "computer science",
          "networks",
          "film and media studies",
          "social media",
          "relationships",
          "black studies",
          "English",
          "literary studies",
          "digital activism and networked communities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Letters stand as one of the most extensive sources of information on daily life in the early modern period and the study of epistolary culture(s) is a vital and growing area in Renaissance studies (see Daybell and Gordon, 2016; Daybell, 2012; Del Lungo Camiciotti and Pallotti, 2014). Access to such archives and collections is rapidly expanding – and changing – in the wake of mass digitization, online editions, OCR and federated search. In this paper I explore the extension of the narrative of archival history and epistolary provenance into the digital realm. Specifically, I compare the contextual afterlife of early modern letters in nascent state archives to their representation in the digital world, with particular emphasis on classification and metadata, surrogacy and access. Going beyond paralleled modern and early modern anxieties of information overload (the standard comparison of the print and digital revolution), this allows me to explore issues of access, search, and retrieval; control, preservation, and loss, then and now. This is an under-studied area ripe for discussion, and this paper aims to test these ideas in preparation for a wider study that connects the gathering, transmission, and preservation of political information in the early modern period to the digital life of these material primary sources and to our lives as digital researchers. There is a ready parallel to be found between the burgeoning administrative and institutional drive to preservation found in the early modern period – essentially the evolution of state archiving – and the informational anxieties of the internet age, where that largest of archives can offer everything and nothing, excess and restriction, results or dead ends. I explore tensions around archives facilitating both preservation and forgetting, which finds its apotheosis in the endless loss and abandonment of digital data, and in digital methods of retrieval as strict gatekeepers (a roulette of keyword search, privations of metadata, and dreams of text analysis). I will use the concept of copia, fundamental to early modern humanism and classical pedagogy, to explore these twin pressures of abundance and lack, of meaningful quantity and meaningless repetition. Copia in the early modern period referred to the abundance of language, where mastery over the myriad ways of expressing a single idea gave students the rhetorical strategies to navigate the vast expanse of language. The incessant imitation of classical models, particularly concerning letter-writing, was encouraged not least by Erasmus in the wildly popular  De Copia, and became a ubiquitous part of humanist education. This concept, of expertise and thus authority being created by sheer mass, by repetition, is particularly apposite when considering rhetoric and knowledge creation today. In fact, this abundance was framed as both knowledge and folly, particularly from the late sixteenth century, when the drive to systematizing information and rise of scientific method pushed against classical humanism (Francis Bacon offers a good example of a writer in this transitional time who both criticized copia and performed it in his criticism). Christine Hoffman, in her recent  Stupid Humanism: Folly as Competence in Early Modern and Twenty-First-Century Culture, has also identified a productive parallel here, and links early modern rhetorical strategy and modern excess in online ‘news’ and social media (Hoffman, 2017). I will focus rather on early modern and digital archive creation in order to explore how access to these constituent building blocks of knowledge shapes our historiography and thus the world we construct. I will use copia’s two semantic faces, abundance and copying, to firstly think through wider preoccupations with sheer tides of (often repeated) information acting as knowledge and secondly to consider our creation and reception of digital surrogates of primary sources. The early modern relationship to copia as copying is complex: on one hand it is intimately related to the massive growth of bureaucracy and paperwork, where the copy is the transmitter of authority, on the other hand the advent of print and the selfsame abundance of paperwork led to associations of degradation and inferior quality. If print could be alternately the thing itself or the degraded copy of the original, the digital surrogate today is held intermittently as the preserved, unquestioned work and as the flat representation that has lost both the materiality and authenticity of the original.  I will combine discursive reflection with concrete examples that draw parallels between early modern and modern concerns, to consider how the preoccupations and experiences of a particularly early modern growth of the archive and associate concern with amassing, preserving and accessing information inflects our understanding of the internet age, and vice versa.  I draw parallels between search and preservation concerns today and amongst early keepers of the state paper office,  demonstrating that long-held anxieties around access and information overload continue through into the digital archive. I place the digital archive in a history of indexing and cataloguing, which capitalizes on recent interest in early modern construction of metadata witnessed by Oxford University’s 2017 conference ‘The Book Index’, for example.  In both this history of information management and in our relationship to archiving today, what is kept, who has access, and how meaningful and stable that access is, are all essential questions. No less do we need to engage critically with the term access in an increasingly business and market-driven university sector. I point to the open-access philosophy as increasing the availability of digital primary sources, particularly around libraries and archives releasing high-quality digital images and the IIIF initiative making sharing images increasingly possible on a practical level, and to the untold opportunity for connecting resources in the abundant meta-archive promised by the LOD philosophy. From endorsed letters held in the labelled drawers of the Elizabethan Secretary of State’s office, to authority files and shared standards, metadata has often been our key to texts. It both enables and restricts our access, it channels our vision and helps construct our understanding of our sources. Understanding the stories of what is kept and the nuance of how it is described is vital to reveal the limits of this vision. We cannot forget that archives are variously permissive, proscriptive, and problematic, constructed through a history of loss, antiquarianism, colonialism, class and power structures: those with power leave records, those with power control them. With the risk of reproducing existing and long-standing power structures in our digital representations of the early modern world, we need to ask what we want from the modern digital archive, and who will be invited in?   How people create, access, and preserve (particularly political) information can tell us much about the power structures, value systems and personal concerns of both the writers and keepers of texts and the society at large.  I argue that understanding the conditions of production and preservation of early modern letters is necessary to fully comprehend their use and meaning: the natural extension of this is that in order to fully engage with these texts we must attend to the methods and conditions of our own access in an increasingly digital scholarly environment. The digital has a natural and increasingly significant place in this textual history: we need to recognize and interrogate the  continuous history or provenance that connects the first moment of textual creation to the most recent instance of representation and remediation in a digital space. Reflecting on this reveals that we can read in what is preserved and how it is described the power structures inherent in the society that creates the archive: this is absolutely true for the digital.  ",
       "article_title":"Abundance and Access: Early Modern Political Letters in Contemporary and Digital Archives.",
       "authors":[
          {
             "given":"Elizabeth",
             "family":"Williamson",
             "affiliation":[
                {
                   "original_name":"University of Exeter, United Kingdom",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-20",
       "keywords":[
          "repositories",
          "sustainability and preservation",
          "knowledge representation",
          "digital humanities history",
          "renaissance studies",
          "library & information science",
          "theory",
          "epistemology",
          "archives",
          "metadata",
          "English",
          "literary studies",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" What constitutes an alignment, different varieties of alignment, or even different degrees of alignment is a topic in need of further interdisciplinary discussion. The co-authors of this paper have been working on the computational alignment of medieval poetry, an exchange that has resulted in the design of a visual analytics system for the exploration of complex textual traditions. The purpose of this paper is twofold: first, to describe how we arrived at the user centered design of the VA system (Heuwing et al., 2016) and second, to introduce an alternative means of alignment, that of Sequence-to-Sequence Models based on recurrent neural networks, that does not oblige the user to adopt a parameter driven approach, but still allows for discovery of baseline potential alignment for subsequent human scoring.  Pre-modern writing exhibiting both textual and performative forms of instability is challenging for alignment. Twentieth-century print editions employed synoptic style layouts for textual traditions where line-level interpolation and excision were most common, as well as rough stanza-to-stanza numbering based on narrative cues in the poem, as in the case of the mid-century edition of the  Chanson de Roland (Mortier, 1940-44). Alignment in print could not be more granular on account of the highly complex patterns of textual recombination found across different redactions.   Sequence alignment algorithms were originally developed in bioinformatics to identify and analyze functional or evolutionary relationships between genome sequences. Unfortunately, these algorithms are not straightforwardly adaptable to the computational alignment of textual traditions rife with orthographic and transpositional variance (Dekker and Middell, 2011). A number of algorithms have been developed and implemented in user centered design models to examine intertextual similarities, but none of them delivers fully satisfactory results for medieval vernacular poetry (Jänicke and Wrisley, 2017a). Our computational alignment compares each line of one edition to each line of another edition, marking all significantly similar line pairs as alignment candidates. Whereas for the human reader such candidates are obviously valid alignments, they are not easy to detect by purely computational means. For example, using CollateX    https://collatex.net/    for aligning a pair of lines from the tradition of the  Vie de saint Marie l’Egyptienne ( Anon_RenartContre1325: Dix sept ans tel vye mena |  Rutebeuf_SteMarie: Dis et set anz mena tel vie) yields the following result:     Having only one word match and one transposed word, the pair of lines would not be classified as an alignment candidate. Whereas morpho-syntactic tagging could be helpful in surmounting the problem of orthographic variance, we are still faced with the problem of word order.  In previous work, we have implemented a user defined parameter system in order to achieve initial alignment results, with subsequent scoring by a specialized user. We developed the “white box” alignment system  iteal    http://iteal.vizcovery.org/    that uses a set of user-configurable parameters to steer the alignment procedure (Jänicke and Wrisley, 2017b):    Edit distance: With orthographically unstable language, variant spellings needed to be taken into account. We define two words as spelling variants if they have the same first letter, and if the string similarity of the remaining substrings is higher than a user-configurable threshold.   Coverage: In order to ensure that a specific proportion of words of both lines are aligned, the user can configure a minimum coverage value of the line.   N-grams: The user can configure the minimum required n-gram size  n that is the largest number of subsequent word matches of both lines.   Broken n-grams: Quite often, the only difference between two lines is a single word in the middle of a line that is either inserted, synonymous, or a transposed stopword. Large n-grams, from this perspective, are not achieved. Thus, we allow the user to consider broken n-grams.    Indeed, a parameter-driven approach has suggested many possible sequential alignments. Traditional scenarios of intertextual expansion or contraction of poetry are visualized quite clearly. Take, for example, the condensation of episodes of Rutebeuf’s  Vie de saint Marie l’Egyptienne in the  Renart le Contrefait that exhibits a conservatism in replication of whole lines or excision of whole lines:     Different redactions of the epic poem the  Chanson de Roland illustrate a more complex, recombinatory intertextuality. The Venice 7 version is double the length of the oldest extant version known as the Oxford version and the Lyons manuscript is 75% the length of the Oxford. Alignment in this case depends heavily on the use of broken n-grams and edit distance since the versions vary significantly in orthography, word choice and order:   Oxford: Ki est de France, si est mult riches hom   Venice 7: Bien est de Franse, mult par est riches hon  and  Oxford: Ja cil d'Espaigne n'avrunt de mort guarant   Venice 7: Ja cil d’Espeigne de mort n’aront garant.  Sequentially the lines above are divergent, and yet semantically they are nearly identical.  Using the aforementioned example from the  Vie de saint Marie l’Egyptienne, an alignment example is considered an alignment candidate by  iteal using a combination of several parameter settings, e.g., a string similarity of 80%, a coverage of 40% and allowing for broken 4-grams:     Different parameter settings yield very different initial alignments for consideration and scoring by the specialized user. Too liberal or too strict of a choice in settings yields either too many possible alignments or almost none at all. In oral literatures textual reuse is not limited to full-line intertextuality, however, but rather exists along a continuum: from small formulaic expressions to partial and full line reuse. It is on this point that  iteal does not allow for more granular scoring of partial line alignments or multi-line segment alignment, as in the examples that follow:   Oxford: Je vos plevis, ja returnerunt Franc.   Venice 7: Je vos plevis, ja sera il tornez,   Lyons: je vos plevis sempres ert retornant  and  Anon_RenartContre1325: Ainsi paist comme beste mue.   Rutebeuf_SteMarie: Si comme une autre beste mue.  To make matters more complex, rewriting of medieval texts engages with different genres and prosodies as well as jumping back and forth between poetry and prose.  Iteal does not perform optimally yet with different forms. Our research, thus far, has focused on poetry, where the common denominator across textual redactions is the poetic line. Below we see some examples of alignments across versions of the  Vie de saint Alexis (one written in octosyllablic verse and the other in decasyllablic), 3-grams matches produce simply too many false alignments to be valid. Alignments based on 4-gram point to common narrative leitmotifs within the text, such as the force against which the saint resists, his father’s home as a setting:   AlexisOctP: Treire par force et par engin   AlexisPRI: II me prendront par force et par poeste  and  AlexisOctP: Que il laissa en la maison son pére   AlexisPRI: Enz la meson son pére issi.  Whereas we implemented the calculation (or exclusion) of alignments using a medieval French stopword list, this is not necessarily valid across our samples, as the proposed alignments below illustrate:  AlexisPQ: Adonc le fist son pére de l’escole partir   AlexisP11: Il le nonçat son pedre Eufemien  and  AlexisPQ: Ad un des porz qui plus est pres de Rome   AlexisP11: Li uns des pers de Romme c’on nommoit Contantin.  Whereas the latter set of aligned lines satisfies a computational condition of a broken 4-gram and minimum coverage of 40%, ultimately the alignment seems silly to a human reader for the collapsing of two substantives,  porz [seaport] and  pers [great men].   A parameter-driven “white box“ system might seem appealing for its algorithmic transparency in the alignment of medieval text versions, however, we are now turning to an alternative “black box” solution that employs Sequence-to-Sequence Models based on recurrent neural networks (Sutskever et al., 2014). While this idea was not implemented initially, as it makes it difficult to backtrack, our work has begun to migrate to such models (Cho et al., 2014; Bengio et al., 2015). As opposed to a parameter set with its concomitant results, the recurrent neural network system functions with requisite semi-automated training indicating which alignments are appropriate, and which ones are not. While taking into account the contexts in which words appear, the neutral network suggests alignment candidates. We can deliberately map  Line-i-of-Edition-A to a certain hash value, and likewise its variant  Line-j-of-Edition-B, thereby training the neural network to find the candidate  Line-k-of-Edition-C automatically, in turn mapping similar lines to the same hash value.  The potential of this computational shift is that we can further nuance the palette of possible alignments, without remaining bound to the traditional starting parameters. We plan to move beyond our original model of line to line comparisons and to accommodate other units of comparison. By presenting the results of work in progress in the final section of our paper, we intend to explore whether recurrent neural networks produce results for similar text genres and prosodies.  ",
       "article_title":"On Alignment of Medieval Poetry",
       "authors":[
          {
             "given":"Stefan",
             "family":"Jänicke",
             "affiliation":[
                {
                   "original_name":"Leipzig University",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          },
          {
             "given":"David Joseph",
             "family":"Wrisley",
             "affiliation":[
                {
                   "original_name":"New York University Abu Dhabi",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-05-02",
       "keywords":[
          "philology",
          "french studies",
          "text analysis",
          "visualisation",
          "English",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Automatic nationality detection of authors writing in the same language (such as Spanish) can be used for many tasks, like author attribution, building large corpora to analyse nationality specific writing styles, or detecting outliers like exiled or bilingual authors. While machine learning provides many methods in this area, the corresponding results are usually not directly interpretable. However, in the Digital Humanities, explainable models are of special interest, as the analysis of selected features can help to confirm assumptions about differing writing styles among countries, or reveal novel insights into country-specific formulations.  In this work, we aim to bridge this gap: Our assumption is that nationality or country of origin of an author is strongly connected to their writing style. Thus, we first present a machine learning approach to automatically classifying literary texts regarding their author’s nationality. We then provide an analysis of the most relevant features for this classification and show that they are well interpretable from a literary and linguistic standpoint.   Related Work The problem of detecting regional linguistic differences is at the core of Digital Humanities, as it touches research questions in both traditional linguistics and modern computer science. In Spanish philology and linguistics, the analysis of different regional varieties has a long tradition (see for example Alvar 1969, Eberenz 1995, Noll 2001). There are well-known differences between the Spanish spoken and written in Spain itself and the variations used in the formercolonies, for example in forms of address (“vosotros/ustedes” vs. just “ustedes”, voseo) and articles (le/la vs. lo). 1 More recently, these differences have been investigated with quantitative methods, for example by applying Zeta to find distinctive words for novels from Spain and from Latin America, respectively (Schöch et al. 2018).    Model  Baseline SVM-Model for classifying author nationality We assume that writers from different countries are distinguishable by a) their vocabulary and b) phrases that are more or less popular in different regions (cf. Section “Related Work”). Thus, we choose to use an n-gram model to represent our corpus in a computer readable way: First, we determine all word n-grams of length 1 to 4 in the corpus. Then, we select the 1000 most frequent n-grams of each length. We also tried selecting the 100 or 10000 most frequent n-grams, which led to slightly worse results. We represent a piece of text as tf*idf vectors of these n-grams (see Manning 2008). We then train a linear SVM (see Steinwart 2008) to predict the nationality of an author given a piece of text. The linear SVM is known for good results in text classification (Joachims 1998) and - essential for interpretability - allows to inspect the importance of specific features.   Enhancing Feature Interpretability When examining our classification model, we observed an over-representation of geographical entities (e.g., frequent locations like Buenos Aires) as well as names. To instead enforce linguistic properties, we replaced all uppercase tokens by distinct UNKNOWN-tokens (except at the beginning of a sentence). For example “¡Oh, María, María! ¡Cómo deseaba triunfar, conquistar Buenos Aires [...]”, becomes “¡Oh, UNK 1, UNK 2! ¡Cómo deseaba triunfar, conquistar UNK 3 UNK 4 [...]”. This ensures that n-grams with proper nouns will never be frequent enough to be used as a feature in our classification task.    Augmenting Training Examples The success of machine learning algorithms depends largely on the amount of training data. Thus, to increase the number of training samples, we split each novel into multiple segments of equal length, assigning each segment the same label as the entire novel.The cross validation split was performed before segmentation, ensuring that no novel was present in both training and test set. The classifier is then trained and evaluated on individual segments, resulting in a set of “votes” for the nationality of each novel in the test set. The nationality is then established by majority vote.     Corpus We use a corpus composed of 100 novels from four Spanish-speaking countries, specifically Spain, Argentina, Cuba and Mexico, written in the 19th and early 20th century (Calvo Tello 2017, Henny-Krahmer 2017). Figure 1 shows the distribution over countries and the distributions over subgenres in the countries. All countries are represented by a roughly equal number of texts. We note that our corpus may have a bias towards a specific subgenre in some countries, which will later be addressed in the analysis of the features.    Figure 1: Distribution of countries and subgenres in our corpus    Experiments We performed extensive experiments on the dataset to determine the accuracy of our approach. The main hyper-parameters of our model are the segment size s, determining how many words a segment contains, and the parameter C of the SVM. We performed parameter optimisation by grid search, choosing from s ∈ {100, 200, 500, 1000, 5000, 10000, 100000, ∞} and C ∈ {10 −5, 10 −4, …, 10 5 }. The setting s = ∞ does not perform segmentation. We also varied the maximum length of n-grams: unigrams (n = 1) vs. n-grams of length 1 to 4. All scores reported below are weighted average F1-scores over 10-fold cross validation.  Generally, our model performed best when using only unigrams, removing uppercase tokens and splitting the novels into segments of length s = 1000 (see Table 1 for details). Table 1: Classification report for the best configuration, using only unigrams, segments of length s = 1000 and C = 10000      This can be explained by the small dataset: Unigrams are likely to occur in multiple samples even in a small corpus, while higher-order n-grams possibly only occur once and can therefore not be used for classification.  Figure 2 shows the results for varying s and C. Segments of a length around 1000 perform best, yielding F1-scores of up to 86.8 %. Very small segments fail to deliver satisfying results, while larger segments still provide reasonable classification accuracy. The value for C must be set high enough, but the specific value does not matter for C > 10.    Figure 2: Weighted average F1-score depending on the segment size s and the cost parameter C of the SVM. The separated line denotes no segmentation (s = ∞). Only unigrams were used as features.  Using all n-grams of length 1 to 4 also delivered good accuracy (highest F1-score of 80.4% for C = 10000, s = 1000). Removing uppercase tokens had a positive effect when using unigrams, while it hardly influenced the accuracy using all n-grams. A detailed view of all results can be found on GitHub 2.    Feature Analysis Using a linear SVM enables us to analyse the 10 n-grams that provide the strongest evidence for and against a country (according to internal weights). In the following, we focus on features that are weighted strongly in all or at least multiple folds of the cross validation. Generally, we identify three feature groups: topical features, features related to the geographical setting and linguistic features. The presence of topical features can be explained by the bias in subgenres that is present in our corpus and is not necessarily representative. The geographical features seem to point to a tendency of the authors to base their stories in their respective home countries rather than other countries.  With regard to the different model variants, the model based on \\emph{unigrams without removing uppercase tokens} tends to select names as its top-features such as the country itself or characteristic cities, for example “Madrid” for Spain. While these features are surely helpful for classification (yielding an F1-score of 81.7%), they are not particularly interesting for linguistic analysis. The features selected after removing uppercase tokens, on the other hand, seem more relevant from a linguistic viewpoint, while at the same time providing the best accuracy. Table 2 shows features that are among the highest weighted for more than 5 folds for each country in this setting. Table 2: Unigrams with large weights assigned by the SVM. Features marked with + and - are signals for and against a country, respectively.      Using all n-grams without removing uppercase tokens, we again find a preference for geographical phrases like “de la Habana”. As with unigrams, linguistic features become more important than topical features when uppercase tokens are removed. Table 3 shows some particularly interesting n-grams with high weights. Table 3: N-grams with large weights assigned by the SVM. Features marked with + and - are signals for and against a country, respectively.        Discussion  Technical Aspects We found that segmenting novels to augment the training data does improve results, but only if the segments are not too short and thus do not contain enough information to detect the author’s nationality. Removing uppercase tokens improves the classification accuracy and makes the selected features more interesting from a linguistic standpoint. We assume that otherwise proper nouns are picked up by the classifier as important clues on the training set, which fail to generalise to the test set.   Feature Interpretation The words and phrases that our algorithms selects for differentiating between nationalities strongly resemble features that humans would consider given the same task. These include well-known linguistic differences (leísmo) as well as country-specific words (hacienda/huerta). However, it also finds phrases, such as temporal expressions, that are not very well known to be specific for some countries, but should be further investigated in future work. We also observe that authors in our corpus appear to have a strong tendency towards writing about their respective home countries, as evidenced by the selection of city or country names.    Conclusion and Future Work We have presented a classifier that is able to distinguish between novels from different countries based on word n-grams. Our experiments show that this classifier is able to select features that are interpretable and reveal interesting insights into the language used in novels from different Spanish-speaking countries. We note that our findings are only based on a limited dataset. However, the tools we have built enable us to replicate the experiments and confirm our findings as soon as larger collections of text become available. Thus, our work is an important step towards combining machine learning with in-depth analysis and discovery of novel concepts in corpus-based linguistic studies through interpretable models. In future work, we believe that replacing the majority vote over segments by more sophisticated methods can further improve our results. We also believe that incorporating linguistic information like parse-trees into our features can help to reveal more interesting insights into subtle linguistic differences between countries.  1    ,     2     ",
       "article_title":" A White-Box Model for Detecting Author Nationality by Linguistic Differences in Spanish Novels  ",
       "authors":[
          {
             "given":"Albin",
             "family":"Zehe",
             "affiliation":[
                {
                   "original_name":"Julius-Maximilians-Universität Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Schlör",
             "affiliation":[
                {
                   "original_name":"Julius-Maximilians-Universität Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Ulrike",
             "family":"Henny-Krahmer",
             "affiliation":[
                {
                   "original_name":"Julius-Maximilians-Universität Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Becker",
             "affiliation":[
                {
                   "original_name":"Julius-Maximilians-Universität Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Hotho",
             "affiliation":[
                {
                   "original_name":"Julius-Maximilians-Universität Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-26",
       "keywords":[
          "computer science",
          "spanish and spanish american studies",
          "artificial intelligence and machine learning",
          "data mining / text mining",
          "metadata",
          "English",
          "content analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Figure 1:  The Rio VideoWall (Dara Birnbaum, 1989), installed at the Rio Shopping Complex, Atlanta. Image courtesy of the Smithsonian American Art Museum, NEA Birnbaum collection.   “Media Preservation between the Analog and Digital” is a project that centers on the development of a digital recreation of pioneering video artist Dara Birnbaum’s now-lost  Rio VideoWall (1989), the first  multi-screen artwork to be installed in a public setting in the United States. In its original instantiation, in downtown Atlanta, the work employed twenty-five identical 27” CRT monitors, stacked in a five-by-five grid, powered by eight LaserDisc players and proprietary computer code written specifically for the piece. Today, however, only a portion of the code remains; neither the CRT monitors, nor LaserDisc players, nor original computers, are in production, and to recreate the artwork—even in a lab setting—would involve a significant reimagining of the original piece. But there are additional considerations that extend beyond hardware and software: the  VideoWall was installed in  the Rio Shopping Complex, a mall in Atlanta’s Old Fourth Ward, a historically African American neighborhood. The artist was attuned to this, and designed the artwork to  combine scenes of the natural landscape that had been displaced by the mall with an unedited live-stream of CNN, an Atlanta-based company, all filtered through the moving silhouettes of mall patrons in real time. Neither the nature footage nor the CNN live-stream—let alone the mall patrons—presently exist (the mall was torn down in 2000), so a recreation of the artwork would need to identify footage that captures the spirit, if not the reality, of the piece. To do so would therefore involve an analysis of the themes engaged by the original artwork: the legacy of segregation, the 24-hour media cycle, surveillance culture, the relationship between art and commerce, and the Anthropocene. This paper will provide a brief overview of the project, with an emphasis on the conceptual challenges it engages, describe the recovery work underway, and describe the current work and next steps toward the  VideoWall’s ultimate recreation.   Recovering the Rio VideoWall : Conceptual Challenges  Recent seminars and symposia have explored the range of issues associated with doing digital art history (e.g. Harvard metaLab’s “Beautiful Data,” 2015, the Getty Foundation’s “Art History in Digital Dimensions,” 2016). At the same time, the field has devoted increasing attention to issues associated with digital preservation (e.g. the BitCurator initiative, or any number of conversations at the Digital Library Federation). And yet, with its hybrid analog/digital design, and its site-specific setting, the  Rio VideoWall presents a unique case study for thinking through the additional conceptual challenges related to the preservation of public media art. For instance, Matthew Kirschenbaum has argued that every media artifact “leaves a “trace,” by which he means a past that can be unearthed and understood. But what happens when the original artifact no longer exists, as is the case with the  VideoWall, which was dismantled and discarded in 1999? And what is the “trace” that is left by public art that makes use of technology as a material support, as in the  VideoWall’s CRTs and LaserDisc players, which are rarely understood in terms of their material properties? When considering the unique case of the  VideoWall, additional questions arise: Can the artwork’s original public setting can be reimagined in a creative way, perhaps online? Or should the  VideoWall be rebuilt physically, and installed in the city of Atlanta, and if so, where? (The original site has long been built over). Or should a recreated  VideoWall be incorporated into a museum’s collection, so that it can be benefit from institutional resources? But what would be lost by limiting community access?    Recovering the Rio VideoWall : The Digital Archive  The first phase of recovering the Rio VideoWall, as well as planning for its recreation, involves the creation of a public-facing digital archive that documents the remaining materials associated with the  VideoWall’s design, construction, and eventual demise. The archive includes Birnbaum’s original proposals and plans for the piece, her own 35mm slide documentation of the  VideoWall and her recently-recovered video footage of the artwork’s public opening, as well as correspondence, press clippings, and archival photographs of the site. The archive also features exhibits with demographic data and visualizations that illustrate the racial and economic makeup of the areas around the Rio Shopping Complex at the time of the mall’s opening in 1987, at the time of its destruction in 2000, and today. Eventually, the site will also feature oral histories of Atlanta citizens who experienced the work. These histories will provide access to lived and felt experiences often absent from the histories provided by critics and scholars, that are nevertheless an essential component of accounting for the impact of public art within a community.   Recovering the Rio VideoWall : The Digital Recreation  Following the completion of the digital archive, the project team will begin the second phase of the project—reimagining the  VideoWall in digital form. We are currently considering, and mocking up designs for, web-based, physical, and VR-based approaches to recreating the artwork. A web-based version might include visual overlays of the original site of the artwork on the current site. A physical recreation might involve a single, very large flat-screen display that could be separated into adjacent or tiling windows, drone footage of the chosen site, and touch-screen capabilities. A VR version of the work could be situated in a number of virtual locations, and would allow for diverse populations to interact with the artwork from around the globe. A workshop, currently being planned, will bring together scholars, conservators, designers, curators, and community organizers to discuss the computational and creative possibilities of each medium. In addition to describing the conceptual challenges associated with the piece, and highlighting certain key features of the digital archive, the paper presentation will present these mockups and solicit audience feedback in preparation for the final design and construction of the  VideoWall.   ",
       "article_title":"Media Preservation between the Analog and Digital: Recovering and Recreating the Rio VideoWall",
       "authors":[
          {
             "given":"Gregory",
             "family":"Zinman",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018-04-25",
       "keywords":[
          "repositories",
          "audio",
          "sustainability and preservation",
          "art and art history",
          "digitisation - theory and practice",
          "public humanities and community engaged scholarship",
          "film and media studies",
          "video",
          "archives",
          "English",
          "multimedia"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Machine vision learning and art historical practice are often poised as operations that are antithetical to one another (Spratt and Elgammal, 2014a, 2014b). A frequent criticism leveled by art historians against machine learning algorithms is that they do little that a trained art historian cannot do already (Bishop, 2017). A second criticism is that the results gleaned from machine vision learning are heuristic exaggerations. And a third criticism is that computer scientists simply do not understand how to approach visual art, and in the process wrongly (albeit unintentionally) define the field of art history for a much larger audience than the one the humanities tend to generate. Much of the value placed on machine vision learning as it pertains to understanding artworks has been on its ability to sort, classify, and match images with similar ones through style and genre (Saleh and Elgammal, 2015, 2016), taxonomies that fail to reflect the current state of the history of art.  The above criticisms present a fair critique of the approach of machine vision learning to the history of art – but only to a certain degree. Such criticisms fail to recognize that art historians are precisely the ones who have the greatest stake in and the greatest potential for contributing to the questions raised by machine learning image analysis. Art historians simply ask different questions about artworks – questions of history, scale, tactility, surface, and representation – than the ones of which computer scientists are aware. One reason for this disjuncture is that art historians have often kept to themselves instead of engaging with other disciplines that are intensely interested in visual imagery.  Rather than simply critiquing and lamenting how computer and data scientists approach visual imagery, this short paper addresses a few “between points”, as I call them, rather than intersections, where art historians can bring much critical insight into machine vision learning. For example, the issue of texture is a complex question in painting, for it can signify the texture of the paint, or the texture of the canvas weave, or how textured paint application is used to represent different physical textures, such as silk or fur. How could these distinctions be brought into machine vision learning? Another issue would be to see if a machine could identify when a painting was re-touched or repaired. Or one might compare how the descriptive terms generated by machine vision learning output correlate to the terms art historians would use when describing an object. The purpose of this paper is ultimately to pose some questions about how art historians and computer scientists might create a better dialogue in their respective practices. ",
       "article_title":"The (Digital) Space Between: Notes on Art History and Machine Vision Learning",
       "authors":[
          {
             "given":"Benjamin",
             "family":"Zweig",
             "affiliation":[
                {
                   "original_name":"Center for Advanced Study in the Visual Arts, National Gallery of Art, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"Name, Institution",
       "date":"2018",
       "keywords":[
          "art and art history",
          "artificial intelligence and machine learning",
          "digital humanities history",
          "image processing",
          "theory",
          "epistemology",
          "English",
          "criticism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    }
 ]