[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper reports on interdisciplinary work carried out for the Palimpsest project, focusing on mining literary works set in Edinburgh, a UNESCO City of Literature. 1 The project’s aim is to use text mining to scour accessible literary works and find those mentioning Edinburgh or places within it. We ground ‘loco-specific’ passages of text by identifying their latitudes and longitudes, so that scholars and the public can both geographically explore their fictional city. Palimpsest is a collaboration between literary scholars studying the use of place in literature and computer scientists working on text mining and information visualisation. Through a range of maps and accessible visualisations, users are able to explore the spatial relations of the literary city at particular times in its history, in the works of specific authors, or across eras and writers.  We present an overview of the project workflow and describe the assisted curation process adopted. It involves automatic retrieval and ranking of accessible literature according to its loco-specificity followed by manual selection of ranked documents, resulting in a set of literary works identified as set in Edinburgh. We report on the fine-tuning of the retrieval and ranking prototype based on literary scholar annotators’ feedback. Palimpsest Figure 1 shows the Palimpsest workflow. The input data is made of five literary document collections amounting to approximately 380,000 works, most of which are out of copyright, as well as a small set of modern books from authors who are well known for their literature being set in Edinburgh (incl. Irvine Welsh, Alexander McCall Smith, and Muriel Spark). The out-of-copyright collections are varied in content and contain literary fiction and nonfiction genres. The data is first indexed using Indri 5.6 2 and ranked using a set of 1,633 Edinburgh place name queries. 3 We use the Indri inference network language model-based ranking approach (Strohman et al., 2005). The ranking score of a document is increased given certain metadata information (including a set of favoured Library of Congress codes and subject terms) or down-weighted for ambiguous Edinburgh place names. We combine the score for genre in the metadata with the location query retrieved from the content of the book. The output of the document retrieval component is a set of ranked Edinburgh-specific candidate documents per collection.     This data was loaded into a web-based annotation tool for manual curation. All Edinburgh place names occurring in the document and snippets surrounding them were displayed to aid in the annotators’—three literary scholars from the School of Literature at the University of Edinburgh—decision making. The subset of works which were manually curated as Edinburgh-specific are further processed by text mining, which geo-references place names by grounding them to their latitude/longitude coordinates using the Edinburgh Geoparser (Grover et al., 2010) 4 and in particular the Edinburgh gazetteer that is being developed in Palimpsest. The output (geo-referenced location mentions and snippets) is stored in the Palimpsest database, which is accessible via web-based visualisations.  Assisted Curation By assisted curation we refer to the process of semiautomatically curating a set of Edinburgh-specific literature from all accessible literature. Related endeavours have relied on the collection of titles, or passages, by a few individuals or via crowdsourcing (e.g., Edinburgh Reads 5 run by Edinburgh Libraries or Global Bookmap 6). The idea for Palimpsest arose out of an initial prototype that visualises a small set of extracts manually collected by literary scholars at the University of Edinburgh. 7 Such an approach results in high-quality data with the disadvantage of missing less well-known but potentially interesting works. In Palimpsest we consider the entire pool of accessible literature accessible to determine a subset of highly ranked Edinburgh-specific candidates automatically using location-based document retrieval. The aim is to uncover a large range of Edinburgh-specific literature, not only famous and well-read titles. Assisted curation by means of text mining alone has shown encouraging results in other domains (e.g., Kristjansson et al., 2004; Alex et al., 2008). We combine text mining and information retrieval for assisted curation and show how user feedback can improve the technical stages to this process.  The manual annotation of the ranked candidates to select actual Edinburgh-specific literature was done using the annotation tool displayed in Figure 2. All ranked documents are displayed on the left-hand panel, listing the title of each work, the author and publication date if available, a link to the original source document, and a list of location mentions identified within the book. When clicking on a title, additional information appears in the right-hand panel, including a graph showing occurrences of place names within a document and snippets containing Edinburgh place names. Based on this information and by following the link to the original source, the annotators can determine a work as being Edinburgh-specific or not, enter further comments, and identify the start and end content pages of a document. When clicking the submit button, a document annotation is saved to the database and disappears from the panel on the left.    An item can be annotated using the annotation scheme shown in Figure 3. We consider documents annotated as  yes or  yes (except) as Edinburgh-specific within Palimpsest. 8 The scheme was developed by the annotators while working on an initial ranking of HathiTrust documents. 9     We used the HathiTrust collection (253,350 documents) to develop the retrieval and ranking component. This resulted in 20,542 ranked candidate documents containing one or more Edinburgh place names. Over a period of two weeks, the annotators curated the ranked documents in order. This resulted in 1,710 annotated documents, of which 200 were considered Edinburgh-specific literature. Initially, the annotators reacted enthusiastically to the annotation and discovered several works set in Edinburgh of which they were unaware (e.g.,  John and Betty’s Scotch History Visit or  Noctes Ambrosianae). As they worked through the documents, however, they lost trust in the ranking. They noticed relevant documents appearing far down the list and sometimes had to go through many documents to find a positive example. They also recorded a list of ambiguous place names ( High Street or  Trinity) mostly referring to other locations as well as a list of words in titles suggesting nonliterary content ( catalogue or  dictionary). Finally, they observed that most Edinburgh-specific documents contain a reference to  Edinburgh or a variant.  Improving the Ranking Based on this feedback, we then fine-tuned the retrieval component. We used the set of 1,710 annotated works as an evaluation set to determine the effect of a modification. There is a body of research on using relevance judgments for improving information retrieval, a good summary of which appears in Manning et al. (2008). We tested the initial ranking (baseline), the following three measures and their combination:  (a) Down-weighting ambiguous place names identified by the annotators.  (b) Removing documents containing nonliterary title words ( catalogue,  dictionary, etc).   (c) Ensuring that  Edinburgh or one of its variants ( Embra,  Edinburrie, etc.) occurs in the work.  Figure 4 shows that down-weighting of ambiguous place names (a) resulted in a small improvement in average precision (MAP) (Baeza-Yates and Ribeiro-Neto, 1999). Filtering documents with nonliterary title words (b) had the highest increase in MAP. The condition of Edinburgh or a variant to appear in the document (c) decreased MAP slightly. However, it resulted in a large decrease in the number of ranked documents, reducing the workload of the annotators significantly. We therefore consider measure (c) to be beneficial as well. When combining all three measures, the retrieval component yielded an improved MAP score of 0.1684 (compared to the baseline MAP of 0.1307), and the workload of documents to be curated was reduced by 60%.    Conclusion The assisted curation process undertaken in Palimpsest attempts to keep the user in the loop during iterative technical development. We received useful feedback from the literary scholars on issues that appeared as they curated documents and considered their suggestions in changing the underlying methods for ranking Edinburgh-specific literature. Our results show that document retrieval performance improved and curation workload was reduced as a result. The improved method was subsequently applied to all document collections, which resulted in very positive feedback from the curators, reporting that the ranking improved considerably. Acknowledgements Palimpsest is funded by the AHRC (Digital Transformations in the Arts and Humanities—Big Data, PI: Prof. Loxley). We would like to thank Dr Anderson, Dr Otty, and Dr Thomson, who were in charge of the curation, and Dr Harris-Birtill, who helped with the data processing for the annotation tool. Notes 1.  http://palimpsest.blogs.edina.ac.uk/.  2.  http://sourceforge.net/projects/lemur/files/lemur/indri-5.6/.  3. This includes entries appearing in at least three of five resources used to construct the Edinburgh gazetteer (OpenStreetMap, OSLocator, Royal Commission for Ancient Historic Monuments of Scotland, Historic Scotland, QuatroShapes of Edinburgh areas). 4.  https://www.ltg.ed.ac.uk/software/geoparser.  5.  http://yourlibrary.edinburgh.gov.uk/fictionmap.  6.  http://www.mappit.net/bookmap/.  7.  http://palimpsest-eng.appspot.com/.  8. We excluded poetry, but we annotated it ( prob. not) to be able to work on it in future.  9. HathiTrust:  http://www.hathitrust.org.  ",
        "article_title": "Palimpsest: Improving Assisted Curation of Loco-specific Literature",
        "authors": [
            {
                "given": "Beatrice",
                "family": "Alex",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Claire",
                "family": "Grover",
                "affiliation": [
                    {
                        "original_name": "University of Edinburgh, United Kingdom",
                        "normalized_name": "University of Edinburgh",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01nrxwf90",
                            "GRID": "grid.4305.2"
                        }
                    }
                ]
            },
            {
                "given": "Ke",
                "family": "Zhou",
                "affiliation": [
                    {
                        "original_name": "Yahoo Labs, London",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Uta",
                "family": "Hinrichs",
                "affiliation": [
                    {
                        "original_name": "University of St. Andrews, United Kingdom",
                        "normalized_name": "University of St Andrews",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02wn5qz54",
                            "GRID": "grid.11914.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "natural language processing",
            "literary studies",
            "geospatial analysis",
            "information retrieval",
            "text analysis",
            "interfaces and technology",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As quantitative textual analyses are based primarily on words, which are lexical objects that, by definition, carry meaning and information, these methods should offer a clear path that leads from analysis to interpretation. If two groups of texts are well differentiated by author (Burrows, 2003), genre (Jockers, 2013), or period (Hughes et al., 2012) based on word correlations or frequencies, it is reasonable to assume that those words that differentiated them should offer us a straightforward approach to understanding why  those words in particular separated  these texts. Yet this is not the case in most digital humanities findings. In authorship attribution, for example, high-frequency words are valued because they are “content-free” and thus are used ‘unconsciously’ by an author (Burrows, 2003). 1 Even when content-laden words are used as the features, it is often the case, as in the Stanford Literary Lab’s first pamphlet, ‘Quantitative Formalism’, that some complex mixture of proportions of many words does the work at separating something like genre (Allison et al., 2011). All too often, reading these kinds of lists of words gets us no closer to understanding  why two corpora are separated into groups. Even topic modeling, which promises results based on interpretable ‘topics’, only adds to this complex problem because the clusters of words that underlie the model are often only subjectively ‘named’ by the analyst. 2  At the root of this problem is the way in which much digital humanities work treats words as feature sets, rather than objects with interpretable weight. This has done much to uncover hidden structural (or stylistic) features of textual corpora, but has required an interpretive leap, from data to metadata, to explain these structures. The actual features that articulate the differences between texts are not easily integrated into the interpretive process. Instead, they establish the  fact of a differentiation whose  meaning is often explained by extratextual or a priori knowledge. In this paper, I argue that we can begin to mitigate this interpretative gap by deriving more complex, higher-order, quantitative metrics through which we can compare textual corpora. The two metrics I develop in this paper, heterogeneity and redundancy, are both based in frequency data, but  meaningfully translate word frequencies into metrics whose relationship to the features is quantitative but whose relationship to the text as a whole is qualitative—that is, they are metrics that can be derived quantitatively, but that offer information that is immediately relatable to the texts and that can be used to interpret the results in a way that is not only quantitatively rigorous but also grounded in a theoretical method for understanding textual or informational difference.  Heterogeneity The first metric that I develop in this project draws a connection between the quantitative analysis of texts and literary theory by way of Mikhail Bakhtin’s theory of heteroglossia. In ‘Discourse in the Novel’, Bakhtin proposes that we understand the novel as a genre of accumulation and mixture resulting in a heterogeneous object of multiple and overlapping language (Bakhtin, 1981). While poetry is dominated by a single, unitary voice, prose writing, particularly novelistic prose, is defined by a ‘diversity of voices’, according to Bakhtin: the more ‘novelistic’ the text, therefore, the more competing voices there should be and the less like the rest of the text any one part becomes. While it is useful to parse the distinction between prose and poetry, we can also operationalize the theory of heteroglossia to understand differences between and within different type of writing by measuring the ‘distance’ between the different parts of a text, thereby calculating how internally diverse it is. For this measurement, I divide the text into equally sized chunks and calculate the distance (using both a Euclidean distance metric and a Kullback-Leibler divergence) between every part and every other part. By combining these internal distances into a single score and scaling them logarithmically to account for text length, I am able to derive a single number that represents the amount of linguistic diversity between different parts of the text. 3 Texts with a high degree of diversity, or heterogeneity, between their parts would be made up of more internal ‘voices’ following Bahktin. As I show in this paper, this metric of heterogeneity not only separates different kinds of writing, but also writing of different periods, or by different authors or even by different genres. And because the metric is a direct measure of internal diversity, the ways that groups of texts are separated by this metric offers critical information about those groups of texts: texts with a low heterogeneity score are mono-vocal: their patterns of language do not vary across the text. Texts with a high heterogeneity score are highly diverse among their constituent parts and depend upon variety, rather than repetition, to engage their readers.  Redundancy The second metric I describe in this paper is based on Claude Shannon’s work in information theory. While metrics of entropy (or information loss) are common in measuring textual differentiation (the Kullback-Leibler divergence is one such metric), the measure of entropy in these cases is made  between texts and offers only a way to characterize how different two texts are based on their shared word frequencies. The two interventions that I make in this paper are (a) to measure redundancy, the opposite of the informational entropy described by Shannon, and (b) to make this measurement  within rather than  between texts. By slightly altering Shannon’s formula for redundancy (1963) I offer a way to calculate the redundancy of a text that is scaled to the possible combinations of words within the text, rather than the possible combinations of words across the language (as it is traditionally calculated). This again allows me to calculate a single number based on word frequencies (in this case, bigram probabilities) that speaks directly to the information content of the text itself. In other words, the redundancy metric that I describe in this paper not only offers a way to classify different kinds of texts based on their score, but in doing so, it also describes how likely each text is to use repeated word combinations or if a text can be characterized by its tendency to combine words in new ways. This measurement therefore, reveals important information about how the text communicates with the reader that allows me to both describe differences between texts and, more importantly, characterize these differences based on features that are legible at the level of the text.   By combining these two metrics, heterogeneity and redundancy, both of which offer data on how repetition and variety functions within groups of texts, this paper demonstrates how these higher-order features can be used reliably to describe surprising differences between corpora that are uninterpretable through standard digital humanities analyses. In this paper I apply this new combination of metrics to two case studies in long 18th-century literature: genre and canonicity. As I demonstrate how these metrics together can differentiate between different kinds of writing, for example—poetry, prose, and nonfiction (Figure 1) as well as between canonical and noncanonical writing (Figure 2), I simultaneously articulate a theory of how the information content of the texts and their internal repetitions can help explain these groupings. This work also reveals a clear historical pattern among these different groups that is not apparent in any other quantitative analysis of the corpora that I use. These two ways of understanding textual difference therefore not only provide new information on literary history and criticism, but they also offer a way to bridge the gap between the quantitative calculation of clusters and the meaning behind these clusters. Despite their apparent simplicity (both redundancy and heterogeneity use a single number to describe a text), I argue that their actual complexity and their derivation from literary theory and information theory, respectively, allow us to understand the texts that they describe with a much greater nuance and in more critical detail than what we would be able to do from reading the texts, let alone deriving the clusters of the texts directly from word frequencies. What I argue that these two measurements provide is a quantitative aesthetic theory: one that articulates both a quantitative and theoretical way of understanding how texts are grouped and, more importantly, the critical significance of and meaning behind these historical, generic, and textual formations. ",
        "article_title": "Discourse, Design and Disorder: Digital Models for an Aesthetic Literary Theory",
        "authors": [
            {
                "given": "Mark Andrew",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "corpora and corpus activities",
            "bibliographic methods / textual studies",
            "text analysis",
            "english studies",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As a textual phenomenon that seems to rely exclusively on the affective states of an audience, suspense would appear to be well outside of the quantifiable aspects of literature that are available to digital humanities study. Even while suspense can be understood as a genre of writing, this generic label is relegated to a relatively small group of texts, ignoring the way that many other genres—for example, horror, science fiction, mysteries, and thrillers—also make use of the same phenomenon. Compounding these difficulties, suspense is also a descriptive category that can be applied at the intertextual level (works can be ‘suspenseful’) as well as at the intratextual level (certain parts of works can be ‘suspenseful’). Finally, suspense can also manifest itself as an epiphenomenon of narrative: disentangling the ‘momentum’ of narrative propulsion from the existential suspense of genre fiction further complicates studies attempting to isolate the origins of suspense effects. Recent work, however, has suggested that careful digital parsing of narrative might provide clues to the origins of this effect. Richard Doust’s study ‘Narrative Structures: The Case of Suspense’ (2010) argues that if narrative units can be digitally coded, then their pattern can reveal the mechanisms of suspense. While very promising, this work relies extensively on precoded (and thus predefined) narrative units, restricting its generalizability across time and genres. Yet the promise of this work is tantalizing. If it is possible to locate lexical or semantic features that are related to the experience of suspense, we would not only gain a much deeper understanding of what suspense is, but we would also be able to investigate narrative processes at the scale of corpora in a way that current digital humanities work cannot. Moreover, such an analysis would also offer a potential bridge between the often rigidly formalist approaches characteristic of quantitative textual analysis and the primarily subjective domain of the reader’s affective experience. In this paper, ‘Suspense: Language, Narrative, Affect’, a collaborative project by members of the Stanford Literary Lab, we seek to unpack the transhistorical and transgeneric commonalities of suspense literature. Our goal is to isolate lexical and narrative features that, within the right context, create the potential for the experience of suspense. Drawing on previous narratological and psychological studies, our contention is that suspense, as a psychological affect, occurs in the negotiation between the reader and the text (for example, Brewer and Lichtenstein, 1982; Jose and Brewer, 1984; Carroll, 2001; Beecher, 2007; Smuts, 2008). That is, we argue that there are textual effects that create the potential for the experience of suspense, which are only realized by a reader encountering the text within a specific context. As part of our goal in this paper is to trace these textual features both across time and across genres that are, to varying degrees, ‘suspenseful’, we have assembled a corpus of 216 texts from between 1780 and 2013 that were unanimously deemed suspenseful by the project group. These texts not only belong to the genre of ‘suspense literature’, but also to a variety of genres for which suspense is an important component, including Gothic fiction, detective fiction, sensation literature, science fiction, and horror. As a control, we also created a corpus of 107 ‘unsuspenseful’ texts sampled from the same time period. To explore both the textual and affective components of suspense we apply a set of innovative digital humanities methods to uncover textual features shared by suspenseful texts (or suspenseful episodes within texts) combined with collaborative social psychological work to explore how these textual features impact reader experience. Through this project we investigate the following questions: (1) Is, in fact, the experience of suspense common among a group of readers? (2) Are there lexical features that are shared by suspenseful narratives or narrative episodes? (3) Do these features form identifiable patterns within narratives that are common among our historically and generically diverse corpus? In this paper, we present the results of these first three phases of investigation. Expert Tagging Before we could begin investigating the specific textual features that were common among suspenseful narrative episodes, we first sought to confirm that a group of readers could reliably and independently come to a consensus in identifying suspense within a narrative. To this end, our first experiment was to use the project participants (eight PhD candidates in English literature) as a group of expert readers who could tag short stories for the effects of suspense and whose consensus we could reliably measure. Would a group of readers with similar educational backgrounds agree on the suspenseful and unsuspenseful parts of a narrative? Our corpus for this project was 13 short stories: each story was divided into discrete narrative units (roughly corresponding to a paragraph), and each unit was rated by each reader as to how suspenseful it was. Not only did this process demonstrate a remarkable consensus among the project participants, but it also revealed a number of narrative patterns of suspense whose commonalities suggest potential categories of suspenseful narratives (Figures 1 and 2). We are currently in the initial stages of a collaboration with members of the Stanford psychology department that will expand this tagging experiment to a diverse group of participants with different educational, disciplinary, and cultural backgrounds. Distinctive Lexical Elements With the results of the tagging experiment, we then turned to the question of whether the sections of the test corpus that were hand tagged by the group as highly suspenseful shared lexical features, as well as features that were distinctive of our suspenseful corpus compared to our nonsuspenseful corpus. For both, we identified the most distinctive words, the set of words that appeared significantly more often in either suspenseful passages or novels as compared to nonsuspenseful passages or novels. These words not only gave us a set of features through which we could identify potentially highly suspenseful passages in nontagged texts but also suggested some important lexical aspects of suspense. For example, words having to do with movement on a vertical axis (descend, ascend, lower, raise, etc.) were associated with suspenseful episodes and suspenseful texts. We also employed a part of speech tagger to identify the parts of speech that were most indicative of suspense and found a disparity between the number of adjectives that appear in nonsuspenseful texts versus suspenseful texts. Narrative Patterns In the final stage of this project we sought to use our intertextual results to study the narrative patterns of suspenseful texts. Is there a pattern of groups of words, or semantic fields, within a narrative that is indicative of that narrative’s potential to create the effect of suspense? To identify these semantic clusters of words, we ran a latent dirichlet analysis, or topic model, on our suspenseful corpora and combined these results with the distinctive words we identified in part 2 of our project. These gave us discrete meaningful fields whose presence and absence we could trace across the narratives of texts in our corpus. To help identify how the patterns formed by these clusters of words helped create the experience of suspense, we also tagged the highly suspenseful (and highly unsuspenseful) passages in small group of novels from our corpus. By plotting the scaled average of our semantic fields within a moving window across the text on the same graph as our tagged suspenseful passages, we can observe, for the first time, the interaction between textual features and the suspense effect. As we were interested in semantically meaningful clusters of words, as well as potentially parts of speech, we plotted both within these graphs (see Figures 3 and 4). As a complementary measure of investigation, we explored how a text’s reading level varies throughout the course of its plot, and how this metric correlates with suspense. To do this, we applied Age-of-Acquisition (AOA) norms 1 to our texts using a simple dictionary method. This AOA dataset contains 30,000 words, each with a score that corresponds to the mean age at which respondents have reported learning (or acquiring) the word. Once applied to our texts, we computed a moving average of this score to examine how AOA fluctuates in suspenseful plots, and we observed that the suspenseful moments correlate with a drop in AOA—that is, the more suspenseful an episode, the lower its reading level.   Through these three avenues of investigation, our paper explores how specific lexical features—including semantic clusters of words, parts of speech, and Age of Acquisition scores—operate within the narrative of texts to create the potential for the experience of suspense. As our study combines this digital humanities work with the methodologies of social psychology, it allows us, for the first time, to explore simultaneously the formal features of a text that are distinctive of suspenseful texts and how these features interact with readers to create the experience of suspense itself. Not only, therefore, do we offer a new way of understanding suspense as a critical narrative process that jointly operates between trackable textual features and the context in which they are encountered, but we also suggest a new avenue through which the formal and quantitative research of digital textual analysis can be combined with other fields of study to gain a holistic understanding of the aesthetic experience of reading. Figures    Figure 1. Expert reader tagging of passages for suspense in Poe’s ‘Pit and the Pendulum’.    Figure 2. Expert reader tagging of passages for suspense in Saunders’ ‘The Falls’.    Figure 3. Moving average of most distinctive ‘suspense’ (red) and ‘nonsuspense’ (blue) words in Mary Shelley’s  Frankenstein (1818); red and blue bands mark locations of passages tagged for high or low suspense, respectively.     Figure 4. Semantic fields in Stoker’s  Dracula: bands mark passages tagged as highly suspenseful.  Note 1. Dataset made available by Kuperman, Stadthagen-Gonzalez, and Brysbaert (2012). ",
        "article_title": "Suspense: Language, Narrative, Affect",
        "authors": [
            {
                "given": "Mark Andrew",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Chelsea",
                "family": "Davis",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Abigail",
                "family": "Droge",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Tasha",
                "family": "Eccles",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Laura",
                "family": "Eidem",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Morgan",
                "family": "Frank",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Fredner",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "J.D.",
                "family": "Porter",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Shephard",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Hannah",
                "family": "Walser",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "other",
            "interdisciplinary collaboration",
            "literary studies",
            "text analysis",
            "english studies",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In a statement on the meaning of ‘race’ within physical anthropology, Alice Brues (2009) raises the spectre of a mismatch between our biological understanding of human variation, and the work that the word ‘race’ performs within academic, and specifically anthropological, literature: ‘The visible difference between different populations of the world tells everyone that there is something there. We had better be prepared to explain what is there, and why, before we discuss what it does or does not mean’ . Yet contrary to the meaning behind her question, it is apparent that race  already has meaning as a clearly articulated concept in both the popular and academic understanding, whether it exists in an objective biological reality or not. Yet despite its ubiquity, few discourses are more contentious. At best, critics claim that race is cultural construction with no basis in biological fact; at worst, the language of ancestry has been accused of perpetuating the dark histories of racial categorization and eugenics. But our understanding of the evolution of this discourse is clouded both by individual agendas and specific cultural concerns: what we lack, we argue, is a comprehensive and critical understanding of the patterns that the language of race has assumed throughout recent academic discourse, and specifically within the discipline of physical anthropology, whose practitioners are, whether consciously or unconsciously, implicated in this discourse through their work on human variation and forensic identification.   Through this paper, we seek to explore the latent discourse of race that characterizes the anthropological study of human variation. How has the emergence of forensic anthropology and genetics altered the way in which physical anthropologists interpret concepts such as race, ancestry, or ethnicity? Are there discrete and recognizable patterns of discourse that have shaped our understanding of race? This project aims to reconstruct this history by bringing to bear the new methodologies of digital humanities on a representative sample of scholarly articles on ‘race’ from the field of anthropology. The goal is not to reify race as a concept, but explore how the discourse is operational within the work of anthropology, considering both its literature that supports it and literature that argues against it. By leveraging the new computational techniques of the digital humanities, we seek to isolate the ways in which the language surrounding the concept of race has changed over the last century and how these discursive shifts have organized the approach to human variation in physical anthropology. We provide comparative data by also exploring the same questions for cultural anthropology and the forensic sciences. The way we talk about race, in other words, is an indicator of how we understand it. Sample As our study is primarily concerned with the ongoing transformation of ‘race’ in the field of physical anthropology, the majority of our sample is taken from articles published in the  American Journal of Physical Anthropology (AJPA) throughout its publication history. Similarly, as current anthropological thinking on race, whether as a proxy for biological difference or as a social reality, is deeply affected by the practical necessities of the forensic sciences, we also drew part of our sample from articles published in the  Journal of Forensic Sciences ( JFS). In order to compare the specific configurations of ‘race’ within the discourse of physical anthropology to the study of the field at large, the final subset of our sample is composed of articles published in  Current Anthropology ( CA). In addition to these three core publications, we also add to our study a corpus of genetics articles that allow us to capture how the language of race is inflected by recent work in human genetics. For this part of our corpus we draw on the journals  Human Biology, the  American Journal of Human Genetics, and  Genetics, using the same search criteria as in our physical anthropology corpus. By combining peer-reviewed academic articles from these six publications, our study is able to examine the transformations in the anthropological discourse of race through time and across the disciplinary configurations of the field.   Our primary physical anthropological sample contains 440 articles published between 1918 and 2013 and that contain some cognate of the words ‘race’, ‘ancestry’, or ‘ethnic’ within either the title or article description. Of these, 299 were published in  AJPA, 79 were published in  CA, and 63 were published in  JFS. In preparation for our analysis, we divided our sample twice: by publication and by period. These two taxonomies of the corpus allowed us to identify trends in discourse over time (period) and across the disciplinary boundaries (publication). The period divisions were informed by the distribution of the sample and correspond to the early to mid-20th century (1918–1972 or Early), the late 20th century (1973–1999 or Mid), and the 21st century (2000–2013 or Late). Our turn towards genetics has allowed us to more than double our corpus size and gives a critical window into the ways in which the discourse around recent technological innovation is bringing back older conceptions of human variation  Methodology Distinctive Words While unsupervised computational linguistic techniques such as topic modeling (Blei, 2006; 2007) would allow us to extract clusters of highly correlated words from across the sample (thus approximating the ‘topics’ of the articles), the variety of these clusters would obscure the nuances of any potential ‘race’ topic. In other words, we seek to capture the ways that the discourse of race manifests itself even (and especially) when the topic of race is not overtly at issue. Instead, therefore, we chose to target specifically the language of ‘race’ and to extract the vocabulary in which it is embedded. We queried our sample of texts for our target terms (‘race’, ‘ancestry’, ‘ethnic’) and extracted the 15 words before and after, effectively performing a collocation analysis on each text. We selected 15 as our horizon as it is the average length of a sentence within our sample, ensuring that each extraction captured at least one complete syntactical unit. After excluding both stopwords and low frequency words (n<50), we applied a Fisher’s Exact Test to each of the remaining words to test for significant word usage (α=0.05) within the vicinity of either ‘race/racial’, ‘ancestry/ies’, or ‘ethnic/ity,’ with the goal of identifying the category of racial terminology with which each word was most highly correlated. These sets of significant words (n=230) represent the lexical ‘fingerprint’ of each of our concepts. By tracing the relationships between articles based on their usage of these words, we are able to identify how the discussion of race structures the corpus across time and disciplinary approach. Similarly, we are also able to trace the relationships between the words themselves in order to uncover the ways in which these individual discourses co-evolved over the course of the 20th century.  Topologies Our primary visualization strategy uses a topological representation of either the corpus (Figure 1) or the semantic fields (Figure 2). Based upon an extension of a visualization technique using Voronoi diagrams developed by Bell Labs (Gansner et al., 2009; Algee-Hewitt and Piper, 2013; Algee-Hewitt, 2011; Okabe et al., 2000), the topology reconfigures similarity as a function of distance within a spatial representation. The resulting map provides a more complex visualization of relationships among the individual members and their local neighborhoods, replacing raw distances and word frequencies with a depiction of relationality based on proximity, area, and conjunction. The close proximity of article-tiles in our corpus maps indicates a strong similarity in their use of the vocabulary of race, while articles located on opposite sides of the topology indicate their difference. Similarly, words appearing next to each other in the word maps appear frequently together, while those far apart are rarely used within the same text. Tile size within these maps is a function of density. The smaller the tile, the more tightly embedded each object is within its surrounding cluster, while large tiles indicate their dissimilarity from the immediate neighborhood. We are also able to visualize another layer of data by adding a color mapping to the topology. Our corpus maps are colored for both period and publication: this reveals a strong disciplinary cohesion between the publication and the precise mix of words that characterize its articulation of the ‘race’ concept. Similarly, there is a less strong, but still significant correlation between the language of race in each article and its data of publication. Our word maps are colored based upon the individual word memberships in our race concept categories. This allows us to observe clusters of words that form across time and within particular disciplines, suggesting the ways in which the discourse of race has been affected by (and, in turn, shaped) the individual disciplines or study (physical anthropology, cultural anthropology, forensic anthropology, and genetics). Further, it allows us to witness the configurations of this discourse across our three period divisions: from the early studies of human diversity in the early 20th century through the current genetics revolution. Through a careful parsing of these topologies, we are able, in this paper, to give a clear and concise account of the ways in which the race concept has been mediated within the different disciplinary discourses of anthropology and, most importantly, the ways in which its evolving central term (from ‘race’ to ‘ethnicity’ to ‘ancestry’) has carried along with it discursive configurations that remain constant across all of our disciplines and periods.     Figure 1. Topology of articles derived from shared clusters of words, colored for publication source.    Figure 2. Topology of ‘race’ terms, clustered by co-occurrence, colored by term group membership.  ",
        "article_title": "The Unspoken Word: Race and the New Language of Identity",
        "authors": [
            {
                "given": "Bridget Frances Beatrice",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Mark Andrew",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "interdisciplinary collaboration",
            "corpora and corpus activities",
            "text analysis",
            "anthropology",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Big data, ubiquitous computing, and the quotidian nature of the digital point to the ways metadata touches all aspects of human life, making it increasingly clear that the quality of metadata and the number and strength of linkages surrounding an object in a library collection is vital. Content discovery is dependent upon the data, search interface, and algorithms that allow us to locate material, causing an awareness of the power of the knowledge structures that underlie these systems more crucial than ever, especially given that these structures are masked by the seemingly silent processing of machines (Kittler, 1999; Lessig, 2006; Olson, 2002). Despite this reality, library bibliographic control, as the professional practice of creating metadata for library collections, continues to be walled and trained along root-tree hierarchies. Nontextual materials are forced within systems that are conceptualized for text-based print resources. Linked Open Data (LOD)—as a form of structured metadata that allows for dynamic interlinking of data across the Web, creating an active web of relationships, rather than a static set of documents and as a technology created for the Web—is a way to rewire these entrenched systems. Objectives  The primary goal of this project is to create a series of experimental, rhizomatic digital networks through the development of LOD prototypes of archival material related to the Mariposa Folk Festival. These prototypes will surface the multitude of relationships and contextual information normally silent in library catalogues and conceptualize this data in a fundamentally different way by activating the rich possibilities of Semantic Web technologies. Models currently being explored for the prototypes include the sonification of the network, and a network conceptualized around ideas of memory, narrative, and time.   Harmony and Dissonance Much of the literature on LOD has focused on the harmonization of data as a means to create semantic relationships on the Web. Open standards provide ‘a common framework that allows data to be shared and reused across application, enterprise, and community boundaries,’ and shifts the Web from being a web of documents to a web of data (W3C, 2013) by connecting discrete pieces of data rather than links between pages (Lange and Pattuelli, 2014). Information encoded as linked data interoperates with other data coded the same way, making for a rich integration of information from various data sources, allowing the Web to function like a giant database. This results in more comprehensive and accurate search results than current federated search systems (Byrne and Goddard, 2010).   The use of standardized vocabularies and schemas creates harmony within an information system, but this comes at a cost. Voices that do not fit within the system must be left out. We are grappling with the implication of these choices and the difficult issue of creating specialized schemas in an environment that prizes standards above all else.   An increasing number of organizations are making LOD available, but there remains a dearth of projects putting this data into action, particularly coming from the academic library sector in Canada. With no standard, out-of-the-box mechanisms for creating and working with LOD, many in the community, while aware of the supposed benefits, are unsure of how to get there (Lampert and Southwick, 2013), leading to increasing calls for the library community to begin to prototype and experiment (Jordan et al., 2014; Salo, 2013; Singer, 2009), and leading Askey et al. (2013) to challenge libraries to be open to failure. Experimentation and divergent thinking is vital to innovation in a sector that is slow to change.  The Case for Music Music, as a sounding art that takes place in time, is not easily captured in the manner of textual or image-based documents, making it challenging to align with practices based on the creation of textual metadata. Musical works give rise to numerous possible instantiations in various media, such as recordings, scores, and moving images, making connecting all these resources challenging (Smiraglia, 2001). Additionally there are complex interpersonal relationships of mixed authorship. A single performance may have a composer, lyricist, performers, and others, such as audio engineers. Because musical objects have a dense network that is often invisible in library systems, it is an ideal case for LOD experimentation. The Mariposa Folk Festival is an internationally recognized Canadian music festival that began in the summer of 1961 and continues to be a key venue for the promotion of traditional music. York University holds the archives of the Mariposa Foundation. In 2009 York University Libraries received a grant from the Canadian federal government to digitize and provide public access to live sound recordings and other archival documents to celebrate the fiftieth anniversary of the Mariposa Folk Festival. Numerous questions related to the challenges of copyright, metadata, and cultural memory from this project are being explored. Methodology We are making use of practice-based research approaches as well as theoretical methods to extend the theory and practice of library systems. This can be thought of as a ‘feedback loop’ where theorizing and prototyping perform a continuous loop, and the generation of questions is the focus over the mobilization of a final, polished product. The initial period involves the development of the conceptual models and the dataset necessary for the deployment of the prototypes. Once the existing metadata have been mapped, the team will begin to build in enhancements using standard schemas and ontologies. Much of the time in the first phase is focused on developing metadata, mapping, and cleanup, with the possible extensions to the York Digital Library’s Islandora platform.  The prototypes are the focus of the second phase. They are intended to extend technical capacity and theoretical ideas around information and mediation. If LOD is to fundamentally change the way information is navigated on the Web, then we must work to change not only the structures of the data but the conceptual models in which they are deployed. Challenges and Implications An early challenge that has developed is that much of the existing data requires checking and correction. We have reduced the size of the dataset to ensure detailed and clean metadata. This smaller grouping of documents is being grouped around a theme and time period. As much as LOD is promising, a great deal of manual work is still required. The real work of this project is around the development of the conceptual models, in particular trying to approach the network from the idea of the sonic and the implications of ‘sound’ on the network idea and attempting to move away from the hegemonic ‘nodes and edges’ approach to LOD.  Copyright has previously proved challenging and remains a challenge for this project. Rather than choosing to not include sound documents, we are attempting to leverage community relationships, link to alternate sources, and find solutions.  Conclusion This talk outlines our initial steps in creating Linked Open Data network prototypes. While we are at the initial stages of the LOD implementation, our work builds upon years of praxis and expert knowledge of library and archival descriptive practices, archival preservation, repository software, and library systems. We will discuss our plans for our LOD networks and other means of pushing the boundaries on what is considered library discovery. ",
        "article_title": "Sounding it Out: The Mariposa Folk Festival and a Linked Open Data Digital Library Prototype",
        "authors": [
            {
                "given": "Stacy",
                "family": "Allison-Cassin",
                "affiliation": [
                    {
                        "original_name": "York University, Canada",
                        "normalized_name": "York University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05fq50484",
                            "GRID": "grid.21100.32"
                        }
                    }
                ]
            },
            {
                "given": "MJ",
                "family": "Suhonos",
                "affiliation": [
                    {
                        "original_name": "Ryerson University, Canada",
                        "normalized_name": "Ryerson University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05g13zd79",
                            "GRID": "grid.68312.3e"
                        }
                    }
                ]
            },
            {
                "given": "Nick",
                "family": "Ruest",
                "affiliation": [
                    {
                        "original_name": "York University, Canada",
                        "normalized_name": "York University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05fq50484",
                            "GRID": "grid.21100.32"
                        }
                    }
                ]
            },
            {
                "given": "Anna",
                "family": "St. Onge",
                "affiliation": [
                    {
                        "original_name": "York University, Canada",
                        "normalized_name": "York University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05fq50484",
                            "GRID": "grid.21100.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "metadata",
            "libraries",
            "archives",
            "museums",
            "music",
            "information retrieval",
            "standards and interoperability",
            "English",
            "GLAM: galleries",
            "knowledge representation",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Can a methodology that allows students, scholars, and teachers to use tools as we build them contribute to the development of those individuals as ‘digital’ humanists? Over the last year as we have been developing the Perseids Platform and the Arethusa Annotation Framework, we have been actively engaging the users at every step of the process and releasing features for them to work with well before the tools are fully finished. Despite the challenges this introduces for the developers as well the users, we have found that using tools mid-development opens a window into exactly how the tools let users manipulate and shape the data. The process also allows both students and researchers to understand their role in the creation, curation, and annotation of texts through the scientific process of creating and using the data. This process has also exposed the critical role the humanist plays as product designer and tester of the tools we develop to support the research and publication process—or, as Stephen Ramsay says, as ‘builders and makers’ (2011).  The Perseids platform is a collaborative editing environment built from a loose coupling of open-source tools and services. 1 Two core values in the development of the platform have been (1) to reuse existing tools wherever possible and not reinvent the wheel and (2) to put the data first, creating data that are reusable and that can be preserved through the use of stable identifiers and serialized according to standard formats. (Data in this case mean all of our texts, annotations, and related objects.) While we aim to produce prototype disseminations of the data created on the platform as web-servable digital editions, this focus on the data, and the reuse of existing tools, has put production of a seamless user interface and polished presentation of output as a lower priority. We also have adopted an Agile 2-inspired development methodology, which requires us to release new features for use early and often. Although up until now we have been following this methodology more in spirit than to the letter, as the number of people involved in the project continues to grow, we will begin to incorporate some of the official practices, particularly the use of user stories to drive our development.   The result is that the end users see much of the ‘inner wiring’ of the tools. For example, in IMGSPECT, 3 our image-to-text mapping tool, students identify regions of interest on an image and map them to the text of the transcription. This tool creates stable identifiers (in the form of CITE URNs 4) for the region of interest and creates a template TEI transcription for the word or characters represented in the image that references the URN of the region of interest in the @facs attribute of the TEI markup element. Although ultimately we plan for automatic population of the underlying TEI XML file for the entire transcription from the image annotations, currently users have to copy the template markup into their file manually. While the value of such extra work is not always obvious, the process affords an opportunity for the students to experience all the steps involved in building a dataset. In particular, the process emphasizes the need to both justify editorial choices in transcribing a text and to create data that conforms to the current best practices and can therefore be reused by others. In doing so, students are using new technology while following the most traditional scholarly principles. Indeed, producing good data that are sharable, interoperable, and can form the basis for sound interpretation is the goal of all scholarship. In this way, working with Perseids allows students to put the highest standards of scholarship into practice and to enter the worldwide conversation about the production of knowledge. By exposing the URNs that link the text to the image, we allow them to see that what they are doing is producing data that not only justify their choices in their own edition but also allows others to reuse this data.   Arethusa 5 is a framework for linguistic annotation and curation that provides a highly configurable, language-independent, extensible infrastructure for close reading, annotation, curation, and exploration of open-access digitized texts. Arethusa is back-end independent, but it has been developed in collaboration with the Perseids project and integrated with Perseids to provide an annotation interface for morpho-syntactic analyses. Going forward, Arethusa will also act as a broker between the Perseids back-end and various other front-end annotating and editing activities, including translation alignments, entity identification, and text editing. Many of the design requirements for the Arethusa morpho-syntactic annotation environment were directly informed by unanticipated scholarly and pedagogical uses of its precursor tool, the Alpheios Treebank Editor. 6 Alpheios exposed the XML of both the treebank annotations it worked with and the configuration files that defined the tag sets available for use. The ability to work with these XML structures allowed the users to see the direct interplay between the data and the tools used for editing them, pushing the limits of the Alpheios interface. As design on the new Arethusa interface started, we actively engaged these users in testing and designing the new features they had asked for. This is not necessarily a novel approach; in Thomas and Solomon (2014), the RoSe project team noted that engaging students in this way greatly improved the usability of their system. However, in addition to the benefits afforded by the tool, we also found that this process developed the humanist users’ analytical skills and experiences, forcing them to evaluate questions such as whether the features they were requesting merited changes in existing standard linguistic representations of morpho-syntactic annotations, and implications on interoperability. It also required them to consider the division of responsibility between user interface functionality and representation of the data manipulated by the interface. The ability to edit the underlying data directly provoked questions about data integrity, as the morpho-syntactic annotations are just one component of a larger publication that includes TEI XML transcriptions, translation alignments, and other annotations.    From a pedagogical standpoint, integrating students into the development efforts has meant that professors and pupils both had to be comfortable with using a technology that is in constant evolution. For this reason, we favor a hands-on approach that focuses on the tasks to be accomplished by the students rather than general instruction about the tools or about digital humanities. In this we agree with Mahoney and Pierazzo, who write, ‘What we should be teaching under the umbrella of the “digital humanities” are not skills—although they too play their part—but new methodologies and new ways of thinking’ (2012, 245). For example, students in an intermediate Greek class were given step-by-step instructions about how to transcribe a Greek inscription as an XML document rather than a lecture about XML, even though they had never been introduced to it. The instructions gave basic XML markup indications, such as <w>, <l>, and <lb>. The professor and students accomplished each task simultaneously so that the students could refer to the on-screen demonstration as they transcribed their own text. After each step, students saved their work and visualized the concrete result of their edits in the preview screen. They were thus able to measure their progress and to gain an immediate understanding of the effect of their work. This approach has been put into practice when using Perseids in classes such as classical mythology, intermediate Greek, medieval Latin, and many others. We have found it to be productive, as it empowers the students to work with the software from the very beginning and to start to feel comfortable with it quickly. The approach has also offered great opportunities to gather feedback from the students and to keep developing the tools in order to make them more intuitive and user-friendly. We will use a study currently under way at Tufts University during the spring 2015 semester to further evaluate the outcomes on and for the students using and participating in the platform. 7   Finally, the ability to work directly with the data has led the Perseids and Arethusa users to think differently about the process of publication. Giuseppe Celano, a scholar who was an integral member of the Arethusa development process, has been exploring the possibilities for sharing treebanking analyses in the form of open micro-publications, realizing that ‘through Perseids, every scholar is provided a free platform allowing a micro-publication which is fully shareable, and so anyone is given the chance to be not only the user but also the contributor of a publication’. 8  Notes 1. http://sites.tufts.edu/perseids/. 2. http://en.wikipedia.org/wiki/Agile_software_development. 3. https://github.com/PerseusDL/imgspect. 4. http://www.homermultitext.org/hmt-doc/cite/. 5. http://sosol.perseids.org/tools/arethusa/app/#/. 6. https://github.com/alpheios-project/treebank-editor. 7. This is a continuation of the work described in Krohn and Crane (2014). 8. Email correspondence, September 27, 2014. ",
        "article_title": "Perseids And Arethusa: Building Tools That Build Digital Humanists",
        "authors": [
            {
                "given": "Bridget",
                "family": "Almas",
                "affiliation": [
                    {
                        "original_name": "Tufts University, United States of America",
                        "normalized_name": "Tufts University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05wvpxv85",
                            "GRID": "grid.429997.8"
                        }
                    }
                ]
            },
            {
                "given": "Marie-Claire",
                "family": "Beaulieu",
                "affiliation": [
                    {
                        "original_name": "Tufts University, United States of America",
                        "normalized_name": "Tufts University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05wvpxv85",
                            "GRID": "grid.429997.8"
                        }
                    }
                ]
            },
            {
                "given": "Gernot",
                "family": "Höflechner",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digital humanities - nature and significance",
            "English",
            "philology",
            "software design and development",
            "digital humanities - pedagogy and curriculum",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Over the past 30 years, developments in computing mean that almost every script and writing system ever created can be coded on a computer and used on Facebook, mobile phones, and in emails, and large numbers of documents can be encoded, searched, and archived in a range of different scripts. In South and Southeast Asia, there are a large number of different scripts, some used by quite small communities. In India, for example, there are around 11 official scripts, with states like Andhra Pradesh, Kerala, Gujarat, Karnataka, and Tamil Nadu having their own unique scripts. Speakers of Tai languages, in particular, have long had a range of different scripts in use among languages that are quite similar, so, for example, the script used by Shan people in eastern Myanmar is quite distinct from that of the Khün in the eastern part of Shan state, and also different from but intelligible with the Khamti script in western Myanmar, and the various scripts used by Tai people in Northeast India. Since the earlier part of this century, a great effort has been made to encode all of these scripts in Unicode, a standard that allows for the encoding of symbols used in writing that can be demonstrated to be in use, or to have been in use in the past. However, negotiating a script into Unicode is a complex issue, involving considerable technical expertise and knowledge of script encoding principles, things that are difficult enough for an academic linguist but virtually impenetrable for members of the speech communities. Combining our expertise in both script encoding and in linguistics, we will raise issues of community involvement in the process by means of several case studies. 1. The decisions to encode different forms of letters used in different scripts, such as Burmese on one hand and Tai varieties in Northeast India on the other, on the same encoding point so that they cannot both appear in platforms like Facebook (Hosken, 2014b).  Below is an example, from an 1821 Tai Phake manuscript called Mahosatha, telling one of the former lives of the Buddha (Jataka). This shows the stark differences in glyph shapes of the characters. The top line is in the Burmese script (Pali language), then shifts to Tai in the second line.     Background: In Unicode, the glyphs for the letters in the Burmese (‘Myanmar’ in Unicode) script were based on their use in the Burmese language, whose 32 million speakers dwarfs the Tai language communities of India (such as Phake with 2,000 speakers). The Myanmar script was published in Unicode in 1998, and the minority users were not consulted in the encoding then, so the default glyphs represent those of the Burmese language, not the Tai languages.  Update: The request (Hosken, 2014b) to Unicode to disunify the characters for Tai (Aiton and Phake) from Burmese was discussed in August 2014, with experts calling in to the Unicode Technical Committee meeting. However, no consensus was reached, so the topic was dropped for the time being. As a result, users on Facebook can’t show a mixture of Burmese and Tai-based languages on the same FB page with the expected glyph shapes.  2. The difficulties that the Assamese community find with the naming of letters used only in Assamese, such as ‘Bengali letter Ra with middle diagonal U+09F0’, a letter not used at all in Bengali but found in Assamese with the pronunciation [wɔ] (Hosken, 2012). Background: The name of the script ‘Bengali’ was already the name of the script in Unicode 1.1 (1993). Due to stability policies, script names cannot be changed.  Update: The controversy reached the point of being discussed in the newspapers in 2012. In response to users, the Unicode Consortium made a few changes to incorporate ‘Assamese’ in the chapter on Bengali and the names list in 2012, but clearly there remains some confusion for Assamese users.  3. Problems for users posed by the Unicode New Tai Lue encoding model, which does not allow users to type the characters in visual order (Hosken, 2014a), the model adopted by the scripts used in neighboring countries of Thailand and Laos. Update: As the result of the request to change the encoding model by Hosken (2014a) and feedback from a public review issue posted on this issue (Unicode Consortium, 2014a), Unicode decided to change its encoding model in the next version of Unicode, Unicode 8.0, which will be released in the summer of 2015. The talk will conclude with suggestions on ways to improve the interaction between linguists, speech communities, and the Unicode standards committee. For example, conveying technical details over email and the phone can become problematic both for the user community and the Unicode Technical Committee (such as how to represent Tai languages on Facebook, above, 1). One strategy that has been successful is to bring a Unicode expert to the users (or to a linguist who works with the users) or to bring an expert to the Unicode committee. However, this involves hurdles, such as finding funding for travel. The urgency for supporting the requirements of small language communities is based on the increasingly widespread use of mobile devices and social media. Why should minority language users in South and Southeast Asia be limited in their electronic communication, so that they have to rely on script-style of the larger language community, instead of having their own preferred style displayed?  Funding This work was supported by the National Endowment for the Humanities [grant  PR-50205 to Deborah Anderson] and the Australian Research Council [Future Fellowship grant FT 1001006614 to Stephen Morey].  ",
        "article_title": "Negotiating The Issues Of Encoding And Producing Traditional Scripts On Computers – Working With Unicode",
        "authors": [
            {
                "given": "Deborah",
                "family": "Anderson",
                "affiliation": [
                    {
                        "original_name": "UC Berkeley, United States of America",
                        "normalized_name": "University of California, Berkeley",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an7q238",
                            "GRID": "grid.47840.3f"
                        }
                    }
                ]
            },
            {
                "given": "Stephen",
                "family": "Morey",
                "affiliation": [
                    {
                        "original_name": "Centre for Research on Language Diversity, La Trobe University, Australia",
                        "normalized_name": "La Trobe University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01rxfrp27",
                            "GRID": "grid.1018.8"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "encoding - theory and practice",
            "standards and interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this paper we seek to elucidate understandings of the self-management of multiple context-dependent identities of Social Networking Sites (SNS) users. Online social networking is now in its second decade and has become a central activity to a large proportion of the global population. This shift is not surprising, as humans are social animals with a need to connect and communicate with each other. SNS augment our existing offline networks, allowing us to keep in touch with people over great distances, share our experiences and associated content, organize our social lives, and discover new contacts beyond physical reach. As previous researchers have noted, many users maintain multiple online identities through which they actively manage their social interactions on the various SNS (Golbeck and Rothstein, 2008).  Most SNS, however, suffer from a common problem that prevents them from capturing the true richness of our offline social networks. As social beings, we tend to participate in different, overlapping social groups, and we adjust our identities to match the contexts, as well as our use to match the constraints imposed by the various SNS. The reasons why people choose to explicitly manage the overlap among social networks, even keeping some networks completely distinct from others, are commonplace and usually not clandestine—for example, teenagers wishing to discuss sensitive health matters in online forums (van der Velden and El Emam, 2013), employees complaining about treatment at work (O’Brien, 2014), or those engaged in political commentary in uncomfortable or dangerous situations (Attia et al., 2011).  The self-management of multiple context-dependent identities represents a topic that deserves greater attention within digital humanities and needs to be further explored and elucidated, since it incorporates the entanglement of online and offline interactivity within and around computer mediated environments (Angelopoulos and Merali, 2013) which can have significant implications for the overall sociability of SNS users (Angelopoulos and Merali, 2015). The self-management of multiple context-dependent identities implies a process in which the users control how the other users with whom they are socially connected perceive them (Baumeister and Leary, 1995; Leary et al., 1995). The extant computer-mediated communication literature highlights the needs of a diverse range of groups to incorporate and maintain multiple identities on a plethora of media, such as hobbyists like cigar smokers (Angelopoulos and Merali, 2013; 2015) and bodybuilders (Ploderer et al., 2008), as well as professionals who need to separate their professional from personal lives (Peluchette et al., 2013). Although the need for users’ multiple context-dependent identities to be further explored and elucidated is highlighted in the literature (Karl and Peluchette, 2011; Peluchette et al., 2013; Talamo and Ligorio, 2001), to date there are very few studies exploring the issue directly (Talamo and Ligorio, 2001).  We adopt a quantitative approach and explore the concept through a survey. We designed the questionnaire based on extensive literature analysis on online/offline identities as well as the environment analysis and the survey planning (Duffy et al., 2000; Karl and Peluchette, 2011; Kodjamanis and Angelopoulos, 2013; Koh and Kim, 2003; Kuhn and McPartland, 1954; Peluchette et al., 2013), and our findings are drawn from a sample of  n=272 participants and guided by a previous pilot study of 60 participants.   Our findings demonstrate that, compared to other SNS, use of multiple accounts is more common within Facebook and Twitter, and considerably less common within LinkedIn. This finding is surprising given the fact that both Facebook and Twitter provide the users with features to manage multiple groups of contacts within a single account, unlike LinkedIn.  Moreover, our findings reveal that behavior between the use of different SNS suggest that different SNS are used to manage different aspects of people’s lives despite the fact that most SNS provide tools for this, yet such tools to manage groups of contacts within SNS are rarely used, as reported by the participants of our study. Such a finding reveals that the tools that are already available and provided by the SNS for the management of multiple context-dependent identities are insufficient and neglected by the users, and thus there is a need for better tools to be implemented by either the SNS or by third parties that take into account the real needs of the users. Exploring the differences between users of multiple accounts and users of single accounts has revealed that those who use more than one account on one or more SNS generally share more personal information with those accounts and tend to engage in more audience management behavior both online and offline. These findings suggest that users of multiple accounts use both control of information and targeted sharing of personal information to manage their identities, highlighting the need for networking activities that can support both behaviors. The combination of these two tendencies also highlights the importance of security and privacy between SNS for these individuals as they reveal a lot of personal information online but want strict control of what information is revealed to whom (boyd, 2010; Edwards and McAuley, 2013; Livingstone, 2008). Practically, the findings of our survey demonstrate the need of users for better tools for the self-management of multiple context-dependent identities, as the current tools provided especially from Facebook and Twitter are insufficient.  The self-management of multiple context-dependent identities is still an issue to be pursued by the organizations behind SNS platforms, and there is a profound need for better tools to be implemented either by them or by third parties. We call, thus, for future research to focus on applied approaches trying to solve this real and practical problem. Our findings are drawn from a sample, which, although adequate for the needs of the study, remains too small to enable us to reflect on a larger scale. Whilst the following step in our future research plans is to expand our scope and explore the concept further, we suggest that future studies should explore the issue on online communities in general, as well as on online communities of hard-to-reach populations (Angelopoulos and Merali, 2013; 2015; Ploderer et al., 2008). ",
        "article_title": "You can leave your hat on-line: Multiple Context-Dependent Identities on Social Networking Sites",
        "authors": [
            {
                "given": "Spyros",
                "family": "Angelopoulos",
                "affiliation": [
                    {
                        "original_name": "Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Dominic",
                "family": "Price",
                "affiliation": [
                    {
                        "original_name": "Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Richard",
                "family": "Mortier",
                "affiliation": [
                    {
                        "original_name": "Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Derek",
                "family": "McAuley",
                "affiliation": [
                    {
                        "original_name": "Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "user studies / user needs",
            "social media",
            "English",
            "internet / world wide web",
            "software design and development"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper addresses organizational practices and activities of digital humanities centers (DHC). The paper draws on an ethnographic study conducted from 2010 to 2013. Through case studies, surveys, interviews, and observations, the study explored how scholars engage with and think about digital research tools and methods, analyzed teams developing digital research tools, and observed organizational practices related to DHCs. The fieldwork was carried out at twenty-three educational, research, and funding institutions in the United States and Europe, and it involved 258 participants including researchers, faculty, students, administrators, librarians, software developers, policymakers, and funders.  DHCs emerged as an academic organizational unit in the 1980s (Fraistat, 2012), representing ‘key sites for bridging the daunting gap between new technology and humanities scholars’ (Terras, 2012, 281). Over the past three decades, the number of DHCs grew, yet not as intensely and widely as humanists’ engagement with digital technologies (Zorich, 2008). DHCs analyzed in this study were founded from the early 1990s up to the present time. All centers are affiliated with research-intensive universities, have a dedicated space, and employ between five and fifteen staff members, mostly traditionally trained humanists with advanced computational skills.  A significant difference among the analyzed DHCs distinguished the bottom-up versus top-down initiatives. In the top-down initiatives, university administrators sought to establish hubs of scholarly innovation, yet often without a clear sense of what that innovation entailed. For instance, one DHC featured new glass walls as a visually transparent boundary between the print and the electronic materials, while flexible workspaces with high-end technologies were designed to adjust to user needs. But while design, ergonomic, and technical features were meticulously considered, the goals for the center were given less attention:  That’s really a big question for us, what kind of services we want to offer. Are people going to know what to do? Are they going to come in, sit down, and check e-mail? [director of a DHC] Another campus of the same university launched its DHC simultaneously, but the initiators were humanities scholars rather than the administrators. They began with a university-wide dialogue, which included humanities faculty and students, information technologists, librarians, and other stakeholders. These early conversations and comprehensive planning helped identify organizational strategies and design possibilities consistent with their goals, resulting in a well-defined mission for the new unit.  A number of the analyzed DHCs aspire to support humanists’ general research and to gradually introduce scholars to digital tools and methods. These centers target the widest spectrum of humanists rather than those already versed in or inclined to using digital tools, thus bridging the critical gap between more and less technologically advanced scholars. Yet, with such a diverse user community, they sometimes struggle to profile their activities. Humanists’ interest in digital tools and methods varies as much as their skills, and addressing such a spectrum of proficiencies and needs is anything but straightforward. Some DHCs thus adopt a more precisely defined approach when establishing goals and characterizing their potential user base. However, parallel functioning of DHCs within the same institution, differentiated according to the funding sources and the target user-base, often increases fragmentation of work and user confusion. Considering the hesitation toward digital scholarship that traditionally trained humanists sometimes hold, DHCs’ outreach often involves two preparatory steps: (1) making humanists aware that they already rely on digital technologies and (2) explaining that DHCs are offering digital tools and methods rather than imposing them. The perception of digital tools and methods as ‘data crunching’ and incompatible with humanistic hermeneutics contributes to lower uptake of these technologies among humanists. DHCs play a considerable role in breaking down the complexities of those shifting epistemic practices and values. One part of the task is familiarizing humanists with the epistemological and methodological elements of the transition towards digital scholarship. Another component depends on making the transition manageable to grasp and adopt: And I think we’ve been able to break that down into smaller steps that people can make without dumbing it down, without making it seem trivial, but really emphasizing the real shift in thinking that’s happening in the humanities, and supporting that with the technical side. [codirector of a DHC] This study showed that humanists favor and best learn in practice, when instruction is closely related to their area of study and when it unfolds organically, through collaboration with peers or graduate students. Instead of skill-based training, humanities education in digital scholarship needs a comprehensive framework encompassing epistemological, methodological, technical, and sociocultural aspects of digital knowledge production. Furthermore, in their outreach activities, DHCs frequently stumble upon the scholars’ lack of time:  These are very busy people, and they are ultimately making choices between do they go and pay attention to our offer of a demonstration about digital tools, or do they spend that hour focusing on their own research questions. We’re competing with natural priorities in a scholar’s life. [director of a DHC] It is therefore vital that education in digital scholarship becomes administratively recognized as part of scholars’ professional development included in their paid time and activity, as well as in their promotion dossiers. DHCs often employ hybrids—traditionally trained humanists who also have good computational skills. They are seen as a necessary link between ‘the two cultures’, who can adeptly translate epistemological and methodological concepts and approaches. ‘Smart organizations will have more of me’, remarked the head of the research and development team at one of the centers, a historian with good programming skills. The advantage of speaking both ‘programese’ and ‘scholarese’, as this interviewee put it, is the capacity to help humanists grasp digital tools and methods while helping programmers understand humanities work.  DHCs have found an efficient way of employing hybrids through engaging humanities graduate students, usually early adopters of technology. Graduate students who participated in the study liked working at DHCs. It allowed them to advance their research skills and to build expertise through participation in important research and decision-making activities. The reversal of instructional roles, in which students were teaching teachers, facilitated students’ understanding of some of the didactic principles motivating them to develop their own pedagogic strategies. Hybrids as a type of scholarly workforce are linked to the concept of alternative academic careers. Participants in this study described alternative academic paths as intentional career choices scholars make. Although this choice allows scholars to continue working in their preferred field, the transition is nonetheless perceived as difficult and consequential: ‘You can’t just yank somebody out of the faculty and out of years or decades of training without some accounting for how they conceive of themselves as a scholar,’ said an associate professor of English. From the administrative point of view, thinking about people’s time and labor is an important issue DHCs need to engage with, because the inherited systems of classifying employees are not well suited to digital scholarship. Traditional organizational systems are not only inefficient for addressing contemporary issues of academic labor and knowledge production; they are also potentially detrimental to the future of scholarly work:  If we can’t get this generation of graduate students comfortable with alternate modes of work, not feeling like they are failures if they don’t get a tenure-track position, and seeing good career paths for themselves within the DH, we’re going to lose that generation of scholars. [professor of history] Among the respondents, two related but disparate theories of the future of DHCs emerged. One suggests that digital tools and methods will progressively become a standard part of humanities research; DH should thus be seen as a transitional moment in the humanities disciplines rather than as a distinct field. Another approach held that the need for innovative work and thinking with technology in the humanities would never cease. Although the use of digital tools and methods will become increasingly mainstream, there will always be a need for research groups on the frontier of innovation. Scholars and administrators both mused about whether DHCs will be needed in the future, or whether digital scholarship will blend into existing disciplinary and departmental structures. Overall, the respondents agreed that we will see a wave of interest in DHCs, some of which will persist, while others will peak quickly, only to fade away.  ",
        "article_title": "Organizational Practices in Digital Humanities Centers",
        "authors": [
            {
                "given": "Smiljana",
                "family": "Antonijevic",
                "affiliation": [
                    {
                        "original_name": "Illinois Institute of Technology, United States of America",
                        "normalized_name": "Illinois Institute of Technology",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/037t3ry66",
                            "GRID": "grid.62813.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "media studies",
            "repositories",
            "organization",
            "user studies / user needs",
            "interdisciplinary collaboration",
            "archives",
            "anthropology",
            "internet / world wide web",
            "digital humanities - facilities",
            "project design",
            "rhetorical studies",
            "social media",
            "English",
            "history of Humanities Computing/Digital Humanities",
            "crowdsourcing",
            "digital humanities - institutional support",
            "interface and user experience design",
            "digital humanities - nature and significance",
            "sustainability and preservation",
            "knowledge representation",
            "digital humanities - pedagogy and curriculum",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The first decade of the ambitious Google Books project has produced a great deal of controversy as well as a valuable resource. The project has been consistently challenged by authors’ and publishers’ groups mounting legal action. The rhetoric that Google uses to justify the project frequently concerns fair use, access, marginal voices, a so-called global library, and the role of access to information in supporting a strong democracy.  In the process of digitization, the books are transformed considerably. The act of reading texts on Google Books is complicated by the many figures crowding the text and the act itself. In addition to the author/s of the work in question, there may be, scattered throughout the text, traces of a person who has gifted the book, of the borrowers whose trace is marked by library loan stamps, and there are the multiple readers who have left physical marks of their reading upon the page. This marginalia can be found in the form of underlines in the text and marks in the margins—from simple ‘X’s, to detailed comments of varying degrees of legibility, to questions asked of the text and doodles drawn on the page. Also present in the text, on occasion, and complicating reception, is the ghostly figure of the book scanner whose labour is made visible by spectral fingers caught in the scan and numerous other kinds of scanning errors that, given the haste of the project, make it through quality control and into the online space.    But the Google Books reader does not leave a trace. I, reading the text on the screen, make no marks upon the text to leave for others. Although my digital trace may be recorded in the IP number, I cannot leave a trace of my reading for the next reader to examine or build upon. I may transcribe the text. I may now download (selected) books to read on a plethora of devices. I may use the clip tool to take or even share a passage with my social network. But I cannot alter the book as it appears in the archive. While I can annotate—publically or privately—a Kindle book, leaving traces in the form of underlines signifying highlights and notes, I cannot annotate a Google book. The object is frozen for all time in its scanned state (or states, since frequently scans exist from a number of different editions or copies of a book).  The presence of the reader on the page of the printed book has been a marginal concern for the field of book history for scholars from manuscript culture and before to the recent obsession with David Foster Wallace’s cacophonous presence on the pages of his library, now acquired by the Harry Ransom Centre at the University of Texas. Marginalia tell us something about the reader’s response to the work, and it tends to be the marginal notes of literary giants that receives attention from scholars (Nabokov, Kerouac, Wilde). The presence of life narrative voices in the archive, and the material conditions they reflect, has significant ramifications for how we see the texts. How then do we approach the case of a published family history that has been housed for over a century in a public library and in that time has been heavily annotated with additions, comments, responses, notes, and interjections? To pursue this train of thought, I want to use as a case study a life narrative texts in the Google Books archive.  Memoir of the Farrar Family was published in 1847 by Timothy Farrar (1788–1874). The book exists in multiple online formats. It is available as full text from Archive.org as well as from Google Books. It is available for purchase on Amazon, with a caveat that the work comes from a scan and may have marks and imperfections. The book is referred to on many genealogical websites, which tells us something about the sort of text that it is. The inside cover says, ‘A Discourse occasioned by the centennial anniversary of Hon. Timothy Farrar, LL. Delivered at Hollis, N.H. July 11 1847 by Timothy Farrar Clary. Printed by Request. Andover: Printed by William H. Wardwell. 1847’.  Google’s bibliographic information for the book tells us that it was originally digitized on 18 September 2007 from a text located at the University of Wisconsin–Madison. What makes this case so curious is that the book as it exists in the Google Books archive is riddled with the markings of a later reader. The handwriting is persistent throughout the pages of the short work, and the marginalia take the form not of active reading, of someone underlining the work as they go, but of a reader supplementing and correcting the text’s facts. This paper considers the implications of Google Books on the life writing it digitizes and on the notion of the library/archive and the interplay of voices in the digitization not only of a polished and published family history but also of the voices added to the text in less formal ways. The paper uses frameworks from the history of the book discourse, marginalia studies, and new media studies to consider the residual marks of other readers in ephemera and marginalia on the page and uses as case studies two works of life writing from the 19th century to examine the implications of autobiographical voices unbound by Google Books and the intellectual labour/vandalism represented by the works’ many readers. I apply Alexander Galloway’s view of the interface as ‘a generative friction between different formats’ (2012, 31) and look to media archaeology and Lisa Gitelman’s work on the history of documents for a means of understanding the layers at work in reading an annotated digitized work of life writing, a genre in which the traces of readers and digitization procedures are more keenly felt.     ",
        "article_title": "Traces of Lives in Digital Archives: Life Writing, Marginalia and Google Books",
        "authors": [
            {
                "given": "Tully",
                "family": "Barnett",
                "affiliation": [
                    {
                        "original_name": "Flinders University, Australia",
                        "normalized_name": "Flinders University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01kpzv902",
                            "GRID": "grid.1014.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "literary studies",
            "digitisation - theory and practice",
            "archives",
            "sustainability and preservation",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The tricks of illumination and digital projection exploit the possibilities of contemporary technical tools—3D architectural mapping, sensors, software-enabled visual distortions, real-time image manipulation of the ever-growing, luminous flux of data—to introduce into public spaces a sense of the ‘technological uncanny’, a phantasmagorical urban world in which to the receptive viewer anything,  seemingly, is possible (Edensor, 2012, 1107).   For many creators of projection artworks, the topography of the built environment itself becomes a different kind of screen (Sassen, 2009, 29) to program and design to spectacular effect. But site-specific projection artworks can also be conceived not only as light and colour displays but as mixed-reality interventions that combine past and the present spaces in novel ways.  This paper will present on a set of public art interventions that seek to ‘surface’ archival recordings about a specific location using contemporary interfaces of projection design and acoustic installation. The paper will address two personal projects produced in 2014–2015 in Bathurst and Canberra that reflect intensive research into multiple film, sound, and image collections as resources through which to design and curate contemporary interpretative displays and events. These are presented alongside the projection work of artist Shimone Attie in Berlin during the early 1990s and the contemporary work of the Niño Viejo (‘Old Child’) collective in Valencia, Spain.  Reflecting across these different, site-specific projection pieces, the paper considers the use of digital projection as a form of ‘site-writing’, or writing a place as opposed to writing about it (Rendell, 2010). Where they draw from and recompose the ‘trace elements’ of past urban spaces, it reflects on how these different projection artworks might be understood to contribute to the ‘displacing forwards’ (Freud, 1901/1960) of contemporary memory practice, while opening up new territories for engaged digital humanities research.  ",
        "article_title": "Uncanny Projections / Site-Writing Places",
        "authors": [
            {
                "given": "Sarah",
                "family": "Barns",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "libraries",
            "archives",
            "museums",
            "video",
            "film and cinema studies",
            "sustainability and preservation",
            "archaeology",
            "audio",
            "multimedia",
            "English",
            "GLAM: galleries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper explores the interactions between software engineers and researchers in developing large-scale digital humanities platforms. In particular, we focus on how a diverse, cross-discipline group contributed to the development and deployment of the Alveo 1 virtual laboratory (Cassidy et al., 2014). We present an overview of how the tool works, its impact through transforming methodologies within the language sciences, and a summary of what digital technologies underpin its framework. We show the process of the Agile methodology 2 in a humanities context, good practice for scoping and implementation of highly technical features, and how stakeholder consultation is key to successful software development processes.   Alveo is an online system designed specifically for research in Human Communication Science (HCS). Human Communication Science encompasses the areas of speech science, speech technology, computer science, language technology, behavioural science, linguistics, music science, phonetics, phonology, and sonics and acoustics. HCS research depends upon datasets (collections) of speech, music, text, video, sounds, and specialised tools by which to search, analyse, and annotate these data. Alveo provides a platform to access these datasets and use the specialised tools. The Alveo system consists of two major components: a data discovery interface and workflow engine. These work independently, but are compatible so that one feeds into the other. The data discovery interface gives both a Graphical User Interface (GUI) and an Application Programming Interface (API) that allow for browsing and searching of data, viewing collections and their metadata, and previewing and downloading documents (text documents, audio files, video files, images, etc.). The data discovery interface supports the creation of stable ‘item lists’ that can be shared with other users and transferred to third-party tools for analysis. The workflow engine uses Galaxy, 3 a scientific workflow platform originally developed by Penn State University for data-intensive biomedical research. Alveo uses Galaxy to provide its users with easy access to a variety of tools for analysing and manipulating human communication datasets. The innovation of the Galaxy component is that it allows for workflow construction independent of data. For example, a researcher builds a workflow that uses PsySound for an acoustic analysis combined with an independent (but linked) analysis using the ParseEval tool on the same language data. The workflow can then be reused on different collections (or selected subsets), shared with collaborators or archived for publication.   The technical complexity of the project is seen in the range of components that underpin the system. These include: Hydra framework comprised of Blacklight 4 to provide a discovery interface and Apache Solr 5 for search; JSON (JavaScript Object Notation) and JSON-LD formatted metadata to build human-readable text to transmit data; and Sesame framework 6 for storing and accessing metadata that is compliant with the Resource Description Framework (RDF) specifications. A core part of the engineering contribution was to evaluate these technologies, and provide advice and consultancy so that outcomes of the project were achieved with maximal functionality but without undue overheads.   During the development of the digital laboratory we utilised work practices adhering to the Agile software development process. Specifically, we used the Scrum software development framework. 7 Scrum is an iterative and incremental process that encourages close collaboration with clients, daily face-to-face communication between team members, and a flexible approach to changing requirements. During the development phase of Alveo, team members met with the major stakeholders on a weekly basis. Meetings alternated between demonstrating the latest version of the system (resulting from the previous two-week period of development) and planning work for the next two weeks. Ad hoc communication with stakeholders took place on an almost daily basis via email, video conference, and an online issue tracking system. By collaborating closely with stakeholders during the development process, there was a greater likelihood that the resulting software system would meet the needs and expectations of those stakeholders.   We argue that large digital projects have better outcomes when software engineers work not  for but  with humanities scholars (Teehan and Keating, 2010; Dombrowski, 2014; Bradley, 2009). The contribution of software engineers to digital humanities projects is not only to implement a digital or computational solution to a problem but to also be highly engaged with the discipline, to introduce options and advice, and to react rapidly and flexibly to changing requirements at all stages along a project timeline.   Notes 1. http://alveo.edu.au/. 2. http://agilemethodology.org/. 3. https://usegalaxy.org/. 4. http://projectblacklight.org/. 5. http://lucene.apache.org/solr/. 6. http://rdf4j.org/. 7. https://www.scrum.org/Resources/What-is-Scrum. ",
        "article_title": "Alveo: Software Engineering with Humanities Researchers",
        "authors": [
            {
                "given": "Jared",
                "family": "Berghold",
                "affiliation": [
                    {
                        "original_name": "Intersect, Australia",
                        "normalized_name": "Intersect",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/017yqqp60",
                            "GRID": "grid.474047.4"
                        }
                    }
                ]
            },
            {
                "given": "Jeremy",
                "family": "Hammond",
                "affiliation": [
                    {
                        "original_name": "Intersect, Australia",
                        "normalized_name": "Intersect",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/017yqqp60",
                            "GRID": "grid.474047.4"
                        }
                    }
                ]
            },
            {
                "given": "Steve",
                "family": "Cassidy",
                "affiliation": [
                    {
                        "original_name": "Macquarie University",
                        "normalized_name": "Macquarie University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01sf06y89",
                            "GRID": "grid.1004.5"
                        }
                    }
                ]
            },
            {
                "given": "Dominique",
                "family": "Estival",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Denis",
                "family": "Burnham",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Peter",
                "family": "Sefton",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "interdisciplinary collaboration",
            "archives",
            "sustainability and preservation",
            "English",
            "software design and development"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper introduces the Digital Mitford project (http://mitford.pitt.edu), and its practices of correlating prosopography information from TEI markup of its range of texts and metadata in a heterogeneous database of letters and literary texts of multiple genres, by prolific 19th-century author Mary Russell Mitford. Our editing team marks prosopography data (the names of historical people, places, and events), plus publication titles and names of fictional characters by Mitford and the many other writers she references. Our prosopography markup contributes to a detailed, centralized file holding canonical references to correlate our project’s wide range of files. This in turn helps us to generate network diagrams, maps, and other useful visualizations and search features to navigate and document patterns of information in the Mitford corpus. We will share network graphs and other visualizations drawn from our prosopography to facilitate serendipitous discoveries among clusters of texts in the Digital Mitford database. * * * Mary Russell Mitford was featured by Franco Moretti in his book  Graphs, Maps, and Trees, in which he mapped a distinctive range of walkable locations in her popular text,  Our Village. However, Moretti’s selection of this text reflects the bias toward prose fiction in digital literary studies, a bias we seek to redress in the Digital Mitford project, which aims to introduce and correlate Mitford’s letters with her staged dramas, her poetry, and her prose fiction. A long-lived, prolific, and highly networked writer of the first half of the 19th century, Mary Russell Mitford (1787–1855) set important precedents for women as professional writers, yet literary scholarship to date has been divided across period and genre lines in evaluating her most popular works. Scholars of the Victorian period have prioritized the popular prose fiction sketches of  Our Village and have studied these as an index of peaceful English nationalism without attention to the politically provocative elements of her plays. 1 Digital humanists, too, may only know about Mitford’s  Our Village thanks to Franco Moretti. Meanwhile, scholars of Romanticism addressing the verse tragedies concentrate on their complex politics, their topical similarity to Lord Byron’s work, and their position within a newly forming liberal ideology of the 1820s—and these studies make only passing references to the  Our Village sketches (see Newey, 2000; Saglia, 2005; Pietropoli (2008). To address these divisions, The Digital Mitford project formed as a large 20-member team of editors in 2013 and 2014 to attempt a comprehensive scholarly archive of Mitford’s writings, with a larger goal of compiling a database of information from Mitford’s collected writings and letters to help address the gaps in our understanding of the overlap of ‘Romanticism’ and ‘Victorian’ writing, and of poetic, dramatic, and prose forms in the 19th century. Comprehensive scholarly editing of Mitford’s correspondence together with her literary texts stands to bridge the substantial gaps in period-based literary scholarship, and serves to bring to light significant proposography data on the print publishing, artistic, political, and theatrical worlds in which Mitford circulated.   The Digital Mitford calls itself an archive but is actually a network of linked data—a database from which we can extract and study information we are collecting about people and texts of the 19th century. Ed Folsom (2007) and Kenneth M. Price (2009) have each discussed the Walt Whitman Archive as something more complex and expansive than an ‘archive’, though they, too, have used the word ‘archive’ for much the same reasons that we have: because the word meets conventional expectations of comprehensive storage paired with digital access of a large corpus of texts. Our collective project deploys TEI text encoding far more significantly to build a database of Mitford’s writings. Mitford appears to have shared with Whitman a strong inclusive tendency reflecting wide reading and conversational intersections with other writers and text, which makes for a richly diverse database.  We expect that our most important contribution to 19th-century studies will be a heterogeneous database containing XML markup of upwards of Mitford’s 2,000 known letters paired with her literary texts published over a long and successful literary career. Through our editing of Mitford, we aim to make available hitherto unknown data about publishers of periodicals, theatre managers and actors, poets, artists, as well as politicians and educators—an extensive network bonded by mutual influence and support. 2 Unusual intersections are coming to light from our coding of Mitford’s letters, her journal, and  Our Village, demonstrating, for example, that Jane Austen is discussed in the same documents with Lord Byron, William Macready, Gilbert White, and Walter Scott. In light of the prolific discussion of writers, performers, and public figures, we have begun producing network graphs to help visualize social interactions, as well as shared contexts in the discussion of fictional characters and literary texts.  The Digital Mitford project, at http://mitford.pitt.edu, involves a large team of editors working individually and collaboratively in set stages of work. Working with TEI XML, our project involves producing digital surrogates of Mary Russell Mitford’s letters, drama, poetry, and prose fiction and essays, to present these texts as an integrated unit, organized chronologically over a series of years. For the ongoing test-bed stage of our project, we have selected a period between 1819 and 1825, the period in which Mitford was especially active as a playwright and prose fiction writer—two very different genres in which she attained remarkable transatlantic popularity. This test-bed phase represents a complete and significant freestanding effort in its own right. Our editing of the hybrid cross-section of Mitford's texts from the 1820s is demonstrating hitherto unexplored intersections among Mitford’s letters, prose fiction, and verse tragedies during the decade in which her reputation as a dramatist and as a periodical contributor and publisher of fiction was growing in the English-speaking world.  This paper introduces the Digital Mitford’s practices of correlating prosopography information from TEI markup of its range of variant texts and metadata within its heterogeneous archive. The TEI’s guidelines include tools for marking and compiling contextual information in central places—an efficient system of indexing—so that our TEI coding not only presents texts for reading, but also tags and identifies contextual information systematically. Thus, our editing team marks prosopographical data (the names of historical people, places, and events) as well as titles of literary texts and names of fictional characters by Mitford and other writers referenced in the archive. We extract our prosopography markup from editing each text to develop a detailed, centralized file listing contextual information. We then ensure that each reference to a particular individual (often with variable names) throughout the archive points to a single entry in our centralized list of persons (or ‘personography’) that includes a complete and standard name, details on birth and death, significant life events, relation to Mitford, and other pertinent information, including relationships with other people, literary texts, fictional characters, events, and places. We are thus tapping into the potential of the TEI as a mechanism not only for archiving the content and structure of Mitford’s texts but also for compiling and connecting contexts and metadata across hundreds of files. Our coding work is internal and shared among the project team, and all project team members are working in TEI under the supervision and guidance of the principal editor and author of this paper.  Contextual markup of named persons, places, and texts supports our goal to generate indexes and search features as well as to produce charts, network diagrams, maps, and other useful visualizations to navigate and document patterns of information in the Mitford corpus, an extensive archive of many different kinds of texts. This paper will demonstrate our work in progress on data visualizations designed to assist navigation of the Digital Mitford Archive. We have begun to develop network graphs with a goal of facilitating serendipitous research discoveries among clusters of related material. We aim to provide data visualization tools to direct our readers to the important intersections illuminated in the prosopography: to connect the worlds of Mitford’s correspondence, her historical tragedies, and her prose fiction sketches in  Our Village. Our network analysis plots will help to highlight relationships among letters and literary texts, and relationships between institutions—such as periodical publishers and theatre houses. We are also working on designing maps as a guide for users of our site to understand the London postal network of Mitford’s day, as well as relationships between fictional representations of places and locations known to Mitford. In discussing the visualizations produced from our test-bed markup, we hope to model and prompt discussion of new directions in digital archive development, to model XML editions as relational databases.  Notes 1. See, e,g., Lynch (2000). For a critique of the overemphasis on nationalism in recent criticism of  Our Village, see Morrison (2008).  2. Describing Mitford’s interactions with just a handful of women writers, Katie Halsey (2011) has discussed the importance of sharing, promoting, and discussing texts as forms of influence and mutual support. ",
        "article_title": "Visualizing the Digital Mitford Project’s Prosopography Data",
        "authors": [
            {
                "given": "Elisa",
                "family": "Beshero-Bondar",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh at Greensburg, United States of America",
                        "normalized_name": "University of Pittsburgh at Greensburg",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02b394754",
                            "GRID": "grid.447541.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "encoding - theory and practice",
            "networks",
            "organization",
            "xml",
            "bibliographic methods / textual studies",
            "analysis and visualisation",
            "digitisation",
            "project design",
            "corpora and corpus activities",
            "relationships",
            "text analysis",
            "spatio-temporal modeling",
            "English",
            "maps and mapping",
            "visualisation",
            "scholarly editing",
            "metadata",
            "graphs",
            "resource creation",
            "and discovery",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper applies network analysis to study literal and analogical relationships in literary texts, exemplified in annotated epic poems. The method is dubbed ‘anti-social’ because it screens out information about characters in order to study how texts reference places, because human readers tend to screen out complex information about places while attending to characters and their social interactions. Rather than plot a geospatial spread of places, the network analysis plots ‘closeness’ and ‘distance’ through closeness centrality measures, and studying which places are most frequently correlated in lines and stanzas of poetry and their footnotes. The network illuminates a text’s worldview by correlating its mythical and geographical places, and by distinguishing between literal and analogical place references. Network graphs will be plotted from Robert Southey’s ‘Thalaba the Destroyer’ (1801) and Erasmus Darwin’s ‘The Temple of Nature’ (1803), both encyclopedic and planetary in their scope, to compare the imagined world-view projected in each. Revised Proposal The sociological roots of network analysis from the 1920s to the 1950s strongly assert themselves in contemporary digital humanities projects, which often deploy network graphs as a way to view human beings as aggregate particles based on broadly defined parameters of shared context. 1 We in humanities fields have not tended so frequently to apply network analysis in the ways that biologists do, to illuminate interactions across small-scale structural units of a complex system, as, for example, the plotting of molecular interactions in protein synthesis. 2 Nevertheless, digital humanities has welcomed borrowings from the biological sciences, such as Matthew Jockers’ ‘19th-Century Literary Genome’, designed to chart networks of literary influence from about 3,600 novels on the basis of topic modelling and stylometric measures, though as Jockers observes, the patterns of influence he models are ‘impossible’ to prove, despite the quantitative basis of his analysis (Jockers, 2012). Jockers’ work with prose corpora of thousands of texts must necessarily screen out formal complexities and microhistorical contexts in order to build a basis for comparison on a macro scale. By contrast, Franco Moretti’s intratextual network graphs of Shakespeare’s plays in  Distant Reading (2013) have prioritized human interactions or the interactions of fictional characters, evoking in literary microcosm the macro-analytical scale of social network theory. Both approaches—macro and micro—are risky in literary studies: Either we strip network theory of its statistical significance to plot character dynamics in small scale, or we potentially overstate statistical significance in our plotting of literary graphs based on social network theory. By contrast, biologists who apply network statistics to study information exchanged between distinct molecular units work closely with cellular structures in a finely tuned model of close reading and locational analysis that might benefit a structurally grounded literary scholarship.   Inspired by the methods and tools of biological network analysis, I propose an alternative, pointedly ‘anti-social’ approach to networking spatial references in complex poetic forms. Unlike Moretti’s approach to literary networks, my investigation screens out characters and concentrates on the proliferation of place references connected by analogy and juxtaposition in annotated epic poetry. Though literary forms are not, of course, as complicated as biological structures, they nevertheless benefit from the combination of both micro- and macro-analysis built from XML markup and statistically processed to measure patterns of distance and proximity of prolific references that are difficult for human readers to trace without assistance. This paper’s ‘anti-social’ network analysis treats poems as structures designed to network concepts together, so that analogy and metaphor constitute a kind of edge that links nodes together in correlations and juxtapositions that give poetic language a distinctive conceptual structure.  My networks are drawn from scholarly encyclopedic long works of late 18th- and early 19th-century poetry by Erasmus Darwin and Robert Southey, poems that responded to an Enlightenment ‘Information Age’ to compile from voyage publications and travel narratives a knowledge base of the world’s places, cultures, species of life, and properties of nature. Southey’s ‘Thalaba the Destroyer’ (1801) and Darwin’s ‘The Temple of Nature: Or, the Origin of Society’ (1803) represent a poetics that modelled world-systems in layers, studying plant physiology in context with human belief systems, and incorporating empirical observations and experiments with speculative juxtapositions of ideas from the microcosmic to the macrocosmic, from the pollination of plants to the evolution of species. 3 The complicated structural machinery of Southey’s and Darwin’s epics challenges readers now perhaps even more than readers at the turn of the nineteenth century. Without systematic computational assistance, our efforts to ‘read’ these poems might well expose us in our twenty-first-century weakness: we cannot easily assess their elaborate interplay of contexts, their investigative reading of a centuries-old archive of records on cultural encounters, their blending of ancient and contemporary sources from voyage logs and travel narratives. To examine such poems as a kind of relational database, considering their structural components—books, stanzas, prose notes, and scholarly apparatus—benefits from the methods and measurements of network analysis.  The hybrid or composite constructions of poetry with frequent prose appendages (or extensive footnotes) was especially marked by the late 18th century in the elaborate scientific poems of Erasmus Darwin and the historical-cultural-mythopoetic syntheses of Robert Southey. Gerard Genette has theorized the paratext as transactional with readers, a boundary of exchange, and as such in the complicated long poems of the early 19th century—with their complex formal verse patterns bordering on prose discourses of authority—the paratext interface is especially challenging to the processing powers of an unassisted human reader (Genette, 1997). As Clare Kinney observes, the formal structures of poetry with embedded prose generate patterns of meaning through ‘cross-referencing of motifs over the spread of a narrative’, but the multiplying reference points of such poems from levels of analogy to annotations are very difficult to follow systematically, and scholarship has tended to flatten discussion of such poems to a discussion of main plots and salient ideas without the benefit of systematic study (Kinney, 1992, 13). If we apply network analysis tools to the structure of these complex texts, we might study them as systems of information exchange in a way related to the network plots of cell biologists, and we may thus be able to observe systematically how references to the world’s geographic and mythical places are held together within an ornate literary structure.  The long poems of Darwin and Southey synthesize world-views—visions of the earth as a planetary system—based on comparisons and contrasts between and among places referenced in poetic language and prose annotation. Places are not always referenced for geospatial accuracy, but more significantly are layered together by analogy. In long poems that elaborate upon non-European cultural contexts through place references to Arabia, Persia, Malaysia, Polynesia, China, and Peru in both main text and paratext, we can examine how these places are patterned by relation to each other in the British global and imperial consciousness of these poems. We can plot with network graphs which analogies—which locations—are most frequently referenced together as measure of ‘closeness centrality’. We can also see which locations are plotted remotely, many path steps removed from centrally plotted locations and with few comparative reference points, and thus mark what places are outlying and remote on the imagined horizons of a world of locational and cultural associations.  To produce an ‘anti-social’ network of locations in these long poems, I have begun with structural and contextual markup in TEI, to collect place references and their positioning within main text line groups and within paratext notes. After extracting this place data and its precise structural position within the poems systematically using XSLT and XQuery, I work with Cytoscape’s tools for network visualization and statistical analysis, originally designed for microbiology, to produce network graphs and tables of statistical information that indicate patterns of relatedness across complex structural units of texts. My project investigates the following questions:  a. How can we study the way complex literary texts layer and cross-reference places by analogy with each other?  b. Can the TEI’s (and XML’s) structural and contextual markup be productively deployed in literary studies to study networks of correlated concepts?  c. In correlating concepts of place in a long scholarly epic, what patterns can we see in networking geographical with mythical places—that is, place connections that cannot be literally mapped with latitude and longitude coordinates but that nevertheless help to construct an imagined topography for readers of poetry?  d. Can we apply the statistical measurement tools of network analysis—stepwise distance between nodes and closeness centrality—to understand how geographic and mythic places are linked by ‘closeness’ and ‘distance’ conceptually, not just geographically?  At the time of drafting this proposal I have begun an ‘anti-social’ networking study of Southey’s ‘Thalaba the Destroyer’, a 12-book epic poem with especially prolific annotations and complex place-time referentiality. To survey the work in progress, please see my project site http://ebeshero.github.io/thalaba/. An early record of the project, designed to introduce students and literary scholars to network analysis, appears on my blog, http://digitalromanticist.wordpress.com/2013/08/23/spectacular-intersections-of-place-in-southeys-thalaba-the-destroyer/. Notes 1. For example, see the Six Degrees of Francis Bacon (http://sixdegreesoffrancisbacon.com/) and the Kindred Britain (http://kindred.stanford.edu/#) projects, drawn from the Oxford Dictionary of National Biography to map human relationships spanning centuries. 2. See, for example, the network visualizations used to predict probable locations of proteins produced by the Washburn Lab and Proteomics Center: http://research.stowers.org/proteomics/ProtNetAnal.html. 3. On the scientific bases and the layering and juxtapositioning of worldviews in these poems, see Fara (2012) and Smith (2010). On Southey, see Majeed (1992). See also Porter (2011), which addresses how Southey’s commonplace book documents a systematic use of Enlightenment scientific methods to collect evidence of cultural patterns that he would apply in his long poems. Without using computational methods, however, neither Majeed nor Porter can study how those patterns were plotted in the long poems within the space of a chapter or an article.  ",
        "article_title": "World-View from Poetic Structure: An “Anti-Social” Network Analysis of Robert Southey’s and Eramus Darwin’s Epic Poems",
        "authors": [
            {
                "given": "Elisa",
                "family": "Beshero-Bondar",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh at Greensburg, United States of America",
                        "normalized_name": "University of Pittsburgh at Greensburg",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02b394754",
                            "GRID": "grid.447541.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "encoding - theory and practice",
            "linking and annotation",
            "poetry",
            "networks",
            "xml",
            "drama",
            "geospatial analysis",
            "information retrieval",
            "analysis and visualisation",
            "relationships",
            "text analysis",
            "english studies",
            "interfaces and technology",
            "spatio-temporal modeling",
            "English",
            "maps and mapping",
            "genre-specific studies: prose",
            "visualisation",
            "scholarly editing",
            "virtual and augmented reality",
            "graphs",
            "knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " While much of digital humanities considers the affordances provided by computational tools, a less noted and more serendipitous consequence of its emergence is the availability of new metaphors for rethinking perennial philosophical questions. The concepts underlying the  workset, a notion that has emerged out of the Workset Creation for Scholarly Analysis project of the HathiTrust Research Center, provide such a metaphor.   The  workset is a collection, typically scholar-built, consisting of a select subset of digitized texts from a larger digital library’s collection (Fenlon et al., 2014). Important for our argument in this paper is the property of worksets that they need to be both stable and dynamic simultaneously. On the one hand, the workset must be coherently stable for the purpose of citability; if the researcher has performed computational analysis on the workset and published the result thereof, then the integrity of the content of the workset at the moment of that analysis needs to be maintained citably for the purpose of reproducibility of results. On the other hand, since the workset itself is a part of a larger digital library, individual elements of the workset are, conceptually speaking, pointers to the corresponding elements held in the library’s collection; those pointed-to elements in the library are mutable or dynamic—for example, the scanned-and-OCR-ed copy of a pointed-to volume in the larger digital library may, at a later point in time, be replaced by a different OCR-ed copy because the latter is of better quality, and a pointer to a volume may end up pointing to null because the library may have had to remove access to the pointed-to volume in response to a takedown notice from a publisher. We argue that this simultaneous, and antagonistic, demand on a workset, of  citability as well as mutability, is a useful (though imperfect) metaphor for the contradictory qualities that text itself embodies. Additionally, we argue that the analogy between the notion of a workset and the notion of text itself, by providing a common unifying framework with which to think about such activities as editorial scholarship and anthology building, can help problematize the distinction between seemingly disparate aspects of textuality.   Bernard Cerquiglini has pointed out that not only did the modern idea of a text as standard and definitive, in which ‘every written word’ is a reflection of a supposedly immutable originary authorial intention,  not have a strong counterpart in antiquity (1999, p. 4), but also that, at least for the first two centuries after the first written works in French started to appear, there was, in mediaeval France, an especially heightened incidence of variance in the written text: ‘in the Middle Ages, literary work was a variable’ (1999, 32); according to Cerquiglini, this suggests that, especially in any nascent literature, variance  is the natural condition of existence of written texts (1999, 21). Elements of the text were mutable, though mutability of text declined over time with progressively wider dissemination of the technologies of textual production and, especially, of printing. Members of the Prague School (Bogatyrëv and Jakobson, 1982, 42) and Russian formalists such as Vladmir Propp (Propp, 1958, 1) have pointed out that this quality of variance has been retained in the genre of the folktale and its retellings. At the same time, as Propp also demonstrated in the case of Russian, functions in folktales serve ‘as stable, constant elements’, their number is ‘limited’, and their sequence within folktales often tend to be ‘identical’ (20). Thus, a folktale, in spite of the variance that its constituent elements might exhibit, nevertheless has a typically stable and easily recognizable structure. A folktale, arguably a seminal embodiment of the textual condition, is thus—if one were to employ the terminology associated with a workset—both a  mutable and a  citable object. The analogy breaks down in one respect: a folktale’s structure is  narrative, that is, a  relational order obtains between the constituent elements of its text, whereas a workset (in its essential implementation) does not necessarily have any notion of internal ordering—one of the reasons why the metaphor is not perfect.   We argue that this interesting, though imperfect, analogy between worksets and text may be useful, by foregrounding the notion of mutability, in approaching the long-standing debate among literary theorists and computation/information-oriented philosophers regarding notions of textuality. For example, the view from computation/information-oriented philosophy, influenced by technologies such as TEI, is that ‘text is an ordered hierarchy of content objects’—the so-called OHCO thesis (Renear, 1997, 118); a typical view of literary theorists such as Jerome McGann, on the other hand, is that texts, especially poetic texts,  are ‘recursive structures built out of complex networks of repetition and variation’, and hence cannot be accommodated within the OHCO paradigm (Hockey et al., 1999). David Golumbia diagnoses this disagreement in perspective as irreconcilable (2009, 107), while Patrick Sahle notes how the onto-epistemological peculiarities of the OHCO paradigm help the articulation of some among various pluralistic textual concepts more than others (2013, 396)—versioning being among the latter (Schreibman, 2002). We argue that the emphasis on mutability imposed by thinking of text in terms of the metaphor of the workset is an instance of how a computationally inspired framework can accommodate variation. The variation and mutability in the workset paradigm is diachronic, whereas the variation that McGann refers to is synchronic; we argue that any synchronic variation is representationally equivalent to a set of diachronic variations.   The workset metaphor provides a way to conceptualize the notions of the two different kinds of editorial scholarship—editing of a single text, and the creation of an edited anthology of multiple texts—within the terms of a common framework. In a 1995 essay, Gunter Martens argues that ‘text’, in the editorial sense, should be understood and interpreted as a process unfolding in time, ‘so that the “text” is understood and interpreted as an historical process’ (1995, p. 195). Martens’ subject is the writer sitting in front of a piece of paper, writing, rewriting, and changing the text. Editorial scholarship, Martens argues, must produce neither the beginning point nor the endpoint of that process, but the process itself as it unfolds over time, that is, as the text mutates. Clearly, such an editorial practice is impossible to create, except as a utopian ideal, within the paradigm of printed editions, but it can be accomplished relatively easily using the conceptual idea of worksets. The historical process that the text embodies can be conceptually approximated by the same underlying mechanisms in a workset that provide support for its mutability. In addition, if we consider this idea of text as process also in its  social dimension, then different versions of a text (as, for example, a book going through various editions, with its textual content undergoing corresponding modifications as the social context of the book changes), constitute mutations over time, and can be accommodated within the conceptual notion of the workset.    We can stretch this idea further. Different translations of a book, or even a sequence of books in which one book shows a strong influence of another, can arguably be considered as constituting a chronological process of mutation of the same ‘text’—a text mutating through different versions of itself. If we stretch the notion of ‘version’ even further, then at the very limit of the notion of ‘version’, one could arguably consider books sharing a common theme as different ‘versions’ of the  same book. The study by Fenlon et al. (2014) of how scholars organize their worksets reveals that scholars often have their workset constituted around a common theme. Such a workset built around thematic commonality is akin to an anthology of different texts, rather than to different versions of the same text. However, we will show that the difference between different versions of the same text, and an anthology of thematically similar texts, is a slippery difference. What conventionally distinguishes a collection of texts as versions of each other from an anthology of similarly themed material thrown together is a corresponding distinction between the implicit notions of temporally mediated causality and spatial synchrony. We will show how the ‘isGatheredInto’ property, proposed by Wickett et al. (2014) to mediate between part-of and member-of types of relationships (and to subsume them both), can provide a way to conceptually unify these twin, but seemingly disparate, ideas of versions-maintenance and thematized collectability (operationizable as the editorial/textual-critical practice of variorum creation, and the curatorial practice of anthology making, respectively). The first of these ideas/practices concerns temporality, and the second concerns spatiality. Such a unification is an illustration of how the consideration of systems that, from an engineering point of view, satisfy a requirement (in this case, the requirement of meeting the simultaneous and oppositional demands of citability and mutability) can  also be helpful, by means of the application of the same principles, in thinking about the metaphysics of textuality, within the context of which literary theorists have noted a similar unification of temporal ‘play’ and spatial ‘structures’ (McGann, 2001, 178).   ",
        "article_title": "Approaching Textuality with the Metaphor of the Digitized Workset",
        "authors": [
            {
                "given": "Sayan",
                "family": "Bhattacharyya",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "J. Stephen",
                "family": "Downie",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "philosophy",
            "libraries",
            "literary studies",
            "corpora and corpus activities",
            "archives",
            "museums",
            "digital humanities - nature and significance",
            "English",
            "philology",
            "morphology",
            "GLAM: galleries",
            "folklore and oral history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital film archives in addition to preserving cinema should also accommodate and provide for the computation of its cinemas by means of the metadata provided to represent the content in them. The provision for the computation of cinema could lead to various applications like semantic search, automatic genre clustering and classification, special visualization tools, and robust comparisons of movies. This paper describes a data model for storing the metadata of a film in an archive based on a discrete theory of cinema that has been in development for the past four years based on Indic theories and other theories of drama and film (Muni, 1996). This theory reduces cinema to a hypergraph composed of tags that assume its full form only with collaborative tagging and extensive visual and textual computing. A system called CinemaScope is also being developed based on this data model which uses HypergraphDB as its database and is being designed and developed to be a semiautomated annotation database system.  Overview of Theory  Cinema is ontologically a three-tier structure:  1. Spatio-temporal flow controlled play of light and sound.  2. Mere-temporal ‘flow’, which is visually created by a play of 25 fps.  3. Trans-temporal content, which is embedded in a ‘flow’. Cinema concludes in a thematic unity. Unit of a discrete contiguum of cinema is punctuation. Idea of punctuation is inspired by Leibnizian distinction between ideal point and actual point on the one hand and continuum and discrete contiguum on the other (Leibniz, 2013). Punctuation is a form of an actual point that tells apart two recognizable and non-identical discernible contents. It has a structure <entity1|entity2, relational context> or graphically <node1|node2, edge>.  There are three classes of punctuations based on the three tiers of ontology of cinema:  1. Temporal punctuation.   2. Vectorial punctuations between embedded content.  3. Mereological punctuations between vectorial contents and the thematic conclusion of cinema. This way the form of punctuation constitutes discrete contiguum of cinema, which is computationally reducible to a giant graph that tells truth about cinema as art.  Ontologically there are categories of discrete content that are embedded in punctuated units of flow like shot and episodes. Some of the categories form the content part of the cinematic narrative (like locale, characters, events, and actions) while the rest (camera work, editing, re-recording) form the expressive part of it (Chatman, 1978). Forces operative on these entities as vector punctuations create kinetics of cinema toward thematic conclusion. The motion in films is through the movement of story or plot from the beginning to the conclusion. Such a motion towards theme is punctuated by points of events and points of considerations. It is through the sequence of events that connection to the theme is established. These events are punctuated with points of considerations. Story can be rendered as sequence of the points of events, and under each event there are several considerations (Dhananjaya, 2004). There are scalars between the sequences of cinema that are merely informative and do not account for the motion of the plot.  Overview of Data Model  In Hochin (2006) and Hochin and Tatsuo (2000), a data model called Directed Recursive Hypergraph Data Model (DRHM) has been described in which the content of multimedia is reduced to nodes and edges of a hypergraph. In Radev et al. (1999a; 199b), the structural and behavioral aspects of data that form multimedia information systems have been modelled as a graph-based object-oriented model, and a possible data model for film is shown. These approaches suggest that a graph-based data model for cinema based on the discrete theory of cinema proposed in the previous section would best represent it. Punctuation or point <node | node, edge> can be seen as triple of tags. This will make possible computing of the discrete contiguum of cinema as a graph. Also there are complex relations between the various entities of cinema which is best represented by hyperedges. The graph data model should accommodate the various ontological entities and their punctuations, the mereological punctuations as well as the vectors between episodes and the considerations between events. The partial data model for the cinematic hypergraph described is represented in the form of Venn diagram as shown in Figure 1.    Figure 1. Venn diagram of cinematic hypergraph—data model. Architecture of CinemaScope The architecture of the tagging system we propose for CinemaScope is a web-based collaborative one. The annotation schema for the hypergraph data model in theory captures the whole graph. But in practice manual tagging, which is required for the annotating the themes, considerations, meanings, and vectors of cinema, reveals only a sub graph of the original graph. This is because the graph also contains codes and conventions (which are many times dependent on the individual viewer’s perspective) that regulate the narratives and enrich their pure meaning. It is therefore essential that the tagging of the cinema be made social so that most of the graph is captured. The high-level architecture of CinemaScope is given in Figure 2.  .  Figure 2. Basic architecture of CinemaScope. Methodology of CinemaScope The database for the system is built on both relational database and graph database (HyperGraphDB). The relational database is used to store all the direct information about the cinema, like its cast, basic plot structure, related files (like script and subtitles), and other relevant information. The graph database is used to graph the tags which have a graphical relation between them. HypergraphDB is a graph-based as well as an object-oriented database with a Java-based API that allows objective modelling of all categories of tags. For example, ontological categories are modelled as follows:  public class OntologicalEntity{   private String entityName ; //Entity Name   private Map < String , Object >attributesNameAndType  = new HashMap < String , Object >(); //Contains the attributes with their names and values   public void setEntityName ( String entityName ){  //Setting The entity Name }  public String getEntityName (){   return entityName;  }  public Object getAttributeValue ( String attribute ){   return attributesNameAndType . get (attribute );  }  public void fillAttribute ( String value ){  /*Code for filling the attributes*/ } } The pre-processing stage involves identification of shots, episodes, and the tagging of basic ontological entities. The pre-processing step cannot be completely automated owing to the limitations of current vision and audio processing algorithms. But this stage of the annotation can be aided with a number of supporting files and techniques. For example, movie screenplays and subtitles that are freely available on the Internet help in giving hints of annotation to the user. Even the processes that are completely automated require manual intervention and editing. There are various shot transition detection methods (Boreczky and Rowe, 1996) along with techniques for identification of camera properties like depth, movement, and angle (Benini et al., 2010). The camera properties identified are dependent on the definitions used in the methods, and they are not completely accurate. The tagging system should provide options for manually editing and supervising the shots and camera properties. Film scripts and subtitles aid in the tagging of ontological entities by supplying hints. The script is first aligned with the subtitles for time stamps based on the work of Ronfard and Theung (2003). A basic version of this has been implemented based on the work of Larissa Munishkina et al. (2013). An example hint given by the system for the film  Indiana Jones and the Raiders of the Lost Ark is as follows:  E.g. Scene #1 (0:00–2:15)  Characters: Indiana Jones, Baranaca  Locale: High Jungle, Peru  Things: Gun, Idol  Events: Shooting, Running The Collaborative or Social tagging follows the pre-processing step, and it helps in the creation of tags and links in cinema. The user is given the provision of defining the relations, which is made possible by the use of HyperGraphDB. Hertzum et al. (2002) suggests that collaboration of film archives should facilitate different archives in identifying a common ground on which to base a collaborator and in acknowledging the distinctiveness of each archive. The architecture of CinemaScope allows identifying a common ground, in terms of pre-processing tags as well as distinctiveness, by allowing the users of different background to provide interpretations and considerations of the story in terms of vectors, relations, and links (each with a different label). The various relations added by the user and the tags obtained from the pre-processing stage build up the graph, which could later be used for various applications like searching. For example, the following code snippet answers semantic queries like ‘Find all the camera properties used when character is holding the whip’:  HyperGraph graph  = new HyperGraph ( HyperGraphLocation );   List < CameraTemporalRelations > cameras  = hg .getAll (hg .type ( CameraTemporalRelations ));   List < CharacterThingTemporalRelations > CTR  = hg .getAll (hg . and (hg .type ( CharacterThingTemporalRelations . class ), hg .eq ( \"spatial_relation\" , \"hold\" ), hg .eq ( \"thing_name\" , \"whip\" )));   Summary  The paper describes the discrete theory of cinema and explains the hypergraph data model of cinema. The paper also describes CinemaScope, a semi-automated web-based collaborative film archive based on the hypergraph data model for annotating the film. This project is different from other movie annotation projects (ANSWER, 2009; Jewell et al., 2005); Lombardo and Damiano, 2010), which fail to capture the flow of cinema and make the computational representation of cinema in a database very descriptive and computationally infeasible.    Figure 3. Prototype of CinemaScope annotation system. ",
        "article_title": "Hypergraph Based Collaborative Film Archive",
        "authors": [
            {
                "given": "Sandeep Reddy",
                "family": "Biddala",
                "affiliation": [
                    {
                        "original_name": "International Institute of Information Technology, Hyderabad (IIIT-H), India",
                        "normalized_name": "International Institute of Information Technology",
                        "country": "India",
                        "identifiers": {
                            "ror": "https://ror.org/00qryer39",
                            "GRID": "grid.462393.9"
                        }
                    }
                ]
            },
            {
                "given": "Navjyoti",
                "family": "Singh",
                "affiliation": [
                    {
                        "original_name": "International Institute of Information Technology, Hyderabad (IIIT-H), India",
                        "normalized_name": "International Institute of Information Technology",
                        "country": "India",
                        "identifiers": {
                            "ror": "https://ror.org/00qryer39",
                            "GRID": "grid.462393.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "repositories",
            "metadata",
            "knowledge representation",
            "archives",
            "video",
            "film and cinema studies",
            "audio",
            "sustainability and preservation",
            "English",
            "multimedia",
            "crowdsourcing",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The main aim of the paper is to show how the data of the Russian National Corpus can be used to explore nonlinguistic objects. The National Corpus is created as a well-balanced, marked-up text collection оf almost 300 million documents of different ages (18th–21st centuries), followed with rich metadata. Being one of the best national linguistics corpora it can also be regarded as a store of cultural memory and social reflection. Then the quantitative analysis of the change of frequency of some conceptually important lexeme or qualitative analysis of its various contexts in different periods can lead us to some new analysis of cultural trends and social processes, focusing primarily on their reflection in social consciousness. It should be emphasized that though some lexical change can be explained by extralinguistic reasons, the study is based on texts that are not thematically connected with this reason. For example, we observe an abrupt increase of the use of lexeme ‘woman’ in the second half of the 19th century, which surely should be associated with the first wave of emancipation in Russian society. But these are not manifestos, but mostly novels that are being analysed. The frequency increase for ‘woman’ in the 1850s signals a change in readers’ interest. This change and its connection with social processes and events are the objects of investigation with the methods of corpus analysis, distant reading, and culturomics. The paper doesn’t claim to make some profound sociocultural research, but the main focus will be on methodology. I will introduce the medium data approach and will argue that it can be used for cultural research, and that it allows solving some of the well-known problems of big-data analyses in textual data. First I will observe the culturomics approach and the benefits of National Corpus use instead of Google Books. Second I will concentrate on one case of medium data cultural research: gender naming in the 19th and 20th centuries in Russia and the Soviet Union. I will show how context analyses can shed light on unexpected data emission, and how competition of lexemes reflects changes of social consciousness. Culturomics was first declared as a new scientific method in a 2010 paper in  Science magazine titled ‘Quantitative Analyses of Culture Using Millions of Digitized Books’ by a group of scientists headed by Jan-Baptiste Michelle. Though very influential and inspiring, the method also has been widely criticized. The two main problems of the method mentioned most often are the trivial results and the dirty, nonreliable data. The methodological problems of culturomics research seem to be the other side of its benefits: the big textual data only allows comparing the queries known in advance. We can observe the effect of censorship by the decrease in Mark Chagall mentions, but we have no instrument to compare the change of semantic context of Chagall’s mentions that accompanied the beginning of content restrictions. That means that we can notice how the political changes are reflected in culture, but we still lack the instrument to explore these processes thoroughly.  In contrast to the influential trend of Big Data, I propose a concept of nedium data. Medium data is the amount of data that allows for quantitative and qualitative studies. The main characteristics of the medium data are  • The reliability of sources, which metadata can be filtered manually.  • The sufficiency of the data amount for reliable statistical measures.  • The possibility of additional semantic mark-up. The medium data concept serves to oppose the current practices where computational methods tend to ignore the complexity of the humanitarian sphere. I argue that the quality of the research can benefit much from the contamination of statistical and computational methods, with expert manual analyses possible only with very pure, precise data of not a tremendous amount. Although the primary data filtration will in this case be a matter of the researcher’s responsibility, this situation really doesn’t differ much from any natural science case, when the researcher has to provide the specific conditions of the experiment. Though the Russian Natural Corpus is much smaller than the Google Books corpus, its dataset surely has some advantages. First, every document has rich metadata. This allows counting not only the edition date but also the creation date, which can be quite important for studying the Soviet period, as its published texts can demonstrate a significant lexical bias due to censorship. Second, the mistakes of object recognition are very rare in RNC. The morphological mark-up, which plays an important role in a morphologically rich language such as Russian, is diverse and multivariate in RNC, while it is limited to POS-tagging in Google n-grams. The complex morphological mark-up gives an opportunity to make distant queries, which are targeted to represent syntactic relations. For example, the query that consists of the verb ‘to steal’ plus the noun in the accusative extracts the change of consumption needs for different periods (what people steal in different times is what they actually need). Their comparison can inspire some new level of social and cultural research. The textual collection and the markup of the Russian National Corpus thus give us an example of the medium dataset that can be used not only for language investigation but also for social research.  How can we benefit from a medium dataset? The research can focus not only on frequency change but also on qualitative context variation. It is not the words themselves but the semantic concepts (synsets, that are different in every period) that are being studied. Medium data allow disambiguation of semantic polysemy, which is usually impossible with big datasets and sometimes can cause damaged results. The semantically close contexts can be merged into classes that enable more transparent and still reliable analyses. The most significant difference between culturomics and medium data can be formulated as follows. Culturomics research results in an overall graphic that often demonstrates quite trivial dependencies. The medium data method allows for treating the graphics not as a diagnosis but as symptom, which serves as an impulse for further research. Finally I will focus on one model case of the medium data method. I will compare the frequency of the words ‘man’ and ‘woman’ in the 19th and 20th centuries. The comparison of two frequency graphs shows that the word ‘woman’ is much more frequent than the word ‘man’ in the both centuries. Is it that women are more often written about? Or are men referred to with some other lexical means? If we compare the two words ‘muzhik’ and ‘baba’, which in the 19th century are used as gender terms of low class, we get the opposite picture: males are more frequent than the females. The answer can be drawn out of the context analyses. The reference of males in general is rarely direct in the 19th century, but mostly implicit together with some specific lexical means, characterizing age, social, or professional status. This contrasts much with female references, for which the gender idea is much more important than the social occupation. I will also follow the changes in male and female word usage that take place in Soviet and post-Soviet epochs.  ",
        "article_title": "Medium data method for cultural studies: the case of gender studies in Russian National Corpus.",
        "authors": [
            {
                "given": "Anastasia",
                "family": "Bonch-Osmolovskaya",
                "affiliation": [
                    {
                        "original_name": "National Research Unversity Higher School of Economics Moscow, Russian Federation",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "corpora and corpus activities",
            "English",
            "data mining / text mining",
            "lexicography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Not all readers finish the books they start reading. Electronic media allow us to measure more precisely how this ‘drop-off’ effect unfolds as readers are reading a book. A curve showing how many people have read each chapter of a book is likely to be progressively going down: the more we advance in the book, the less readers got to this point. This article is an initial study about the shape of these ‘drop-off’ reading curves.  The data we analyse were gathered from readers of a series of novels of different length written by one single author. During the last twenty years, Daniel de Roulet wrote a nine-book saga about the interleaving stories of two families over a time range of seventy years. In 2014, for the publication of the concluding tenth book, he decided to release an alternative retelling that constituted of a reordering of all the 297 chapters, forming a coherent saga titled  La simulation humaine. This new saga was made available free of charge to the general public in the form of a website 1 and a mobile iOS 2 and Android 3 app. It has been composed in a way that is readable from one end to the other. This creates the possibility of reading nine chosen subsequences, each one making for a self-standing story. Each possible reading path is characterised by its number of chapters and is also given a title, as shown in Table 1 below.    Story title Number of chapters   Une fleur de cerisier 1   Sur les barricades 1   Les liquidateurs 1   Air Force One 4   Fukushima au début 5   Le pucelage d’un Helvète 10   L’ingénieur et la fillette 30   Fissions 33   La réalité, mais digitale 38   La simulation totale 174 to 297   Table 1. Stories of  La simulation humaine and their respective sizes, in chapters.   Methodology  Formally, the collected data are very similar to state-of-the-art website analytics. Every time a chapter is loaded, it is possible to know the precise date, time, and type of device used. Additionally, each reader is uniquely identified using a browser session and a cookie on the website, or a device identifier on the mobile app.  Over a period of eight months, a total of 7,000 chapters were read, out of which approximately half were not relatable to regular readers (be it search-engine queries or users refusing to be tracked). For this study, we thus kept the subset of data where at least two chapters were related to the same person. We also left out the paths presenting clear test patterns (for instance, paths consisting of multiple repetitions of the same chapters) or no overall significant reading time. In all, we considered 310 unique reading paths. Each path is represented by a chain of tuples ( c i ,t i), with  c i being a chapter and  t i the time the reader spent on it (that is, the interval in seconds between start of  c i and  c i +1). Given the average reading speed of an educated adult (Kershner et al., 1964) and the relative homogeneity of our corpus, we marked transitions as ‘skipped’ when the said time interval was less than 60 seconds, thus considering the chapter as not having been read. We then aggregated the total number of read chapters and plotted them according to their respective stories’ orderings, resulting in one  reading curve for each story.   Characterisation of Reading Curves  All reading curves are expected to start with a strong drop-off effect, corresponding to the considerable number of readers who merely peek at the start of the story, but don’t finish it. Of course, this effect is especially noticeable in the context of digital and mobile content (‘Localytics Indexes’,  Q3 2014). 4   The classic ‘drop-off’ reading curve is characterised by an exponential drop-off as illustrated in Figure 1. In our samples, this phase was shown to typically last for the first three or four chapters of the story. Then the rate of the monotonic drop-off can vary depending on the attractiveness of the texts. Figure 1 shows two examples, one with a very high drop-off rate and another one where this effect is present but less important. Notice that not only is the drop lower in the second case but the effect also wears off earlier.    Figure 1. Two examples of exponential drop-offs with different rates/slopes. More interestingly, some ‘drop-off reading curves’ are also characterised by a plateau that occurs typically once the first chapters are passed. This could be because readers who reach a certain point in the book are convinced to go on till the end. Figure 2 shows an example of this plateau characterised by an almost null drop-off rate once chapter 6 is passed.     Figure 2. An example of a curve with a plateau in  La réalité, mais digitale.  In some cases, the drop-off reading curves show unexpected gaps in the reading plateau. This case could typically be attributed to readers who skipped one or several chapters (hence resulting in low reading times that were filtered out of our data), but then resumed reading later on in the same story. This ‘skimming gap’ could thus be interpreted as a mild sign of boredom, not sufficiently strong to stop the reading but significant enough to speed it up. This phenomenon can be seen in Figures 2 and 3.    Figure 3. A skimming gap in  L’ingénieur et la fillette.  Moreover, some other stories never achieve the plateau stage and continuously lose readers as the chapters go by. In the example shown in Figure 4, this decrease also features a couple of stop points, which could possibly be interesting starting points to investigate why the subsequent drops occur.     Figure 4. Continuously decreasing readers in  Fissions.   From Reading Curves to Chapters Classification These simple examples show the potential richness of drop-off curves among the various reading analytics curves. Two reading regimes can be identified from this initial study: the immersion mode, characterised by very small drop-off rate, and the critical mode, characterised by dropping and skimming behavior.  We segmented the chapters of our corpus using these two categories and trained a machine learning classifier to try to identify key features to predict whether a chapter is immersive or leading to a potential drop-off. We decided to use an implementation of a J48 pruned tree, given the generally good scores obtained by this method in computational linguistics (Pedersen, 2001; Youn and McLeod, 2007) and general purposes classification (Omid, 2011). A visual representation of the resulting tree is shown in Figure 5.    Figure 5. J48 tree predictor for critical and immersion reading modes. This structure yields a 95% correct classification in a 10-fold cross validation on our dataset, which makes it overall a pretty accurate predictor. It basically confirms our empirical observations on the general shape of the reading curves, putting the start of the immersion phase latest after the fourth chapter. Interestingly, it also hints that using shorter chapters in the critical phase could hasten the immersion process. Furthermore, this result shows that the other features characterising the chapters—such as their position in the saga, length in words, or presence of main characters—had no significant influence on the drop-off rate prediction.   Conclusion and Further Works  We are aware that these preliminary results may be highly linked to the studied corpus and not generalizable. However, the concepts and tools that we proposed in this article could well be extended or adapted to look at more general reading patterns, with or without the active participation of the authors of the texts to be analysed.  As more people read  La simulation humaine, we expect to collect more analytics and be able to refine our results, as well as confirm or falsify our hypotheses with deeper quantitative analyses. Additionally, we’ll aim to propose tools that may detect critical reading phases, suggest improvements to the author, and predict drop-offs for new stories.   Acknowledgements  This research was made possible by a very precious and close collaboration with Daniel de Roulet, Swiss architect, computer scientist, and author. This research is supported by the  Fondation Jan Michalski pour l’Ecriture et la Littérature.   Notes  1. http://www.simulationhumaine.com. 2. http://appstore.com/la-simulation-humaine. 3. http://play.google.com/store/apps/details?id=com.simulationhumaine.app. 4. http://www.localytics.com/resources/app-indexes-q3-2014/. ",
        "article_title": "Anatomy of a Drop-Off Reading Curve",
        "authors": [
            {
                "given": "Cyril",
                "family": "Bornet",
                "affiliation": [
                    {
                        "original_name": "DHLAB, EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "DHLAB, EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "publishing and delivery systems",
            "media studies",
            "user studies / user needs",
            "literary studies",
            "bibliographic methods / textual studies",
            "text analysis",
            "English",
            "crowdsourcing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Thousands of resources in dozens of Australian Indigenous languages are becoming available online through the Living Archive of Aboriginal Languages (www.cdu.edu.au/laal). Many of these resources were developed in bilingual school programs in remote schools across the Northern Territory over several decades, and more materials continue to be added from other communities and languages of this region. This archive provides access to a wealth of written and illustrated texts in endangered languages, many of which were developed from stories told by community elders detailing ancestral knowledge and life, as well as everyday life and contemporary stories, primers, translations from English, instructional materials, memoirs and other genres. These texts are useful as an academic resource for various domains, such as linguistics, anthropology, education, history, natural sciences, Indigenous knowledges, environmental studies, and visual arts. They are also (re)finding their place in the cultural lives of Aboriginal people living on traditional lands, now that there are only a handful of officially sanctioned bilingual programs. The coming to life of this archive opens up significant questions in the field of digital humanities, around postcolonial digital archiving, the curation and uses of cultural heritage materials, the documentation of endangered languages, and studies of the appropriation by Indigenous peoples of digitising technologies for their own ancestral knowledge and culture work.   The creation of the Living Archive has involved the digitisation of thousands of small books, the identification and collation of metadata, the development of a simple interface for search and browse, and efforts to locate the original storytellers and illustrators to seek permission to make these materials public on an openly accessible website (Bow et al., 2014). While the archive serves as an infrastructure for research and other academic work, one challenge was to design a web interface that would also meet the needs of Indigenous community members. The use of a map interface to allow navigation by place or language, and the presentation of books by cover image, both allow the site to be navigated with minimal or no textual input. Standard search and browse functionality, including full-text search, allows more conventional text-based interaction with the materials. User testing included Indigenous users who provided a useful perspective on the website as well as the content and potential uses of the archive. The engagement of the traditional owners of the materials and content has been a key component of developing a generative, agentive archive, rather than simply a storage place for materials.      Screenshot from the Living Archive of Aboriginal Languages website, www.cdu.edu.au/laal. The process of bringing the archive to life has involved carefully articulating and enhancing how its liveliness brings communities together with their schools, old people with young, and students and researchers from around the world with knowledge authorities in the communities of origin. It has been important to maintain careful consideration of traditional approaches to ownership of knowledges in terms of negotiating copyright and intellectual property, while balancing ethical issues around privacy and open access. Most of the books were produced for use in schools and contain no secret or sacred knowledge. Copyright for the objects belongs to the organisations that produced them, rather than individuals—for example, copyright for materials produced by schools with bilingual programs belongs to the NT government, which has licensed the digitisation and online display of their materials. Beyond this, however, the project team chose to seek the approval of the original creators of the materials. Identifying and locating these people has been a particular challenge for the project, but was considered important to recognise their moral rights and also involve them (or the families of those who had passed away) in the renewed digital life of their stories and illustrations.  To maximise the discoverability of the materials and access to them, additional points of entry to the archive have also been developed. In response to a request for offline access, a mobile app (LAAL Reader) 1 now allows users to download materials in bulk (for example, from a specific community or language) for storage on local devices to enable their use offline. Schools in southeast Arnhem Land have created libraries on school iPads for classroom use of the materials in local languages using this app. Efforts to ensure interoperability and reusability have involved use of software-independent formats and open-source tools, and long-term security is being negotiated with other archives in case of major disaster. The metadata is available for harvesting through OAI-PMH protocols and accessible through the National Library of Australia’s Trove database; related XML protocols make the materials discoverable through the Open Language Archives Community (OLAC) and the Australian National Data Service (ANDS). 2 One of the challenges of making the data accessible and interoperable has been the use of ISO 639-3 standards for identification and naming of languages, which do not conform to local classification and nomenclature. A dual-level representation of language names was formulated to balance conformity to an international standard and accurate representation of community preferences for language names.   Networking with users and potential users is helping the project team to identify various research questions and uses of the archive in different contexts. For example, working with ANU’s Centre of Excellence for the Dynamics of Language to include materials from the Living Archive in a larger corpus of Australian and Papuan languages will facilitate new avenues of inquiry in data mining and corpus analysis. The archive is included in various academic programs, such as training for Aboriginal Education Support workers through the Batchelor Institute of Indigenous Tertiary Education, and the Bachelor of Indigenous Languages and Linguistics degree at the Australian Centre for Indigenous Knowledges and Education. Proposals for research projects are included on the project’s accompanying web site, including evaluation of the site in terms of its usability for educational purposes, the use of resources on the archive for developing language learning programs, and collaborative activities with language owners on the use and revitalisation of stories in communities where language is no longer embedded in the curriculum. The project also maintains a social media profile, in an effort to engage a range of different users in schools, remote communities, and academic contexts. The extension of the archive includes the development of an API to give appropriate authorities a ready way of curating, extending, and customising their collections. This will allow for enhancements under the authority of original story owners, such as correcting or extending metadata, associating audio and video materials, and creating appropriate categories and themes for the existing materials. Workshops in communities are training and equipping people to engage with the archive, promoting intergenerational knowledge transmission and collaboration within schools and community groups. Such engagement and its outcomes can inform further research activities in the wider domain of digital humanities. The documentation of endangered languages has much to offer the field of digital humanities (Drude et al., 2012). The Living Archive project provides authentic data from minority languages in danger of extinction, on which research techniques and tools developed for major world languages can be tested and extended. The development of computational and statistical methods of analysis, annotation, markup, glossing, and the like can support and enhance the work of language documentation and conservation—for example, by automating some of the manual processes of extracting metadata and improving optical character recognition of minority languages. Data mining, textual analysis, and topic modelling can enhance the content of the archive and provide data for novel research questions for many different disciplines. Besides supporting academic research, such activities should also involve members of the communities where these languages are still important—in collaborative work, skills development, and furthering opportunities to develop digital means of documenting and preserving their linguistic and cultural heritage.  The affordances of such a body of literature are yet to be fully explored, and situating the archive within the context of digital humanities exposes it to new research contexts, which should be negotiated under the authority of the appropriate story owners. Curation and storage of cultural material is a contested space, with a number of constraints on access and ownership. This project is negotiating these cultural, technical, and epistemological challenges in its attempt to create a Living Archive rather than simply a storage facility or resting place for these valuable materials. Notes 1. https://www.cdu.edu.au/laal/reader-app/. 2. Trove: http://trove.nla.gov.au/work/188836876?q&versionId=205468516; OLAC: http://www.language-archives.org/archive/espace.cdu.edu.au; ANDS: http://researchdata.ands.org.au/living-archive-of-aboriginal-languages/316817#. ",
        "article_title": "Bringing to life the Living Archive of Aboriginal Languages",
        "authors": [
            {
                "given": "Cathy",
                "family": "Bow",
                "affiliation": [
                    {
                        "original_name": "Charles Darwin University, Australia",
                        "normalized_name": "Charles Darwin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/048zcaj52",
                            "GRID": "grid.1043.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "licensing",
            "digitisation",
            "linguistics",
            "repositories",
            "archives",
            "sustainability and preservation",
            "and Open Access",
            "English",
            "resource creation",
            "and discovery",
            "copyright"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The production of digital tools and the debate about how they fit with the humanities has a long history. Much of it has the character of frustration. Many digital tools have been built, but almost all them have had little or no effect upon the humanities as a whole. This has been a theme of mine for many years (see Bradley 2005; 2009), but the phenomenon has certainly been noted by many others. Edwards reports that Martin Mueller conducted a distant reading of the titles of monographs and articles in scholarly journals to measure mainstream interest in what he terms ‘literary informatics’ and concluded that it remains a niche activity with ‘virtually no impact on major disciplinary trends’ (Edwards, 2012, 216). The 2005 Summit on Digital Tools at the University of Virginia reports, ‘Only about six percent of humanist scholars go beyond general purpose information technology and use digital resources and more complex digital tools in their scholarship’, and I suspect that no one thinks that the number is much higher today. More recently, in an article titled ‘Building Better Digital Humanities Tools,’ Gibbs and Owens note that ‘despite significant investments in the development of digital humanities tools, the use of these tools has remained a fringe element in humanities scholarship’ (Gibbs and Owens, 2012, abstract).  Gibbs and Owen present a User Experience (UX) perspective drawn largely from interviews of humanist scholars, and tool builders will definitely benefit from many of the observations that they make. However, I believe that the challenge is more fundamental than trying to find out what potential users say they want. If we are looking for a sea change in humanities scholarship driven by the uptake of our tools, we should, instead, remember two innovators who both achieved one through their innovations and who understood the limitations of the UX user-oriented perspective. Henry Ford is reputed to have said, ‘If I had asked people what they wanted, they would have said faster horses’, and Steve Jobs claimed, ‘It’s really hard to design products by focus groups. A lot of times, people don’t know what they want until you show it to them’ (Ott, 2013). Perhaps a perspective on why digital tools for humanists are not being taken up needs to go outside of a set of (albeit valuable) UX observations. So, if the UX perspective is not the whole story, what else is there? Here I take a different approach, centered on the way many DH toolmakers think of the word ‘tool’, and attempt to broaden this idea of tool to fit it better with more of what goes on in the humanities, and therefore to make digital tools more influential there. My contention is that we need a more fundamental rethinking of tools and their function. I start with Ramsay and Rockwell’s observation that ‘Digital artifacts like tools could . . . be considered as “telescopes for the mind” that show us something in a new light’ (Ramsay and Rockwell, 2012, 79). The striking thing about this metaphor is that it draws parallels with a particular kind of tool—the scientific instrument. One way of seeing the assumptions that come loaded with this categorization is to look into the history of the conception of the telescope itself as a tool within science: something Malet (2005) reviews in his discussion of the early conceptualisation of telescopes in the 16th and 17th centuries. As a tool for ‘knowing’ and ‘discovery’—as something that revealed problems rather than solving them, opening discussions rather than closing them (238)—Malet claims that it became evident that, like this kind of humanities-oriented digital tool, the telescope needed to find a theoretical basis for what it did. Malet tells us of the evolution of this thinking over the 16th and 17th centuries—beginning with a view of the telescope as being essentially theoretically transparent: first as a kind of intimate extension of the eye (Malet, 2005, 245) or a kind of prosthesis for human vision (260) but eventually, starting with Kepler, to see the need for a theoretical basis that drew on optics and perspective, as to why materials seen through the telescope could be taken to have veracity (261). Of particular relevance to us is that Malet makes evident the separation of a theoretical basis for the telescope (drawn from optics and perspective) from a theoretical basis for what it revealed (such as the moons around Jupiter, or the lunar craters). 1 How did these two very different research domains successfully connect with the telescope? DH toolmakers need a similar discussion: How does the theoretical basis of text analysis tools actually successfully connect and enrich discussions in the very different theoretical world of modern literary criticism?   Although viewing humanities digital tools as analogues of scientific instruments is one paradigm, there are others, and we see alternatives in James Feibleman’s (1967) classification scheme for (nondigital) tools. The closest category he has to Malet’s  tools for knowing is his ‘tools for receptors’: the class of tools that ‘like spectacles, telescopes and vibrating membranes[,] extend the receptors’ in humans, allowing us to observe things that otherwise we cannot, or only with difficulty, experience. But Feibleman reminds us that there is a more familiar use of the word  tool than this—tools like spades or bicycles that ‘extend the effectors’, the parts of humans that allow us to make things, things like holes in the ground or trips to the grocery store (Feibleman, 1967, 330). These kind of tools in fact fit more naturally with the more conventional use of the word  tool, as is implied with ‘woodworking tools’ or ‘gardening tools’. Once this is noticed, we can see, perhaps, that Steve Jobs’ early classification of computers as ‘bicycles for the mind’ (see Popova, 2011) involves a very different sense of the potential of digital tools than we find in Ramsay and Rockwell’s seemingly parallel phrase ‘telescopes for the mind’.   The DH has had a significant history of tool making of this kind, too. One thinks of TuStep (2014) (for the making of sophisticated academic publications), for example, and Zotero (for the making of bibliographies), and we can see in the significant uptake of Zotero that the right tool, solving the right problem, can have a significant impact. The consideration of this kind of tool opens up a different perspective on issues, including on questions such as the UX’s  ease of use. Are woodworking tools, or a good violin, ‘easy to use’? Do they produce results easily? If not, why is this  not a reason that they have failed to find a user community? In fact, these tools help us to better understand the place of expertise in tool use, and its significance in the process of producing, in the hands of a master, good things from tools—an idea one can see in TuStep as well.  A third category of tools that brings significant relevant insights is ‘tools for cognition’—tools that are meant to help us think better. An extended and useful description of the idea of cognitive tools (although for the related field of tools to facilitate learning) appears in Kim and Reeves (2007), and they, in turn, make frequent reference to the idea of cognitive tools from its original definition by Netchine-Grynberg (1995). Tools for cognition are interesting because they combine characteristics of both Feibleman’s tools for receptors and effectors in potentially powerful ways. Whereas tools for effectors allow us to better execute ideas we already have in our head to produce something external to it, and tools for receptors take things outside our heads and put new ideas in our heads, tools for cognition—because the product they produce are ideas in our heads, too—form a kind of positive feedback loop, perhaps of the kind characterised by Jerome McGann as  autopoietic systems (McGann, 2004, 200).  Kim and Reeves use mathematical notation as an example of a tool for cognition. Mathematical notation not only serves to communicate ideas (tool for effectors), but is used by the person working on a problem to help him or her struggle with his or her research: scientists put mathematical fragments in their notebooks to help them think. Perhaps mathematics is not the right tool for humanities scholarship, but scholarly writing can act as a tool for cognition, too: many scholars write notes for this same purpose. Indeed, my Pliny environment (Bradley, 2008) is meant to explore how a tool to support the writing of fragments, as scholarly notes, might help humanities research. Scholarship in the humanities involves an interconnection of tasks that could benefit from both Feibleman’s tools for receptors and effectors, and in the task of working with the materials, involving cogitative work that tools for cognition could support. Perhaps a discussion that engages with these three kind of activities might move us better to understanding where digital tools could fit with the humanities? Note 1. For various reasons, this issue is even more evident in the history of the microscope. See Hacking (1981). ",
        "article_title": "How about Tools for the whole range of scholarly activities?",
        "authors": [
            {
                "given": "John",
                "family": "Bradley",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Humanities, King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "other",
            "user studies / user needs",
            "interface and user experience design",
            "digital humanities - nature and significance",
            "English",
            "software design and development"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Folklore, by its very nature, is deeply linked to place. This link is particularly strong in legends (Gunnell, 2008; Tangherlini, 1994; Lefebvre, 1974). Consequently, being able to discover the correlation between places mentioned in legends and the topics those legends address offers folklorists a powerful method for exploring the complex relationship between storytelling and place. By projecting these correlations into a historically accurate cartographic viewing environment, ‘ElfYelp’ affords researchers an opportunity to explore large, complex cultural archives through a geographically tuned lens. Early folklorists were intrigued by the possibility of discovering geographic correlations between traditional stories and the places where they were found (Krohn, 1926). These early studies imagined that the mapping of cultural artifacts could help locate the ‘origin’ of those forms, an approach critiqued by C. W. von Sydow (von Sydow, 1948). One of its main failings was the mobility of peoples: where something was collected had little to do with where it originated. Von Sydow’s rebuff had the unfortunate collateral effect of leading many folklorists to abandon cartographic endeavors and to adopt the equally flawed assumption that folkloristic phenomena are evenly distributed across the landscape. With the advent of relatively easy-to-use GIS software and accurate, machine-actionable historical gazetteers, there is now a renewed interest among folklorists in the power of maps. In this new cartographic method, maps are used to discover latent geographic patterns in the overall corpus, allowing researchers to explore how storytellers have conceptualized the landscape through narrative. For example, are there places in the landscape that are home to more supernatural creatures than others? Discovering patterns leads to important second-order questions: Do certain areas, rich in a set of topics, have topographic or structural features that lead to these associations? Are there political, social, or economic events in these areas that might help explain these patterns? Project Overview and Related Approaches In the current work, we extend our preliminary research into geo-semantic browsing of Danish folklore (Broadwell and Tangherlini, 2014) by developing a tool to discover and compare different regions’ topic ‘fingerprints’. Our results are further enhanced via a more sophisticated labeling of the Kristensen collection (Broadwell et al., 2014; Kristensen, 1980) and a refined method for the interpolation of geographic regions. This work builds upon emergent methods in computational data mining and spatial search to support micro-targeted marketing and location-aware search and recommendation services. Sizov’s proposal to generate ‘folksonomies’ of geolocated photograph captions in social media (Sizov, 2010) led to Yin et al.’s technique of ‘Latent Geographical Topic Analysis’ (LGTA), which establishes a ‘location-text joint model’ for geo-LDA (Yin et al., 2011). This model assigns a probability for each topic (based on Gaussian spatial distributions) that any point on the map might ‘generate’ a text containing that topic in some proportion. A refinement to this approach uses polygonal regions that avoid topic overlap as much as possible (Kling et al., 2014). Considerable modifications of these techniques are necessary to address folklore questions. The insistence of the latter approach on avoiding topic overlap, for example, works against the goals of folklorists, since many interesting research questions are based on overlap, such as: Do areas that have a high concentration of ‘ghost’ stories also have high concentrations of ‘wise minister’ stories? Data Processing and Analytical Methods In our data, each of the 20,431 Danish legends in the target corpus is treated as a locus of place/content co-occurrence. We resolved place references in the stories to approximately 1,750 distinct latitude/longitude pairs via historical gazetteers (DigDag, 2008). Several content-based attributes are associated with each story: a vector of prominent keyword frequencies as well as vectors of 36 high-level and 772 fine-grained categories assigned by the collector and his assistants. We regularized the inconsistencies of the high-level index by using it to train a Naive Bayes classifier that we then applied to the full corpus (Broadwell et al., 2014). To reduce sampling bias from the collector’s habits, we aligned historical census data with administrative records to calculate the population densities for the approximately 2,000 parishes in Denmark, which we used to normalize the co-occurrence matrices by population. To map the aggregated co-occurrence frequencies of a given story attribute to geographic locations, we calculated the spatial ‘decay’ of the observed z-values based on the standard deviational ellipses for five sample storytellers (Tangherlini, 2010). This model of story ‘decay’ is predicated on an idea of how far a storyteller’s stories might ‘reach’ into the surrounding community. We favor a simple Kriging approach to interpolating between observed points given the decay radius, rather than merely layering Gaussian curves to create ‘hot spots’; importantly, Kriging has the potential to take into account landscape features such as hills and marshes that may impede the ‘reach’ of stories (Cressie, 1993). Comparing the maps of the original indices to those of the alternative NB classifier reveals hidden geographical affiliations in the collection, much as the differing story labelings between these two classifiers highlighted interesting ‘borderline’ stories (Broadwell et al., 2014). For example, the alternative index assigns more stories mentioning the area around Breum Kilde to the ‘Witches’ category. Previous studies have linked this area to several stories containing oblique references to witchcraft that were not reflected in the collector’s high-level index; these references likely derive from the location’s historical associations with a Catholic monastery that was also the site of the last witch burning in Denmark (Tangherlini, 2000) (see Figure 1). Yet story labels like those described above, as well as standard topic modeling, do not use as a formative basis the idea that folklore topics may exhibit affinities for particular geographical regions. To explore this aspect of folklore, we employed a version of LGTA that finds a specified number of geographical regions and assigns a set of ‘geo-topics’ to each region in varying proportions; the geo-topics are themselves built from term vectors associated with each story. Story labels based on the 772-label secondary indexing scheme proved particularly effective in constructing geo-topics. Building co-occurrence matrices of places to topics, labels, and keywords also enables the comparison of arbitrary locations. Ranking a location’s associated attributes allows the identification of regions that share similar ranked lists of topics (e.g. witches, cunning folk, animal disease) via a standard vector comparison algorithm (cosine similarity). LGTA provides such a ranking for geo-topics; for other region features such as keyword co-occurrence counts, we use a weighting system similar to the term frequency-inverse document frequency (TF-IDF) scores commonly used for web searches (Broadwell and Tangherlini, 2012). Experiments To test the usefulness of our models, we devised two experiments. The first asks, ‘Where are the elves?’ A population density–corrected Kriging map reveals various regions that have a high concentration of stories labeled as ‘elf stories’ by the original collector or the Naive Bayes classifier, or regions that have ‘elf’ geo-topics high in their topic ranking (Figure 2). Consulting a base layer of historically accurate maps allows the user to interrogate the landscape surrounding these areas and ask, ‘What else is here?’ (streams, woods, etc.). Similarly, the population density maps provide a second set of considerations: Do people situate elves in places that are less populated? Finally, the experiment allows a user to consider other areas that have similarly high concentrations of elf stories. Are there certain landscape features that may help explain why certain storytellers associated a particular place and similar places with a specific type of threat?  A second experiment inverts the problem, asking, ‘What do I find here?’ A user draws an arbitrarily large bounding box on the map, and the system returns a ranking of keywords and geo-topics for that geographic area (Figure 3). It also identifies similar regions, based on the ranked list of topics. The underlying premise is that areas for which certain topics appear highly ranked will share features, be they environmental, institutional, or demographic (or a combination of all three). As such, this approach can help stimulate research questions: Are there types of places that were related, in the minds of storytellers, with certain types of stories? Conclusion ElfYelp provides a productive research environment for interrogating the relationship between storytelling and the concept of place. The tool, which we believe can be a key component of a ‘folklore macroscope’ (Tangherlini, 2013; Börner, 2011), raises more questions than it answers, but in so doing allows for the sustained exploration of how people use stories to comment, construct, and negotiate concepts of space and place. Figures    Figure 1. The ElfYelp geo-LDA interface, showing the geo-topics and most prominent labels of a region (highlighted) historically associated with witches.    Figure 2: A comparison of the place co-occurrence maps for stories assigned to the ‘Ellefolk’ (Elves) category by Tang Kristensen and his assistants (left) and by a Naive Bayes classifier (right). Simple Kriging was used to interpolate the z-values between the observed locations.    Figure 3. Rankings of Naive Bayes story classifications for the specified region according to the ElfYelp  Spøgelsesskop (‘spooks-scope’) interface.  ",
        "article_title": "ElfYelp: Geolocated Topic Models for Pattern Discovery in a Large Folklore Corpus",
        "authors": [
            {
                "given": "Peter Michael",
                "family": "Broadwell",
                "affiliation": [
                    {
                        "original_name": "UCLA, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Timothy R",
                "family": "Tangherlini",
                "affiliation": [
                    {
                        "original_name": "UCLA, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "geospatial analysis",
            "text analysis",
            "information retrieval",
            "interfaces and technology",
            "English",
            "data mining / text mining",
            "folklore and oral history",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The field of the digital humanities is built on the back of the archive. Researchers and the public have access to ever-growing online compilations of important texts (e.g., Dalmau and Courtney, 2011; Allori and Kaborycha, 2013), physical artifacts (e.g., Hunter et al., 2014; Streiter and Goudin, 2014), and lived experience (e.g., McGrath and Peaker, 2014). The power that these digital repositories afford—access to vast amounts of primary sources, processing tools to make sense of them, etc.—has redefined scholarship in the humanities.  We should ask ourselves, however, what the process of creating sweeping digital collections leaves out. That is, as we move to automate the processes of digitization (e.g., Lenkiewics and Drude, 2013; Lombardi, 2014), do we risk losing aspects fundamental to the human experience? If Mark Algee-Hewitt (2013) is correct that distant reading is three-fourths interpretive, if the materiality of the original text is key to understanding its contents (Lavin, 2013), then a close experience with literary artifacts supports effective distant reading. Automated digital archiving—while undisputedly valuable to certain researchers—potentially shortchanges students and budding scholars.  This paper describes the student-centric production of the Victorian Short Fiction Project (VSFP). Grown organically from a paper-based compendium of student research to a wiki of more than 160 articles, the VSFP conscientiously objects to removing the human experience from the curating process, even in the digital age. Two principles guide VSFP procedures: First, we prioritize quality of experience above speed of delivery. Second, the VSFP nurtures new scholars whose interaction with both the printed and digital text is integral to their development.  The VSFP is a pedagogical project. Students in Victorian literature classes at Brigham Young University select a 19th-century periodical from the Special Collections library. They then choose a short story published in that periodical; prepare a digital transcription of the story, including an introduction and annotations; and post their work to the VSFP.           Figure 1. The VSFP homepage. Though short fiction is plentiful in Victorian literature, scholars have typically downplayed its significance, looking instead to French and American fiction for the genesis of the genre (e.g., Killick, 2008; Korte, 2003; Orel, 1986). The VSFP creates a unique opportunity for undergraduate students to contribute to an area of inquiry in professional literary scholarship. It includes little-known fiction by Charles Dickens, Sir Arthur Conan Doyle, and Joseph Conrad, just to name a few authors. And we have fiction by a host of authors whose work has been all but forgotten, which makes the VSFP valuable for understanding the wider development of the short story during the century.  By embedding the students’ experience with the digital humanities within a traditional literary classroom experience and within a scholarly conversation in a literary field, they learn to see the digital humanities as a natural part of their academic studies, and as a relevant part of their career preparation. However, anyone familiar with the practices of the digital humanities will realize that building a digital collection of short fiction one student-prepared text at a time is, quite frankly, appallingly inefficient. The typical procedure would be to set up a system of culling all short fiction from Victorian-era periodicals and digitizing them en masse.  What would be lost in a typical process is exactly the rationale that gave rise to the VSFP in the first place. We need to nurture a new generation of digital humanists, not just by employing them as student workers on our large projects, but by showing them how the digital humanities is inextricably linked to the rest of their field. One student’s comments are representative of how students appreciate such connections:  I learned how to work in an electronic environment, and that knowledge is invaluable in today’s electronically mediated world. Also, knowing that the material I was going to post would be public motivated me to fastidiously ensure that the work was a polished representation of my writing and researching abilities. Overall, this project was one of, if not the most influential educational activity I’ve taken part in as a student here at BYU. I was able to research lesser known material which contributed to the sense of accomplishment I felt. I had the sense that I was actually contributing to the discourse within literary studies and not simply churning out another banal paper or project, and at the same time I also feel that the knowledge I gained from the project is valuable because it is not a topic that the whole of literary studies is familiar with, and I can therefore feel a sense of ownership for what I’ve done and feel pleased with the effort I put into it. Note the aspects that the student pinpoints as particularly valuable: digital literacies, personal ownership, and legitimate participation in a community of practice. We know from constructivist theory and research that these qualities contribute to better performance (Ennis et al., 1999) and early professionalization of novices (Lave and Wenger, 1991). Other student comments reinforce the professionalizing aspects of the project. Students are motivated to do their best work because they know it will outlast the class.  There is another aspect of student responses that deserves attention. Students often express a sense of sheer wonder at the process of bringing archival materials to a 21st-century audience. As one student put it, “The project made me interested in spending time in the archives. . . . And doing the project made me realize how valuable books are, and how I need to take much better care of books than I do. They are really priceless and need to be taken care of if I want them to last for the next hundred years.” Another student observed, “[I]t was just impressed on me even more how great it is to be able to look at the original text and how much more you learn from a publication b[y] looking at that original copy. For example, for my journal, the format of it and the way it was illustrated and even the way it smelled was enriched by reading and made its audience seem more real.” Ironically, the tactile experience of working with the archival holdings makes this digital project even more rewarding and beneficial for the students.  As the field of the digital humanities charges bravely on in the construction of new knowledge, as we increase our understanding of the human experience by applying computation methods to centuries-old disciplines, we must not lose appreciation for the essential—yet often unacknowledged—physical aspects of that experience. We must be attentive to the balance between creation of or from digital content and the curation and appreciation of source artifacts, especially as we cultivate a new generation of scholars. Such awareness may serve as content knowledge that will influence our interpretation of digital representations.  The VSFP will continue its deliberate efforts by posting approximately 20 new articles each year with the hope of influencing future literary scholars. Meanwhile, we are beginning the process of seeking NINES certification so that this digital repository will be of increased value to researchers of Victorian literature.  ",
        "article_title": "Digitizing Slow and Deliberate: The Victorian Short Fiction Project",
        "authors": [
            {
                "given": "Leslee",
                "family": "Thorne-Murphy",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Jeremy",
                "family": "Browne",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation - theory and practice",
            "english studies",
            "English",
            "digital humanities - pedagogy and curriculum",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Long before the Silk Road arose from Han Dynasty China, a ‘tin road’ supported an eastern flow of this rare metal from Central Asia to the Aegean basin. Like the Silk Road, the ancient tin road operated as a relay trade where goods were transferred between groups along segments of the route. The basics of the Old Assyrian trade are well understood (Veenhof and Eidem, 2008). From their home city Aššur, on the Tigris in northern Iraq, the Assyrian traders trucked the tin and textiles to the central plateau of modern Turkey, where they sold the goods through dozens of cities across the plateau. But aspects of the commercial organization are still debated (Larsen, 2007; Stratford, 2014). This paper describes a nascent project that applies network analysis to an Old Assyrian corpus to characterize commercial organization.  Approximately 23,000 Old Assyrian documents exist (written in cuneiform on clay tablets), most written between 1900 and 1850 BCE. These documents, excavated from the site of Kültepe at intervals since 1884, are stored at more than a dozen European, North American, and Turkish museums. Researchers have transcribed, photographed, and/or catalogued roughly half of them, though fewer have been translated into modern languages.  Working from these transcriptions and catalogues, Dr. Edward Stratford selected 538 documents that concerned a merchant named Pušu-ken and his associate, Salim-ahum. Dr. Stratford translated the documents from transcriptions, photographs, and tablets, using the University of Chicago’s Online Cultural and Historical Research Environment (OCHRE) to record the text and its associated metadata.  Recent attempts to model Old Assyrian commercial networks (Bamman et al., 2014) have shown promise, but several non-trivial barriers limit the accuracy of these models. First, previous analyses have analyzed co-occurrence on a document-by-document basis. Each document, in fact, describes many ‘occurrences’, such as financial transactions, commercial orders, payment disputes, etc. A more accurate analysis would define associations in terms of interactions between individuals rather than co-occurrence within documents.  The second barrier to an accurate analysis of Old Assyrian trade records is the confusion caused by individuals who share informal appellations (see also Bodard et al., 2014). For example, more than 100 individuals in Old Assyrian texts named Aššur-malik can be distinguished by their patronymic designations (e.g., Aššur-malik, son of Al-ahum). However, patronymics were not normally used in Old Assyrian correspondence. Experts familiar with the corpus must rely on context clues—geography, known associates, and specific roles—to deduce the identity in question.  Despite its tremendous utility, OCHRE’s ability to link names to individual persons—and thereby distinguish between two individuals with the same name—is still in development. Jeremy Browne worked with Stratford to develop a prototype database and interface to extract names from the corpus and help Stratford quickly map instances of names to specific individuals.   Building the Database and Preliminary Analysis  This prototype relational database began with a single table to store the translated text segmented into over 2,000 occurrences. Browne wrote routines that extracted from the occurrences almost 1,000 unique non-English words, which Stratford classified as names, places, or technical terms (e.g.,  minas). The names—which are pertinent to this project—were stored in a new data table, while places and technical terms were set aside for future use. The routines that recorded names also recorded the association between names and occurrences.   At this point Browne and Stratford could perform a preliminary analysis of the corpus. (Remember, this analysis only considers names, not individuals, within occurrences. The ongoing labor to disambiguate individuals from common names is described below.) Nearly half (287 out of 606, or 47%; see Figure 1) of the names found in the corpus only occur once. While these names represent ‘dead-end’ nodes in the commercial network, they can reveal the importance of the one or more names with which they co-occur.        Figure 1. A histogram of name-frequency in the corpus.  Visualizing the entire network via Gephi and Elijah Meeks’ GexfD3 library proved problematic because of Pušu-Ken’s and Salim-ahu’s super-degree-centrality. Because the entire corpus was selected based on the documents’ association with those individuals, they are connected to virtually every name in the database by one or two links. Removing Pušu-Ken and Salim-ahu from the dataset revealed a much clearer preliminary picture of the network (see Figure 2). The ring of dots around the periphery represents names and occurrences who only co-occur with Pušu-ken and/or Salim-ahu, but the dots appearing in the intra-space may be very informative.  This analysis may provide hints as to which names represent multiple individuals. For example, Figure 3 highlights the subnetwork of Buzuliya, a name whose two occurrences are otherwise geodesically isolated. It is unlikely that one lightly connected person took part in such disparate events, so Buzuliya likely represents two individuals.  Figure 4 displays the constellation of occurrences in which the name Lamassi occurs. It is tempting to apply the above criteria here and conclude that this name cannot represent a single individual. However, Lamassi was the name of Pušu-Ken’s wife, who one would expect to occur in a diverse set of interactions. Despite the variety of occurrences in which this name appears, we must be cautious in blindly applying such heuristics.        Figure 2. The commercial network sans Pušu-Ken.        Figure 3. Indication that the name  Buzuliya refers to two people.         Figure 4. Lamassi’s subnetwork.   Name Disambiguation Interface  The database was expanded with the addition of a persons table that contained patronymic designations compiled by Stratford. Browne wrote routines that attempted to match these patronymics to names extracted from the corpus. If a given name in a patronymic was not found in any other patronymic, the system would link that name and all of its associated occurrences with that patronymic. Also, if a name only occurred once in the corpus, and no patronymic contained that name, then the name was added as its own patronymic, and its occurrence was linked to that new patronymic.  These procedures accounted for almost 300 of the 606 names found in the corpus. As the Old Assyrian expert, Stratford must comb through the remaining names. To assist Stratford, Browne created an AJAX-based web form. The interface presents two select boxes: a box above displaying the names in the corpus, and a box below displaying the known patronymics. There is also a small form at the bottom to add new patronymics to the data table (see Figure 5).  When the user clicks on either a name or a patronymic, the text from its associated occurrences appears on the right. For names, the occurrences appear with the name highlighted in blue. The user may click on any of the occurrences displayed for the selected name, and link them to the selected patronymic.        Figure 5. The name disambiguation interface.  Stratford will disambiguate homonymous individuals, and, by the end of 2014, he and Browne should have more detailed and accurate analyses of this 4,000-year-old commercial network to share. It is possible that, as happened with Jackson’s (2014) analysis of medieval Scottish charters, previously unknown connections and communities will be revealed.   Moving On  Besides the individual-disambiguated network analysis, Browne and Stratford plan to map specific business transactions and assets to further characterize Old Assyrian commerce and society. Methods that Suen, Luenzel, and Gil (2013) used to analyze an array of media may be adapted to uncover nuances between various types of transactions. The OCHRE metadata for the original clay tablets describe features of handwriting style, so there is an opportunity for ‘transcriptionship’ attribution. Ongoing spectral analyses of the clay tablets may reveal the place of transcription as well. Finally, Stratford and Browne are investigating various metadata standards such as SNAP:DRGN (Bodard et al., 2014) to allow OCHRE and the prototype database to interoperate with other such corpus systems.  ",
        "article_title": "LinkedIn circa 2000 BCE: Towards a Network Model of Pušu-ken’s Commercial Relationships in Old Assyria",
        "authors": [
            {
                "given": "Edward",
                "family": "Stratford",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Jeremy",
                "family": "Browne",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "corpora and corpus activities",
            "relationships",
            "archaeology",
            "English",
            "graphs",
            "near eastern studies",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A lack of interoperability between digital resources for humanists, whether big or small, plagues our research community. The challenge of the ‘Million Books’ problem identified by Gregory Crane (2006) and the opportunity of examining aspects of humanistic phenomena that appear, as Alan Liu (2012) puts it, ‘only at scale’ are lures that drive the digital humanities community forward in developing new tools and exploring emerging modes of enquiry for large datasets. Yet only a small handful of well-funded projects are actually in a position to investigate such phenomena due in part to restrictions on the use of commercially held large data collections but at least as much on the lack of interoperability and barriers to aggregation between open collections, many of which originate in the digital humanities community. This ‘multivarious isolation’, identified by Rebecca Frost Davis and Quinn Dombrowski (2011), stems from a number of intellectual, institutional, and social factors, but a 2011 Research Information Network report found that the biggest obstacle for researchers in our century was incompatible data structures and disparate and scattered data sources. If digital research in the humanities is to be significantly advanced, this problem must be addressed. This paper describes the approach to this challenge taken by the Canadian Writing Research Collaboratory (CWRC) (Brown, 2011), a virtual research environment hosting and supporting a range of projects, including ‘seed’ projects with existing data in a wide range of formats. CWRC aims to bridge the gap between the digital humanities community and humanities scholars who lack an understanding of best practices and standards, as well as bridging diverse datasets (Brown, 2011; 2013). We describe the approach to interoperability built into the Collaboratory’s systems and argue that such an approach based on linked data principles supports scaling up beyond the Collaboratory to form a basis for interoperability across disparate and distributed projects with overlapping content. The resulting interoperability is in some senses quite rudimentary, but at the same time constitutes a step towards a more robust ecology of interlinked resources. Core to the approach of developing the Collaboratory such that it facilitates interoperability is the entity-based nature of the architecture, as expressed in entity records, entity schemas, and an entity management system. The approach is similar to that employed by the Humanities Networked Infrastructure (HuNI) project, which brings together a large collection of diverse datasets from various Australian providers (Burrows, 2013), and that of the New Zealand Electronic Text Collection’s Entity Authority Tool Set (EATS) (Norrish, 2007). Entity records in the context of CWRC have two purposes: to enable Semantic Web–type linked open data processes and to facilitate traditional authority control and information retrieval in a digital repository. In order to achieve these two purposes, CWRC entities are structured as elemental, atomic units of information, and, specifically, they are modeled as proper noun–named entities in four classes: persons, organizations, places, and titles (works). The core characteristics that are common to all four types of entities are an indication of the preferred or authoritative name, variant names if any, and important administrative data such as record creation date, contributing project or projects, and an access condition statement. Entity schemas are used to model and govern the creation of entity records to ensure consistency among the entities; this is achieved by defining required elements and attributes, content types associated with each element, and the nesting hierarchy of elements. The CWRC XML entity schemas consist of a custom CWRC omnibus entity schema that models the person, organization, and place entities, and the Library of Congress’ MODS schema, which models the title entities; the omnibus entity schema was influenced by concepts from several existing schema standards, including EAC-CPF, FOAF, GeoNames, ISAAR (CPF), MADS, and MODS. Following the codified approach of authority records, the CWRC entity types parallel the Library of Congress person, organization, geographic name, and title name authority files, and the entities’ core structure contains the standard three components of an authority record: authoritative form, cross-variant forms, and administrative notes. These entity records provide the basis for a linked data entity within the Resource Description Framework (RDF), with a dereferencable uniform resource identifier (URI) and associated RDF triples that translate the information about that entity from the schema, as well as some of the information about that entity from elsewhere in CWRC, into an open Semantic Web dataset for use within and beyond the Collaboratory (Simpson and Brown, 2013). With the exception of the use of MODS records for bibliographical entities, the CWRC entity schema is deliberately minimalist, more so than the EATS data model for instance, serving as an authority list that brokers between different data formats used by various projects about entities, rather than requiring all projects within CWRC to adopt the same standard for recording detailed information about those entities. For instance, the Lesbian and Gay Liberation in Canada (2015) project led by Constance Crompton and Michelle Schwartz is recording prosopographical information using the Text Encoding Initiative, while the Orlando Project (2015) has a rich bespoke XML schema for encoding prosopographical information, while bibliographical or editing projects may use MODS or METS or EAD. The CWRC approach allows the same person as described by documents using a range of schemas to refer to a common, minimalist entity, while permitting divergent representational practices and individual project autonomy. It also allows us to preserve and make somewhat interoperable existing digital datasets that we can only partially remediate, and to bring together XML materials with materials in other media. The CWRC entities thus facilitate the collocation of a wide range of resources that reference that entity. Rather than acting as receptacles for prosopographical information, they act as authority records across CWRC, but connect using linked data principles to other authoritative and often more detailed records from individual projects, facilitating information retrieval and aggregation. The approach facilitates linking into the Semantic Web more generally; indeed CWRC entities beyond those harvested from our seed projects will only be minted if users cannot locate a linked data identifier for that entity amongst trusted linked data entity collections such as the Virtual International Authority File. This pragmatic approach to entities is complementary with, and indeed promotes, a number of existing standards, bridging datasets through a set of linked data that will support interoperability and leverage that bridging into conformity with Semantic Web standards. As Joris van Zundert has argued, the idea of uniformly behaving humanist scholars is a pipe dream; furthermore, he argues, ‘any meaningful humanities research . . . cannot be entirely governed by standards’ (2012, 173). CWRC’s strategy focuses on building interoperability from how scholars with a range of technological expertise and different degrees of willingness to engage with complex standards actually work, not how we think they should work in the best of all possible worlds. The paper will demonstrate the user experience that stitches together the entity architecture with other components of the Collaboratory to enable users to incorporate relatively technical tasks into their workflow. The entity management system provides a user interface to create and edit entities, as well as mechanisms to perform entity extraction on source documents and global merging of these extracted entities into the CWRC common pool of core named entities. The centerpiece of CWRC’s interoperability strategy is a web-based XML and RDF Editor called CWRC-Writer (Brown et al., 2014; Rockwell et al., 2012). It aims to integrate the activities of correcting, writing, formatting, and editing a text with managing references to entities so as to minimize the additional work required for disambiguation, merging, and management. Tagging an entity within CWRC-Writer creates XML tags such as persname, orgname, or title, as well as equivalent RDF annotations employing the Open Annotation data model (2013), a practice that makes the project importantly about links rather than simple tagging (Hendler, 2011). Having the user provide the entity linking up front means that the act is informed by their knowledge of the material, and integrating it into the writing interface minimizes the additional time required to enhance the information. The immediate payoff to users will be the creation of links between their materials. This section of the paper demonstrates how entity tagging works within CWRC-Writer and shows our progress towards the cross-project aggregation of materials that entity tagging will support. To further support the essential role of entities within the Collaboratory, we are exploring implementing, where possible, supplementary technologies and practices. These include support for the construction of a semiautomated named entity recognition (NER) tool (Barbosa et al., 2012), developing Semantic Web capability through ontology development, the exposure of data through a publicly accessibly triplestore (Brown and Simpson, 2013), and pursuing synergies with other linked data projects in the humanities.  The Canadian Writing Research Collaboratory employs entities to showcase the benefits of interoperability while providing immediate benefits to researchers. Our conclusion will touch on the ways in which this approach provides a foundation on which to build more advanced services in the future. ",
        "article_title": "An entity-based approach to interoperability in the Canadian Writing Research Collaboratory",
        "authors": [
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Guelph, Canada; University of Alberta, Canada; Canadian Writing Research Collaboratory",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Jeffery",
                "family": "Antoniuk",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; Canadian Writing Research Collaboratory",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Brundin",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; Canadian Writing Research Collaboratory",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Simpson",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; INKE Research Group",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Mihaela",
                "family": "Ilovan",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; Canadian Writing Research Collaboratory",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Warren",
                "affiliation": [
                    {
                        "original_name": "Carleton University; University of Guelph, Canada",
                        "normalized_name": "Carleton University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02qtvee93",
                            "GRID": "grid.34428.39"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "metadata",
            "relationships",
            "standards and interoperability",
            "English",
            "graphs",
            "linking and annotation",
            "networks",
            "semantic web",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Studies based on the visualization and analysis of temporal profiles of words using a so-called n-gram approach have been popular in recent years (Michel et al., 2011; Delahaye and Gauvrit, 2013). However, most of the studies so far discuss the case of remarkable words that are mainly found due to the researcher’s intuitions for finding ‘interesting’ curves using the n-gram viewer. In this paper, we investigate how we could inverse the problem and automatically explore the space of temporal curves in search for words. For instance, we could be interested in asking the system to retrieve all curves ‘similar’ to a given one. This would entail the definition of a method to describe temporal profiles and classify them according to a predefined distance.    Figure 1. Four examples of curves: (1) The word ‘1889’ is very popular in the year 1889 but ceases to be as interesting quickly after; (2) the term ‘olympique’ is very popular every four years; (3) the term ‘informatique’ keeps getting more popular over time; (4) the term ‘URSS’ is very popular only between 1950 and 1990. The study presented in this paper uses a database of 4 million articles covering a period of 200 years and is composed of digitized facsimiles of ‘Le Journal de Genève’ and ‘La Gazette de Lausanne’. Each article has been OCRed and indexed in an SQL database. For each indexed word, the yearly temporal profile showing the relative number of word occurrences in the corpus has been pre-computed. We designed an n-gram viewer that allows for the simultaneous comparison of up to four words. In order to compare these curves with one another and therefore be able to retrieve ‘similar’ curves automatically in the database, we need to find a simple way of describing them. The problem could be approached in a very general manner (how to describe an unknown curve by approximating it on a given decomposition base) or could take into account the fact that we are dealing with temporal profiles that could be described by a given limited number of archetypical curves. The second option implies having some a priori knowledge about the kind of curves one might encounter but has the advantage of allowing for a very compact description of general curves using a family of possible profiles. We choose this second option in this paper.  The first step of our curve analyzer is to associate a given curve to a preexisting curve family. This can be done in a hierarchical manner. At this stage of our research, we decompose curves into four basic families: ‘Dirac’ curves (with a single peak), periodic curves (with regular peaks of popularity), monotonic linear curves (either increasing or decreasing) and ‘square’ curves (associated with a predefined period). It is clear that these four families do not cover the entire spectrum of possible curves but they do provide a possible starting point for investigating the space of ‘remarkable’ curves. To determine if a given curve can be reasonably approximated by one of the four families, we carry out a sequence of tests, starting with the periodicity of the curve. To evaluate whether a curve is periodic we simply compute its Fourier transform and automatically check for hidden periodicity. If the curve is detected to be periodic we extract the period and try to fit a ‘comb’ function as defined by the following formula:    f   x ; a , b , m , t   =        0 ,         i f   x < m                                                                                                                      a ,          i f   x ≥ m     a n d       x - m     m o d u l o   t = 0        b ,          i f   x ≥ m     a n d       x - m     m o d u l o   t ≠ 0        The fitting is done using Particle Swarm Optimization method (PSO) (Trelea, 2003), and the quality of the fitting with the theoretical curve is measured using the classical least squares error (minimizes the sum of squared  residuals).     Figure 2. Temporal profile (blue) of the word ‘olympique’ with fitted curve (green) and Fourier transform (right). Using the model, the extracted period becomes a way of comparing periodic curves with one another. Figure 3 shows different curves sorted by periodicities.    Figure 3. Temporal profile (blue) of the words ‘olympique’, ‘fifa’, ‘recensement’, and ‘halley’ with fitted curve (green). For each curve, the sum of the squared  residuals between the actual curve and the fitted curve is calculated, which then represents the error of the comparison between them. This measure is used to optimize the fitting and allows for determining the category the actual curve belongs to when the optimization is done for all predefined categories of curves.   For ‘peaks’ we try to fit the curve with a classical model used in the analysis of ‘fads’ (e.g., the use of this model in the context of the analysis of Internet Memes [Bauckhage et al., 2013]) by using the Fréchet curve (Alves and Nevers, 2010) as defined by the following formula:    f   x ; a , s , m   =          a   s         x - m   s       - a - 1     e   -       x - m   s       - a     ,   i f   x > m   a n d   s > 0        0 ,                                                                                                   e l s e                                                                            Using this model, fitted curves can be described using the position and value of the peak, allowing optimization with only one degree of freedom. These three parameters can be used to compare curves with one another as shown in Figure 4.    Figure 4. The year 1889 is much like 1906 using the fitted Fréchet curves. The years 1848 and 1940, because of their historical significance, have rather different parameters despite the fact that they belong to the same family. For the years 1900 and 1914, the error of fitting is higher, meaning that they might belong to a different category of curves. The same approach can be conducted with linear monotonic curves and ‘squared’ curves.  We believe that inversing the problem of n-gram visualization by enabling automatic search in a space of curves could profoundly transform research in this area, going beyond the intuitive search for remarkable curves. It is likely that many different levels of information, combining semantics and grammatical constraints with historical contexts, are implicitly coded in n-gram temporal profiles. Understanding how to classify and study these curves is important for harnessing the power of this set of statistical tools. The solution based on families of simple archetypical curves briefly described in this article is certainly not the only way of approaching this question but constitutes an initial attempt to demonstrate the potential of this overall research goal.  ",
        "article_title": "Inversed N-gram viewer: Searching the space of word temporal profiles",
        "authors": [
            {
                "given": "Vincent",
                "family": "Buntinx",
                "affiliation": [
                    {
                        "original_name": "EPFL (École polytechnique fédérale de Lausanne), Switzerland",
                        "normalized_name": "École Polytechnique Fédérale de Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s376052",
                            "GRID": "grid.5333.6"
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "EPFL (École polytechnique fédérale de Lausanne), Switzerland",
                        "normalized_name": "École Polytechnique Fédérale de Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s376052",
                            "GRID": "grid.5333.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "linguistics",
            "media studies",
            "natural language processing",
            "corpora and corpus activities",
            "information retrieval",
            "text analysis",
            "English",
            "semantic analysis",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " HuNI (Humanities Networked Infrastructure) is an innovative virtual laboratory that takes a new approach to the problems of integrating and linking data in the humanities and creative arts. It aggregates data from thirty Australian datasets, using a Data Model based on six core entity types. HuNI users are able to construct their own links between the individual entities in the HuNI service. This ‘social linking’ function is accompanied by the ability to create and annotate collections of entities, as well as to import and export data.  The initial HuNI project, funded by an Australian government grant under the NeCTAR programme (2012–2014), was completed with the launch of the HuNI service in October 2014. More than 730,000 entity records were included in this initial version of HuNI. The research community served by HuNI spans the humanities and creative arts in Australia, and includes researchers across literature, cinema and media studies, history, linguistics, Aboriginal studies, and art (Verhoeven and Burrows, 2014). This paper will report on a project to extend the scope and content of HuNI, using the corpus of digitized newspapers assembled by the National Library of Australia and made available as part of its Trove service (http://trove.nla.gov.au/newspaper). This Trove corpus contains 13.7 million pages and 133.8 million articles, from more than 700 newspapers published between the earlier 19th century and the mid-1950s (Sherratt, 2014). An extensive crowdsourcing project has been running for several years, aimed at correcting the OCR files for the corpus. The project, being carried out during 2014 and 2015, has two main objectives:  1. Testing and evaluating solutions and methods for carrying out Named Entity Recognition against the OCR files in the Trove digitized newspapers corpus.  2. Exporting the information about entities identified from Trove so that they can be incorporated into the data aggregate that forms the basis for the HuNI service.  This work has the potential to expand HuNI’s content significantly. It extends HuNI’s data, for the first time, to entities derived from full-text digital resources. It is enabling the HuNI team to evaluate the usefulness of the Trove corpus for this purpose and is serving as a pilot for the development of an ongoing production service of this kind. The project is being carried out with the assistance of a grant for the Microsoft Azure for Research programme. The project is using the Azure cloud service to store a copy of the OCR files for the Trove newspaper corpus. It is also deploying the GATE software on Azure in order to test its capabilities against the Trove newspaper corpus. GATE is a full-lifecycle open-source solution for text processing, developed at the University of Sheffield and widely used around the world (Cunningham et al., 2011). GATE is already available as a packaged Virtual Machine for cloud deployment, and the project is extending that cloud deployment to Azure.  In the paper, we will report on the initial results derived from the use of GATE to identify named entities in the Trove newspapers corpus. This will include a discussion of the issues encountered, an evaluation of the results obtained, and an assessment of the effectiveness of the overall process. We will also discuss the methods being developed for packaging up these results for ingest into HuNI. This will include demonstrating and evaluating the pipeline for this process and showing how entity records added to HuNI can be used to link back to their occurrence in the Trove corpus. The paper will also discuss the remaining work to be carried out during this project. The final stage of the project will involve deploying Azure’s own Machine Learning tools against the Trove corpus. It will then be possible to compare the results obtained from Azure Machine Learning with those obtained through the use of GATE. Progress towards the deployment of a production service for adding Trove entities to HuNI will also be discussed. ",
        "article_title": "Enriching the HuNI Virtual Laboratory with Content from the Trove Digitized Newspapers Corpus",
        "authors": [
            {
                "given": "Toby Nicolas",
                "family": "Burrows",
                "affiliation": [
                    {
                        "original_name": "University of Western Australia, Australia; King's College London",
                        "normalized_name": "University of Western Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/047272k79",
                            "GRID": "grid.1012.2"
                        }
                    }
                ]
            },
            {
                "given": "Alwyn",
                "family": "Davidson",
                "affiliation": [
                    {
                        "original_name": "Deakin University",
                        "normalized_name": "Deakin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02czsnj07",
                            "GRID": "grid.1021.2"
                        }
                    }
                ]
            },
            {
                "given": "Steve",
                "family": "Cassidy",
                "affiliation": [
                    {
                        "original_name": "Macquarie University, Australia",
                        "normalized_name": "Macquarie University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01sf06y89",
                            "GRID": "grid.1004.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "natural language processing",
            "English",
            "cultural infrastructure",
            "digital humanities - facilities"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Each object in contemporary cultural heritage collections has its own history and its own historical significance, as Neil McGregor demonstrated so vividly with one hundred objects chosen from the collections of the British Museum (McGregor, 2010). An important part of that history is the process by which each object came to reside in its current location and its current collection. Each object has usually been part of a series of collections over its lifetime, and this movement of objects between collections has its own history. Similarly, each collection has its own history of formation and (usually) dispersal. These collections include personal and individual collections, private institutional collections, and modern public collections. These relationships between cultural objects, collectors, and collections over time are an important example of what Alan Liu has described as ‘network archaeology’ (Liu, 2012) —the recovery and analysis of cultural, social, and artistic relationships at a particular period of time. As well as studying how and why some objects survived while others did not, and how and why the ownership of these objects changed, this ‘network archaeology’ can also address several larger research questions. Cultural collections can reflect broader historical trends and are shaped by them. In the European context, these include the dissolution of religious institutions, the decline of royal and aristocratic patronage, the rise of public cultural institutions (especially museums and libraries), the emergence of wealthy collectors in the industrial era, European global expansion and imperial power, and the repatriation of cultural objects. The network of relationships between people and institutions involved in the ownership and transmission of cultural collections can also reveal a good deal about the more general networks of cultural influence and social and political relationships in a particular society. In the 19th century, the English collector Sir Thomas Phillipps (1792–1872) assembled the largest private collection of European medieval and early modern manuscripts and documents. It is estimated to have contained more than 40,000 items, making it considerably larger than most of the collections in public institutions today, and included many manuscripts of considerable historical, textual, and artistic significance. The manuscripts had very varied geographical origins across Western Europe, are written in various different European languages, and cover a wide range of different subjects and topics. Their modern locations are spread across the globe—the dispersal of the Phillipps Collection took place gradually over more than one hundred years, and numerous institutions and collectors were involved. As a result, the history of the Phillipps Collection provides a much richer and more varied set of data than a single contemporary institutional collection would provide. In this paper, I will report on a project to reconstruct and analyse the history and provenance of the manuscripts that formed the Phillipps Collection. The scale of the Phillipps Collection has proved a significant challenge to traditional research methods in the past; the English librarian A. N. L. Munby spent more than a decade compiling a overview of Phillipps’ collecting activities and of the dispersal of the collection up to the mid-1950s (Munby, 1951–1960). In this project I am employing innovative data modeling and analysis techniques to build a digital environment for tracing the entire history of these manuscripts, as far as it can be known. I am interested in mapping the provenance events and ownership networks that, taken together, constitute the history of these thousands of manuscripts over hundreds of years.  My paper will focus particularly on four key technical aspects of the project.  •  Frameworks for modeling and representing the data relating to ownership and provenance, using an event-based approach   Events are central to provenance research, but they have proved difficult to represent in existing ontologies and data models, with a variety of different approaches being used. I will discuss the various alternatives—including CIDOC-CRM and the Europeana Data Model—before presenting my own approach based on property graphs (Blanke et al., 2013).  •  Techniques for importing and combining existing data relating to manuscript histories   The existing data relating to the Phillipps manuscripts are scattered across numerous digital and physical sources, in multiple languages. They are, inevitably, in a variety of different formats and schemas, ranging from relational databases and MARC records to handwritten notes and card indexes. Capturing these data and aligning them to a common data model are complex tasks that require multiple ingestion paths and crosswalks.  •  The deployment of suitable software to manage the data and to support analysis and visualization   Suitable software is critical for a project of this kind. I will report on the options available, and discuss the reasons for my choice of the graph database software Neo4j to store, manage, and present the data (Van Bruggen, 2014). Like Blanke et al. (2013), I consider that graph databases are a good fit for ‘how humanities researchers think about their data and its relationships’. I will also discuss the implementation of the data model for aggregating the provenance data used in this project.  •  Methods for visualizing and analyzing the data produced by the project, and for making them available for re-use by other researchers   I will look at a series of use cases and research questions related to the aggregated data, and will demonstrate how Neo4j can be used to produce analyses and visualizations in response to these requirements. I will also discuss methods for linking the data produced by this project with the wider Linked Data cloud, in order to enable wider contextualization and analysis. I will compare the results made possible by my software environment and data model with those produced by the Schoenberg Database of Manuscripts—a relational database that focuses on manuscript provenance. The graph database approach enables more sophisticated and complex pattern-matching across the full range of provenance data. ",
        "article_title": "The History and Provenance of Cultural Heritage Collections: New Approaches to Analysis and Visualization",
        "authors": [
            {
                "given": "Toby Nicolas",
                "family": "Burrows",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "medieval studies",
            "metadata",
            "libraries",
            "archives",
            "museums",
            "relationships",
            "English",
            "graphs",
            "GLAM: galleries",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper presents the socio-technical implications of designing and deploying an interactive, community-driven, and community-contributed archive.  In 2015 the Theatre Royal Nottingham (UK) will be celebrating its 150th anniversary, having first opened its doors in September 1865. This is a major landmark for Nottingham and the ideal time and catalyst to explore the venue’s past and engage the city and its residents with its performance heritage. At present, there is a large amount of records available related to the history of the Theatre Royal Nottingham. However, an archive has not been developed yet, and there is the need of establishing a system to organise the records and ultimately make them accessible to the public and the researchers interested in exploring them. This archive will allow understanding, preserving, and enriching the history and the integrity of the Theatre Royal, and constructing the memory of the building by creating a bridge between information and the community.  Building an archive is considerably expensive, and it requires a significant amount of time and human effort (Poole, 2010). The maintenance of the archive resources in the long term implies additional investments, and therefore sustainability becomes a key issue. It is then vital to ensure that such effort in financial resources, time, and research is balanced with the public and community engagement. In the research library and archive sector, it is reported that at least £130 million of UK public money has been spent in digitisation in a decade (Bültmann et al., 2005). Despite the significant expenditure, it is hard to find ‘evidence of use’ of the digitised resources (Warwick et al., 2008), and research shows that about one-third of the digital resources in the humanities are not accessed (Warwick et al., 2006).  To improve the balance between available digital resources and access by the public, cultural institutions are progressively exploring alternative paths of involvement of their audiences in the use and appropriation of their on-site and online assets. In this framework, models of public participation have been identified (Simon, 2010), and crowdsourcing-based initiatives have been classified (Carletti et al., 2013; Oomen and Aroyo, 2011) to provide insights into the nature of those emerging relationships. The participatory culture—nurtured by the Web 2.0 expansion—has amplified the scope for diverse collaborations between organisations and the public and, at the same time, raised audiences’ expectations to have an active role and to become ‘prosumers’ (producer and consumer), rather than simply consumers of cultural activities. The technological infrastructures are at the core of those new collaborations. The value of involving the public in the design and development of interactive systems is widely acknowledged, and a rich literature on participatory design has been produced by the Human-Computer-Interaction research community. However, it is essential to acknowledge that participation goes beyond the involvement of the participants as informants in design. Participation is an ongoing engagement and should aim for learning and long-term empowerment of the people involved (Dearden and Rizvi, 2008). Community participation in the design of interactive systems can represent the first step to assure a dynamic and permanent commitment in the deployment.  With this sociotechnical scenario in mind, we launched a participatory design process involving the Theatre Royal audiences to elicit their relationships with the archival materials, as well as suggestions for the development of the interactive system. Three focus groups were planned and designed to investigate audience engagement with physical and digital archival resources, and to inform the design of an online archive. When the Theatre Royal opened the call for interest, almost 150 people expressed their intention to participate in the focus groups. This overwhelming and unexpected response was interpreted as a clear sign of the will of the people to be involved actively in the ‘life’ of the Theatre. Due to logistic constraints (e.g., time, space, budget), we were able to involve 38 people in the three events targeted, respectively, at theatregoers under 26 years old, theatregoers over 26 years old, and nontheatregoers.  In the three focus groups, we addressed two main questions:   1. How do we design and promote audience engagement with cultural resources?   2. How do we design interactive systems entailing the collaboration between cultural institutions and their public?  For the focus groups, intersubjective meaning-making tasks were designed to experiment how to engage the different target groups with the physical and the digital resources. Different levels of engagement were observed during the three events. Engagement was observed in terms of spontaneous or incited interaction with other people and/or the archive, time spent interacting, and additional interactions not expected and/or requested by the facilitators. Based on our observations, four critical activities were also identified to create, manage, and display a collaborative online archive:   1. Developing a suitable database structure.  2. Digitising the archive and storing data in a suitable format.  3. Interface development.  4. Curating experiences.  Through a rapid prototyping technique, Chronopticon, a timeline prototype, was also developed in response to the public feedback on the design of an interactive system for the Theatre.  The focus groups revealed that there is space and interest both to design on-site and online interactive experiences, thus offering alternative and personalised ways to access archives. There is also scope to develop innovative tools designed for audience engagement, as well as co-constructed with the public. The design of the database and the framing of the community participation seem to represent the critical factors to focus on and to curate. When initiating a co-created project, it is central to ensure the long-term public engagement with the resource, as well as its maintenance. Involving the community beginning with the first phases seems to be a potential solution to address the sustainability concerns. Besides that, the experience and knowledge of local communities constitute an important part of the sense-making process of archive materials.  In this paper, we present the results of this pilot project, and recommendations to translate the findings into future actions to design and develop a community-driven and community-contributed archive.  ",
        "article_title": "Exploring Community Engagement in the Design of an Online Archive",
        "authors": [
            {
                "given": "Laura",
                "family": "Carletti",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Angeles",
                "family": "Munoz",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Jesse",
                "family": "Blum",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "James",
                "family": "Goulding",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Victoria",
                "family": "Shipp",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            },
            {
                "given": "Joanna",
                "family": "Robinson",
                "affiliation": [
                    {
                        "original_name": "University of Nottingham, United Kingdom",
                        "normalized_name": "University of Nottingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ee9ar58",
                            "GRID": "grid.4563.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "metadata",
            "libraries",
            "interface and user experience design",
            "archives",
            "museums",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "crowdsourcing",
            "software design and development",
            "information architecture",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Creating Public Accessible Virtual Heritage Models Why is this necessary? Hal Thwaites (2013) wrote in ‘Digital Heritage: What Happens When We Digitize Everything’: In the very near future some critical issues will need to be addressed; increased accessibility to (and sharing of) heritage data, consistent interface design for widespread public use and re-presentations of work, the formalization of a digital heritage database, establishment of a global infrastructure, institutionalized, archival standards for digital heritage and most importantly the on-going curation, of work forward in time as the technology evolves so that our current digital, heritage projects will not be lost to future generations. We cannot afford to have our digital heritage disappearing faster than the real heritage or the sites it seeks to ‘preserve’ otherwise all of our technological advances, creative interpretations, visualizations and efforts will have been in vain. So there is an international need to collate and store digital heritage models of heritage sites (Reinhard, 2013). We also lack a way to provide access to the models, sites, and paradata, which the London Charter (Denard, 2009) defines as ‘information about human processes of understanding and interpretation of data objects’). Although there are charters such as the London Charter and the Seville Charter, as there are few publicly accessible models (Barsanti et al., 2014), there is also no shared standardised evaluation data, and many scholars have complained about user experience issues and a scarcity of suitable pedagogical material (Economou and Pujol, 2008).    Technical Obstacles A serious technical obstacle is the absence of a shared, secure, feature-rich format for 3D models (Koller et al., 2009). International efforts to remedy the above issues include work by 3D Icons  (3D HOP) in CIDOC CRM, Europeana, Smithsonian Institute X3D BETA, Fraunhoefer (X3DOM ON GITHUB), Ariadne, CARARE, EU EPOCH, and V-MUST.   Long-term, we wish to investigate how 3D models can be better linked to library and archival systems of literature and multimedia that communicate important historical and cultural aspects of the simulated heritage site. Current journals that feature papers and 3D models typically lack integration with text resources, and they also have limited interactivity and immersion (Elsevier). A second long-term aim is to develop evaluation mechanisms to understand how the viewed and downloaded heritage models and simulations are used. Choosing a format that is robust, durable, well-supported, free, highly interactive, cross-platform, and easy to create or export to or export from is a serious challenge.   In Australia This is also an Australian problem. CSIRO (CSIRO, 2014) have released a report, stating, ‘Australia’s cultural institutions risk losing their relevance if they don’t increase their use of digital technologies and services’. Michael Brünig (Mansfield et al., 2014) has stated that the Australian GLAM industry is worth 2.5 billion Australian a year, roughly only a quarter is digitalized, and there are 629km worth of archival material. Brünig notes there is a shift to open-access models and greater collaboration with the public, but that we need to explore new approaches to copyright management that stimulate creativity and support creators. We also need to build on aggregation initiatives such as the Atlas of Living Australia, standardise preservation of born-digital material to avoid losing access to digital heritage, and exploit the potential of Australia’s Academic and Research Network (AARNet) and the National Broadband Network (NBN) for collection and collaboration.  There is another problem. Australian heritage includes large rock art sites, and the Gigapixel photographs, panoramas, and 3D data require a large amount of storage and new ways of navigating via online databases. There are 19 UNESCO World Heritage listed sites in Australia, including some of the oldest rainforests and one-third of the world’s protected marine areas. So we have vast and remote natural landscapes where pre-tour visits are expensive, 3D models and landscapes are not part of ICOMOS reports, and the Australian Burra Charter only recently reflects 3D. Of course there are also copyright, contestation, and forbidden knowledge issues that require legal experts and indigenous consultants.   Current Status For the presentation of this short paper I will discuss the above issues, our data management plan, and our communication with UNESCO to tackle these problems. Our research development unit will support the venture with funding for postdoctoral scholars and postgraduate students. I have proposed to UNESCO that our group will collate and archive the related heritage data, provide training material that can be developed and expanded by others, and recommend ways in which 3D models can be better linked to scholarly articles and related digital material and can be included in classroom teaching. We have linkages and ongoing relationships with other relevant universities in Australia and the wider Asian area, and organisations in Europe like DARIAH, NeDiMAH, and Europeana Cloud, and have an understanding from iVEC Curtin for storage of the initial models and related assets, but we are interested in talking to organisations about longer-term storage. The first task is to survey and develop a library of heritage models using a robust data management plan. However, a secondary task is to develop strategies to encourage scholars to submit to an agreed format, and then develop a workflow to provide models suitable for dissemination and general learning.   ",
        "article_title": "Infrastructure Requirements For A UNESCO World Heritage Archival Infrastructure",
        "authors": [
            {
                "given": "Erik Malcolm",
                "family": "Champion",
                "affiliation": [
                    {
                        "original_name": "Curtin University, Australia",
                        "normalized_name": "Curtin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02n415q13",
                            "GRID": "grid.1032.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "libraries",
            "interdisciplinary collaboration",
            "archives",
            "geospatial analysis",
            "museums",
            "sustainability and preservation",
            "archaeology",
            "interfaces and technology",
            "English",
            "GLAM: galleries",
            "digital humanities - facilities",
            "digital humanities - pedagogy and curriculum",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Images and Written Language Yet visualisation is at least as old as written language. The earliest symbols are artistic rather than literary. Historically, the distinction between text and symbol has been blurred, from early European languages to Asian languages and as part of world history in general  . Even today, language is geographically influenced  . One can include cave paintings; they reveal the long-term association between image, space, and meaning  Brown, 2012). In some prehistoric caves the paintings are apparently spatial pointers to reverberant spaces (and reverberation apparently indicates spirituality) while some archaeologists believe Lascaux cave paintings are maps of the stars (see  http://news.bbc.co.uk/2/hi/science/nature/871930.stm). In Australia, traditional Aboriginal paintings are visualisations of mythical knowledge as well as environmental resources; they are cultural schemas and ‘totemic maps’  .   Textual Literacy To improve public access to digitalised material  we also need to tackle the problem of literacy, digital literacy, and digital fluency  . Multimedia, visualisations, and sensory interfaces can communicate across a wider swathe of the world’s population. And although literacy is increasing, technology is further wedging a fundamental divide between those who can read and write and those who cannot  . Yet developing visual literacy is still nascent, even though more and more people read by viewing graphical interfaces, and text-based interfaces cause serious problems for people with low levels of literacy  .    Non-Text-Based Media Are Part of Digital Humanities Archives are not just text, and the digital humanities  are collaborative and interwoven. Even the book itself is a material, embodied experience. Literature itself is linked to both the image  and to materiality  ; the materiality of Icelandic sagas and runic inscriptions are considered by various scholars to be essential properties  . Humanities is/are not merely multimodal but also embodied experiences. The objects in and on which the humanities are described, critiqued, and preserved are more than just holders for text; they are essential artefacts that give researchers essential clues in the interpretation of text and author. Material objects are not merely brute objects; they are symbolic as well, inscribed into the lived and symbolic world  .     Scholarly Arguments Research labs that call themselves visualisation labs include Kings College Visualisation Lab ( http://www.kvl.cch.kcl.ac.uk/dhi.html), Wired! Lab Digital Art History and Visual Culture (http://www.dukewired.org/), AliVE—Applied Laboratory for Interactive Visualization and Embodiment ( http://www.cityu.edu.hk/scm/alive/), and the founder of CAVE VR, EVL—Electronic Visualization Laboratory (https://www.evl.uic.edu/). The name also appears in conferences and in journals—for example, the  Journal of Visualization and Computer Animation  . Can they all be wrong?   The above labs and journals suggest that visualisation can also be the development of a simulation rather than the depiction of data or a model. These laboratories may produce high-resolution graphical models to represent archaeological data, but they can also produce simulations to test hypotheses. Beat Schwendimann  explains the distinction succinctly:   A model is a product (physical or digital) that represents a system of interest. A model is similar to but simpler than the system it represents, while approximating most of the same salient features of the real system as close as possible . . . [while] . . . a simulation is the process of using a model to study the behavior and performance of an actual or theoretical system. . . . While a model aims to be true to the system it represents, a simulation can use a model to explore states that would not be possible in the original system.  By creating a simulation rather than a model, we can test hypotheses. So, yes, I am also arguing that games can be considered to be visualisations, of the designer’s mental model of the game world. Games (with examples like  Papers, Please; September 12; and  Space Refugees) are persuasive  and rhetorical simulations.  Earlier definitions don’t appear to understand the persuasive and rhetorical nature of visualisations, perhaps because they wish to use them as value-neutral tools. For example, McCormick et al.  defined visualisation as ‘to form a mental image of something incapable of being viewed or not at that moment visible’ . . . (Collins Dictionary) . . . ‘a tool or method for interpreting image data fed into a computer and for generating images from complex multi-dimensional data sets’. This definition invalidates visualisations that predate the large use of data, does not attempt to explain the mental model and process behind the visualisation, and believes visualization must focused on image generation.   More interestingly, Kosara  posited three interesting criteria for visualisation: it must be based on (nonvisual) data, produce an image, and the result must be readable and recognizable. He added this interesting subcriteria: ‘In addition to readability, a visualization has to be made with the intent to communicate data three steps: realizing that data is being visualized by the image, understanding what is being visualized, and how the display is to be read’. Unfortunately he also tried to create a simple spectrum ranging from practical visualisations to sublime/artistic visualisations. I believe he has conflated two quite different concepts, at least according to Kantian aesthetics. But he has come closer to a more useful and generic definition: visualisation should reveal the process behind the output.  But where is visualisation as a research tool in its own right? Can’t visualisation actually  create new research questions or at least prove difficult to answer questions? Examples in my field, virtual heritage, include a photo-realistic model which showed colours not actually visible in the ruins of the remaining temple  , computer modelling to deduce the astronomical function of ancient Roman obelisks (http://idialab.org/virtual-meridian-of-augustus-presentation-at-the-vaticans-pontifical-academy-of-archeology/), or digital data-driven maps to create historically derived visual descriptions of ancient Roman journeys ( http://orbis.stanford.edu/). There have also been more general humanities-orientated papers arguing that visualisation can be reflective and critical  (Dörk et al., 2013; Jessop, 2008; Robichaud and Blevins, 2011).     Summary Visualization is an extremely significant aspect of digital humanities, and writers such as Burdick et al.  agree, but we need to improve our understanding and communication of visualisation as being part of the humanities not just now but also historically, before the advent of computer data.  And to recap: historically text has not lived in a hermetically sealed hermeneutic well all by itself. A world with literature but without the arts is intellectually and experientially impoverished. Critical thinking and critical literacy extend beyond the reading and writing of text. Visualization can make scholarly arguments relevant to the humanities. Therefore non-text-based research should figure more prominently in digital humanities readers and monographs.   ",
        "article_title": "Seeing Is Revealing: A Critical Discussion on Visualisation And The Digital Humanities",
        "authors": [
            {
                "given": "Erik Malcolm",
                "family": "Champion",
                "affiliation": [
                    {
                        "original_name": "Curtin University",
                        "normalized_name": "Curtin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02n415q13",
                            "GRID": "grid.1032.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "digital humanities - nature and significance",
            "English",
            "history of Humanities Computing/Digital Humanities"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Motivation A common task nowadays is the retrieval of information using search engines such as Google. Often, the results of a usual keyword search are not satisfying, and the user is forced to reformulate the query in order to improve the quality and to increase or reduce the number of search results. When searching ancient texts for passages related to a specific topic, humanities scholars encounter similar problems. Considering  epilepsy as an example, the traditional keyword search for the corresponding Latin term  morbus comitialis (‘disease of the assembly’) yields incomplete results. Operating a truncated search for  morb* comiti* extracts more but less accurate results. It also yields false positives like a passage from Titus Livius ( Ab urbe condita libri 41,18,16) containing both the words  morbus and  comitia but in a political context only. A further issue regarding the mentioned example is the existence of texts that relate to  epilepsy but don’t use the corresponding terms explicitly (e.g., Lucretius,  De rerum natura 3,487ff). Here, the disease is paraphrased via occurring symptoms such as  spuma (‘foam’) and  concidere (‘collapse’). Furthermore, searching for  epilepsy only by the term  morbus comitialis is incomplete because there are many different terms denoting the disease. Therefore, a simple keyword search is inadequate in finding appropriate results.  The prior goal of the eXChange 1 project is to extract more and more accurate results by extending the traditional keyword search with a so-called concept search. For this purpose, we develop a concept editor that supports the scholars in modeling their ideas of concepts. Using these concept models, we operate a concept search on ancient corpora and adequately rank the results, such that texts supporting the modeled concept are ranked higher. The significance of our concept modeling methodology is to empower the humanities scholar to construct concepts using her expert knowledge, and iteratively refine the model after analyzing the search results.    Related Work  Besides the aforementioned traditional keyword search, more sophisticated approaches have evolved over time. A  semantic search (Guha et al., 2003) takes the contextual meaning of a term into account by automatically considering synonyms, word variation, and so on, whereas a  concept search (Giunchiglia et al., 2009) automatically retrieves data that is conceptually similar to the formulated query. These search strategies can be supported with  topic maps (Park et al., 2002) or  concept maps (Novak et al., 2006), where certain topics and relationships can be modeled in a graph structure. Based upon reference texts such as glossaries, thesauri, and classification systems, these models can be automatically generated for modern languages. Appleford introduced human-generated  topic profiles that consist of various allowed and disallowed terms forming a Boolean query (Appleford et al., 2013). 2 The major difference of our suggested concept modeling compared to existing techniques is that concepts in information retrieval are formulated primarily using existing semantic knowledge from thesauri, topic maps, semantic networks, and so forth. Our suggested concept modeling allows the scholar to express her understanding of the concept model without the need of any semantic knowledge.   Teevan argues that even with a perfect search engine, a poorly constructed search question may not lead to the right answer, so the user needs to be provided with a directed search system (Teevan et al., 2004). This becomes quite relevant in case of ancient corpora that comprise a particular complexity and uncertainty, which leads to a potential evolution of concepts in ancient times. Also, the sparse presence of reference texts for ancient languages requires a semiautomated generation of concepts. For this task, we present a new methodology to assist humanities scholars in searching ancient texts by empowering them with a directed search environment in the form of a concept editor (discussed in the next section), where a scholar can model a concept using simple graph building tools. This was also a requirement of the collaborating humanities scholars as they wanted full control over the search process, which an automated search system fails to provide.   The Concept Editor   Figure 1 shows a screenshot of the concept editor 3 containing the concept  fever. Various shapes are provided for concept modeling. Initially, the scholar defines the main concept  fever, drawn in blue. Other shapes representing subconcepts or terms can be drawn via drag&drop to support structuring the concept in a meaningful way.      Figure 1. Concept editor. The  fever concept consists of the subconcept  fever symptoms, which is connected to related Latin terms, and the subconcept  fever labels, listing all known Latin terms used to describe fever. The editor provides four ways of connecting a shape to its predecessor discriminated by color. Green shapes support the given concept while red shapes are contradictory. Saturated colors represent definite knowledge of the scholars, whereas light shades represent assumptions. According to the model, texts containing the terms  febris or  febrire (‘to have fever’) were most likely used to address the concept  fever. Terms like quartana may denote fever (‘quartan’) but also serve to name other things. Thus, they are colored in lighter shade.  Each term shape is connected to all possible spellings and word-forms contained in the database. A popup on demand provides a list and tagcloud that allow one to observe and select terms related to the corresponding concept. Figure 2 shows the selected word-forms for the term  quartana. As  fever in Latin is female, other gender forms of the adjective  quartanus can be excluded. By taking advantage of language-specific grammar rules, it will help the scholar to avoid irrelevant results.      Figure 2. Selecting word-forms.    Figure 3. Concept  epilepsy  The current model for the concept  epilepsy is shown in Figure 3. It also contains subconcepts for labels and symptoms. Most of the Latin terms denoting epilepsy are represented by two single words (e.g.,  morbus comitialis or  morbus maior). The subconcept  symptoms is further subdivided, showing, e.g.,  the affected parts of the body. The unlikely symptom  fever is imported as a contradictory subconcept, so all  fever-related terms represent a contradictory relation to the concept  epilepsy. Additionally, potential causes of epilepsy (e.g.,  vinum [‘vine’]) are given, and a list of contradicting political terms reduces the relevance of political texts for which the term  comitialis is common.  After the concept is built, it is stored for persistence and forwarded to a concept search module. The selected word-forms and spellings of all terms are the basis for the search. For each text of the corpus, we compute a relevancy rank according to the proximity to the given concept. Each occurring ‘green term’ increases the rank of a text, and each occurring ‘red term’ decreases it. Finally, the result list contains all texts ordered by decreasing relevance. 4   Preliminary Results  The results of the search for the concept  epilepsy are partially consistent with the humanities scholar’s expectations. So, texts of authors popular for their medical works are highly ranked, e.g.,  De medicina by Aulus Cornelius Celsus and  Naturalis historia by Pliny the Elder. Furthermore, a very low rank is calculated for the unrelated political text  Ab urbe condita libri, written by the Augustan chronicler Titus Livius—one of the top results of the traditional keyword search.  The appearance of two texts with a high rank is surprising for the scholars:  De re coquinaria by Apicius on cuisine and  De re rustica by Columella on agriculture. That these texts appear among the results may be due to the term  vinum. First, it can be assumed that it refers to epilepsy, but a detailed examination of the respective passages is required to find out if it occurs in remarks on epilepsy or, say, outline the relationship between nutrition and health (dietetics).  Conclusion The benefit and potential of the presented concept search system compared to the traditional keyword search are not only in their multifacetedness but also the enabled persistence of the modeled concepts. Depending on the accuracy of search results, the scholar can iteratively improve results, either by adding or excluding terms from concepts. Retaining the control on the search and the ability to steer the search into a desired direction, now supported by the concept editor, was a major concern of the scholars. Often, digitized ancient fragments are neither associated with an author nor with a certain title. This complicates their automated classification as authors are often related to specific genres. Based upon the modeling of generic concepts (e.g., the concept  political terms), the potential genres of unclassified texts could be hypothesized.  As a side benefit of the concept modeling, we automatically collect and store the modeled hierarchies and relationships between (sub)concepts. Slightly enhancing the scholar’s modeling capabilities, we might be able to assemble thesauri for ancient languages in the future. Notes 1. Digital Humanities project, eXChange: Exploring Concept Change and Transfer in Antiquity, http://exchange-projekt.de/. 2. Operating on social web sources, one is able to discover occurrences that match the given profile while dropping text passages that contain any of the disallowed terms. We consider that excluding text passages due to a disallowed term may result in the loss of important text passages. 3. The web-based concept editor is written in JavaScript and uses the libraries mxGraph (https://jgraph.github.io/mxgraph/) and jQuery (http://jquery.com). 4. In the current status, the search algorithm works on whole texts only. In the future, we will provide a list of text passages with a high density of related terms to further facilitate the scholar’s workflow. Furthermore, we will provide a transparent view of the results in the form of visual statistics illustrating the reasons for the ranking of a single text passage and the distribution of a concept’s related terms in general. ",
        "article_title": "Modelling Concepts to Improve the Search Capabilities on Ancient Corpora",
        "authors": [
            {
                "given": "Muhammad Faisal",
                "family": "Cheema",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Judith",
                "family": "Blumenstein",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Gerik",
                "family": "Scheuermann",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "data modeling and architecture including hypothesis-driven modeling",
            "relationships",
            "information retrieval",
            "data mining / text mining",
            "English",
            "graphs",
            "knowledge representation",
            "networks",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Within the epistemological explorations of the digital humanities, one of the profound phenomena is the parallel movement of institutional infrastructure development and moving boundaries between various academic fields. At times these movements develop with synergism, at other times in isolation; occasionally they are in conflict. Using a specific case at the University of Lausanne in Switzerland as a starting point, this paper seeks to illustrate and explore how digital humanities and life sciences interact epistemologically. The aim is to demonstrate the impact of research infrastructure on the epistemological reconfiguration of academic knowledge, and vice versa. Thus one may regard such an idea in Foucault’s terms: ‘words’ and ‘things’ are interrelated. At the beginning of 2015, a new step for DH developments was taken at the University of Lausanne. Collaborations on diverse modes have been developed between the DH team and their bioinformatics colleagues—namely, applications for common projects at Swiss and international levels (research infrastructure, data-life-cycle management, DH applications); publication of a new editorial form, the eTalks; 1 a common day of training and education on data visualization. 2 Finally we have enrolled two part-time IT and DH postdocs to create an exchange of knowledge between common projects: one has a doctorate in archeology with a good knowledge of databases, and the other a doctorate in bio-IT bioinformatics with a strong interest in DH. There is much to be learned from the 25-year-old experiment at the Swiss Institute of Bioinformatics (SIB) 3 that could shed greater light on the digital humanist turn.  Indeed, the digital humanities adventure started in Switzerland at the University of Lausanne in 2010, with the encouragement of the vice-rector research, Philippe Moreillon, a professor in medicine stating that DH would probably follow an evolution similar to the path taken by bioinformatics. This statement sounded perhaps slightly opaque to certain ears at that time, yet it has become increasingly clarified and useful with regards to elements related to the development of DH at UNIL and in Switzerland. Thus, the digital humanists at the UNIL have begun discussions with the SIB and its IT bioinformatics competence center, VITAL-IT. 4 In a 2014 lecture in Lille, 5 I drew some common points between the developments of bioinformatics and digital humanities. Ioannis Xenarios, director of VITAL-IT, has rewritten Claire Warwick’s definition of the digital humanities 6 for bioinformatics, replacing a few words: ‘Bioinformatics is an important multidisciplinary field, undertaking research at the intersection of digital technologies and biology/medicine. It aims to produce applications and models that make possible new kinds of research, both in the biology/medicine and in computer science and its allied technologies. It also studies the impact of these techniques on knowledge representation, data mining technologies, knowledge-base, archives and digital culture’. 7  At a national level, Switzerland has started an innovative long-term program of at least eight years (2013–2020). The program, Scientific Information: Access, Processing and Safeguarding, 8 will attempt to coordinate all the efforts and strategies regarding computing academic challenges. Within this program, all fields are invited to build the new digital world of science. The title of the program, adopted for the years 2017–2020, clearly relates IT services and information to fundamental research: ‘ Service et information numérique: un nouveau lieu de la recherche’. One of the main themes of this program is ‘data-life-cycle management’, a perspective encompassing entire cycles from the production of primary data to final publications. Currently, it is unknown whether a common protocol of ‘data-life-cycle management’ for humanities and life sciences is possible and adequate: surely it deserves to be investigated. Such data management rationalization efforts might sound uncommon to certain digital humanist ears and minds, but even a milestone DH work such as Franco Moretti’s  Graphs, Maps and Trees (2005) is deeply inspired by IT research and what is traditionally referred to as ‘hard sciences’. Not only is this true of its content, but also in its genealogy. Indeed, it was preceded by another Italian book, by Giulio Barsanti, using the same aforementioned title, which explains the emergence of modern sciences:  L’Albero, la scala, la mappa (1992). 9 This genealogy will be examined in this paper.  To better understand what is at stake in such transformations in a specific university and country, we have to look back. Until now, funds for fundamental research and research infrastructure were separate in Switzerland. It is the legacy of what was referred to in 2012 as a ‘blind spot in modernity’ (see Clivaz, 2012, 32): the underestimated influence of writing material on ideas and concepts, as thoroughly demonstrated by Roger Chartier (1995, for example). The climax of printed culture has led to such a situation, as one can observe in the posture adopted by William Osler. In 1919, this famous Canadian physician and professor often described as the ‘father of modern medicine’ (see Fins, 2008),  was invited to make a presidential address before the Classical Association at Oxford (Osler, 1919). 10 He spoke about ‘the old Humanities and the new science’, honoring the classical medical heritage of Hippocrates and Galen, and underlining the role of the humanities as ‘the hormones’ of intellectual life: ‘the men of your guild secrete materials which do for society at large what the thyroid gland does for the individual’ (Osler, 1919, 3). With a deep attachment to humanities, this lecture deplored the fragmentation of knowledge: ‘Specialism, now a necessity, has fragmented the specialties themselves in a way that makes the outlook hazardous’ (Osler, 1919, 5). Yet at the very same time, he belonged to this ‘specialism‘ trend with respect to his work; all his thinking appears congruent with a child of the maximal stage of printed culture. This famous lecture was followed by an exposition (Warner, 2014) of Osler’s rare books; he was a book lover (Osler, 1901). It is evident that the following decades of the 20th century saw the life sciences and humanities increasingly separated, and since then, knowledge has become like a broken mirror. Such a genealogy will be further examined, and will contribute, along with all the previous elements mentioned, to a consideration of the role of the digital humanities and IT research in life sciences within the epistemological knowledge turn we are currently facing. When the boundaries between research and infrastructures throughout all the fields are overcome, a new world can emerge.  Notes 1. etalk.vital-it.ch/rites-funeraires; etalk.vital-it.ch/dh; etalk.vital-it.ch/mooser (all accessed 1 March 2015). 2. http://edu.isb-sib.ch/course/view.php?id=195. 3. http://www.isb-sib.ch/. 4. See  http://www.isb-sib.ch/ and  http://www.vital-it.ch/: ‘Vital-IT is a bioinformatics competence center that supports and collaborates with life scientists in Switzerland and beyond. The  multi-disciplinary team provides expertise, training and maintains a high-performance computing (HPC) and storage infrastructure, so as to help develop, maintain and extend life science and medical research’.  5. Claire Clivaz, Panorama institutionnel des Digital Humanities à partir du cas suisse: vers la dissémination Digital Humanities?, http://publi.meshs.fr/ressources/panorama-institutionnel-digital-humanities/@@video. 6. See http://www.ucl.ac.uk/dh/courses. 7. Ibid.  8. See http://www.swissuniversities.ch/en/organisation/projekte-und-programme/suk-p-2-wissensch-information-zugang-verarbeitung-speicherung/; national strategy English summary: http://www.swissuniversities.ch/fileadmin/swissuniversities/Dokumente/EN/UH/SUK_P-2/SUK_P-2_NationaleStrategie_20140403_EN.pdf. The CUS P2 program is presided over by Prof. Dr. Martin Täuber (rector of the University of Bern), and directed by Dr. Roland Dietlicher (ETHZ) and Gaby Schneider (University of Basel). 9. Thank you to my sociologist colleague Franco Panese for this reference. 10. Thank you to the DARIAH colleague who provided me this reference in September 2014 in Rome, and whose name I have unfortunately forgotten. ",
        "article_title": "Beyond Boundaries: Digital Humanities, Life Sciences and IT research",
        "authors": [
            {
                "given": "Claire",
                "family": "Clivaz",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne (CH), Switzerland",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digital humanities - institutional support",
            "interdisciplinary collaboration",
            "digitisation - theory and practice",
            "digital humanities - nature and significance",
            "English",
            "cultural infrastructure",
            "knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " What does it mean to build visualization tools that support the research process in the humanities? In this paper we will trace the evolution of our thinking about data-driven tools beginning with case studies in early modern intellectual history and eventually including a wide range of projects from classics, social history, performance studies, and other fields. We will give concrete examples of how individual tools were designed and whether those tools ultimately failed or succeeded to provide scholars with a means to gain insights into historical data. Through these examples, this paper argues for the role of an open design process in the development of visualization tools for humanities research that brings designers, developers, and scholars into deep collaboration to build nuanced and rigorous tools for humanities research.  Mapping the Republic of Letters Mapping the Republic of Letters was formed on the assumption that intellectual history is one of the fields that stands the most to gain from the influx of big data. By combining metadata from library catalogues and large-scale digitization projects, the project seeks to maximize the transformative effect of all this information. The cartographic, chronological, and network visualizations ultimately produced allow researchers to examine some of the big questions that intellectual historians have long struggled with: How do intellectual networks function? How interconnected are they? How independent are these networks from other social networks?  The 2009 Digging Into Data Challenge grant award launched an active tool development phase in the Mapping the Republic of Letters (MRofL) project at Stanford. In partnership with DensityDesign Research Lab in Milan, the team began to engage in a tool design process in response to concrete research questions. The data—based on individuals and their correspondence, travel, and publications—were multidimensional and qualitatively rich. It became clear that historians who wish to bring data visualization tools to bear on the study of the past face a number of challenges. Many available tools had a steep learning curve and were ultimately of limited help for humanists. These tools rest on assumptions about the completeness and empirical value of data that often do not hold true for humanities research. Historical data can be incomplete and messy: statistical analysis can be a helpful to a limited extent, but interpretation at the most fundamental level is required to uncover meaning. Humanists also ask questions about the data that cannot be answered by numerical analysis. We needed tools that help us filter, contextualize, compare, and see the gaps in our data.  Humanities + Design  Humanities + Design, a research lab founded in 2012 by Dan Edelstein, Paula Findlen, and Nicole Coleman, emerged directly out of lessons learned and opportunities for humanities data analysis discovered through MRofL. The mission of the lab is to produce, through the lens of humanistic inquiry, new modes of thinking in design and computer science to serve data-driven research in the humanities. We believe that humanistic inquiry, grounded in interpretation, has much to contribute to the development of technologies if they are to help us reveal ambiguity and paradox, allowing human-scale exploration of complex systems. In the laboratory environment, theoretical and methodological discussions happen side by side with hands-on work with digital materials. Humanities scholars and students, designers, engineers, and computer scientists engage together in ongoing tool design as defined by the specific needs of participating humanities projects.  Palladio Project The award of the 2012 NEH Implementation Grant for Networks in History allowed the lab to pursue the development of visualization techniques and rich interaction with data that supports ‘thinking through data’ rather than using prescribed algorithms for data analysis. Palladio is a web-based demonstration application that allows any researcher to upload, visualize, and explore complex and multidimensional data, directly in a web browser.  It has been designed for humanistic inquiry, with a special focus on historical research.  The Palladio visualization system combines a primary view (for example, Map, Network Graph, and Tabular views) with filters to make it easy to query a dataset.  There is no need to create an account, nor do we store any data .  Researchers can save and shared the work they have done in the browser as a Palladio Project. Palladio’s TimeLine and TimeSpan filters encourage filtering and sorting temporal data, and allows the filtering of two or more discontinuous time periods. A Facet filter is also particularly useful when exploring multidimensional datasets and drilling down to specific aspects of one’s data. Using case studies (examples listed later in this document) we will discuss how scholars have used Palladio, highlighting those instances when uses of the tool diverged from our expectations or led us toward new insights that we incorporated (or plan to incorporate) in future versions.  Open Design  The development of Palladio has been an iterative process. We have been eliciting and incorporating feedback from the academic community concerning Palladio’s current and potential features and uses. Most specifically, we have engaged in sustained discussion with a small and inter-disciplinary group of scholars, known as Open Design Contributors. Our paper will offer insight into this design process and the ways that it has directly influenced current and future iterations of Palladio, as well as other tools.  Summary The core innovation of our project is the design of visualization techniques that emphasize the contextualization and interpretation of data in cases where we lack the metrics for useful quantitative analysis. The two other key innovations both involve the leveraging of novel technologies that are particularly important to the study of cultural heritage data: we use new flexible data models to let individual scholars create and apply their own data categorizations, and we use open linked data sources to reconcile datasets against established authority files, in order to link entities across datasets and thereby explore networks across collections.  Additional Case Studies to Be Discussed   Case Study: Toward More Complex Ways of Displaying Travel   Kate Elswit, Lecturer in Theatre and Performance Studies at the University of Bristol, ‘Ballet, Digital History, and the Cold War: Visualizing the Labor of Dance Touring’  Dance scholar Kate Elswit has been using Palladio in her research on the labor of dance touring. She writes, ‘Such [visualization] techniques enable us to feel the passage of time differently.’ Following discussion with Elswit and other scholars interested in tracing travel routes, we have been thinking about how to display point-to-point travel in ways that go beyond simple flight-path-like visualizations. How to account for the differences in traveling at night rather than in the day? How to represent different levels of comfort, safety, and efficiency in travel? Case Study: Questions of Scale and Incomplete Data  Molly Taylor-Poleskey, PhD Candidate, Department of History, Stanford: Food Culture in Brandenburg-Prussia   Taylor-Poleskey uses a large base of manuscript sources detailing the yearly consumption of one of the palaces of Prince-Elector Friedrich Wilhelm of Brandenburg-Prussia. She argues that the elector’s cultural agenda helped transform his territories over the course of his reign from dilapidated and war-torn to stable and powerful. To support her argument, she wants to see how tastes and consumption patterns changed over time, to consider how such changes might reveal the court’s aesthetic values and cultural ambitions. Creating visualizations in Palladio have helped her analyze what proportions of different foods or food groups were consumed. We have worked with Taylor-Poleskey toward creating visualizations that privilege the display of relative magnitude, and that are especially sensitive to working in different registers and scales. As some of the years in the sources she studies have incomplete or missing data, her use case has also aided us in thinking about how best to work with and represent incomplete data in ways that are not misleading or overly simplistic.  Case Study: Toward New Palladio Data-Visualization Iterations  Office of the Historian, US State Department, Foreign Relations of the United States  We will share results from our ongoing work with Thomas Faith at the Office of the Historian at the US Department of State, with whom we have been working toward the goal of producing an integrated version of Palladio that would function as a visual browser for extant online data concerning the foreign relations of the United States. The State Department project is one of many we are working on, as we look to help other researchers to implement customized versions of Palladio that can be used as search, analysis, and visualization exploratory tools within extant large-scale research projects. ",
        "article_title": "From Mapping the Republic of Letters to Humanities +Design Research Lab: Creating Visualization Tools for Humanistic Inquiry",
        "authors": [
            {
                "given": "Catherine Nicole",
                "family": "Coleman",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Giorgio",
                "family": "Caviglia",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Maria",
                "family": "Comsa",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Braude",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Dan",
                "family": "Edelstein",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Giovanna",
                "family": "Ceserani",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "interface and user experience design",
            "relationships",
            "spatio-temporal modeling",
            "English",
            "graphs",
            "software design and development",
            "analysis and visualisation",
            "knowledge representation",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Introduction  This paper describes the results of a computational study of Early Modern Tragedy with particular reference to Elizabeth Cary’s 1613 closet play,  The Tragedy of Mariam. Critical responses to  Mariam are contradictory and this paper explores the extent to which a function word study can provide insights that complement or clarify criticisms arising from more orthodox approaches.    Theoretical Framework  This paper suggests that Langacker’s Cognitive Grammar provides a theoretical framework to support an interpretive application of computational stylistics. Cognitive Grammar is aligned with other functional approaches to linguistics and appears to provide a theoretical explanation at the linguistic level for the results that are produced in a function word study, providing:  • a framework for the types of authorship studies that have dominated computational  stylistics; and  • a way of extending the results in a more interpretive context.  Key to this paper is Langacker’s view of function words, which he argues “make a semantic contribution to the constructions they appear in, even though this contribution may be fairly schematic” ( Theoretical 19). This is through the role function words play in the construal of meaning.   This paper considers  Mariam in the context of other plays from the period. The study   includes a total of 60 plays printed between 1580 and 1641. Within this group of 60 there  are 12 closet plays and 48 plays written for the commercial stage. As well as  Mariam, the set of texts includes the other eleven closet tragedies associated with the “Sidney Circle”. The only other female-‐authored text in the group, Mary Sidney’s  Antonius, is a translation, as is Thomas Kyd’s  Cornelia.   The Sidnean closet tragedies have been described as “strikingly alike, and strikingly unlike any other dramas in English” (Witherspoon 179). Their known authorship, along with their formal similarity, makes these plays an interesting set for non-‐authorship study. Critical responses to  Mariam are divided on several key questions including ideas about women’s speech, notions of performance and privacy, and how closet drama relates to early modern literary and dramatic culture. Much of the criticism is strongly biographical in its approach, reading the play in the context of Cary’s conversion to Catholicism in 1626 and related conflicts.   Mimetic readings are further encouraged by the existence of a biography of Cary by one of her daughters. Although biographically informed arguments of  Mariam have been developed in more complex ways, Cary’s textual engagement is typically considered in terms of ideological and personal constraints as a text that is about a woman’s right to freedom of conscience within the bounds of marriage. It is then suggested by analogy that these claims extend to relationships between subjects and rulers, drawing on the common Early Modern metaphors of state as a commentary on the failings of patriarchal systems of government.   Results of these analyses, however, are somewhat conflicting. Suggestions that the play is the most stageable of the closets sit uncomfortably alongside claims that the play is rejection of female performativity in favour of interiority. Closet drama is acknowledged as a specifically political form and male-‐authored closet texts are commonly seen as reflecting Jacobean anxieties over the relation between monarch and advisor and engaging with the ‘advice’ genre. This is not, however, the case with  Mariam. One study, by William Hamlin, resists the tendency to consider  Mariam in terms of constraints on agency and reads it instead as an exercise in moral philosophy that explores the problem of doubt and epistemological uncertainty. Given the diversity of critical responses to Mariam, the application of computational stylistics seemed to offer a way to contribute something new to this body of criticism.   Method and Results  As a first step a discriminant analysis on the basis of the frequency scores of 241 function words was carried out to see what sorts of differences, if any, would be picked up between the two groups of plays. 1 The discriminant analysis used 563, 2000 word segments (104 from closet plays and 459 from stage plays). Step-‐wise leave-‐one-‐out analysis was used to cross-‐validate the results. A step-‐wise discriminant analysis used only 38 function words from the full list to identify 100 per cent of the segments correctly as either a closet segment or a stage segment. Figure 1 graphs the discriminant scores and shows the groups  separating into two distinct sections, both of which represent something close to separate normal curves.      These results were subjected to a range of further tests to exclude the possibility of artefact playing a role in the delineation of closet or non-‐closet status. In all cases, the results showed that discriminant analysis continues to find evidence of a difference between closet and non-‐closet plays on the basis of function words, even when the test is not ‘told’ whether the segment in question is a closet segment or a stage segment. As a way of understanding how Cary’s play relates to other plays from the period, the texts were subject to a Principal Component Analysis (PCA). Given that  Mariam conforms very closely with the formal requirements of neo-‐classical tragedy, is constrained by an adherence to a strict rhyme scheme, and that Cary would have been subject to a restrictive set of gendered cultural assumptions concerning authorship and publication, the identification of  Mariam as more like a play for the stage than any of the other Sidnean texts was a surprising result (Figure 2).     Figure 2: Principal component analysis for 60 tragedies in 4000 word segments for 100 most frequently occurring function words The stage texts with which  Mariam bears the closest relationship appear to be Jonson’s  Sejanus (1603) and  Catiline (1611), as well as Marston’s  Wonder of Women (1606), Chapman’s  Revenge of Bussy D’Ambois (1613) and Goffe’s,  The Courageous Turk (1632). Jonson’s two texts draw heavily on classical dramatic traditions but were first performed at the Globe and so are treated as plays for the public stage in this study, although they have been described as “even more classical, though less specifically Senecan than the School of Pembroke” (Lucas 112).  In order to identify the function words that occur most frequently in  Mariam in comparison to the other texts in the study, a table of  z-‐scores was produced. Table 1 lists the fifteen highest z scores for  Mariam grouped by word class.     Table 1: Fifteen highest  Mariam z-‐scores -‐ 241 function words in 60 tragedies  Discussion The study aims to link particular variables that are identified as markers for  Mariam with a specific rhetorical strategy. The paper argues that Cary’s use of auxiliary verbs, modal auxiliaries and conjunctions can linked to Hamlin’s reading of the text, especially the notion of epistemological uncertainty, the consideration of issues from multiple perspectives, and the struggle to achieve emotional stability and a position of psychological strength and stability.  Using Cognitive Grammar as a framework, the paper argues that Cary’s use of function words operates a structural level to reinforce the themes explored in the play. Auxiliary verbs function to control the extent to which the reader engages emotionally with the text and control how the elements of a sentence being profiled relate to the conception of reality that is being discussed. Cary’s frequent recourse to particular auxiliary forms points to conditional constructions and is a grammatical sign that the counterfactual space is in focus (Fauconnier, \"Mental Spaces\" 357-‐8). At a fundamental level, the modals that are selected as significant in  Mariam reveal the type of internal conflict with which the characters are engaged: they struggle with difficult moral questions about conscience, duty, and integrity as well as challenges to their autonomy. In terms of conjunctions, the effect is one of “mental juxtaposition” (Langacker,  Investigations  354) relating to cognitive phenomena associated with notions of “dynamicity, fictivity, and mental spaces” (Langacker,  Investigations 354). Cary’s use of conjunctions, like her use of modals and auxiliaries, can be linked to features of the genre in which she is writing, and to the lack of equilibrium that characterises the internal world of the characters who employ these forms.  Conclusion Through a computational stylistic analysis, the contradictory readings associated with  Mariam begin to make sense. The text is the most theatrical of the closets, but many other features signal a strategic alignment with classical conventions and literary/philosophical discourses typically associated with closet texts. The idea of contradiction is present at a very basic linguistic level and is central to understanding the inconsistencies identified by scholars working in more orthodox traditions. Computational Stylistics provides an insight into the text as a work of moral philosophy, highlighting the way the variables function to  construct the world of the play and the readers as interpreters, providing subtle clues about the uncertainty and doubt that prevails in the text and in the world. ",
        "article_title": "Function word stylistics and interpretation: Elizabeth Cary's Mariam",
        "authors": [
            {
                "given": "Louisa",
                "family": "Connors",
                "affiliation": [
                    {
                        "original_name": "University of Newcastle, Australia",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "literary studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The ARTFL Project would like to submit a proposal for a short paper on the development of an API for PhiloLogic4, our next-generation corpus query and text retrieval platform for digital humanities databases, and to demonstrate Android PhiloReader Apps that we are building to interact with that API. 1  A primary goal of the PhiloLogic4 project has been to allow ARTFL or any digital humanities group using this software to develop a variety of results display and user interfaces with ease. 2 For example, the databases ARTFL has already released in beta form under PhiloLogic4 for traditional browser access have features such as frequency sidebars for query results, links within those sidebars for faceted browsing, and dynamic Time Series reports. 3 ARTFL’s PhiloReader Apps extend and exemplify this fundamental design goal. They take advantage of PhiloLogic4’s simple set of query parameters and flexible results object formatting to enable text search and retrieval on handheld devices.   ARTFL intends these apps to serve as a lighter-weight alternative to web browser apps for interacting with the PhiloLogic4 installations of text databases on our main production servers. The interface has been designed with a focus on the reading functionality that PhiloLogic4 already offers in its web incarnation. Users can conduct word and/or metadata search from a toggling drawer with the aim of finding and reading text sections. 4 Word search results can be returned in concordance and frequency reports. Any metadata value can serve as a frequency report option, though the most common are word use by title, by date, by author, and even by speaker in collections of plays. More intensive text analysis would require the search form accessible through a web browser.   Functionally, the Android code interacts with PhiloLogic4 databases simply by sending search queries and then displaying the results that PhiloLogic4 sends back over the network. Those results, like all the data the PhiloLogic4 API passes both internally and externally, are formatted as JSON objects. From the search terms the user enters, the app builds a query URI compliant with the PhiloLogic4 API, which includes parameters such as number of results per page and metadata values, as well as a parameter that calls specifically for a JSON object (‘&format=json’). For basic concordance and bibliographic searches, the URI currently points at the main script PhiloLogic4 uses to handle all interaction from web browsers (‘dispatcher.py’). 5 For example, a basic concordance query for ‘sun’ from the Shakespeare app has this URI:   http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/dispatcher.py?report=concordance&q=sun&method=proxy&title=&start=1&end=25&pagenum=25&format=json  For the web version of PhiloLogic4, the dispatcher directs calls for secondary queries, like frequency searches and table of contents requests, to scripts that run behind the scenes. In order to minimize extra coding on the server side, the app calls those CGI scripts directly for these same queries. A frequency by title search for ‘sun’ from the app points to the frequency generation code to get PhiloLogic4’s internal JSON object:  http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/scripts/get_frequency.py?report=concordance &q=sun&method=proxy&title=&frequency_field=title&format=json  The JSON object of a concordance result, for example, can contain chunks of the search result, bibliographic metadata, and a PhiloLogic id used for linking into larger sections of the text. The Android code renders the JSON into a string array and displays search results in a listview. 6 The user can then select individual list items to get larger text sections. The Android code submits a second query to the PhiloLogic4 database for that specific text object which is, again, returned as JSON and also contains navigational links to the previous and next sections of the text. This full-text JSON is rendered for the user to read inside a webview. 7 We use a webview to display text objects in order to apply the same CSS formatting rules that we use for web versions. The PhiloReader also allows users to bookmark text objects for easy access in later sessions by retaining its PhiloLogic object pointer.   At the time of writing this proposal, we have succeeded in developing a functional API and Android code to work with it as proof of concept. Going forward, we will continue to work to make the API as generic and simple to use as possible, and fully RESTful compliant.  Our ultimate goal is to allow other development teams to pick and choose any subset of PhiloLogic4 functionality through the API to build their own interfaces. For instance, developers could integrate concordance search functionality into a traditional desktop app under Windows or MacOS, or into a web environment like Drupal or Django. Developers could also use the API to plug frequency or collocation reports into modern visualization tools like d3.js. And since the API always returns pointers to original text objects, any new interface will be able to have links from results display into fuller document context.  ARTFL chose Android Java as the initial development language for the apps because of existing in-house capability. At the time of the conference, we will make available a skeleton version of our Android code for other groups to adapt or simply examine to see how we interact with the API. In the coming months we intend to have parallel apps developed for iPads, though we are not certain they will be ready in time to present at DH. Nevertheless, ARTFL believes these Android apps demonstrate the ease and flexibility of using the PhiloLogic4 API to develop new ways of interacting with text databases.  Screenshots    Figure 1. Search for term ‘sun’ in  Romeo and Juliet in the Shakespeare database.      Figure 2. Full-text view from  Romeo and Juliet with search term ‘sun’ highlighted.     Figure 3. Frequency by title search results for ‘flay’ in ECCOTCP.    Figure 4. JSON object of frequency by title search results for ‘flay’ in ECCOTCP, http://artflsrv02.uchicago.edu/philologic4/ecco_tcp_demo/scripts/get_frequency.py?report=concor dance&q=flay&method=proxy&title=&author=&frequency_field=title&format=json. Notes 1. At the time of writing, we have built demonstration apps around the ECCOTCP text collection and the MONK project’s Shakespeare’s Plays data. Versions of these apps can be downloaded from http://artflsrv02.uchicago.edu/downloads/app_download/. 2. For general background on PhiloLogic4, see Allen, T., Gladstone, C. and Whaling, R., PhiloLogic4: An Abstract Query TEI System,  Journal of the Text Encoding Initiative, 5 (June 2013). Development is ongoing; code resides at https://github.com/ARTFLProject/ PhiloLogic4.  3. See http://artflproject.uchicago.edu/. 4. See the ‘Screenshots’ section for images of the interface and an example of a PhiloLogic4 JSON object. 5. These calls that the app makes to the dispatcher script are subject to change as we continue to refine the API. Eventually, we might choose to bypass the dispatcher, as we do for frequency and table of contents calls from the app, and instead communicate with CGI scripts exclusively. 6. ListView is Android terminology for a layout that displays a vertically scrollable list. See http://developer.android.com/guide/topics/ui/layout/listview.html. 7. Again, Android terminology for a view that displays web pages. See http://developer.android.com/reference/android/webkit/WebView.html. ",
        "article_title": "PhiloLogic4 And The Android PhiloReader Apps: Toward Building A Full-Featured PhiloLogic API",
        "authors": [
            {
                "given": "Charles M.",
                "family": "Cooney",
                "affiliation": [
                    {
                        "original_name": "ARTFL, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Clovis",
                "family": "Gladstone",
                "affiliation": [
                    {
                        "original_name": "ARTFL, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Walter",
                "family": "Shandruk",
                "affiliation": [
                    {
                        "original_name": "ARTFL, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Morrissey",
                "affiliation": [
                    {
                        "original_name": "ARTFL, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Glenn",
                "family": "Roe",
                "affiliation": [
                    {
                        "original_name": "The Australian National University",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "publishing and delivery systems",
            "project design",
            "interface and user experience design",
            "mobile applications and mobile design",
            "programming",
            "text analysis",
            "English",
            "management",
            "software design and development",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The 1623 Shakespeare Folio may well be the most important single book we have for the study of literature in English. It contains 18 plays that do not survive anywhere else,  Macbeth,  Twelfth Night, and  Julius Caesar among them, and includes what are generally held to be superior texts of 18 plays by Shakespeare that had already been printed, or at least valuable alternative texts for them.   Establishing sound texts for the plays continues to occupy the attention of a large group of scholars worldwide, with new editions appearing every few years. Where multiple varying early texts exist, choices must be made, but this can only be done if we understand the patterns of transmission that gave rise to them—their bibliography. This remains a problem because the plays were printed in a marketplace where copyright was vested in printers rather than authors, and was in any case not well developed, and no one in the chain extending from authors to book buyers was concerned with keeping records for posterity. Work on these problems goes back to the 18th century, and we now know a lot about theatres, companies, writers, printers, booksellers, and book buyers in general, but in the form of general tendencies, not hard-and-fast rules, and rarely enough about particular cases. Here we are compelled to rely on scanty documentary evidence, extended with ingenious speculation, guided by general knowledge of the practices of the day, and on close scrutiny of particular passages.  The paper will introduce a new line of evidence about the bibliography of these plays from computational stylistics. The methods are comparative and cumulative, subject to the checks against bias and the estimates of underlying reliability provided by statistical models. This provides ways of arbitrating between existing explanations, and some entirely new perspectives as well, such as systematically comparing degrees of likeness between versions.  The oldest and perhaps still dominant theory about the place of the Folio in the Shakespeare textual constellation is that the compilers of the volume, John Heminges and Henry Condell—close associates of Shakespeare, and both mentioned in his will—were able to collect fair-copy authorial manuscripts of the plays for their venture. Heminges and Condell in the preface to the Folio contend that earlier versions were nothing but ‘diuerse stolne and surreptitious copies, maimed, and deformed by frauds and stealthes of iniurious impostors’, whereas in their volume, Shakespeare’s works are ‘offer’d to your view cured, and perfect of their limbes; and all the rest, absolute in their numbers as he conceived them’. Many scholars have followed Heminges and Condell’s lead on the illegitimacy of quarto versions of the plays, arguing that these texts derive from actors or audience members reconstructing dialogue from memory or from shorthand notes. Others have proposed that the non-Folio versions are distinct early authorial versions, pirated manuscript copies, acting versions as opposed to the reading versions printed in the Folio, or versions by other dramatists later adapted by Shakespeare for the Folio version. In this paper I focus on two of the Folio plays with earlier printed analogues, applying computational-stylistic methods to test rival theories about the nature of the Folio text and its relationship to earlier versions. In applying computational stylistics to bibliographical questions in Shakespearean drama the study follows a chapter by Arthur F. Kinney (2009) and a book by Lene B. Petersen (2010). Kinney showed that changes between the Folio and Quarto versions of  King Lear in uses of some alternative forms such as  which and  that were persistent and consistent, indicating close and purposeful revision, most likely authorial. This finding supports the most significant proposal in Shakespeare textual studies of the past 50 years, the separation of Quarto and Folio versions of  King Lear into an early and revised state, challenging the earlier view that these were two partial witnesses to a single lost original (Taylor and Warren, 1987). Petersen aims to show patterns in some quartos that reflect memorial reconstruction, such as simplification, repetition, and transposition, but as reviews have pointed out, she fails to account for the inherent variability of her data and only rarely offers results that pass significance tests (Egan et al., 2012).  Henry VI, Part 3  Henry VI, Part 3 is one of the more extreme cases of uncertainty in the Shakespeare canon. We have a prima facie likelihood that Shakespeare was at least one of the authors, given that the play is included in the Folio—titled unequivocally  Mr William Shakespeares Comedies, Histories, & Tragedies—but scholars continue to disagree on internal, stylistic grounds about whether he wrote the whole of it, or if only part, which part, and who were his fellow authors.   As well as the Folio we have an early Octavo edition from 1595, which is different enough from the Folio version to have led some scholars to suggest that it is in fact by a different author, and should be regarded as a source for the Folio rather than an alternative version. Another group of scholars has argued that the Octavo is a memorial reconstruction of the play by actors, whereas the Folio derives from authorial ‘foul papers’ or working drafts. The latest Arden edition casts doubt on both the memorial construction and the foul papers theories, finding that neither of the two texts altogether fits its proposed category (Cox and Rasmussen, 2001, 176).  I will present findings that the 1595 Octavo is by the same author or authors as the 1623 Folio version, but the nature and extent of the differences vary greatly from act to act, with the two versions of Act IV in particular being as far apart as any two versions of an act of any of the Folio-Quarto pairs tested to date. The first two components of a Principal Component Analysis with very common words as variables and a large background set of plays in segments as observations will be used to establish a two-dimensional space as a basis for calculating distances between different versions of the same act.  Hamlet Up to the early 19th century, only one Quarto of  Hamlet was known, the one printed in 1604, but in 1823 a second separate printing was discovered, from 1603, half as long as Q2 (1604) but with a hitherto unknown scene and many alternative readings. The consensus view is that this is a memorial reconstruction of the play, but an older view that Q1 represents a much earlier authorial version of the play is revived in a recent book by Terri Bourus. She argues the case from stage history, printing history, and comparative readings of the various versions.   Computational stylistics offers an opportunity to bring some entirely different evidence to bear. Competing views put the date of composition of Q1 in the late 1580s or the late 1590s. Stylistic analysis is effective in classifying plays by date over this sort of span; the language of the drama changed quickly in this period, consistently and collectively, to the point that plays can be reliably assigned between half-decades separated by a half-decade (Craig, 2013). In later plays, words like  very and  most are suddenly more common, as is the indefinite article, and words like  doth,  hath, and  thou forms retreat. In this paper I will show the results of dating classification tests with function-words data, lexical-words data, and word n-grams.   A late 1580s date for Q1 would undermine the idea that  Hamlet is part of the moment of transformation in Shakespeare’s drama in 1599–1600, which is central to a number of accounts of his development and of the development of early modern culture more generally (Shapiro, 2005; Bloom, 1998; Kermode, 2000). A late 1590s dating would lend weight to the idea that an earlier lost version of the Hamlet story existed but by a different author, possibly Thomas Kyd.   ",
        "article_title": "A Computational Bibliography Of Two Plays From The Shakespeare Folio",
        "authors": [
            {
                "given": "Hugh",
                "family": "Craig",
                "affiliation": [
                    {
                        "original_name": "University of Newcastle, Australia",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "scholarly editing",
            "literary studies",
            "bibliographic methods / textual studies",
            "text analysis",
            "english studies",
            "English",
            "renaissance studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " TEI Simple is a Mellon-funded project developing a customisation and extension of the Guidelines of the Text Encoding Initiative. Its intent is to create a highly constrained and prescriptive subset of the TEI suitable for a straightforward representation of standard early modern and modern books and a processing model documentation for easy web presentation. The project will also develop a mapping to non-TEI ontologies, and a method of indicating the status and richness of a digital text. The outputs from TEI Simple will be integrated into the TEI infrastructure and supported by the TEI Technical Council. The project runs from September 2014 to July 2015. As DH2015 is scheduled near the end of this project, it is an ideal location to both announce the project to the wider DH community while simultaneously reporting on the status of the outputs.  The project is directed by Sebastian Rahtz (University of Oxford), Brian Pytlik Zillig (University of Nebraska-Lincoln), and Martin Mueller (Northwestern University). Other project staff include Magdalena Turska (DiXiT Project / University of Oxford), Lou Burnard (consultant), and James Cummings (University of Oxford). The Advisory Committee consists of Pip Willcox (Bodleian Library, Oxford), Suzanne Haaf (Deutsches Textarchiv, Berlin), Matthias Goebel (University of Gottingen), and James Cummings (University of Oxford).  All of the work for the project is undertaken openly through a public github repository: https://github.com/TEIC/TEISimple.  TEI Simple Objectives  TEI Simple has the following high-level objectives:   1. Definition of a new  highly constrained and  prescriptive subset of the TEI Guidelines suited to the representation of early modern and modern books. The degree of detail supported will be sufficient to encompass, at a minimum, the current practices of the TCP’s EEBO, ECCO, and Evans collections plus those of other major European initiatives such as Text Grid or the German Text Archive (DTA) and the Consortium Cahiers in France.    2. Developing and implementing processing rules for TEI Simple and creating a notation (as an extension to TEI’s ODD metalanguage 1) for specifying such rules, referencing web standards such as XPath, CSS, and XSL FO.    3. Formal mapping of the elements used by TEI Simple to the Conceptual Reference Model of the International Council of Museums (CIDOC CRM), allowing for full interoperability with the Europeana Data Model, in order to facilitate the participation of projects in the Europeana repositories.   4. Definition and implementation of machine-readable descriptions of the encoding status and richness of TEI texts, providing ‘TEI Performance Indicators’, which help to document expectations for the encoded text.   5. Full integration of TEI Simple into the TEI Guidelines and infrastructure with ongoing maintenance by the TEI Technical Council.  More information concerning each of these objectives is detailed below.   TEI Simple Subset  This subset of TEI Simple attempts to remove ambiguity for both encoders and developers. While simple does not necessarily mean ‘small’, nor does it mean ‘simplistic’, the decision was made to base the selection of elements on TEI texts from a set of large archives or text collections. This included  • Text Creation Partnership: Evans, ECCO, EEBO (including the unreleased phase 2).  • Oxford Text Archive: All TEI P5 files.   • Deutsches Textarchiv.  • Documenting the American South.   • CESR.   • OBVIL: corpus critique.  The decision was also made to concentrate primarily on the <text> element, not the metadata stored in the <teiHeader>. The constraining of TEI elements, and limiting of encoding options, meant that the 546 (as of TEI P5 version 2.7.0) elements could be limited to 104 (at time of writing) not including <teiHeader> elements. It is anticipated that the current list of elements may change over the course of development of TEI Simple, but the elements selected were noticed to generally fall into a particular set of categories:  castlist <actor>, <castGroup>, <castItem>, <castList>, <role>, <roleDesc>  character <g>  editorial <abbr>, <add>, <addSpan>, <am>, <choice>, <corr>, <del>, <desc>, <ex>, <expan>, <gap>, <handShift>, <orig>, <reg>, <sic>, <space>, <subst>, <supplied>, <unclear>  interpretation <author>, <date>, <foreign>, <hi>, <measure>, <name>, <num>, <q>, <quote>, <ref>, <rs>, <seg>, <time>  linguistic <c>, <pc>, <s>, <w>  pictures <figDesc>, <figure>, <graphic>  structure <ab>, <address>, <addrLine>, <anchor>, <back>, <body>, <bibl>, <cb>, <cit>, <div>, <floatingText>, <formula>, <front>, <fw>, <group>, <head>, <item>, <l>, <label>, <lb>, <lg>, <list>, <listBibl>, <milestone>, <note>, <p>, <pb>, <sp>, <speaker>, <spGrp>, <stage>, <text> , <TEI>, <teiCorpus>, <title>  table <cell>, <row>, <table>  titlepage <publisher>, <pubPlace>, <docAuthor>, <docDate>, <docEdition>, <docImprint>, <docTitle>, <imprimatur>, <titlePage>, <titlePart> wrapper <argument>, <byline>, <closer>, <dateline>, <epigraph>, <opener>, <postscript>, <salute>, <signed>, <trailer>  We aim to link this simple taxonomy to the distinctions made in Patrick Sahle’s ‘Text Wheel’, and investigate the relationship of this to default behaviour in the Processing Model.         In addition, TEI Simple has customised the available attribute values for a number of attributes. It has removed the @rend and @style attributes, preferring to use @rendition to document the appearance of the original object. In this attribute it uses a closed list of values that its implementations know about in the form of a private URI of ‘simple’: (e.g., ‘simple:bold’).   TEI Simple Processing Model  A ‘cradle to grave’ processing model is at the heart of the TEI Simple project. The TEI Simple processing model offers a bridge across the divide between encoders and developers: the aim is to lower the access barriers to working with TEI-encoded texts in various web environments.  The TEI Simple project has developed a notation by which a TEI profile records the intended processing for documents meeting that profile. The TEI Simple Processing Model notation provides a way to document the intended output rending in the TEI customisation profile (TEI ODD file). This is done by means of a proposed <model> element for use in the context of TEI ODD at a fairly high level and in an abstract manner. The agreement on notation here, though, enables the generation of document-specific transformations. While the implementation of TEI Simple uses an open function library as a method of documenting the further processing in the case of TEI Simple documents, this same processing model notation will be incorporated into the TEI infrastructure where it will be of benefit for those wishing to use a high-level form of output specification, and who wish to develop their own function library.   TEI Simple Mapping to RDF  Although simple presentation in web pages is an important aim of TEI Simple, it is also important to represent structural and semantic markup in the open data interchange format of RDF. The Europeana Data Model (EDM) and the Conceptual Reference Model of the International Council of Museums (CIDOC-CRM) are parts of an evolving ecosystem of metadata standards for cultural heritage documentation across the many languages and cultures of Europe. TEI ODD already has a notation (the <equiv> element) for expressing the relationship between TEI and RDF, and this has been used to map the elements from TEI Simple to CIDOC-CRM and FRBRoo.   TEI Simple Performance Indicators  Although TEI Simple is designed to be very constrained, the decisions as to what to mark up are still left to the encoder. Do they choose, for example, to explicitly identify names of people and places? Will they mark where spelling has been normalized? Will all the words be marked with part of speech information for linguistic analysis? For a simple example, are <name> elements not present in a text because they have not been encoded, or because there are no names in this text? This will affect the  query potential of a corpus of texts, but cannot be determined simply by analyzing the markup. The TEI Simple project has created an extra level of metadata notation for enabling the automatic profiling of a text.    TEI Simple Integration with TEI Infrastructure  The outputs of the TEI Simple project are being fully integrated with the existing TEI infrastructure and thus are available for all TEI users whether using TEI Simple or not. The acceptance of TEI Simple by the TEI Technical Council is one of the success criteria for the funding received. As part of the integration with TEI Infrastructure the TEI Simple project has created a teitosimple XSLT conversion that checks texts conformance to TEI Simple, converts elements where possible, and maps attributes such @rend to known @rendition values.   Further Development of TEI Simple  This paper will conclude with a look at resources built on top of TEI Simple and what work might be developed from it. The TEI Simple project limited its scope to early modern and modern printed books. Under the aegis of the  DiXiT project (an EU Marie Curie ITN) James Cummings and Magdalena Turska have been investigating TEI Simple as a starting point for the creation of scholarly digital editions of simple (but potentially multi-witness) manuscript materials.   Note 1. ODD is an acronym for ‘one document does it all’. The TEI ODD is an example of ‘literate programming’ and combines code and discursive documentation in a single document. ",
        "article_title": "TEI Simple: Power, economy, and a processing model for encoders and developers",
        "authors": [
            {
                "given": "James",
                "family": "Cummings",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Sebastian",
                "family": "Rahtz",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Brian",
                "family": "Pytlik Zillig",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska - Lincoln",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Mueller",
                "affiliation": [
                    {
                        "original_name": "Northwestern University",
                        "normalized_name": "Northwestern University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/000e0be47",
                            "GRID": "grid.16753.36"
                        }
                    }
                ]
            },
            {
                "given": "Magdalena",
                "family": "Turska",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "xml",
            "English",
            "encoding - theory and practice",
            "standards and interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This talk considers one of the ways in which various digital tools can be used in an integrated fashion to map the public reception of new print and drama during the Elizabethan period. Using the final line from Wallace Stevens’ ‘The Snow Man’ as a prompt, the paper extolls the virtue of using digital technology to make new assumptions about human consciousness in historic times and places that were once ‘there’ even if they are currently or ostensibly ‘not there’. This paper also acknowledges the limits of digital reconstruction and its use in interrogating the past. Indeed, finding nothing may mean that there is nothing to be found.  The thesis forwarded here is that the stage plays that were performed in Elizabethan public amphitheatres were inundated with formally crafted echoes, not only of recent print, but also the public reception of recent print. This public reception is most immediately apparent within the St Paul’s precinct in the City of London and would be next to impossible to re-create without the search capabilities of  EEBO-TCP,  ESTC,  Lost Plays Database, and other digital platforms. When reconstructing the relationship between stage performances and the public reception of printed work, we are able to see how a publicly performed play—say, one about the tragic outcomes of Romeo and Juliet—echoed recent public discourse and action in early modern London, discourse and action that was fueled by new print.   In particular, the cultural environment of Paul’s Cross Churchyard, at that time located on the northeast side of St Paul’s cathedral, is ground zero for this public discourse community. This area of the cathedral is lost to history, not there anymore, but with the digital reconstructions of the  Virtual Paul’s Cross Project, it can now be visualized again. The churchyard featured the Paul’s Cross pulpit, and this unique public space echoed with Reformed church sermons from the beginning of the Elizabethan period. Bookshops increasingly flanked this area during the Elizabethan period, making the churchyard the center of the book-selling industry before and during Shakespeare’s time. Controversial and often dramatic sermons from Paul’s Cross echoed religious print held by these enclosing bookshops and, in turn, were at times formed into print editions and sold from the same stores.   Decades ago, it was speculated that Paul’s Cross, with its surrounding bookshops, was a physical prototype for the Elizabethan public amphitheatres that appeared nearby before and during Shakespeare’s time. The challenge of this particular project is to examine another prototype—that is, the prototype of consciousness that developed in the churchyard and that made popular amphitheatre performances possible and indeed helped to sustain the public theatre for a period. The key to drawing the public to the theatre was to stage plays that echoed popular print and public discourse, specifically in the Paul’s Cross Churchyard and in the cathedral nave, Paul’s Walk.  From the records that remain of lost Elizabethan plays before Shakespeare, those plays that are not there but that in some ways are, we can find clues to indicate the relationship between churchyard culture and public theatres. From information that can quickly be accessed at  Lost Plays Database, it is abundantly clear that Elizabethan dramatists before Shakespeare drew from popular stories and themes that were on sale in courtyard bookshops in order to craft plays for the early public amphitheatres. From the 1590s onward, an increasing number of plays were preserved in print, and of course from these editions we can link specific references echoed within amphitheatre performances more acutely to courtyard culture. For instance, through  EEBO-TCP and  ESTC searches, the relationship between churchyard and public stage can be established quickly and with more certainty.  Shakespeare’s  Romeo and Juliet follows from the set formula for drawing dramatic titles from popular print editions in Paul’s Cross Churchyard. Looking more closely at the text, in Shakespeare’s version, Mercutio in particular makes a number of seemingly haphazard references that have kept textual scholars busy and sometimes vexed for ages. With digital searches, Mercutio’s references, for instance, to the names of classical and romantic heroines in lyric poems, the further references to sword fighting and to Tybalt’s continental pretentions, can all be directly linked to print editions held in churchyard bookshops and to the presumed reception of these editions among young gallants even within the church precinct. These links occur in many cases no more than two or three years before the performance of the play, first at the Theatre playhouse and then at the Globe. The there-ness of such relationships become uncertain when we attempt to link Mercutio’s description of Tybalt to what we know about the behavior of young gallants in the St Paul’s precinct, although such considerations are intriguing. Digital searches also tempt us to hear the echoes of Thomas Nashe’s satiric attacks on Robert Green in Mercutio’s character. The attempt to use digital searches and re-creations to determine the extent that Tybalt might be a caricature of an actual person, though, might quickly lead us into the nothing that is really not there.  Current search methodologies and visual re-creations could be supplemented by small digital platforms that mine and input information and that eventually give us the ability to access bookstore and other information quickly. Among several suggestions for a small data digital effort that would not require the enormous amount of data input that burdens larger efforts would be a project that makes a searchable database of the ownerships, leases, and primary holdings of select courtyard bookshops specifically during the 1590s. This type of resource would be enormously helpful to scholars and editors, particularly if the database could be corroborated with references to popular discourse and to stage plays that were performed during or close to that decade.  ",
        "article_title": "‘Nothing That Is Not There and The Nothing That Is’: Tracking the Digital Echoes between Churchyard and Theatre in Shakespeare’s London",
        "authors": [
            {
                "given": "Thomas Winn",
                "family": "Dabbs",
                "affiliation": [
                    {
                        "original_name": "Aoyama Gakuin University, Tokyo, Japan",
                        "normalized_name": "Aoyama Gakuin University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/002rw7y37",
                            "GRID": "grid.252311.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "english studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " There is growing interest in intangible cultural heritage for narrative generation (Peinado and Gervás, 2006), yet with no test collections of tales or myths available, the computational modeling of narratives is still in its infancy (Finlayson et al., 2014). On the other hand, folk narratives are a global phenomenon, amply documented by fieldwork. These on the one hand pin down the major tale types as canonical motif sequences (Uther, 2004), so that storyline variants can be grouped under such types as document classes. Also, there exist lists of indexing criteria, which can be used to describe the content of such narratives on different conceptual levels (Thompson, 1955–1958). Therefore, methods that can accommodate nested compositional structure are required to index such material.  At the same time, information societies increasingly exploit digitized content for knowledge discovery, the scalability of data often leading to unforeseen opportunities for data analytics (Virshup et al., 2013). As there are apparent structural similarities between the recombinative transmission mechanisms of hereditary material in biology vs. cultural memories inherent, e.g., in texts, departing from earlier work on narrative genomics (Darányi et al., 2012; Ofek et al., 2013), and by calling in significant theoretical and development progress in analogical information retrieval and inference in the biomedical domain (Cohen et al., 2012; 2014), here we report on a new integrated methodology for narrative analysis. The idea goes back to literature-based discovery (Swanson, 1986; Cohen et al., 2010), and implements Propp’s formalism to analyze Russian fairy tales as representations of sets of concept-relation-concept triplets, or predications, in high-dimensional space.  Predication-based semantic trawling (PBST) is designed to both aggregate material conformant with specific semantic markup in a knowledge base and to improve the robustness of the same knowledge base by a feedback loop. The idea goes back to the metaphor of a fishing net with variable mesh size, implemented as (1) a knowledge base with a learning component; (2) storing semantically marked-up source material as a test collection, where markup regulates content granularity, i.e., the mesh size; (3) using the knowledge base for information filtering (Brin, 1998; Agichtein and Gravano, 2000) by trawling external data; (4) iteratively expanding the test collection by trawling results; and (5) periodically analyzing incoming data with the results fed back to the learning base.  Our new methodology is designed with scalability, quality, and automation in mind. To this end, we consider the respective integration of available technological steps into a single processing workflow as the missing link to get from little to big data in folk narrative analysis (Malec et al., 2014). These technological steps come from a combination of biomedical text analysis with the Proppian analysis of fairy tales.  In biomedical text analysis, Predication-based Semantic Indexing (PSI) provides the means to efficiently search across tens of millions of concept-relation-concept triplets, known as semantic predications, extracted from the biomedical literature using a Natural Language Processing (NLP) system called SemRep (Rindflesch and Fiszman, 2003). SemRep uses the UMLS 1 and MetaMap 2 to map relevant expressions from free text to concepts in a controlled vocabulary, and extracts relationships between these concepts using underspecified syntactic parsing, a set of indicator rules, and constraints present in the UMLS semantic network. PSI derives high-dimensional vector representations of concepts from the predications they occur in, effectively circumventing the combinatorial explosion of possible pathways between concepts by converting the task of traversing individual predications into the task of measuring the similarity between composite concept vectors. Consequently, search time for single, double, or triple predicate paths is identical once the relevant concept vectors have been constructed. This method can also detect double and triple predicate pathways connecting example pairs of therapeutically related drugs and diseases, and use these inferred pathways to guide search for treatments for other diseases. Further, PSI has been used to mediate semantic search by utilizing high-dimensional vector representations to infer the nature of the relationship between query concepts and other concepts in relevant documents. Inference is accomplished in high-dimensional space using Expansion-by-Analogy, a novel analogical approach to pseudo-relevance feedback, in which the relationships between query concepts and other concepts in documents they occur in guide the query expansion process. The semantic vector–based approaches developed show improvements in performance over a baseline bag-of-concepts model, and these are most pronounced on queries that are not conducive to keyword-based search (Cohen et al., 2014). This approach can be used to create predication-based semantic space for folk narratives.   V. J. Propp’s theory that the canonical form of Russian fairy tales is a compulsory sequence of actions—called  functions and selected from a list of 31 typical activities performed by typical actors—was based on a limited sample of cca 50 fairy tales from the Afanas’ev collection, itself comprising cca 600 stories, selected and compiled in the 19th century (Propp, 1968). Whereas the in-principle applicability of the scheme, with or without modifications, has been extensively debated ever since, researchers have started to look at the reproduction of Propp’s conclusions only recently (Bod et al., 2012). The scheme lends itself to semantic markup (Malec, 2010; Lendvai et al., 2010), with subject-verb-object (SVO) triples underlying Proppian functions suitable for predicate encoding of tale content, an observation first publicly observed among Western readers by Rumelhart (1975).   The following are typical examples of predication from the biomedical informatics domain:   Concept_1  Relation  Concept_2   Mammalian Oviducts LOCATION_OF Sterility  Thymol turbidity test DIAGNOSES Disease  Epididymis LOCATION_OF Obstruction  In comparison, predicates based on Russian fairy tales look like the following:   Concept_1  Relation  Concept_2  Baba Yaga IS_A donor  Golden apple IS_A gift  Baba Yaga LIVES_IN hut on chicken legs  Donor GIVES gift (direct object)  Donor GIVES_TO protagonist (indirect object)  In the Proppian sample corpus, e.g., Baba Yaga gives a magic apple to Ivan Simpleton, who therefore is the protagonist, 3 exemplifying the ‘donor’ function. The following tale segment helps to identify this function (Afanas’ev 96: ‘Morozko/Jack Frost’):   The poor little thing remained there shivering and softly repeating her prayers.  Jack Frost came leaping and jumping and casting glances at the lovely maiden.   ‘Maiden, maiden, I am Jack Frost the Ruby-nosed!’ he said.    ‘Welcome, Jack Frost! God must have sent you to save my sinful soul.’   Jack Frost was about to crack her body and freeze her to death, but he was touched by her wise words, pitied her, and tossed her a fur coat. 4  Here the ‘maiden’, a stepdaughter, is ordered by her stepmother to be left out to the elements in the forest, a plot element representative of Cinderella-like tales (ATU Type 510A) called ‘mat’ padcheritsa’ or ‘Zolushka’ tales in Russian folklore tradition. Jack Frost is clearly the donor. The verb ‘tossed’ is mapped to the ‘GIVES_TO’ relation for its predicate .   Steps in the processing workflow are as follows:   • Apply PoS tagging, 5 anaphora resolution, 6 and semantic role labeling with SemLink 7 (Palmer et al., 2010).    • Normalize concepts and predicates and identify triplets (Rusu et al., 2007).   * Decompose tales sentence by sentence, noting context with respect to Proppian function sequence.   * Shallow parsing of texts using Stanford parser or OpenNLP to identify SVO.   * Extract triples.   • Situate these normalized concept and predicate triplets as metadata within the marked-up corpus as the Proppian SemRep (PSR) output.   * Through a kind of hermeneutic circle, create an analogy of the UMLS in biomedical domain, except with domain knowledge particular to folk narratives, e.g., a controlled vocabulary for predicates (GIVES_TO, ACCEPTS, PUNISHES, etc.), concepts (hero).   * Index predications into PSI-space and index corpus with permuted random indexing (PRI), then visualize results by Epiphanet (Schvaneveldt, 1990) and the query/d3.js interface.   * Extract triples from the Morphology itself to create a knowledge base for purposes of reasoning about the Russian fairy tale domain.  The PSR tool allows for the sequential encoding of marked-up text by circular holographic reduced representation (De Vine and Bruza, 2010), the binary spatter code (Kanerva, 1996), or PRI (Sahlgren et al., 2008) to map them in predication space. For this, different open-source software components are available, prominently the SemanticVectors package. 8 The output of PSR enables the analogical retrieval of predicates, finding missing pieces of evidence to reason about dramatic personae and the plot logic of magic tales.   The core of the tale set for the initial knowledge base are a subset of more than 45 tales in English translation to reflect the subset of Afanas’ev’s tales in Propp’s schema in appendix III. The second sweep will cover a larger subset of the Afanas’ev collection, namely that which Propp himself stated at the outset as his concern 9 before the semantic trawling of web resources may commence.   We have presented here a very rough model and a proof of concept. There will likely be room to address some aspects of the complex issue of the dynamic learning of narrative macro-structures, which tend to be distinctive of particular traditions and populations (Colby, 1973). Finally, our contribution ought to be seen in the light as ‘bootstrapping’ an artificial system to facilitate analogical reasoning about a limited subset of themes, structures, and encoding devices of traditional narrative. Notes 1. http://www.nlm.nih.gov/research/umls/.  2. http://metamap.nlm.nih.gov/. 3. This is a special case where we need an indirect object, Ivan Simpleton.  4. https://github.com/kingfish777/ProppModel/blob/master/000_096_Morozko.xml. 5. http://nlp.stanford.edu/software/lex-parser.shtml. 6. http://cswww.essex.ac.uk/Research/nle/GuiTAR/gtarNew.html. 7. http://verbs.colorado.edu/semlink/. Semlink can create mappings between PropBank, VerbNet, WordNet, and FrameNet. 8. https://code.google.com/p/semanticvectors/.  9. Tales #93-270 from Afanas’ev, inclusive of the tales already mentioned. ",
        "article_title": "Toward Predication-Based Semantic Trawling for Folk Narrative Studies",
        "authors": [
            {
                "given": "Sándor",
                "family": "Darányi",
                "affiliation": [
                    {
                        "original_name": "University of Borås, Sweden, Sweden",
                        "normalized_name": "University of Borås",
                        "country": "Sweden",
                        "identifiers": {
                            "ror": "https://ror.org/01fdxwh83",
                            "GRID": "grid.412442.5"
                        }
                    }
                ]
            },
            {
                "given": "Scott",
                "family": "Malec",
                "affiliation": [
                    {
                        "original_name": "University of Texas School of Biomedical Informatics, Houston, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Trevor",
                "family": "Cohen",
                "affiliation": [
                    {
                        "original_name": "University of Texas School of Biomedical Informatics, Houston, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Dominic",
                "family": "Widdows",
                "affiliation": [
                    {
                        "original_name": "Microsoft Bing, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "encoding - theory and practice",
            "linking and annotation",
            "folklore and oral history",
            "information retrieval",
            "internet / world wide web",
            "semantic analysis",
            "data mining / text mining",
            "ontologies",
            "digitisation",
            "content analysis",
            "corpora and corpus activities",
            "text analysis",
            "English",
            "software design and development",
            "metadata",
            "morphology",
            "resource creation",
            "knowledge representation",
            "and discovery",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " One would think in one of the most advanced industrialized countries in the world, in the state most renowned for technological innovation, an average citizen can easily find out what was said in a public session by his or her elected representative. Surprisingly this is not the case in California in 2014. In the California State Legislature, important legislative decisions on pending bills occur in committee hearings. The videos of these hearings are publicly available, 1 but California is one of the many US states that do not transcribe or close-caption them. The committee hearings contain a treasure trove of information, 2 but it takes extraordinary time and effort to find relevant information in the hours upon hours of raw, untranscribed, and unindexed footage. The ability to access and search transcripts of legislative committee hearings is a great resource for political insiders, media, government watchdog organizations, and interested citizens. It is one way to enable public accountability of elected officials (as well as government officials and lobbyists participating in the hearings) for the things they said at these hearings, and, more importantly, for the effects of the hearings on the state’s legislation.   We present our contribution to government transparency: the Digital Democracy (DD) platform. The goal of the project is to provide automated, inexpensive, timely, accurate, and informative knowledge extracted from legislative hearing sessions.  The Digital Democracy System Design  The Digital Democracy platform obtains data about the legislative committee hearings: the video archives, the information about the state legislature, and so on. Figure 1 shows the design of the DD system. The main source of information for the DD platform is the Cal Channel 3 video archive of legislative sessions, a service provided courtesy of cable TV companies that operate in California. In addition, the DD platform obtains information on the nature of legislative sessions (bills, legislators, committees), political contributions, registered lobbyists, and financial disclosures from a variety of existing online databases provided by the state of California or by good government organizations. The information is stored in the Digital Democracy database (DDDB), from which it is delivered to the end users and used for analytical tasks.     Figure 1. Digital Democracy system architecture.  Access to the information and the knowledge stored in the DD database is provided through a variety of means. The beta version of the DD platform uses a web portal 4 as the means of accessing the data and interacting with the system. Mobile applications and social media plugins, complete with alerting mechanisms, are planned for the future.    Users and Use Cases   We consider three core categories of users of the platform: (a) individual citizens, interested in legislative affairs; (b) media and watchdog organizations observing and covering legislative sessions; and (c) legislators and their staff. For the first category of users, the key use case is the ability to find all committee hearings associated with a specific bill, watch the hearings and read transcripts, and search for the things a specific lawmaker (e.g., representing their district) said in those hearings. The second group of users is interested in searching the database for specific topics and being able to listen to and watch the portions of committee hearings related to the topics of their interest. Finally, the third category of users wants to use the DD platform to find specific moments during committee hearings where important decisions were made or important words were said.  The Beta Site In 2014 we piloted the beta site 5 for the project. The site provided the basic navigation and search functionality for a collection of California legislative committee hearings from the 2013–2014 session. Users could search hearing transcripts, browse hearings and bills discussed at them, and look up information about the hearing participants. Screenshots of the beta site depicting results of the hearing transcript search and a hearing page are shown in Figure 2.   Evaluation We conducted a beta test of the Digital Democracy site from March to July of 2014. The beta testers (public officials, their staffers, journalists, political activists) had access to about 30 committee hearings, primarily devoted to the discussion of California’s state budget. Tables 1, 2, and 3 and Figure 3 summarize the results of the beta test. Conclusion The Digital Democracy platform is a research and reporting tool providing popular access to an aspect of the political discourse that was hitherto not available. We find that Digital Democracy is well received and useful, but has the potential to be much more. As we continue building the new features for our upcoming releases, we take into consideration the additional recommendations and feedback we receive from our beta testers.    Do you feel that the ability to search legislative hearing videos using the Digital Democracy tool would be useful? If you were able to search and clip a video to share on social media, would you find this feature useful? If you were able to run analytics to identify trends in speaker statements, speaker interactions, voting records, and donor data, would you find this feature useful? During the beta test, did you find the Digital Democracy site easy to navigate? During the beta test, did you ever have problems with the video not playing correctly?   YES 27 23 24 23 3   NO 1 6 3 5 23   NO RESPONSE 1 0 2 1 3   Table 1. Tester feedback on usefulness of Digital Democracy features and potential features.       Figure 2. Screenshots of the Digital Democracy beta site.    Searching Keywords in Transcripts Searching Speakers Searching Issues/Topics More Complex Analytical Searches   EXTREMELY USEFUL 22 20 19 13   USEFUL 5 7 6 11   NOT USEFUL 2 2 2 3   NO RESPONSE 0 0 2 2   Table 2. Tester feedback on usefulness of search functionality.    Figure 3. How users would use Digital Democracy data.    In Free-Form Feedback  number example   Thought Very Useful 8  ‘This looks to be an extremely useful tool. I thought the site was easy to navigate and the links to video and text were seamless and really helpful’.    Suggested an Improvement 7 ‘ Creating an alphabet list, at the top of the “browse speakers” feature in addition to the already present features, would allow for ease if searching by speaker’.    Wanted to See More 7 ‘I  think this is a great start for a test. Look forward to seeing it grow with the inclusion of other subject areas (ie. energy, transportation, justice system, etc).’    Had Difficulties 2 ‘ Actually, the site never loaded at all. I could not give any real assessment as a result. But I would very much be interested in this kind of material being available and searchable’.    Wanted to Collaborate 1 ‘ Would be interested in discussing how we integrate our services’.    Table 3. Types of user feedback in free-form responses.  Notes . http://www.calchannel.com/recentarchive/. 2. As evidenced by the firsthand experience of the first author, who served in both the California State Assembly and the California State Senate. 3. http://www.calchannel.com. 4. http://www.digitaldemocracy.org. 5. http://www.digitaldemocracy.org. ",
        "article_title": "Digital Democracy Project: Making Government More Transparent one Video at a Time",
        "authors": [
            {
                "given": "Sam",
                "family": "Blakeslee",
                "affiliation": [
                    {
                        "original_name": "Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Alex",
                "family": "Dekhtyar",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Foaad",
                "family": "Khosmood",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Franz",
                "family": "Kurfess",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Toshihiro",
                "family": "Kuboi",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Hans",
                "family": "Poshcman",
                "affiliation": [
                    {
                        "original_name": "Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Giovanni",
                "family": "Prinzivalli",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Christine",
                "family": "Roberston",
                "affiliation": [
                    {
                        "original_name": "Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Skylar",
                "family": "Durst",
                "affiliation": [
                    {
                        "original_name": "Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "publishing and delivery systems",
            "digitisation",
            "speech processing",
            "interdisciplinary collaboration",
            "video",
            "audio",
            "English",
            "multimedia",
            "resource creation",
            "software design and development",
            "information architecture",
            "and discovery",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This article discusses the methodology used in the Venice Time Machine project (http://vtm.epfl.ch) to reconstruct a historical geographical information system covering the social and urban evolution of Venice over a period of 1,000 years. Given the time span considered, the project used a combination of sources and a specific approach to align heterogeneous historical evidence into a single geographic database. The project is based on a mass digitization project of one of the largest archives in Venice, the  Archivio di Stato. One goal of the project is to build a kind of ‘Google map’ of the past, presenting a hypothetical reconstruction of Venice in 2D and 3D for any year starting from the origins of the city to present-day Venice.  Venice is an extremely relevant case study for such an enterprise for at least two reasons. Its transformations are well documented by a large number of sources (numerous historical maps, huge administrative documents, etc.). Its evolution has essentially been ‘organic’ in the sense that most of the urban transformations have left visible traces, and in many cases the original shape of the city has not changed too much. In this article, we demonstrate how sources are used to reconstruct a dense network of information about Venice covering the 1,000-year period.  The project uses a large variety of sources, as summarized in Table 1.  The Historical Geographical System (HGIS) is initiated using the most up-to-date information about Venice contained in the ITB ( infrastruttura di dati territoriali di base), produced by public authorities (Type A sources in our classification). The quality and the resolution of these initial data are crucial for the success of the project.   Going back in time, the most ancient survey that geometrically measured the ground of the city corresponds to the Napoleonic cadaster (1808–1813) drawn in 1808. Along with the Austrian and the Austrian-Italian cadasters, the Napoleonic cadaster is the most ancient ‘geometric intermediate’ plan that can be used to reconstruct footprints of buildings, streets, channels, wells, public monuments, and other important metadata (census information, property and renting, height, functions and conditions of the buildings) (Noizet et al., 2013). Partial surveys archived at the Archivio di Stato and other cartographic sources can be used in addition to the cadasters (e.g., the ‘Combatti’ map of the 19th century, and the ‘Ughi’ of the 18th century) for the planimetries of monumental buildings (B and C sources). To geo-reference historical maps and align them with contemporary GIS coordinates, one must choose control points homologous between the raster and the vector files (Figure 1). This is a crucial moment of the analysis, a process that needs a significant knowledge of cartographic conventions, both in terms of drawing and historical notions (Gregory et al., 2001). Each cartographic source has a certain number of peculiarities linked with the city history, the 2D architectural description, the hierarchical importance of the drawn elements, and different values ​​of granularity in the map design. Cartographic sources of type  B and C must be constantly crossed with iconic sources of type  D, documenting the city through paintings or engravings, and bibliographical sources of type  F studying Venetian urban history. Combining these sources, we can outline each structural class of elements, reediting different layers for lands, buildings, porticos, bridges, wells, banks, sea sands, public monuments, and so on (Gregory et al., 2001). Good geometric approximation of a diachronic visualization of data until at least the 1729 Ughi map can be achieved using this approach.  For the preceding centuries, some Venetian public institutions have managed to produce very comprehensive, albeit incomplete, documentation of several pieces of the urban structure. All these documents, currently being digitized at the Archivio di Stato di Venezia, provide an indispensable source to perform spatial analysis and introduce other information about every structural element of the city previously chosen. Another ‘temporal nexus’ of particular importance in the history of the Renaissance cartography is the Jacopo de Barbari bird’s-eye view of Venice, published around 1500 (see Balistreri et al., 2000). This source, which we can include as a general iconographic type D source, is at the same time an excellent document from which one could extract a 2D plan and an accurate 3D reconstruction of the city (Figure 2). Thanks to the realignment of the perspective and an architectural analysis of the elements, conducted ad hoc, we can infer and draw a ‘2D de Barbari map’ and proceed to extend the layers of the HGIS up to the year 1500. Each structural unit of the GIS is also an entry to a database. The objects are organized hierarchically based on their scale from the largest entity, the laguna, to the footprint of each structure. A list of sources (from type A to F) can be associated to each element, cataloguing all the evidence that could be used to define its structure and the events associated with it.  Events describe all the major changes that affect the structural unit (e.g., construction, renovation, extension, destruction) but also other historical moments (e.g., consecration, property change). Each event is associated with a (potentially fuzzy) time span.  Particular series of administrative documents (type E) are extremely important to complement the other sources. Venice performed a series of tax campaigns in which all Venetians had to spontaneously describe their property (Condizioni di decima). These tax campaigns provide six additional temporal nexus for the years 1514, 1566, 1582, 1661, 1771, and 1740. For each of these years, they play the role of a ‘snapshot’ of the urban structures for the whole city. The main information concerns the owners and tenants of the structural unit, providing a lot of social information about them (name, religion, citizenship, origins, profession, political role, etc.). Other kinds of information present in the archives (death certificates, testaments, inventories, and other notarial acts) can be used to extend the information contained in these snapshots in order to create dense social networks of the past. Spatial queries can be performed, crossing social data with geographic information—studying, for instance, the urban repartitions of foreigners, the organization of particular professions in specific neighborhoods, and so on.  Precise addresses in Venice are linked with the first cadasters. Earlier, administrative documents provided only names or textual descriptions of the places they mentioned. Luckily, most names of most Venetian places have been rather constant in time. The toponomastic services designed in the database allow the transformation of place names into the corresponding geographical structural unit. In cases where names are unknown or absent and only descriptions are present, the database allows for a fuzzy zoning of the hypothetical designated areas. Between 1000 and 1550, most historical evidence is of this kind.  The diversity, quantity, and accuracy of the Venetian administrative documents are unique in Western history. By combining this mass of information, it is possible to reconstruct large segments of the city’s past: complete biographies, political dynamics, and the urban evolution of most Venetian neighborhoods. The documents are intricately interweaved, telling a much richer story when they are cross-referenced. This precise and detailed reconstruction can only be achieved as a collective endeavor.  Acknowledgements The Venice Time Machine is an international scientific program launched by EPFL and the University Ca’Foscari of Venice with the generous support of the Fondation Lombard Odier. About a hundred researchers and students currently collaborate on this program. We would like to thank in particular Oliver Dalang, Melanie Fournier, and Marc-Antoine Nuessli for their work on the historical geographical information system used in the project.    Type A: Contemporary GIS models (venetian ITB) and 3D scanning data. They constitute the starting point of the modelisation.   Type B: Historical maps covering the whole city, sufficiently accurate to be geo-referenced. The most ancient of such representations is the Napoleonian cadaster published in 1808.   Type C: Partial cartographic elements used as illustrations of particular administrative documents (S.E.A. series of documents from Archivio di Stato di Venezia).   Type D: Iconographic documents such as paintings or engravings of the city.   Type E: Primary sources (e.g., tax declarations) and documents describing textually the evolution of the city   Type F: Secondary sources (e.g. monographies or articles) studying certain aspects of the evolution of the city.   Table 1. Six types of sources used in the project.    Figure 1. Choice of homologous control points.  Figure 2. 3D reconstruction from ‘de Barbari’ bird’s-eye view of Venice, 1500.  ",
        "article_title": "Venice Time Machine: Recreating the density of the past",
        "authors": [
            {
                "given": "Isabella",
                "family": "di Lenardo",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "digitisation - theory and practice",
            "archives",
            "sustainability and preservation",
            "English",
            "renaissance studies",
            "databases & dbms",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Art is immensely complex by its very nature. Conceptual meanings and storied existences make art objects and cultural heritage sites valuable learning tools. Yet the very essence that transforms these precious materials from mere objects to fascinating, priceless cultural heritage artifacts often cannot be understood by present-day viewers through observation alone. Art objects are reliant on external sources to communicate their messages and meanings for them, to reveal their hidden stories to the world. The transformative potential of digital mobile technology as a new means to engage global public audiences with art and cultural heritage sites is changing the nature of how ongoing visitor learning problems are addressed.  Museums are now developing mobile applications to provide more in-depth and interactive learning experiences, which help users generally become more comfortable interacting with art and expand museums’ audiences to include entirely virtual visitors from around the globe. Mobile applications can build meaningful context around art resources using the vast expert knowledge held by museums while allowing users more control over their learning as they develop their own understanding and personal relationships with art (Hartig, 2013; Lewis, 2013).   This paper will investigate how mobile applications designed to reveal hidden meanings of art effectively enhance user learning processes for understanding art. This paper will present the findings from a critical state-of-the-art survey conducted on five illustrative location-generic mobile applications produced by leading international art museums that focus on revealing hidden meanings in art within informal learning contexts that would most likely take place off-site from the museum. State-of-the-art practices, outstanding best-practice examples, as well as current design weaknesses will be discussed. Additionally, this paper will also present a new prototype mobile application about Bulgarian Orthodox art, which was created from a series of requirements that were derived from the state-of-the-art survey results.  Mobile applications are constructivist learning tools by nature because of their focus on contextual learning, making connections and building relationships, and allowing the learner to control their own learning process (Hein 1991; 1999; Ravenscroft in Sharples et al., 2000, 7). Location-generic mobile applications allow users to learn outside traditional space and time constraints and are most effective and broadly appealing in informal, relaxed learning situations (Kukulska-Hulme, 2013).  The adaptability of mobile devices’ abilities to present practically limitless multimedia information from numerous information sources, voices, and perspectives is also transforming learning possibilities (Helal et al., 2013). One could speculate that being able to study works of art on a mobile device anywhere, in private, can have a transformative positive impact on learning through intimately interacting with digitized works of art, especially for audiences unable to view them in person as well as those who may feel intimidated going to museums because they are less comfortable, knowledgeable, or familiar with art.  The surveyed mobile applications reveal the often deceptively complex and subtle nature of the artistic process and enhance visual details of art objects that would otherwise go unnoticed to the untrained eye or are physically inaccessible because they are hidden or no longer exist. Numerous histories and other fascinating tidbits of knowledge are also given new life by being reintroduced to broad audiences.  These objectives are commonly carried out by strategically combining new digital visual content with archival information to illustrate the human and nuanced nature of the myriad stories underpinning works of art. Museum practices are also becoming more transparent through digital methods, which helps raise the public profile of museums as research and educational institutions. Digital technology provides innovative and cost-effective methods for museums to publish research and explain curatorial, art conservation, and other behind-the-scenes practices with broad public audiences and create new ways of learning unlike ever before.  Synthesized results indicate the most pedagogically successful guides presented the content in such a way that the works of art came to life in the mind: art topics became stories; paintings became settings; and artists, subjects, and museum workers became characters in a vivid narrative explored in the diverse content of the application. One could speculate these are the stories that the user will remember long after the digital experience is over. Survey results indicate that when users control their learning environment and are guided to actively uncover hidden layers and meanings of art in an interactive environment, it helps users actively learn about art objects’ individual histories and significance while encouraging users to develop their own personal meanings and relationships with art. Housen (2014) and Yenawine (1997) emphasize these are equally important to objective understandings of art.  Complex spatio-temporal concepts that would be difficult to otherwise explain to general audiences are easily understood through digitally enhanced methods. Interactive design capabilities of mobile devices, including sophisticated native multimedia and multisensory features, allow the user to manipulate, construct, compare, augment, and respond to the environment in a fluid continuum in both physical and virtual space. Survey results indicate that giving the user the ability to watch works of art transmogrify as a direct result of their actions and being able to see them in extremely close detail helps the user better understand the nuanced beauty, the emotional resonance, the personal artistic touches that demonstrate the deceivingly sophisticated nature of these works. These digitally enhanced features uniquely enhance users’ critical visual investigative and comparison abilities, which are critical for constructing meanings of art (Alexander and Goeser, 2013; Housen, 2007; 2014; Isaacson et al., 2011; 2012; Rubino, 2011, 9).  In some cases content created for mobile devices provides opportunities for better user-driven visual analysis and personal study of art objects than what is publicly viewable in the galleries or anywhere else. This makes them universally appealing for everyone, from those only casually interested in art, to students and educators, and even professionals, including curators, conservators, and other researchers.   Results of the state-of-the-art survey suggest the challenging nature of designing mobile applications because of the delicate balance that must be achieved between a number of factors in a dynamic digital environment. Weaknesses in content, design, and navigation in several mobile applications indicate their negative influence on learning and user experience.  * * *  Where Heaven Meets Earth: A Visual Guide for Understanding Bulgarian Orthodox Art is the first known attempt to help new audiences learn about Bulgarian Orthodox art with a mobile application. It exemplifies a practical solution to a common interpretive problem that many museums and historic sites face: providing sufficient didactic and innovative interpretive material to its visitors without interrupting the aesthetic experience on site. It is location generic by design, so it can also be used to prepare or reflect on a site visit or exclusively by virtual visitors who may never have the chance to physically visit a Bulgarian Orthodox heritage site. It was developed based on a set of requirements that were derived from the formative results of the state-of-the-art survey, and it is an example of a low-cost and efficient method of producing an educational mobile application featuring extensible design without advanced technical skills.  According to Avgerinou and Pettersson’s Theory of Visual Literacy, each person’s ability to interpret art is constrained by one’s reasoning skills along with one’s existing knowledge, context clues, and readily available resources, because the visual language, symbols, and prescribed meanings encoded within works of art are culturally specific and therefore not universally understood (2011). Generally speaking, the Christian Eastern Orthodox religion and its artistic traditions, which the Bulgarian Orthodox Church is a part of, are not well understood by audiences outside countries where it is a dominant religion. Many tourists come to Bulgaria without much knowledge of Orthodox art, but are eager to learn more about it. However, most religious sites in Bulgaria do not have didactic and interpretive information available to help visitors understand and interpret the rich visual iconographic language and layers of symbolic biblical and cultural meanings that are hidden within Orthodox art because they are foremost working churches and monasteries. Without didactic information available on-site, many visitors cannot fully engage, understand, and appreciate the artistic and religious value of Bulgarian Orthodox art or its significance in broader Christian, Bulgarian, Balkan, Slavic, world religions, and global cultural heritage.  Where Heaven Meets Earth provides mostly basic explanatory historic, artistic, and identification information for pre-literate Orthodox art audiences, which they can then use to construct more informed objective and personal meanings of Bulgarian Orthodox art (Yenawine, 1997).   The paper will conclude with a list of actionable recommendations collated from research, the survey, and lessons learned during the mobile application development process to help practitioners get started on their first mobile application project.  ",
        "article_title": "A Closer Look: Developing Mobile Applications to Reveal Hidden Meanings of Art",
        "authors": [
            {
                "given": "Laura Kathryn",
                "family": "Downing",
                "affiliation": [
                    {
                        "original_name": "Glasgow School of Art, United Kingdom; Trinity College Dublin",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            },
            {
                "given": "Séamus",
                "family": "Lawless",
                "affiliation": [
                    {
                        "original_name": "Trinity College Dublin",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "libraries",
            "mobile applications and mobile design",
            "archives",
            "museums",
            "English",
            "GLAM: galleries",
            "resource creation",
            "and discovery",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Our paper presents an ongoing project aiming to create a close-to-work  virtual ambience and tо develop interactive research tools for digital humanities (DH), implementing the traditional experience of arts and humanities into a dynamic interface for cultural information retrieval. The approach, discussed in this paper, offers a humanitarian perspective to human-computer interaction and interface design. The project develops the ‘expressive medium’ and ‘exploratory laboratory’ view of DH (Svensson, 2010). Three aspects are highlighted concerning the deployment of an exploratory laboratory as an educational vehicle for expanding the knowledge of information and communications technology (ICT) students in humanitarian areas. The educational aspects include:   1. Complementing essentials of global experience on the ‘big arena’ of real professional interaction where business actions require the mastership of culturally rich ‘mixed mode’ interaction. Global experience here is considered to be  (a) Professional experience to work in a distributed global environment, as well as   (b) Extension of mental models to interact intuitively focused at the ‘big picture’ of real life, in real teams and real markets.  2. Perceived sensitive channels of cross-cultural background impacting interaction, which are the dynamic mechanisms enacting our practical solutions and business procedures.  3. Disclosing the hot spots of a multidisciplinary interconnectedness and applying associatively a methodological sensibility to various historical and cultural key aspects of modern phenomena.  This DH initiative is implemented within the framework of UWS PX (Professional Experience) projects for ICT students. The PX projects create an engaged learning environment in which students address real problems and form collaborative student teams; these teams are provided with initial requirements by a ‘client’; through the interaction with the ‘client’ and participatory agile software development with the ‘client’, students develop a shared concept and implement it as a computing system.  This is the formal vehicle, where an interdisciplinary team of lecturers in the humanities and in information technology from NBU Sofia acts as a ‘client’ for UWS PX student teams during the last four years. Due to geographical distance they communicate and co-create only by using virtual channels of interaction.  The development of a shared concept requires ICT students to focus on a selected variety of sensitive profiles innate to the areas of arts and humanities. They have to immerse into the multilingual reality of southeastern (SE) Europe (6000 BC–AD 1000) where ‘national mindsets’ and spoken languages were not a barrier for communication on the commercial routes. Since 2011 a shared understanding of a multifaceted cultural focus at various times and geographical areas has been developed which is to be accessed on a map/timeline (Duridanov et al., 2013). The paper presents the outcomes of a four-year collaborative work focusing on the development of an integrated associative environment, aiming to educate the so-called digital natives or Millennium users—terms used to denote youngsters who grow up with digital technologies and interact with them intuitively from an early age.  The associative environments that we build up within the series of projects are computer-generated environments based on the idea of human perception and spontaneous associations in everyday life; they are intended for the use of collaborative research and participatory learning. The main managing principle to be highlighted is a ‘spontaneous protocol’ (Duridanov and Zareva, forthcoming), which requires the user to click continuously or hold on a website as a ‘second home’, to change roles within ‘game strategies’ as well as levels of knowledge. The whole procedure of creating associative environments should be considered as an organically developed sensibility. We intend to disclose the ‘hidden mechanisms’ of how digital natives and digital immigrants interact and construct a Web 2.0 ambience on educational ground.  The vision of the project is to build a DH educational environment as an open-source ‘virtual library’ equipped with access and analysis tools. It borrows and extends the idea from the movie  Disclosure (1994) (Duridanov and Simoff, 2007; 2008; Duridanov et al., 2013). The so-called library is the core, which expands gradually into an ‘educational center’, where students have online access and interfere in a noncommercial way, play and develop ‘virtual scenarios’ and ‘associative ambiences’. The center will be hosted by New Bulgarian University Sofia (Duridanov et al., 2013) and will continue to develop the ‘infrastructure’ with further collaborative projects between both universities, with the participation of other research units as well.   The following components of an integrated environment have so far been developed:   (A) A digital library, allocated in a 3D animated ambience (using Second Life), emulates a four-folded access to a ‘systematic catalog’ which appears on two websites as walls of an octagonal room securing linguistic and geographic/historic access (via map with a timeline). The fourth access to religious rituals of everyday life will succeed touching a word (via linguistic access), a location (via geographical map), or a historic moment (on the timeline below the map). The four-folded access is extracted from the methodological works of the late Prof. Ivan Duridanov, where the names of gods, tribes, mountains, rivers, and cities in ancient SE Europe have been proved to be ‘interconnected’ and create an ‘associative environment’ whose ‘windows’ could be opened to enter a past moment of cultural history or location within that moment. The main problem to be solved here was the accessing of multilingually presented names in PDF documents. The implementation of Latin alphabet as a ‘standard language’ on STICKY NOTES had to be replaced by opening the page via accessing a FAVOURITE, because of the too long ‘opening duration’ of a document (Duridanov et al., 2013). Presentation of some of the outcomes can be viewed as follows:   Part 1: http://www.youtube.com/watch?v=7CQx_10wq9  Part 2: http://www.youtube.com/watch?v=rbealR7f1JA  Part 3: http://www.youtube.com/watch?v=IFUaGetbl2o   (B) A geographical map with a timeline presented here is to be accessed via a website with a ‘double window’ to a mind map of a historical moment or a geographical location to disclose visually the above-mentioned ‘associative links’ that at first appear as ‘hidden knowledge’ to the student. The main problem to be solved was here the ‘designing’ of rivers extracted from the scientific documents. Presentation of some of the outcomes can be viewed at http://youtu.be/jf5imrPoZHU and http://youtu.be/k2p7U7YvMPY.  (C) Virtual environment for cultural reenactment. Here were tested two 3D animation programs (Blender, 3D Unity) to approach easily an Orphic ritual of everyday life focused on the initiation of 12 figurines into an ‘inside knowledge’ by a priestess. The Thracian ritual selected by NBU classical scholars highlights a very modern way (it could be said) to access the ‘secrets’ of knowledge applied in ancient times. Therefore the selection of 3D animation appears be an intrinsic part of an agile development of a participatory learning ambience having modern educational value, because it visualizes the ‘hidden point of view’ of a participant as well as that of a visitor. The process and the outcomes of this work will be demonstrated during the presentation.  Methodologically the project follows an agile development strategy to cope with the unpredictability. The methods face three problematic fields:   (a) Innovative ‘infrastructure’ of associatively developed participatory learning in a virtual environment.   (b) Communications between the two project teams proceed only ‘virtually’ and develop a cross-cultural attitude.   (c) ‘Mind navigation’ proceeds intuitively to structure a ‘mental merger’ between the traditionally opposed areas of informatics on the one side and arts and the humanities on the other side. This means a complete revision of familiar semantic notions, i.e., a ‘code breaking’ procedure that has been negotiated explicitly step-by-step for ‘both sides’, not only at the very same beginning. Here is to be stressed the introduction of an interdisciplinary ‘mixed approach’ enacting constraints of arts and humanities (classical studies, linguistics, history, social psychology, and archaeology) into an ICT strategy for students and research fellows of all areas.  The main results are to be conceived on three levels:   • ‘Fusion’ of various segments of disrupted knowledge.   • Development of close to market professional know-how and behavior sensibility.  • Project management milestones developed as integral parts of an integrated Open Source Educational Center for the so-called digital natives.  ",
        "article_title": "Digital Humanities educational vehicle - new patterns of research and learning approach in the Digital Age of Access",
        "authors": [
            {
                "given": "Ludmil",
                "family": "Duridanov",
                "affiliation": [
                    {
                        "original_name": "New Bulgarian University, Bulgaria",
                        "normalized_name": "New Bulgarian University",
                        "country": "Bulgaria",
                        "identifiers": {
                            "ror": "https://ror.org/002qhr126",
                            "GRID": "grid.5507.7"
                        }
                    }
                ]
            },
            {
                "given": "Joanne",
                "family": "Curry",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Stanislav",
                "family": "Ivanov",
                "affiliation": [
                    {
                        "original_name": "New Bulgarian University, Bulgaria",
                        "normalized_name": "New Bulgarian University",
                        "country": "Bulgaria",
                        "identifiers": {
                            "ror": "https://ror.org/002qhr126",
                            "GRID": "grid.5507.7"
                        }
                    }
                ]
            },
            {
                "given": "Simeon",
                "family": "Simoff",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Desislava",
                "family": "Zareva",
                "affiliation": [
                    {
                        "original_name": "New Bulgarian University, Bulgaria",
                        "normalized_name": "New Bulgarian University",
                        "country": "Bulgaria",
                        "identifiers": {
                            "ror": "https://ror.org/002qhr126",
                            "GRID": "grid.5507.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "cultural studies",
            "digital humanities - pedagogy and curriculum",
            "interdisciplinary collaboration",
            "interface and user experience design",
            "virtual and augmented reality",
            "agent modeling and simulation",
            "programming",
            "archaeology",
            "anthropology",
            "English",
            "classical studies",
            "software design and development",
            "digital humanities - facilities",
            "multilingual / multicultural approaches",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the era of large-scale stylometry—which is about to come—some basic but difficult methodological questions have not been answered yet. Certainly, the most important one is whether a given method, reasonably effective for a collection of, say, 100 texts, can be scaled up to assess thousands of texts without any significant side effects. When one deals with historical corpora, however, this question becomes much more complex, since several additional factors have to be taken into consideration. Spelling variation, insufficiently trained NLP models, corpora a priori unbalanced—these are the obvious issues. However, one should also take into account less obvious yet equally important factors that make any stylometric investigations nontrivial, to say the least. These include editorial corrections, punctuation introduced by modern scholars, hidden plagiarism and/or text reuse problems, innumerable authorship issues, and many other sources of potential stylometric ‘noise’ (Eder, 2013).  In the present study, some of the above issues will be undertaken. The benchmark, however, will be focused on one aspect of text classification only, namely authorship attribution. Other stylistic layers—e.g., genre recognition and topic distinction—will be addressed in a follow-up study.  Dataset  The complete collection of  Patrologia Latina that was used in this study, and that will be soon made available in the form of a carefully prepared corpus with morphosyntactic annotation (A. Guerreau, 2014, pers. comm.), gives us a great opportunity to test some of the above assumptions and possible drawbacks of the state-of-the-art stylometric methods. The aforementioned collection consists of 5,821 texts by over 700 authors—the Latin Church Fathers. It covers a time span of over 1,000 years (2nd–13th centuries). Even if the texts represent a few genres, the collection is thematically very consistent: for obvious reasons, theology overwhelms other topics. At the same time, however,  Patrologia Latina is a pre-Internet example of a big-yet-dirty text collection, published in the years 1844–1855; the goal was to publish all the available material in a relatively short period of time, with the assumption that particular volumes would be gradually replaced by carefully prepared critical editions.   The above factors make the electronic collection of  Patrologia Latina a relatively difficult benchmark corpus, close to real-life attribution cases. Since the number of texts to be analyzed is very high, one cannot inspect them manually and/or emend the transcription. Also, one cannot reliably exclude all external quotations, passages copied from other sources, and similar intertextual links; the same applies to any instances of (hidden) mixed authorship. In this case big data means big noise. On the other hand, however, being large and noisy, the collection is a perfect case for massive authorship attribution test, and particularly for exploring the ‘needle in a haystack’ attribution scenario (Koppel et al., 2009). Moreover,  since the texts are supplemented with grammatical codes (lemmata and POS-tags), it gives us a unique opportunity to test the attributive performance of lemmatized vs. nonlemmatized corpus, POS-tag  n-grams vs. MFWs, and so forth. Last but not least, the corpus is supplemented with consistent metadata—the authors’ names are easily retrievable, as well as genre, chronology, and the length of particular texts.   Preselection of Texts  The collection of  Patrologia Latina contains a few dozen texts known to be anonymous, as well as several works of uncertain authorship. For obvious reasons, these texts were excluded from the analysis. What should be emphasized, however, is the fact that the authorship identification of the remaining works is as reliable as the 19th-century scholarship on the Church Fathers. To give an example: the treatise  On the Spectacles ( De spectaculis), for centuries attributed to St. Cyprianus and published under the name Pseudo-Cyprianus in  Patrologia Latina, is nowadays assumed to have been written by Novatianus. In the whole collection of over 5,000 texts, there must be a good share of similar, yet still undiscovered, mismatches.  The corpus was further reduced to contain only those texts that had at least 3,000 tokens, since sample length is known to be a major issue in attribution (Eder, 2014). For the sake of supervised text classification, which requires at least two texts per author to carry out the attribution test, further culling was performed—the authors of fewer than three works have been filtered out. After all the reductions, the corpus consisted of 1,665 works by 197 authors. Since the number of authors is one of the most important factors affecting the performance of attribution methods ( Luyckx and Dealemans, 2011), two smaller corpora have been prepared as well. The first consisted of a randomly chosen subset from the entire  Patrologia Latina (10% of texts) and further abridged to meet the criteria of 3,000 tokens and known authorship (resulting in 81 texts by 15 authors); the second corpus was based on a 5% subset from the original collection, further reduced according to the same rules (22 texts by five authors after culling). The experiments discussed in this paper have been performed on the big corpus, and then replicated using the two smaller subcorpora.   Method of Benchmarking  A standard procedure of supervised classification has been applied. Namely, the input dataset was divided (randomly) into two groups: a ‘training set’ containing two texts per each author, and a ‘test set’ for all the remaining samples. Then the stage of ‘guessing’ was applied, which was aimed to examine if the samples from the second group were correctly linked to their actual authors as represented in the training corpus.  A great number of independent controlled experiments have been performed for every combination of different parameters: three types of style markers (most frequent words, POS-tags, lemmata) in two variants (with and without punctuation), three levels of  n-grams (1-grams, 2-grams, 3-grams), at five levels of culling (0%, 20%, 40%, 60%, 80%), for twenty different vectors of the most frequent features (100, 150, 200, 250, . . . , 1,000), and for six classifiers. The classifiers were as follows: support vector machines, nearest shrunken centroids (Jockers et al., 2008),  k-Nearest Neighbor classifier, naïve Bayes, classic Delta (Burrows, 2002), and a home-brew classifier available in the stylometric package ‘stylo’ (Eder et al., 2013) under the name Eder’s Delta. For each test, 20-fold cross-validation was performed.  To compute such a big number of stylometric tests—the total number of experiments exceeded 30,000—a tailored version of the R package ‘stylo’ was used, supplemented by a few high-performance packages, e.g., ‘multicore’ or ‘ff’. Even still, it took several weeks to complete the task on a 6-processor Xeon machine equipped with 16 Gb of RAM.   Results   The outcome of a vast majority of tests was disappointing: attribution accuracy was unexpectedly poor. First suspicion that the number of almost 200 authorial classes was responsible for this effect turned out to be unfounded when smaller subsets of  Patrologia Latina were scrutinized. The results were obviously better, but still unsatisfactory. For the subset of 81 texts by 15 authors, the optimal attributive success achieved was as high as 50% for the most effective set of input parameters (Figures 1 and 2). This surprisingly weak authorial signal in the corpus of medieval Latin deserves further investigation.      Figure 1. Performance of four methods of classification tested on the Most Frequent Words (grey lines in the background represent ca. 600 remaining combinations of style-markers, n-grams, classifiers, etc.).    Figure 2. Performance of four methods of classification tested on bi-grams of frequent words including punctuation marks. Massive stylometric experiments allowed a systematic comparison of particular parameters’ performance. It turned out that the most accurate classifier was nearest shrunken centroids (especially when applied to long vectors of features), then came support vector machines. Eder’s Delta outperformed the classic Burrowsian measure in almost every instance; both  k-nearest neighbor classifier and naïve Bayes occupied the tail of the ranking. When it comes to the style-markers (features) examined, some counterintuitive results could be observed as well. Frequencies of Most Frequent Words (MFWs)—that is, the type of style markers routinely used for authorship attribution—proved to be suboptimal (Figure 1), while the best performance was achieved using frequent word bi-grams, or combinations of two adjacent words including punctuation (Figure 2). Lemmata and POS-tags followed a similar pattern: they were more accurate when combined into bi-grams including punctuation.   Although it has been shown that punctuation generally increases attribution accuracy (Baayen et al., 2002), this statement must be considered suspicious in the context of ancient/medieval Latin. It is a fact commonly accepted that Latin originally had no punctuation—it was introduced by early modern scholars (Reynolds and Wilson, 2013 [1978]). Apparently, then, it should always be avoided as an authorial style discriminator. On the other hand, however, Latin punctuation is not a randomly scattered artifact, but it inescapably follows the rules of language. Even if artificial, it reveals some linguistic characteristics of analyzed texts, like in the Platonic cave, in which vague shadows reflect some actual phenomena. Interestingly, punctuation was a stronger style discriminator than a more explicit indicator of syntax, such as  n-grams of POS-tags.  ",
        "article_title": "Taking Stylometry to the Limits: Benchmark Study on 5,281 Texts from “Patrologia Latina”",
        "authors": [
            {
                "given": "Maciej",
                "family": "Eder",
                "affiliation": [
                    {
                        "original_name": "Pedagogical University, Krakow, Poland; Institute of Polish Language, Polish Academy of Sciences",
                        "normalized_name": "Pedagogical University",
                        "country": "Mozambique",
                        "identifiers": {
                            "ror": "https://ror.org/0331kj160",
                            "GRID": "grid.442441.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "medieval studies",
            "authorship attribution / authority",
            "literary studies",
            "English",
            "classical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Harpur Critical Archive (HCA) is both a digital archive and a scholarly edition of the poetic works of Charles Harpur. It addresses complex problems of availability that are a direct result of Harpur’s life story. Born in 1813 in colonial NSW only 25 years after European settlement, Harpur was Australia’s first important native-born poet. Although of ex-convict stock he acquired a literary education. His poetry would be deeply in tune with the contemporary taste for wild nature and the sublime. Although he would never become a radical original in his poetry, he was easily able to master 18th-century verse forms and was equally at home with the blank verse of the Romantics, with its flexible imitation of the tones of the speaking voice and the inner rhythms of thought. His mission would be to give poetic expression to the shared colonial experience of a natural antipodean world. He began contributing poems to the Sydney newspapers in 1833. The young man soon proved adept not only at love poems but at satires and squibs usually directed at his natural enemies, the Exclusivists, who separated themselves from those like Harpur with the convict taint. An Australian patriot, he was soon preaching republican, as opposed to monarchist, politics in the vain attempt to secure independence from Britain for the still-young colony. He signed one of his manuscript volumes resonantly: ‘Charles Harpur / An Australian’. He added long prose notes to the newspaper appearances of his poems to prosecute his various agendas. Unluckily for Harpur, the local book-publishing industry would not get onto a royalty-paying footing for literature until the 1890s, more than two decades after his death in 1868. Never wealthy, Harpur could not pay to be published in book form. Newspapers were his only option. From the 1840s, he kept cuttings of his poems as they appeared. In his later years, yearning for a London book publication that would never happen, he copied out, revised, and sequenced his newspaper and manuscript poems into manuscript books. At his death there were 24 such compilations, now in the Mitchell Library in Sydney, as well as nearly 800 newspaper appearances. There were also some pamphlets of his poems; a number were appended to his play  The Bushrangers published in 1853, and a posthumous volume appeared in 1883, a substantial selection organised by his widow but edited by a literary gentleman who adapted and abridged the poems to meet the new tastes of the day.  In total, then, for the period to 1900 we have over 2,800 versions of some 700-odd works, some of which are very long. Capturing the documentary evidence, presenting edited reading texts, annotating them, and collating the variant versions has been recognised as a scholarly necessity since the late 1940s but has never been realised in book or any other form. The HCA aims to make good by seeking simple rather than complicated digital solutions.  The conceptualisation behind the technical design has been crucial. The project has two, linked expressions: archival and editorial. The archival expression aimed to gather and display the basic data: digital images of each manuscript page and newspaper or other printed appearance, and transcriptions of each one. This took about four years. The 5,000 manuscript pages were transcribed in a simplified subset of TEI-XML by master’s students at Jadavpur University in India, specially employed for the job. The transcriptions were painstakingly checked in Melbourne by a specialist research assistant, who also transcribed the newspaper poems. Along with copies of the tiff and jpeg images, the transcriptions are now archived by the Sydney University Library in its D-Space database. It can store files for preservation purposes but never delete or modify them. Done under a signed agreement, this arrangement ensures long-term storage by an institution well set up to provide it and with an interest in doing so. This arrangement is a fail-safe; a Google search will find these files, which by themselves are not user-friendly; but metadata directs the user to our site, charles-harpur.org. That in turn flicks the user to the government-funded NeCTAR cloud-server that actually hosts the HCA. The project has a back end where the files are stored and managed, and where the editor and any registered collaborators work. There is also a public-facing front end, where readers will find a variety of ways of accessing and understanding Harpur’s oeuvre. For the editorial expression of the HCA project, the facsimile images of the manuscripts and ephemeral printed objects are the central resource. Apart from the 5,000 manuscript pages professionally scanned in colour from the originals in the Mitchell Library, we have nearly 800 images from newspapers, using TROVE wherever possible, and images of another 500 pages from book, pamphlet, and broadside printed materials. Harpur’s correspondence is also represented. Readers are thus now in a position to understand the actual material basis of the editorial endeavour that has produced the reading texts that they encounter.  We envisage various categories of reader. Schoolteachers not knowing the titles or first lines in advance but wanting a few poems on rivers, or about the bush or Aborigines, will be able to find them via the subject index. It was semi-automatically generated from the subject terms given to Harpur’s poems indexed for the AustLit database. Local historians from, say, the NSW south coast or the Hunter Valley or the Hawkesbury—all of which Harpur celebrated—can similarly find what they are looking for. Social historians wanting to gather evidence of Harpur’s interventions in colonial debates, or literary critics wishing to follow the gradual unfolding of his poetic career, will have, via a timeline of biographical events and Harpur’s compositions, reliable datings and reading texts for the actual versions written at any particular time—rather than, as previously, being restricted to texts of works of indeterminate date. That traditional aesthetic perspective is now being productively complicated by the historical one at version rather than work level offered by the HCA. The back-end system is based on AustESE, a NeCTAR-funded project for writing tools for scholarly editing, based at the University of Queensland during 2012–2013. Its ontology provides a model of the primary associations between people, material objects, and the concepts commonly used in scholarly editing. The primary classes of the ontology are Artefact, Version, Work, Agent, and Event.  The AustESE WorkBench acts as a repository for transcription files and image files. Each such digital resource then serves as a target for analysis, specifically for annotation and textual commentary. As the user-creator adds metadata, relationships or associations among the digital surrogates are automatically asserted. A text-to-image linking tool allows the user to create links between individual words on the page of a manuscript or printed text and its transcription, mostly automatically. A minimal markup editor allows editing to be performed in an environment that presents together the page image and its transcription, together with a live preview of how the edited text will appear in the final reading text. These three components are scrolled synchronously, so that the editor never loses track of which part of the page-image corresponds to the section of text being edited. There is also an events editor that can deal with fuzzy dates and an editor for creating ordinary HTML documents intended for the scholarly editor’s use in creating annotations and introductory materials.   A Multi Version Documents (MVD) service allows the assembly and textual collation of versions of the same work, thus providing the basic data for the editor’s commentary and text-editing. The TEI-XML files are first separated into text and markup. If those versions contain deletions and additions, they are automatically separated out into layers of alteration, which may also be manually adjusted to show chronologically linked revisions. No traditional textual apparatus display is necessary since the archival recording and listing of editorial emendations that it provided are now taken care of by other means. Simplicity has been our touchstone, together with our belief that the archive does not replace the edition, especially not its provision of reading texts, textual commentary, and other annotation.  Although the users of the system are, in the first instance, the project creator and any assigned collaborators, AustESE allows later users to be granted permissions to add, for example, new annotations, transcriptions, or images. Thus the project need never be closed. One obvious moment to publish is at the point when the archival capture appears to be complete, and the more interpretative editorial stage is about to begin. As the editorial phase progresses, further publication in tranches may be conveniently done. It is likely that Sydney University Press will be the publisher of the HCA, probably in EPUB3 format. These products will likely be sold commercially. Each one will be a subset of the project aimed at a particular audience and responding to a perceived need, even as the website of the whole HCA remains open and free. The press appears content with this arrangement of combining two of the dominant digital media of today in a single scholarly publishing enterprise.  ",
        "article_title": "The Harpur Critical Archive",
        "authors": [
            {
                "given": "Paul R",
                "family": "Eggert",
                "affiliation": [
                    {
                        "original_name": "Loyola University Chicago, University of New South Wales Canberra",
                        "normalized_name": "Loyola University Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04b6x2g63",
                            "GRID": "grid.164971.c"
                        }
                    }
                ]
            },
            {
                "given": "Desmond Allan",
                "family": "Schmidt",
                "affiliation": [
                    {
                        "original_name": "Loyola University Chicago, University of New South Wales Canberra",
                        "normalized_name": "Loyola University Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04b6x2g63",
                            "GRID": "grid.164971.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "publishing and delivery systems",
            "scholarly editing",
            "literary studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Semiotic Framework  Kralemann and Lattmann (2013, 3399–400) claim that models should be understood in the semiotic theory put forward by Peirce, with three main classes of signs: symbols, icons, and indexes. They claim that models are icons because the dominant relation with the original objects they represent is one of similarity. 2 Different shades of iconic similarity between sign and object as theorized by Pierce correspond to three kinds of models in Kralemann and Lattmann: 3   •  Image-like models—for example, photographs where single qualities such as forms and shapes are seen as similar to the original objects.   • R elational or  structural models—that is, diagrams where the ‘interdependence between the structure of the sign and the structure of the object’ (Kralemann and Lattmann, 2013, p. 3408) enables the modeller to make inferences about the original by manipulating its model   •  Metaphor-like models, which represent attributes of the original by a specific quality of their own not recognised by convention or habit.  Models do not act as signs in virtue of themselves. What establishes the model as a sign is the interpretative act of a subject, whether as creator or reader. The practical act of modelling connects the model to its interpretation, that is, to its specific semantic content in a given social and institutional context (Kralemann and Lattmann, 2013, 3402–3). The modeller’s judgement depends on his or her presuppositions connected to ‘theory, language. or cultural practice’ (Kralemann and Lattmann, 2013, 3417). It is evident that Kralemann and Lattmann reiterate the concept of models as middle ground between theory and objects. 4 The relationship of iconicity between the model and the object being modelled is partly externally determined (it relies on the similarity between the model and the object) and partly internal (it depends on theory, conventions, prior knowledge). Based on this duality they stress, on the one hand, the subjectively determined dependency of models on prior knowledge and theory and, on the other, their independence from these in light of the specific conditions of the objects being modelled.     Modelling in Digital Humanities  In this paper we take previous research (Ciula and Eide, 2014; Marras and Ciula, 2014) one step further by mapping Kralemann and Lattmann’s trichotomy of models as icons to digital modelling exercises in historical research. The prototypical cases we chose are a starting point to investigate how such models relate to the cultural objects they represent.     Figure 1. Models representing objects in context. If we accept Kralemann and Lattmann’s argument it follows that by modelling we link models to qualities and relationships already existing in the objects being modelled. Such linking is based on choices that are made for a certain end, informing and motivating the act of modelling. Models are created in actual scholarly situations of production and use and cannot be seen as decontextualised operations. A model is partially arbitrary in that the same inferences drawn by manipulating one model could have been reached in other ways—for instance, using a different model. In this framework, models operate as sign functions. To understand their epistemic role, we need to look at both how they come to be (context) and how the similarity relation with the object is realised. By analysing the association of syntactic attributes of the source object with the attributes of the model we focus indeed on the contextual elements. However, these need to be complemented with the analysis of the specific sign-function in which production and use of models is enacted, as indicated in Figure 1. Three examples will be used to analyse the three types of sign-functions in a DH context:  •  Image-like models. We will use an example from digital palaeography research (Ciula 2005; 2009), where the abstract model letter acts as an image-like model of the samples it was algorithmically generated from. What we can learn about the objects of analysis (the medieval handwritten letterforms) depends on the features being selected in the modelling process. However, the inferential power of the model is mainly based on a strong sensorial similarity between model and object.    •  Relational models. As an example we will use models of landscapes described in historical sources (Eide, 2013). The inferential power of the model relies on the common relational structure between object and model. What new we can know about the object of analysis very much depends on the correspondence between the structuring of the textual expressions in the modelling process and the structure of the map model.    •  Metaphor-like models. 5 We will use the example of network models used to capture information about references to persons in historical sources. These do not only tie specific textual passages to real-world historical entities, they also form parts of networks of co-references (Eide, 2009). The association of a fishing tool (the net) to describe relationships between people is metaphorical. The inferential power of the model leverages on a deep cognitive similarity between the model and the object. It can generate unexpected connections between the objects it represents, which exist ‘only’ metaphorically in a network.     Figure 2. Image-like model. Morphological features of segmented letter forms are modelled into an average morphing letter. Inferences on the manuscript handwriting are based on the analysis of the morphing letter-models in virtue of a ‘sensuous resemblance’.    Figure 3. Relational model. Relational textual expressions are modelled into geometrical relations. Inferences on space as expressed in the text are drawn in virtue of the corresponding spatial structure in the map.    Figure 4. Metaphor-like models. Person names and their relationships as referred to by a document are modelled, respectively, into entities (nodes) and into properties connecting them (links). Assertions of co-reference are also modelled into properties connecting entities. Thus the net is now used to model social relations as well as assertions about people. Common for all three types of models is the inferential power operating at the interplay between their ‘intrinsic structure’ and their ‘extrinsic mapping’ (Kralemann and Lattmann 2013, 3409). Indeed, the features being selected in the modelling process are influenced by contextual elements of different kinds, including hypothesis, scholarly methods and conventions, sample selection, and the technologies being used. However, the epistemic power of the model relies more on extrinsic aspects (sensorial similarity) and less on intrinsic aspects in image-like models, while it depends very much on intrinsic aspects (cognitive similarity) and less so on extrinsic aspects in the case of metaphor-like models (Elleström, 2013).      Conclusion In the paper we will focus on some aspects highlighted in Kralemann and Lattmann’s theory with respect to the role of  context in modelling acts and the nature of  representational relation between objects and models through practical examples. We believe that these two foci are where modelling practices in DH meet with this semiotic framework in productive ways.   We will address the questions outlined in the introduction by contextualising this framework with specific examples of image-like, relational, and metaphor-like modelling with respect to source-based digital humanities. Prior knowledge is a sine qua non to create models in the first place and to use them as interpretative tools with respect to the objects they are signs of (Ciula and Eide, 2014). The relationships between modelling processes and interpretative outcomes are neither mechanical nor directly causal; however, the type of similarity on which modelling relies shapes the interpretative affordances of those ‘anchor’ models. Modelling processes bring about investments and burdens with respect to our knowledge of the objects we model. In particular, models as signs relate to the interpretation of those objects in different ways, from the immediacy of visual similarity on the image end of the iconic continuum to the deep cognitive similarity on the metaphorical end.       Notes 1. By source-based modelling we mean the creation and use of models representing some kind of historical sources and cultural artefacts, such as documents and manuscript pages, together with any of their relevant components and their structure. 2. We do not dwell in this paper on the complexities around the cognitive concept of similarity. 3. The distinction between the three types of icons is not meant to be clear-cut. We follow Elleström (2013) in seeing these types as grades of a continuum rather than separate categories. 4. Past research concerned with the experimental or techno-sciences recognise models including computational models as mediators between theory and objects of analysis (e.g., Winsberg, 2003; Morrison, 2009). This is in line with sign-vehicles functioning as mediators between denotational and connotational qualities, between thing and meaning (MacEachren, 2004, p. 246). 5. In Kraleman and Lattman these models are claimed to be based on semiotic similarity, but this appears categorically misleading to us, so we privilege the concept of metaphor taken from Peirce.  ",
        "article_title": "Contextual Modelling in Digital Humanities",
        "authors": [
            {
                "given": "Arianna",
                "family": "Ciula",
                "affiliation": [
                    {
                        "original_name": "University of Roehampton, UK",
                        "normalized_name": "University of Roehampton",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/043071f54",
                            "GRID": "grid.35349.38"
                        }
                    }
                ]
            },
            {
                "given": "Øyvind",
                "family": "Eide",
                "affiliation": [
                    {
                        "original_name": "Universität Passau, Germany",
                        "normalized_name": "University of Passau",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05ydjnb78",
                            "GRID": "grid.11046.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "historical studies",
            "English",
            "semantic analysis"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The term ‘software-based artwork’ refers to art where software is the primary artistic medium.  Although software-based artworks might all be characterised as having code at their core, there is great variation in their constituents and form—ranging from arrays of material and hardware components installed in a physical space, to born-digital works created in 3D engines. Such works form complex systems, exhibiting a range of dependencies on changing hardware, software, interfaces, and technological environments. Software-based artworks may involve proprietary software or bespoke elements coded by the artist, and many exhibit particular behaviours, such as responding to a visitor or searching for keywords on the Internet. Such characteristics raise significant challenges for the long-term preservation of these artworks. This paper presentation will report on the early stages of the development of a documentation-centric approach to these challenges. These new strategies have potentially wider applications in digital preservation and might be applied to other complex digital objects, such as commercial software, video games, and scientific simulations.   Our primary research aim is to address how software-based artworks are to be described and represented for the purposes of preservation, understanding, and access. Up until recently, there has been little consensus regarding preservation strategies or established methodologies for identifying the characteristics or significant properties to be preserved. Appropriate strategies will typically be dependent on the specifics of each artwork; for example, in some cases it may be essential to preserve the code, in others the behaviour may be the crux of the work. There are a diversity of design approaches and implementation platforms used, and thus a variety of factors that can affect their sustainability, making it essential to capture detailed technical information about the components of an artwork and the digital environments in which they are created, curated, and stored. It is also increasingly important to provide a sense of what these works are like even when they are not on display (or for those unable to visit). We examine how to describe and represent a software-based artwork from various perspectives, ranging from that of a broad viewing audience to those involved in making decisions about the conservation and display of an artwork. Foundational in this is the development of a detailed documentation model for  the management of these decisions, based on the preservation ecosystem concept. This approach considers the artwork a living object in a changing environment, shifting the focus of decision-making from the object to its dependencies. Preliminary work in the development of an ecosystem model for software-based artworks will be reported on . This work also draws upon museum practice in relation to cataloguing, and emerging standards of practice within conservation for creating technical documentation. As well as identifying the technical information that could or should be included in a hypothetical catalogue entry, we examine whether the nature and content of current conservation-focused technical studies need to be rethought. Part of this work will involve the creation of detailed technical entries for a number of software-based artwork case studies.  The particular artworks to be examined include a broad range of different types of software-based artwork, including those in the Tate collection and a number of Internet-disseminated works.   Closely linked to the work outlined above is a secondary research aim: an exploration of what might constitute technical art history for software-based artworks. Technical art history is an evolving field that focuses on the material choices of the artist, the processes involved in creation, and the relationship of these to the meaning, history, and context of the work. Research in this field has up until now been largely framed around artworks that use traditional artistic materials such as paint (e.g., analysis of chemical composition), and so we examine the potential of digital analytical techniques to gather analogous information for software-based artworks.  In addressing both of the aforementioned research aims, this research draws on a number of fields. Existing techniques from areas such as digital forensics and software engineering provide ways by which software can be analysed and documented for purposes of appraisal and preservation. Particular avenues of interest include  automated metadata generation, runtime and network analysis, disk imaging and analysis, and the use of software decompilers. An initial exploration into the value of analytical tools that enable the behaviour and live environment of software to be captured and described will be reported on.   In digital preservation this research shares a number of synergies with areas of current interest to the community, such as analytical approaches to describing complex digital objects and the modelling of technological and semantic change. This research represents the first detailed technical study of software-based art to be based on the use of such analytical methods applied to the software, systems, and media on which they were developed and presented. The knowledge created by this research feeds directly into the refinement of the conservation, risk-assessment, and cataloguing methods used to support Tate’s current and future collection of software-based art, and will potentially be of great interest to many other institutions and individuals collecting similar kinds of work. It also represents a significant contribution (in both theoretical and practical terms) to research in digital preservation in a cultural heritage context, and is likely to be of interest to many other areas of research within the digital humanities. This work is the result of a Collaborative Doctoral Partnership between King’s College London and Tate, funded by the AHRC, and builds in part on work by research teams from both institutions engaged in the four-year EU-funded research project PERICLES (2013–2017).  ",
        "article_title": "Technical Narratives: Novel Approaches to the Analysis and Technical Description of Software-Based Art",
        "authors": [
            {
                "given": "Tom",
                "family": "Ensom",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Humanities, King's College London; Collection Care Research, Tate",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Hedges",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Humanities, King's College London",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": "Pip",
                "family": "Laurenson",
                "affiliation": [
                    {
                        "original_name": "Collection Care Research, Tate",
                        "normalized_name": "Tate",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04s55st49",
                            "GRID": "grid.422817.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "repositories",
            "libraries",
            "archives",
            "museums",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Current models of interoperability discussed in the digital humanities are premised on standardization (Grassi et al., 2013; Doerr, 2003). Working toward this level of interoperability is a praiseworthy objective that requires enormous amounts of effort. However, the problem of standardization is that it does not reflect the multiplicity of perspectives that characterize humanistic practice throughout the world. In other words, standardization often supports a hegemonic view of the humanities. Addressing this limitation is a pressing challenge for the digital humanities, which have been criticized for their lack of linguistic diversity (Galina, 2014) and for not representing oppositional, non-hegemonic views of culture (Koh and Risam, 2013; McPherson, 2012).  This paper suggests that the existing technology stack of semantic web applications (such as OWL ontologies and SPARQL endpoints) can be used to develop a non-hegemonic approach to interoperability in digital humanities projects. The paper focuses on theatre and performance studies but presents conclusions that are applicable to other areas of humanistic inquiry.  Shared Conceptualizations? An ontology is often defined as a ‘formal, explicit specification of a shared conceptualization’ (Gruber, 1993). The ideal of a shared conceptualization is well suited to areas of science where collective efforts are aimed at constructing universal knowledge models. This explains the rapid adoption of ontologies such as the Gene Ontology Consortium (GOC), since ‘much of biology works by applying prior knowledge (“what is known”) to an unknown entity’ (Stevens et al., 2000).  This teleological epistemology might be ill-fitted to describe the work of scholars in the humanities. The humanities are historical and interpretive, and newer conceptual models do not disprove previous ones. Previous knowledge is not aggregated and falsified. Rather, knowledge production is the result of interpretation and disagreement. Coordinated efforts to find shared vocabularies in the humanities are laudable efforts. However, they do not take into account the atmosphere of ‘sophisticated disagreement’ (Strine et al., 1991) that is intrinsic to many aspects of humanistic practice. In the humanities, concepts are often ‘essentially contested’, as argued by W. B. Gallie (1964).  The essentially contested nature of concepts in the humanities is not just a methodological fine point. A multi-vocal debate in the humanities is a necessary recognition of non-normative subject positions. The progressive recognition of alternative views has been one of the main developments in the study of the humanities. As argued by Edward Said (2004), this is probably one of the main contributions from the humanities to the current world.  Failed Inclusivity in Theatre and Performance Studies This paper aims to suggest ways in which semantic web technologies can be used in the development of a global, inclusive dialogue in the field of theatre and performance studies. In order to trace the contours of a truly inclusive digital scholarship in these areas, it is important to consider the criticism launched against previous attempts to create an international dialogue in theatre and performance.  In the 1960s and 1970s intercultural theatre was presented as a creative methodology that had the potential to include and represent multiple perspectives on theatre. However, critics such as Rustom Bharucha (1993) have identified a contradiction in intercultural discourse. Although it aims to represent a set of varied perspectives, it is still premised on an essentially Western view of the world. Likewise, Paul Rae (2011) shows that most academic discourse on intercultural theatre is conducted in English and that few Western interculturalists are multilingual.  Another example comes from performance studies and the idea of a ‘broad-spectrum approach’ (Schechner, 1988). The broad-spectrum approach aims to consider a multiplicity of practices (such as theatre, rituals, political protests, and performance art) as part of a continuum rather than as distinct categories. However, this approach betrays an Anglophone perspective. When translated into other languages, the term “performance” does not cover the same semantic space and cannot be applied to the same range of practices considered in the spectrum (Taylor, 2003).  The previous examples demonstrate how interculturalism and the broad-spectrum approach can be criticized for their lack of cultural and linguistic diversity. However, this argument can also be can be extended to other non-hegemonic perspectives not always represented in official histories and conceptualizations of theatre and performance. How to work towards this goal and avoid the pitfalls of these previous attempts?   An ‘Essentially Contested’ Ontology?  This paper suggests that semantic web technologies can offer a possible way of ensuring a multi-vocal dialogue in theatre and performance studies. Instead of setting up standards and controlled vocabularies, the semantic web stack can be used to make data available from a multiplicity of sources (for example, with SPARQL endpoints).  The Web Ontology Language (OWL) can be used to specify different (even contrasting) models of intercultural theatre and different versions of a broad-spectrum classification. Perhaps different scholars and institutions can make their data and schema available by setting up independent SPARQL endpoints. Rather than trying to collapse or merge these ontologies, federated data models can be used to show a multiplicity of perspectives (cultural or otherwise).  Creating ontologies and linked data is becoming increasingly easy with the use of free software such as Stanford University’s Protegé and the Open Link Virtuoso Universal Server. Nevertheless, this solution has its problems. Scholars would require technical knowhow and server space. However, the entry bar for this public, global debate is perhaps much lower than the one set by the publishers regulating the previous exchanges on interculturalism and on broad-spectrum approaches to the study of performance.  Following Lins Ribeiro’s notion of  non-hegemonic globalization (2009), I would like to propose the concept of a non-hegemonic interoperability that would still use the semantic web stack. As Lins Ribeiro notes, non-hegemonic globalization movements also make use of established networks—for example, environmental NGOs that benefit from the same global communications network as the official institutions that support a capitalist globalization. Likewise, free or open-source tools developed for the semantic web applications can be used to foster an global, inclusive dialogue on theatre and performance research.   ",
        "article_title": "Non-hegemonic Interoperability: Towards a Global Conversation in Digital Performance Research",
        "authors": [
            {
                "given": "Miguel",
                "family": "Escobar Varela",
                "affiliation": [
                    {
                        "original_name": "National University of Singapore, Singapore",
                        "normalized_name": "National University of Singapore",
                        "country": "Singapore",
                        "identifiers": {
                            "ror": "https://ror.org/01tgyzw49",
                            "GRID": "grid.4280.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "knowledge representation",
            "multilingual / multicultural approaches",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Factors adversely affecting the sustainability of digital collections have multiplied during the last decade. Many early implementations were inextricably connected with online projects, making them vulnerable to rapid technical evolution and ensuing obsolescence. There has been considerable research effort directed at decoupling Internet presentation from collection management and at exploiting network-distributed and virtualized collection deployment. Further, contemporary collection ecosystems have adopted internationally recognized metadata standards to gain a level of independence from deployment technologies. However, these trends have only shifted the focus of concern to the stability of such standards themselves: little research attention has focused on the portability of metadata infrastructure, which continue to evolve rapidly and grow in complexity. The proliferation of metadata definition activities and frequent revision of standards has made it difficult to develop implementation roadmaps, and standards has also become tainted by the competing interests of territorial industry associations and government agencies. Additionally, research funding by collections remains committed predominantly to new metadata development work, with only marginal consideration of long-term migration as standards evolve. A related and potentially more serious problem arises from the growing recognition that  annotation metadata must be structured independently of  object metadata. As early as 1992 Bearman observed, ‘Descriptive standards for libraries and archives have historically been much more thorough in specifying how to describe the things within a collection than in specifying how to describe the context surrounding those things’. By 2009 the Open Annotation Collaboration was more specific, claiming, ‘The importance of annotating as a scholarly practice [and] limitations of existing tools supporting annotation of digital content has had a retarding effect on the growth of digital scholarship’. 1 While progress has been made in sustainability of collection deployments and of object metadata, significant benefits and long-term effectiveness of annotation metadata—set out comprehensively in Lee’s ‘Framework for Contextual Information in Digital Collections’ paper (2011)—remain on the horizon. Even if standards-based metadata elements have been employed in creating a digital collection, there is little alternative today but to incorporate annotation information ad hoc into the ‘miscellaneous’ mechanisms provided by object metadata containers. This practice leads to both difficulty of use of annotation in research projects and the frequent loss of investment in annotation when collections must be migrated to new deployment technologies or metadata standards. The Open Annotation Data Model (OADM) 2 Phase III US roll-out commenced in April 2013, 3 with Europe following a few months later, and OADM remains the leading activity addressing the ‘creation, capture and preservation of contextual information’. However, OADM is natively RDF-based, 4 and its effective integration with predominately XML-based collection ecosystems is currently challenging. Only after development of annotation tools that overcome this hurdle can sustainable metadata standards enter widespread use that address annotation as well as object metadata in a coherent manner. Significantly, such a breakthrough will itself create pressure for re-implementation of existing digital collection metadata solutions. Effective long-term accessibility of annotation metadata, even by current standards-based schemes, will require redistribution of annotation in existing collections and its encoding using structured annotation methodologies such as OADM. In turn, this will influence the future development of standards for object metadata encoding.   Gains in both accessibility and sustainability made through adoption of metadata standards are therefore contingent, for a multiplicity of reasons, on periodic re-implementation of collection metadata infrastructures as standards evolve. Strategies requiring human intervention on a per-asset basis to achieve this rapidly become untenable in large collections—even in the absence of funding restrictions. Developing alternate strategies to enable collections repeatedly to migrate metadata infrastructures is critical, not only to remain compliant with emerging standards, but also to gain the real functionality benefits sought from structured annotation by Bearman and Lee and, ultimately, for long-term sustainability. This paper presents progress with techniques for automating the transformation of metadata in response to evolution of standards—thereby making such migration possible.   Our approach arose in collaboration between the University of Westminster (UOW), the University of Heidelberg (UHEI), and the Institut d’Asie Orientale CNRS-Lyon (IAO-CNRS) on managing collections of contemporary Chinese imagery. In particular, after developing software migration tools for reclaiming existing digital collections at UOW that had become difficult to maintain because of obsolescence of support technologies, we elected to create new collections compliant with the VRA Core4 standard. 5 This was achieved by building a mapping-driven exporter for our NoSQL migration platform ( freizo), 6 which generated VRA asset-by-asset. In this way, the results of the transformation could be viewed using the collection management interface and the exporter mapping updated to tune the VRA profile that was implemented—providing an iterative transformation cycle that, when approved, could be applied to the complete legacy collection. The resulting test collection—of Mao-era posters—was transferred to the UHEI Tamboti 7 collection ecosystem to validate its VRA implementation. We report subsequent creation of a VRA-compliant digital collection combining the assets of both the UOW and UHEI poster collections. During this work, however, we noted the necessity of representing annotation (relatively mature in the case of the UOW collection, which was based on reclamation of an Internet project dating from 2001) within VRA description containers. The transformation approach enabled us to produce multiple versions of the poster collection automatically—implementing different annotation solutions in order to support research into linking both annotation tools and virtual research instruments with the collection. Consideration of potential response to VRA Core4 being superseded by future versions was also evaluated in terms of revision of the transformation mapping.   Parallel work in collaboration with IAO-CNRS, concerning historic photographs of Shanghai—part of the large-scale Virtual Cities 8 project initiated by IAO—had highlighted similar concerns about sustainability of annotation using VRA description mechanisms. IAO has developed a range of innovative virtual research instruments able to exploit extensive geo-location information, which forms part of the annotation of this collection. Creating a VRA version of the collection using  freizo transformation created potential vulnerabilities when interfacing the virtual research tools to the VRA collection; such tools might cease to operate correctly in the event of VRA Core4 being superseded were geo-location information to be encoded in miscellaneous VRA description containers. In contrast, we report results with an OADM-based annotation solution implemented using IIIF annotation extensions generated through an alternate  freizo transformation of the collection.  Notes 1. http://www.openannotation.org/about.html (accessed 23 February 2015). 2.  http://www.openannotation.org/spec/core/ (accessed 23 February 2015).  3.  http://www.openannotation.org/phaseIII.html (accessed 23 February 2015).  4.  http://www.w3.org/RDF/ (accessed 23 February 2015).  5.  http://chinaposters.westminster.ac.uk/zenphoto/ (accessed 23 February 2015).  6. http://www.data-futures.org/freizo.html (accessed 23 February 2015). 7.  http://www.asia-europe.uni-heidelberg.de/de/hra-portal.html  (accessed 23 February 2015).  8.  http://www.virtualshanghai.net/Photos/Albums (accessed 23 February 2015).  ",
        "article_title": "Improving Compliance With Evolving Standards Using Computed Transformation of Digital Collections",
        "authors": [
            {
                "given": "Cornwell",
                "family": "Peter",
                "affiliation": [
                    {
                        "original_name": "University of Westminster, United Kingdom",
                        "normalized_name": "University of Westminster",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04ycpbx82",
                            "GRID": "grid.12896.34"
                        }
                    }
                ]
            },
            {
                "given": "Dan",
                "family": "Granville",
                "affiliation": [
                    {
                        "original_name": "Data Futures Ltd., United Kingdom",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alexandra",
                "family": "Eveleigh",
                "affiliation": [
                    {
                        "original_name": "University of Westminster, United Kingdom",
                        "normalized_name": "University of Westminster",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04ycpbx82",
                            "GRID": "grid.12896.34"
                        }
                    }
                ]
            },
            {
                "given": "Eric",
                "family": "Decker",
                "affiliation": [
                    {
                        "original_name": "Heidelberg University, Germany",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            },
            {
                "given": "Christian",
                "family": "Henriot",
                "affiliation": [
                    {
                        "original_name": "University of Lyon, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "xml",
            "metadata",
            "archives",
            "standards and interoperability",
            "sustainability and preservation",
            "English",
            "cultural infrastructure",
            "linking and annotation",
            "asian studies",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The project ‘Digital Network Analysis of Dramatic Texts’ follows in the tradition of structural analysis approaches in the literary studies (Titzmann, 1977) and is aimed at advancing them in recourse to established methods like Social Network Analysis (Wasserman and Faust, 1998). The project also enhances these elder approaches through automated data acquisition and analysis in order to handle much larger corpora and to be able to generate comprehensive relational data to analyse structural transformations in the history of literature. As theoretical foundation we used a network-analytic conceptualisation of dramatic interaction (first ideas in Moretti, 2011; critical reading and theoretical reconceptualisation in Trilcke, 2013; also a detailed research summary). In continuation of concepts of dramatic configuration (Marcus, 1973; Pfister, 1977; Pohlheim, 1997), we resorted to a basic operationalisation according to which an ‘interaction’ takes place if two characters are listed as speakers within a given segment of a text (usually a ‘scene’). For our first automation purposes, we considered ‘interaction’ as ‘scenic co-presence of two speakers’. Based on this concept of relations, network data is extracted automatically—both the global networks of ‘interactions’ of the plays (density, average degree, connectedness, etc.) and data that characterises individual actors (degree and various other centrality indices). The workflow we created also allows the collection of data on a meso-level (e.g., identification of clusters) and includes visualisations of the extracted network data, which in turn contributes to the analysis of the structural transformation in the history of literature. Corpora of Dramatic Texts For the automated analysis of theatre plays, a reliable and sufficiently large (German-language) corpus was needed. The following corpora were reviewed:  • Deutsches Textarchiv / German Text Archive (DTA): 54 dramatic texts.  • Wikisource: 50 dramatic texts.  • Projekt Gutenberg-DE: 641 dramatic texts.  • TextGrid Repository: 690 dramatic texts. The DTA corpus has the best quality of TEI markup, but so far only incorporates few texts. The latter also applies to the German-language branch of Wikisource. The Gutenberg-DE archive is problematic due to the poor markup it provides (just some basic XHTML). Thus, only the TextGrid Repository (containing basic TEI markup) was really applicable. First, we extracted all dramatic texts from the repository, 690 texts altogether that are marked as ‘drama’ in the ‘genre’-field of the metadata. Most of them are German plays from about 1500 to 1930, plus a dozen translations of Greek tragedies and some Shakespeare plays. Acquisition of Network Data As an intermediate step, we created a list of relations between all the persons appearing in a play for each of the 690 TEI files and wrote them into a CSV file, one of the standard formats for the storage of network data. To extract the speaker data, usually two separate steps are required: The recognition of the individual segments of a play and the recognition of individual speakers. To facilitate the following work, the script first splits the files. For each level recognised in the document tree, a subdirectory is created containing all the individual parts of the TEI files, along with the respective index files. Different kinds of outputs are generated this way: for one, a detailed register of all <speaker> tags, but also all the <rs> and <person> tags. To obtain unambiguous reference targets, ID numbers are assigned (also facilitating later interventions, if erroneously assigned names must be manually corrected). Furthermore, co-occurrence lists are created. In the bottom directory, the occurrence of all speaker pairs in all files are counted. In the upper directories, the values of all subdirectories are added. In addition to the recognition of the structure, the correct assignment of names is one of the bigger challenges. Ideally, all <speaker> tags would contain a @who attribute to provide a unique ID for each character. If this is not the case (or if <rs> or <person> tags were used instead), the script has to analyse the textual content of the tag. Possible misspellings (due to the transcription or the original) and grammatical changes have to be considered. For instance, in Lessing,  Nathan the Wise V/1, we encounter different sorts of Mamalukes, be it ‘A Mamaluke’, ‘The Mamaluke’, ‘A second Mamaluke’, or just ‘Second Mamaluke’. In this case, some linguistic knowledge about adjectives is enough to rule out mistakes when assembling the lists of relations. But there are more complicated cases when automatising this process, e.g., if multiple characters speak up at once (‘All’).  In addition to trying to clarify these cases automatically, there is still the possibility of manual intervention in cases of doubt. Here, the generated index files with unique IDs will be of help. For an upcoming version of the script, it is intended to provide a simple GUI to allow easy editing in such cases. Data Analysis and Visualisation The data analysis was done by using the igraph package via Python (3.4.x). It was used for both the visualisation of the graphs and the calculation of the network data. For a first visualization, we fed the graph data into a spring-embedding method (Fruchterman-Reingold), which tries to arrange related nodes closer together (clustering). A first impression of the entire corpus is provided in Figure 1. It comprises nearly 700 plays from 2,500 years of theatre history, starting chronologically at the top left with the ancient Greeks ranging to the bottom right and the second quarter of the 20th century.    The visualised graphs (a ‘distant reading’ of its own) also suggested that most of the calculated CSV had at least minor flaws in them due to ambiguous markup. These findings contributed to the error handling in the previous step (Acquisition of Network Data). Some basic network calculations were done on the basis of the 12 (completed) theatre plays by Gotthold Ephraim Lessing. Corresponding diagrams are show in Figure 2.    Conclusions The extracted (and adjusted) network data will serve as a basis for further statistical calculations and also be made publicly available. Our research focus will now shift to implementing further calculation tools for the network analysis of theatre plays (e.g., to calculate the betweenness centrality to determine the importance of individual characters in a network). We will also work on enhancing the network data (quantify speech units, include stage presence of non-speaking persons, etc.) and try to build multiplex networks that not only capture the ‘interactions’, but also consider parental or instrumental relations. ",
        "article_title": "Digital Network Analysis of Dramatic Texts",
        "authors": [
            {
                "given": "Peer",
                "family": "Trilcke",
                "affiliation": [
                    {
                        "original_name": "University of Göttingen, Germany",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Fischer",
                "affiliation": [
                    {
                        "original_name": "Göttingen Centre for Digital Humanities, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Dario",
                "family": "Kampkaspar",
                "affiliation": [
                    {
                        "original_name": "Herzog August Library Wolfenbüttel, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "literary studies",
            "relationships",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Workflow Our workflow consisted of four steps that were performed in parallel: (1) compilation of a suitable (German-language) corpus; (2) collection of data using the temporal tagger HeidelTime (Strötgen and Gertz, 2012) for the automatic extraction of temporal expressions according to the guidelines of the temporal markup language TimeML (Pustejovsky et al., 2003); (3) data analysis (from heat maps to individual cases); and (4) development of an Android app for exploring the ‘literary year’. Out of the two biggest corpora with German literary texts, the TextGrid Repository 1 and Gutenberg-DE, 2 we chose the latter. We prepared the corpus so it would only contain fiction and ended up with 2,735 works by 549 authors, the majority of which had been published between 1840 and 1930. The resulting 900 MB of text were fed into HeidelTime to extract date specifications. Just using the explicit (and therefore very correct) expressions, we created a calendar heatmap (where ‘1’ means 0–9 occurrences, ‘2’ means 10–19 occurrences, etc., and ‘+’ means 90 or more occurrences; days with more than 50 occurrences are  highlighted). Some expected and unexpected accumulations turned up:  JAN:  +333222323131323223222222222131 FEB: 43322222133212332332212322231 MAR:  73334322233243422433 63232322252 (21 st) APR:  +33233223432223223332322223323 MAY:  +3544332353 64353232424323223244 (12 th) JUN:  733233323333324432343324433233 JUL:  9444332333243 652333432224223223 (14 th) AUG:  83 6442232 72444 63344533332323222 (3 rd, 10 th, 15 th) SEP:  854433233332234233233221222323 OCT:  +353322224223552253432222222133 NOV:  944233333 723225213232222222224 (10 th) DEC: 55223412132232321322333 92122224 (24 th)  As we can see, first days of a month and fixed holidays (New Year, Christmas) have a much higher frequency. But some other days also stick out, an example being the ‘10th of August’. A look into our results files showed that 74% of its occurrences provide a temporal setting for the fictional plot. The above-average frequency of the 10th of August, though, has to do with a historical event, the storming of the Tuileries Palace on 10 August 1792. About 21% of the occurrences are references to this historical date (cf. Georg Büchner’s play  Danton’s Death: ‘ First Citizen: Danton was with us on the 10th August, Danton was with us in September’). Therefore, it is necessary to distinguish between historical dates that left their traces in literary texts and explicit dates that provide a temporal setting of a fictional plot. The collection and analysis of such date accumulations will be systematically expanded, in regard to specific authors, languages, and nations.    Significance for Literary Studies Along the lines of Hans Ulrich Gumbrecht’s study on the year 1926, it would be useful and feasible to assemble the literary history of individual days. Every date has its own literary history, as is indicated by the example of Paul Celan and the ‘20th of January’. In Celan’s prose poem  Conversation in the Mountains (1960), he alludes to Georg Büchner’s short story  Lenz, which also describes a passage through the mountains. Büchner’s text starts with the sentence, ‘The 20th of January, Lenz walked through the mountains’. In  Der Meridian, Celan’s acceptance speech for the Georg Büchner Prize (Germany’s most prestigious literary accolade), he stresses that Lenz’s hike through the mountains takes place on a 20th of January and extends the temporal frame by referring to another 20th of January, the one of 1942, when the Wannsee Conference took place in Berlin. Celan concludes, ‘Perhaps one may say that every poem has its “20th of January” inscribed?’ (cf. Sieber, 2007, 114).  The automatic extraction of date expressions from large corpora makes such simultaneities visible and enables their systematic exploration.   Development of an Android App to Facilitate the Exploration of Date Expressions in World Literature To get an idea of the seasonal cycle of literature, we developed an Android application that works like a calendar and, for each day of the year, presents passages from canonised texts of world literature that take place on that very day. Screenshots are shown in Figure 1. It is well known that James Joyce’s  Ulysses takes place on 16 June 1904. But there is just one inconspicuous mentioning of the day in the novel (the secretary Ms Dunne types it in on the keyboard); it is made visible in the app. Other examples for such passages are 12 June in Günter Grass’ novel  The Tin Drum (birth of Oskar Matzerath’s declared son Kurt), 29 February in Thomas Mann’s  Magic Mountain (where the date is of importance as a special variant of the Walpurgisnacht; see Figure 1), and 27 July in Stefan Zweig’s  Chess Story (on that day, imprisoned protagonist Dr. B. gets hold of the chess book).  Our app thus represents a database of fictional dates that will be available for further research. To be able to map the entire ‘literary year’, though, we will also have to take approximate temporal expressions into account, which we will be attempting in the next section. The Seasonal Cycle of Literature We already mentioned the very specific ratio between exact and approximate dates. In the search for anomalies in the works of individual 19th-century authors, we came across Theodor Fontane and Theodor Storm. A collection of just the month specifications in the fictional works of both authors yielded the results shown in Table 1. In conformity with the popularity of the first of May, the whole month is strongly represented in the narratives of both authors. However, the summer months (of the Northern Hemisphere) are not. Fontane’s narratives seem to especially take place between September and January, Storm’s works between August and November. Given that every month name has a specific tonal-associative character and creates a stylistic effect, both authors seem to favour autumnal/wintry settings and moods.      Conclusion In this abstract, we presented a method to find date accumulations in large literary corpora. We described the relation between approximate and exact dates and introduced a growing database of exact date specifications in world literature. Furthermore, we approached the seasonal cycle of literature and certain authors to try to answer the question, ‘When does [German] literature take place?’ in a macro-analytic fashion. The results obtained to date already show the potential of the ongoing research.   Notes 1. http://www.textgridrep.de/.  2. http://projekt.gutenberg.de/.   ",
        "article_title": "When Does (German) Literature Take Place? – On the Analysis of Temporal Expressions in Large Corpora",
        "authors": [
            {
                "given": "Frank",
                "family": "Fischer",
                "affiliation": [
                    {
                        "original_name": "Göttingen Centre for Digital Humanities, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jannik",
                "family": "Strötgen",
                "affiliation": [
                    {
                        "original_name": "Heidelberg University, Germany",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "german studies",
            "literary studies",
            "corpora and corpus activities",
            "text analysis",
            "information retrieval",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Background and Technical Infrastructure  DHQ’s technical design was constrained by a set of higher-level goals and needs. As an early open-access journal of digital humanities, the journal had an opportunity to participate in the curation of an important segment of the scholarly record in the field. This meant that it was more than usually important that the article data be stored and curated in a manner that would maximize the potential for future reuse. In addition to mandating the use of open standards, this aim also strongly indicated that the data should be represented in a semantically rich format. Of equal concern was the need for flexibility and the ability to experiment with both the underlying data and the publication interface, throughout the life of the journal, without constraint from the publication system. Both of these considerations moved the journal in the direction of XML, which would give us the ability to represent any semantic features of the journal articles we might find necessary for either formatting or subsequent research. It would also permit us to design a journal publication system, using open-source components, that could be closely adapted to the  DHQ data. At the journal’s founding, several alternative publishing platforms were proposed (including the Open Journal System), but none were XML-based and none offered the opportunity for open-ended experimentation that we needed.   DHQ’s technical infrastructure is a standard XML publishing pipeline built using components that are familiar in the digital humanities. Submissions are received and managed through OJS through the copyediting stage, at which point articles are converted to basic TEI using OxGarage (http://www.tei-c.org/oxgarage/). Further encoding and metadata are added by hand, and items from the articles’ bibliographies are entered into a centralized bibliographic system that is also XML-based. All journal content is maintained under version control using Subversion. The journal’s organizational information concerning volumes, issues, and tables of contents is represented in XML using a locally defined schema. The journal uses Cocoon, an XML/XSLT pipelining tool, to process the XML components and generate the user interface.      DHQ’s Evolving Data and Interface  As noted above,  DHQ’s approach to the representation of its article data has from the start been shaped by an emphasis on long-term data curation and a desire to accommodate experimentation. The specific encoding practices have evolved significantly during the journal’s lifetime. The first schema developed for the journal was deliberately homegrown and was designed based on an initial informal survey of article submissions and articles published in other venues. Following this initial period of experimentation and bottom-up schema development, once the schema had settled into a somewhat stable form we expressed it as a TEI customization and did retrospective conversion on the existing data to bring it into conformance with the new schema. At several subsequent points, significant new features have been added to the journal’s encoding: for example, explicit representation of revision sites within articles (for authorial changes that go beyond simple correction of typographical errors), enhancements to the display of images through a gallery feature, and adaptation of the encoding of bibliographic data to a centralized bibliographic management system.  These changes to the data have typically been driven by emerging functional requirements, such as the need to show where an article has been revised or the requirements of the special issue on comics as scholarship. However, they also respond to a broader set of requirements that this data should represent the intellectual contours of scholarship rather than simply interface. For example, the encoding of revision notes retains the text of the original version, identifies the site of the revision, and supports an explanatory note by the author describing the reason for the revision. Although  DHQ’s current display uses this data in a simple manner to permit the reader to read the original or revised version, the data would support more advanced study of revision across the journal. Similarly, although our current display uses the encoding of quoted material and accompanying citations in very straightforward ways, the same data could readily be used to generate a visualization showing most commonly quoted passages, quotations that commonly occur in the same articles, and similar analyses of the research discourse. The underlying data and architecture lend themselves to incremental expansion.    Analysis The approach  DHQ has taken offers several significant advantages and also some corresponding disadvantages. The most important advantages are   • The autonomy the journal has to control all aspects of its own data modeling and interface.  • The high value of the resulting data, from a historiographic perspective.  • The ease of long-term curation of the data, including continuing evolution of our modeling decisions.  • The ease of long-term evolution of the publication infrastructure, including migration to other XML-based systems as needed.  • The scalability of a template-based infrastructure: with the system in place, each article requires no incremental work in styling or design; all effort goes towards consistent representation of semantically valued features. These advantages all carry a burden of cost and effort: autonomy and control necessarily entail responsibility for maintaining appropriate levels of expertise and undertaking the labor necessary to build and revise technical systems. Because our article work flow includes some hand encoding in TEI, our managing editors need to be better trained and more expert than if they were simply formatting articles in Word and exporting PDF. However, there are also some less obvious tradeoffs.  DHQ’s publication model gains its efficiencies and scalability through an emphasis on uniform handling of repeated features, but this means that it is comparatively difficult to accommodate individual authorial requests for special handling. These entail not only extra effort at the time of publication but also the long-term prospect of special attention during the future data curation activities and updates to the interface and publication system. Authors familiar with content management systems such as WordPress or Scalar are accustomed to being able to exercise a significant level of control over the formatting and behavior of their text and accompanying media such as images and video. Long-term data curation is a less visible feature of such publishing systems.   Even more interesting and challenging are the special cases that entail semantically distinctive features. Although such submissions are rare, they have provided some valuable test cases in which the data being represented is not a straightforward ‘article’ but some other rhetorical mode: commented program code, dynamic HTML that provokes reader interaction, an article in the form of a comic book. In handling these cases,  DHQ has sought to find ways to accommodate the distinctive form of the original piece while also giving it a proxy presence within the standard  DHQ XML archive, so that its content can be searched and analyzed as part of the larger  DHQ corpus of DH scholarship. As these cases accumulate, the editors seek to identify repeated needs that could become part of the regular  DHQ feature set.   In the full version of this paper, we will consider in greater detail the role of authorial design in digital humanities publication, and the possible convergences between XML-based systems like  DHQ and content-management based systems like Scalar.    Future Directions  DHQ is now completing a multiyear project to centralize its bibliography, and the next step will be to develop interface features that exploit this data. We are also in the planning stages of a project to explore internationalization of the journal through a series of special issues dedicated to individual languages. In both cases, these amplifications of the journal represent natural extensions of the journal’s existing architecture, and although both are substantial projects, they are made feasible by the investment already made in strongly modeled data and an extensible publication infrastructure. In the fuller version of this paper, we will discuss both of these developments in greater detail.   ",
        "article_title": "Challenges of an XML-based Open-Access Journal: Digital Humanities Quarterly",
        "authors": [
            {
                "given": "Julia",
                "family": "Flanders",
                "affiliation": [
                    {
                        "original_name": "Northeastern University, United States of America",
                        "normalized_name": "Universidad del Noreste",
                        "country": "Mexico",
                        "identifiers": {
                            "ror": "https://ror.org/02ahky613",
                            "GRID": "grid.441462.1"
                        }
                    }
                ]
            },
            {
                "given": "Wendell",
                "family": "Piez",
                "affiliation": [
                    {
                        "original_name": "Piez Consulting Services",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Walsh",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "publishing and delivery systems",
            "licensing",
            "xml",
            "project design",
            "English",
            "and Open Access",
            "standards and interoperability",
            "copyright",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The State Library of NSW is custodian for a suite of extraordinary collections documenting the heritage of Australia and Oceania. In an age where technology has become entrenched in everyday life, the Library is working to transition digitisation of these collections from ‘a special project’ to ‘business as usual’. In this way the Library seeks to ensure collections are more meaningful to more people.  Interrogating Our Collections  Increased access to collections is highlighted here through the lens of the Library’s diaries from World War I. Commemorations of the war have generated significant community enthusiasm for learning about the history of this conflict, particularly the personal stories of Australian servicemen and women. In response to increased interest in the diaries, and associated materials, the Library embarked on a project to transcribe (in 2008) and digitise (in 2013) over 1,100 diaries. An important, resource-intensive, and parallel project was the upgrading of the provenance history of the diaries and a close examination of their copyright status (which is varied). Considerable work was also undertaken to enhance descriptive metadata (including authority records)—the very foundation of subsequent data-sharing projects linking these diaries to other data repositories. This has facilitated significantly easier access to the collection and simultaneously presents opportunities to interrogate this material in new ways, including patterns that might be revealed from a geo-location analysis. There are also options for general crowdsourced tagging and high-level interactions with the diaries, to track individual word usage and assist in unpacking broader themes around grief and loss or post-traumatic stress disorder. Such research will enable the identification of trends that are there—anticipated and unanticipated—but also those trends that we might expect but are missing.  Interrogating Ourselves  Interrogating a collection such as this demands that the Library interrogate its own practices. How do we curate collections and subsequently allow for easy navigation of content in an increasingly digital world? For the State Library of NSW, another key question is how do the neat and polished presentations of digitised material convey the complexity of the Library’s holdings? The World War I diaries, for example, are in sharp contrast to the symmetrical war service records—formatted information on standardised index cards—of those who served. The diaries, which like many collections have their own peculiarities, are very much a suite of personal objects with differences between volumes, including binding type, paper type, handwriting styles, and the inclusion of sentimental objects such as flowers and postcards, while some diaries have been taken apart and mounted into albums. This has generated critical conversations around not only which collection should be digitised but also how much detail from each collection needs to be captured (core content only or core and peripheral content?) to meet the needs of researchers and maximise the potential of the digital environment. Additionally, the Library needs to ask how it structures meaningful relationships to avoid being simply a cipher of content. How do we manage the risk of privileging certain kinds of content? In a space where datasets are made available with a ‘take it and play’ philosophy, how do we measure interaction with, and scholarly outputs from, materials made available? Indeed, all libraries need to ask questions that support the future of data collection and data use. Decision making needs to be transparent and produce results that align collections with digital humanities researchers; especially those looking to distribute content in a less labour-intensive, mediated way. Conclusion Digitisation is a great equaliser, democratising knowledge, yet this has the potential to merely change repositories from onsite to online. Collecting institutions can be fantastic interpreters of their often unique collections and need to maintain this role. They also need to be prepared to share, as research projects can be enhanced through the combination of new data sources such as transcriptions and geo-coding and traditional metadata, all of which have resource implications. The authors argue that the questions asked of the World War I materials held by the State Library of NSW can also be asked of some of the other collections for which the Library is a custodian. Moreover, potential to explore these collections within a digital humanities framework needs to be examined, and new opportunities explored: What would happen, for example, if war images were put through facial recognition software? Of particular importance, alongside the creation of new knowledge, is the generation of new ways of storytelling and how traditional research can intersect with creative practice to produce different ways of seeing and understanding.  ",
        "article_title": "Interrogating Our Collections; Interrogating Ourselves",
        "authors": [
            {
                "given": "Louise",
                "family": "Denoon",
                "affiliation": [
                    {
                        "original_name": "State Library of NSW, Australia",
                        "normalized_name": "State Library of New South Wales",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04evq8811",
                            "GRID": "grid.454054.4"
                        }
                    }
                ]
            },
            {
                "given": "Elise",
                "family": "Edmonds",
                "affiliation": [
                    {
                        "original_name": "State Library of NSW, Australia",
                        "normalized_name": "State Library of New South Wales",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04evq8811",
                            "GRID": "grid.454054.4"
                        }
                    }
                ]
            },
            {
                "given": "Rachel",
                "family": "Franks",
                "affiliation": [
                    {
                        "original_name": "State Library of NSW, Australia",
                        "normalized_name": "State Library of New South Wales",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04evq8811",
                            "GRID": "grid.454054.4"
                        }
                    }
                ]
            },
            {
                "given": "Richard",
                "family": "Neville",
                "affiliation": [
                    {
                        "original_name": "State Library of NSW, Australia",
                        "normalized_name": "State Library of New South Wales",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04evq8811",
                            "GRID": "grid.454054.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "libraries",
            "archives",
            "museums",
            "sustainability and preservation",
            "English",
            "GLAM: galleries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital humanities are slowly beginning to take shape in Argentina, hand in hand with the country’s economic growth in the last decade. So far, the most important digital efforts have been undertaken by researchers from the fields of literature and history. But what happens when digital humanities meet art history in the Argentine context? A few digital art history projects have been active since the 2000s, funded by several governmental institutions and private museums. However, most of them are repeating the art historical canon, maintaining a narrative organised around styles and masterpieces, while obliterating the careers of women in art. Gender studies in art history are still thin on the ground in Argentina. In scholarship terms, research focused on women artists is very recent, having begun in the 1990s. There is much to do ahead, fostering the investment of undergraduate students, museums’ staffs, and the public. So what happens when digital humanities meet feminist efforts in art history? This paper aims to discuss and present a digital humanities project that has developed from my doctoral dissertation research. This project will attempt to encourage visitors to contribute information on women artists, helping to destabilise art historical narratives and to encourage the interest in women artists from Argentina. MArEAr (Spanish for ‘making dizzy’) hopes to make widely available the information collected for my doctoral dissertation, which will encompass the careers of women artists from the 1880s to the 1920s. These artists are not featured in survey books and are also absent from many encyclopedias. Their works are scattered in museums and private collections. They are, even today, little-known names not connected to any work visible in museums’ galleries. ",
        "article_title": "Feminism and the digital: towards a database of Argentine women artists",
        "authors": [
            {
                "given": "Georgina Gabriela",
                "family": "Gluzman",
                "affiliation": [
                    {
                        "original_name": "Universidad de San Andrés, Argentine Republic",
                        "normalized_name": "University of San Andrés",
                        "country": "Argentina",
                        "identifiers": {
                            "ror": "https://ror.org/04f7h3b65",
                            "GRID": "grid.441741.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "gender studies",
            "English",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " New computer technologies are rapidly changing the information base of historical research. The most significant progress is observed in historical demography. International projects aimed at creating the Big Data Base (IPUMS-USA, NAPP, Mosaic, Viennese Database on European Family History Church books, Demographic Database Umeå, and others) help establish a new research paradigm that eliminates the national boundaries of research practices and allows conduct of cross-national analysis of family history, birth and mortality rates, nuptiality, and migrations. Accessibility of the Big Data Base, its flexible interface, different modes of search, sampling, and data processing enable researchers to model their own resource with regard to the aims and targets of a specific research project. As a result, not only the information environment of Russian science changes dramatically, but also its methodology and research technologies.  The first experience of creating databases containing mass historical data goes back to the early 1980s. The process was enhanced by the establishment of Association ‘History and Computer’, founded in 1992. The most interesting results were achieved in the sphere of historical demography. In the 1990s–2000s the following large-scale projects were realized: in St. Petersburg State University, a group of historians supervised by Prof. S. G. Kashenko built up a database drawing on the register books of Olonets  guberniya of the 18th to early 20th centuries. Somewhat later, the same source was used by historians led by Prof. V. V. Kanishev, from Tambov State Technical University. Census records ( revizskie skazky) and census records of homesteads ( podvornye opisi) became the object for modelling databases for several  uezds of Penza  guberniya in the 18th and 19th centuries (Research of Historical Demography of the Russian Empire, 2013). Another centre for creating historical demographic databases was Altai State University. This is where the remaining records of the first general Russian population census of 1897 were found. They were used as a foundation for a database that was further expanded by adding the data of parish records.   Therefore, several regional centres gave rise to a new trend in historical demography that can be called ‘Digital Historical Demography’. The peculiar feature of this trend is that digital resources are based on primary historical sources. All projects were focused on specific locations—that is, they researched specific settlements,  uezds of Russia in the 18th and 19th centuries. Since the databases were developed for specific research projects, they were not made widely accessible to historians, except for the information and reference system created under the supervision of Prof. V. N. Vladimirov. It was called Historical Occupation Studies and included the database Population of Barnaul in the 19th to Early 20th Centuries, based on register books, consolidated records of the first general Russian population census of 1897, and other large-scale resources. Among the most interesting recent projects is Database of Demographic Indices in the Regions of Russia and Other Countries, which has been developed by the National Research University Higher School of Economics since 2011.   In 2014 the research laboratory International Centre of Demographic Studies of Ural Federal University started developing its network resource, Ural Historical Digital Archive. The project’s main target is to create a multipurpose digital resource, based on archival materials. The resource is to take the form of a database and full-text selection of digitized library and archival documents, maps, photographs, and image documents (e-archive) on regional history. The first stage (2014–2015) includes building up the following databases: (1) based on register books of several parishes in Ekaterinburg Diocese of 1800–1918; (2) based on All-Russia Census of Members of the Russian Communist Party of 1922–1924; (3) database All-Soviet Census of 1959, Sverdlovsk Region; (4) database Population of Cities and Rural Settlements of the Middle Urals in the 19th to 21s Centuries; and (5) database Subpolar Census of 1926/27. At the next stage (2016–2017) it is planned to extend the archive by digitizing and providing archeographic description of  revizskie skazky of Perm  guberniya—census records of homesteads and books.   The project developers have chosen to abide to the following principles:   1. Orientation towards primary large-scale historical sources (population census records, register books, and others), containing personal information.  2. All-inclusive coverage of all existing documents and records.  3. Application of the data format that provides storage, import, and integration of diverse resources and their collective use  4. Openness and accessibility of the resource for the international academic community.  Taking into account the fact that the documents stored in Russian archives—for example, original census report forms—can be incomplete and fragmentary, it is necessary to formalize and digitize all the preserved diverse document complexes on the Ural history of the 18th, 19th, and early 20th centuries.  As practice shows, it is possible to overcome scientific isolation of researchers spread across the world. Historian-demographers from a number of countries have been successfully realizing the European Historical Population Samples Network (EHPS-Net) project, aimed at developing general-format Intermediate Data Structure (IDS) for over 20 demographic databases. If the Ural Historical Digital Archive gets included into the EHPS-Net environment, it will become possible to compare the Russian demographic processes with European and world ones, and determine their peculiarities and future prospects. Integration with the IDS will enable Russian researchers to study the natural population change as well as distinctive features of ethno-confessional communities. ",
        "article_title": "Historical And Demographic Database: Russian Experience And Prospects",
        "authors": [
            {
                "given": "Liudmila",
                "family": "Mazur",
                "affiliation": [
                    {
                        "original_name": "Ural Federal University, Russian Federation",
                        "normalized_name": "Ural Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/00hs7dr46",
                            "GRID": "grid.412761.7"
                        }
                    }
                ]
            },
            {
                "given": "Oleg",
                "family": "Gorbachev",
                "affiliation": [
                    {
                        "original_name": "Ural Federal University, Russian Federation",
                        "normalized_name": "Ural Federal University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/00hs7dr46",
                            "GRID": "grid.412761.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "historical studies",
            "English",
            "resource creation",
            "and discovery",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We have been conducting research on digitalization methods for the restoration of Shoso-in Monjo since 2002. Shoso-in Monjo is a collection of historical documents from 8th-century Japan, comprising approximately 15,000 documents in total. 1 It is the largest group of historical documents from ancient Japan or East Asia, and is highly valuable data. However, because of the historical background of Shoso-in Monjo, the documents have become disorganized compared to their original, 8th-century state. A “restoration” to this original state is necessary in order to conduct research using Shoso-in Monjo. To resolve this issue using computers, we reconstructed a virtual Shoso-in Monjo and created a comprehensive, all-encompassing database of the scattered Shoso-in Monjo information. We called this database the Shoso-in Monjo Database: ‘SOMODA’. 2  We conducted a markup of the Shoso-in Monjo textual data using XML (eXtensible Markup Language), thus structuring the database, and linked image data and index information. Thereby, we created a system through which any information from the Shoso-in Monjo can be acquired.    Figure 1. The main page of SOMODA. SOMODA does not just display complete texts or images; it is a system containing various information derived from data that was restored, and a system through which this information can be viewed simultaneously. We developed the latter database in 2007. After considering how SOMODA ought to be developed from now on—that is, approximately seven years after the database was first created—we concluded that it is necessary to simplify the data search and information discovery model of Shoso-in Monjo along with enabling cooperation with many databases, thereby ensuring effective information discovery based on knowledge data. We therefore adopted an ISO standard Topic Maps as an ontological method and applied it to the SOMODA data. In the next section, we report our findings. Characteristics of the Topic Maps Topic maps are defined as follows by ISO 13250 Topic Maps Data Model. ‘Topic maps is a technology for encoding knowledge and connecting this encoded knowledge to relevant information resources. Topic maps are organized around topics, which represent subjects of discourse; associations, representing relationships between the subjects; and occurrences, which connect the subjects to pertinent information resources’. 3  Furthermore, it is possible to assign Subject Identifiers to topics to identify/differentiate subjects within a topic map. Internationalized Resource Identifiers (IRI) are used as Subject Identifiers. Subject Identifiers that have been made public are called Published Subject Identifiers (PSI). By using PSIs, it is possible to accurately identify/differentiate subjects based on unique IRIs, without being troubled by issues such as synonyms, homonyms, and polysemy, which are difficult to resolve on the Web today. Based on this, it will be possible to assign academic IDs one by one to the Shoso-in Monjo on the Web while enabling a high level of cooperation with other digital archives (see Figure 2). 4   Figure2. The ontology of Shoso-in Monjo topic map   The Generated Shoso-in Monjo Topic Map  Based on these characteristics, we created a topic map for the Shoso-in Monjo as described below. First, based on the historical particulars of, and changes to, Shoso-in Monjo, we noted the relationship between the state of the Shoso-in Monjo today and the structure of the 8th-century original. At present, Shoso-in Monjo has a layered structure of association: Series ( Shozoku), Sub Series ( Chitsu), Volume ( Kan),  dankan (the smallest unit of paper remaining from the original 8th-century documents), page. We created a representation of this layered structure. Next, we noted the original structure as being the  Chobo (the original Documents)— dankan (the broken piece of Documents)—page. We then assigned PSIs to each page and  dankan, making higher-level use of the Shoso-in Monjo digital data possible. This will become the foundational information for the Shoso-in Monjo topic map as a whole.  We next extracted information on personal names in ancient Japan from existing dictionaries. Additionally, we affixed associations regarding the social status, occupation, and location to the personal names based on existing research articles on Japanese history. Then we affixed associations with the ‘pages’ in the Shoso-in Monjo in which these personal names appear. This allows one to understand what kinds of people are described in the Shoso-in Monjo and in what kinds of historical records they appear. Third, we extracted information relating to East Asian scriptures and sutras from existing dictionaries. We then noted the version, anthology, and author of the sutra. Further, we affixed associations between the titles of these sutra and the places in which they actually appear within historical records. Through this, the relationship between book titles and publications as entities can be understood.  The Usefulness of the Shoso-in Monjo Topic Map  We anticipate that the Shoso-in Monjo topic map will be useful in the following ways.  • Connecting personal names in the Shoso-in Monjo with sutra titles.  • Discovery of semantic information that may not be found using a conventional search model, such as whether a given person had anything to do with a unique scripture at a sutra-copying place. For instance, the relationships between the holders of annotated editions of sutras at a given time during the 8th century and their locations have been visualized in Figure 3, using the topic map we generated as the source. Based on this, information such as what kinds of texts a set of annotated editions belonging to a given monk are and whether there is any bias regarding the authors of these annotated editions can be understood. Further, by adding information on the location and time, information can be obtained on whether the holder of an annotated edition or the location of the edition changes with time. This will become an important source of information for analysing how the monks of the 8th century studied sutras.   Figure 3　Affixing associations between the whereabouts of sutras and the holders of records using the topic map*1: PageID *2: Sutras Name *3: Name of Monk *4: Current Node *5: Position    Connecting registers and sutras/personal names. The Shoso-in Monjo had a history of causing problems for research due to its exceedingly complicated structure, which made it difficult to see the big picture. The Shoso-in Monjo database created up to 2007 is what resolved this issue. However, it was not suited for viewing multiple materials relating to a specific keyword at once. That has been resolved and it has become possible to mechanically analyze whether, for instance, a given sutra appears in a specific register (document).   Connecting personal names with each other. Many personal names can be seen in the Shoso-in Monjo. Accordingly, it will be possible to analyze whether particular connections exist between personal names or between occupations. Based on this, it is possible that the personal relationships that have hitherto been only vaguely identified within the Todaiji sutra-copying center—the focus of the writings in the Shoso-in Monjo—will become clear.  In the future, we can expect that large amounts of research information/knowledge beyond what has already been described will be organically connected to the historical records of the Shoso-in Monjo within the Shoso-in Monjo topic maps. Challenges and Prospects The development of the database created in this study has only just begun, and there are the following challenges. 1. Processing complex time transitions. While the Shoso-in Monjo is a collection of documents from a relatively short period—that is, the mid-8th century—there are still some temporal changes; within those temporal changes, there are also changes to the ontological structure. How these changes are processed will be investigated from now on. Further, while the contents of the Shoso-in Monjo texts are from the 8th century, it is necessary to take up to the Meiji Period into consideration in order to include subsequent changes in complex historical record structures. How to respond to this kind of circumstance will be a challenge hereafter. 2. Establishing appropriate topics. While the first step is inputting information on personal names and scriptural titles based on historical records, inputting more abstract research knowledge/information is desirable if one is to achieve the original goals of the topic map. It will be necessary to constantly consider the extent to which establishing these topics conforms to the goals of Shoso-in Monjo research. At present, we have obtained permission to make secondary use of the index appended to the research papers of the Shoso-in Monjo, and our next plan is to put it to practical use. In the future, we believe it will be necessary to extend the research of the Shoso-in Monjo outward by further brushing up on ontology and creating links to historical records from the same period as the Shoso-in Monjo, such as the Shoku Nihongi, and to databases on Buddhist sutras. Using this system, it will be possible to develop materials for an even higher level of research in history and humanities in Japan. Notes 1. A detailed explanation about Shoso-in Monjo in English appears at Bryan Lowe and Chris Mayo, Guide to Shoso-in Research, https://my.vanderbilt.edu/shosoin/. 2. See Shoso-in Monjo Database SOMODA, http://somoda.media.osaka-cu.ac.jp/. 3. ISO 13250-2: Topic Maps—Data Model, http://www.isotopicmaps.org/sam/. 4. Shoso-in Monjo Topic Map, http://www.somoda.jp (scheduled to open in 2015). ",
        "article_title": "Digitalization of Shoso-in Monjo and Extraction of Knowledge",
        "authors": [
            {
                "given": "Makoto",
                "family": "GOTO",
                "affiliation": [
                    {
                        "original_name": "The National Institutes for the Humanities, Japan",
                        "normalized_name": "National Institutes for the Humanities",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/040xx6p34",
                            "GRID": "grid.471880.2"
                        }
                    }
                ]
            },
            {
                "given": "Motomu",
                "family": "NAITO",
                "affiliation": [
                    {
                        "original_name": "Knowledge Synergy Inc. Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "English",
            "knowledge representation",
            "asian studies",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Working from user location and source document metadata, this paper seeks to understand how users analyse text through mobile devices. It will present our initial findings from the first 18 months of the project and catalogue the next steps for Textal as an open service. We will discuss the technical challenges of creating bespoke analysis workflows as well as providing insight into promoting digital humanities to the general public through mobile platforms. This paper presents a collaborative project between UCL Centre for Digital Humanities (UCL DH) and the Bartlett Centre for Advanced Spatial Analysis (UCL CASA).  Textual analysis aims to return relevant logical data requests to users’ queries from digital documents using natural language and a common vocabulary (Hobbs et al., 1982). Unstructured text analysis still remains a challenge for today’s automated algorithms (Borodkin et al., 2014) and high-performance computing systems as these systems attempt to classify relevant data from human-created datasets. With the exponential increase of user-submitted content on social media sites and the vast amount of information contained on the Internet in various file formats, providing an overview of large text corpuses through automated processes still remains a challenge. Resources available to researchers, such as TAPoR (Text Analysis Portal for Research) (TAPoR, 2014), programming libraries such as Natural Language Toolkit (NLTK) (Bird, 2006), and a Python library for processing natural language from human language data (NLTK Project, 2014), to name a few, allow researchers to analyse complex or large datasets but often neglect the visualisation aspect of analysis. The creation of data visualisations is often left to the researcher after analysis is completed. These tools provide interfaces via web-based portals, purpose-built desktop applications, or APIs that may be slow, complex, or hard to use, which can often become a barrier for people who may need extensive knowledge before a tool, or API, becomes useful.     Figure 1. Textal word cloud visualisation of user’s biomedical paper (Cooper, 2014). Word clouds are graphical representations of textual data depicting the frequency of a given word in relation to other words within a source document (Seifert, 2008). This visualisation method derives from tag clouds, which are weighted lists of keyword metadata, first introduced as a search technique through the photo-sharing web service Flickr in 2004. Word clouds became a widely used technique among researchers analysing large texts after Wordle popularized the automated creation of such visualisations through its online tool (Feinberg, 2009). However, word clouds hide important information about the structure of the original document, context (McNaught and Lam, 2010), and statistics about individual words, which are hidden behind the visualisation.  Textal is a smartphone application that incorporates the visual style of word clouds, the interactivity provided by mobile devices, and the power of natural language processing workflows into a single easy-to-use tool (Figure 2). The application utilises the ubiquitous ‘pinch and zoom’ feature of touchscreen mobile interfaces, allowing users to explore the data behind the word cloud by touching individual words. Textal hides the complexity of unstructured text analysis through various cloud endpoints and distributed workflows. These workflows allow users to create and interact with word clouds in real time while the statistics of the document are processed on cloud servers in the background. As demand increases on the system, the processing engine can scale dynamically using on-demand computing available from cloud platforms as well as local virtualisation hardware to reduce latency.  The system was created in response to an increasing trend towards emergent mobile and web-based technologies and seeks to understand how mobile technologies can be harnessed within digital humanities research. Textal is the first attempt to build a standalone application that brings together tools and workflows for use by researchers to analyse unstructured text as well as giving the general public a tool to easily create word clouds. The app aspires to tap the potential for public engagement by capitalising on the popularity of the word clouds as a ‘gateway’ to text analysis (Meloni, 2009).    Figure 2. Textal iOS Application main screen, word cloud visualisation showing underlying statistics. Textal was launched on the iOS App Store on 14 June 2013, along with the associated website http://www.textal.org (Textal, 2015), and analysis of the data has been compiled up until 24 February 2015, the first one year, eight months, and 10 days (640 days) of the service. In this time, Textal has been downloaded onto 5,749 devices in five separate territories and has been translated into six languages. Users have created over 3,000 word clouds with over 100 million words processed, showing a need for such a tool from both the general public and as an analysis tool for researchers. This paper discusses not only the bespoke creation of a distributed workflow engine for text analysis but also the insights gained from the usage data of the application’s global user base and the textual data crowdsourced from users of the application. We will explore the use of the Textal API within the wider context of research into real-time social media analysis using distributed systems and provisioned cloud computing being carried out at the Bartlett Centre for Advanced Spatial Analysis. We will also address the ramifications for the academic field in embracing mobile platforms, such as pace of technology upgrades, uptake, and dissemination in a crowded mobile application marketplace. ",
        "article_title": "Textal: Unstructured Text Analysis Workflows Through Interactive Smartphone Visualisations",
        "authors": [
            {
                "given": "Steven James",
                "family": "Gray",
                "affiliation": [
                    {
                        "original_name": "The Bartlett Centre for Advanced Spatial Analysis, University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities, University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Rudolf",
                "family": "Ammann",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities, University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Hudson-Smith",
                "affiliation": [
                    {
                        "original_name": "The Bartlett Centre for Advanced Spatial Analysis, University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "mobile applications and mobile design",
            "text analysis",
            "English",
            "internet / world wide web",
            "crowdsourcing",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital tools have transformed the small press and little magazine literary publishing industry in fundamental ways. Poet Sandra Beasley (2009) observed, ‘As glossy magazines die by the dozen and blogs become increasingly influential, we face the reality that print venues . . . are rapidly ceding ground to Web-based publishing’ (1). Online literary journals, e-zines, and print publications’ websites have enabled literary works to disseminate beyond cultural, ethnic, and nationalistic boundaries, and have broadened the possibilities for incorporating these works into research and education efforts.  But while online literary publishing has expanded the availability of literary works, discovery and documentation of online-only or ‘born-digital’ literature is extremely limited: Many online-only works can only be accessed by browsing individual publications or searching for specific works or authors. Furthermore, increasing numbers of print journals publish some works only on their websites, but these works are typically not indexed in the same manner as those published in the publications’ print issues. In light of this issue, the authors launched a year-long investigation into access and discoverability of online-only literature, with a focus on online-only poetry, with support of an Institute of Museum and Library Services (IMLS) National Leadership Planning Grant. This long paper discusses the analyzed results of surveys and interviews conducted with creative writing faculty, literary publishers, and librarians, and examines how digital publishing and electronic literature have transformed the research environments and professional practices of literary publishers and scholars alike.  Background  N. Katherine Hayles (2007) defines electronic literature as ’digital born’, a first-generation digital object created on a computer and (usually) meant to be read on a computer’. Online literary magazines descended from the zine culture of the mid-20th century, as photocopiers and mimeograph printing gave way in the 1990s to desktop computer publishing, and digital media already was being utilized in works of hypertext fiction and electronic literature created beginning in the 1980s (Wright, 2001; Hayles, 2007). Existing initiatives for documenting electronic literature include the Electronic Literature Organization’s ELO Directory (http://directory.eliterature.org/) and its Preservation, Archiving, and Dissemination study; the ELMCIP Electronic Literature Knowledge Database (http://elmcip.net/knowledgebase); the Australian Directory of Electronic Literature and Text-based Art repository (ADELTA); nt2 of Canada; as well as the work of the Consortium for Electronic Literature (CELL).  Despite these cataclysmic changes in the literary landscape, relatively little has been written about the impact of digital technologies and changing information behaviors in the literary arts. Paling conducted a series of studies examining uses of information technologies by literary publishers and writers, and also proposed a possible metadata schema for organizing literary works (Paling and Nilan, 2006; Paling 2008; Paling and Martin 2011; Paling 2011). Stevens and McCord (2005) also examined the preservation of e-zines and correlative factors for stability of e-zines. Green (2014) also recently explored the publishing behaviors of creative writing faculty members via a citation analysis of print and digital literary magazines.  But this is the first study to comprehensively explore the needs for access, curation, and preservation of online literary publishing via direct engagement with writers and publishers on the impact of digital technologies on research and professional practices in the literary arts. Data and Methodology This study examines poetry exclusively published on web-based media—ranging from static text on simple web pages to complex multimedia works—and for the purposes of this study is termed ‘online-only poetry’. The authors chose to focus on poetry, as it is one of the most prolific genres published on the Web, and the technologies used to create online poetic works are diverse and used for other literary genres as well.  To collect data input from stakeholders, the authors utilized a mixed methods approach of three online surveys and a subsequent series of nineteen individual interviews. The surveys and interviews were conducted with the three primary stakeholder groups identified by the authors: creative writing faculty, humanities librarians, and literary publishers. The respondent pool was gathered via several methods: For faculty, the authors identified institutions with graduate programs in creative writing with the Association of Writers and Writing Programs’ database; for literary publishers, the authors used the Poets & Writers Magazine Database to identify online journals and their editors; and the librarians were solicited via the listserv for the Literatures in English section of the Association of College and Research Libraries. The authors received a total of nearly 200 responses to the surveys: approximately 960 writing faculty members in creative writing programs and departments were surveyed, with a response rate of 12.5%; approximately 152 humanities librarians were surveyed with a response rate of 58%; and approximately 945 literary journal editors and publishers were surveyed with a response rate of 13.4%. These response rates are on par with standard response rates to surveys. In follow-up interviews, the authors conducted seven interviews with creative writing faculty and professional writers, five with journal editors and publishers, and seven interviews with librarians. The surveys and interviews queried all respondents about their experiences in discovery, access, use, and preservation of online-only poetry, as well as publication practices with online journals and engagement with students, colleagues, and library users around online poetry. Analysis In the analysis of the survey and interview data, a significant number of writers and publishers observed that their poetic works had far broader accessibility, exposure, and impact from online publications than in print, and that online access had transformed their own reading practices as well. For example, survey respondents were asked to report on their online behavior regarding discovering and reading poetry, two activities that the authors regarded as connected but distinct for purposes of the survey. Respondents indicated that they visit the Web to read poetry with great frequency: 71% indicated they did so either daily (36%) or weekly (35%). Virtually all survey respondents (99%) reported using the Internet to read poetry, even if they were among the respondents who preferred print or expressed skepticism about the overall quality of work published online. As one publisher noted, ‘Publishing online permits greater access to, and for, a reading and writing audience. Not only due to the immediacy of publication release, but to the global nature of the Internet itself’.  The overarching goal of this study was to determine the need for, and interest in, an index to online-only poetic works. While our findings indicated enthusiasm for this prospective tool, the survey and interview revealed several issues related to its design and construction. One common concern was the curation of online poetry to indicate quality standards such as peer review; as one respondent noted, ‘We must be careful to retain as high standards for online publishing as we do for print’. Another major identified need was for features and functionality that would enable users to find and collect works by various criteria, or as one respondent put it, ‘It would be divine if there were something like Pandora for poetry!’ Respondents suggested metadata fields for categories such as genre, geographic location, and time period written, and they also indicated a desire to use the index to create and curate their own collections of digital poetry. Another notable need was for digital preservation of online-only poetry.  Suggested solutions to the access and preservation of born-digital poetry included a searchable index of online-only poetry, mobile phone apps, and an API feed of metadata, as well as potential collaborations with the organizations and stakeholders, such as the Electronic Literature Organization. But ultimately, most respondents did not see sharp divisions between the print and digital cultures of literary publishing. One respondent noted, ‘I want poetry online to help readers remember the page. In my best-case imaginings, online journals and podcasts get better, and print journals get better, and the online ones make it easy to get to the work and also remind people of other formats in which to experience it’. Conclusion One of the most notable results of the study was how literary culture long has navigated between the printed page and digital networks in ways that foreshadowed how many humanities scholars today intertwine digital tools and archival materials. As one poet noted, Scholars I’ve met who specialize in digital media seem to be very invested in the idea of the opposition. . . . For me it is about an expanded set of artistic tools, presences, venues: diversity and range. It is not about *reducing* the range of these experiences to a digital experience. This perspective on the experimental approach to the use of digital technologies in creative works reveals the expanding role of digital content in contemporary literary culture. This study shows how this transformation reveals a new avenue of humanities data curation that engages scholars, information professionals, and publishers equally in the essential work of making digital literature accessible to all.  Funding This work was supported by the Institute for Museum and Library Services [grant number LG-51-12-0499-12]. ",
        "article_title": "Capturing Virtual Verse: A Needs Assessment for Access and Preservation of Online-Only Literature",
        "authors": [
            {
                "given": "Harriett Elizabeth",
                "family": "Green",
                "affiliation": [
                    {
                        "original_name": "University of Illinois at Urbana-Champaign, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Rachel",
                "family": "Fleming-May",
                "affiliation": [
                    {
                        "original_name": "University of Tennessee at Knoxville, United States of America",
                        "normalized_name": "University of Tennessee at Knoxville",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/020f3ap87",
                            "GRID": "grid.411461.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "publishing and delivery systems",
            "digitisation",
            "repositories",
            "libraries",
            "archives",
            "museums",
            "creative and performing arts",
            "english studies",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "resource creation",
            "internet / world wide web",
            "and discovery",
            "including writing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2009 the French Ministry of Higher Education and Research created a facility, the Digital Scientific Library (Bibliothèque Scientifique Numérique [BSN]), to provide national-level support for the scientific and technical information sector and to inform on a variety of issues. A part of the work of BSN is focused on the various aspects of the publishing process, considering good practice policies and direct and indirect costs related to the activity.  This article presents the BSN as an emerging facility and, more specifically, describes the protocol and results from a survey implemented by the ‘publishing’ group focused on the costs associated with publishing in human and social sciences from the perspective of actors actively involved in developing editorial added-value.   The Digital Scientific Library, a Facility for French Public Digital Sciences Research Institutions  Publishing articles in scientific journals is expensive for French public institutions for a number of reasons, from financing the editorial process and production, to purchasing libraries and providing access to researchers. The issue of making publicly funded scientific publications available in Open Access form has increasingly become the subject of heated debates. 1  In addition, since 2012 and with the long-term perspective of Horizon 2020, national and European financial institutions have both included in their recommendations the use of Open Access publication (recommendations by the European Commission in July 2012 and the Horizon 2020 December 2013 release), and the French Ministry of Higher Education and Research clearly positioned itself on the subject with its declaration in 2013, ‘Scientific information is a common asset that must be available to all’ (Fioraso, 2013). Among the priority actions outlined by the Ministry of Research to achieve this goal are the following three proposals: to develop  Green Open Access, to provide support for  Gold Open Access, and to promote the development of a third road emphasizing innovation and sustainability.   The BSN publishing group is tasked with implementing scalable recommendations, adaptable to changes in the state of the art within the sector, editorial practices, usage, and market demands. The long-term objective of the group is to produce a sustainable scientific digital guide outlining:  • Best practices promoting innovative content use, dissemination, and preservation.  • Conditions for access to make the greatest number of research results available, ideally in Open Access form.  The Survey of Editorial Costs for Humanities and Social Science Journals  This survey, conducted by the BSN publishing group, is designed to collect the most recent information available representing a diverse range of situations related to editorial costs for research journals, for all disciplines, for a comprehensive and up-to-date assessment. The objective is to identify common parameters of different journals related to their operations and editorial financing and to provide a set of realistic and adaptable guideline recommendations for journals targeting Open Access publication. To achieve this, it is necessary to identify all manufacturing costs for scientific journals and differentiate shares of the public and private sectors in terms of financial support, contributing to increasing editorial value or providing support for production, dissemination, and distribution, for both printed and digital works. The main question is: What is the cost of producing a scientific article, and what is the cost to transition to Open Access? The survey began with 10 qualitative interviews with journal managers or managing editors to produce detailed descriptions of the scientific and financial operations of their journals and to discuss how they viewed Open Access publishing (or issues related to the process of transitioning to Open Access). These first interviews allowed us to understand the stages of the editorial chain and provided information on the time and cost required for each necessary task within that chain for more than 15 journals. To complement and support these first results, a quantitative survey with 66 questions was then sent out, between July 2014 and November 2014, to 300 people working for university publishers or French research centres for which a part of their activity is dedicated to publishing double-blind peer-reviewed scientific journals. In the end, both cumulative surveys helped to collate results for 50 French humanities and social science academic journals. Conclusion and Discussion This work was commissioned by the publishing group at the Digital Scientific Library (BSN) joined in the process of observation and revaluation of balances by clarifying the French publishing landscape for sustainable academic publishing committed to the goal of making all scientific productions available through Open Access by the year 2020. However, up to now, very little has been known about the economics of scientific journals, and consequently, public policy makers lacked the necessary information needed to develop informed policies for publishing. We want to emphasize that, despite what we see in some of the calculations and graphics presented, switching to Open Access with APC could not be calculated by substituting the cost per article, as the APC for an article is covered by public support. This would have made our analysis of scientific editorial production purely financial when the issues are much more complex. This survey on publishing costs of HSS journals shows the following:  • Cost analysis is an unfamiliar prism of perspective for most public actors involved in producing journals. It was urgently needed, particularly in the context of reports with commercial publishers.  • Academic journal producers and research units develop all scientific content up to the stage of page layout within their public institutions before delivering it to a private publisher who handles distribution and commercial sales, with the revenues rarely returned to the public institution.  • Paying salaries for editorial work represents the most important cost involved in publishing articles in HSS journals.  • Costs paid for by the private publisher (printing, distribution) represent less than costs supported by public sector funds (salaries for editorial work).  • The role of the private publisher is often limited to managing distribution, particularly in the context of emerging digital production and distribution. This is not to reduce the usefulness or effectiveness of this function. To the contrary, we now know that the highest costs are spent in the earlier stages of the publishing process, and this cost—which is often hidden or ignored—shows that the state invests heavily in the process of publishing journals, which has a direct consequence: this investment must be recognized, and the state as an investor should be given rights to the production it finances.  • Cost distribution between public and private funding is essential to the relationship between the public sector and private operators.  • The need exists for a French or even European infrastructure to better coordinate public and private contributions.  • The issue of empowering researchers with technical skills in digital humanities elicited very different responses and merits further investigation in a follow-up study. This survey did not examine purely scientific costs, such as issues of authors’ remuneration and work done by reviewers and editors overseen by researchers, but this issue was central to our discussions with journal managers. The first results of this survey will, hopefully, be a source of ideas and reflection for practical ways to approach Horizon 2020 in France and for prospects of further enlargement in Europe. Our discussions with professionals in the industry showed us that issues surrounding scientific editorial production in humanities and social sciences in France are approximately on par with those in Spain and Portugal (Medici, 2014). It would appear that the necessary conditions have been met for building common foundations for good practice guidelines in Europe, and perhaps the future possibility of developing editorial costs within the Dariah infrastructure (Digital Research Infrastructure for the Arts and Humanities). Authors’ Note This article compiles the main conclusions of a collective work conducted by the publishing group of the Digital Scientific Library handled by the French Ministry of Higher Education and Research. We want to thank all stakeholders in this group and the institutions they represent for their involvement, their careful proofreading, and their support: Marin Dacos (Openedition), Domique Roux (University Presses of Caen), Emmanuelle Corne (Foundation for the Sciences of Man), Céline Vautrin (College of France Publications), Jacques Lafait (Nanosciences Institute of Paris), Sylvie Steffann (CNRS-Inist), Jean-François Lutz (University of Lorraine), Annie Leblanc (French Atomic Energy Commission), and Sébastien Respingue-Perrin (University Library of Evry Val d’Essonne, Couperin). Note 1. ‘We have seen the Open Access culture developing for several years; we now need to consider how this affects the economics of scientific publication’ (Chartron, 2006). ",
        "article_title": "Publish: Whatever The Price? A French Study On Structuration Of Costs During Publishing Process In Digital Humanities",
        "authors": [
            {
                "given": "Emmanuelle",
                "family": "Corne",
                "affiliation": [
                    {
                        "original_name": "CNRS, member of BSN7 group, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Anne-Solweig",
                "family": "Gremillet",
                "affiliation": [
                    {
                        "original_name": "CNRS, member of BSN7 group, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Odile",
                "family": "Contat",
                "affiliation": [
                    {
                        "original_name": "CNRS, member of BSN7 group, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digital humanities - facilities",
            "French",
            "digital humanities - institutional support"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The emerging technologies have recently challenged the libraries to reconsider their role as a mere mediator between the collections, researchers, and wider audiences (Sula, 2013), and libraries, especially the nationwide institutions like national libraries, haven’t always managed to face the challenge (Nygren et al., 2014). In the Digitization Project of Kindred Languages, the National Library of Finland has become a node that connects the partners to interplay and work for shared goals and objectives. In this paper, I will be drawing a picture of the crowdsourcing methods that have been established during the project to support both linguistic research and lingual diversity.  The National Library of Finland has been executing the Digitization Project of Kindred Languages since 2012. The project seeks to digitize and publish approximately 1,200 monograph titles and more than 100 newspapers titles in various, and in some cases endangered Uralic languages. Once the digitization has been completed in 2015, the Fenno-Ugrica online collection will consist of 110,000 monograph pages and around 90,000 newspaper pages to which all users will have open access regardless of their place of residence.  The majority of the digitized literature was originally published in the 1920s and 1930s in the Soviet Union, and it was the genesis and consolidation period of literary languages. This was the era when many Uralic languages were converted into media of popular education, enlightenment, and dissemination of information pertinent to the developing political agenda of the Soviet state. The ‘deluge’ of popular literature in the 1920s to 1930s suddenly challenged the lexical orthographic norms of the limited ecclesiastical publications from the 1880s onward. Newspapers were now written in orthographies and in word forms that the locals would understand. Textbooks were written to address the separate needs of both adults and children. New concepts were introduced in the language. This was the beginning of a renaissance and period of enlightenment (Rueter, 2013). The linguistically oriented population can also find writings to their delight, especially lexical items specific to a given publication, and orthographically documented specifics of phonetics.  The project is financially supported by the Kone Foundation in Helsinki and is part of the Foundation’s Language Programme. One of the key objectives of the Kone Foundation Language Programme is to support a culture of openness and interaction in linguistic research, but also to promote citizen science as a tool for the participation of the language community in research. In addition to sharing this aspiration, our objective within the Language Programme is to make sure that old and new corpora in Uralic languages are made available for the open and interactive use of the academic community as well as the language societies. Wordlists are available in 17 languages, but without tokenization, lemmatization, and so on. This approach was verified with the scholars, and we consider the wordlists as raw data for linguists. Our data is used for creating the morphological analyzers and online dictionaries at the Helsinki and Tromsø Universities, for instance. In order to reach the targets, we will produce not only the digitized materials but also their development tools for supporting linguistic research and citizen science. The Digitization Project of Kindred Languages is thus linked with the research of language technology. The mission is to improve the usage and usability of digitized content. During the project, we have advanced methods that will refine the raw data for further use, especially in the linguistic research. How does the library meet the objectives, which appears to be beyond its traditional playground?  The written materials from this period are a gold mine, so how could we retrieve these hidden treasures of languages out of the stack that contains more than 200,000 pages of literature in various Uralic languages? The problem is that the machined-encoded text (OCR) contains often too many mistakes to be used as such in research. The mistakes in OCRed texts must be corrected. For enhancing the OCRed texts, the National Library of Finland developed an open-source code OCR editor that enabled the editing of machine-encoded text for the benefit of linguistic research. This tool was necessary to implement, since these rare and peripheral prints did often include already perished characters, which are sadly neglected by the modern OCR software developers, but belong to the historical context of kindred languages and thus are an essential part of the linguistic heritage (van Hemel, 2014).  Our crowdsourcing tool application is essentially an editor of Alto XML format. It consists of a back-end for managing users, permissions, and files, communicating through a REST API with a front-end interface—that is, the actual editor for correcting the OCRed text. The enhanced XML files can be retrieved from the Fenno-Ugrica collection for further purposes. Could the crowd do this work to support the academic research? The challenge in crowdsourcing lies in its nature. The targets in the traditional crowdsourcing have often been split into several microtasks that do not require any special skills from the anonymous people, a faceless crowd. This way of crowdsourcing may produce quantitative results, but from the research’s point of view, there is a danger that the needs of linguists are not necessarily met. Also, the remarkable downside is the lack of shared goal or the social affinity. There is no reward in the traditional methods of crowdsourcing (de Boer et al., 2012). Also, there has been criticism that digital humanities makes the humanities too data-driven and oriented towards quantitative methods, losing the values of critical qualitative methods (Fish, 2012).  And on top of that, the downsides of the traditional crowdsourcing become more imminent when you leave the Anglophone world. Our potential crowd is geographically scattered in Russia. This crowd is linguistically heterogeneous, speaking 17 different languages. In many cases languages are close to extinction or longing for language revitalization, and the native speakers do not always have Internet access, so an open call for crowdsourcing would not have produced appeasing results for linguists. Thus, one has to identify carefully the potential niches to complete the needed tasks.  When using the help of a crowd in a project that is aiming to support both linguistic research and survival of endangered languages, the approach has to be a different one. In nichesourcing, the tasks are distributed amongst a small crowd of citizen scientists (communities). Although communities provide smaller pools to draw resources, their specific richness in skill is suited for complex tasks with high-quality product expectations found in nichesourcing. Communities have a purpose and identity, and their regular interaction engenders social trust and reputation. These communities can correspond to research more precisely (de Boer et al., 2012). Instead of repetitive and rather trivial tasks, we are trying to utilize the knowledge and skills of citizen scientists to provide qualitative results. In nichesourcing, we hand in such assignments that would precisely fill the gaps in linguistic research. A typical task would be editing and collecting the words in such fields of vocabularies where the researchers do require more information. For instance, there is lack of Hill Mari words and terminology in anatomy. We have digitized the books in medicine, and we could try to track the words related to human organs by assigning the citizen scientists to edit and collect words with the OCR editor.  From the nichesourcing’s perspective, it is essential that altruism play a central role when the language communities are involved. In nichesourcing, our goal is to reach a certain level of interplay, where the language communities would benefit from the results. For instance, the corrected words in Ingrian will be added to an online dictionary, which is made freely available for the public, so the society can benefit, too. This objective of interplay can be understood as an aspiration to support the endangered languages and the maintenance of lingual diversity, but also as a servant of ‘two masters’: research and society.  ",
        "article_title": "Nichesourcing The Uralic Languages For The Benefit Of Linguistic Research And Lingual Societies",
        "authors": [
            {
                "given": "Jussi-Pekka",
                "family": "Hakkarainen",
                "affiliation": [
                    {
                        "original_name": "National Library of Finland, Finland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "museums",
            "GLAM: galleries",
            "lexicography",
            "networks",
            "organization",
            "linguistics",
            "xml",
            "libraries",
            "interdisciplinary collaboration",
            "archives",
            "data mining / text mining",
            "digitisation",
            "project design",
            "corpora and corpus activities",
            "digitisation - theory and practice",
            "relationships",
            "English",
            "crowdsourcing",
            "software design and development",
            "multilingual / multicultural approaches",
            "text generation",
            "scholarly editing",
            "graphs",
            "morphology",
            "resource creation",
            "and discovery",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The rapid growth in social media communications research in the last five years has seen the assembly of complexly connected datasets of a scale and scope previously rare in humanities scholarship, highlighting the sociocultural intricacies of follower networks (see Weller et al., 2013). Big data inquiry of social media has also been driven by the emergence of publicly accessible, integrated aggregation, indexing, query, and visualisation approaches (Hansen et al., 2011; Burnap et al., 2013). Indeed, the big data moment has challenged humanities researchers to develop innovative methodologies to extrapolate new social knowledge, and brought humanists and computer scientists together in the pursuit of robust eResearch infrastructure and workflows to support these objectives. This paper explores a novel social media network analysis (SMNA) research methodology, which resulted in the development of a Twitter visualisation tool for the NeCTAR research cloud. It explores the workflow issues of using large-scale eResearch infrastructure for digital humanities research, and discusses the results of the research program on social media network mapping. In doing so, the authors demonstrate the complexities of interdisciplinary humanities and computer science research collaboration, while revealing new insights made possible through eResearch partnerships. SMNA as a methodology requires the development of an integrated workflow using National eResearch Collaboration, Tools and Resources (NeCTAR) as the computing service to host both a Twitter scraper and a network visualisation tool. We used the native Twitter application programming interface (API) to maximise flexibility, minimise costs, and reduce reliance on commercial third-party data providers. The workflow included scripting an automated ‘clean-up’ phase so that hundreds of thousands of raw tweets could be gathered and indexed in a useful format. Analysing the characteristics of social networks across large datasets is computationally taxing, and to solve this problem we set up an interactive visualisation program, Gephi, in the NeCTAR cloud environment.  During the discovery and implementation phases of the project we made the following key observations. In Australia, there has been significant investment in eResearch infrastructure, including large-scale storage, data discovery, and high-performance and cloud computer systems. However, these services often have imposing barriers to entry for humanities researchers, who either lack the technological skills, desire, or established collaborative networks to apply these methodologies to their field of research (Meyer and Dutton, 2009). Moreover, eResearch assemblages are generally constructed by computer or physical scientists, who may be unfamiliar with the philosophies and theoretical perspectives of social scientists, and so it is not always clear that these technologies can immediately address the humanities’ ‘big data’ problems (Meade et al., 2013). However, in constructing the NeCTAR-based Twitter research infrastructure and pursuing an extended collaborative approach, the yield from the original research corpus provided more unique and novel results than either discipline could deliver alone. Traditionally, computer science researchers have sought interdisciplinary collaborations in the natural sciences and engineering to find appropriate research problems that balanced scale, complexity, and solvability. The materialisation of big data as a research concern within the humanities now gives software engineers new real-world applications of their techniques, a process that is key to the computer science discipline’s evolution (Hopcroft et al., 2011). The research presented in this paper demonstrates that early methodological discussions between humanists and computational specialists strengthened the research design, producing collaboratively designed research questions, while avoiding the high knowledge barriers to lay eResearch entry (Goggin et al., 2014). For example, in this research context, which drew on the expertise of University of Sydney media researchers and NSW Intersect’s computer scientists, a series of new research questions emerged while interrogating the initial social mapping results. The development of a novel computational research workflow emerged from the humanities scholars’ interest in understanding the languages, norms, or rules of communication activities within the Twitter social media platform. Following that research focus, the computer scientists explored a deeper understanding of the relationships between individual users, and the positive and negative communicative sentiment expressed in users’ interactions. The collaboration required significant mutual investment in exploring each research contributor’s agencies and abilities, and ongoing calibration of the research design. The resulting workflow and research tool have enabled a rigorous interrogation of the communication activities of complex follower networks within the Twitter platform, and the evolution of a methodology that addresses ethical concerns about big data research raised by boyd and Crawford (2012) along with Ess (2014). It will also underpin the development of an automated research tool that can be rolled out across a number of humanities research projects, and within virtual lab environments. ",
        "article_title": "Social Media Data: Twitter Scraping on NeCTAR",
        "authors": [
            {
                "given": "Jonathon",
                "family": "Hutchinson",
                "affiliation": [
                    {
                        "original_name": "The University of Sydney",
                        "normalized_name": "University of Sydney",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/0384j8v12",
                            "GRID": "grid.1013.3"
                        }
                    }
                ]
            },
            {
                "given": "Jeremy",
                "family": "Hammond",
                "affiliation": [
                    {
                        "original_name": "Intersect, Australia",
                        "normalized_name": "Intersect",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/017yqqp60",
                            "GRID": "grid.474047.4"
                        }
                    }
                ]
            },
            {
                "given": "Fiona",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "The University of Sydney",
                        "normalized_name": "University of Sydney",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/0384j8v12",
                            "GRID": "grid.1013.3"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Yazbek",
                "affiliation": [
                    {
                        "original_name": "Intersect, Australia",
                        "normalized_name": "Intersect",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/017yqqp60",
                            "GRID": "grid.474047.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "media studies",
            "interdisciplinary collaboration",
            "corpora and corpus activities",
            "relationships",
            "social media",
            "English",
            "graphs",
            "internet / world wide web",
            "digital humanities - facilities",
            "data mining / text mining",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Kindai Digital Library (KDL) is an online collection of out-of-copyright books published in Japan in the 19th and 20th centuries run by the National Diet Library (NDL) of Japan. 1 It offers full access to images scanned from books previously only available in the NDL. As of November 2014, the total number of volumes in the collection has grown to around 360,000. Although digitized text of the scans is not available due to the technical difficulties of applying OCR to old Japanese type fonts, the collection has been a precious resource for studies of the history of modern Japan since its start in 2003.  KinDigi Social is an online platform for social annotation of digitized books in KDL; it offers iOS and Android clients to browse books in the KDL collection and provides a set of features to create and share annotations and tags, allowing users to exchange and discuss historical knowledge and ideas concerning the books. The project is one of the experimental projects supported by the NDL Lab, an R&D section of NDL, 2 and is going to be available at http://kindigi-social.org/ by the spring of 2015. In this paper, we will discuss the aims, methods, and implementation of the KinDigi Social platform.  Aim of the Project The past several years saw growing interest in the use of social media for humanities studies in the digital humanities world. Possibilities of online collaboration for research in the humanities have been discussed on various occasions and put into practice in a number of research projects (Siemens et al, 2012; Barr and Tonra, 2014). Our research attempt is to apply the methodologies and techniques suggested by the results of those studies to the KDL. The need for such an attempt becomes clear when we see the numerous references and comments on books in the KDL made by professional and amateur historians not only in their academic papers but also in their blogs and social media posts (search ‘kindai.ndl.go.jp’ on Twitter, for instance). Although this sort of user-generated content is often quite valuable in that it can provide better understandings of the historical context of books, it has been left scattered over the Web and has not been archived for later references, which has resulted in a great loss of intellectual resources. Thus KinDigi Social aims to be a platform that aggregates and accumulates such annotations, and enables interdisciplinary discussion and collaboration among scholars for the purpose of further knowledge creation. Methods KinDigi Social allows its users to create annotations on both image region parts of a book and on the text itself. Each annotation accepts replies from other users so that idea-exchanging and discussion can take place. It is also possible to label each annotation with Twitter-like hashtags. On the other hand, the mere capacity to create annotations is not enough to build a sustainable social medium that will keep driving engagement of users and that can endure long-term use as an archive of historical knowledge; there are difficulties concerning user interfaces, information sharing, archiving and reuse of user-generated content, among other issues. Therefore we took the following three additional measures:  •  Mobile first development: Since currently the KDL doesn’t offer a user interface optimized for mobile devices, its users need to keep their eyes on PC displays to read the books held in the KDL. The loss of productivity derived from the limitations of this user interface cannot and should not be ignored. Although a desktop client is also planned for KinDigi Social, we have especially focused on building a rich mobile client that enables users to access the KDL almost anywhere using their iPads or other mobile devices, and provides them the same user experience gained from the use of mobile readers such as the Amazon Kindle (see Figure 1). One large problem with displaying scanned book images on mobile devices is their size; the images from the KDL have margins surrounding the actual books. For this we developed a lightweight image analysis library that automatically detects edges in a book image and removes unnecessary margins from it (see Figure 2). The code is open-source and available on Github. 3   •  Support for real-time collaboration: As the recent success of social media shows, real-time communication between users is indispensable for their continual engagement. In order to support this kind of collaboration, KinDigi Social implements the following Twitter-like system: user activities, such as the creation of an annotation, will immediately prompt a notification sent to the user’s ‘followers’ through their social feeds, for real-time responses and information sharing (see Figure 3). Likewise it is possible to ‘watch’ any book in KDL just like a GitHub repository so that the user will get a notification when an annotation is created on a book. This system also contributes to forming loosely bound clusters of users. For those who do not want to make their annotations public, support for private annotations is also planned.   •  Data modeling following the Open Annotation specification: As a description model of user-generated annotations, KinDigi Social implements the Open Annotation Data Model, an RDF-based framework designed for modeling annotations on web content whose specification has been developed by a W3C community group (Hunter, 2010). 4 It provides controlled and machine-friendly vocabulary for modeling not only annotations but also replies from other users, folksonomy tags, and links to other resources. The adoption of the Open Annotation framework makes it possible to store and serialize user-derived content in an interoperable and reusable way for their long-term preservation and for secondary uses such as text analysis.     Figure 1. The iOS client of KinDigi Social running on an iPad.       Figure 2. An example of the automatic detection of book edges.       Figure 3. An example of social feed of user activities.       Figure 4. System architecture of KinDigi Social platform. Implementation As shown in Figure 4, KinDigi Social is a server-client system. The web server is written in Ruby on Rails and is hosted on a physical server in the NDL Lab where it retrieves scanned images and the bibliographic metadata of books in the KDL through the APIs provided by the NDL. It also exchanges JSON messages with its iOS and Android clients through its REST API. The mobile clients are built with HTML5 using the Apache Cordova framework, 5 so that a single source code will generate distributions for different platforms.   Annotation data generated by users is at first stored in a relational database (PostgreSQL), and is regularly converted and dumped into an RDF database and made public through its SPARQL endpoint. In this way annotation data can be easily retrieved and processed by standard semantic web technologies. We chose a relational database as a primary method of data storage purely for performance and maintenance reasons. The user authentication is done via OAuth 2.0 protocol; users need to use their Facebook or Twitter account to log in to KinDigi Social (Authentication via OpenID is also planned). This implementation is intended to make it easy to connect with other social media platforms and to prevent vandalism by anonymous users.  Conclusion and Future Directions Our basic standpoint is that every historical digital collection should behave like a living organism; it should continue to grow, absorbing its users’ knowledge and ideas and connecting to other historical resources. The online collaboration of scholars enhanced by real-time and mobile computing, and the standardized archive system built with Open Annotation model will contribute to the transformation of currently static digital collections into such dynamic organisms. Although our project is still in its very early stages and we first need to evaluate how KinDigi Social can contribute to humanities research based on digital resources, a possible extension of our project would be to include support for multiple digital archives, and collections other than the KDL. A prime candidate is the digital archive administered by the Japan Center for Asian Historical Records (JACAR), 6 which holds the official documents of the Japanese Cabinet dating from the period prior to World War II. If it were possible to search the annotations created in both the KDL and JACAR seamlessly with a single query, that would greatly benefit studies of the history of modern politics in Japan, and we therefore hope to offer this functionality with our project.  Notes 1. Kindai Digital Library. http://kindai.ndl.go.jp/?__lang=en. 2. NDL Lab, http://lab.kn.ndl.go.jp/cms/. 3. Kindai-cropper (a github repository). https://github.com/yuta1984/kindai-cropper. 4. Open Annotation Data Model, http://www.openannotation.org/spec/core/. 5. Apache Cordova, http://cordova.apache.org/. 6. Japan Center for Asian Historical Records, http://www.jacar.go.jp/english/index.html. ",
        "article_title": "KinDigi Social: A Mobile-centered Social Annotation Platform for the Kindai Digital Library",
        "authors": [
            {
                "given": "Yuta",
                "family": "Hashimoto",
                "affiliation": [
                    {
                        "original_name": "Kyoto University, Japan",
                        "normalized_name": "Kyoto University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/02kpeqv85",
                            "GRID": "grid.258799.8"
                        }
                    }
                ]
            },
            {
                "given": "Yasuyuki",
                "family": "Araki",
                "affiliation": [
                    {
                        "original_name": "National Diet Library, Japan",
                        "normalized_name": "National Diet Library",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/05gj5ka88",
                            "GRID": "grid.462665.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "interface and user experience design",
            "programming",
            "social media",
            "English",
            "linking and annotation",
            "crowdsourcing",
            "software design and development",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Woolf Online project, 1 which recently completed its second phase of development at Loyola University Chicago’s Center for Textual Studies and Digital Humanities, in collaboration with the Department of Computer Science, sought to address the following fundamental questions about the nature and development of literature:   • How does a literary text come into being?  • What kinds of influence are at work upon the writer during the process of initial composition, and thereafter? The Woolf Online project sought to investigate various ways in which different recoverable histories of a particular text could be used to illuminate the process of its composition. By recording the history of a particular text—its cultural, political, and autobiographical contexts and their interaction—visually we began to answer some of the following important questions:  • How is textual history related to other histories of a text?  • What use does literary criticism make of textual and contextual histories? Publication of Digital Scholarly Editions As part of the development of the Woolf Online project, we developed an extensible development and publication framework called Mojulem, 2 for editing, publication, and visualisation of digital scholarly editions. We are now continuing this development with the Verne Digital Corpus, 3 which focuses upon the work of Jules Verne, including original French language editions and their myriad, often questionable, English language translations. Mojulem allows us to build on the concept of ‘knowledge sites’, as suggested by Peter Shillingsburg, 4 supplementing a core publication framework with modules/plugins such as OCR, editors, and image viewers.   Mojulem also enables us to host multiple projects within one installed framework, thereby enabling cross-project research, where applicable, and the option to aggregate specified data. Development of Mojulem, with the Woolf Online project and Verne Digital Corpus as examples of the current ongoing working environments, initially followed the need for four underlying core structures. These structures include CorPix, CorTex, CorCode, and CorForm, which are detailed as follows.  CorPix Manuscripts and printed texts materially unite the iconic and lexical, the autographic and allographic, 5 whereas all digital representations separate these constituent elements into images and transcriptions. With Mojulem projects, the common default display reunites the image and transcription by mapping the one to the other at the pixel level. CorPix software currently includes eHinman, 6 Transparent, 7 TransparentOCR, Magnify, and Zoom. For example, pixel-level positioning and coordinate fixing is an inherent feature of both Transparent and TransparentOCR, within both editor and visualisation tools. eHinman is a digital adaptation of the original Hinman collator, and enables fade from the image of a page from one copy to another, thereby enabling a visual collation of multiple copies. Transparent is used within both the visualisation and editing stages of a project’s development, enabling editor and user alike to view the image as the primary entry consideration for the project.  CorTex The CorTex is the stable resource containing the merged or compacted plain text transcriptions of the variant expressions of a work. It stores all information about text and variations, ready to be extracted for display of variation amongst versions; it is not necessary to recompute them. The CorTex is the entity to which all standoff properties (markup, annotations, links, etc.) point and on whose stability the system depends. It is the source of each version’s text and variation from other texts. The stability and endurance of the CorTex is protected by multiplying duplicate copies locked with a digital signature, which verifies for each user that a CorTex copy is viable. Analysis of the CorTex variable forms provides statistical feedback to guide the production of a conflated text—for example, with the English language translations of a given Verne edition. Whilst these statistical results are no guarantee of an ultimately correct translation, they offer a conflated text with the highest viable agreement amongst the provided collated texts. Textual disagreements are currently resolved by assigning probability values, a higher value defining a greater probability of accuracy and agreement amongst the collated texts. Using such probability results, we are currently able to filter problematic passages in each translation to conflate a text with the highest probability of agreement amongst the translations per edition.  These results can then be provided for further research and assessment, and act as a suitable starting guide for further analysis of the conflated text, and translation in the example of Verne’s texts. CorCode CorCode is the add-on value of analysis, argument, and explanation. Mojulem stores markup separately, as standoff properties, applying it as the user invokes it for the rendering of a specific item’s image or text within a given visualisation, such as a transparent view of a page of the Initial Holograph Draft of ‘To the Lighthouse’. 8 To do this, Mojulem includes an editor that saves text and encoding separately, and filters for converting legacy, code-embedded transcriptions, including TEI-encoded documents, into separate forms with markup analysed into properties, and filters for reversing this process.   CorForm A CorForm 9 is a CSS stylesheet, containing special formatting rules, used to transform the overlapping properties of the CorCode into HTML. Each CorCode has a default CorForm, but other CorForms can be used in combination or as alternatives. Since a CorTex may have many CorCodes, and each CorCode many CorForms, structuring or formatting of the text can be attained by specifying some combination of already available resources, or by supplying new ones.  The CorPix, CorTex, CorCode, and CorForm are aggregated for a project within the Mojulem framework. Each such item is identified by a unique key, which is used as an index into the repository or database. In addition to the initial four cores, identified above, we have begun development of CorAssess, allowing effective assessment and analysis of Cor data for the Verne translations and conflated English language texts. CorAssess CorAssess works in tandem with the CorTex to provide analysis of statistical and end results relative to the conflated text output. CorAssess allows us to visualise where text has been conflated based upon resolved disagreements, the points of disagreement and resultant probabilities between collated texts per edition, and variance between collated texts, and also visualise alternative resolution patterns relative to variation distance for given points of disagreements in the conflated text. With Verne texts, for example, we will be able to visualise conflation decisions and offer alternatives for given decisions and disagreements, where applicable, in our conflated English language translations. Why Verne? After Woolf Online, we chose to focus upon the corpus of Jules Verne, including original French language editions and English language translations. We have begun collecting, collating, and preparing digitised copies of as many digitised editions as extant online. We have also been digitising early editions to provide an ever-growing dataset of Verne material.  The nature of early English language translations of Verne’s editions is a continuing source of frustration for those interested in the works of Jules Verne. His early categorisation as predominantly a children’s author in English language countries, unlike the publishing by Hetzel, coupled with early restricted access to original French language editions, simply compounded the issue. The corpus of Jules Verne offers an interesting opportunity for literary and contextual analysis coupled with data processing and automated analysis. The myriad existing digitised English language translations, including US and British variant editions, often more prevalent than their counterpart—the original French editions in our current digitised corpus—allows us to examine the development of those texts by comparing agreements, disagreements, omissions, and continuing revisions in said translations since a novel’s first edition. We are hoping to use this analysis to filter the noise of years of collective translations to collate a unified English translation for each French language edition. We will then be offering a comparison of French language edition against a filtered, collated English language edition. This will allow further consideration of the requisite merits of the English language translation directly juxtaposed to the original French language edition. Conclusion The development and combination of the initial four cores, CorPix, CorTex, CorCode, and CorForm, within the modular and adaptable framework Mojulem, allowed the second phase of the Woolf Online project to begin to approach the fundamental questions about the nature and development of literature, as briefly outlined in the introduction. With the addition of CorAssess, we are now beginning to address additional issues with the publication, transmission, and development of texts. We are also testing, and proving, the viability of Mojulem beyond the Woolf Online project. The corpus of Jules Verne provides a particularly fascinating opportunity to test these cores and provide a resultant conflated, English language translation per extant French language edition.  This paper will briefly introduce the Mojulem framework and its initial four cores, grounded in the example of the Woolf Online project, and detail the ongoing developments to augment this work with the above new work on the corpus of Jules Verne, and the ongoing Verne Digital Corpus. Notes 1. Project site is available at  http :// www .woolfonline . com.   2. Our current test framework for the Woolf Online project, as of 3 November 2014, can be viewed at  http:// dhdev . ctsdh . luc . edu / projects / edfu /.  3. Initial development project site is available at  http :// www .scifidocs . com /.  4. P. L. Shillingsburg,  From Gutenberg to Google: Electronic Representations of Literary Texts (Cambridge University Press, 2006).  5. Nelson Goodman noted this distinction to separate art forms with a unique authentic form—for example, painting and sculpture—from art forms that are performative with multiple copies, including writing and music. 6. An earlier stand-alone example can be viewed at  http :// dhdev . ctsdh . luc . edu /testing / imaging / ehinman - dep / v 6/.  7. A test example page of the Initial Holograph Draft can be viewed at  http :// dhdev . ctsdh . luc . edu / projects / edfu /? node = content /image / gallery & project =1& parent =2& taxa =6& content =353& pos =21 and transcription =  http :// dhdev . ctsdh . luc . edu / projects / edfu /? node = content / text / transcriptions & project =1& parent =2& taxa =6& content =5136&pos =21.   8. For example, Image =  http :// dhdev . ctsdh . luc . edu / projects / edfu /? node = content / image / gallery & project =1& parent =6& taxa =24& content =385& pos=49 and transcription =  http :// dhdev .ctsdh . luc . edu / projects / edfu /? node = content / text / transcriptions & project =1& parent =2& taxa =6& content =5164& pos =49.  9. An earlier TEI-derived stand-alone example of this concept, using material from the Malory project, can be viewed at  http:// dhdev . ctsdh . luc . edu / testing / tei / teiparser _ full /.  ",
        "article_title": "From lighthouse to the moon: a guiding light to the corpus of Jules Verne",
        "authors": [
            {
                "given": "Nicholas John",
                "family": "Hayward",
                "affiliation": [
                    {
                        "original_name": "Center for Textual Studies and Digital Humanities, Loyola University Chicago, United States of America; Department of Computer Science, Loyola University Chicago, United States of America",
                        "normalized_name": "Loyola University Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04b6x2g63",
                            "GRID": "grid.164971.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "french studies",
            "bibliographic methods / textual studies",
            "information retrieval",
            "translation studies",
            "information architecture",
            "data mining / text mining",
            "publishing and delivery systems",
            "digitisation",
            "content analysis",
            "corpora and corpus activities",
            "text analysis",
            "English",
            "software design and development",
            "scholarly editing",
            "interface and user experience design",
            "programming",
            "resource creation",
            "image processing",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A typical text analysis workflow combines the following two stages:  • Preparing data. Philological methodology establishes the base text, critical apparatus, and some analytical information like text metadata (author, date, domain, etc.) to be able to compare domains, authors, etc. with respect to the text’s content (word forms or word lemma frequency, n-gram frequency, etc.).  • Processing. Analytic methodology provides the framework to analyze a corpus or sub-corpus of texts with the help of word patterns search and display (concordances) or count (frequency lists) and comparison tools (factorial analysis, clustering). It is convenient to characterize the relation between the data and the processing stages as contractual. That contract is based on the encoding conventions of the digital representation of texts interpreted by the software to manipulate and display them through interfaces. Simple conventions (like Unicode character encoding of raw text) can correspond to file formats directly interpreted by the software; more elaborate ones (like XML-TEI encoded text) correspond to guidelines that need tuning to be processable by software. This article presents a text analysis workflow implemented in the TXM analysis software in which the data/processing philological contract complexity can be progressively adapted to the needs of the user, balancing costs of encoding with interests in processing and analysis. It will also introduce the delegation model to call external tools (like ‘natural language processing’ [NLP] software as lemmatizers) to automatically enrich encoded texts on the fly while importing them into the platform for analysis. The TXM Analysis Software The open-source GPL-licensed TXM software implements an analytic methodology articulating those stages. It provides an import framework that allows reading of corpus sources at various levels of encoding and a classic toolbox for text analysis and mining, composed of a versatile and efficient full text search engine, text reading and browsing, sub-corpus and partition building, co-occurrence analysis, factorial analysis, and clustering. It is available as a desktop application for Windows, Mac, or Linux, as well as a web portal software for a server accessed through a web browser (Heiden, 2010). The TXM platform can be downloaded for free at http://sf.net/projects/txm. Importing Corpora into TXM at Progressive Levels of Encoding Starting from raw text corpora, the first level of contract is simple and has a low encoding cost barrier: texts may have properties (called metadata) that can be used to compare them or their properties, and words are defined by their character constituents. The text format provides all information needed explicitly to the software. To analyze a corpus at such a level of encoding, TXM provides the Unicode ‘TXT+CSV’ import module. The CSV in the name expresses the possibility to associate metadata to each text through an external CSV table file. That table encodes the metadata of each text on a separate line, as in the following excerpt of the Brown corpus metadata table defining the ‘type’ and ‘reference’ metadata (Francis and Kucera, 1964): 1    id type reference   a01 PRESS: REPORTAGE Sample A01 from The Atlanta Constitution   a02 PRESS: REPORTAGE Sample A02 from The Dallas Morning News, February 17, 1961, sec. 1   a03 PRESS: REPORTAGE Sample A03 from Chicago Daily Tribune   a04 PRESS: REPORTAGE Sample A04 from The Christian Science Monitor, May 11, 1961, p. 1   a05 PRESS: REPORTAGE Sample A05 from The Providence Journal    Table 1. Brown corpus metadata (the ‘id’ column encodes text identifiers, ‘type’ the genre, and ‘reference’ the source).  When it comes to TEI encoded texts, the contract is more complex and fuzzy, especially because the TEI provides guidelines for encoding but not as prescriptive as a format can do. One can typically use several different ways to encode a particular text component. This is needed for scholars to adopt the technology: it provides ways to encode texts we haven’t read yet, for which we haven’t established the base text or the critical apparatus yet. For a given project, the TEI encoding practice should be much more specific and documented because scholars need to share the same conventions to cross-validate their philological work. In such a context, it is possible to tune the software based on the conventions documentation to import the specific TEI sources and build the necessary corpus model to process it. To analyze a corpus at such a level of encoding, TXM provides pre-tuned TEI import modules for each documented TEI practice—for example, the ‘XML-TEI BFM’ import module for texts encoded according to the BFM corpus TEI encoding practice (Heiden et al., 2010). To analyze a corpus encoded with an unknown TEI practice, TXM provides the generic ‘XML/w+CSV’ import module. That module provides a simple way to adapt any XML or TEI encoding through an XSLT transformation stylesheet into the ‘XML-TEI TXM’ format designed for the TXM corpus model. TXM is available with a library of several XML and TEI adaptation stylesheets: https://sourceforge.net/projects/txm/files/library/xsl. The ‘XML/w+CSV’ import module is part of the progressive TXM import framework, which is built on a concentric format model (every format is based on the format of lower-outer level): Unicode TXT < XML < TEI < XML-TEI TXM: Figure 1. TXM progressive import framework workflow.  In that framework, import modules can take a corpus at any level of encoding passing through the final TXM pivot ‘XML-TEI TXM’ format before compilation by the software (search engine indexes, text edition, etc.). The corpus model of TXM is composed of the following: • Each corpus consists of a set of  texts with properties called ‘metadata’ (author, title, date, genre . . .).  • Each text is composed of nested optional internal  structures that have properties.   • Each text is composed of a sequence of  words that can have properties. Words can be embedded in any textual structure.  Each text has an HTML edition built during the import process. Some  textual planes can be built on demand, such as separating comments or notes from content body, etc.  For each import encoding level, some corpus model elements can be built, or not:    TXT XML TEI   Text units files files files   Metadata CSV CSV teiHeader   Structures n/a all TEI specific   Words raw <w>? <w>?   Textual planes n/a front XSL TEI specific   For each import level, a file corresponds to a text unit. TXT level cannot provide either internal structures encoding for texts or planes and tokenizes words from their ‘raw’ character constituents. XML level provides TXT level processing and can optionally encode some or all words with a <w> tag; all other tags are interpreted as text structures, and a front XSL can filter out some elements on the fly. TEI level provides XML level processing, text metadata are taken from the TEI header, and every tag has specific semantics related to structures or planes. Within such an import framework, the scholar can adapt the encoding effort to the level best suited for the elements she wants to be able to manipulate in the analysis software. A typical progression is to start with raw text (TXT) and progressively encode more information (sometimes based on findings made with the analysis tools). When entering the XML level, the first elements encoded are very often some specific structures (e.g., sentence, paragraph, or section) or words (e.g., compounds or entities) and page breaks to control text edition pagination with respect to the sources and concordance references (e.g., page number). NLP Annotation at Word Level As any level of encoding supposes the processing of words (implicitly or explicitly encoded), TXM integrates the possibility to call external NLP tools working on words to automatically enrich the sources through a delegation model. In that model, tools and their parameters are first declared once. Then, for each import module, they can be automatically called as many times as needed on a representation of the texts compatible with their processing (texts are converted to the format needed by each tool), and the result of their processing is reinjected back into the sources. As an example, TXM automatically annotates words with their lemma and morphosyntactic category with the TreeTagger software. In that model, the quality of the automatic encoding is delegated to external tools. In the case of XML type sources, if a word is already encoded with some properties (typically by using a <w> element), the NLP annotation is just added to the initial encoding. Adding systematic word level annotation can be difficult to keep compatible with original XML encoding. This is why TXM provides stand-off and inline ways to store word annotations into a TXM-specific TEI extension scheme called ‘XML-TEI TXM’. That representation can be exported to other tools or re-imported in any TXM. Conclusion We presented the TXM progressive import framework allowing the scholar to choose the right level—and effort—of encoding needed for the analysis of its sources. TEI level encoding is not mandatory to use the tool, simple raw text can be analyzed, and XML encoded sources provide a lot of useful services for analysis, but TEI provides the best reference target level of encoding as a language to establish a contract between philological and analytic work. Note 1. The sample BROWN corpus can be downloaded from the TXM software website, http://sourceforge.net/projects/txm/files/corpora/brown. ",
        "article_title": "Progressive philology with TXM: from 'raw text' to 'TEI encoded text' analysis and mining",
        "authors": [
            {
                "given": "Serge",
                "family": "Heiden",
                "affiliation": [
                    {
                        "original_name": "ENS de Lyon, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "scholarly editing",
            "xml",
            "natural language processing",
            "text analysis",
            "encoding - theory and practice",
            "English",
            "software design and development",
            "data mining / text mining",
            "standards and interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " One of the enduring questions in the field of linguistics is why some regions are linguistically diverse, while others remain or become relatively homogeneous. For example, New Guinea is home to 13% of the world’s languages, while having only 0.1% of the world’s population and 0.4% of the land area (Nettle and Romaine, 2000, 80). It has often been assumed that terrain plays a role in the different amounts of diversity in different regions, as near-impassable terrain means that population movement results in isolated groups without further contact, while flat terrain without significant obstacles or easily traversable bodies of water lead to continual long-term contact between different language groups (cf. Marck, 1986; 2000). In recent years the impact of terrain on the diversity of New Guinea languages has been overshadowed by research on other proposed factors, such as economic, social, cultural, and linguistic structural predictors (cf. Pawley, 2007; Currie and Mace, 2009; Lupyan and Dale, 2010; Greenhill, 2014). None of the existing research, however, fully accounts for the diversity question. With recent advances in the accessibility of mapping technology, 3D rendering, and satellite imagery, the time is ripe to consider terrain as a possible factor in language change models. Agent-based modelling (ABM) represents a useful approach for considering the plausibility of terrain as one determinant of linguistic diversity. Agent-based models assume one or more computer-based autonomous, linguistic, reactive, proactive, and intentional agents who interact with each other and their environment (Wooldridge and Jennings, 1995; Gilbert and Troitzsch, 2005). Given an initial set of states distributed across a group of agents, and a finite set of rules, these interactions can then show interesting emergent patterns over time (Axelrod, 1997). As both Axelrod (1997) and Epstein (2008) argue, agent-based models can be useful as much for their explanatory as for their predictive value—as sorts of ‘thought experiments’ (Axelrod, 1997, 4) for elucidating how complex phenomena might emerge from simple assumptions. Increasingly, ABMs have been applied to social phenomena such as economic systems (Farmer and Foley, 2009), political cooperation and decision-making (Axelrod, 1997), urban development (Batty, 2007), social networks (Macy and Willer, 2002)—and language change (Steels, 1997).  However, agent-based modelling remains an underexplored—if tantalising—approach to understanding models of language change. As one example, Parkvall et al. (2013) proposed an agent-based model for the evolution of creoles in which agents follow extremely simple rules about talking to each other and updating their lexicon to ‘learn’ new words from each other during interactions. They tested this model for various combinations of founder languages, and found that it successfully predicts the lexifier language in the resulting creole.  The idea behind such a simulation is not to include  all possible factors in language change but to pare them down to the minimum needed to get a realistic outcome. This shows us which elements are most important in language change. Elements that have been considered in previous agent-based simulation of language change include social networks (Troutman et al., 2008), genetic predisposition of the agents to language learning (Baronchelli et al., 2012), and diversity of founding population (Parkvall et al. 2013), but to our knowledge, no one has included geographical factors.  Here we present a model of language change in two sample regions: Palmerston Island (Cook Islands) and a range of terrain types in Papua New Guinea using a GIS-based agent-based model. In the first case the terrain is extremely flat, with no rivers or other potential obstacles to divide groups. The only terrain effects that could affect language change are the relative density of bushland that makes certain paths across the island easier to traverse than others. People are therefore more likely to encounter those who live near those paths than those who live in the denser bushland. In Papua New Guinea, on the other hand, we consider hilly terrain, rivers, and swampland as well as bush, so that population movement is much more constrained.  We argue relatively simple ABMs can represent complex emergent patterns typical of language differentiation among these relatively geographically proximate groups. We build upon existing work with a web-based ABM framework, Fierce Planet (Magee, 2012; 2014). The affordance of this framework over more established ABM frameworks such as NetLogo, Repast, and CORMAS (e.g., Bajracharya and Duboz, 2013) is the relative accessibility of simulations; these can be easily deployed and shared with other researchers and communities. We adapt that framework for three-dimensional rendering using WebGL and Three.js, and import geographical models for both locations. For Papua New Guinea, we import DEM models developed by USGS. For Palmerston Island we construct a DEM model using World Machine since it is too small and low-lying to be picked up by satellite.  Both height and foliage act as cost-based constraints on the movement of members of different tribal groups. As a general rule, this constrains interactions between groups, while reinforcing bonding ties within groups. Our model shows that, given a common linguistic origin, over time geographical constraints act as a factor in the gradual divergence of dialect and language between two groups. This is especially so when one or both groups are exposed to exogenous influences. We also model a range of alternative scenarios, where connections between groups become easier or more difficult due to alterative pathways between them.  We make two contributions with our paper. First we demonstrate how linguistic diversity can be partially but plausibly explained by longitudinal accumulation of linguistic and other social interactions within versus between groups. We show that geographical influences can shape this accumulation. Second we show how complex geographic ABMs can be developed using current web standards. This facilitates broader engagement with the assumptions and algorithms of ABMs, and encourages critique and feedback from other researchers and the public. ",
        "article_title": "Geo-Language Games: An Agent-Based Model of the Role of Terrain in Language Diversity",
        "authors": [
            {
                "given": "Rachel Marion",
                "family": "Hendery",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Liam",
                "family": "Magee",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "linguistics",
            "agent modeling and simulation",
            "anthropology",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We present an efficient web-based collaborative working environment for the creation of a new digital Sanskrit dictionary, making it possible to abstract from the structured representation of a dictionary entry and instead focus on content decisions when adding new entries through a convenient input mask.  *** Sanskrit is, among other things, the liturgical language of Hinduism. With its textual transmission dating back to the 2nd millennium BC and written records in the form of inscriptions dating back as far as the 1st/2nd century AD, Sanskrit texts make up one of the richest cultural and intellectual archives of the pre-modern Asian world. At the end of the 19th century Böhtlingk and Roth published their groundbreaking Sanskrit dictionaries (Böhtlingk, 1855–1875; 1879), creating an indispensable tool in indological research and laying the foundation for modern lexicographical Sanskrit studies. They have since never been superseded. The only comprehensive effort to create an addendum based on the same scientific standards was made by Schmidt (1928). Since then a large number of significant scientific advances in Sanskrit lexicography have since been made. While the portfolio of dedicated large-scale dictionaries is manageable (Böhtlingk, 1855–1875; 1879; Apte, 1957–1959; Grassman, 1873; Monier-Williams, 1899) and largely accessible for research (Kapp and Malten, 1997), the exact opposite holds true for the enormous amount of individual lexicographical accomplishments that have been made over the course of generations. This cumulative knowledge, published in different forms and places (glossaries, specialized dictionaries, articles in journals and anthologies), evades targeted access by being scattered both temporally and editorially, making it nearly impossible to obtain an extensive overview of the current progress of studies of Sanskrit vocabulary and its semantics. In consequence, one has to largely rely on the state of knowledge from 1928 when translating Sanskrit texts. This deficit will most certainly cause indological research to qualitatively fall behind other philologies over time. To counteract this process, a systematic scientific revision of the accumulated advancements made in Sanskrit lexicography since 1928 is urgently needed. Our project, Nachtragswörterbuch des Sanskrit (NWS), aims at creating a digital Sanskrit dictionary whose lemmatic content consists of about 150 publications, containing an estimated 11,000 pages worth of scientific collections of Sanskrit vocabulary. The goal is to make this content available in a single digital dictionary with uniform presentation and structure by systematically analyzing each lexicographic collection with academic expertise. This includes identifying, extracting, and transferring relevant content into a new and unified lemmatic structure without adding further information. A transcription is decidedly not part of our project. We identify the key challenges that we have to overcome as follows.  First and arguably the biggest challenge is the sheer amount of content. Due to the scientific assessment of the source material on a per-entry basis, an automated approach can be ruled out. Instead, our main priority is to make this workflow of identifying, extracting, and transferring lexicographic content into our dictionary as convenient and efficient as possible by providing a tailored working environment for the indologists on our team. The second challenge is due to the fact that the content of our dictionary consists of various different sources. In order to avoid duplication of information we include only the first (oldest) occurrence of duplicate content. Reviewing the source material by date of publication is one necessity for this; however, the challenge lies in being able to access the live state of the dictionary in order to look up previously entered content under the same headword, keeping in mind that this content may not necessarily have been entered by the same person but could have come from anyone on our team. This brings up the next challenge, which is the fact that multiple users will be working on the dictionary, quite possibly at the same time, needing access not only to the content they are working on themselves but to the whole dictionary in its current state. To address these challenges and meet the necessary requirements we opted for a web-based collaborative working environment, enabling us to provide an up-to-date version of the dictionary to all team members. The most important feature of this working environment is the ability to add new entries to the dictionary using a tailor-made web input mask (see Fig ure 1). This allows abstracting from the internal deep structured database representation of a dictionary entry and instead working with a specialized user interface. This allows the indologists on our team to focus on the content as well as the actual lemmatic structure instead of how to represent that structure internally. Furthermore we exploit the fact that publications are predominantly being reviewed one at a time from front to back. Using that knowledge we can prevent the need to input repetitive information by automatically filling in certain input elements in the web mask when entering consecutive entries. Whenever we can’t automatically deduce information with certainty, we try to anticipate the input by offering context-sensitive suggestions that update themselves while the user is typing. Using this feature is an invaluable resource when working with scribal abbreviations of Sanskrit texts, which in our case already amount to over 2,400 in total. By starting to type any part of an unabridged Sanskrit texts name one can effectively search for the corresponding abbreviation and accept the correct suggestion only by using a couple of keystrokes. The same feature is also used in different contexts, e.g., when choosing among more than 250 literary references or when entering cross-references. While convenience features are certainly the most notable when using our working environment on a daily basis, another important aspect is the added benefit of automatically validating new entries before they are added to the dictionary. 1  The most difficult part during development proved to be making the input mask adaptable enough to support the full range of expressiveness of the underlying deep structure of a dictionary entry, while offering a clear, intuitive, and uncluttered user interface. We achieve this by using an approach where the user starts with a basic form, dynamically adding new structural elements whenever needed. This allows for arbitrarily complex entries when needed but provides a minimalistic view by default. While this approach works just fine for the overall structure of the dictionary entry, a different method is needed when it comes to tagging single words or even just characters of a text inside an input field. To this end we developed a feature that allows tagging parts of texts inside an HTML text input field in a way best described as analogous to using a highlighter. By making a text selection using the mouse or keyboard, the user can then tag the selection by using a hotkey combination or clicking a button in a floating context-sensitive toolbar. As Sanskrit is predominantly written in Devanagari, transliteration support including automatic Unicode normalization is another feature built into the input mask. Furthermore, by allowing each user to view dictionary entries in their textual representation and flag them as correct or give a written opinion on why they are incorrect, proofreading becomes a group effort.  The structured content of our dictionary is being stored in a relational database whose schema we derived from a subset of the TEI-P5 guidelines on encoding dictionaries. 2 Exporting to TEI conform XML is therefore possible. Furthermore we use the Ruby on Rails framework to generate different textual representations from the structured database content. This allows us to use a singular data source but have different textual representations of our dictionary entries without duplication of content.        Figure 1. A screenshot of the input mask for new entries shows the autocompletion feature, the floating highlighter toolbar, as well as the personalized bookmarks menu in the top right.   Results Development of this working environment initially took six months. We started testing our environment under live circumstances in June 2014. What was initially supposed to be a test phase ended up being the beginning of the use of our working environment in production. We have since iteratively integrated new and improved existing features. At the time of writing this, our team of four indologists has been successfully using this environment to review more than 30 sources, creating a total of nearly 10,000 dictionary entries. Looking at the time frame of our project, we are more than confident that this modern working environment will contribute to the timely completion of our digital Sanskrit dictionary.  Future Work In the future we plan on integrating the content of the Cologne Digital Sanskrit Dictionaries 3 into our dictionary, creating a unified digital Sanskrit dictionary. Furthermore, additional Sanskrit publications could be reviewed and integrated into the NWS as required, creating an even more comprehensive lexicographic Sanskrit resource. The possibility to realize such a project has been established with the development of this working environment.   Funding This research was funded by the Deutsche Forschungsgemeinschaft (DFG) as part of the Kumulatives Nachtragswörterbuch des Sanskrit project, a cooperation between the Martin-Luther-University Halle and the Philipps-University Marburg under the direction of Prof. Dr. Walter Slaje, Prof. Dr. Jürgen Hanneder, Prof. Dr. Paul Molitor, and Dr. Jörg Ritter.  Notes 1. All literary references and citations are being checked for consistency. We’re currently not using a routine to check the whole entry for typing errors. 2. Text Encoding Initiative. P5: Guidelines for Electronic Text Encoding and Interchange, Section 9–Dictionaries. 3. Cologne Digital Sanskrit Dictionaries: http://www.sanskrit-lexicon.uni-koeln.de. ",
        "article_title": "An Efficient Collaborative Web-based Working Environment For The Creation Of A Digital Sanskrit Dictionary",
        "authors": [
            {
                "given": "Sascha",
                "family": "Heße",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Katrin",
                "family": "Einicke",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Jörg",
                "family": "Ritter",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "interdisciplinary collaboration",
            "project design",
            "interface and user experience design",
            "English",
            "management",
            "internet / world wide web",
            "asian studies",
            "lexicography",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The disciplines now maintained in universities have changed sharply since the 18th century, when they were still largely theological. However, the 18th century’s academies and encyclopedias set the stage for this transformation. How can the digital humanities help us understand and articulate such changes in the disciplinary structure within systems of knowledge? Specifically, our project is designed to use methods of network analysis and visualization to explore commonalities and differences between the  Encyclopédie (1751–1772), Diderot’s landmark encyclopedia of the 18th-century Enlightenment, and the Wikipedia of today. One of Diderot’s powerful innovations was a pervasive system of cross-references, networking the many thousands of articles into a navigable system. Wikipedia is built upon a similar system, both hyperlinking its articles together and recommending related articles under the heading of ‘See also’. Our project wagers that a juxtaposition of these cross-reference networks allows us to historicize the particular structures of knowledge interconnectedness as manifested in the  Encyclopédie and in its modern Wikipedia descendant. Our results show that, perhaps unintuitively, cross-references more commonly ‘cut across’ demarcations of Enlightenment-born disciplines in Diderot’s  Encyclopédie than in Wikipedia.     Figure 1. The article cross-reference network of the  Encyclopédie. Each node is an article; each edge, or link, indicates a cross-reference between them. Articles without cross-references are not displayed here. The network was visualized in the software Gephi and colored by its network modularity algorithm with default settings.  Although 65 times larger than the  Encyclopédie, Wikipedia, like Diderot’s encyclopedia, works within categories and operates by selection. 1 One covert but powerful principle of selection within Wikipedia is that of modern disciplines. Although Wikipedia is not specifically organized by disciplines, they figure powerfully. Just one specific instance is that of the well-known article on the ‘philosopher’ in the  Encyclopédie, assigning the broadest possible intellectual role to the kind of man of letters Diderot signified by the term. In Wikipedia, by contrast, ‘philosopher’ is specifically about practitioners of the modern formal discipline of philosophy.  Below, we compare these two encyclopedias by combining interpretive readings of cross-reference networks drawn from particular articles (Experiment 1) with a large-scale computational approach (Experiment 2). For the digitized text of the  Encyclopédie, we turned to ARTFL, a comprehensive database of thousands of 18th-century French texts, including all 74,000 articles in Diderot’s  Encyclopédie (‘ARTFL  Encyclopédie Project’, n.d.). Mark Olsen and other researchers involved in the project have computationally identified and hyperlinked the  renvois, or cross-references, embedded in its articles (‘Renvois Navigation’, n.d.) . Gilles Blanchard and Mark Olsen have published on the relationship between the  Encyclopédie’s ‘tree of knowledge’ and its  renvois, as two distinct models of knowledge organization, showing which parts of the tree are more strongly connected via cross-references than others. In this project, we juxtapose and compare the cross-reference networks of two encyclopedias created more than two centuries apart, in order to make visible and better understand the historicity of knowledge networks.  Experiment 1: Case Studies of Key Article Networks First we drew up a list of articles on a wide range of topics that reflected a diversity of disciplines and that contained a large number of cross-references in both encyclopedias. These articles include, for instance, ‘philosopher’, ‘commerce’, ‘agriculture’, ‘nature’, and ‘existence’. For each selected article, we created a network linking it to all of the articles it cross-references; then we repeated this process only once for each of the cross-referenced articles. The resulting network is a directed cross-reference network with a depth of 2. For this experiment, we considered as cross-references all  renvois of the  Encyclopédie article, and all links under ‘See also’ in the corresponding English Wikipedia article. 2     Figure 2. An article cross-reference network, centered on the article ‘commerce’ in the  Encyclopédie, emanating two levels deep.     Figure 3: An article cross-reference network, centered on the article ‘commerce’ in the English Wikipedia, emanating two levels deep. For example, one of the most striking comparisons we’ve found is the article on ‘commerce’. The  Encyclopédie links ‘commerce’ to such diverse topics as art, navigation, and agriculture at the first level, and poetry, mathematics, and construction at the second (Figure 2). Wikipedia’s version of ‘commerce’, by contrast, is tightly disciplinary (Figure 3). At the first level of cross-references, nearly all of the headings are from business, trade, and manufacture. At the second level, at the furthest reach from the core of economics, we have ‘shoplifting’ and ‘classical liberalism’. Clearly, when it comes to ‘commerce’, the unannounced, implicitly defined discipline adhered to by Wikipedia is business management—no room for categories such as tolerance, winter, magnetism, the military, or poetic theory.  In these case studies, we see over and over again an increase in disciplinarity of the sort we see in ‘commerce’—but also a move to technical practices more generally, some of which are disciplinary, some of which are commercial, and some of which are institutional abstractions. Experiment 2: Large-Scale Network Comparison A potential objection to these English Wikipedia examples lies in our translation of 18th-century French concepts into 21st-century English. The translation of the words from French to English potentially introduces some inconsistencies. With this in mind, we move from particular article networks to a big-data approach, comparing the overall citational structure of the  Encyclopédie to that of the French Wikipedia (hereafter  Wikipedie). Although the French version is about a third smaller than the English, it is still 20 times larger than the  Encyclopédie. In order to compare it to the  Encyclopédie, therefore, we reduced both to the subset of headwords appearing in both encyclopedias. This left us with about 16,000 articles. Because the  Wikipedie is smaller than its English equivalent, and less likely to have maintained ‘See also’ sections, we decided to consider as cross-references all of the hyperlinks in each article. This is a departure from our earlier practice (see note 2).  For every possible pair of the remaining articles, we calculated the shortest path between them, in the cross-reference networks of both the  Encyclopédie and the  Wikipedie. Then we looked at which links in each network were the most traversed among all of the shortest paths taken. By shortest path, we mean the smallest number of  renvois necessary to reach a second article from a first. These most traversed links are, literally, structurally central to each system of knowledge: if one were navigating either encyclopedia by its cross-references, these would be the connections that you would be most likely to cross. In network theory, this type of centrality is known as  betweenness centrality.  Our study of these central links showed a significant difference in the types of relationships so central for the two encyclopedias. 3 By classifying them in terms of their analogical relationship, we discovered important differences in the way in which the two cross-reference networks are structured. In the  Encyclopédie, these relationships are most often characterized by a ‘structuring’ or ‘governing’ analogy (Figure 4). For instance, ‘langue’ is governed by ‘gammaire’, ‘terre’ by its ‘axe’, ‘destin’ by ‘Dieu’, ‘religion’ by ‘foi’, and ‘justice’ by ‘droit’. 4 In each case, the two concepts are distinct in kind, but they are brought together by a type of relationship in which the first concept depends on the second to give it order and form.        Figures 4 (left) and 5 (right): Links with greatest betweenness centrality, in the  Encyclopédie (left) and  Wikipedie (right) networks, respectively. Here, nodes consist only of articles present in both encyclopedias, and edges only the cross-references between them.  In the  Wikipedie, none of the most important links are characterized by this type of analogy (Figure 5). Instead, the most frequent type of relationship is one of membership between two concepts of the same kind. For example, an ‘ecclesiastique’ is a member of the ‘clerge’; ‘vin’ is a type of ‘esprit’; ‘addition’ is a type of ‘arithmetique’. 5 Although the governing relationships in the  Encyclopédie place the entries in large conceptual hierarchies, the taxonomic relationships in the  Wikipedie point to disciplinary relationships, where knowledge is interrelated along an axis of specialization and depth. Compared with the  Encyclopédie, the  Wikipedie is less likely to make the metaphysical connection between arithmetic and, for example, logic; instead, it connects arithmetic to its constituent parts: addition, subtraction, multiplication, and division. We believe this juxtaposition of cross-reference practices and network structures reveals a notable shift toward disciplinarity in modernity, made visible by a combination of computational and hermeneutic methods.  Conclusion From our research, then, it appears that the broad inclusivity of Wikipedia is, if anything, more deeply disciplinary than even the planned knowledge system of the  Encyclopédie. It is worth wondering about how powerfully Wikipedia is driven by formal disciplines despite its largely informal surface structure. We are finding that where Wikipedia is not more clearly disciplinary, many articles still move toward a greater level of abstraction and institutional formation than in the  Encyclopédie.     Notes 1. The English Wikipedia contains 4.6 million articles. See http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia (accessed 2 November 2014). 2. In this experiment, we decided to leave aside the interlinear hyperlinks within most Wikipedia articles, because we believe they more poorly reflect the intentional editorial decisions of Diderot and his collaborators in making the  renvois than do the cross-references contained under the ‘See also’ section. We decided to use the English Wikipedia in this experiment because, as the largest Wikipedia, it has received the most editorial attention and is most likely to contain the ‘See also’ section.  3. For this experiment, we annotated the type of analogy evident in the top-15 most central links from the shortest-path traversals of both encyclopedias’ cross-reference networks. 4. English translation: For instance, language is governed by grammar, Earth by its axis, destiny by God, faith by religion, and justice by right. 5. English translation: For instance, a clergyman is a member of the clergy; wine is a type of spirits; addition is a type of arithmetic.  ",
        "article_title": "Knowledge Networks, Juxtaposed: Disciplinarity in the Encyclopédie and Wikipedia",
        "authors": [
            {
                "given": "Ryan",
                "family": "Heuser",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Bender",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "media studies",
            "historical studies",
            "relationships",
            "English",
            "graphs",
            "data mining / text mining",
            "multilingual / multicultural approaches",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital literary geography (DLG) has already made possible important discoveries into the geographic patterns of large-scale corpora of fiction. For instance, Matthew Wilkens has shown that the rise of literary regionalism, argued to have taken place in response to the Civil War, does not actually manifest in a large corpus of American fiction published in the surrounding decades. For Wilkens, this methodology—counting how often place-names appear in texts—can effectively operationalize a text’s ‘geographic investment’ with particular places (Wilkens, 2013, 804). We find the concept of geographic investment as foundational to DLG as patterns, according to Barbara Piatti, were to the emergence of literary geography. Early maps, such as William Sharp’s 1904 map of the chief localities of Walter Scott’s novels, ‘succeeded to visualise a couple of important aspects’ of the emerging field, namely ‘the distribution of fictional settings (“gravity centers” vs. “unwritten regions”)’ (Piatti et al., 2009, 181). At the same time, we believe that a key challenge for DLG will be to demonstrate its ability to further qualify and differentiate what is meant by ‘investment’. It seems important to understand, for instance, that the rise of mentions of India in 19th-century British novels is predominantly due to their frequent appearances in off-hand explanations of the origin of someone’s wealth or some imported good (Jockers et al., 2012). Similarly, it seems essential for DLG to develop methods by which it can articulate a range of ways in which place-names operate in fiction, visualizing distinct modes of attention both historically and geographically. This paper presents a visualization and interpretation of the geographic investment with real London places in 18th- and 19th-century English-language fiction. 1 It also attempts to visualize two distinct qualifications of this investment. We have applied emerging techniques in crowdsourcing to derive from a consensus of readers two pieces of information regarding the way in which a place-name was invoked in a fictional passage. Was it the setting of the passage, or simply mentioned? Was the emotion of fear, or happiness, associated with the place in the passage?  In addition to narrative setting, we take emotion to play an important role in a project such as this, balanced between questions of literary form and social geography. In the affect-theoretical work of Sianne Ngai, emotions act ‘as a mediation between the aesthetic and the political in a nontrivial way’ (Ngai, 2004, 3). In their polarity between positive engagement and negative withdrawal, happiness and fear can be felt in the tone in which places are invoked in fiction. Mapping these tonal associations reveals one emotional spectrum along which fiction affectively mediated its relationship to London throughout two centuries of urbanization, industrialization and the rise of urban poverty, literacy, the bourgeoisie, and the novel. Experiment 1: Geographic Investment Method We developed a list of locations in London to search for in a corpus of texts, through a combination of computational toponym discovery (using the Named Entity Recognizer of the Stanford NLP toolkit) and research into historical gazetteers and maps of London. From this list, 161 places were chosen for all experiments presented in this paper. 2 We read random passages mentioning each of the place-names, identifying ten passages per half-century in which the place-name actually referred to the London place we assumed it to. These passage annotations also supplied a statistic on the likelihood of a particular name to refer to a place. For instance, in fiction from 1750 to 1800, ‘Bond Street’ seems to refer to the street about 100% of the time, while the ‘Tower’ to the Tower of London about 90% of the time. These likelihoods are multiplied against the total count of each place-name across all fiction from a given half-century, and visualized in the following map.        Map 1. Fiction’s geographic investment with London, 1700–1900. The four tiles, read left to right, represent a fictional geography of the works published in the four half-centuries of 1700–1900, respectively. Circles represent discrete places in our corpus—streets, buildings, squares—while polygons represent wider spaces such as districts or neighborhoods. The depth of color, and the size of circle, reflect the same data: the overall likelihood for fiction of the period to mention that place.    Interpretation We find a ‘gravity center’ of fictional attention to London spaces, located in the City and West End, that is surprisingly stable across two centuries of dramatic urban expansion. Comparing the distribution of fictional attention to population by London borough across the 19th century, we find that the most frequently mentioned boroughs—the City, Westminster, and Camden—are those that least correlate with population change. 3 All three decline in relative population while remaining stable in their dominance of relative fictional attention. The responsiveness of literary representation to social change has always remained a contentious question for literary theory: here, we find not responsiveness, but a kind of ‘stuckness’, a tendency to continually reinvest London places already imbued with centuries of public meaning.  Experiment 2: Crowdsourcing Emotional Investment Method We used the Amazon crowdsourcing platform, Mechanical Turk, to derive a consensus of readers’ annotations to overlay onto the foregoing map. For each place in each half-century, we gave ten passages to twenty readers each. All twenty annotated whether the highlighted place-name acted as the setting of the passage. Ten annotated whether the emotion of fear was associated or experienced in the place; ten for happiness. 4 Averaging these annotations per place per period produced the following two maps.        Map 2. Likelihood of narrative setting. The four tiles, read left to right, represent a fictional geography of the works published in the four half-centuries of 1700–1900, respectively. Circles represent discrete places in our corpus—streets, buildings, squares—while polygons represent wider spaces such as districts or neighborhoods. The depth of color, and the size of circle, reflect the same data: the overall likelihood for fiction of the period to act as the setting of a passage.       Map 3. Emotional polarity in fiction. The four tiles, read left to right, represent a fictional geography of the works published in the four half-centuries of 1700–1900, respectively. All places are now represented as geographic polygons. The color scale, from green to red, represents the likelihood of a place to be associated with happiness, subtracted by its likelihood to be associated with fear, in the fiction of that period. Gray areas are not likely to be associated with either (neutral).    Interpretation We observe a spatial bifurcation in fiction’s emotional representations of London places: locations in the City, South, and East are more likely to be associated with fear than locations in the West and North. We test this observation by modeling the spatial patterns in the likelihoods of narrative setting, fear, and happiness, with the spatial patterns of four social-geographic data: the location of the place, whether in the City, or South, East, North, or West of it; the function of the place, whether Prison, Church, Square, etc.; the age of the place, whether deriving from the Roman and medieval eras or from the Tudor era onward; and finally, the social class of the place, as measured using remote sensing techniques applied to its immediate vicinity on Booth’s 1889 map of income classes in London. These four independent data are treated as factors in an odds ratio in order to measure the extent to which they statistically affect the likelihood for setting, fear, and happiness. The significant odds ratios (>1.4) are visualized in the following network.    Figure 1. Factors influencing setting, fear, and happiness. Emotional representations of fear and happiness articulate two distinct geographies of London. Prisons, hills (which have prisons associated with them), pre-modern buildings, and places in the City are more likely be associated with fear. Parks, churches, squares, theatres, modern buildings, places in the West, and places around which Booth had indicated upper-class residents lived are more likely to be associated with happiness. Interestingly, the only factor making both emotions more likely is a factor of narrative form: whether the passage was embodied in the place through its narrative setting. We interpret these results further and argue for the relevance of DLG in articulating nuanced geographies of literary history. Notes 1. Our corpus derives from the Literary Lab’s fictional corpus, and includes 5,000 works of English-language fiction published between 1700 and 1900.  2. Places were chosen if appearing with a minimum frequency of 0.2% of the total occurrences of London places in any half-century of fiction from 1700 to 1900. In addition, the name of each London district identified on Booth’s 1889 map of London was included to ensure sufficient spatial representation of parts of the South and East of London. 3. Population data was retrieved from UK census data on historical population in London by borough: data.london.gov.uk/datastore/package/historic-census-population. The ‘West End’ is spatially represented in the data by the contemporary borough of Westminster. 4. Our process of quality control was as follows. We applied statistical techniques to determine whether readers behaved, on average, more like other readers than would random chance. In borderline cases, manual inspection was made to determine whether the data was unlikely to have been entered sincerely, in which case we excluded it from our analysis. ",
        "article_title": "Mapping the Emotions of London in Fiction, 1700-1900: A Crowdsourcing Experiment",
        "authors": [
            {
                "given": "Ryan",
                "family": "Heuser",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Van",
                "family": "Tran",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Annalise",
                "family": "Lockhart",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Steiner",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "literary studies",
            "geospatial analysis",
            "text analysis",
            "english studies",
            "interfaces and technology",
            "spatio-temporal modeling",
            "English",
            "crowdsourcing",
            "analysis and visualisation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In August 2014, for the first time in its history,  Science included an article of an art historical nature. Art historian Maximilian Schich (2014), together with a team of experts on network theory, modeling, simulation, and visual analytics, mapped cultural migration on a global level by aggregating data of Freebase.com, the General Artist Lexicon, and the Getty Union List of Artists’ Names. Mauro Martino (2014) made an animated visualization in which the dynamics in the birth and death locations of more than 150,000 artists and scientists on a global level can be followed over a period of 2,000 years in a five-minute movie. We agree with some of the critique that the predominantly American–North European nature of the dataset, its focus on artists, and its scope on birth and death dates are too limited to map cultural migration on a global level. It should be complemented with data about the roles and interactions of various actors such as painters, comissioners, cultural agents, scholars, academies, etc., and about the artefacts they produced. However, the article in  Science describes an interesting experiment in which a combination of qualitative and quantitative methods were tested that are needed for any global history. Moreover, as the title explains, the authors did not claim other than to provide ‘a network framework of cultural history’.   In this paper we discuss the outcomes of another experiment that might be used to complement and enhance this global network framework gradually both with ingests of other cultural data and overlays of manually created networks of micro-histories. In a private-public project known as Mapping Notes and Nodes in Networks (Álvarez Francés and van den Heuvel, 2014), a small interdisciplinary team experimented for nine months for two days per week with the software application Nodegoat to create multilayered networks of actors and documents that are potentially relevant for the history of the creative industry in Amsterdam and Rome in the Early Modern Period. 1 The project started with the integration of three complementary, but heterogeneous (meta)datasets: the full text searchable Biographical Reference Works of the Huygens Institute for the History of Netherlands; ECARTICO, a comprehensive database that allows analysing and visualising data concerning painters, art consumers, art collectors, art dealers, and others involved in the cultural industry of Amsterdam and the Low Countries in the Early Modern Period; and finally HADRIANUS, a database of Dutch artists and scholars from the Middle Ages up the 20th century who stayed in Rome, developed by the Royal Netherlands Institute in Rome. Researchers who heard of the initiative offered their own sets. Frits Scholten and Arjan de Koomen provided the data about the migration of Dutch sculptors throughout Europe that their students had assembled in the project Sculptors on the Move. Susanna de Beer, with a small group of students, developed in her Mapping Visions of Rome project a typology of descriptions of Rome in humanist poetry. We embraced this rather arbitrary selection of sources to complement our three initial datasets, because it reflects the main research question of the Mapping Notes and Nodes project: ‘How can we assess which information is relevant when integrating reused completed datasets with those in development?’ Despite initiatives such as the Internet Archive or Library Alexandria 2.0 we claim that full data integration of digitized sources and digital-born data on a global level is impossible. Therefore, strategies are needed that can bring together top-down and bottom-up approaches and enable handling hybrid forms of data integration in continuous flux. For that reason in this project the emphasis was not on quantitative analyses but on a qualitative approach to allow scholars to connect and complement these divergent datasets and to create and visualize networks to see and interact with connections that they might not do otherwise. To this end, it was decided to develop a viewer for Nodegoat that visualises the co-presence of historical actors (nodes) and (meta-)data of sources and documents (notes) in Amsterdam and Rome in partially overlapping, multilayered networks (see Figure 1).     Figure 1. Network of persons with both relations in Rome and in Amsterdam and their interrelations. The assumption is that the co-presence of artists, artisans, scientists, art agents, ambassadors, patrons, sponsors, and entrepreneurs at a certain location in a given time period allows for recognizing potential networks, while contextual topical information (introduction letters, guest lists, audiences, commissions, contracts, artefacts) enable users to assess whether there were actually contacts between these persons. When new data is added, the overlap of the multilayered networks changes, resulting potentially in new answers and other questions. The building of hybrid networks implies perhaps a less systematic research of the history of the creative industry of Rome and Amsterdam, but at the same time both practical and theoretical arguments support this more explorative approach. First of all, this approach seems to reflect the way (digital) humanists assemble information on the basis of rather pragmatic criteria such as time, money, and availability of data. If complete data integration is impossible, a tool that on the basis of metadata enables assessing the likelihood that one combination might lead to better results than another could at least support prioritizing the digitization program necessary for research. The second, theoretical argument is that the proposed incremental approach with changing perspectives on data in flux stands closer to hermeneutic methods (Capurro, 2010). Such methods appeal to many humanities scholars who try to give sense to data from multiple perspectives in continuous processes of reinterpretation (Akker et al., 2011). For that reason we chose a couple of cases that could demonstrate the multidimensional relationships from data about the cultural industry of Amsterdam and Rome from different perspectives. Since the Ecartico database provides visualizations of the places where artists lived in Amsterdam, we decided to map the geolocations of Dutch artists in Rome as well, using inventories of the parochial archives by Hoogerwerff (Tamminen, 1943).    Figure 2. Location of the ‘Bentvueghels group’ with pop-up of artists living in the same house. This mapping reveals a very high concentration of Dutch artists living three or four together in one house, almost from door to door in just a couple streets (see Figure 2). This tight community of artists, known as the Bentvueghels, worked and partied together in Rome between approximately 1620 and 1720.  However, we did not only look at relations between artists. Other cases show networks of actors in the creative industry from the perspectives, respectively, of a Dutch pharmacist and a Dutch engineer in Rome. Pharmacist Hendrik Raef, alias Corvinus (ca. 1554/1567–1639), probably provided pigments to painters and was a member of the Bentvueghels group. The Dutch civil engineer Cornelis Meijer (1629–1701) created on commission of three successive popes constructions against floods and to make the Tiber navigable. These constructions are hard to link to the creative industry, but that is different for the commissions he gave to several Dutch and Flemish artists in Rome to make engravings after his designs (see Figure 3).     Figure 3. Network of people and academies (nodes) and artefacts (notes) around engineer Cornelis Meijer. Given the possibility to indicate the nature of relationships as well, we also mapped his competitors. Another case looks into the Dutch contacts of Italian engineers who on behalf of Cosimo III, grand duke of Florence (1629–1701), were involved in espionage of Dutch industries and fortifications. While these cases demonstrate relationships between different actors (the nodes) in overlapping networks, other reveal links from the perspective of the artefacts or contextual documents (notes). For instance, the above-mentioned typology of descriptions of Rome in poems of humanists is linked to categories in ICONCLASS to compare these associations with the classification of visual depictions of the Eternal City and its monuments (see Figure 4).    Figure 4. ICONOCLASS numbers connecting depictions and descriptions of monuments in Rome. The combination of incomplete, heterogeneous digitized, and manually added data will result in much fuzziness and uncertainties in interpretations of the history of the cultural industry of Amsterdam and Rome. In order to assist the user in assessing the quality of data, the producer hereof can annotate them with levels of uncertainty. Although this experiment for a multilayered network of actors and documents was set up around some cases relevant for the history of the creative industry of Amsterdam and Rome, its features of digital hermeneutics to deal with overlaps, multiple perspectives, fuzziness, and uncertainties in data have a much larger potential to enhance existing network frameworks developed for global history. Acknowledgements The authors are indebted for the data and expertise of Marten-Jan Bok and Harm Nijboer (University of Amsterdam-Ecartico); Marieke van den Doel and Arthur Westeijn (Royal Netherlands Institute in Rome-Hadrianus); Susanna de Beer and Petrie van der Heijden (Leiden University); Hans Brandhorst and Etienne Postumus (Mnemosyne-Iconclass); Arjan de Koomen (UvA) and Frits Scholten (Rijksmuseum; VU University Amsterdam); Sebastiaan Derks and Ronald Sluijter (Huygens Institute for the History of the Netherlands) Alette Fleischer (independent researcher—Corvinus and Bentvueghels); and Erin Downey (Temple University Philadelphia: Patronage Guistiniani and Bentveughels).  Note 1. Charles van den Heuvel was main applicant and p.i. of the Mapping Notes and Nodes in Networks project. Leonor Álvarez Francés was embedded researcher in LAB1100, the studio of Pim van Bree and Geert Kessels that developed the described features for this project in Nodegoat. Ingeborg van Vugt and Simone Wegman (Leiden University) worked as interns on this project at the Huygens Institute for the History of the Netherlands. ",
        "article_title": "Mapping Notes And Nodes: Building A Multi-Layered Network For A History Of The Cultural Industry",
        "authors": [
            {
                "given": "Charles van den",
                "family": "Heuvel",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute, Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Pim",
                "family": "van Bree",
                "affiliation": [
                    {
                        "original_name": "LAB1100, Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Geert",
                "family": "Kessels",
                "affiliation": [
                    {
                        "original_name": "LAB1100, Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Leonor",
                "family": "Álvarez Francés",
                "affiliation": [
                    {
                        "original_name": "Leiden University, Netherlands",
                        "normalized_name": "Leiden University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/027bh9e22",
                            "GRID": "grid.5132.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "cultural studies",
            "relationships",
            "English",
            "graphs",
            "cultural infrastructure",
            "networks",
            "databases & dbms",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In May 2011, designer and digital futurist James Bridle introduced the world to the idea of the New Aesthetic, a kind of emerging art and cultural critique that called attention to our acts of mediation, to the underlying systems and protocols that produce particular computational visualizations, and to the human viewpoints that frame those considerations. For Bridle, the New Aesthetic revealed the increasingly blurred distinctions ‘between “the real” and “the digital”, the physical and the virtual, the human and the machine’ (2012, n.p.), and it made us increasingly aware of a kind of ‘network vision’ manifest when the digital spills into the real—what William Gibson has called ‘eversion’ (2007). To this point, the ‘digital spill’ of New Aestheticism has been predominantly marked by the novelty of pixelated representation (Terrett, 2012)—8-bit graphics (and their overt pixelated qualities) manifesting in cultural spaces, screen glitches, down-sampled satellite imagery, and the blurry-squared edges of render ghosts. But for Bridle, the pixelated imagery serves merely as a kind of visual shorthand, a surface orientation. What the New Aesthetic points to, however, is the underlying systems that produce those pixelated representations, the media in which they are being undertaken (and of which they are reflective), and the human-technology assemblages that allow us to make sense of these things.  As such, Bridle’s year-long curatorial exercise on the New Aesthetic tumblr was more than just a collection of images and artifacts suggestive of an aesthetic of the network itself. Rather, the New Aesthetic oriented us to a moment of contact between what Lev Manovich has labeled ‘Turing-land’ and ‘Duchamp-land’ (1996). It brought together new media art and contemporary art, offering a series of convergences wrought with the dangling threads of the socio-politico-techno-cultural structures that underlie our ubiquitous human-technology assemblages. In so doing, the New Aesthetic helped us attune to design and making practices that sit between new media and contemporary art, as well as attuning to many rooted outside these frameworks (ranging across a variety of industry practices).  With this impact on design and/or our making activities, it should come as no surprise, then, that the field of rhetoric studies would have interest in the practices and purposes (if not emerging paradigm) of the New Aesthetic. For rhetoric has always been concerned with making, with the ways in which we produce discourse and/or artifacts for particular audiences, of particular moments in time, space, culture, etc. And rhetorical scholars, particularly those connected to digital rhetoric, have been increasingly concerned with our acts (and artifacts) of mediation, the systems (computational, cultural, communicative, etc.) that produce or allow for particular types of material and digital utterances and engagement, and, perhaps most importantly, the human-technology assemblages at the center of digital making and what those assemblages mean for rhetorical activities in the 21st century (both within the narrowed lens of digital rhetoric and in the general sense of communication, identification, persuasion, engagement, and the like). As such, if the New Aesthetic can be marshaled toward providing structures or explanatory models that could easily map (into) emerging rhetorical/humanistic practices, then it would have much to offer a range of critical and creative inquiry practices, from those practices featured in targeted subfields like digital rhetoric studies to larger umbrella structures like the digital humanities.  The problem here is that New Aestheticism actually resists the very kind of codification needed for it to be ‘marshalled’ anywhere. This is in keeping with aesthetic turns in general (cf. Rogoff, 2012), but for Bridle, the ‘resistance’ to application is less a matter of avoiding New Aesthetic tropes, and instead is due to the fact that the New Aesthetic is an ongoing process of critical engagement, born of a networked culture, taking place in/on/of networked worlds, and mirroring certain conditionalities of the network itself—which, in Bridle’s view (2013), renders it resistant to codification. Bruce Sterling performatively makes this point in his ‘An Essay on the New Aesthetic’, where he offers over 30 ‘is’ statements trying to grasp and situate New Aestheticism—ranging from ‘The New Aesthetic is a native product of a modern network culture’ to ‘It is rhizomic’ to ‘the New Aesthetic is really a design fiction’ (n.p.). Aside from offering glimpses of the potential qualities of the New Aesthetic and, perhaps more importantly, helping mainstream the New Aesthetic by publishing his essay in  Wired, Sterling’s articulation comes up short in providing a kind of conceptual whole to what the New Aesthetic is or may become. And it most certainly does not provide a set of tropes for employing the New Aesthetic (as hermeneutic or heuristic).   This is perhaps what has made the New Aesthetic so valuable. It generated a lot of interest and critique not because it offered any prescriptive tropes, but rather because it was more significantly concerned with attunement, digital attunement—i.e., with attuning us to the circulating intensities of representation, mediation, and enactment central to our hybrid cybernetic/human cultures (what Manovich would call ‘computer culture’, Beth Coleman ‘x-reality’, and N. Katherine Hayles ‘mixed reality’). In this view, the New Aesthetic isn’t meant to be a revolutionary aesthetic like the aesthetic movements of yore (Dadaism, Futurism, Cubism, etc.), but rather an awareness-aesthetic—one oriented towards opening us to what McLuhan has referred to as ‘the numbed stance of the technological idiot’ (1994/1964, 18).  But for all of its promise as an awareness-aesthetic, no one has been able to articulate what the attunements of New Aestheticism are or how they might inform larger conversations and practices in the digital arts, digital humanities, digital rhetorics, or digital literacies. Part of this tension stems from Bridle’s own hesitation to codify the New Aesthetic, preferring that it reflect the unstabilized and unfixable tenets of the network paradigm from which it emerges—an emergent aesthetic stemming from ‘a new natural order’ (i.e., the network) (Bridle, 2011). While I agree with Bridle’s not wanting to arrest what is, by all indications, an ongoing performance, a process of critical engagement and critique, the many works on the subject (his own as well as the many other commenters: ranging from Bruce Sterling and Catherine David to Joanne McNeil and Ian Bogost) provide enough for us to begin to map temporary contours of the New Aesthetic—a snapshot (perhaps snapchat) of our fleeting, yet-ever-present New Aesthetic moment.  This presentation, then, will attempt to articulate these contours without arresting them. It will work to situate these gestural shapes in relation to rhetorical and mediating practices (from hyperrhetoricity to hypermediacy), and attempt to leverage them in order to foster a frame of engagement that may help us make sense of the increasingly blurred lines between screen-mediated and non-screen-mediated experiences. It will do this in three parts: one, it will identify the conceptual purview of the New Aesthetic (history and trajectory); two, it will introduce and respond (in brief) to key criticisms lobbied at New Aestheticism; and three, it will articulate four contours of the New Aesthetic and explore those contours in relation to current digital rhetoric / digital making practices, with particular attention on issues of design (functional and experiential), representation (particularly the leveraging of metaphors from our engineered devices), and mediation (small-screens practices and mediated being). As the limitations of the talk prevent full development of these concepts and applications, the talk will be designed to showcase the key frames, contextualize their impact, and demonstrate their possibilities as a way to invite further questions, criticisms, and creative responses.  ",
        "article_title": "Mapping the Contours of the New Aesthetic, Opening Considerations for Digital Rhetoric",
        "authors": [
            {
                "given": "Justin",
                "family": "Hodgson",
                "affiliation": [
                    {
                        "original_name": "Indiana University, United States of America",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "other",
            "cultural studies",
            "media studies",
            "digital humanities - nature and significance",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Interchange and Interoperability In his 2011 paper at the Balisage conference, Syd Bauman (2011) draws a clear and useful distinction between two terms that are often lazily conflated:  interoperability and  interchange. Interoperability, he says, involves ‘little or no human intervention’; data is pulled from one site or system and plugged unchanged into another. Interchange, on the other hand, involves human actions, either through direct communication between individuals, or through indirect human mediation in which the re-user of data makes an effort to learn about it through reading documentation supplied with it, and then modifies or transforms it in order to put it to use in his or her own work.  The Text Encoding Initiative, as Unsworth (2011) points out, originally aimed at interchange rather than interoperability, with the ‘I’ in TEI standing in the Guidelines document itself for ‘Interchange’ rather than ‘Initiative’. Perhaps because of the increasing range of big-data projects and tools looking for texts and corpuses they can absorb and process, the focus nowadays is on interoperability and on the difficulties and failures we encounter with it (see, for example, McDonough [2009] for a detailed discussion of the chequered history of interoperability in XML encoding standards used in libraries and archives). But interoperability is hard, especially in the case of literary and historical encoding projects with unique interests and research agendas producing densely encoded texts. The TEI schema is as large and complex as it is because its user community has a wide variety of precise requirements and the need to express them at different levels of specificity. As Bauman reminds us, ‘Interoperability and expressiveness are competing goals constantly in tension with each other’, and encoding with interoperability in mind results in encoding likely to be ‘substandard, less faithful to the document, or outright incorrect’. We obviously do not want this; but when Desmond Schmidt (to take one example) complains that ‘because the choice of tags is guided by human interpretation, TEI-XML encoded files are in general not interoperable’, it is this very expressiveness he is complaining of. In this paper I would like to re-focus on interchange, for several reasons. Interchange is a common way for TEI files to be re-used, and this is especially common in teaching environments and among novice users. Despite this, other than insisting that detailed documentation is essential and expecting re-users to read that documentation, we do little in the way of encouraging and assisting in the re-use of our data. Paying more attention to the needs of users who might want to take and work with our TEI files has a number of significant benefits to the source project, not only in terms of ordinary documentation, but also with regard to clarifying, streamlining, and normalizing practices and schemas. Finally, I would argue that making sincere efforts to enhance the practicality of interchange actually contributes significantly towards enabling interoperability, the much harder problem. I will be primarily concerned with TEI here, but the principles involved are generalizable to any formal data structure.  Barriers to Interchange  If we share our XML at all (and many projects do not), we typically share a single version of it: the version we created and customized in order to be as expressive, as faithful to the data, and as useful for the goals of our project as possible. On the face of it, this is rather strange. Most modern web-based digital edition projects (to take one large subset of the XML encoding world) provide a wide variety of output formats, including various web views (for desktops and mobile devices), printable formats such as PDF, and even plain text. Interactive web views of documents often allow the user to choose between a variety of display options—TAPAS, for instance, allows the user to switch between diplomatic and normalized views, and between ‘TAPAS Generic’ and ‘TEI Boilerplate’ renderings. 1 It is regarded as good practice to try to predict the variety of  users who will visit a project, and to answer their needs as flexibly as possible. And yet, when it comes to our XML data, we do not do this; we typically provide a single XML dump for a document (in the TAPAS case, ‘Download TEI’). In most projects, this XML view is likely full of peculiarities of encoding, rarely used elements and attributes, intra-project links, and other features that, even if well-documented, will present barriers to re-use. A brief analysis of the encoding in our Map of Early Modern London project shows many such issues, from which I examine three:  • Use of many project-specific private URI schemes for linking:  <pb  facs= ‘moleebo:13311|2’ />   • Complex encoding of dates with custom dating attributes: 2   <date  notBefore-custom= ‘1438-08-31’ notAfter-custom= ‘1439-08-30’ datingMethod= ‘mol:julian’ calendar= ‘mol:regnal’ >17thof   <name  ref= ‘mol:HENR2’ >Henry VI </name></date>   • Standoff markup in the form of pointers to personographies, taxonomies, etc. in other files within the project:  <l>The honour due by graue Antiquitie, </l>   <l>Then giuen to  <ref  target= ‘mol:LOND5’ >London </ref>s   <name  ref= ‘mol:DRAP3’ type= ‘org’ >Draperie </name>, </l>   <l>By Royall  <name  ref= ‘mol:RICH2’ >Richard </name>, who in me, </l>  All of these encoding practices are perfectly reasonable, and are well-documented, but they are likely to stand in the way of scholars wishing to take one of our documents out of its surrounding infrastructure and work with it in another context. The solution is obvious: in all of these cases, we can replace the problem feature with a more common encoding. For example, to make our date encoding more broadly useful, we could easily convert the date from the Julian calendar to the proleptic Gregorian, and remove the  @datingMethod attribute:   <date  notBefore= ‘1438-09-09’ notAfter= ‘1439-09-08’  calendar= ‘mol:regnal’ >17th of   <name  ref= ‘mol:HENR2’ >Henry VI </name></date>  Similarly, we can expand the private URI schemes into full URLs, so that  ‘moleebo:13311| 2’ becomes:   <pb  facs= ‘http://eebo.chadwyck.com/fetchimage?   vid=13311&page=2&width=1200’ />  Intra-project links to personographies, gazetteers, bibliographical references, and similar resources can be expanded in similar ways. However, many users will prefer a version of the document that does not depend on resources in linked files. We can provide for this by importing external resources into the appropriate locations in the host document, creating, for example,  <particDesc> containing a  <listPerson> in the header, and a  <listBibl> for a bibliography in the  <back>.  With strategies such as these, we can create a complete, standalone version of the encoded document that eschews unusual project-specific encoding practices. Targeting Specific Output Formats Producing a more standardized and self-contained document is one step on the path to effective interchange, but it will be useful only to a subset of potential users. The TEI schema, even if we restrict ourselves to the more common encoding practices, is still huge and daunting particularly to novice users, and problematic for the tools they are likely to use (TEI stylesheets, collation tools, rendering tools such as TEI Boilerplate, 3 and so on). We can go further by providing output in simpler, more constrained versions of TEI, such as TEI Lite 4 or the nascent TEI Simple. 5  Naturally, there are tradeoffs here. We might characterize such conversions as lossy downsampling; most highly expressive project-specific encoding strategies will suffer in the translation to a much simpler and more constrained format. For example, the current version of the TEI Simple proposal states that ‘We will preclude use of  @rend and  @style’, and ‘We will produce a closed list of values for  @rendition using a pseudo-protocol of “simple:”’. Our project uses  @style and  @rendition extensively with CSS code to describe the appearance, layout, and typography of primary source documents. To convert this to TEI Simple, we will have to create a complex system that analyses that CSS code and converts it to the nearest possible representation in ‘simple:’ tokens, discarding information that cannot be represented. Similarly, Simple (currently) lacks the  <mentioned> element, which we use extensively; this will have to be mapped to a more generic representation, such as  <seg type=‘mentioned’>.  This is an uncomfortable prospect, perhaps, but it does have some positive aspects. First of all, it causes us to evaluate all our encoding practices and to consider whether we really do need to use a particular formulation when something plainer might do. Second, downsampling or simplification in the process of rendering a widely used constrained format constitutes what we might call ‘enacted documentation’; by mapping our encoding to a simpler representation, we provide another level of description of our encoding practice. For example, this template, part of a conversion to TEI Lite, shows how the  <supplied> element, not present in TEI Lite, is replaced with a  <seg> element, documenting in the process not only how the output is produced but how the  MoEML project typically uses  <supplied> and its attributes:     <!-- No <supplied>. Have to make do with <seg>. -->  <xsl:template  match= \"supplied\" >  <seg  type= \"supplied\"  n= \"{concat(@reason, if(@evidence) then concat('; evidence: ', @evidence) else '')}\" >  <xsl:apply-templates  select= \"@*|node()\" />  </seg>  </xsl:template>  It might seem that such conversion processes should be undertaken by the end user, not the source project. However, in dealing with unusual encoding practices or rare elements and attributes, the source project’s encoders will have a much better understanding of what is intended, and how best to represent it in the target output; the example above, for instance, shows that in  MoEML documents,  <supplied> can be assumed always to have  @reason, and often also  @evidence. An outsider coming to the project may not be willing or able to undertake the work involved in reading the project documentation and building an appropriate transformation such as this; and even worse, an automated tool may simply throw away important information without comment. For instance, the Abbot Text Interoperability Tool, 6 in converting one of our project documents to TEI Lite, simply throws away without comment the five  <egXML> elements that are the heart of the document; these could actually be represented quite well using  <eg>, which is available in TEI Lite. When lossy conversions are written by the progenitors of a project’s data, they are likely to be considerably less lossy.  Conclusion While it has been argued that TEI XML has not lived up to its early promise with regard to interoperability, the focus on interoperability (unmediated re-use) has largely eclipsed interchange (human-mediated re-use), which is far more practical in most cases. By putting some effort into providing a range of different versions of our XML source encodings, targeted at different use-cases and user groups, we can greatly facilitate the re-use of our data in other projects; this process causes beneficial reflection on our own encoding practices, as well as an additional layer of documentation for our projects. Furthermore, by generating specific highly constrained output formats such as TEI Simple, we also help to facilitate interoperability.  In the Map of Early Modern London project, we have been enhancing the variety of XML renderings of our documents, and now provide four different views: ‘Original,’ ‘Standard,’ ‘Standalone’, and TEI Lite; see, for example,  http://mapoflondon.uvic.ca/METR1.htm. TEI Simple will be added when it stabilizes. A further documentation page 7 provides an explanation of how each of these formats is derived, and suggests what kind of end-use it might be appropriate for. We are aiming to present a model example of how we might stimulate interchange and contribute to interoperability. Based on the demonstrable value of outputs created specifically for interchange, I would argue that such outputs should be incorporated as a formal component of plans and grant applications for all digital edition projects, and that granting agencies should be encouraged to look for and value this factor in project plans.  Notes 1. See, for example, http://tapasproject.org/digital-mitford/lettertotntalfourd28oct1821/16145. 2. See Holmes, Jenstad, and Butt (2013) for an argument that sophisticated encoding of historical dates is essential for some types of interoperability. 3. http://teiboilerplate.org/. 4. http://www.tei-c.org/Guidelines/Customization/Lite/. 5. https://github.com/TEIC/TEI-Simple. 6. http://abbot.unl.edu/cocoon/vicar/Vicar.html. 7.  http://mapoflondon.uvic.ca/xml_outputs.htm.  ",
        "article_title": "Whatever Happened to Interchange?",
        "authors": [
            {
                "given": "Martin",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "Humanities Computing & Media Centre, University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "publishing and delivery systems",
            "digitisation",
            "repositories",
            "scholarly editing",
            "xml",
            "user studies / user needs",
            "digitisation - theory and practice",
            "archives",
            "encoding - theory and practice",
            "sustainability and preservation",
            "English",
            "resource creation",
            "and discovery",
            "standards and interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Stephen Crane (1871–1900), best known for  The Red Badge of Courage, is widely considered an innovative writer of great promise. He died at the age of twenty-eight, having completed five novels, a large number of short stories, some journalism and war correspondence, and two books of poetry. He left behind  The O’Ruddy, an unfinished picaresque novel later completed by Robert Barr (see Hoover, 2010, for an analysis) and several unpublished stories and sketches (some of them unfinished). Considerable uncertainty surrounds the genesis of some of his posthumously published and unpublished fiction.  Crane’s adult life was unconventional and, for the times, rather scandalous. In 1896 he met Cora Taylor, ‘the madam of a pleasure resort, the Hotel de Dream’ (Wertheim, 1997). They traveled together as journalists and then moved to England, living as man and wife, though they never married. Crane contracted tuberculosis shortly after this and died in 1900. After his death, Cora, calling herself ‘Mrs. Stephen Crane’, tried to sell some her own fiction and some of his unpublished fiction, and she is known to have finished at least one of his stories, ‘The Squire’s Madness’ (Bowers, 1969). Opinions differ about how much she had to do with several other stories (Bowers, 1969; Wertheim, 1997; Gilkes, 1960). A contemporary source reported that ‘Mrs. Crane has taken upon herself the labor of completing seven or eight short stories that her husband left unfinished’ (‘The Lounger’, 1901, 199). One biographer puts the number at ‘half a dozen’ (Berryman, 1962, 259). She may also have had a hand in ‘The Ideal and the Real’ and ‘Brer Washington’s Consolation,’ two controversial stories with racist and pro-slavery themes that remained unpublished until 1978 (Linder, 1978, 2). Stephen also apparently had a hand in some of Cora’s work (under her pseudonym, Imogene Carter) as one of the first female journalists (Gilkes, 1960; Berryman, 1962). Finally, a late money-making project of Crane’s,  Great Battles of the World, was researched and partly written by Kate Lyon (the mistress of the writer Harold Frederic), though the full extent of her contribution is unclear (Wertheim, 1997, 136).  Given this rather confused situation, it seems worthwhile to apply modern methods of authorship attribution to the questions surrounding the authorship of at least the following stories:  ‘The Man from Duluth’   ‘The Surrender of Forty Fort’  ‘The Battle of Forty Fort’  ‘A Self-Made Man’  ‘A Man by the Name of Mud’  ‘At the Pit Door’  ‘An Illusion in Red and White’  ‘Manacled’  ‘A Desertion’  ‘The Ideal and the Real’  ‘Brer Washington’s Consolation’ The situation regarding the war correspondence and the war sketches partly by Kate Lyon seems too problematic for solution, as the amount of journalism by Cora or Cora and Stephen is quite small, and there seems to be insufficient additional writing by Kate Lyon for comparison. Crane’s reputation rests on his fiction, however, so that the stories seem the important focus, and there is at least a modest amount of Cora’s own writing to compare with Stephen’s.  Although she was not very successful in selling her own fiction after Stephen’s death, at least ‘What Hell Might Be’ (a short prose poem), ‘Cowardice’, and ‘The Red Chimneys’ were known to have been published (Gilkes, 1960; Wertheim, 1986). After some frustrating searching, I acquired e-texts of all of them. ‘The Lavender Trousers’, ‘Elbridge Carter’s Dream’, and ‘An Old-World Courtship’ were never published, but the manuscripts for these stories exist in the Stephen Crane Collection at Columbia University (they are discussed briefly by Gilkes [1960], who apparently read them in manuscript), and I have used OCR and manual transcription to produce e-texts of them (I am grateful to Columbia University Library for access to this material). Another story, ‘José and the Saints’, was reportedly unpublished and the manuscript lost (Gilkes, 1960, 277–78), but a contemporary source reports that she ‘found a ready market’ for it (‘The Lounger’, 1901, 199), and I have since discovered that it was published in 1902 and is available online. Together, these texts should be sufficient for an attempt to unravel the authorship of Stephen’s final stories. (I also collected two pieces of non-fiction by Cora for comparison—‘Arundel Castle’ and ‘The 17th Regiment of Light Dragoons’—but preliminary testing showed that Cora’s non-fiction is so distinct from her fiction that it is inappropriate for inclusion.) Before attempting to measure Cora’s participation in Stephen’s late fiction, some aspects of this problem require further comment. Stephen’s fiction is quite varied in setting and theme, and contains several groups of more or less closely related stories. More than a dozen stories are set in the fictional Whilomville (loosely based on Port Jervis, New York, where Crane spent part of his childhood). Ten are loosely based on camping experiences of Crane and some friends northwest of Port Jervis. Six are set in Cuba, four concern an infantry regiment in the fictional country of Spitzbergen, and others have Mexican or western settings. Some are satiric, some comedic, some serious, and many treat war in some way. The one story we know Cora completed for Stephen, ‘The Squire’s Madness’, is a parody of a Gothic tale. Given this range of settings and subject matter, it seems important to select stories for testing against the questionable pieces so as not to privilege Stephen or Cora inappropriately. Cora’s seven independent stories are varied as well: besides the brief prose poem mentioned above, two take place at least partly in Mexico, two are set in England (one is a ghost story), one is set in Stephen’s Whilomville, and one is a maudlin story of an affair followed by a suicide. (This is an extreme form of problems identified in Hoover [2004] and, more thoroughly, in Eder and Rybicki [2013].) Fortunately there is an excellent resource in the  Stephen Crane Encyclopedia (Wertheim, 1997) to aid in this effort.  My analysis is not yet complete, but Figure 1 shows some preliminary results, based on eight late Stephen Crane stories and the eight available Cora Crane stories, balanced as well as possible by kind and length. Here, I have divided the stories into sections of about 1,500 words to reduce the effect of size differences.    Figure 1. Cora Crane vs Stephen Crane: 800mfw, 1,500-word sections. Cora’s stories all cluster separately from Stephen’s late stories (initial testing showed a fairly strong chronological development in Crane’s fiction, so I selected stories only from the last four years of his life). ‘What Hell Might Be’ is an outlier almost certainly because it is only 423 words long, as testing in 600-word sections confirms. An analysis based on the 900mfw separates all the sections correctly, and less accurate results, such as the one shown in Figure 2, based on the 800mfw, also suggest that they are distinct enough that it should be possible to detect even relatively small amounts of Cora’s writing in Stephen’s late fiction, if it exists. (One of the two texts by Cora that appears in the left cluster of Stephen’s texts is her half of ‘The Squire’s Madness’.)    Figure 2. Cora Crane vs late Stephen Crane: 800mfw, 600-word sections.  As a first attempt at evaluating Cora’s contribution to Stephen’s late fiction, I tested the same eight stories against the eleven texts of questionable authorship mentioned above, with the results seen in Figure 3.     Figure 3. Cora vs Stephen and 11 questionable texts: 400mfw, 1,500-word sections. The presence of all but one of Cora’s stories in the central cluster, the correct clustering of the two parts of ‘The Squire’s Madness’, and the presence of only one of Stephen’s stories, ‘At the Pit Door’, in Cora’s cluster do not support the idea that she had much to do with the posthumous stories. (‘At the Pit Door’ is a lighthearted sketch about a line outside a theatre; no typescript or manuscript exists.) Furthermore, the appearance of the two racist stories published in 1978 as an outlying cluster suggests that their very heavy dialect make them quite distinct. An analysis of the same texts divided into smaller sections of approximately 1,000 words is shown in Figure 4.       Figure 4. Cora vs Stephen and 11 questionable texts: 500mfw, 1,000-word sections.  This analysis generally supports the previous one. Here only Cora’s ‘Elbridge Carter’s Dream’ clusters with Stephen’s texts, and only Stephen’s ‘An Illusion in Red and White’ and ‘Manacled’ cluster with Cora’s texts. (Crane’s publisher already had both of these stories in January 1900, five months before Crane’s death [Bowers, 1969].) The two racist stories remain outliers. Dropping the two outliers and retesting in 600-word sections gives the results in Figure 5, which tell much the same tale. Here one section each of Cora’s ‘José and the Saints’ and ‘Elbridge Carter’s Dream’ cluster with Stephen, but none of Stephen’s and none of the questionable texts join the Cora cluster. The eleven questionable sections that form a cluster to the left of Cora’s cluster have often been suggested as having possible contributions from Cora, but they are all set in 18th-century Pennsylvania, and one, ‘Ol’ Bennet and the Indians’, was published before Crane’s death. Dividing the same set of texts into sections of approximately 500 words and performing t-tests on all the words with a frequency of three or more yields only 101 significant words (p < .05). Repeating the analysis in Figure 5 using only these words correctly clusters all the sections except one (from ‘Elbridge Carter’s Dream’), and suggests only one section of ‘A Desertion’ and all the sections of the two racist stories as being by Cora. Preliminary Delta testing lends some equivocal support to these attributions and weakly suggests the possibility of Cora’s hand in ‘At the Pit Door’, ‘An Illusion in Red and White’, ‘The Man from Duluth’, ‘A Desertion’, and ‘A Self-Made Man’. A demonstration that Cora was a major contributor to Stephen’s posthumous fiction would have been a more exciting result, but the weakness and inconsistency of the attributions of sections of the stories of questionable authorship to Cora suggests that any part she played was a relatively minor one.    Figure 5. Cora vs Stephen and nine questionable texts: 400mfw, 600-word sections. ",
        "article_title": "Cora Crane's Contribution to Stephen Crane's Posthumous Fiction",
        "authors": [
            {
                "given": "David L.",
                "family": "Hoover",
                "affiliation": [
                    {
                        "original_name": "New York University, United States of America",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "literary studies",
            "text analysis",
            "english studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The use of rare n-grams in authorship attribution has generated considerable recent commentary and polemic. Vickers argues for the expansion of Thomas Kyd’s canon on the basis of 3- to 6-grams not found elsewhere in plays produced during his lifetime but shared by Kyd’s and some anonymous plays (2008). Although Jackson (2008) challenged his method, Vickers has repeated and expanded his claims (2009), arguing that Middleton did not adapt  Macbeth (2010) and that Shakespeare added about 2,600 words to Kyd’s  The Spanish Tragedy (2012). He has also produced a wide-ranging polemical dismissal of computational stylistics in general and the most frequent words in particular (2011). For responses to Vickers’ flawed critique, see Burrows (2012) and Antonia et al. (2014). Here I test his method on Victorian drama.  I have shown that the method is not reliable for similar-sized corpora of late-19th-century novels and modern American poetry (Hoover, 2011; 2012). Vickers has been insufficiently systematic and has not checked his method adequately on texts of known authorship. The huge number of individually rare n-grams ensures that many otherwise unattested n-grams show matches between any authorial corpus and many individual texts, whether those texts are by the same author or not. However, given that authorship attribution methods vary in accuracy with texts of various genres and lengths, it seems important to test Vickers’ method on drama, using a corpus that mimics the shape and size of his corpus of early modern plays. (Plays seem to present special difficulties for authorship attribution because they typically consist almost entirely of the spoken dialogue of multiple characters.) Vickers emphasizes the unusual nature of the London drama scene in the late 1500s—where plays were being produced very rapidly and often collaboratively by a relatively small number of authors—as a reason to expect repeated n-grams to be especially appropriate as authorship markers (2008). He also usually limits his search for repeated n-grams to a restricted corpus of 55 to 75 plays produced during the productive lifetimes of his candidate authors. This initially seems reasonable, but it also inflates his chances of success and underestimates the likelihood of random matches not resulting from common authorship. Consider that ‘We know the titles of 1,500 plays performed between 1590 and 1642, of which only a few hundred survive . . .’ and that ‘the names of barely a dozen dramatists survive’ (Vickers, 2008, 13). Given these facts, rare n-grams found in one author’s often small and/or problematic corpus and an anonymous play but not in a restricted and incomplete reference corpus seem a dangerous indicator of authorship. We cannot know how many of these n-grams occurred in lost plays. Rare n-gram matches can support attributions or suggest the common authorship of anonymous plays, but they cannot provide the conclusive proof that Vickers seeks, claims, and uses to assert the unreliability of probabilistic methods based on frequent words or n-grams. The small corpus and the uncertainty of dates, authorship, and collaboration, combined with substantial spelling variation, suggest that many early modern authorship questions are beyond confident solution. As noted above, rare n-grams have not proven effective for 19th-century fiction or modern American poetry, but extending the test to drama will provide a better comparison with Vickers’ method. I created a corpus of Victorian drama that mirrors his 55-play corpus (Vickers, personal communication) but without its drawbacks. I selected authors with enough plays available to match the authorial corpora of the nine known authors in Vickers’ corpus, and tried to match the lengths of the plays as closely as possible. For example, below are the seven plays by Henry Byron I use to match Vickers’ seven plays by Marlowe (sizes approximate):    I also selected nine plays by nine additional authors to match Vickers’ nine anonymous plays. (None of my plays is a collaboration or is of uncertain authorship, as some of Vickers’ unfortunately are, but about half my authors were involved in collaborations.) To overcome the limitations of Vickers’ corpus, I collected 20 more plays by 10 authors (one to three plays each), for testing. I used WordSmith Tools (Scott, 2012) to collect the 3- to 6-grams appearing at least twice in the 75 plays (about 173,000; n-grams appearing only once obviously cannot show matches). For my first test, I removed Buckstone’s six plays from the 55-play corpus, leaving 49 as the reference corpus. I tested a four-play Buckstone corpus against his two other plays and the 20 remaining plays by others by deleting all 3- to 6-grams present in the reference corpus (leaving 37,000), and then deleting all those absent from the Buckstone corpus. I calculated the number of matches between the remaining 6,500 Buckstone n-grams and the 22 test plays. The two remaining Buckstone plays have 363 and 241 n-gram matches with his corpus that are not found in the reference corpus (Vickers’ test). The 20 other plays have 59 to 254 such matches, and two have 241 or more, showing that plays by other authors can equal or outscore those by a candidate author. The results for Taylor (also with a six-play corpus, testing a four-play corpus against the remaining two Taylor plays and the 20 test plays) are worse: nine plays have more matches with Taylor than one of his own plays does. Although Vickers reports raw numbers of matches, as I have just done, the variation in length among his plays and mine makes this seem unwise, so Figures 1 and 2 graph the numbers of matches divided by text length for the Buckstone and Taylor tests above. Two other plays outscore both of Buckstone’s test plays, and three outscore the other (Figure 1). For Taylor, the results are a bit better, with only one play outscoring one of Taylor’s (Figure 2). These results do not, however, suggest a conclusive method.    Figure 1. N-grams absent from the reference corpus with matches between the Buckstone corpus and 22 plays.    Figure 2. N-grams absent from the reference corpus with matches between the Taylor corpus and 22 plays. I created a larger reference corpus for further testing by collecting 45 more plays, for a total of 100—86 in 14 authorial corpora of two to 17 plays each, and 14 by 14 authors not otherwise represented in the corpus. To mirror the case of the 2,600 words added to  The Spanish Tragedy, I also collected two more full plays and 15 sections of 2,600 words by the 14 authors of the 86 plays (including at least one for each of the 13 authors with at least three plays), and eight sections of 2,600 words by eight additional authors. The goal was to test what might have happened had the corpus of early modern plays been larger and to see how sections by authors from outside the corpus compare to authors’ sections by authors in the set.   I again collected all the 3- to 6-grams appearing twice or more in the entire corpus (206,000), removed the Buckstone authorial corpus, and treated the remaining plays in authorial sets as the reference corpus. I deleted the n-grams found in the reference corpus, then retained only those found in a 6-play Buckstone authorial corpus and checked for matches between Buckstone and the remaining texts. I did the same for Taylor. The results can be seen in Figures 3 and 4.    Figure 3. N-grams absent from the reference corpus with matches between the Buckstone corpus and 39 other texts.    Figure 4. N-grams absent from the reference corpus with matches between the Taylor corpus and 40 other texts. The Buckstone test succeeds, with all three of the 2,600-word sections of his test play as well as the entire play showing more matches with Buckstone than any of the other texts do, though it does not make a strong case for the certainty Vickers claims. The Taylor test fails badly: the whole Taylor play only barely outscores one other text, and the 2,600-word section trails behind 15 other texts. Tests on two of the other five authors with at least a six-play corpus are clearly successful, and tests on two fail badly. Clearly, in a real authorship test, this method will often produce strong erroneous attributions. 1  Finally, I tested twenty-one 2,600-word sections against all seven authors with at least a six-play corpus and against a composite ‘author’ comprising six plays by six authors (from among the original 20 test plays). Seven sections are by the seven authors and the remaining 14 by other authors; obviously, none is by the composite author. As Figure 5 shows, the tests for Bernard’s and Taylor’s sections fail; Bernard’s corpus is outscored by four other authors, including the composite one, and Taylor’s is outscored by two. The Phillips and Poole tests are very successful, and the correct author has the highest proportion of matches for the remaining three authors, but none of those cases is clearly distinguishable from the results for tests on sections not by any of the seven authors. And it is quite disconcerting that the composite author often scores first or second. Furthermore, these results are artificially successful because I limited each authorial corpus to exactly six plays. When larger authorial corpora are tested, the largest often outscores most or all of the other authors for most sections, showing the importance of the size of each author’s corpus to this method. As welcome as a conclusive method for authorship attribution would be, especially for relatively small amounts of text, the rare n-grams method seems unlikely to provide it.    Figure 5. N-grams absent from the reference corpus with matches between the eight authors and twenty-one 2,600-word sections of plays by 21 authors.  Note 1. Because of the unusual nature of the rare n-grams test, it is difficult to compare results using other methods. However, in a Delta test on authors in Vickers’s 55-text corpus, including only the seven authors with three or more texts and testing 30 texts as if they were unknown (21 whole plays and nine sections of about 2,600 words by one of the authors in the set), 23 texts are correctly attributed to their authors. Among the seven failures are four of the nine short sections. Curiously, there are three errors for Buckstone in the Delta tests, but none for Taylor, reversing the results of the rare n-grams test. ",
        "article_title": "Rare N-Grams, Victorian Drama, and Authorship Attribution",
        "authors": [
            {
                "given": "David L.",
                "family": "Hoover",
                "affiliation": [
                    {
                        "original_name": "New York University, United States of America",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "drama",
            "literary studies",
            "corpora and corpus activities",
            "text analysis",
            "english studies",
            "English",
            "poetry",
            "genre-specific studies: prose"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or R script, and rarely described, discussed, or even mentioned. For ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. Furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. Finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. I suggest that we should pay a little more attention to the theory and practice of tokenization. 1  Consider a hypothetical case. Let’s say I want to analyze 5,000 novels, have access to the texts at HathiTrust, download 5,000 novels in plain text, and tokenize them. Below is part of a page from Elizabeth Gaskell’s  Cranford, from HathiTrust (Gaskell, 1910 [1851], 107):     Figure 1.  Cranford, Elizabeth Gaskell, from page 107.  A human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor OCR problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). I tokenized this passage with The Intelligent Archive (2012), KWIC (Tsukamoto, 2004) WordSmith Tools (Scott, 2012), Voyant (Sinclair et al., 2012), and Stylo (Eder et al., 2014). 2 Even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. KWIC and WordSmith produce identical lists, as do Voyant and Stylo, but neither of these match The Intelligent Archive.  Now consider Charles Chesnutt’s  The House Behind the Cedars (1900, 13), also from HathiTrust:     Figure 2.  The House Behind the Cedars, Charles Chesnutt, from page 13.  The dialect in this passage is challenging even for human readers, and the OCR is more problematic. For example, the printed text (judging from the PDF) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” This classic OCR problem occurs several times in this novel. And in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common OCR problem. Those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts. The use of apostrophes in the Chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. Although The Intelligent Archive, KWIC, and WordSmith Tools produce exactly the same lists for this brief passage, and Voyant has the same number of types and tokens, Voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. Stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in  ain^t, gentleman^s, and  spen^s (replaced with a caret). It produces six more tokens and four more types than the other programs, and many more differences in the word list. Unfortunately, in Chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by Voyant and Stylo are quite inaccurate. Furthermore, only KWIC and WordSmith Tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. Only WordSmith Tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.  Obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. Worse yet, these problems are found even in relatively carefully edited texts like those from Project Gutenberg. Although Gutenberg’s  The House Behind the Cedars does not have spaced contractions, and correctly has  he’s in the first line and  you’ll in the final line, the 29 initial and final dialect apostrophes remain problematic. The Gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. The Intelligent Archive and Stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but KWIC, WordSmith Tools, and Voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. The situation is still more complex if a double-hyphen is preceded or followed by a breaking character. If this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (Dickens’  Dombey and Son has more than 2,200). And this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (For a practical example of the effects of error, see Matt Jockers’ discussion of topic modeling and several ‘topics’ that arose from OCR error and metadata (Jockers, 2013, 135).  It might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. In some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot.  Let’s consider a few further tokenization questions: He said, “That’s ’bout ‘nough, sho’.” “That’s ‘bout’, not ‘fight’; ’nough said,” Nough said. “John tried that ‘Nough told me to’ on me,” Bill whined. He remarked, “John said, ‘Bout starts at nine.’” He remarked, “John said, ‘It’s ’bout time.’” He remarked, “John said, ‘‘Bout time.’” Can these apostrophes/single quotes be handled correctly computationally? How about the two single quotes before ‘Bout’ in the last example? I visited the U.S.S.R. Four tokens? Seven? Is the final period part of the final token? I visited the U.S.S.R.! Four tokens? Seven? Is the final period part of the final token? Is that C------? Is ‘C------’ the token ‘C’ followed by a dash, or the token ‘C------’? What about ‘C—’? Or ‘C-’? C------ is here. Same questions. Oh d--n it! Is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? How about ‘d---n’? or ‘d-n’? or ‘G-d’? I said--never mind. If ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here? That’s what I--a mistake, sorry. How do we get ‘d--n’ correct without failing here? You’re a real %#@$! Three tokens? Four? Does the last include the final ‘!’? What if there were a period after the ‘!’? You’re a real %#@$!. How about now? I am working on a Python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but I despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. For many years I have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems. Perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and Maciej Eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. And improving the quality of the corpus had a relatively small effect on the attribution of the Federalist Papers (Levitan and Argamon, 2006). More study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. For smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. Consider Ramsay’s (2011) analysis of  The Waves, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see Hoover [2014a] and Plasek and Hoover [2014], for discussion). Another example that replicates an experience I have had several times is that a Full Spectrum analysis (Hoover, 2014b), based on Craig’s version of Burrows’s Zeta (Burrows, 2007; Craig and Kinney, 2010) can give strange results if uncorrected texts are inadvertently included. For example, in a test of Charlotte Brontë versus Anne and Emily Brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of Anne and Emily in the analysis both used single quotation marks for dialogue.  Far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis. Notes 1. As a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the NLP community. For example, Dridan and Oepen (2012) and Chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. Even if the problems had all been solved within the NLP community (a fact not in evidence), however, this would not diminish the force of my argument for the DH community, where there has been much less attention paid to them. 2. These programs represent a variety of those used in DH work (in order): a mature Java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from OUP, a widely used online tool, and a recently developed set of tools written in the currently popular R. ",
        "article_title": "The Trials of Tokenization",
        "authors": [
            {
                "given": "David L.",
                "family": "Hoover",
                "affiliation": [
                    {
                        "original_name": "New York University, United States of America",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "natural language processing",
            "programming",
            "text analysis",
            "English",
            "software design and development",
            "standards and interoperability"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The approach of digital humanities has widely interested many humanists from all disciplines. We can see it in the new methodologies introduced at DH2014 in Lausanne, where more than 700 registered participants gathered from around the world. However, digital humanities are still new practices and, in many cases, unachievable practices for many humanists. In the international workshop ‘New Perspective on Comparative Medieval History: China and Europe, 800–1600’, which took place in Oxford in January 2014, one discussion about ‘Isn’t the Siku Quanshu (Database) Enough?’ 1 reflected a common but critical debate between two groups of humanists. Scholars are satisfied with large commercial text databases, and they question why their colleagues invest their research time in data preparation (for example, encoding research texts in TEI) for computational analysis rather than read through the entire search results returned by databases. In the specific case of encoding texts in TEI, scholars often find themselves spending years doing manual encoding before computational analysis can be applied, despite the fact that the TEI standard has already saved a lot of work in schema design. We propose that in addition to defining a standard schema for encoders, efforts must be made to develop semi-automatic markup tools to speed up the tagging process.   In this paper we will introduce MARKUS, a semi-automatic markup platform that is designed to automate the markup process of different kinds of named entities in the domain of classical Chinese (historical) texts. In particular, we focus on the possibility of providing an infrastructural but low-cost and thus sustainable markup service that is solely built upon JavaScript and HTML5, both extremely basic and well-supported technologies.  MARKUS was developed as a tool to speed up the tagging process for the Communication and Empires project (http://chinese-empires.eu), which applied the TEI-markup approach to a corpus of 112 notebooks of the Song dynasty of historical China. We manually tagged quotes, interlocutors, authors, titles, and topics for each entry of five notebooks following the TEI standard. However, based on the above tagging experience, we realized that if we wanted to analyze all the people mentioned in the texts, it would be simply impossible to manually tag them within the limited time of the project; every hour we could only finish approximately six to seven tags manually. This labor-intensive tagging process is a common barrier for humanists researchers interested in putting the digital humanities approach into practice. Therefore, MARKUS aims to be an infrastructural, user-friendly, openly available, and sustainable markup service for Sinologists to overcome this barrier of encoding texts.  MARKUS currently provides three markup functions to help its users to tag classical Chinese texts: automated markup, keyword markup, and manual markup. Instead of providing a centralized and powerful (often complicated) web application, we try to make the service easy to operate by separating each markup function into different single web pages as a single task (Figure 1). In the meantime, all the web pages still share consistent interface design (Figure 2). Users can focus on a single task at a time while still following our step-by-step workflow to accomplish the entire tagging process.   Figure 1. The step-by-step workflow interface.    Figure 2. All markup functions follow a consistent interface design.  The workflow starts with uploading a text file to MARKUS (step 1). After the text is loaded, the user can use the automated markup function (step 2.a) to scan all named entities known to the system. Then the user can choose to apply keyword markup (step 2.b) to scan and tag texts against a list of terms or a regular expression given by the user. At the last step (step 2.c), the user can verify and refine all the markups manually.  The automated markup function of MARKUS is currently capable of identifying commonly needed types of named entities in Chinese historical research. MARKUS is built in with 355,000 personal names, place names, temporal references, and official titles based on the results of other digital projects, namely the China Biographical Database (CBDB; http://isites.harvard.edu/icb/icb.do?keyword=k16229) and the China Historical GIS (CHGIS; http://www.fas.harvard.edu/~chgis/). Named entities for more specific research interests—for example, terms collected in the Buddhist Studies Authority Database Project (http://authority.ddbc.edu.tw/)—will be incorporated.  MARKUS uses a color-coding scheme to display markups according to their tags. MARKUS also associates tagged texts with their sources by an identifier defined in the sources in order to provide better interoperability between projects. For example, MARKUS associates a personal name with its CBDB person ID, so that users can link to CBDB to get more information about the person.  The built-in lists of named entities only covers the basic needs of Sinologists. Alternatively, MARKUS allows users to upload their own lists of terms and to write regular expressions to tag texts with observable patterns. After the automated and/or the keyword markup, users can validate, add, or remove markups manually while reading through the text. A range of online reference tools and dictionaries (CBDB, CHGIS, ZDICT, 2 and Wikipedia) have been integrated into the interface to assist the reading and validating process. We also provide a batch edit function to speed up the process of removing or adding a tag throughout the text in a batch. The markup result can be saved as a MARKUS file or exported as a HTML or a TEI file. Users can also further choose to export all the tags along with the passage identifiers to a tabular file (CSV, HTML table, or Excel file) to conduct further statistical, temporal, spatial, and social network analyses.    MARKUS is still in its early development stage. More functionality, such as visualizing the markups in charts, maps, or a timeline, will be added to provide an infrastructural markup service for Sinologists. MARKUS is freely accessible via http://dh.chinese-empires.eu/beta, and the source code will be released at the end of the project.  In order to make MARKUS as sustainable as possible, we chose to develop MARKUS as a non-centralized web application with the most basic technologies. It is solely written in JavaScript and HTML5 without any server side scripts, 3 meaning it can be hosted in any free web hosting service, even those with the most basic facilities. It is then quite possible to provide the MARKUS service for a long term without any funding. However, this also limits the computing power that MARKUS can provide. During the upcoming development phase, while MARKUS is still funded, we plan to extend MARKUS to include more advanced markup functionalities such as applying machine learning and text mining techniques for automatic markup. It will require a dedicated server to provide higher computing power and online storage for uploaded texts, which will lead MARKUS toward a server-side implementation.  Funding This work was supported by the Arts & Humanities Research Council and the National Endowment for the Humanities. Notes 1. Siku quanshu is a one of the largest digital corpora of classical Chinese texts for Chinese cultural studies. It has been digitized as a commercial database. The discussion has been depicted in a blog post by Hilde De Weerdt with her view on this common debate (De Weerdt, 2014). 2. ZDIC (http://www.zdic.net/) is an online Chinese dictionary. 3. MARKUS requires HTML5 web worker (http://www.w3.org/TR/workers/) and File (http://www.w3.org/TR/FileAPI/) API to provide a better user experience in heavy computation process and file loading/saving functions. JQuery (http://jquery.com) and Bootstrap (http://getbootstrap.com) are the major JavaScript libraries used in MARKUS. ",
        "article_title": "MARKUS：a Fundamental Semi-automatic Markup Platform for Classical Chinese",
        "authors": [
            {
                "given": "Hou Ieong",
                "family": "Ho",
                "affiliation": [
                    {
                        "original_name": "Leiden University, The Netherlands",
                        "normalized_name": "Leiden University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/027bh9e22",
                            "GRID": "grid.5132.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "interface and user experience design",
            "English",
            "software design and development",
            "digital humanities - facilities",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The year 2015 marks the 100th anniversary of  New Youth magazine (1915–1922), the single most influential periodical in the history of modern Chinese literature and language. It was in  New Youth that the progressive writers advocated to replace classical Chinese with written vernacular as the standard of modern written Chinese language. At first, just like other literati in late the Qing and the early republic, they wrote most of their arguments on language changing in classical Chinese. The dramatic shift of language happened after the May Fourth movement in 1919, and the practice of writing vernacular in every genre spread from the magazine to other publications across the country, finally changing the landscape of written Chinese for good. For a century, scholars have emphasized the importance of  New Youth but failed to answer questions such as how long it took for the literati as well as the general public to accept the use of a new written standard and exactly how the new standard spread. Teaching computers to distinguish between classical and written vernacular Chinese may make it possible to bring in more digitized texts from that period to study and find the answers.   In this study, we adapt the concept of statistical learning theory and propose two types of quantitative analysis, supervised learning and unsupervised learning, to explore the writing style of classical Chinese and modern Chinese. These two types of methods differ in whether the researchers provide keywords. The unsupervised methods, unlike the supervised methods, are data-driven and determine keywords based on the data attribute and pattern. Since there is no standard operating procedure for applying the unsupervised methods, we will apply the idea of species diversity to determine the keywords.  Supervised learning has long been used in linguistics to differentiate authorship via keywords, and we choose 10 function words each for classical and modern Chinese as the keywords. Together with a proposed dispersion measure G* index, we can evaluate the trend of function words used in volumes 1 and 11 of  New Youth magazine. For example, the following graph (Figure 1) shows the trends of G* indices for 20 function words chosen. Note that, if the G* curve of a function word is always under the diagonal line, then the frequency of this word is increasing. On the other hand, the G* curve of a function word over the diagonal line indicates that the word frequency is decreasing. In other words, the function words of classical Chinese become infrequent while those of modern Chinese become more common in  New Youth magazine.      Figure 1. G* curves of 20 function words (black line: classical Chinese; red line: modern Chinese). For the unsupervised learning, we will focus on comparing species (or vocabulary) richness, unevenness, and sentence structure. We use numbers of words and vocabularies to measure the richness. There are quite a lot of unevenness measures, such as Gini’s index, entropy, and Simpson’s index, and they are often used for measuring the statistical dispersion. The analysis of sentence structure includes its length and the punctuation in a sentence. We will use an example to demonstrate our approach by judging the species richness and unevenness. Table 1 shows the related statistics of 11 volumes of  New Youth magazine. The number of words is increasing from volume 1 to volume 11, but the number of vocabularies shows a decreasing pattern. This indicates that the species diversity becomes lower. The unevenness measures reveal similar information, and earlier volumes have higher species diversity. Note that a smaller value of Simpson’s index (and a larger entropy value) indicates higher species diversity. Based on these analyses, it seems that later volumes have lower species diversity and people can read the articles without recognizing many words, which matches the purpose of the May Fourth Movement.    Volume Words Count  Vocabularies Count  Simpson Index Entropy   1 248,833 4,379 0.004568 6.654036   2 291,848 4,344 0.004500 6.649539   3 290,038 4,227 0.004954 6.541824   4 305,020 4,298 0.004172 6.539378   5 343,519 4,125 0.004672 6.461579   6 389,407 3,848 0.005749 6.348547   7 586,942 3,850 0.006053 6.328604   8 461,731 3,753 0.006035 6.320355   9 437,748 3,745 0.005574 6.322103   10 342,778 2,980 0.005700 6.177278   11 489,223 3,093 0.005712 6.212699   Table 1. Descriptive statistics for  New Youth magazine.  In addition, we will apply the measures in this study and use them to classify an article for whether it belongs to the group of classical or modern Chinese. First, the articles from all 11 volumes of  New Youth magazine are identified manually into the groups of classical or modern Chinese. Next, we use cross-validation to determine feasible classification functions (and the best one, if possible). We separate the articles in  New Youth magazine into two sets: training data and testing datasets. The training data are used to select important variables and construct the classification function, and this function is used to determine the labels of the testing data. We then calculate the classification error (or prediction error) of the testing data, and the function with the smallest error is treated as the best model. The process of cross-validation is often repeated several times (e.g., 1,000 times) to reduce the influence of chance. Usually we choose the classification function with the smallest average prediction error from 1,000 replications. Also, we can check the important variables in the classification function and examine possible interpretations of these variables. We expect the interpretations are helpful in differentiating the styles between classical Chinese and modern Chinese.   ",
        "article_title": "Identifying Classical and Written Vernacular Chinese Language: A Statistical Study based on Texts from New Youth Magazine (1915-1922)",
        "authors": [
            {
                "given": "Li-hsing",
                "family": "Ho",
                "affiliation": [
                    {
                        "original_name": "National Tsing-hua University, Taiwan, Republic of China",
                        "normalized_name": "National Tsing Hua University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00zdnkx70",
                            "GRID": "grid.38348.34"
                        }
                    }
                ]
            },
            {
                "given": "Ching-Syang Jack",
                "family": "Yue",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Wen-huei",
                "family": "Cheng",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "natural language processing",
            "English",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  What Is Old Is New Again There are various approaches to digital reproductions of old artifacts. One typical approach, for example, is to make a careful scan of a rare book to preserve the book’s look and feel in high-resolution images. Other approaches, however, aim to extract as much as information from the artifacts in order to be processed by machines. Such an approach will instead convert (manually, automatically, or by a mix of manual and semi-automatic procedures) the content of a book into pure text so that it can be indexed and text-mined by computers. The digital representation of an old book is its computer text. Such a representation opens up opportunities for further reuse (e.g., audio books, hyperlinked pages, machine-translated scripts, etc.).  When working on old maps, researchers can take the latter approach: One can strike to retrace the contours and elements depicted in the maps and convert them into interlinked digital objects. Old routes in ancient atlases become road networks in online maps supported by GIS (Geographic Information Systems). We can then make travel plans as if we were living in the past by using information extracted from the old maps. The routes, however, can be calculated with the help of computer algorithms. A well-known autography can be taken apart and its essentials re-created by repurposing modern-day social network services. Researchers and students can now indulge in an interactive chronology of the people, social circles, and events described in the book. This short paper is a preliminary report of our experience in using OpenStreetMap to digitize and put online a small portion of Taiwan Baotu. OpenStreetMap is both a grassroots, collaborative effort in global map-making, and a reliable web map service that is free for all to use (Ramm et al., 2011; Haklay and Weber, 2008). We modify and reuse the software behind the OpenStreetMap’s web map service and apply it to geospatial data digitized from old maps. We hope that what we do will come to exemplify a new approach to represent and repurpose old maps; we aim to create online resources from ancient maps where old traces become new links. Further, the links can be collaboratively added and forged when such online resources are shared and reused. Note that OpenStreetMap has also been adapted for other purposes. In many of these cases, datasets from elsewhere are overlaid over OpenStreetMap, and it becomes convenient to browse them as themed annotations attached to a certain geographic area of interest. Often the online OpenStreetMap service is used as the source of the background map; this necessarily generates a modern map of the area (Amat et al., 2014). Such an overlaying method is frequently applied to other web mapping services as well, so that online maps enriched with collections of cultural objects (in the forms of digital images, web links, etc.) can be viewed and shared by many (Presner et al., 2014). In this work, instead, we give new lives to old maps so that they can be the background maps, be interacted with, and be enriched with other datasets. However, as OpenStreetMap depends on the use of precise geospatial coordinates, our approach is accordingly limited in its applicability to ancient maps that cannot be recoded using modern coordinate systems.   Why Taiwan Baotu? The Taiwan Baotu is a set of 457 topographic maps covering a major part of Taiwan. Each map is 1:20000 scaled. It was produced in 1904 when Taiwan was under Japanese rule. The collection was first published in 1906; another edition with redacted place names was released in 1920. Taiwan Baotu was derived from the output of an island-wide land survey, but had also incorporated other types of geographic information. The maps illustrate administration areas with their detailed boundaries. They contain place names, as well as land-use, transportation, landmark, and other information about Taiwan in the early 1900s. At Academia Sinica, Taiwan Baotu was previously scanned as high-quality images. These images are available as online services from which they can be used together with other map images. Like most web-based map collections, they are delivered as map tiles to user browsers to be examined. Intrinsic details in Taiwan Baotu, such as administrative boundaries and place names, are not available as machine-processable datasets. Parts of the Taiwan Baotu had also been digitized using GIS software package. The traced contours of the maps are stored as layers of vector data (in the Shapefile format, to be exact). As such, they can be visualized and analyzed by various GIS software, but such activities cannot be easily performed and coordinated over the Web. As we have been participating in a multidisciplinary project relating to the Taijiang Inner Sea Area (Tainan, Taiwan), we aim to represent Taiwan Baotu as a web of interlinked resources upon which researchers can further enrich and collaborate with one another.    OpenStreetMap as Infrastructure OpenStreetMap is a mass collaboration on mapping the world as it is now. We aim to learn from its success and use its technical infrastructure to represent the Taijiang Inner Sea Area as it was in the past. OpenStreetMap probably is best known for its practices and tools for collaborative map-making. In our work, we have not used much of the collaborative part of OpenStreetMap but rely on its underlying sub-system for map rendering, tile serving, and overlapping. To represent Taiwan Baotu in OpenStreetMap, we first took as input an existing collection of Shapefile layers. We renewed, merged, and converted these layers into new datasets following the OpenStreetMap data format.  An OpenStreetMap system is built upon a collection of open-source software packages: a database for storing map data, a rendering system for converting data into map tiles, and a tile server. The tile server responds dynamically to a user’s request for the map of a certain area and of a particular scale. OpenStreetMap software is released under free/open-source software (FOSS) licenses; everyone can freely use and modify the software to serve one’s needs (as long as the software license is respected). This means that the experiments we have done with Taiwan Baotu with OpenStreetMap can be reproduced and validated by others; the experience can be shared in the research community (which is not true if proprietary software or a proprietary service is used).    Taiwan Baotu in OpenStreetMap Our current work focuses on Redrawing Taiwan Baotu, converting map data for use in OpenStreetMap, and providing an interactive layer on top of Taiwan Baotu in OpenStreetMap. These can be applied to any historical maps with definite geospatial coordinates.    Figure 1. Redrawing maps in QGIS. First, QGIS is chosen as the tool to redraw the historical maps (see figure 1). The redrawing is actually an iterative process: Feedback from the OpenStreetMap end will trigger editing at the QGIS end, and the data modified with QGIS is again fed to OpenStreetMap for visualization and verification. The result is saved in the ESRI Shapefile format. Second, we convert datasets from the Shapefile format to the OpenStreetMap format. We developed a program to do the conversion automatically; we also provided a web form for people to upload and convert Shapefile datasets so as to try to bridge the various gaps between the researchers and the programmers. There are several existing conversion programs, which are unable to perform this job because our datasets contain Chinese characters. Our program supports Unicode and is able to import the result directly into the OpenStreetMap database after a map dataset is converted. Once the dataset is converted, it becomes readable and editable by simple text editors. Then the OpenStreetMap service will convert map datasets into map tiles that are then sent to display in user browsers.    Figure 2. A comparison of a paper map and its digital representation in OpenStreetMap. Finally, we built an interactive map of Taiwan Baotu on which users can contribute data on top of the visualized map. Figure 2 shows paper and digital maps of Taiwan Baotu in the same screen. We use this platform to review the quality of the redrawn map: one can compare the difference between the paper map and the digital map. We also provide a platform for users to connect external data to the historical map on display.    Figure 3. Attaching external information to a historical map. The external data is usually a URL to an article, an image, or a video accessible over the Internet. We provide a form for users to fill in the URL of an external resource that is related to a location on map (see Figure 3).   Openness in Research and Scholarship Our method represents a workable and reproducible approach for the digital representation of historical maps where intrinsic map details are kept and opened up for reuse. As we keep much information in the maps in digital and interactive forms, these historical maps become more helpful for the historians, researchers, teachers, students, and others who are interested in the maps and wish to contribute to the development of digital humanities. Recent research and observation about research data sharing and reuse (of which maps are but one category) shows that open scholarship is the norm. Formal and informal scholarly communication (sharing and reusing map data offer one example) is converging, and open access to data is a paradigm shift (Amat et al., 2014). Our methods are small but concrete steps to open up historical maps for better sharing and reuse.  ",
        "article_title": "Old Traces, New Links: Representation of Taiwan Baotu in OpenStreetMap",
        "authors": [
            {
                "given": "Jheng-Peng",
                "family": "Huang",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taipei, Taiwan; National Taiwan University of Technology and Science, Taipei, Taiwan, Taiwan, Republic of China",
                        "normalized_name": "National Taiwan University of Science and Technology",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/00q09pe49",
                            "GRID": "grid.45907.3f"
                        }
                    }
                ]
            },
            {
                "given": "Hao-Syong",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taipei, Taiwan",
                        "normalized_name": "Academia Sinica",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bxb3784",
                            "GRID": "grid.28665.3f"
                        }
                    }
                ]
            },
            {
                "given": "Hsiung-Ming",
                "family": "Liao",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taipei, Taiwan",
                        "normalized_name": "Academia Sinica",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bxb3784",
                            "GRID": "grid.28665.3f"
                        }
                    }
                ]
            },
            {
                "given": "Tyng-Ruey",
                "family": "Chuang",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica, Taipei, Taiwan",
                        "normalized_name": "Academia Sinica",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bxb3784",
                            "GRID": "grid.28665.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "repositories",
            "archives",
            "geospatial analysis",
            "digitisation",
            "historical studies",
            "English",
            "interfaces and technology",
            "and Open Access",
            "history of Humanities Computing/Digital Humanities",
            "software design and development",
            "copyright",
            "maps and mapping",
            "visualisation",
            "licensing",
            "data modeling and architecture including hypothesis-driven modeling",
            "programming",
            "sustainability and preservation",
            "resource creation",
            "knowledge representation",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The issue of doubtful and wrong attributions of the great number of Chinese canonical texts that were translated from Indian originals during the Eastern Han dynasty (25–211 C.E.) to the Tang dynasty (618–907 C.E.) has been much debated in the field of Sinological and Buddhist studies over the last few decades, e.g., by Zürcher (1991), Harrison (1993), and Nattier (2008). Scholarship has increasingly become aware of the fact that the authorship—or, more precisely, translatorship—of the early Chinese translations recorded in the canonical catalogues and in a number of historical records is quite often unreliable. Although not strictly speaking a case of suspicious translatorship attribution, the content of the fourth division of the Chinese translation of the Indian Dīrgha-āgama (長阿含經, Taishō 1) appears at odds with the rest of the Dīrgha-āgama collection, and scholars of early Buddhist texts have reasons to suspect that this section may have been attached to the rest of the collection at a later stage. The Dīrgha-āgama is a collection of early Buddhist discourses, preserved, in addition to the Chinese version transmitted by the Dharmaguptaka school, in Pali and Sanskrit. The Pali and Sanskrit recensions were transmitted, respectively, by the Tharavāda and (Mūla-)Sarvāstivāda schools. In terms of content, Taishō 1 closely corresponds to its Pali counterpart, the Dīgha-nikāya, and the extant Sanskrit version. There is, however, a major structural discrepancy between the Chinese translation and its parallels, in that the Chinese translation has four main divisions, instead of three. The fourth division is actually a single discourse, DĀ 30 (世記經), in five fascicles, which in terms of content is also out of place in a collection of early Buddhist discourses. In view of the activity record of the main translator of Taishō 1, Zhu Fonian (竺佛念) (Nattier, 2010; Anālayo, 2013; Hung, 2013), it seemed worthy investigating whether the redactor responsible for the addition was Zhu Fonian. To investigate this hypothesis, we apply quantitative translatorship analysis methods to analyze the text. The main idea of methods based on quantitative approaches to translatorship analysis is to check whether different texts have similar (translation) style, i.e., similar vocabulary usage pattern(s). After quantitative analysis, we study the translation phrases that have been identified by the algorithm in detail. This qualitative form of analysis is necessary to seek the confirmation of our judgment based on the quantitative testing result. Thus our analysis combines quantitative testing with a qualitative approach to the data. Research Method The digital text of Taishō 1 is taken from the CBETA 2011 corpus. 1 The Taishō 1 collection consists of 22 fascicles and contains 30 different discourses. The discourse in question is the last discourse, DĀ 30, which, as mentioned above, consists of the last five fascicles of Taishō 1. For our analysis, we remove from the available digital text the entire markup, front and back matter, as well as all the headings added by the editors.   In order to extract significant and meaningful units, which are then used as stylometric measurements, the Chinese sentences are first tokenized with Variable Length N-gram algorithm (abbreviated as VL-ngram, cf. Hung et al., 2009). VL-ngram is an enhanced form of traditional n-gram extraction algorithms. In the VL-ngram algorithm, instead of using fixed-length grams, we first generate all possible grams from our texts—i.e., all bi-grams, tri-grams, quad-grams, and so on—up to the longest possible n-gram. Then we remove all non-significant grams from the features set. To do so, we define an arbitrary number of documents, referred to as the ‘document threshold’, within which a gram must appear as a threshold to merit inclusion in the features set. With the proper setting of the document threshold, we are able to avoid the problem of selecting too many content-related words for inclusion in the features set, which would bias the analysis toward content analysis rather style analysis. Afterward, we use Principle Component Analysis (PCA) on the extracted profiles. The PCA is a statistical procedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called  principal components. We then plot the values of first and second components in 2-d charts and check whether the translation style of the disputed part of, for example, a scriptural collection or a single work is consistent with that of other parts in the same collection or work.    Analysis Results  In the discussion below, ‘F17’ refers to the first 17 fascicles of T 1, and ‘L5’ to the last five fascicles of T 1. Figure 1 shows the analysis result with D, the document threshold, set to 4. In Figure 1, all of the five points in L5 lie in the fourth quadrant. In contrast, the points in the F17 group are spread across the remaining three quadrants. This result shows that the five fascicles in L5 have a similar style but behave very differently compared to the fascicles in the F17 group.     Figure 1. PCA result of T 1 with D = 4.    Figures 2 and 3 illustrate the PCA result with D set to 6 and 8, respectively. The results of these two experiments are very similar to those shown in Figure 1. Figure 2. PCA Result of T 1 with D = 6.    Figure 3. PCA Result of T 1 with D = 8. In the subsequent analyses, we raise the value of D to 10, 12, and 14 and perform PCA analysis again. The following Figures 4–6 illustrate the result of PCA analyses with three different settings of D.    Figure 4. PCA Result of T 1 with D = 10.    Figure 5. PCA Result of T 1 with D = 12.    Figure 6. PCA Result of T 1 with D = 14. To our surprise, from Figures 4, 5, and 6, we notice that the difference between L5 and F17 starts to decrease. When D is set to 14, all points in L5 are enclosed by points in the F17 group, and there is no longer a clear boundary between the L5 and F17 groups. In fact, the results show that the L5 and F17 groups share a very similar pattern of usage of translation phrases, as if they choose the translation phrases from the same pool. Because the five fascicles of DĀ 30 belong to the same narrative and are very similar in content, part of our analysis is biased by the high content similarity of DĀ 30. That is, the difference in the preliminary result is very possibly from difference in content, rather than from diverse translation styles. To remove content-dependent conditions of analysis, we examine the constitution of the first and second components, and identify a number of most distinctive grams of DĀ 30. We find out that many of the resulting grams are topic-related. We remove these grams from the features set, and perform the analysis once again. The analysis results are illustrated in Figures 7 and 8 below.    Figure 7. PCA Result with D = 6, with removing grams that are highly related to the topic of DĀ 30.    Figure 8. PCA Result with D = 8, with removing grams that are highly related to the topic of DĀ 30. In Figures 7 and 8, the points of F17 and L5 are actually clustering so as to belong to a single group. This evidence confirms the previous observation that F17 and L5 are very different in content but very similar in style. At this point, the results become consistent and show no difference between DĀ 30 and the remaining 17 fascicles. This means that in terms of translation style, no significant differences between the first 17 and the last five fascicles have been detected.  More evidence to support this observation comes from the constitution of the first and second components. Then we notice that there are many long strings that appear both in DĀ 30 and in the remaining 17 fascicles. Notably, as shown in Table 1, some of these strings are very rarely or not at all found outside of T 1. Further, most of them are not constrained to a specified topic. Such a situation is likely to occur when the texts were translated by the same translator or translation team. Also, most of the matched long strings are used in narrative descriptions and clichés that commonly occur in many other early Buddhist discourses. However, these strings are largely only used in T 1. We therefore believe this demonstrates that F17 and L5 are indeed by the same translator, Zhu Fonian.   Text String  Fasc. in F17  Fasc. in L5  Occurr. (excl. T 1)    所以者何？我今如來．至真．等正覺，亦說此 13 19, 22 0   邪婬、兩舌、惡口、妄言、綺語、貪取、嫉妬、邪見 7, 15 18 0   生死已盡，梵行已立，所作已辦，更不受有 5, 9 22 4 (T 190: 1; T 212: 3)   盡壽不殺、不盜、不婬、不欺、不飲酒 3, 7, 11, 13, 15, 17 20 1 (T 125)   遠塵離垢，得法眼淨，猶如淨潔 13, 15 20 0   剃除鬚髮，服三法衣，出家修道 1, 6, 11, 13, 14, 15, 16, 17 19 0   身行不善，口行不善，意行不善 6, 8 22 2 (T 1548: 2)   苦聖諦，苦集、苦滅、苦出要諦 2, 3 20 0   Table 1. Significant long strings used in both the F17 and L5 groups. Note 1. CBETA (Chinese Buddhist Electronic Text Association) is a mature, curated digital edition of the widely used Taishō edition of the Buddhist canon. ",
        "article_title": "Applying a Combined Quantitative and Qualitative Analysis Method to Evaluate the Translatorship of the Fourth Division of the Chinese Translation of the Dīrgha-āgama",
        "authors": [
            {
                "given": "Jen-Jou",
                "family": "Hung",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan, Republic of China",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "linguistics",
            "text analysis",
            "English",
            "translation studies",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The success of Buddhism in China can be partly attributed to the great number of texts translated from Indian languages during the Eastern Han dynasty to the Tang dynasty (618–907 CE). The vast amount of Buddhist texts that were translated from both Sanskrit and other Indian languages between the 2nd and the 11th centuries not only had a far-reaching cultural impact on Chinese society but also made itself felt on Chinese language in terms of syntactic and lexical patterns of change.  Over the years, scholars have leveraged traditional text-critical methods to research language change in Chinese as evinced in the corpus of Buddhist texts. Philologists have investigated doctrinal, literary, and linguistic aspects in order to ascertain the authorship, dating, authenticity, and so forth of these texts. It has been found that many of the authorship or translatorship attributions of early Chinese sutras are unreliable.  Traditional philology has its limits, however, and the advantages of qualitative, computational approaches to the analysis of Buddhist texts are increasingly explored. Digital philology as a branch of the digital humanities, with its application to Buddhist materials, stands to open new horizons for research. For European languages, text analysis has been a very successful field. As for Chinese Buddhist texts, a digital version of the Taishō edition of the Chinese Buddhist canon is freely available through the efforts of the Chinese Buddhist Electronic Texts Association (CBETA). The availability of such a corpus in digital form enables researchers to apply statistical methods or artificial intelligence algorithms to text analysis. However, this type of research is only rarely applied to the study of Chinese Buddhist texts. We believe there are two main issues behind the present under-exploitation of these resources. First, the design of the CBETA XML markup is mainly aimed at providing a correct and comfortable display of the text on the computer screen rather than preparing the text for future possible uses for the sake of quantitative analysis. Second, the performance of quantitative analysis on digital texts still requires high-level skills in computer programming and advanced statistical knowledge, which creates a high barrier for scholars in the humanities who are now attempting to navigate these tools. In order to assist researchers we develop various resources for the computational analysis of the CBETA corpus. The main tasks include: 1. Creation of a ‘text-analysis friendly’ version of the CBETA corpus. 1 This dataset has following features:   • The markup is compliant with TEI P5.  • All information that is not critical for corpus analysis has been taken out—e.g., critical apparatus, markup of menu items, etc.  • The representation of text structures has been simplified and unified; only the <div> element with ‘type’ attribute is allowed for representing the text structure.  • Each textual block is wrapped with an <ab> element with a type attribute, which is used to distinguish text block with different types (prose, verse . . .).   • Every non-Unicode character is assigned a unique code point in the Unicode private-use area (‘undisplayable’ is not a good category; you can display everything, of course). 2. As an alternative to the XML format, we created the N-gram statistic dataset. 2 The dataset is generated by transforming the XML file into plain text format and removing all punctuation from the text. Then the long text strings are cut with fixed lengths to generate n-grams, and the occurrences of each gram are calculated.    3. We developed an online tool called the ‘Buddha N-gram Viewer’. 3 This tool allows users to visualize the overtime occurrences of phrases in Chinese Buddhist texts. It also provides detailed lists of the matches of phrases in the text, which enables the researcher to trace the text back to the original source and look it up there.  Figure 1. Occurrences of ‘Thus have I heard’ as ‘如是我聞’ and ‘聞如是’ in the Buddha N-gram Viewer.    Figure 2. Nirvana as ‘泥洹’ and ‘涅槃’ in the third fascicle of the 長阿含經 (the Chinese translation of the Dīrgha-āgama; T 1). Notes 1. The dataset is available at https://github.com/ddbc/CBETA_TAFxml. 2. The dataset is available at https://github.com/ddbc/CBETA-ngram. 3. The tool is available at http://dev.ddbc.edu.tw/BuddhaNgramViewer/. ",
        "article_title": "Digital Corpus and Toolset for Performing Text Analysis on Chinese Translation of Buddhist Scriptures",
        "authors": [
            {
                "given": "Jen-Jou",
                "family": "Hung",
                "affiliation": [
                    {
                        "original_name": "Dharma Drum Institute of Liberal Arts, Taiwan, Republic of China",
                        "normalized_name": "Dharma Drum Institute of Liberal Arts",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05hkpfm52",
                            "GRID": "grid.501594.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "authorship attribution / authority",
            "digitisation",
            "linguistics",
            "text analysis",
            "English",
            "resource creation",
            "data mining / text mining",
            "and discovery",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Due to increasing access to sophisticated analytical techniques, painting conservation has evolved into a highly interdisciplinary research area that requires the integration of knowledge and ‘big data’ about art history (artists, artistic techniques, art provenance), the physical and chemical properties of paint and pigments, the chemical processes associated with treatment and cleaning methods, and advanced high-resolution characterization techniques. The ultimate aim is to determine the precise cause of the degradation or discoloration that is occurring in a particular artwork or across a set of paintings and the optimum methods for treating and preventing it. The challenge is that the data and information required by art conservators to inform decision making are highly heterogeneous and distributed across databases, scholarly publications, and the Web. Expertise is also distributed across art galleries, conservation centres, and universities around the globe. Although it is possible to find some concentrated authoritative collections of information on the Web (e.g.,    Journal of the American Institute of Conservation, Smithsonian Museum Conservation Institute [MCI], Getty Conservation and Research Institutes, CAMEO materials database, and Forbes Pigment database), the information is often embedded within databases or is highly unstructured data hidden in textual documents. Relevant information may exist, but it is difficult to find, extract, re-use, interpret, correlate, or compare.  For example, it is very difficult for art conservators to find other past examples of conservation issues that most closely match the problem they are trying to solve. Moreover, the experimental data underpinning publications that describe the long-term effects of different environmental conditions (humidity, temperature, UV light) on different paints and pigments is not accessible, verifiable, or re-usable. Typically, the raw data (FTIR, spectrographs, X-ray diffraction images, SEM/TEM images) associated with the analysis of a particular painting or paint samples are not available even if described within a related publication. Our community of conservators, curators, and scientists want a Web-based search interface that provides answers to questions such as  • What is the best way to treat zinc oxides occurring in paintings by Rover Thomas?  • What are the factors that cause or accelerate the occurrence of lead soaps in paintings by R. Godfrey Rivers?  • What is the best solvent for removing varnish from the painting ‘Epiphany’? In order to answer such questions, a broad range of provenance information, characterisation data, publications, and experimental results need to be aggregated and synthesized. Hence, a key objective of the Twentieth Century in Paint project 1 was to investigate the IT infrastructure and services required to provide decision support tools to an online network of art conservators (APTCAARN—the Asia Pacific Twentieth Century Conservation Art Research Network). An initial analysis of the user requirements of APTCAARN members identified a growing demand for the following:   • An online repository where experiments and experimental results can be described, stored, shared, and discussed.  • A search interface that provides seamless access to publically available databases about paints and pigments.  • Tools to enable the extraction of structured data from publications about art conservation issues.  • A SPARQL query interface that supports querying across all of the above. This paper will describe the architecture, ontology, repository, and services that constitute the Art Conservation Knowledge-base. It will describe the OPPRA (Ontology for the Preservation of Art) ontology, the experimental data capture interface and archive, the mapping tools (that map external databases to OPPRA), and the GATE pipeline (that integrates a Named Entity Recognition [NER] tool for tagging concepts and named entities within paint conservation publications, with a Relation Extraction classifier for identifying OPPRA-based relations between named entities) to extract structured data from a corpus of key publications on art conservation. We will also give a demo of the ontology-based federated search interface, designed to support the queries described above. Finally, the results of evaluating the system both from a user endpoint and empirically will be presented.    Figure 1. Modelling and extracting structured data from a publication about experiments on lead soaps in a painting by Van Gogh. Note 1. http://www.20thcpaint.org/. ",
        "article_title": "A Collaborative Interdisciplinary Knowledge-Base for the Art Conservation Community",
        "authors": [
            {
                "given": "Jane",
                "family": "Hunter",
                "affiliation": [
                    {
                        "original_name": "The University of Queensland, Australia",
                        "normalized_name": "University of Queensland",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00rqy9422",
                            "GRID": "grid.1003.2"
                        }
                    }
                ]
            },
            {
                "given": "Suleiman",
                "family": "Odat",
                "affiliation": [
                    {
                        "original_name": "The University of Queensland, Australia",
                        "normalized_name": "University of Queensland",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00rqy9422",
                            "GRID": "grid.1003.2"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Drennan",
                "affiliation": [
                    {
                        "original_name": "The University of Queensland, Australia",
                        "normalized_name": "University of Queensland",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00rqy9422",
                            "GRID": "grid.1003.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "libraries",
            "archives",
            "museums",
            "sustainability and preservation",
            "data mining / text mining",
            "English",
            "GLAM: galleries",
            "knowledge representation",
            "databases & dbms",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " While the mutually beneficial role of interdisciplinarity in DH projects is often highlighted, the impulse to rethink one’s own discipline-centered dogmas and theory as a result of such cooperations is seldom addressed. In heureCLÉA, 1 a DH project involving a team of literary scholars and a team of computer scientists and machine learning specialists, both parties have encountered this phenomenon. In the following, we would like to demonstrate how interdisciplinary incompatibilities are brought to attention in the DH context, and how this realisation can help to improve a participant theory—in our example, the theory of narrative. 2   The Setting: heureCLÉA  heureCLÉA’s goal is to develop a ‘digital heuristic’: a discovery tool that supports literary scholars in analysing narrative texts. We focus on the generation of automated annotations concerning  temporal aspects of novels, short stories, etc. 3 By automatically identifying time-related phenomena, our digital heuristic-tool shall provide bottom-up support for hermeneutic analyses.  Three steps lead toward this goal:  1. A small corpus of short stories is manually and collaboratively annotated with the help of a narratological tagset for temporal phenomena.  2. These annotations are then analysed and modelled in a computational approach that combines rule-based NLP methods with machine learning procedures. 4   3. Once the automatically generated output meets defined performance criteria, the digital heuristic is implemented as a functional module in the text analysis platform CATMA. 5  This looks straightforward, but as soon as the first manual annotations were discussed in phase 2, we encountered problems that point to fundamentally different, discipline-specific approaches to text annotation.  The Problem: Interdisciplinary Incompatibilities To begin, heureCLÉA’s markup ‘philosophy’ is consciously non-prescriptive: 6 we do not enforce inter-annotator agreement. Instead, we invite human annotators to consider their annotation practice as an interpretive activity rather than a taxonomy-ruled declaration of objective phenomena. This approach is supported by CATMA, our web-based platform for collaborative text annotation. 7 Accordingly, our annotators made different and even contradictory interpretive decisions when tagging certain text segments.   From a computer science point of view, this renders parts of the relevant CATMA markup data  noisy, i.e., too ambiguous to allow for reliable statistical ML-analysis and prediction. The literary studies’ perspective onto such annotator disagreement, however, is quite the opposite. 8 Rather than constituting a methodological obstacle, substantially different interpretations of texts derived from incompatible analyses are regarded as a natural consequence of the polyvalence of literary texts (cf. Jannidis et al., 2003), or to phrase it provocatively: by definition, a non-ambiguous text triggering uniform high-level interpretations lacks aesthetic quality (cf. Jakobson, 1979/1960). Of course, such judgement hinges on the definition of ‘high-level’—the interpretation of a text’s philosophical message will certainly fall into this category. But what about statements concerning more formal, seemingly uncontroversial characteristics of narrative, such as their temporal construction both in terms of the  what (story) and the  how (discourse) of narration?  Two examples:  1. Standard narratological taxonomy uses the category of  order to identify whether a series of events is narrated in its original temporal order, or whether the chronologies of events and narration differ. The following passage was annotated as a  prolepsis, i.e., ‘flash forward’, by some annotators: ‘Maybe you expect me there at The Post Inn. Then we go to Ammerland together. It’s going to be a grand trip’ (Wedekind,  The Vaccination, our translation). 9 But not everybody agreed: a second group of annotators did not detect a deviation from the chronological representation of events.    2. The second example concerns the analysis of  duration or  speed of the narration: how long does it take to report events in relation to how long it takes for these events to happen? The following passage was sometimes annotated as  scene, i.e., very slow narration, and sometimes as  summary, i.e., very fast narration: ‘I have gazed outside all night, and methought this was how death must be like, or the after-death: over there and outside an infinite, hollowly roaring darkness. Will a thought, a notion of mine linger and weave on there and eternally hark to the intangible roaring?’ (Mann,  Death, our translation). 10  From an ML perspective these interpretive variances produced  noisy annotation data—but how about the humanist’s view: Isn’t this the hallmark of aesthetic  originality? Are conflicting annotations not indicative of a semantic richness owed to a narrators’ ability to evoke, e.g., polyvalent temporal orders? If so, the phenomenon would indeed have been adequately encoded  in nuce by conflicting manual annotations on the inconspicuous level of narrative form. 11  To test this hypothesis, we attempted to reconcile the two different annotation-centered methodological perspectives—that of the computer scientist and that of the literary scholar—by taking a decision-tree-inspired look at the human-generated annotation: What exactly had triggered the diverse output? Where were the choice points?  The Solution: Improving Narratological Theory via NLP-Parametrisation   Revisiting examples of  noisy narratological annotation from a humanities perspective, we eventually realised that some did  not result from genuine high-level polyvalence of literary texts. Instead, they could be traced back to one of two shortcomings—methodological and/or theoretical:  Ill-Defined Concepts Some base-level narratological concepts proved simply ill-defined. Consider example one: The usual definition of the concept  prolepsis (flash forward) does not specify whether temporal lookaheads must concern ‘actualized’ events in the fictional world. The annotation of the passage from example 1 thus varies according to how one defines the modal parameter ‘actualized’ vs. ‘hypothetical’. Problems of this kind were resolved by simply ‘pre-parametrising’ the relevant narratological categories so that previously non-explicit theoretical premises were now formally included in the category definition as functional parameters. This usually results in narrower definitions and, respectively, more consistent—less  noisy—annotations. As for the definition of  prolepsis, we confined this to ‘actualized’ anticipations. Accordingly, example 1 does  not contain a prolepsis, for the future events in question—the meeting at the Post Inn, the journey together—are purely hypothetical.   Undetected Dependencies Other contradictory annotations were traced back to opaque dependencies between specific base-level narratological concepts and over-arching theoretical assumptions in narratological theory . In a computational view, such non-explicit premises function as hidden parameters. This finding explains our second example: the analysis of  duration depends on the underlying notion of what constitutes an  event. If only specific occurrences in the fictional ‘outside world’ are considered an event, then the narration speed in example 2 is low. Since only the protagonist’s thoughts are reported here, nothing ‘happens’ empirically in the fictional world; it is only the narration that progresses. If, on the other hand, thoughts are considered events, too, then the narration speed is, of course, very high: a whole night of ‘gazing and thinking’ is narrated in just a few sentences!  To address such cases where annotation variance stemmed from non-explicated theoretical premises, we adopted a less deterministic approach: such incompatible markup decisions are now accepted on condition that their underlying assumptions are explained in the markup. Annotators must, for example, document their notion of  event when analysing  duration. High-level parametrisation of this type will be implemented in the heuristic module by offering the users to choose their own parameters. Automated markup suggestions will then be adapted to these choices. From a technical perspective, this is realised using transparent machine learning models (such as Decision Trees)—in contrast to less transparent models (e.g., Support Vector Machines). Transparency is reflected in the possibility of being able to retrace and visualise the concrete decision process, i.e., factors that are involved in the prediction, which not only allows for an adaptation of the underlying model based on user-input (so-called interactive or feedback-driven machine learning) but also facilitates a reasoning and discussion over the changes to the model due to user input.  Conclusion and Outlook These examples demonstrate how interdisciplinary DH collaboration, apart from the defined project goals, can also yield more general theoretical and methodological insights. Methodological incompatibilities observed in the pragmatic domain can motivate one of the partner disciplines to optimize its elementary taxonomic definitions, and even stimulate theoretical revisions, as the example of inconsistent human annotation and the introduction of pre-parametrisation and flexible parametrisation of categories demonstrates.  DH-collaborations are particularly well suited to yield such benefits beyond the pragmatic: on the one hand, we practice the humanities’ hermeneutic approach, a methodology and epistemology that are phenomenological, synthesis-oriented, and historically contingent; on the other hand, we employ the formal approach of computational analysis and mathematical modelling, which strives for an abstract, fully parametrised representation and analysis of data structures and processes.  This methodological bifurcation is not a question of enduring a life in  two cultures ( sensu C. P. Snow, cf. Snow [1993]); it is the fundamental and highly productive dialectic of DH’s epistemology and practice.   Notes   1. heureCLÉA is funded by the German Ministry for Education and Science (BMBF) as part of the eHumanities initiative. For further project details, see www .heureclea .de.    2. While this paper only addresses narrative theory as a beneficiary discipline, we have also experienced cases demonstrating the opposite constellation. For an illustration of the benefits that the interdisciplinary approach has brought for NLP, see Gius and Jacke (2015) as well as Bögel et al. (2015).  3. In addition to rather basic aspects like  tense and  dates, we are concerned with more complex temporal phenomena that are constitutive of all types of narrative representation: every narrative features a story, and this story is presented in a specific way. The more complex temporal phenomena with which we are concerned all refer to the temporal relation between story and representation. This relation can be analysed with regards to three aspects:  order (when did an event happen? when is it told?),  frequency (how often does it happen? how often is it told?), and  duration (how long did it take to happen? how long did it take to tell about it?); cf. Genette (1972).  4. For first results concerning automated annotation, cf. Bögel et al. (2014). 5. Cf. www.digitalhumanities.it/catma. 6. heureCLÉA—like its markup tool CATMA—is based on the methodological premise of ‘hermeneutic markup’, as described in Piez (2010).  7. For CATMA, see www . catma . de.  8. For a more theoretical-methodological discussion of this divergence, cf. Gius and Jacke (2015). 9. ‘Vielleicht erwartet ihr mich dort im Gasthof zur Post. Dann fahren wir zusammen nach Ammerland. Das wird eine prächtige Tour’ (Wedekind,  Die Schutzimpfung).  10. ‘Ich habe die ganze Nacht hinausgeblickt, und mich dünkte, so müsse der Tod sein oder das Nach dem Tode: dort drüben und draußen ein unendliches, dumpf brausendes Dunkel. Wird dort ein Gedanke, eine Ahnung von mir fortleben und -weben und ewig auf das unbegreifliche Brausen horchen?’ (Mann,  Der Tod).  11. For a more detailed exploration of the phenomenon, see Meister (2003), who explores the resulting combinatorial potential of incrementally more complex and polyvalent action-constructs in Goethe’s novella cycle  Unterhaltungen deutscher Ausgewanderten.   ",
        "article_title": "Beyond Pragmatics: Disciplinary Profits of Interdisciplinary Approaches",
        "authors": [
            {
                "given": "Evelyn",
                "family": "Gius",
                "affiliation": [
                    {
                        "original_name": "University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Janina",
                "family": "Jacke",
                "affiliation": [
                    {
                        "original_name": "University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Jan Christoph",
                "family": "Meister",
                "affiliation": [
                    {
                        "original_name": "University of Hamburg, Germany",
                        "normalized_name": "Universität Hamburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00g30e956",
                            "GRID": "grid.9026.d"
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": "Bögel",
                "affiliation": [
                    {
                        "original_name": "Heidelberg University, Germany",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            },
            {
                "given": "Jannik",
                "family": "Strötgen",
                "affiliation": [
                    {
                        "original_name": "Heidelberg University, Germany",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "german studies",
            "drama",
            "metadata",
            "literary studies",
            "natural language processing",
            "interdisciplinary collaboration",
            "bibliographic methods / textual studies",
            "information retrieval",
            "encoding - theory and practice",
            "text analysis",
            "English",
            "linking and annotation",
            "semantic analysis",
            "data mining / text mining",
            "poetry",
            "genre-specific studies: prose"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the summer 2014 issue of  CEA Critic authors Lindsay Thomas and Dana Solomon remarked on the notable lack of discussions of pedagogy in the development of DH in undergraduate institutions (2014). Arguing that DH pedagogy should be something far more than an afterthought, Thomas and Solomon outlined how their undergraduate project RoSE at the University of California, Santa Barbara developed students to be active users and researchers of DH. In the same issue of  CEA Critic, E. Leigh Bonds drew on the discussions of Melissa Terras, Stephen Ramsey, Alan Liu, et al. about the fundamental difference between the learning goals of DH courses and those of traditional courses in the humanities (2014). In the case of working with archival materials, how do we teach students to be makers and doers together? Or, in Liu’s terms, how do we develop a pedagogical hermeneutic of ‘practice, discovery, community’ (2009)?  This paper will focus on the teaching experience of Drs. Katherine Faull and Jakacki in the fall of 2014 to show how the planning, design, and execution of a new project-based course—HUMN 100: Digging into the Digital and the Humanities Now!—introduced students to the world of digital humanities through the use of selected digital tools and methods of analysis. This course, taught within the Comparative Humanities program, was designed specifically for first- and second-year students in order to encourage the development of digital habits of mind at the earliest phases of their liberal arts curricular experience (Clement, 2012). Developed to encourage examination and experimentation with a range of digital humanities approaches, students worked with primary archival materials to encourage digital modes of inquiry and analysis. The decision to root the course in a multifaceted analysis of archival materials provided the rare chance for students to engage in the research process typical for a humanities scholar: namely, the discovery of artifacts, the formulation of research questions, followed by the analysis and synthesis of findings culminating in the publication of initial findings in a digital medium. In the process, we introduced students to the basic structure of how to develop a DH research project.  For Bucknell University, the focus in digital humanities scholarship and learning has been primarily on spatial thinking (Pacchioli, 2013). It was important to both instructors to emphasize that objective in the development of the course and its learning outcomes, and so they focused upon the importance of finding materials that would be of interest to students ‘in place’. We decided to run the course in two sections, anticipating an opportunity to reflect different perspectives of the instructors’ expertise with DH methods and tools. One section focused on the Colonial mission diaries of the Moravians from Shamokin, Pennsylvania. Written in English, the diary sections selected dealt with interactions between some of the first Europeans to the area and the Native peoples they met and worked among. The other section considered a subset of the diaries of James Merrill Linn, one of the first graduates of the university and a soldier in the American Civil War. Both choices reflect and extend Bucknell’s interest in digital/spatial thinking in terms of its place in the larger historical and cultural narrative.  In this paper we demonstrate how scaffolding the course—establishing a parallel structure that spanned both sections—accommodated both core texts while reinforcing the importance of considering how different DH-based methods would strengthen students’ understanding of that subject matter. This approach allowed both instructors to develop more sophisticated and complex course modules while assisting one another through the strengths and skills of each. We will discuss how this challenged us to consider whether we were co-teaching two sections of one course or two courses in collaboration, and where we were successful in identifying moments that offered a richer learning environment for both sections, supporting each other in our separate sections when our own DH expertise and pedagogical approaches were needed by the other. In essence, we had to learn how to teach one another while we were teaching the subject matter to our students.  This course was developed to encourage examination and experimentation with a range of digital humanities approaches, and how digital humanists apply computational methods that involve textual analysis and data visualization. The sequencing of the modules was carefully designed so that the ‘product’ of each module then became the ‘data’ of the next module. Throughout the course, students developed a database of names, people, places, and connections that grew organically out of the focus of each specific section. This data, crucial to the success of the students’ assignments, needed some restructuring as we moved onto the next module; for example, students’ close reading in the TEI module led to the development of a prosopography that led to data for entry into Gephi that was then built out in adding geospatial data for GIS. Extensive use was made of online platforms that emphasize important forms of digital engagement, including collaborative online writing environments. Each module ended with a short assignment and also a reflective public-facing blog post that became an unmarked form of intellectual interaction.  Student enrollment in the two sections of the course was purposely limited to first- and second-year undergraduates with no background in digital humanities. Accordingly, the course was designed with the goal of not only exposing them to tools of distant and close reading, and network and spatial visualization, but also requiring that they learn to think critically about what each of these methods reveals in the manuscript texts they themselves transcribed; finally, the students were required to produce digital artifacts.  This paper will argue that the placement of this course within the interdisciplinary context of the program in Comparative Humanities underscored the program learning goals both of comparativity and interdisciplinarity and the course-specific goals of a new pedagogical hermeneutic. Teaching students to compare meaningfully intellectual materials of different or opposing types, and to theorize the difference between textual and material artifacts, narrative and non-narrative texts, and visual and analytical modes of thought was central to the course. To promote this, each module required students to read key secondary texts that were then integrated into their own reflections on the module. What does Johanna Drucker say about the visual rhetoric of visualization (2014)? How does Elena Pierazzo argue for the epistemic difference of diplomatic editions (2011)? What do Daniel Rosenberg and Anthony Grafton say about the development of timelines and the conceptualization of history (2000)? The interdisciplinary humanistic approach was thus clearly and directly linked to the learning goals of the course. Students also learned to identify, use, and discuss the advantages and disadvantages of different DH methodologies and tools and were encouraged to identify and use key DH terms and concepts. As a result, students learned to develop research questions that could be answered with DH tools and methodologies, and work collaboratively in groups to create projects that related to their own research interests.  The development and implementation of HUMN 100—a course without precedent at Bucknell and few guiding models at other undergraduate institutions—was far from easy. While both instructors had co-taught courses before, neither had developed what both agreed was a high-risk, high-profile course that could have significant impact on our colleagues as well as on our students. We knew that we were establishing a foundation that could (hopefully) scale to a much broader presence for the digital humanities across the Bucknell curriculum. 1 We realized early on in course development that in order for our students to understand the evolving nature of DH research, we would have to reveal our own status as learners. Teaching unfamiliar material—not only across sections, but within a particular class—required an at times uncomfortable degree of transparency.   However, for all the challenges involved in teaching the class, there were moments of glory. Disengaged students became engaged; solitary learners recognized the essential need to collaborate in order to succeed; participants recognized the transformative nature of the course to their own concepts of the humanities. Students were eager to participate in crowdsourced data collection; they were intrigued to visualize ego-networks as they learned the concepts of network theory; they were excited to see their marked-up transcriptions published in an online digital edition. Through these experiences, they realized that they were creating a community of young DHers and expressed eagerness to take part in more of these learning experiences.  We believe that this paper is important to the community that will be present at DH2015 because it provides a model for how digital humanities can and should be taught at the earliest stage of an undergraduate’s university experience, and how this type of learning experience is transformative in terms of demonstrating the interdisciplinarity within the humanities. If such courses are well planned, modestly ambitious, and truly collaborative in both conceptualization and execution, they promote radically new ways of understanding the goals of humanistic enquiry, a new pedagogical hermeneutic for teachers as well as students.  Note 1. In ‘Digital Learning in an Undergraduate Context: Promoting Long-Term Student-Faculty (and Community) Collaboration in the Susquehanna Valley’ (under review), we argue for the importance of such DH methods courses as crucial to the development of a larger curricular program of research-based learning environments that engage students in digital public humanities projects. ",
        "article_title": "Pedagogical Hermeneutics and Teaching DH in a Liberal Arts Context",
        "authors": [
            {
                "given": "Diane Katherine",
                "family": "Jakacki",
                "affiliation": [
                    {
                        "original_name": "Bucknell University, United States of America",
                        "normalized_name": "Bucknell University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00fc1qt65",
                            "GRID": "grid.253363.2"
                        }
                    }
                ]
            },
            {
                "given": "Katherine Mary",
                "family": "Faull",
                "affiliation": [
                    {
                        "original_name": "Bucknell University, United States of America",
                        "normalized_name": "Bucknell University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00fc1qt65",
                            "GRID": "grid.253363.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "repositories",
            "content analysis",
            "project design",
            "archives",
            "relationships",
            "text analysis",
            "sustainability and preservation",
            "English",
            "maps and mapping",
            "graphs",
            "management",
            "digital humanities - pedagogy and curriculum",
            "networks",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The National Library of Wales (NLW) has been an enthusiastic adopter of digitization and networked technologies since the late 1990s as a means of opening up the nation’s cultural heritage. The digital plays a key role in the Library’s core strategic aim of ‘Knowledge for All’, not only to provide greater access to resources online, but also to enhance our potential impact and help us engage more effectively with our users, both existing and future (NLW, 2014). To address the challenges of delivering effective, usable, and sustainable digital resources, the Library established its own Research Programme in Digital Collections in 2011 to contribute to the development of a more coherent digital agenda. 1 Adding value and impact to the Library’s digital content and the institution as a whole lies at the heart of what the Programme is seeking to achieve. It can no longer be assumed that digital resources, once built, will be used (Warwick et al., 2008). Attempts have therefore been made to examine potential barriers to the adoption and use of such resources and to better understand issues surrounding their sustainability. 2 It is within this context that the Programme operates.   The Programme’s main areas of focus include developing an understanding of the use, value, and impact of the Library’s existing digital content; identifying ways of enhancing this content for research, teaching, and community engagement; and developing new digital content that addresses specific research and educational needs. This work is being realised through collaboration with partner libraries, museums and archives, cultural heritage organisations, universities, and other key stakeholders that cross institutions, collections, and disciplinary traditions, both nationally and internationally, with the aim of bringing tools, methods, and content together to create digital resources that are seamless and open. This paper will examine the work currently ongoing at the Library as part of its digital programme and, in particular, its attempts to foster collaboration in the creation and development of improved environments that enable more effective use, re-use, and linking of its digital content. Historical Digital Texts The creation of digital text, and especially historical digital text, is hugely challenging. The issues posed by the original text, including issues such as legibility and physical condition, are often exacerbated by attempts to process and subsequently analyse these texts digitally and will inevitably have implications for research. Such issues are demonstrated by problems of accuracy with the underlying OCR in the Library’s  Welsh Newspapers Online project 3—a free, searchable digital archive of over 100 historic newspapers of Wales, both Welsh- and English-language, dating from 1804 to 1919 in the Library’s holdings—which are often caused by blemishes and imperfections on the pages of the original. As Tim Hitchcock has recently argued, underlying issues of the design and structure of digital texts can be hugely problematic for research and algorithm-based searching (2013). Re-use and analysis of such content must, therefore, incorporate these concerns. As such, engaging expert users and curators of the material in the creation of a digital resource and the development of discovery and analysis tools is key.   Collaboration and Co-Creation A collaborative approach was used by the Library in Cymru1914.org, 4 a JISC-funded project to digitize primary sources relating to the Welsh experience of the First World War and its impact on all aspects of Welsh life, language, and culture. The project has virtually unified fragmented and often difficult-to-access materials from the libraries, archives, museums, and special collections of Wales to form a consolidated digital collection of interest to researchers, students, and the public on life in Wales during this significant period of change. The project was developed as a process of co-creation: a collaboration between the Library; the libraries, archives, museums, and special collections of Wales; and researchers of the literature, history, language, and politics of the period. Their input was used to inform the selection of content, the development of the user interface, and the testing of functionality. This model of active collaboration reunites curatorial and scholarly roles, redressing concerns regarding the gradual segregation of these roles in the current academic and cultural heritage landscapes (Schnapp and Presner, 2009). Such approaches also work towards further embedding digital resources in the life cycle of scholarly research, thus increasing its value, impact, and sustainability in the long term.   The Library is also looking to explore new collaborative opportunities to enhance its existing digital content through the application of crowdsourcing approaches (Dafis et al., 2014). The Library’s first collaborative crowdsourcing project, Cymru1900Wales, 5 a classification project to gather the place names of Wales from the georeferenced Ordnance Survey maps of 1900, has been an opportunity for the Library to experiment with crowdsourcing and enable greater engagement with new and remote audiences. These possibilities are being developed through a co-supervised PhD with the University of Wales to assess the feasibility of such approaches for the transcription of the Library’s online collection of Welsh wills and to build virtual communities around the content. There is already significant interest in the wills, especially in family and local history circles. Crowdsourcing is an opportunity for the Library to use this interest in a positive way to add value to its collections and the institution itself, and to engage the community in its work. Such an approach gives the Library the opportunity to build closer links with its users across the world and to encourage connections between academia and the public. It is a way of allowing the public to meaningfully experience and understand their pasts, which, in essence, is the cultural heritage sector’s primary objective (Owens, 2012).   Analysis and Linking of Content Users are becoming more demanding in their expectations of what digital resources can deliver. For these demands to be met and to ensure maximum usage, particularly within scholarship, environments must be created that allow for content to be analysed in more effective and complex ways. This includes resources that facilitate both close and distant readings of texts, allowing for the exceptional to be studied alongside large-scale aggregations of data (Moretti, 2005). The Library has experimented with simple visualisation tools, such as n-grams, giving researchers the opportunity to analyse patterns and trends on larger scales than would be possible in a non-digital environment. The possibilities of linked data are also under consideration as a means of transcending collection boundaries. Enabling links to be more easily made between documents within and across collections in this way would remove the data from its Welsh silo and allow it to be more easily incorporated into a broader, more international context. This would see the Library becoming a truly global institution for and of the Welsh and other Celtic peoples.  In keeping with its commitment to providing free and open access, NLW is also in the early stages of facilitating more advanced analysis of its digital content by opening up some of its raw data for others to download and interrogate for their own research purposes. These datasets will come from some of the Library’s biggest collections, including the newspapers, and it will be available during 2015. 6  Conclusion This paper will demonstrate that cultural heritage organisations, such as the Library, are frequently spaces in which research and development in the digital humanities can be nurtured and encouraged, and where collaboration with colleagues within and beyond the humanities and the academy can take place. NLW projects, such as Cymru1914.org, have shown the value in building better connections and partnerships with libraries, archives, and museums, along with researchers and the public more generally. This has enabled the development of digital resources that allow for data to be used and reused, that encourage meaningful engagement with different user communities, and that permit digital outputs to be repurposed for new and unforeseen purposes. We must look to build collaborations that will facilitate the integration of tools and data, and the disassociation of text and data from its platform or delivery mechanism, thus ultimately liberating digital resources and scholarship, and ensuring that our work does more than merely replicate print culture digitally. Acknowledgement The authors would like to thank Professor Lorna M. Hughes for her assistance in writing this proposal. Notes 1. NLW Research, www.llgc.org.uk/research.  2. University College London,  The LAIRAH Project: Log Analysis of Internet Resources in the Arts and Humanities, http://www.ucl.ac.uk/infostudies/LAIRAH/ . Oxford Internet Institute, University of Oxford.  TIDSR: Toolkit  for the  Impact  of Digitised Scholarly  Resources,  http://microsites.oii.ox.ac.uk/tidsr/welcome .   3. Welsh Newspapers Online, http://welshnewspapers.llgc.org.uk/en/home.  4. Cymru1914: The Welsh Experience of the First World War, http://cymru1914.org/.  5. Cymru1900Wales, http://www.cymru1900wales.org/.  6. The data, when available, will be accessible from  http://data.llgc.org.uk .  ",
        "article_title": "Beyond the Library Walls: The National Library of Wales Research Programme in Digital Collections",
        "authors": [
            {
                "given": "Rhian",
                "family": "James",
                "affiliation": [
                    {
                        "original_name": "National Library of Wales, United Kingdom",
                        "normalized_name": "National Library of Wales",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03rjyp183",
                            "GRID": "grid.421620.3"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "McCann",
                "affiliation": [
                    {
                        "original_name": "National Library of Wales, United Kingdom",
                        "normalized_name": "National Library of Wales",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03rjyp183",
                            "GRID": "grid.421620.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "cultural studies",
            "repositories",
            "libraries",
            "interdisciplinary collaboration",
            "archives",
            "museums",
            "information retrieval",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "resource creation",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Since John Burrows first proposed Delta as a new stylometric measure (Burrows, 2002), it has become one of the most robust distance measures for authorship attribution (Juola, 2006; Stamatatos, 2009; Koppel et al., 2009). It has been shown to render very useful results in different text genres (Hoover, 2004a) and languages (Eder and Rybicki, 2013). Nowadays, Delta is widely used not the least because there is the free  stylo package in R (Eder et al., 2013). There have been several proposals to improve Delta (Hoover, 2004b; Argamon, 2008; Eder et al., 2013; Smith and Aldridge, 2011; Kestemont and Rybicki, 2013). In the following, we report on a series of experiments to test these proposals using collections of novels in three languages. Our results will show that one of Hoover’s and one of Argamon’s measures show good results, but are outperformed in general by Burrows’ Delta and by Eder’s Delta. Most notably, the modification of Delta proposed by Smith and Aldridge shows a remarkable improvement of the results in all languages and has the advantage of providing a stable increase of performance up to a specific point, unlike the other measures, which are very sensitive to the amount of most frequent words (mfw) used. These results also allow discussion of some of the theoretical assumptions for the success of these measures, even if we are still far away from providing a ‘compelling theoretical justification’ (Hoover, 2005) for their success.  Material and Methods Text Collections We built three collections of novels (English, French, German), each consisting of 75 texts: 25 authors, each represented with three novels. The collection of British novels contains texts published between 1838 and 1921 coming from Project Gutenberg (www.gutenberg.org). The collection of French novels contains texts published between 1827 and 1934 originating mainly from Ebooks libres et gratuits (www.ebooksgratuits.com). The collection of German novels consists of texts from the 19th and the first half of the 20th centuries, which come from the TextGrid collection (http://www.textgrid.de/Digitale-Bibliothek). Text Distance Measures Argamon (2008) proposed three variants of Delta, each one improving on one aspect of Burrows’ Delta.  He argues that Burrows’ Delta inherently assumes that the distribution of a particular word across multiple texts follows the multivariate Laplace distribution, but it uses the standard deviation as a normalization factor, which only makes sense for a Gaussian distribution. Quadratic Delta assumes a multivariate Gaussian distribution of the words. Linear Delta, on the other hand, adjusts the normalization to take into account the calculation of spread for a Laplace distribution. Like Burrows’ Delta, both variants are based on the (implausible) assumption that the frequencies of the words are independent. The third variant, Rotated Delta, rotates the frequency differences into a space where they are maximally independent. It also assumes a Gaussian distribution. An analysis of the word frequency distribution in our English test set shows indeed that the normal distribution represents the data much better than the Laplace distribution (Figure 1a). The same is true for German high-frequency words (Figure 1b). We should therefore expect Rotated Delta to perform best, and Quadratic Delta to yield better results than Linear Delta or Burrows’ Delta. Eder’s Delta slightly increases the weights of frequent words and is meant to perform better with highly inflected languages. It should perform better for German and French than for English. Smith and Aldridge replace the Manhattan distance used by Burrows with cosine based on findings in text mining that the latter shows greater reliability with large word vectors (Smith and Aldridge, 2011). Their experiments show an impressive improvement for English texts. We implemented these measures in Python, using the output of  stylo, where a parallel implementation existed, as a benchmark (see the appendix for the formulas).  Evaluating Distance Measures In order to provide useful performance indicators for these measures for authorship attribution, we concentrated on the question of how well a particular distance measure allows distinguishing between a situation where two compared texts are written by the same author from a situation where two texts are from different authors. Besides relying on the Adjusted Rand Index as a well-known but rather abstract measure for clustering quality (Everitt et al., 2011, 264f.), we also established a simple algorithm to count clustering errors representing the researcher’s intuition of correct clustering. In order to obtain another more subtle performance indicator independent of any clustering algorithm, we grouped the calculated distance scores into two sets: ingroup comparisons containing distances between texts actually written by the same author, and outgroup comparisons. The larger the difference between the ingroup distances and the outgroup distances, the better a distance measure is assumed to perform. After evaluating several potential performance indicators (for example, t-values, or using proportions of distribution overlap) in terms of how well they correlate with the number of clustering errors, we settled on the simple difference of z-transformed means because it showed the best correlation with the clustering error measure. Results and Discussion Most interestingly, we could not only confirm the findings of Smith and Aldridge on our English texts, but also show that Cosine Delta outperforms all other measures on our three collections (Figures 2–4). Equally important, it proves to be more stable with increasing mfw size. While Burrows’ and Eder’s Delta usually show a peak around 1,000–1,500 mfw, and then behave a bit erratically on longer word vectors, Cosine Delta reaches a plateau at 2,000 and stays there (Figure 5). As Smith and Aldridge argue, based on their very different data, that using word vectors longer than 500 words doesn’t improve the performance, we assume that this number is a function of the corpus size. Our empirical tests did not substantiate Argamon’s theoretical arguments. Eder’s variant didn’t show consistently better results with more highly inflected languages like German and French and performed similarly to Burrows’ Delta. Both Quadratic Delta and Rotated Delta perform much worse than should be expected on theoretical grounds. And Linear Delta, although being among the top-five distance measures, seems to be an improvement over Burrows’ Delta only under special circumstances. Argamon’s modifications were based on correct assumptions about the kind of distributions Delta is working on, but nevertheless those algorithms did not perform better, something that points to the operation of factors not yet understood. The fact that those algorithms consistently perform differently in different languages and that these differences cannot, or at least only partially, be explained by the degree of inflection (Eder and Rybicki, 2013), adds to this enigma at the moment. There is almost no other algorithm in stylometry we know as much about as Delta, and yet there is still no theoretical framework that can explain its success. Future Work We tried to assemble corpora similar in genre, time, etc., but there is the real possibility that the variation we attribute to the languages is really only one between the specifics of the corpora. So it is important to test the robustness of our findings using different corpora. Another line of future investigations is the testing of more variations of Delta, using Cosine Delta as a starting point. Also, a systematic study of how the length of the mfw list determines Cosine Delta’s performance in relation to the size of the texts and the corpora could allow the automatic choice of the best parameters. And last but not least we have to analyze the connection between the performances of some variants of Delta and specifics of different languages in order to gain a deeper theoretical insight into the working of Delta in general. _____ The python script implementing these distance measures can be found at  https://github.com/fotis007/pydelta.    Figure 1. Empirical distribution of word frequencies compared to idealised Gauss and Laplace distributions. Indicated for the three most frequent words in the English (a) and German (b) dataset.    Figure 2. Performance of distance measures on English texts. Indicated in terms of both the difference between z-transformed means of ingroup (same author) and outgroup distances, as Adjusted Rand Index (higher values indicate better differentiation), and in terms of clustering errors (lower values indicate better differentiation). Distance measures are sorted according to their maximum performance in all test conditions.    Figure 3. Performance of distance measures on French texts. For a detailed explanation, see Figure 2.     Figure 4. Performance of distance measures on German texts. For a detailed explanation, see Figure 2.     Figure 5. Difference between z-transformed means of ingroup and outgroup distances as a function of word list length. Indicated for selected delta measures on the German text collection. Appendix: Text Distance Measures Most delta measures normalize features first and then apply a basic distance function.   Distance Measure Basic Distance Function  Feature Normalisation    Burrows’ Delta Manhattan Distance z-score   Argamon’s Linear Delta Manhattan Distance diversity   Eder's Delta Manhattan Distance z-score · Eder's Ranking Factor:      n -   n   i   + 1   n        Eder’s Simple Delta Manhattan Distance square root   Manhattan Distance Manhattan Distance –   Argamon’s Quadratic Delta Euclidean Distance z-score   Argamon’s Rotated Delta Euclidean Distance Stripped-down eigenvectors of covariance matrix   Euclidean Distance Euclidean Distance –   Cosine Delta Cosine Distance z-score   Cosine Distance Cosine Distance –   Correlation Distance Cosine Distance center   Hoover's Delta-P1 (own measure) z-score   Canberra distance Canberra Distance –   Bray-Curtis distance Bray-Curtis Distance –   Chebishev Distance max abs. distance –   Basic Distance Function Definition   Manhattan Distance     ∑  i = 1   n    ∣    f   i   ( D ) -   f   i   ( D ʹ ) ∣      Euclidean Distance       ∑  i = 1   n    ∣    f   i   ( D   )   2   -   f   i   ( D ʹ   )   2           Cosine Distance    1 -     f  ⃗  ( D ) ⋅   f  ⃗  ( D ʹ )   ∥   f  ⃗  ( D )   ∥   2   ∥   f  ⃗  ( D ʹ )   ∥   2          Canberra Distance     ∑  i = 1   n      ∣   f   i   ( D ) -   f   i   ( D ʹ ) ∣   ∣   f   i   ( D ) ∣ ∣   f   i   ( D ʹ )         Bray-Curtis Distance      ∑ ∣   f   i   ( D ) -   f   i   ( D ʹ ) ∣   ∑   f   i   ( D ) +   f   i   ( D ʹ )        Hoover’s Delta-P1    (   P  ¯  + 1   )   2   -   N  ¯    , P = {   z   i   ∣   z   i   ≥ 0 } , N = {   z   i   ∣   z   i   < 0 }      z-score        f   i   ( D ) -   μ   i       σ   i          Diversity      1   n    ∑  j = 1   m        f   i   (   D   j   ) -   a   i        ,     a   i   = m e d i a n ( ⟨   f   i   (   D   1   ) , ⋯ ,   f   i   (   D   m   ) ⟩ )      Eder’s Ranking Factor      n -   n   i   + 1   n        ",
        "article_title": "Improving Burrows’ Delta – An empirical evaluation of text distance measures",
        "authors": [
            {
                "given": "Fotis",
                "family": "Jannidis",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Steffen",
                "family": "Pielström",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Christof",
                "family": "Schöch",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thorsten",
                "family": "Vitt",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg, Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "authorship attribution / authority",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Understanding audiences is crucial to the design of digital humanities projects, and understanding that there may be more than one audience and designing for that should be part of the initial discussion for DH design. Past that, though, we need better theorization of the phenomenon of the audience in the digital humanities. My examples will be two DH projects I have been involved with. Folkvine (http://folkvine.umbc.edu/) and Chinavine (http://chinavine.org/) both collect and make available folk arts, the first in Florida and the second in China. The goal for both of these projects has been to serve multiple audiences. Both approached the question of multiple audiences slightly differently. Awareness of multiple audiences could lead to a more reflexive and reflective attitude for designers and for the audiences. It could be less about consumption of digital content and more about rethinking the presentation of self and community. My attempt to introduce hermeneutics here is to move past the notion that questions of audience are just questions of usability or reception. DH projects create a world between the architects, subjects, and users; this is an attempt to interrogate one aspect of that world. ",
        "article_title": "Conceptualizing DH for Multiple Audiences: Folkvine and Chinavine",
        "authors": [
            {
                "given": "Bruce",
                "family": "Janz",
                "affiliation": [
                    {
                        "original_name": "U of Central Florida, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "philosophy",
            "user studies / user needs",
            "project design",
            "digitisation - theory and practice",
            "creative and performing arts",
            "folklore and oral history",
            "English",
            "asian studies",
            "including writing",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " To document unprecedented editorial interventions to a map, we adapted for graphical editing the existing TEI mechanisms for handling textual-critical apparatus. The historical map underlying The Map of Early Modern London was originally printed in seven sheets that cannot be cleanly joined because of differences of scale and missing information. To create a coherent and navigable graphical user interface for georeferenced data, we skewed, resized, and shifted the sheets, and interpolated new conjectural material. To document our emendations and interpolations, we treat each graphic witness and reconstruction as a TEI <surface> element linked to a <witness> in <listWit>. We use the <zone> element to identify areas, then create an apparatus based on these definitions, treating the idealized map as the lemma, and the other related <surface>s and <zone>s as readings. We use custom values for the intervention @type, @resp for persons/roles responsible, and @source to document supporting evidence.  Background Most GIS projects use maps to display georeferenced data. Theoretically, any georeferenced surface will suffice, with the latitude and longitude coordinates serving to pin the data to the surface, but historical GIS projects sometimes wrestle with a historical map or map-like object. Locating London’s Past 1 has georeferenced and georectified 2 the Rocque map of London to display data from 1660 to 1800. The Map of Early Modern London (MoEML) 3 is a literary GIS project that takes an even earlier map-like object as the site’s main interface. 4 The Agas Map of 1560s London, a hybrid genre combining plan, bird’s-eye view, and landscape, does not lend itself well to georeferencing and georectification and is in many ways an inconvenient surface for displaying data, historical or literary. However, when MoEML took shape in 1999 as an early experiment in digital humanities, the goal was to mark up a digital surface and identify all its features—a procedure very much like annotating a text. As the project has matured, we have come to conceive of the Agas Map not just as one of a number of graphical user interfaces (GUI) that permit exploration and visualization of MoEML gazetteer data but also as an editable text in its own right.   These two treatments of the map demand the coherence and navigability that make a GUI an effective skin for data, 5 and the application of the rigorous editorial principles and practices that inform MoEML diplomatic transcriptions and editions. We conceive of our structured composite of map witnesses as an ‘edition’. This edition includes our ‘copytext’ in the form of the 2013 digital scans we took as our starting point, as well as a reconstructed ‘reading text’ that meets the criteria of coherence and navigability. Users can compare the two ‘texts’ to see where they differ, but we also plan to document all emendations to the copytext, which include skewing, resizing, shifting, and most importantly, interpolating new material drawn by a graphic artist based on our historical research. In textual critical terms, we aim to produce a thorough textual apparatus documenting our editorial process. As a Text Encoding Initiative project, MoEML records editorial emendations using TEI markup. The TEI Guidelines provide mechanisms for handling critical apparatus in textual work. 6 We have repurposed those mechanisms for graphical editing in order to document our textual interventions to a map.   The Map Creating a single unified Agas Map ‘surface’ for MoEML was a multi-step process. For copytexts, we had imperfect textual witnesses and surrogates (print and digital) of those witnesses. First printed in ca. 1561, the Agas Map of Tudor London survives in three 1633 copies of an altered Jacobean version: one in the London Metropolitan Archives (LMA); one at the National Archives, Kew; and one at the Pepys Library, Cambridge. We gain a possible glimpse of the earlier Tudor map through an unreliable 18th-century witness, George Vertue’s 1737 pewter-plate version that shows houses where the 1633 witnesses depict the Royal Exchange (opened in 1571), and the Elizabethan arms instead of the later Stuart arms. In 1905 the London Topographical Society (LTS) used the LMA witness to create new lithographs of seven true-to-size sheets. Around 1981, in association with the Guildhall Library, publisher Harry Margary reproduced facsimiles of the LTS sheets. Because the LTS and LMA maps were originally printed in seven sheets, any attempt to produce a single map requires stitching. We have stitched together freshly scanned (in 2013) hi-res images of the Margary sheets. These sheets, like their LTS and LMA predecessors, do not match along their edges. Previous attempts to stitch the sheets together, including our own 2006 stitching, have normally involved simply joining sheets along their edges. This practice produces a number of jarring and confusing areas on the map where streets do not line up, buildings are chopped off in the middle, or whole strips appear to be missing, as in this image of the join running through the Smithfield area (Figure 1).     Figure 1. Infelicities along the sheet boundaries. The gap makes it difficult to ‘read’ the map in this area. Furthermore, when a site belongs properly in the gap between sheets where data appears to be missing, there is no zone on the surface to which we can point in order to locate the site in London’s topographical space. 7  We therefore decided to create an ‘idealized text’ of the map. We allowed ourselves to make changes both minor (skewing, stretching, and redrawing sections and lines) and major (adding in significant missing buildings, and filling empty space with plausible content based on historical evidence) to create a new artifact that satisfies the need for a continuously legible surface. This lengthy process involved careful historical research, consultations of other maps, contributions by a graphic artist, and many hours of altering lines on the computer. The result is shown in Figure 2 below.    Figure 2. Idealized reconstruction of the Agas Map showing repaired join in the Smithfield area.  Documenting Our Editorial Interventions  Such substantive interventions must be carefully documented. First, we must be able to identify source documents (witnesses). TEI P5 provides straightforward mechanisms for identification of witnesses. Each distinct object (each Margary sheet, each historical map, each fragment created by the graphic artist, and our complete idealized reconstruction) can be encoded as a TEI <surface> element, as in TEI Example 1, where each <surface> is identified by a unique @xml:id, given Cartesian coordinates (where the x,y coordinates of the upper-left corner are always 0,0), and associated with a graphic object (a .jpg in these cases).    TEI Example 1. TEI elements for a Margary sheet and for the constructed ideal map. In one case, a single Margary sheet contains two ‘pages’, the top left piece and the bottom left piece (although they are arranged horizontally). We can deal with this sheet by creating a single <surface> with two <zone>s, as in TEI Example 2, where each <zone> is given a unique @xml:id and defined by its four pairs of x,y coordinates on the <surface>:    TEI Example 2. A single Margary sheet containing two segments of the map. Each of these primary <surface>s and <zone>s can be linked to <witness> elements in a witness list. TEI Example 3 shows that each <witness> has a unique @xml:id and a prose description.    TEI Example 3. Witness information is linked to <surface>s and <zone>s through the @facs attribute. Second, we need to define ‘areas of interest’ on these surfaces, also using <zone> elements in the <surface> element. For each of the other witnesses, we can define a <zone> on the <surface> of the idealized map to which the witness attests, as in TEI Example 4, where we indicate which part of the ideal map corresponds to the witness with the @xml:id of ‘w_margary_top_2’:    TEI Example 4. A <zone> on the idealized map corresponding to a cleaned-up version of one of the Margary sheets. We can now create an apparatus based on these definitions, treating the constructed idealized map as the lemma, and the other related surfaces and zones as readings. In TEI Example 5, we use the <lem> and <rdg> elements to link one of the Margary sheets to a <zone> on the idealized map. The @type attribute allows us to declare the relationship between stemma and lemma:    TEI Example 5. An apparatus entry linking one of the Margary sheets to its corresponding <zone> on the idealized map. We can now document the interpolation of missing material. In Figure 3, we see that, because of a bad join, much of a significant building, Durham House, is missing.    Figure 3. Durham House is missing along this join. Figure 4, including TEI code, shows the supplied image of Durham House in the idealized map; the people responsible are identified through pointers to <respStmt> elements that identify them and their role in this work (editor, researcher, artist), and several sources supporting the emendation are listed, including a 17th-century floor plan for the building and another map.     Figure 4. The insertion of Durham House, with (simplified) TEI encoding that documents it. Conclusion In a literary forum, taking a textual-critical approach to a graphical text is unprecedented. Normally, one would expect to see a lag between developments in textual theory and a Text Encoding Initiative response, since new conceptions of textuality stretch the capacities of the TEI. However, our new model for editing graphical texts uses existing TEI markup to document textual interventions. Our innovative use of the TEI textual apparatus captures in great detail the nature of an editorial change, the person(s) responsible, and the supporting evidence. Our next challenge is to generate the apparatus for collating emendations to this map-like text. Notes 1.  Locating London’s Past: A Geo-Referencing Tool for Mapping Historical and Archaeological Evidence, 1660–1800. See http://www.history.ac.uk/projects/research/locating-london, and the project itself at  http://www.locatinglondon.org/. 2. See ‘Mapping Methodology’, http://www.locatinglondon.org/static/MappingMethodology.html. 3. Janelle Jenstad, ed., The Map of Early Modern London, mapoflondon.uvic.ca, 2006–present. 4. See MoEML’s description of the Agas Map at http://mapoflondon.uvic.ca/map.htm. 5. See JISC Digital Media’s Graphical User Interface Design: Developing Usable and Accessible Collections, at http://www.jiscdigitalmedia.ac.uk/guide/graphical-user-interface-design-developing-usable-and-accessible-collection. 6. See chapter 12 of the TEI Guidelines, ‘Critical Apparatus’, at  http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TC.html.  7. The scope of this paper does not allow for detailed discussion of how we encode our location files in order to point locations to specific zones on the Agas Map. We record the correct point, line, or polygon on the Agas Map using facsimile, surface, graphic, and zone elements. See ‘The Facsimile Element’ in Landels et al., ‘Understand MoEML’s Website and Document Structure’, http://mapoflondon.uvic.ca/website_structure.htm#website_structure_location_docs. ",
        "article_title": "How To Edit a Map in TEI",
        "authors": [
            {
                "given": "Janelle Auriol",
                "family": "Jenstad",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Kim",
                "family": "McLean-Fiander",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Greg",
                "family": "Newton",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "scholarly editing",
            "geospatial analysis",
            "bibliographic methods / textual studies",
            "encoding - theory and practice",
            "interfaces and technology",
            "English",
            "renaissance studies",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Many methods for computational analysis of audio have been developed in the fields of music information retrieval and machine learning. Typically used for copyright protection and music recommendation, such methods can also be used to analyze audio artifacts embedded in a specific online community. Susurrant, an open-source tool I am currently developing, enables exploratory data analysis on a corpus of digital audio accompanied by textual and social network metadata. (Such a corpus can be derived from a music-centered social media platform like SoundCloud.) This tool utilizes probabilistic topic modeling, a family of techniques for inferring latent variables (‘topics’) that could have generated the observed data (Blei, 2012). Sonic features (both rhythmic and spectral content) and the text of user tags and comments are analyzed jointly in a single model. The result is a kind of auditory concordance for the corpus, linked directly to associated textual features. This builds upon work using topic models to identify latent sources in audio (Hoffman et al., 2009) , model musical influence (Shalit et al., 2013), and analyze shared taste in online communities (Dietz, 2009).  Susurrant is meant to help researchers gain a new perspective on their audio corpora. It facilitates what Wendy Hsu terms ‘augmented empiricism’, a combination of ethnography and computational analysis that aims to ‘[document] social and cultural processes with empirical specificity and precision’ (Hsu, 2014). In this short paper, I will give a brief demo of the software (available at www.susurrant.org) and suggest ways that it might be useful for other scholars working with sound. Methodology Susurrant comprises several components: a database for storing audio metadata with textual data and social network graph, a script for semi-automated data collection, and a browser-based application that provides both visual and auditory display of analysis results. For the most part, these components make use of existing software packages such as the Neo4j graph database, the Essentia feature extraction library (Bogdanov et al., 2013), and the machine learning library MALLET (McCallum, 2002); thus, the visualization interface constitutes the bulk of this work’s contribution. The browser-based visualization/sonification of the corpus lets one listen to a resynthesized version of the variables characterizing each audio ‘topic’ along with representative audio samples and user comments. In another mode, it displays the subsets of the social network most closely associated with each topic, enabling comparison of the distributions of different sonic features across the community. I present a case study using an initial corpus of audio files and user commentary downloaded from SoundCloud. This corpus consists of music played in nightlife spaces that cater to queer and trans people of color, and is an integral part of my ethnographic work on queer of color nightlife and its online mediation. I will show how I am using this software to analyze sound in a specific social and cultural context.  Theoretical Implications  As well as offering a software package to facilitate multimodal analysis, this work can contribute to the theorization of ‘algorithmic listening’, a term for modes of computational analysis that have mostly been used in commercial applications but could readily be put to other uses. We are already surrounded by algorithms that listen. For the most part, these algorithms act as censors (as in YouTube’s Content ID system) or as recommendation engines (like those of Pandora, Clear Channel’s iHeartRadio, or SoundCloud). They rely on massive data stores and computing resources that are inaccessible to the end user, stripping away context and rendering their operations opaque even as they come to shape more and more of what we hear. Instead, Susurrant captures a cross-section of meaningfully related sounds as they exist within a specific interpretive community. It analyzes audio and textual features together and presents them in an integrated fashion to the researcher, ensuring that important contextual details are retained. Further, it helps us understand how an algorithm ‘hears’, when we listen to the variables learned by the model together with the original samples from which they were inferred. Conclusion The tool provides a novel means for exploratory data analysis of sound. By using sonic as well as textual data, this method calls attention to the aspects of sounds that have significance within an interpretive community and allows for exploration along different axes than those provided by extant platforms. I welcome dialogue about ways to improve this tool and extend it for other use cases. ",
        "article_title": "Susurrant: A Tool for Algorithmic Listening in Networked Soundscapes",
        "authors": [
            {
                "given": "Chris A",
                "family": "Johnson-Roberson",
                "affiliation": [
                    {
                        "original_name": "Brown University, United States of America",
                        "normalized_name": "Brown University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05gq02987",
                            "GRID": "grid.40263.33"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "content analysis",
            "natural language processing",
            "relationships",
            "video",
            "information retrieval",
            "audio",
            "music",
            "social media",
            "multimedia",
            "graphs",
            "English",
            "internet / world wide web",
            "software design and development",
            "data mining / text mining",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Information visualization plays an important role in analyzing and understanding large amounts of quantitative data, a task getting evermore important in our age of big data. The increasing volumes of digital data sources being created and the growing use of quantitative methods for their analysis in the digital humanities additionally motivates the application of information visualization approaches in this particular discipline. It is no surprise that visualization techniques, including network visualization, trees, and all kinds of diagrams and charts, rise in popularity in the digital humanities as an elegant means to convey knowledge and insights buried deep inside large heaps of quantitative data.  Information visualization does not have to stop at the stage of creating static images, which it is still largely used for. Adding interaction can be a game changer in many respects. Visual representations are often not authoritative, since they were created based on quantitative data, which in turn may have been derived through automatic processing steps. Such automatic processing can be a source of uncertainties and errors that are then reflected in visual representation. Even with an entirely noiseless data source, such as manually prepared data, inadequate mappings to visual representations may lead to misinterpretations. For those two reasons, uncritical interpretation of such visual representations is likely to cause problems. Interaction can be used to let users navigate to the data sources of visualizations and make the methods abstracting this data more transparent for users to understand and control their accuracy and validity. For text documents as the data source, interaction helps to bridge the gap between distant and close reading, to use Moretti’s terminology (2005). Thinking further along these lines, the next logical step would be the introduction of facilities for updating, steering, and improving the analytic processes through interactive feedback on the visual representation. With the mentioned interaction methods achieving a new level of control, the development of a visual analytics approach offering such possibilities comes into reach. In the following we illustrate our effort towards this goal for the analysis of text documents with approaches developed in the context of the project ePoetics—Corpus Analysis and Visualization of German Poetics Towards an ‘Algorithmic Criticism’. 1  Interactive Visualization Concepts We developed two interactive visualization methods to support literary analysis with the help of advanced navigation concepts. Both visualize text documents at different levels of abstraction, including the distribution of various details. These include search terms, available annotations, and automatically extracted information, e.g., named entities or derived concepts. With the simultaneous depiction of overview and detail, visually emerging patterns can be readily perceived and the reason for their occurrence can be identified quickly. While the approaches were developed with the goal of offering both overview and detail at the same time, they differ in their scope of application. The first one provides a multi-level visual abstraction of a single literary work (Koch et al., 2014). It supports researchers in analyzing text from different ‘distances’, acknowledging the inherent hierarchical structure of a document. The second approach offers just two abstraction levels but is designed to let researchers compare an aspect of interest between different texts (John et al., 2014). Data The ePoetics project aims at analyzing and visualizing a digitized collection of very specific historic documents, namely 20 selected German poetics from the years 1770 to 1960 (Richter, 2010). Poetics are secondary sources including discussions and criticism of literary texts that can be regarded as one of the building blocks of modern literary studies. The corpus was digitized through a double-keying process, and layout, structure, and contained literary concepts were manually annotated.  Approaches  The multi-level visualization approach enables a researcher to browse and inspect documents based on the inherent structure of the document, e.g., chapters, subchapters, pages, and lines (Figure 1). It encompasses several views to show abstractions of the text, results of search requests, and annotations appropriate to the user’s needs. These views can be attached to each level, as is most adequate for the task at hand. The hierarchical visualization and the navigation concept are based on the SmoothScroll approach (Wörmer and Ertl, 2013). It provides analysts with means to navigate through the visualization and to keep track of the current position across all aggregation levels.  As an interactive visual approach to literary text analysis, it combines the concepts of distant and close reading. Analysts are provided with a complete overview of the document and visual representations of different levels of abstraction that depict aggregated information about text passages of different lengths.    Figure 1. Schematic representation of a three-layer SmoothScroll view. The left layer displays a coarse view of the entire text document, the right layer a detailed but clipped view of individual lines. Highlights indicate which portion of the less detailed layers correspond to the section visible on the detail layer (see Koch et al., 2014). Several abstract views can be attached to each layer, including word clouds, bar charts, and pictograms. The word clouds highlight prominent concepts and serve as a summarization of a text passage. Exploring these concepts may lead to further analyses and is conducive to the development of new hypotheses and ideas. Bar charts show the number of occurrences of annotations—e.g., persons, quotes, or search terms in a section of the text—while pictograms display the location on a small multiple of the respective page. This gives users an overview of the distribution annotations and helps them to find passages for further analysis. An additional feature is to display facsimiles of the original pages. This gives the analysts immediate access to all non-textual information. This may include handwritten text or pictures, which will typically look different when converted to a digital text format. The approach assists in literary text analysis with a flexible combination of different visual abstractions that can be adapted to summarize all the information that is important for the task at hand. An analysis example is shown in Figure 2. In the depicted configuration, the term ‘Wallenstein’ (3rd word cloud) arouses the analyst’s interest. After selecting ‘Wallenstein’ the bar charts and pictograms show the distribution of this term and let her navigate quickly to all relevant text passages. In this case, additional literature related to ‘Wallenstein’ is of particular interest to the analyst. As all citations are automatically identified and highlighted on multiple levels of abstraction, she can easily find and analyze them, and thus quickly gain a valuable resource for further studies. Figure 2. Emil Staiger’s ‘Grundbegriffe der Poetik’ divided into layers, showing chapters (word clouds), subchapters (bar charts and pictograms), pages (pictograms), lines of text, and scanned images of the actual pages. The second approach we want to discuss is designed to support and expedite the analysis of multiple texts in parallel. The implementation is depicted in Figure 3. Three selected text sources are displayed next to each other as a ribbon. Users can select text passages for analysis, which open up a text box for the document at the corresponding position. Green bars show the position of the selected annotations. The selected text passages are placed next to each other for easy comparison. The scrollbars allow navigation through the documents. For further analysis, users can search for arbitrary terms whose distribution is displayed as orange bars on the ribbons.    Figure 3. Three selected texts are displayed. Each of the sources has a scale indicating the page number of the original documents to the left, a ribbon showing the position of the selected annotations (green bars), and another for the search results (orange bars). The focus + context technique of this approach supports a smooth switching between distant and close reading of scholarly sources. It allows the comparison of text passages across different documents to contrast and compare texts from, e.g., different authors or different periods of time. The overview visualization enables the comparison of multiple documents on an abstract level with respect to the distribution annotations and terms chosen by the analyst, while the text boxes allow for free navigation through the single texts and make it easy to find interesting passages for comparison. Future Work The approaches have been evaluated through expert feedback that suggests that they are both effective for literary analysis. For future work, we are planning a comprehensive study of their effectiveness compared to existing text analysis approaches and are aiming at examining different analysis strategies accommodated by the approaches. As previously mentioned as a general goal, we are currently working towards a comprehensive analysis approach, including automatic processing of text and other data that can be steered and adapted interactively according to users’ needs. We have already conducted multiple experiments to include such methods into our approaches, and we plan to extend these techniques for various new applications. We believe that the analysis of other types of text and literary works can benefit considerably from the visualization and interactive techniques developed in the ePoetics project. Note 1. ePoetics (www.epoetics.de) is an ongoing research collaboration between the University of Stuttgart and the Technical University Darmstadt funded by the German Federal Ministry of Education and Research (BMBF). ",
        "article_title": "Interactive Visual Analysis Of German Poetics",
        "authors": [
            {
                "given": "Markus",
                "family": "John",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Steffen",
                "family": "Koch",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Florian",
                "family": "Heimerl",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Andreas",
                "family": "Müller",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": "Ertl",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Jonas",
                "family": "Kuhn",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "german studies",
            "visualisation",
            "text analysis",
            "English",
            "digital humanities - facilities"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " 16 November 2014 marks the fifth anniversary of the Australian government’s apology to Forgotten Australians and Former Child Migrants. When then–prime minister Kevin Rudd stood in the Great Hall of Australia’s Parliament House in 2009, he moved a motion of apology to the approximately 500,000 people who were placed in orphanages, children’s homes, and other institutions in Australia in the 20th century. In that speech was a line committing to ‘a national database that will collate and index existing state identified records into a national searchable data base, accessible to state and other care leaver services and also directly to care leavers themselves’ (Prime Minister, 2009). The resulting three-year National Find & Connect Web Resource Project (http://www.findandconnect.gov.au/) concluded in November 2014. Though work on the web resource will continue (at a substantially reduced level) for some time, this milestone provides a significant opportunity to examine one of the largest publicly funded digital history projects in Australia. In this paper, senior members of the project team will reflect on what worked and what could have been done differently—on success stories and lessons learned. In preparation for funding the proposed ‘national database’ the government undertook a scoping study, looking at activities around the country. Some states did have existing guides to institutions and archival records, but these were static, point-in-time print documents. By contrast, in Victoria researchers from the University of Melbourne and Australian Catholic University were already working digitally, as part of the Australian Research Council–funded project Who Am I? The Archive as Central to Quality Practice for Current and Past Care Leavers (Forgotten Australians). A significant component of the project was the Pathways dataset and web resource, which documented the history of child welfare, collections of records, and key resources (including publications and images) in Victoria from the 1840s to the present (O’Neill et al., 2012; McCarthy and Evans, 2012). As noted by the government in September 2010, ‘It is important that the website design is based on sound principles which will stand the test of time and cater to the complexity of the content and search functions’, and that ‘No other example of a specialised website designed to cater for care leavers, with the search functionality and archival integrity of Pathways, was identified in Australia or overseas during the scoping study’ (Department of Families, 2010). Following a tender process, the core Who Am I? project team was entrusted with expanding the Pathways model nationally. In this paper, we will explore how the project team scaled up a local, relatively contained state-based resource produced as part of a research project to produce a national, government-funded resource utilised by Forgotten Australians and Former Child Migrants and their families, as well as support services, advocacy groups, governments, record holders, past providers, researchers, and the general public. The Find & Connect web resource covers the history of out-of-home ‘care’ for children and young people spanning eight states and territories and nationally (each jurisdiction with its own legislation and context) and more than 150 years, with a primary focus on 1920–1989. At the time of writing, the resource consists of nine underlying databases and more than 1.5 million words of scholarly content, presented to the general public through more than 16,000 web pages. The content has been created and curated by 25 core staff (with assistance from many others) including archivists, historians, and technology staff, overseen by three senior academics from the University of Melbourne (Associate Professor Gavan McCarthy and Professor Cathy Humphreys) and the Australian Catholic University (Professor Shurlee Swain). With regard to technology, the data collected is structured as a nonhierarchical network of entities and relationships, utilising international standards for the description of entities, archival resources, publications, and digital objects while also extending those standards where required to meet the needs of the project. Over the course of three years, archivists collaborated with technology staff to develop capability in rendering, indexing, and visualising content, including standardised XML (McCarthy et al., forthcoming). Underpinning all this, the technology base of the eScholarship Research Centre was expanded to ensure more robust preservation of data and support high-availability web services. Challenges emerged in a number of key areas:  •  Technology: Some of the underlying tools were utilised due to their informatic strength and proven capability; however, legacy issues created problems when working as part of a distributed national team and development of the underlying technology was not as rapid as initially hoped.   •  Web design and usability: In attempting to present complex historical narratives and networked information models to nonspecialist audiences, the team uncovered a range of usability and design issues. Some of these have been largely overcome while others require further work (Jones and O’Neill, 2014).   •  Resourcing: The balance of archivists to historians to technology staff was planned and budgeted from the outset, based on a projected understanding of the project and its parameters. Some of these early decisions put limits on what was possible later in the project.   •  Scalability: Some of the processes and practices around content development, editing, quality assurance, and informatics that produced high-quality results for the Who Am I? project were put under significant strain when working at scale.  Despite these challenges, the project has achieved notable success, including substantial visitor numbers, very positive feedback from stakeholders and the community, markedly improved results in usability testing, and the fostering of communities of practice around the country with an enhanced awareness of the issues facing the sector. In terms of scholarship, the project has researched and documented the history of institutions providing out-of-home ‘care’ across the country, and the records and resources related to those institutions, in more detail than ever before; and this information has been captured and stored in sustainable, extensible ways for the benefit of researchers and the community at large. Our experience also has findings relevant to public history and digital scholarship more broadly. The team’s long, iterative approach to content development and interest in sustainability and persistent citability was distinctly different from most other government websites and online content development projects. Conversely, historians working around the country found their practice and priorities were influenced and shaped by an unfamiliar immediacy and regularity of feedback on their work (much of which was first made publicly accessible while still just a work-in-progress) from users and the large, multi-disciplinary project team of which they were a part. Historians and archivists also met regularly with users and stakeholders in the project, and this ongoing engagement in a sometimes charged, politically active space meant academic freedom was tempered by the need to negotiate language, representation, and content with individuals, groups, and governments. In this long paper, senior members of the Find & Connect web resource team will explore the success of the project, reflect on the challenges encountered, and raise some of the implications for public and digital history more broadly. They will conclude by outlining their hopes for the Find & Connect web resource in the future, and the ways it could be linked more effectively to other digital initiatives within Australia and internationally. The Find & Connect Web Resource Project is funded by the Australian government’s Department of Social Services (previously the Department of Families, Housing, Community Services and Indigenous Affairs). ",
        "article_title": "Scaling Up Digital Public History: Lessons Learned From The Find & Connect Web Resource Project",
        "authors": [
            {
                "given": "Michael Alastair",
                "family": "Jones",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": "Rachel",
                "family": "Tropea",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "information architecture",
            "historical studies",
            "repositories",
            "user studies / user needs",
            "metadata",
            "project design",
            "archives",
            "sustainability and preservation",
            "English",
            "management",
            "cultural infrastructure",
            "knowledge representation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " It’s the canonical founding story of the digital humanities: In November 1949, the Italian Jesuit scholar Father Roberto Busa came to IBM headquarters at 590 Madison Avenue in New York to meet with the founder and head of the company, Thomas J. Watson Sr. He was seeking technical and financial support for the project of mechanizing the building of a massive lemmatized concordance to the works of St. Thomas Aquinas. Drawing on Father Busa’s own papers, recently accessioned in Milan, as well as IBM archives and other sources, I aim to uncover the historical complexities behind the founding story, the history behind the myth. In this paper, based on a book in progress, I’ll look at a few details of that 1949 meeting between Father Busa and Watson using the metaphor of the exploded-view diagram—like those used by engineers, descended from examples in the notebooks of Leonardo, for example—suspending and zooming in on constituent details of the meeting and the meanings each detail invokes, in order to understand the story as part of a larger history of computing in the 1940s and 1950s, and as part of the prehistory of the digital humanities.  In a paper delivered at DH 2014, Geoffrey Rockwell and Stefan Sinclair briefly consider Father Busa’s work with IBM, along with other examples of ‘the period of technology development around mainframe and personal computer text analysis tools, that has largely been forgotten with the advent of the web’ (2014). They advocate a media-archaeology approach (they cite Zieliniski, 2008; I’d add Parikka, 2012) as a way to begin to ‘understand how differently data entry, output and interaction were thought through’ in the mainframe era (2014). I share those goals. In my case, I’ll also draw on the related approach known as platform studies, examining specific calculating and computing platforms available to Father Busa and his collaborator at IBM, Paul Tasman—including the ‘bootstrapped’ systems they configured for punched-card processing, which soon included early tape-drive stored-program computers. The affordances of these platforms led to the inclusion in Father Busa’s research agenda on the  Index Thomisticus of similar work on the newly discovered Dead Sea Scrolls in the later 1950s. I’ll look at relationships between the layers in each platform: hardware, software, human agents, history, and culture. My central question is how specific technologies afford and constrain cultural practices, including the academic research agendas of humanities computing and, later, digital humanities.  My point of departure is a seemingly insignificant detail: a poster Father Busa says he carried into the CEO’s office that read, ‘The difficult we do right away; the impossible takes a little longer’. In his telling, the poster becomes a rhetorical device for persuading IBM to collaborate. I show that in the 1940s, at least for Watson, it would also have been closely associated with the SeaBees, the U.S. Navy Construction Brigade, and its famous ‘can-do’ philosophy, and, thus, with the wide context of the postwar period of reconstruction and of the role of technology in the new era, including IBM’s own complicated history in Europe. Just months before the meeting with Father Busa, IBM had created its World Trade Corporation, and, significantly, it was to this international subsidiary and the technical head of it, Paul Tasman, that Father Busa’s project was assigned. The collaboration belongs in that wider historical context, as well as the more specific context of the particular punched-card machines Busa and Tasman went on to use for the initial experiments in machine-generated indexes and general ‘language engineering’, the results of which were first published in Busa (1951). I’ll look at how the punched cards continued to be used for years, even in hybrid (and transatlantic) workflow systems that, at one point, included punching, sorting, and collating the cards in Italy, then bringing them to New York for transfer to and sequential processing on the magnetic tapes of the room-sized IBM 705. (Publicity photos from the era show Father Busa sitting at the control console of the 705, its tall tape drives in the background.)  I’ll also consider one example of an ‘adjacent possible’ platform (Johnson, 2014, 156), a road not taken, as it were: the SSEC, the first publicly recognized large-scale calculator (and about the status of which as a stored-program  computer there has been a history of debate). Father Busa would have had to walk past this big machine on his way to the fateful meeting, a kind of pre-mainframe, operating out in public, on display in the custom-designed showroom on the street level of 590 Madison Avenue, on the corner of 57th Street, and he would perhaps have seen the plaque mounted there with Watson’s words, dedicating it to science, education, and government, and the exploration of ‘the consequences of man’s thought to the outermost reaches of time, space, and physical conditions’. The machine was in newsreels, a  Vogue magazine spread, newspapers (one cross-marketing ad with Shell Oil included a cartoon depicting it as a colossal female ‘Oracle on 57th Street’), and was even used in an early cold-war Hollywood film noir, which cast IBM engineers and operators as extras, all during the years Father Busa was demonstrating his system for linguistic analysis using the available punched-card machines. The example of the SSEC likely led to Busa’s interest within a few years in using the 705 (in a line that replaced the SSEC in IBM’s public showroom starting in 1952, just when Busa and Tasman were demonstrating their techniques at IBM). Despite the movies and other representations of the popular imagination, fueled by publicity campaigns, the ‘questions’ posed to the ‘oracles’ that were early 1950s computers were, of course, in the form of mathematical calculations, and the data to be processed was for the most part numbers, not natural language. But according to Tasman, the development of Information Retrieval at IBM, including the standard KWIC (Key Word In Context) protocol, grew out of Father Busa’s early experiments (1968).  The whole campaign around the SSEC, in terms of its mode of publicity and its computing aims, bears a striking resemblance to the most recent IBM campaign this past season (Fall 2014), at a different showroom, down at Astor Place in New York, with another personified machine, but this time one that  does answer natural language questions by recourse to very deep data to which it applies artificial intelligence in the form of cognitive computing. Watson, named for the founder and first CEO, is in part descended from Father Busa’s project, started in that CEO’s office, and on the kind of research it represented, its processing of language instead of numbers, and eventually (and perhaps at first accidentally), its focus on viewing a text-base in the way a database would later be viewed—as a store of information to be mined, analyzed, retrieved effectively, and even rearranged algorithmically in order to reveal patterns or answer new questions. At mid-century, the SSEC may have been (literally) the poster-child for these aspirations for computing, but it was the ‘humanistic’ work with punched-card machinery, like the experiments of Father Busa and Paul Tasman, trying at first just to sort words more efficiently, only later learning they could engage in more sophisticated text analytics, that marked a significant swerve that would eventually lead into these larger aspirations—not to mention to what came to be known as the digital humanities. It’s in that more complicated sense that the prehistory of the digital humanities, at least one prehistory, is punched-card humanities.  ",
        "article_title": "Punched-Card Humanities: Roberto Busa and IBM in Historical Context",
        "authors": [
            {
                "given": "Steven Edward",
                "family": "Jones",
                "affiliation": [
                    {
                        "original_name": "Loyola University Chicago, CUNY Grad Center ARC (2014-2015), United States of America",
                        "normalized_name": "Loyola University Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04b6x2g63",
                            "GRID": "grid.164971.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "natural language processing",
            "digital humanities - nature and significance",
            "information retrieval",
            "English",
            "history of Humanities Computing/Digital Humanities",
            "concording and indexing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper addresses exploratory search in large collections of historical texts. By way of example, we apply our method to a collection of documents comprising dossiers of the former East German Ministry for State Security and classical texts. The bases of our approach are topic models, a class of algorithms that define and infer themes pervading the corpus as probability distributions over the vocabulary. Our topic-centered visual metaphor supports exploring the corpus following an intuitive methodology: First, determine a topic of interest; second, suggest documents that contain the topic with ‘sufficient‘ probability; and third, browse iteratively through related topics and documents. Our main focus lies on providing a suitable bird’s-eye view onto the data to facilitate an in-depth analysis in terms of the topics contained. * * * When dealing with large collections of digitized historical documents, very often only little is known about the quantity, coverage, and relations of contained topics. In order to get an overview of the topics covered, an interactive way to explore the data is needed that goes beyond simple ‘lookup’. The notion of exploratory search has been coined by  to cover such cases.  Consider a large corpus of documents. Typically, we know the source and the broader scope of such corpora, but not necessarily the content of individual documents. One classical option to explore this data is based on keyword search. While this approach is useful when the user ‘knows’ what she is looking for, an iterative exploration of the corpus is not possible. Our approach is a structured one. We provide the user with a bird’s-eye view of the data. She then identifies topics of interest and finds the documents related to them. Additionally, documents can also be related to other topics, a connection that helps to reveal new and interesting insights previously unknown. We are also able to identify different contexts, i.e., topics, in which specific terms appear. Especially when working with historical texts, this might help to reveal new aspects of known concepts. Topic modeling  has become one of the main tools for unsupervised text analysis and for gaining insight into corpora via exploratory search and analysis by identifying semantic classes of words in the corpus, coined ‘topics’. An introduction on how topic modeling can help humanists in their research is given in  .   However, little effort has been made to develop methods to use the outcome of these models in applications  visually.  Traditional linguistic approaches such as the vector space model  translate documents into high-dimensional feature vectors (typically in combination with, e.g., the tf-idf  ] term weighting). Visualizing such high-dimensional structures is a research field on its own. Established approaches include projective techniques, like the Text Map Explorer  , Multidimensional Scaling (MDS)  , Sammon’s mapping  , and neuro-computational algorithms like Self Organizing Maps (SOM)  .   Compared to the algorithms used in this paper, the insights obtained from a pure vector space model are rather limited. In fact, we reduce the dimensionality of the space in which documents are defined from the size of the vocabulary to a number of semantic classes (topics) that appear across documents.  Topic Models A topic model is a Bayesian hierarchical probabilistic model. It defines an artificial generative process for document generation, describing how the actually observable data (the words in the documents) get into their place. In a topic model, this is controlled by two latent factors: the topics themselves and the documents’ topic proportions. A topic is defined as a probability distribution over the word simplex—i.e., in every topic each word has a certain probability, and the probabilities in each individual topic sum to 1. The set of words with the highest probability is assumed to describe the individual topics thematically. The second factor, the document topic proportions, is again a set of probability distributions (one for each document) defined over the topic simplex. Every topic gets some probability in a document, and their probabilities sum to 1. Simply put, the words that we see in a document are then generated by first finding a topic through the document’s distribution over topics and then finding a word from the chosen topic. Both choices are random draws from their respective distributions. During inference, we reverse this process in order to get approximations for the governing latent factors that best give rise to the observed words—that is, we want to find the setting of the latent factors for which the observed words are highly likely. In most cases, the outcome of such models is presented as a set of sorted word lists for each topic, topic similarity is being neglected, and only hand-picked topics are selected as examples.  Visualization Approach Visualization is our method of choice to inspect the knowledge uncovered from the data. However, the model outcome is obviously inappropriate for direct visualization. Without using thresholds, presenting entire probability distributions as sorted lists of words and values is not very handy and quickly results in information excess and cluttered visualizations. Even working with thresholds does not immediately lead to parameter settings that are independent of the input data, e.g., how many words are actually necessary to obtain a reasonably good impression of a topic found by the model. That is, depending on the semantic quality of words and topics, a flexible level of detail is necessary to identify meaningful information in a topic, i.e., in a distribution over words. On the other hand, the amount of information relevant for each element of the topic model is assumed to be rather small. Therefore, the visual implementation of these elements should focus on the pivotal parts of the distributions, while increasingly disregarding irrelevant parts. In the end, the relations between the input documents, the latent topics found by the model, and the actual probabilities of a topic’s keywords are the elements containing interesting insights about the data. We present an interactive visual analysis tool to find and display these relations. Our approach is closely related to that of  , who also attempt to directly visualize the output of topic models. However, their approach generates a set of static websites that can be browsed to explore the dataset, providing a lightweight but largely text-based application to solve the visualization tasks. There exists an overview over the different topics, but each topic is described by the three most probable words only. There is no possibility to further investigate a topic  and to keep track of the others. As a framework for our visualization we chose OpenWalnut. 1 This framework primarily aims at medical imaging but is highly modular and can be easily reused to fit our requirements.  We define the following distinct exploration tasks that a topic model is fit to execute:  1.  Examining a topic. Examining a single topic is difficult because it is a probability distribution. In the visualization, this information helps the user to quickly identify key words and their relative importance for a topic.    2.  Overview of the Topics. This task visually summarizes the set of latent topics found by the topic model.    3.  Finding Different Semantics of Polysems and Homonyms. One advantage of topic models is the automatic disambiguation of semantic meanings of words into topics.    4.  Identifying Documents Covering a Topic. This task is at the core of exploratory analysis: Having identified a topic of interest, explore a set of documents covering it.    5.  Finding Related Topics of a Document. Inspect topics related to a document, or, in a transitive way, documents related to these topics.   We present visual implementations for these tasks to provide the user with interactive means to browse through relations between documents, topics, and words. This way, the user uncovers expected or (more interestingly) unexpected facts that eventually lead to interesting documents. Using smooth level-of-detail transitions and by interacting with topic distribution charts, the user freely navigates through the data by concatenating single exploration tasks—following focus-and-context concepts and an intuitive methodology: overview first, details on demand. Experiments We report use cases of fitting topic models to two different datasets. One of the datasets is the series of publications ‘The GDR through the Eyes of the Stasi: The Secret Reports to the SED Leadership’ (German: ‘Die DDR im Blick der Stasi. Die geheimen Berichte an die SED-Führung’) that reveal the Stasi’s 2 specific view of the GDR, containing references to real and perceived oppositional conduct as well as to economic and supply problems.   The second dataset is the ECCO-TCP, 3 a set of classical literature and non-fiction texts.  In Figures 1 through 5 we pictorially describe how the defined exploration tasks are met using our visualization. Note that we omitted the application’s control panel in Figures 1–3 for spatial reasons.",
        "article_title": "Exploratory Search Through Interactive Visualization of Topic Models",
        "authors": [
            {
                "given": "Patrick",
                "family": "Jähnichen",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Patrick",
                "family": "Österling",
                "affiliation": [
                    {
                        "original_name": "Image and Signal Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Tom",
                "family": "Liebmann",
                "affiliation": [
                    {
                        "original_name": "Image and Signal Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Gerhard",
                "family": "Heyer",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Kuras",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Gerik",
                "family": "Scheuermann",
                "affiliation": [
                    {
                        "original_name": "Image and Signal Processing Group, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "content analysis",
            "natural language processing",
            "interdisciplinary collaboration",
            "English",
            "semantic analysis",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Motivation A Variant Graph is a data structure that can be used to model the similarities and differences among various editions of a text (Schmidt et al., 2009). Last year we presented a set of design rules for Variant Graphs (Jänicke et al., 2014) that are implemented by the web-based tool TRAViz. 1 Figure 1 shows a Variant Graph for 24 different English translations of  Genesis 1:1.     Figure 1.  Genesis 1:1 in 24 English translations.  Designing the graph that way, the humanist is able to analyze similarities and differences among various text editions on verse level. Although TRAViz can also be utilized for larger text entities such as sections or chapters, the resultant visualizations are hardly readable, and an analysis of the Variant Graphs becomes a laborious task. Therefore, TRAViz remains a close reading visualization tool only to be used when the user has a specific research question for a desired text part, such as the analysis of numerous different translations of the Sixth Commandment, commonly recited as, ‘You shall not kill’ (Jänicke et al., 2015). Research questions like, ‘For which Bible books are the various translations most similar?’ or ‘For which chapters of book X do the rather similar translations A, B, and C differ most and why?’ cannot be answered with TRAViz. As Moretti suggests, such kind of questions require distant reading approaches to be answered (2005). This paper proposes a visualization that offers a distant view on Variant Graphs calculated for each Bible verse in order to support the humanists in detecting and analyzing occurring patterns on higher text hierarchy levels (the entire Bible, books, chapters) as provided by TRAViz (verses). The Bible Corpus The Bible is a very good use-case for the purpose of designing a distant reading visualization for Variant Graphs. First, it is a very influential and well-known text, which supports easy evaluation of results. Second, its structure includes a four-level hierarchy that makes views of varying distance on the text possible: the whole Bible (level 1) consists of books, each book (level 2) is divided into chapters, and each chapter (level 3) is composed of verses. Each verse (level 4) can be visualized utilizing a close reading visualization like TRAViz, but all other hierarchy levels require a distant reading solution. The Bible corpus of our project consists of 24 English Bible translations spanning a period from the 14th (Wycliffe Bible) to the 21st centuries (e.g., Catholic Public Domain Version). It includes editions in Middle and Modern English, a great variety of translation dependencies (e.g., editions based on the King James Version) as well as a diversity of translation motivations (e.g., simplified language in the Bible in Basic English). The versatility of the corpus allows for a multitude of research questions to be asked by the humanists of our project. One of their use-cases is outlined below. Visualization Design Figure 2 shows a screenshot of the system that our humanists work with. Arranged in columns, the top panel lists all available Bible editions either sorted by year of publication or in alphabetical order. On demand, the user is able to compose a desired set of editions by clicking the corresponding checkboxes. Below the Bible editions, the humanist is able to tweak the visualization dependent on the given research question. In particular, the user can  • Define a threshold value for the majority of editions.  • Either highlight text passages that are similar or dissimilar to a certain extent from the predefined majority.  • Adjust the percentage of words that need to be similar or dissimilar regarding the majority to highlight a text passage. The bottom panel visualizes a ‘(dis-)similarity fingerprint’ for all 24 editions of the Bible on level 1; the user can interactively navigate between the various hierarchy levels. Each Bible book receives a rectangular block with its width reflecting the number of chapters. According to the configuration in this example, a tiny rectangle for a chapter of an edition is drawn in its assigned color, 2 if at least 50% of its contained words differ from the majority of at least 10 editions. The resultant pattern reflects, e.g., three salient editions for the New Testament, which reveals individual translation styles: the Wycliffe Bible (1380, dark yellow) is the only translation in Middle English, whereas the God’s Word Translation (1995, blue) and the Bible in Basic English (1949, light green) aim to be understood very easily nowadays and, thus, choose to deviate from other editions that tend to be more antiquated and sophisticated in language and style.     Figure 2. Distant reading of 24 Bible editions. A Use Case Concentrating on similarity (80% similarity, majority of seven) highlights Bible editions that are very similar in almost all chapters of all books (Figure 3). Most of these editions are based on the King James Version, which is known as the most influential translation (Ryken, 2011). But surprisingly, albeit claiming to be ‘as exact a translation as possible’ 3 in Modern English from the original languages Hebrew and Greek, the Darby Bible (1890, light blue) is also highlighted as very similar.   To determine the role of the Darby Bible, we remove all Bible editions not (apparently) connected to the King James Version. Now, clicking the book  Matthew and highlighting dissimilarity (50% dissimilarity, majority of seven), we detect a cluster of derivations for verses 16–18 of chapter 7 (Figure 4). As implied on the book-level, the chapter-level confirms the Darby-deviations by highlighting the corresponding passages in the individual verses based upon a majority of four (Figure 5).     Figure 3. The whole Bible (level 1) visualizing similarity.    Figure 4. Dissimilarity of selected editions in  Matthew (level 2).     Figure 5. Dissimilarity of selected editions in  Matthew 7 (level 3).  Indeed in  Matthew 7:16 (Figure 6), Darby and some other editions differ in word order (but not so much in the translations); among others he chooses the elder word ‘ye’ instead of ‘you’ and as the only translation he writes ‘a bunch of grapes’ instead of ‘grapes’ and instead of ‘figs of thistles’ he writes ‘thistles figs’ (?). In  Matthew 7:17 (Figure 7) and  Matthew 7:18 (Figure 8), the main structure of the sentence remains in Darby, but some words are translated differently: instead of ‘bring forth’, Darby uses the word ‘produce’ (followed by World English Bible [2000, brown] and A Voice in the Wilderness [2004, purple]), the word ‘nor’ instead of ‘neither’ (followed by A Voice in the Wilderness), and when fruit are described, Darby uses the not morally associated adjective ‘bad’ instead of ‘evil’, and ‘worthless’ instead of ‘corrupt’.     Figure 6.  Matthew 7:16 (level 4).     Figure 7.  Matthew 7:17 (level 4).     Figure 8.  Matthew 7:18 (level 4).  All in all, with this setting, even when choosing passages marked as dissimilar, the close view confirms what the distant view implied, which is, how close the Darby Bible sticks to the other editions concerning word order, language, and most of the words themselves. Most deviations are single words that are substituted by synonyms or something similar. Nevertheless, the Darby variations tend to be the very obvious ones, next to those of A Voice in the Wilderness, but rarely seem to change the meaning of the text in a significant way. Thus, it seems that the Darby Bible, which tried to translate the ancient languages as exactly as possible, may have had a significant impact on later translations based (not only) on the King James Version. Conclusion In contrast to projects dealing with a rather small number of text editions on a close reading basis, 4 the presented novel technique provides a distant reading of Variant Graphs for a potential high number of editions. Just reading 24 translations of the Bible would require a huge amount of time, and comparing those editions would take even longer. Being able to see similarity and dissimilarity on various text hierarchy levels enables the user not only to save time by directing to passages that could be of interest, but it can even raise new and worthy research questions. This way of visualizing bears even the potential to cause serendipity by showing relations that have not yet been seen.  In the future, we plan to provide an Open Source library to enable the application of the visualization for other hierarchically structured texts. Visualizing the (dis-)similarity between various editions of Homer’s epics could be one of the interesting examples. Being an important work for philologists, we could also extend the visualization to display the occurring transposed verses. Acknowledgements The authors like to thank the Baker Publishing Group for the permission to include the God’s Word Translation in our database. This research was funded by the German Federal Ministry of Education and Research. Notes 1. http://www.traviz.vizcovery.org/. 2. The fixed edition order simplified the crucial task of selecting colors. Following the suggestions made by Harrower et al. (2003), we defined a set of 24 varying colors ordered by alternating hue and saturation intensity, so that visually similar colors are not placed next to each other. 3. According to the introduction to the 1890 edition, which can be found online at http://www.ccel.org/bible/jnd/darby.htm#2. 4. Other projects also provide solutions for the visualization of textual variance. CollateX (http://collatex.net/), primarily focused on developing alignment algorithms for text editions, uses the GraphViz library (http://www.graphviz.org/) to visualize rather small texts analogous to TRAViz. Some web-based platforms allow reading of parallel texts in the browser. Juxta Commons (http://juxtacommons.org/) shows variant patterns between two texts, and with Versioning Machine (http://v-machine.org/), multiple editions are displayed next to each other. Both tools support only close reading for a small number of text editions. ",
        "article_title": "A Distant Reading Visualization for Variant Graphs",
        "authors": [
            {
                "given": "Stefan",
                "family": "Jänicke",
                "affiliation": [
                    {
                        "original_name": "Leipzig University, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Annette",
                "family": "Geßner",
                "affiliation": [
                    {
                        "original_name": "Göttingen Centre for Digital Humanities, University of Göttingen, Germany",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "relationships",
            "text analysis",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Fixed in the public consciousness by countless appearances and interviews, and intimately associated with a wide range of affairs of state during the 1970s, including the US invasion of Cambodia, withdrawal from Vietnam, ‘opening to China’, détente with the Soviet Union, and the Watergate affair, the public persona of former national security advisor (1969–1975) and US secretary of state (1973–1977) Henry A. Kissinger has consistently fascinated historians. 1 A decidedly emotional champion of dispassionate ‘realpolitik’, a Cold Warrior and ‘secret swinger’, 2 questions about Kissinger’s fascinating paradoxes and internal contradictions have been a significant focus of the study of ‘Kissingerology’ (Hanhimaki 2003).  Trying to understand what at first glance can appear to be apparently incompatible motives and behavior, historians soon encounter a second problem—one of scale. A classic ‘big data’ catch-22, the extensive and vast array of documents, diplomatic cables, transcripts, and other correspondence available for study greatly complicates the task of historically situating and interpreting Kissinger. This deluge of information is an increasingly common frustration for historians of the 20th century, and as larger and larger archives of human cultural output are accumulated, scholars are beginning to adapt, develop, and employ tools, methods, and interpretive frameworks from fields like computational linguistics, visual design, and textual studies that can overcome ‘information overload’ and facilitate new historical interpretations of ‘big data’ archival collections.           A topic model of the DNSA Kissinger ‘memcons’. In this force-directed diagram, the documents are distributed according to their weight in the topic model and colored according to their former classification status. Formerly ‘Top Secret’ documents are colored blue, ‘Secret’ documents are colored yellow, and ‘Confidential’ are colored magenta. The size of the circles represents the number of participants.   As detailed on the website http://www.quantifyingkissinger.com, this project is an historical interpretation of the DNSA (Digital National Security Archive)’s Kissinger Collection, using techniques like data visualization, topic modeling, sentiment analysis, and word frequency and collocation analysis to facilitate a mix of ‘distant reading,’ ‘networked reading’, and ‘close reading’. The DNSA’s Kissinger Collection is a small but substantial subset of the vast volume of material generated by the Nixon administration, a collection comprising over 18,000 declassified meeting memoranda (‘memcons’) and teleconference transcripts (‘telcons’) detailing Kissinger's correspondence during the period 1969–1977, including a wide range of extremely useful archivist-supplied metadata (e.g., date, classification status, origin, etc.) that facilitates more detailed and nuanced explorations into the text analytics. Faced with the seemingly impossible task of truly understanding a man who appears to embody a host of mutually exclusive dichotomies, historians who grapple with some of the more paradoxical aspects and events of Kissinger’s policies and persona often seek to explain inconsistencies and/or emotionality as either a logical failure of his political philosophy or as irrational divergence from an otherwise ubiquitous, calculating rationality. Countering the decentralized, ‘accidental’ view of emotionalism in most of the historiography, historian Barbara Keys’ ‘The Emotional Statesman’ places Kissinger’s emotionality in the center of her analysis, arguing that it was often the rhetorical and political flexibility emotionality offered that led Kissinger to such behavior.                   Figure 1. Scatter plot graph: Frequency of words found in colocation with the word ‘laughter’ in the Kissinger memcons, in order of frequency (x-axis), by colocation (Mutual Information) score (y-axis).   Exploring Kissinger’s ‘predispositions’ towards a Soviet-preponderant worldview as a ‘natural’ emotional reaction resulting from his intense personal relationship with Soviet ambassador Anatoly Dobrynin (as detailed by Keys), the project’s word collocation analysis of the phrase ‘[laughter]’ 3 surfaces evidence of the unique and complex nature of their friendship. To Keys, this relationship was a compelling emotional factor in Kissinger’s seeming unwillingness to view the Vietnam War as anything other than a Cold War conflict, one that nevertheless served an emotional purpose: reinforcing a sense that matters remained in his and Dobrynin’s capable hands to resolve no matter how arduous and unpleasant the peace talks with North Vietnamese envoy Le Duc Tho became.            Figure 2. Force-directed diagram: ‘Bombing’ word correlation/collocation in telcons (yellow) and memcons (blue). Words colorized by hand relate to Vietnam (green) and other countries in Indochina (red).   In addition to analyzing the impact of Kissinger’s social relationships on his geopolitical outlook and outcomes, the project examines also his attitudes towards matters of secrecy and violence. A number of word collocation analyses reveal patterns suggestive of a highly selective, event-driven use of the ‘private’ telcons (which Kissinger expected to remain within his possession after his departure from the White House) versus a more diffuse, persistent use of the official, redacted memcons—as, for example, when both the word ‘bombing’ and one of a number of names for various countries in Indochina were found.           Figure 3. Line/scatter plot graph: Frequency of ‘[Cc]ambodia’ and ‘bombing’ word collocation with collocation distance) on a timeline.   The initial text analysis data averages word frequencies irrespective of the timeline, and more detailed word frequency analysis along a timeline facilitated a ‘networked reading’ approach 4 in which documents are identified for closer reading and subsequent analysis through the text analysis data, and related according to commonalities in date, word frequency, topic weight, and/or subject and participant metadata.            Figure 4. A selected ‘network’ of a group of conversations and memos from 1970 in which ‘[Cc]ambodia’ and ‘bombing’ are found in collocation, connected by participant. Edges are labeled with the date of each individual’s participation.   A complete historical understanding must by necessity include this sort of close reading of relevant documents, but a few interesting bits of possible evidence from the project’s distant and networked reading already call for further analysis: hints of ‘triangular diplomacy’ written in the proximity of graphed nodes relating to negotiations over Indochina with the Soviet Union and China, and clustering of various ‘secrecy’ topics highly suggestive of a classification scheme based upon both national security priorities and/or those of Henry Kissinger himself. Topic clusters in the model represent SALT Negotiations between the USSR and the US, Nixon’s journey to China, the Paris Peace Talks to end the war in Vietnam, the OPEC oil embargo, crises in the Mideast and shuttle diplomacy, the coup in Greece and subsequent invasion of Cyprus by Turkey, and the Rhodesian Bush War.           Figure 5. A section of the topic model visualization of the ‘memcons’. In this force-directed diagram, the documents are distributed according to their weight in the topic model and colored according to their former classification status. Formerly ‘Top Secret’ documents are colored blue, ‘Secret’ documents are colored yellow, and ‘Confidential’ are colored magenta.   The examinations of this project have not been limited to historiographical inquiries of Kissinger’s foreign policy, nor to his personal/emotional motivations. By combining computational and emotional history approaches, insights about the man and the geopolitical focus of the administration he served emerge—new avenues for understanding themes of seduction, secrecy, humor, and violence. Beyond detailing and exploring existing field conversations about former US secretary of state Henry Kissinger, the project has begun to surface deeper understandings and new questions—about how this new kind of distant knowledge is formed, and the possible ramifications for historical research in general. This application of computational techniques to the study of 20th-century US diplomatic history has generated useful finding aids for researchers, provided essential testing grounds for new historical methodologies, provoked new questions, and prompted new interpretations of Kissinger and the individuals with whom he corresponded.  Notes 1. Kissinger was the only official ever to hold both offices simultaneously, from 22 September 1973 to 3 November 1975. 2.  LIFE magazine, 28 January 1972 , https://books.google.com/books?id=D0AEAAAAMBAJ&pg=PA70.  3. As well as related (but different) search and wildcard phrases like ‘[laughs]’, ‘[laugh*],’ etc. 4. An approach described by Dr. Lisa Marie Rhody. ",
        "article_title": "'\"Everything on Paper Will Be Used Against Me\": Quantifying Kissinger'",
        "authors": [
            {
                "given": "Micki",
                "family": "Kaufman",
                "affiliation": [
                    {
                        "original_name": "CUNY Graduate Center, United States of America",
                        "normalized_name": "The Graduate Center, CUNY",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00awd9g61",
                            "GRID": "grid.253482.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "organization",
            "linguistics",
            "interdisciplinary collaboration",
            "archives",
            "semantic analysis",
            "analysis and visualisation",
            "data mining / text mining",
            "digitisation",
            "historical studies",
            "content analysis",
            "project design",
            "corpora and corpus activities",
            "rhetorical studies",
            "text analysis",
            "spatio-temporal modeling",
            "English",
            "software design and development",
            "visualisation",
            "metadata",
            "digital humanities - nature and significance",
            "sustainability and preservation",
            "resource creation",
            "and discovery",
            "databases & dbms",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Comic books are a unique and increasingly popular form of entertainment combining visual and textual elements of communication. This work pertains to making Japanese comic books more accessible. Specifically, this paper explains how we detect elements such as speech bubbles and character eyes present in Japanese comic book panels. Some possible applications of the work presented in this paper are automatic detection of text and its transformation into audio or into other languages. Automatic detection of elements can also allow reasoning and analysis at a deeper semantic level than what’s possible today. 1 We present our process for speech bubble detection in this paper.  Background A page in a comic book consists of multiple panels, in which the scenes of the comic are depicted. Each panel has multiple elements, including text and graphics. Text in comic books is typically enclosed in speech bubbles. We use elements as a term broader than ‘object’. Prior works by others have focused on separating text and graphics in documents (Wong et al., 1982), but not on comic book panels. Our work is important for the digital humanities community because we cover an art form that the community has not covered before. Approach We use a heuristic-based expert system, implemented by the authors, and machine learning for detecting elements present in images. Figure 1 shows a block diagram of our speech bubble detection pipeline. The images are segmented by the image processing system, which segments an image into regions and extracts candidate regions that are likely to contain elements of interest, which are text. Components of connected black pixels are profiled into text and non-text. Then the statistics of the candidate regions are computed, and the statistics are used to formulate the heuristics. Some of the statistics used include whether the region is enclosed by black pixels and the percentage of the area of the image the region occupies. The regions are classified whether they are elements of interest by the expert system based on the heuristics.    Figure 1. The Speech Bubble Detection pipeline. We use Naive Bayes and Maximum Entropy from NLTK (NLTK Project, 2014), and support vector machine (SVM) from WEKA (Hall et al., 2009) as our learning algorithms to detect speech bubbles because of their performance and ease of use. We compute statistics of the speech bubble candidate regions in the images. For example, we compute the number of black pixels as well as the ratio between black pixels and white pixels in the regions. We also compute the size of the region in terms of the percentage of the area of the image the region occupies, and the horizontal and vertical range of the region. We use the information as the features on which the machine learning algorithms are trained. Some of the features used are heuristic-based features inspired by the expert system. Those features are binary features, such as whether the size of the region falls within a certain range, as well as whether the region is enclosed by the edges of black pixels. We allow gaps between edges that are smaller than 3 pixels wide. We also compute the vertical and horizontal distribution of the black pixels as histograms of black pixel counts and use them as features. In total, 58 features are used. We use supervised learning and semi-supervised learning methods. In supervised learning, each candidate region is labeled as either positive or negative by humans. A positive sample means the region is a speech bubble. In semi-supervised learning, not all the training data used is hand labeled. Some of the training data is produced by the system itself. In our approach, unseen samples labeled by the trained machine learning system are added to the training data.  Speech Bubble Detection Two hundred forty-three panels from  Doraemon, a popular Japanese comic book—which amount to the total of 804 samples—are used to evaluate the expert system and the machine learning system. Each panel contains zero, one, or multiple speech bubbles.   Table 1 shows the performance of the expert system on the panels. The expert system achieves 95% accuracy with 91.3% precision, 97.7% recall, and the F-score of 0.944. Since the possible applications of our work require no speech bubbles lost in the process, high recall is more important than high precision. Table 1. The performance of the expert system.    Accuracy Precision Recall F-score   Expert 0.95 0.913 0.977 0.944   The same 243 panels used in the evaluation of the expert system are used for evaluation of the machine learning system. The 804 samples are divided in a 2-to-1 ratio into 539 training sets and 266 test sets. The Max Entropy is trained multiple times (10 and 20 iterations) before tested on the test sets. Table 2 shows the performance of the three algorithms averaged over 5 runs.  All three algorithms produce results comparable to that of the expert system. Notably, the Maximum Entropy does very well in terms of recall, scoring above 0.98, which is better than the score of the expert system. Among the machine learning algorithms the SVM produces the best result. Whether the region is enclosed by the black edges is one of the best discriminators. Table 2. The performance of the machine learning algorithms.    Accuracy Precision Recall F-score   Naive Bayes 0.908 0.897 0.889 0.892   Max Ent (10 iter.) 0.934 0.881 0.981 0.928   Max Ent (20 iter.) 0.928 0.863 0.985 0.920   SVM 0.943 0.900 0.973 0.935   In our semi-supervised learning experiment, the machine learning algorithms are trained on the training sets, which include samples labeled by the machine learning algorithms themselves as well as the samples manually labeled. In order to assign more weight on the manually labeled samples than on the machine labeled samples, we duplicate the manually labeled samples in the training sets.  Although the performance is not as good as that of the supervised learning, all three algorithms produce results comparable to that of the expert system. Especially the Maximum Entropy and SVM do very well in terms of recall, beating the expert system. The performance of the Naive Bayes improves as more correctly labeled training data are included in the training sets. Since different algorithms tend to err on different samples, we also experiment with an ensemble machine learning method, in which multiple classifiers are combined to arrive at final classifications. We experiment with two variations of the ensemble method: The Ensemble AND, in which a sample is classified as positive if both of the classifiers agree that a sample is positive, and the Ensemble OR, in which a sample is classified as positive if at least one of the two classifiers agree that a sample is positive. Among our ensemble classifiers, the Ensemble AND of the SVM and the Maximum Entropy with 20 iterations performs best with the F-score of 0.941, as shown in Table 3.  Table 3. The performance of the ensemble classifiers.    Accuracy Precision Recall F-score   SVM AND Max Ent (20 iter.) 0.947 0.928 0.955 0.941   Figure 2 shows a result of our image segmentation process. All the text in the original image on the left is segmented and assigned the same color (red). All the other connected black pixels are grouped as non-text components. Note that black pixels that are very close in the original image are connected by a process called RLSA smoothing (Wong, 1982).  Figure 2. An example of segmentation of text. Figure 3 shows some of the images produced as the result of the speech bubble detection. The first image from the left shows that all speech bubbles are correctly detected. The second image from the left is an example of the false negatives. The speech bubble in this image is missed because the edge of the bubble is broken and the gaps between the edges are more than 3 pixels wide. The third image from the left shows a case of the false positives. The lens of glasses are identified as speech bubbles because the eye contained in the lens has similar black pixel count and size to text. The fourth image from the left contains another example of the false positives. It is a false positive because the text is not contained in a speech bubble. It is identified as a speech bubble because the text is completely surrounded by edges. The machine learning system makes more mistakes than the expert system. However, the machine learning system sometimes correctly detects speech bubbles on which the expert system makes mistakes.  Figure 3. Examples of the images produced as the result of speech bubble detection.  Conclusion  We are able to implement an expert system that detects the elements very accurately and a machine-learning system that does the same job as well as the expert system. The main disadvantage of the expert system is that the heuristics need to be modified when new samples are added. With machine learning, we let the machine learn from the examples by itself. We have demonstrated that machine learning can be successfully applied to a very difficult task, such as element detection in comic book panels, and they can do the task as well as the expert system.  Future Work An interesting future goal is to be able to detect even more elements in each panel. If the machine could detect all the elements present in the panels, the scenes depicted in the panels could be described automatically.  Note 1. Readers interested in more details of our work can read our full-length paper at https://www.researchgate.net/publication/270546570_ELEMENT_DETECTION_IN_JAPANESE_COMIC_BOOK_PANELS and also access our code at https://github.com/tkuboi/ComicbookElementDetection. ",
        "article_title": "Element Detection in Japanese Comic Book Panels",
        "authors": [
            {
                "given": "Toshihiro",
                "family": "Kuboi",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Foaad",
                "family": "Khosmood",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "information retrieval",
            "English",
            "translation studies",
            "data mining / text mining",
            "image processing",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Providing rich narrative assets for games featuring interactive storytelling is both difficult and expensive. Of particular concern to us is the problem of believability for non-player-characters (NPC) in video games and interactive worlds. In addition to art and voice assets, which can be substantial, a good NPC requires narrative assets such as universe-appropriate character background, life history, personality, speech mannerisms, and behavioral peculiarities.  In this paper, we discuss our tool for generating NPCs for use in Interactive Fiction (IF) projects. The tool is intended to assist IF authors with NPC creation. Discussed are the two parts of our character creation process: the statistical model-based generator and the translator for interfacing with a game engine. A user study of 40 subjects is conducted to assess utility of this tool for IF authors, based on the system generating characters from the  Game of Thrones universe. Most subjects indicate that the system would be useful for IF authors. The results are positive and demonstrate that such a technique could work for creating believable characters in an interactive fiction context.   Creating Narrative Assets  Traditionally, interactive entertainment character narratives, including all possible dialogue, is developed by the project writing staff. There are no quality tools for automated mass character creation and realistic procedural vocalization. Therefore, the process yields limited content and often limited ranges of expression. This limitation in character expression is a limitation on the potential of video games as a storytelling medium. The most popular solution in games where characters are critical is to only flesh out certain key characters with whom the player is supposed to interact. Generally these NPCs have names and can be engaged in conversation. Most traditional role-playing games take this route. They tend to create a small, set number of ‘named’ characters that have responses to player action and a large number of generic characters (e.g., ‘commoners’) who all have identical or formulaic responses to the situation. These other characters in the world are left as nearly blank husks, having at most a few lines of general response and not interacting with the player in any meaningful way.  To make NPCs interesting requires a large budget and is out of reach of all but the most well-funded studios. The most basic character for an independent low-budget 2D fighting game can cost upwards of $150,000 USD (Plunkett, 2012). The salaries of relevant staff are constantly growing: $102,000 USD average for a video game writer, with voiceover artists averaging $128,000 USD in the United States. 1 Another solution is to limit the amount of exposure that a player has to other characters. This can be done at the expense of player experience. Large crowds (such as busy city scenes or other large gatherings) become off limits or are moved into the scenery. As a result the game world can come to feel very empty and small, even if the scenery is incredibly expansive. Lankoski and Bjork (2007) address this problem in their examination of character believability in the context of a character from  The Elder Scrolls IV: Oblivion.  Presented here is a procedural method to create unique characters with diverse personalities that can then be imported into a game and interact with the player—in this case, an Inform 7 work of interactive fiction. Inform 7 is an English-like declarative programming language and development environment for interactive fiction. 2 It was chosen because it could provide a proof of concept with many characters without requiring visual representations of characters. What results is a pool of characters with diverse personalities and detailed experiences that should be depended upon to act as more realistic extras, complete with personal histories, ambitions, and relationships.   Related Work  While there are a wide variety of rudimentary character generators for pen-and-paper-type games available online, these focus on specific rule sets, 3 with application to role-playing games. Bayesian networks have been explored in natural language generation research, but almost none specifically for large-scale personality or character generation. Corradini’s group sets out to focus on creating a single realistic depiction of a very specific character, in this case, Hans Christian Anderson (Corradini et al., 2005). Lebowitz goes a long way toward establishing a story generation system for creating believable worlds and descriptions by emphasizing the need for consistency and coherency (Lebowitz, 1984). Kienzle et al. (2007) demonstrate the use of a generational model to create variety in NPC character action, with the emphasis on in-game abilities rather than personality or social interaction. Merrick also focuses on behavior, discussing a technique for making persistent non-player characters (such as those in massively multiplayer games) more realistic by having them change and react to changes in the environment over time (Merrick and Maher, 2006). Lankoski and Bjork (2007) examine what makes a character believable in the context of  The Elder Scrolls video game series and role-playing games in particular.    System Overview  Our system is broken into two largely independent portions. The first part generates the features of the characters. In order to establish realistic characters, it was necessary to implement a Bayesian Belief Network of different character features and events and use these to build a person structure from which language describing the character could be generated. The second step in the character representation process is to translate the features and stories that the character has experienced into a format that a player can interact with.  Character Model and Generator The first nondeterministic trait to be generated per character is the location where the character is from. In the universe of  A Song of Ice and Fire this is very important to cultural and political identity. The location distribution is limited to broader geographical regions. This is primarily because, being a fictional universe, there is no reliable way to estimate the populations of smaller regions without more evidence (Various, 2012). The names generated come from a list of the characters (in this case, male) in the novels. The list itself includes about 1,400 individual characters. 4 These names are then chosen at random (some combination of the first and the second) and assigned to the character.      Figure 1. How the ‘profession’ event interacts with the rest of the model. The next set of traits to be created for a character are those that are present from birth. These are physical traits (blue-eyed, tall, strong, short, etc.) and a limited number of psychological traits (such as level of intelligence and initial temperament) as well as social class. Stochastic Life Events The next step in creating a realistic character is to give it a series of life events to build a story arc for the individual. For the purpose of this generator, the character’s life has been broken into nine different life event categories ranging from early childhood trauma to marriage to late life-changing events. Not all of these event categories will happen to everyone (some have less than a 50% of occurring) while others (like profession) are guaranteed for everyone. Each event category has a range of possible outcomes. Initially, the probability of each of the events in the category is the same, but each event has a number of potential influencing traits and a corresponding multiplier that changes the distribution (after normalizing) of the event list. The influences can cascade across multiple events. Translator and Game Interface The translator takes the Bayesian model for the character and transforms it into usable Inform 7 code. The conversations that the users have with the characters generated through the generator and translator are facilitated by one of the many extensions available to the Inform 7 IDE, Conversation Framework 5 by Eric Eve. This framework allows the user to ask the character about specific topics from within the interactive experience.     Figure 2. User distribution of familiarity with the Game of Thrones universe (5= ‘most familiar’). User Study All 40 participants in the study are computer science undergraduate students at California Polytechnic State University (Cal Tech). About two-thirds of the participants are from the Interactive Entertainment Engineering class, and have experience with Inform 7 and interactive fiction authoring due to having completed class projects. The overall familiarity with Inform 7 is around  80 to 85%. Tutoring and supervision are offered for all participants.     Figure 3. Believable background and conversation scores (5=most believable). X axis represents portion of users. 40 subjects were surveyed. The participants are then asked a series of questions about their familiarity with the  Game of Thrones (or  Song of Ice and Fire) universe, the believability of their character in the context of the universe, whether they would use this character in one of their own works, and how helpful they thought this generation method could be making realistic secondary characters for interactive fiction.  People find the generated characters to have a believable backstory. The average score was a 3.7 out of 5 in that area. The best result from the generator was that 54% of the people polled would be willing to use the character unedited in a work of interactive fiction. The rest stated that they would use the character with some editing. None of the people polled responded that they would not use the generated character in any form. When asked about their willingness to use the unaltered generated character as a non-major character, the generator received an average score of 4.1 out of 5. The average score for using an unedited version of the generated character as a major player in a work of interactive fiction was a 2.3. This was foreseen as the generator was never meant to supplant major characters driving a main plotline but as more complex NPCs meant to provide context and exposure. When looking at individuals who had a higher than average (4 or 5) familiarity with the  Game of Thrones universe, the number of people willing to use the character unedited went down a small amount to 50%, but those who did say they would use the character gave the primary character the same average score (2.3) while giving the non-major character a significantly higher score at an average of around 4.5. Finally, when asked the question ‘Do you think this kind of character generation can be helpful to interactive fiction authors?’ the average score was a 4.23.     Figure 4. Subject responses to ‘Are you willing to use this character in your own story?’ (5= ‘most willing’). 40 subjects surveyed.  Notes  1. http://www.indeed.com/salary/q-Video-Game-Writer-l-United-States.html. 2. http://en.wikipedia.org/wiki/Inform. 3. See, for example, http://www.pathguy.com/cg35.htm. 4. http://www.namenerds.com/uucn/listofweek/icefire.html. 5. Ibid. Sample Output   Sample translated code to create character ‘Josmyn’ in Inform 7 Sample game playthrough with same character    The quizzing table is the Table of ComplexNPC0 Answers. Table of ComplexNPC0 Answers  subject response rule response table suggest  ... adolescence -- Table of ComplexNPC0 adolescence 3 Table of ComplexNPC0 adolescence response ‘A couple of friends and I went on a fun adventure’. ‘I became more diligent in my adolescent years’. ‘I traveled between Oldtown and Volantis as a cabin boy’.   >greet Josmyn Greetings (You could ask him about himself, home, life, class, childhood or adolescence.) >ask about adolescence A couple of friends and I went on a fun adventure. >ask about adolescence I became more diligent in my adolescent years. >ask about adolescence I traveled between Oldtown and Volantis as a cabin boy.    ",
        "article_title": "Game of Thrones for All: Model-based Generation of Universe-appropriate Fictional Characters",
        "authors": [
            {
                "given": "Matthew",
                "family": "Parker",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Foaad",
                "family": "Khosmood",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            },
            {
                "given": "Grant",
                "family": "Pickett",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University, United States of America",
                        "normalized_name": "California Polytechnic State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/001gpfp45",
                            "GRID": "grid.253547.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "natural language processing",
            "agent modeling and simulation",
            "English",
            "games and meaningful play"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " AustLit contains thousands of full text items ranging from seminal works of 19th- and early-20th-century Australian literature through collections of early science and speculative fiction, to a large corpus of children’s literature, alongside selected criticism and scholarship. In addition, AustLit bibliographical records link outwards to tens of thousands of full text items available online.  This paper presents the results of a project undertaken by the AustLit team in 2014 and 2015 to totally refactor the existing AustLit full text corpus, including a massive expansion of the corpus by identifying and harvesting literary texts published in newspapers in the period covered by the National Library of Australia’s (NLA) database of digitised newspaper available through Trove. 1  A number of different formats and digitisation protocols have been used over the past 14 years to build a corpus of works that has the potential to support a range of different use cases. That potential had not been met until the total restructure of the AustLit database and content management system over the past two years provided an opportunity to look again at the material we have and the way we deliver that material to researchers and readers. A major factor in AustLit’s future plans to deliver full text is the NLA’s newspapers database. It offers a valuable opportunity to build our corpus and advance knowledge about the place of literature in culture and reading practices across the 19th and early 20th centuries. Newspapers were the primary form of transmission for literature during the period covered by the NLA’s database; the possibility of identifying and unlocking the literary content in the database thus allows us to support new research into reading culture. This paper will present the refactored full text system AustLit developers have created to expand utility, readability, and research opportunities. It will also discuss an innovative method of identifying and harvesting poetry from the NLA’s newspapers database.  In July 2014, AustLit contained just over 10,500 links to poems identified within the NLA’s Digitised Newspapers collection. Each of these links had been manually created by a combination of inspired searching for words from a known poem, searching for known literary columns, and systematic browsing through each page of each issue of nominated newspaper titles across specific date ranges.  One of AustLit’s many new research projects is the Colonial Newspapers and Magazines Project, undertaken by researchers at UNSW, Canberra. This project is creating a literary ‘map‘ of Australia’s colonial period by collecting and recording information about the reading habits of Australians before 1900 and linking these findings into AustLit’s data structures. This huge task has been begun by concentrating on three specific years in the 1800s, and whilst producing accurate and near-complete results, the only feasible method of browsing every page of selected titles is extremely labour intensive.  Hence, we started exploring an automated approach to identifying at least some relevant content. As a starting point, we noted the effectiveness of Ted Underwood’s genre identification approaches on 17th- and 18th-century texts digitised by HathiTrust. 2 We used his vocabulary information to produce two vocabulary frequency lists: one of all words and one of words found in works of poetry.   We first trained using a naive Bayesian classifier with the ‘all’ and ‘poetry’ vocabulary frequencies, and ran the trained classifier on a training set of newspaper articles identified as poetry and on another set whose genre was unknown.  The known poetry article list was derived from the 10,500 articles linked to as poetry in AustLit. The unknown set was generated by randomly selecting articles from NLA’s digitised newspapers. We found that whilst providing a useful signal, vocabulary alone was not sufficient to reliably classify articles as poetry or not-poetry. Examination of classification failures led us to explore additional signals to add to our classification heuristics:   • Text justification.  • Variations in length of successive lines.   • Apparently rhyming lines.   • Presence of digits in OCRed text.   • Presence of a small number of ‘marker’ words.  Our initial results correctly classified just over 80% of articles associated with AustLit poetry links as poetry. Manual examination of the articles not identified (false negatives) revealed that the vast majority were articles containing a small amount of poetry set within a sea of prose. A significant other group where written using a vocabulary not typical of poetry (words such as ‘proclamation’, ‘neutrality’, and ‘precautions’, which pushed the classifier towards rejection as poetry), and there was another large group of articles with such poor OCR that few words were accurately identified. An error in AustLit linking to the incorrect article was also identified.  We then measured the effect on the classifier of automatically correcting the OCR of articles and found it gave only slight improvements to false positives and negatives, because the predominant reasons for rejection of a known poetry article as non-poetry were not related to correctable OCR.  We then implemented the following set of refinements to the classifier, which lifted our successful classification rate to over 85% whilst keeping ‘false positives’ below 1%:  • Improved rhyming detection heuristics.  • Used article metadata to exclude advertisements.   • Internal article segmentation in an attempt to identify ‘islands’ of poetry contained in predominantly prose articles.   • Use of cues from social tagging and commenting.  We aim to harvest what appears to be vast numbers of poems published in Australian newspapers during the 19th and early 20th centuries and to deliver that full text to AustLit users within an enhanced discovery and reading environment. This project also allows AustLit to expand a project that has neither the funding nor the research staff to build on the initial 10,500 manually created records and links into the newspapers database. While the creation of nuanced, human-derived records is no longer possible for the colonial newspapers project, the hope is that this method will provide it with a data boost by building the AustLit full text corpus and record store with greatly reduced human input, thereby enabling the analytical research into the period’s reading and publishing culture.  Notes 1. http://trove.nla.gov.au. 2. See http://tedunderwood.com/2012/07/27/getting-everything-you-want-from-hathitrust/. ",
        "article_title": "Discovering and Rediscovering Full Text: Unearthing and Refactoring",
        "authors": [
            {
                "given": "Kerry",
                "family": "Kilner",
                "affiliation": [
                    {
                        "original_name": "The University of Queensland, Australia",
                        "normalized_name": "University of Queensland",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00rqy9422",
                            "GRID": "grid.1003.2"
                        }
                    }
                ]
            },
            {
                "given": "Kent",
                "family": "Fitch",
                "affiliation": [
                    {
                        "original_name": "The University of Queensland, Australia",
                        "normalized_name": "University of Queensland",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00rqy9422",
                            "GRID": "grid.1003.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "natural language processing",
            "literary studies",
            "lexicography",
            "archives",
            "bibliographic methods / textual studies",
            "information retrieval",
            "digital humanities - facilities",
            "data mining / text mining",
            "content analysis",
            "corpora and corpus activities",
            "text analysis",
            "english studies",
            "English",
            "cultural infrastructure",
            "programming",
            "sustainability and preservation",
            "machine translation",
            "image processing",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Senga is a Web-based participatory curation system designed for the general public. The Senga interface consists of four steps—search, collect, order, and share—to make an image sequence called a tour. We claim that these steps mimic the basic process of curation, so a tour can be considered as an exhibition. Senga is a type of crowdsourcing system similar to crowd annotation, but the uniqueness of Senga is in context-dependence, meaning that an interpretation of an image depends on the neighborhood of an image sequence. Through the analysis of more than 2,500 tours created on Senga since its release in 2007, we showed that it has potential to be used by curators to draw new ideas for the exhibition. We conclude the paper by proposing the mathematical model of exhibitions with a preliminary result on analyzing state transition diagrams and on synthesizing tours.   What Is Senga?   ‘Senga’ (Japanese for ‘thousand images’) 1 is a web-based participatory curation system that allows users to make a ‘tour’ of images and publish it on the Web as an exhibition. Senga is based on the idea of fragmentation and recombination of digitized books. We have digitized more than 200 academically relevant books about the Silk Road (Digital Archive of Toyo Bunko Rare Book 2), but the academic atmosphere around those books was a barrier to the general public accessing the wealth of cultural heritage information inside the book. Our original motivation was to crop attractive parts of the book to make a collection of image fragments so that users could later collect and recombine them into a new collection for their purposes. Providing image fragments, however, is a common idea exemplified in the British Library’s million photographs at Flickr commons or the Mechanical Curator (Baker, 2013). Our contribution is in designing an interface to make a ‘tour’ of image collection.   Figure 1 shows four steps to make a tour on Senga—namely, search, collect, order, and share. A user first sets a theme, searches images by various criteria, collects appropriate images, orders them in a meaningful way, and shares them as a tour. We claim that these steps mimic an exhibition’s curation process, and a tour is a basic form of an exhibition. Hence we call Senga a participatory curation system for sharing exhibitions online.     Figure 1. Overview of Senga and four steps (search, collect, order, and share) to make a tour.  Context-Dependent Annotation  Senga can be regarded as a crowdsourcing system similar to crowd annotation. In crowd annotation, a user is asked annotate images with tags (Oomen et al., 2014) and named entities, 3 and we call it context-independent annotation because metadata description is not affected by how we view the image. On the contrary, Senga deals with context-dependent annotation such as ambiguous or creative interpretations. For instance, a red rectangle is red if it is contrasted with other colors, but is a rectangle if contrasted with other shapes. An ambiguous situation also offers a chance for the exploitation of creative interpretations and contextualization. Senga asks a user to think about the meaning of an image in contrast with other images, which raises users’ interest to look into images.   Motivation to Participate  Senga does not offer a clear goal, however, and users of Senga are expected to find their own goals. After the release of Senga in August 2007 (Kamida and Kitamoto, 2007), we held several outreach events for the general public to use Senga and observed how they understand the concept of the system. The biggest challenge seems to be setting a goal from a pool of images on the interface, or in other words, setting the theme of a tour. Some people quickly decided on a theme and looked intrinsically motivated for improving the quality of the tour. An interesting observation is that the capability of goal-setting does not depend on age; even small kids can quickly find their themes, while some adults got lost and could not concentrate on making the tour. Since October 2011, Senga was permanently installed at Toyo Bunko Museum in Tokyo so that museum visitors can freely use it. To increase the value in the museum, we added the mechanism of extrinsic motivation, namely a free souvenir. A tour made in the exhibition room can be picked up at the museum shop after communicating with the staff to print their tours on postcards. Here a postcard serves as a tool to make a path to the museum shop and keep memories of the museum until the next visit.   Analysis of User-Generated Tours   Basic Statistics  As of 14 October 2014, Senga produced 2,579 tours. The system provides 3,772 image fragments, and 3,335 images (88%) appear at least in one tour. To analyze the collection of tours from the viewpoint of an exhibition, we first define an ‘exhibition’ as ‘an activity for the general public to arrange artifacts or events with an intention’ (Kawaguchi, 2009 [author’s translation]). This definition suggests that the most important factor of exhibition is ‘intention’, so we focus on two factors—arrangement and title—that are related to the intention of a tour. We check the arrangement and the title for intention and judge if both are consistent. Finally we mark a tour ‘creative’ as long as the arrangement and the title suggest interesting or unique ideas. Evaluation of 2,579 tours was performed by one woman who is not a domain specialist. Such subjective evaluation should be performed by multiple people, but evaluation by a single person is at least consistent and can be used as a good starting point for a preliminary evaluation. As a result, 34% of the tours were judged intentional and consistent, and 6% of the tours (most of them intentional and consistent) were judged creative. In contrast, 41% of the tours were judged intentional but not consistent.  Top-Down and Bottom-Up Approaches  The first research question is why we have more inconsistent tours than consistent ones. Our hypothesis is that this difference originates in two different approaches to making tours: top-down and bottom-up approaches. In a top-down approach, the theme of a tour is defined in the beginning, so the title could be easily given. On the other hand, in a bottom-up approach, a user starts from collecting images and later tries to give a title that best describes the collection. This is not an easy task for image collections without explicit themes, so a typical solution is to give a title not related to the content, such as the date of the visit. The result suggests a hypothesis that the bottom-up approach is slightly more popular than the top-down approach, which is probably opposite the habit of professional curators.  Variety of Contexts  The second research question is to measure the variety of contexts at an image level. Figure 2 shows the analysis of image sequences in tours to count images that come just before or just after the target image. Here the leftmost image is the target image, and other images to the right are ordered according to the number of appearances in the neighborhood. In (a), the target image was used in the context of a colorful pattern, but in (b), the target image was used in the context of either animal, pairing, or circular. Multiple interpretations emerged through the comparison of multiple images, but this level of variety is limited to a local context.   Figure 2. Variety of contexts for a single image. Creativity of Users  The third research question is to characterize users’ creativity at a tour level. Figure 3 shows an example of creative tours. The first tour has the title ‘couple’, suggesting that the intention is to arrange face images in alternating directions. This interpretation, left or right, is the most important meaning in this tour, but it may have no value in single-image annotation. The second tour has the title ‘fashion show’, suggesting that the intention is to arrange well-dressed people. These creative user-generated tours offer new interpretations for a global context and show potential for the professional curator to draw new ideas for their exhibitions.     Figure 3. Creative tours: (a) couple, (b) fashion show.  Mathematical Model of Exhibition  The final challenge is to represent the collection of tours using a mathematical model to enable deeper analysis. If we assume that an exhibition has meaningful context over a local image sequence in the neighborhood, the exhibition can be mathematically represented as a Markov chain. Here an image is a node, and transition from one image to another is a directed edge between nodes. The union of nodes and edges from all tours constructs a graph structure of all tours. This graph represents collective contexts that Senga has produced, and the analysis of this graph structure leads to the collective analysis and synthesis of tours. Figure 3 shows the state transition diagram of a Markov chain for (a) intentional and consistent tours and (b) others, visualized by Gephi 0.8.2beta (Bastian et al., 2009). Specific shapes depend on the layout algorithm, but it looks as if (a) has more structure than (b), and this impression could be validated by quantitative analysis of the graph.        Figure 4. State transition diagrams from (a) creative and consistent tours, (b) other tours.  The graph can also be used for synthesis. An example is a random walk on the graph that generates a synthetic tour. Frequency of state transitions is first converted into probability, and a self-avoiding random walk 4 generates an image sequence as a tour. Figure 5 shows an automatically generated tour. It starts from a pattern image and gradually shifts toward Buddha and finally people, and it gives locally consistent state transitions.      Figure 5. Automatically generated tour using self-avoiding random walk.  Conclusion  The paper introduced a participatory curation system, Senga, and proposed a mathematical model of exhibition for analysis and synthesis. Future research challenges include detailed analysis of the mathematical model to characterize creativity.  Notes 1. Senga Silk Road, http://dsr.nii.ac.jp/senga/. 2. Digital Archive of Toyo Bunko Rare Books, http://dsr.nii.ac.jp/toyobunko/. 3. Your Paintings Tagger, http://tagger.thepcf.org.uk/. 4. Eric W. Weisstein, ‘Self-Avoiding Walk,’ from MathWorld—A Wolfram Web Resource, http://mathworld.wolfram.com/Self-AvoidingWalk.html. ",
        "article_title": "Senga: Participatory Curation and the Mathematical Model of Exhibition",
        "authors": [
            {
                "given": "Asanobu",
                "family": "Kitamoto",
                "affiliation": [
                    {
                        "original_name": "National Institute of Informatics, Japan",
                        "normalized_name": "National Institute of Informatics",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/04ksd4g47",
                            "GRID": "grid.250343.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "libraries",
            "interface and user experience design",
            "archives",
            "museums",
            "English",
            "GLAM: galleries",
            "internet / world wide web",
            "crowdsourcing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In recent popular and scientific products of historiography, ever more similar-looking photographs of (mostly symbolic) happenings are used as an instrument to tell a story about important events or structural trends. These types of images can be called iconic photographs (Hariman and Lucaites, 2007). We consider iconic photos as photographs that have been reproduced more than once and have a special composition. They refer to archetypes and have the potential to be an archetype itself, and thus represent more than what is being displayed. Although this symbolic meaning is immediately obvious, it may change over time when the photograph is published in different contexts during its afterlife (Kroes, 2007). Since a group of people know these photographs and attribute the same meaning to it, they are part of a ‘collective memory’ (Kleppe, 2013a). Well-known examples of iconic photographs are the photo of a girl running naked and screaming after a napalm bombardment in Vietnam in 1972 or the man in front of a line of tanks at Tiananmen Square, Beijing, in 1989. These examples can be considered to be ‘global super icons’. However, iconic photographs also exist within a national context (Paul, 2008). This paper describes how we determined which Dutch photographs can be called iconic by using IPTC technology. This technique is being used in the media industry to transfer information on photographs in unified standards. We applied this technology to efficiently research our two questions that form two elements of the above-formulated definition of an iconic photograph: (1) Which photo is published most often in a dataset and therefore functions as an iconic image, and (2) How can the changing symbolic meaning during the afterlife of iconic photos be studied?  Method  The International Press Telecommunication Council (IPTC) develops technical standards for news organisations. 1 By applying the same standards, a photographer can embed the information of a photo inside the file and send this to an editor at a newspaper who can download the metadata in the local ICT-infrastructure. This technique not only facilitates the exchange of files between journalists, but academics, digital libraries, and cultural heritage institutions can also use IPTC to include information about their objects in the digital files (Grijsen, 2012; Reser and Bauman, 2012).   We used this technology to determine which Dutch photos are published most often by analysing 5,000 photographs in 400 Dutch history textbooks, published in the period from 1970 to 2000. All photos were digitized and analyzed by assigning 41 variables (such as topic, caption, person, and year). However, finding similar images in a large dataset can be a challenge given the high level of subjectivity when interpreting photographs (Finnegan, 2006; Rose, 2007) and the lack of standardized thesauri to describe photographs (Kleppe, 2012). To overcome this ‘semantic gap’ (Smeulders et al., 2000) we formulated a list of historical events based on a literature review, creating metadata that were tailored to our research question (Wallace, 2010). Together with all factual information about the photo, this data was included in the IPTC fields that are embedded within the digital file of the photograph by using the commercial software program Fotostation Pro. 2 This program not only allowed us to do full text searches through all assigned metadata but we were also able to share our research data with other researchers who could import the information in the IPTC fields of their photo-, editing- and viewing software. Moreover, we were able to export all values to CSV-files that were importable into statistical software packages such as SPSS.   Results  By making frequency tables of the list of topics we described about all photos, we calculated which topics were most present in the set of photographs. We then manually went over the images that illustrate these topics to find the images that were used most often. This method allowed us to answer our first research question on which photo is published most often and therefore functions as an iconic image. Results show that a 1912 photograph of Dutch socialist politician Pieter Jelles Troelstra is used most often in the analyzed textbooks. On the photo, Troelstra gives a speech in which he pleads for universal suffrage (see photo 1).     Photo 1. The photo of Pieter Jelles Troelstra that was published most often in the analysed Dutch History textbooks. Source: Cornelis Leenheer, IISG, https://www.flickr.com/photos/iisg/4071852722/in/set-72157622724066432.  To study our second research question on the changing symbolic meaning of iconic photographs during its afterlife, we could return to our database since each photo is not only described based on the list of historical events, but we also noted factual information such as the chapter title in which the photo was published and the accompanying caption. By going over this information we could examine in which context the photo of Troelstra was used. By simply typing in the name ‘Troelstra’ in Fotostation Pro, we found all books in which the photo was used and could go over the information in the accompanying IPTC field. By following this approach we could answer our second research question on the changing symbolic meaning during the afterlife of iconic photographs.  In the case of the photo of Troelstra, we found that the photo is incorrectly dated in one-third of all history textbooks. Nowadays, in Dutch historiography Troelstra is not known for his plea for universal suffrage but mainly for his failed attempt to start a revolution to overthrow the queen in 1918. Our research shows that in one-third of all Dutch history textbooks the photo of 1912 is used to illustrate the events of 1918 instead of the demonstration of 1912, clearly illustrating the changing symbolic meaning of this iconic photograph during its afterlife as visual illustration in history textbooks.  Discussion  Even though our database is relatively small, the case study of the photo of Troelstra shows that by adding metadata in the IPTC fields, we were able to quickly track down all the textbooks in which the photo is used to determine the context in which the photo appears. Studying this afterlife can even be taken a step further when databases with the same approach can be linked, e.g., collections that are described with the ICONCLASS System (Brandhorst, 2012) or the GTAA (Oomen and Brugman, 2010). It will then be possible to research how photos are being reused in other types of historical sources, such as newspapers or magazines. Therefore we made our database available for future researchers (Kleppe, 2013b) in order to be reused to answer humanities research questions or further exploration on the use of IPTC within the digital humanities.  However, going over the frequency tables and subsequently looking up the information in the database and IPTC fields remained a manual process. Even though the software allowed us to quickly search through all data, we still had to rely on our own judgment and scrutiny, leading to inevitable human errors or false interpretations. Therefore, we envision that the current developments in image retrieval by using image recognition (Wu et al., 2009) will be of assistance for this type of research. At this moment, several search engines offer reversed image lookup (RIL), such as Google Images and Tineye. 3 Users can upload an image and retrieve similar images located at indexed websites. This type of technology is already being used by commercial parties to track down copyright-protected images across the Web, 4 to assess the impact of scholarly images online (Kousha, 2010), to find patterns in large image databases (Losh, 2014; Manovich, 2009), or to analyse the reuse of digital images of cultural and heritage material (Terras and Kirton, 2013). Applying this type of reversed image lookup to find similar images within a dataset has only recently been explored by Resig (2013) and Doug (2014), who developed software based on TinEye’s MatchEngine. 5 Given its promising results, scholars using visual sources to study the afterlife of imagery should explore the possibilities of computer vision to find the recurrent use of similar images in visual datasets.   Notes 1. http://www.iptc.org. 2. http://www.fotoware.com/en/Products/FotoStation/.  3. https://images.google.com; https://www.tineye.com/. 4. http://blog.photoshelter.com/2013/04/find-your-images-online-using-reverse-image-search-on-google/. 5. https://services.tineye.com/MatchEngine. ",
        "article_title": "Tracing the afterlife of iconic photographs using IPTC",
        "authors": [
            {
                "given": "Martijn",
                "family": "Kleppe",
                "affiliation": [
                    {
                        "original_name": "Erasmus University Rotterdam, Netherlands, The",
                        "normalized_name": "Erasmus University Rotterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/057w15z03",
                            "GRID": "grid.6906.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "historical studies",
            "video",
            "audio",
            "English",
            "multimedia"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Globalization is a current theme in literary studies. Are authors writing increasingly for a global audience, with novels that appeal to readers in many countries, through shared ‘global’ cultural knowledge—e.g., references to concepts with which people around the globe are familiar? The Beyond the Book project aims to investigate whether we can measure how ‘international’ a novel is in its references to cultural knowledge. This could be useful in predicting the international appeal of novels and help publishers choose titles for translation that may appeal to readers in different markets.  Although many potential factors contribute to a publisher’s or editor’s decision to translate a novel, we wish to know if textual aspects are a factor, and if so, to what extent. As a first step, we focus on the named entities in a set of Dutch novels, including novels that have been translated from Dutch to English.  We link the named entities to English Wikipedia articles and use the Wikipedia edit history to measure the relative contribution of Dutch Wikipedians to each entity as a proxy for how specific the cultural references are to Dutch readers.  The main research questions addressed are  • How can we use named entities in novels as representations of cultural references?   • How can we measure the relation between cultural references and international appeal?   The Translation Market    The World Book Market   From UNESCO’s Index Translationum database, which charts worldwide book translations, Heilbron (2010) finds a four-level structure: around 55 to 60% of all translations are from English, around 10% from German and French, 13% for each of seven or eight other languages, and the rest are translations from ‘peripheral’ languages (including Dutch). For peripheral languages, translations from other languages are common, whereas in English-speaking countries, translations make up only a small fraction (around 3%) of the book market.     Figure 1.   The Dutch Book Market   Import  Across all genres, in the Netherlands only about 34% of translated books come from languages other than English (UNESCO Index, up to 2006). This percentage fluctuates over time: the report ‘Publishing Translations in Europe: Trends 1990–2005’ by the Mercator Institute, which provides data for the Dutch market for translated fiction, states that 72.6% of all translations are from English. 1  Export  The Dutch export market is very different from the import market. For spreading Dutch novels, Germany is the ‘gate market’ that opens up the possibility of translation into other peripheral markets in Europe (Heilbron, 2010). From the roughly 200 translations receiving funding from the Letterenfonds per year, 60 are adult fiction, and German takes up about 30% of those; English, only 17%.     Figure 2. The Decision Makers  Franssen and Kuipers (2011) asked 23 Dutch editors how they decide which foreign titles to purchase for the domestic market. When editors assessed the text, they were purely focussed on ‘universal’ manuscript quality, not international appeal. We found in our interviews with 12 Dutch decision makers that, when looking to sell international translation rights for Dutch novels, a translation into English is deemed the most desirable of all. Hence, we focus on translations to English.  Methodology   Technical Pipeline   This section describes the necessary steps to go from the full text of a novel to a meaningful measure of relative interest from a country in that novel.  Identify Words of Interest  The full text of a novel is split into arbitrarily sized units (e.g., a paragraph), which are processed using Semanticizer (Meij et al., 2012), to produce a list of Named Entities (NEs). These NEs represent the terms ( w i) that have a high probability of being used as links to other Wikipedia pages, if the given text was an article in Wikipedia. Each of these link-terms has a probability of being a link associated with it  P link( w i); link-terms are filtered by their probability, and terms with probability below a certain threshold are discarded. The frequency of an NE in a given text  F( w i) represents the relative importance of that NE.   Country Interest  Next, the relative interest from a country in each link-term is determined. This is estimated on the basis of the number of edits from a given country to the Wikipedia page corresponding to the NE. The English version of Wikipedia is used for this study, since it contains edits from many countries, including the Netherlands. Each Wikipedia article is built from contributions of several users over time. The interest in a particular topic is gauged by the number of contributors from a specific country. It is possible to determine the country of origin of the contributor, either by IP address (recorded for anonymous contributors) or by analysing the geographical categories the contributor belongs to (e.g., Wikipedians in the Netherlands).  This proposed methodology has limitations. Contributions from ‘bots’ (i.e., contributions made automatically by programs) need to be ignored. Registered users can state their nationality on their user profile, but not all users provide such information. The fraction of unknown contributions is treated as an uncertainty  C( w i). For instance, if 70% of the contributions to a given page are from known sources, then the interest from a given country can be assessed with a 0.7 confidence.      Figure 3. Each country provides a different volume of contributions to Wikipedia. Thus, country contributions on a specific topic must be normalized by that country’s contributions to the whole English Wikipedia. The interest of a given country on a particular NE  I Q  (w i ) is then the normalized percentage of contributions from that country to the Wikipedia article for the NE in question.  To illustrate these steps, the following graphs show the percentages of contributions from various countries to the English Wikipedia, 2 the expected and observed contribution percentages to the English Wikipedia article on ‘ice hockey’, and the relative country contributions to the ‘ice hockey’ article.     Figure 4.     Figure 5.    Figure 6. Integrate over Complete Novel  The relative importance  R of a given NE  w i for country  Q is calculated as follows:      where  J Q( w i) is the weighted interest of country  Q on the word:      And the interest from that country for a whole novel can be calculated as the average interest of the identified NEs in the novel:     Where  N is the total number of NEs in the novel.   Experiments  For the experiments, 492 Dutch (mostly literary) novels are used, all published between 1933 and 2008. Of these, 318 have been translated into other languages, but only 27 have been translated into English (based on data from WorldCat). The following features are determined:  • The  number of translations to English.   • The  number of entities in four categories: person, location, organization, and miscellaneous.   • The relative  Dutch contribution to Wikipedia articles on entities in each category.  The correlations between the number of English translations and the two features per entity category are shown in the figure below:    Figure 7. There is a weak correlation (0.38) between a Dutch novel getting translated into English and its number of miscellaneous entities (events, buildings, etc.). These can be interpreted as cultural references that readers from different countries may be familiar with. The number of locations has a positive but very weak correlation with getting translated, while the number of person, location, and organisation names has a very small negative correlation.  The number of translations to English is weakly correlated with the relative contribution from Dutch Wikipedians to the entities in the miscellaneous category. These results suggest that novels that get translated from Dutch to English contain many references to Dutch culture. The locations in translated novels have a relatively low Dutch contribution, which suggests that their settings are perhaps more international. I t is intriguing that the international appeal of these novels may lay just as much in their ‘Dutchness’ as in their ‘global’ references.   Conclusions  The aim of this paper is to investigate the role the text of a novel plays in determining whether it gets translated.  Cultural references seem to contribute to a novel’s chances of getting translated. The classic writers’ adagium, ‘Show, don’t tell’, may actually work; cultural references provide concrete, familiar details that anchor the story to the ‘real world’. Their realism could lead to a greater appeal for readers.  These are preliminary findings. The current set of books is very small, and the methods are crude. The next steps will be to refine our methods by incorporating more textual features, look in more detail at how they relate to translation decisions, and apply our methods to a larger set of novels, in different genres and languages.  Notes 1. There are gaps in the available data of 1993–1999, so the picture is not entirely accurate. 2. See http://stats.wikimedia.org/wikimedia/squids/SquidReportPageEditsPerLanguageBreakdown.htm. ",
        "article_title": "Predicting the International Appeal of novels",
        "authors": [
            {
                "given": "Carlos",
                "family": "Martinez-Ortiz",
                "affiliation": [
                    {
                        "original_name": "Netherlands eScience Center",
                        "normalized_name": "Netherlands eScience Center",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/00rbjv475",
                            "GRID": "grid.454309.f"
                        }
                    }
                ]
            },
            {
                "given": "Floor",
                "family": "Buschenhenke",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Karina",
                "family": "van Dalen-Oskam",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands; University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Marijn",
                "family": "Koolen",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, Netherlands, The",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "text analysis",
            "English",
            "translation studies",
            "linking and annotation",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In two previous papers at DH we have presented work in progress on computer simulation of language diffusion. In this paper, we offer the results from our completed research program, highly suggestive findings about how the process of linguistic change may operate.  Computer simulation is the only practical way to model linguistic diffusion. We have successfully simulated diffusion with a cellular automaton, which uses update rules with respect to the status of its neighboring locations to determine the status (whether a linguistic feature is used or not) at a given location. As shown in Figure 1, each target cell may become live (if dead) if a certain number of neighbors is live, whether from the eight neighbors immediately next to the target, or alternatively from the 24 neighbors in the first and second rows around the target. The same calculation takes place for a target cell to stay live if it is already live.      Figure 1. Cellular automaton (green target cell, evaluated by status of eight first-order neighbors, or 24 first- and second-order neighbors).   All locations in a matrix are evaluated, and then the new status for each one is displayed all at once (one generation). Throughout hundreds of generations we can watch regional distributional patterns emerge. In so doing we model human interactions, as speakers talk or write to each other and change their behavior based on that of their neighbors. We validate our results by comparison to actual linguistic data from survey research: we always observe clustered patterns in the survey, and we know that our simulation is successful if similar clusters emerge from the cellular automaton, as shown in Figure 2, the status of our simulation after 1,000 generations with a random factor of .01% (one decision overturned randomly in 10,000). This sort of clustered behavior is characteristic of complex systems (Kretzschmar, 2009), as they are studied in physics, evolutionary biology, economics, and other fields, where nonlinear (or ‘fractal’) distributions of variants regularly emerge at every level of scale in scale-free networks.      Figure 2. Simulation after 1,000 generations.   The cellular automaton is not the only form of simulation that could be applied to speech, but it is perhaps the simplest. Stanford and Kenny (2013), for example, employed an agent-based model to model chain-shifts, where ‘agents’ could move between locations to spread a linguistic feature, whereas in a cellular automaton the location of each cell is fixed. The Stanford and Kenny model, however, uses numerous unvalidated assumptions about how information is shared and about how agents move. Similarly, Baxter et al. (2009), Blythe and Croft (2009), and Ellis and Freeman-Larsen (2009a) all create more complex simulations that do achieve results but lack validation. Our cellular automaton shows that the complexity of agent movement or other similar parameters in such simulations may not be necessary as well as being unvalidated. After extensive testing of possible rule sets in our two-dimensional model, only one rule set produces stable, clustered results of the kind we always observe from real data (2,3,4 live neighbors to become live, 5,6,7,8 neighbors live to stay live). And this rule set, with suitable adjustment by social weighting and a small random factor, is sufficient to produce results that match the clustered patterns that arise in real survey data.  After substantial experience with the computer simulation, we have observed a number of characteristics that are highly suggestive for how the complex system of speech may operate in actual human populations of speakers:  1. While we have only ever found one rule set that produces clusters, the Bailey set (2,3,4/5,6,7,8, for N=1), other rule sets may be useful, such as proportional rules 90/10, 75/25, and 60/40 that all produce estimates of ‘Where people say X’. The Bailey set, however, eventually produces stable clusters of locations on the grid that match the kind of clustering we observe in Density Estimation statistical processing from the same data.  2. The relative ages of locations (how many consecutive generations a location has been live) always occur in a nonlinear distribution, with the most one generation old, then many two generations, then small numbers of older locations. This suggests that the persistence of features, not just use of features, is important in language diffusion. Persistence is what accounts for the creation of long-term stable clusters of locations.  3. Inclusion of a random factor overturning decisions from the rules up to .06% (six decisions in 10,000) slows down the process of cluster formation, but more than .06% randomness throws the simulation into a chaotic (everchanging) condition where no stable clusters form. This suggests that proximity, not random decisions by speakers, controls language diffusion. However, inclusion of a small random factor preserves nearly all of the long tail of infrequent responses in the nonlinear distribution after 1,000 generations, and so it is necessary to include random decisions by speakers in order to achieve the nonlinear distributions we know to exist in survey data.  4. Inclusion of a social factor also creates clustered behavior (N=2, 25% social weighting). Clusters appear in different places for the social groups defined by characteristics such as age or level of education. Clusters also appear in different places for the same social groups depending on different social information in different seeds, where social information proportional to original survey speakers is added randomly to empty matrix cells. Social ‘proximity’ is thus important to the creation of nonlinear clustering in scale-free networks.  5. When variants fill the grid, they rapidly increase in number of locations up to about 4,000 locations (c. 50%), then hit a plateau where the number of locations only rises very slowly. Persistence in the plateau stage produces stable clusters. The simulation thus has a life cycle for all surviving variants: constant motion across the grid, smaller temporary clusters for up to 250 iterations as a variant builds density across the grid, and (in addition to smaller temporary clusters) larger stable clusters after a variant reaches 50% density, a process that make take 1,000 iterations. This suggests that features in actual speech may also show a life cycle, e.g., common use across wide areas, temporary small areas in which particular features become very common for a time, and stable, potentially large areas in which features are persistent for long periods.  6. Running the simulation across all the variants does not produce the A-curve of values that we see in the survey data, so the A-curve in the survey data does not arise merely from the effects of proximity. However, we can create a separate array that represents what speakers remember in their  active speech, while allowing the rules to run across all the variants represents what is available to speakers in their  passive speech. As shown in Figure 3, this ‘active memory’ array does show an A-curve across all surviving variants when a small random factor is included, which acts on all variants but preserves many low-frequency variants that would otherwise die out. This suggests that the operation of the complex system preserves variants across wide areas in passive understanding of language, but that active use of language involves common use of a smaller number of variants per speakers with a nonlinear preservation of variants across the whole population; this active use of language is what a survey normally elicits. The simulation thus addresses individual human cognitive capacity.     Figure 3. Chart of ‘active memory’ for simulation with random factor of .01% after 1,000 generations (see ‘active’ and ‘enforced active’ tallies at right). Our use of a simple cellular automaton in a successful simulation suggests how we might better understand the survey and other data we have already collected, and also suggests how we might do a better job of collecting additional empirical data about language in future. The simulation indicates that we should use care in creation of overly complicated simulations when a simpler one will do. ",
        "article_title": "Computer Simulation of Diffusion: New Suggestions about the Process of Language Change",
        "authors": [
            {
                "given": "William",
                "family": "Kretzschmar",
                "affiliation": [
                    {
                        "original_name": "University of Georgia",
                        "normalized_name": "University of Georgia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00te3t702",
                            "GRID": "grid.213876.9"
                        }
                    }
                ]
            },
            {
                "given": "Ilkka",
                "family": "Juuso",
                "affiliation": [
                    {
                        "original_name": "University of Oulu",
                        "normalized_name": "University of Oulu",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/03yj89h83",
                            "GRID": "grid.10858.34"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "linguistics",
            "data modeling and architecture including hypothesis-driven modeling",
            "agent modeling and simulation",
            "english studies",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This position paper advocates joint efforts towards establishing a modular architecture for \"deep\" analytical approaches to DH collections such as text corpora. The core idea is to augment a typical DH project agenda with an element of project-independent and interdisciplinary bootstrapping, aiming to identify analytical subtasks beyond standard processing steps, which can be used as building blocks across different complex DH modeling processes (adopting McCarty's (2005) concept of modeling as a transition through temporary, provisional stages). Our discussion focuses on text-oriented DH projects. The guiding hypothesis is that despite the fundamentally different characteristics that the overall models may have across projects and disciplines, they can be broken down into modular steps many of which can be shared and/or developed in a joint effort. E.g., the detection of persona mentions (e.g., the teacher) in texts is a non-trivial task that is a necessary pre-step for, among others, social network extraction or intertextual relation detection. This task is related to standard Natural Language Processing (NLP) tasks (Named Entity Recognition, coreference resolution), but it targets no inherently linguistic category and a proper treatment requires insights from the humanities disciplines (here literary studies, but similar concepts underly network analysis in history/political science). We envisage a collaborative bootstrapping process of refining a persona identification module, taking into account its applicability in different higher-level research questions. Inspired by a well-established experimental methodology from computational linguistics, annotated test data and evaluation metrics should be fixed as operational prototype specification -- possibly starting with a temporary initial specification, whose role in the overall module structure is subsequently adjusted through bootstrapping. Such specifications foster the exploration of technical modeling alternatives on the computational side and provide the basis for critical reflection (cf. Gibbs, 2011) of the analytical possibilities and limitations on the humanities side, which can subsequently lead to revised characterizations. Although the goal of re-usable cross-disciplinary modules is very obvious -- from an engineering point of view -- a typical pattern in many DH venues focuses on the application and one-time adjustment of tools, thus underexploiting the true potential of modularization in DH projects. Some prominent interdisciplinary projects building service-oriented architectures have failed to connect with scholars (cf. Dombrowski, 2014), and experience shows that one of the hardest interdisciplinary challenges is to establish a mutual understanding about the strengths and limitations of some modeling component -- yet, when this has been achieved systematically, the potential for further developments is enormous. To reach an effective sharing of modeling components, methodological commonalities have to be identified across projects and the adequacy of an approach has to be assessed in a replicable fashion: we argue that a simple, but effective way for this is to highlight the relevant (and only the relevant) dimensions in reference annotations for some dataset(s) -- even if some related dimensions are yet ill-understood. This way of thinking is natural to computational linguists, but takes some getting used to for humanities scholars: The initial \"sharable\" version of a subtask appears trivial and/or insufficiently justified from theory. Our plea is to set these reservations aside and engage in a cyclic process advancing towards a modular toolbox of deeper and more complex notions. Ideally, this will lead to growing collections of shared reference datasets highlighting particular expectations for submodules; a project with slightly novel working assumptions may well add datasets with deviant characteristics (as long as decisions are well-documented at the meta-level). Methodologically, a rich collection covering a broad, interleaved space is the most helpful and may feed \"shared task\" initiatives for computational modeling (which have been highly influential, e.g., at the Conference on Natural Language Learning). Based on such data collections, methodological innovations can be tested systematically across projects and disciplines.   The Issue At first glance, the situation seems to be perfect: Independently developed tools and models from NLP (integrated, e.g., by European infrastructure initiatives like CLARIN and DARIAH) can be applied to DH text corpora, facilitating the scaling up of research methods beyond the classical ‘analogous’ approach. This scenario comes with a rewarding division of labor: NLP developers can provide and adapt tools; trained scholars in the humanities have full competence to apply the analytical machinery in their research. Often, however, project constellations are different: Available methods require non-trivial adjustments, data preprocessing poses considerable challenges, and/or analytical questions are addressed for which no tools are available. Although such constellations bear great potential for innovative ideas, practical issues often dominate the collaborative work. The time-consuming preprocessing and tool adjustment steps are often not publishable in the computational communities, and if the adjustments are successful, leading to new algorithmic approaches to scholarly questions, often most of the funding period has passed, so there is no time to work out and advertise the general methodological contribution.  Any predominantly content-oriented characterization of a collaborative DH project (otherwise surely the best justification of an agenda) walks a thin line of sacrificing methodological generality to respond to specific issues. Somewhat unfortunately, aspects that make a project interesting and challenging from a humanities perspective (exploration of new corpora / taking a different perspective than previous work) act against the objectives of systematic method development, which has to emphasize replicable results and modularization.    Addressing the Methodological Bottleneck We argue that the potential in advanced computational models and modular task definitions is underexploited in DH. So far, our considerations seem to imply that only dedicated mid- to long-term DH projects can overcome this status quo. However, we’d rather draw a different conclusion that works for smaller projects in a lively community: Rather than characterizing a DH project’s analytical questions top-down and relative to the specific data situation-which implies that the actual implementation cannot happen until after preprocessing and adaptation-characteristic methodological facets are anticipated early on, using prototypical manual annotation of reference data for  preliminary specification. Ideally, such specifications are based on reference data that are considered well-understood in the humanities disciplines and have known structural properties.   Incidentally, this is the workflow that is behind NLP modules that are now broadly applied: Early linguistic corpus annotation activities in the 1980/90s (which to this date provide the training data, e.g., for part-of-speech tagging and parsing tools) had to make theoretically loaded, controversial decisions, and a long and methodologically diverse process followed. Few people would have anticipated the usefulness of syntactic parsing tools, well beyond NLP standard tasks (and syntactic theoreticians would probably have vetoed many design decisions if they had been in charge). It is important to note that we do not assume that all ‘modules’ are working fully automatically. The collection of building blocks developed in the DH community may well include manual validation and hermeneutic interpretation steps. This ensures that the method-driven, modularization-friendly agenda that we advocate bears no implication about the characteristics of the composite DH model for a particular project. But for the non-computational building blocks, comparison and critical exchange across disciplines is just as helpful.   Discussion To some ears, our proposal may sound like a blunt attempt of computational linguistics to superimpose its purely method-oriented view on humanities scholars, who take their ultimate incentive from questions about content and interpretation. We have no intention to question established DH methodologies. Yet from conversations we had with researchers from a broad range of text-oriented DH initiatives, the picture arises that there is a great interest in tools solving problems similar to NLP problems, but so far there is no established methodological framework within DH for critically reflected development of such tools. The researchers we talked to tend to agree that community efforts towards a sharing of reference datasets and insights about the use of modular components for higher-level questions would be a very good complementation of the established DH methodology. Seeing the specification of partial tasks for computational modeling as a cyclic, collaborative effort also avoids the detachment of scholars in the role of a mere ‘user’ of custom-tailored tools. Instead the DH scholar is  empowered to influence crucial properties of the improved model by reacting to experiences from earlier stages.    Notes 1. We acknowledge that this paper has been inspired by various methodological discussions about DH (including the Dagstuhl Seminar on  Computational Humanities in July 2014 and the Leipzig workshop ‘Informatik und die Digital Humanities’ on 3 November 2014), involving a considerable number of researchers.  2. We consciously picked a sample task for which the beginnings of the methodology have already been established: Jannidis et al. (2015) present ongoing work on an approach for persona detection that expands the established Named Entity recognition framework. Bamman et al. (2014) is an example of advanced modeling in computational linguistics that takes literary characters into account. Reiter (2014) presents an approach to discover structural similarities over narrative texts that builds on the detection of characters/personae.  ",
        "article_title": "A plea for a method-driven agenda in the Digital Humanities",
        "authors": [
            {
                "given": "Jonas",
                "family": "Kuhn",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Nils",
                "family": "Reiter",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart, Germany",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "digital humanities - nature and significance"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Two years ago, a New Aesthetic was announced by Bruce Sterling in a widely circulated and debated text published on  Wired magazine: ‘An Essay on the New Aesthetic’. This aesthetic is, for him, as significant as any of our past avant-gardes: ‘the New Aesthetic is one thing among a kind: it’s like early photography for French Impressionists, or like silent film for Russian Constructivists, or like abstract-dynamics for Italian Futurists’ (Sterling, 2012). New Aesthetics can also be understood as a ‘collaborative attempt to draw a circle around several species of aesthetic activity—including but not limited to drone photography, ubiquitous surveillance, glitch imagery, streetview photography, 8-bit net nostalgia’ (Battles, 2008). It is concerned with documenting a contemporary phenomenon of ‘eruption of the digital into the physical’ (Sterling, 2012). A new language and sensibility have been identified in the intersection of humans and their machines, one that is related directly to the visual coding and translation of machinic interferences, glitches, and noise to the augmented field of human perception, which is enabled by pervasive digital media.   As in the case of any avant-garde, the New Aesthetic was first identified and presented by a cluster of young artists, led by James Bridle, a British computer programmer and designer who coined the term and developed the concept by doing content curation in his Tumblr page, http://new-aesthetic.tumblr.com/. He refuses to identify the New Aesthetic with a movement, however, stating that it ‘may be considered a work, a conversation, a performance, an experiment, and a number of other things (although, please, not a movement)’ (Bridle, 2013). Despite all his efforts to refuse being labeled as the founder of an artistic current, his Tumblr page became a classic contemporary work of digital curation as an art form. It basically is a gathering point of images and information about instances in which the digital becomes physically embodied, trying to raise awareness of the physical reality of information networks, together with 3-D printed objects or sculptures reminiscent of pixelated works. The objects portrayed point toward the material embodiments of data networks, ranging from satellites and drones to cellphones. The power of observation is here more important than the power of creation. The various technological affordances present in digital media are reshaping our relation to the material world, creating the conditions for the emergence of  immaterial physicality: ‘a transfer instantiating the immaterial in a physical form, a “print-out” whose tangibility then becomes the operative dimension in asserting the presence of an immaterial, digital, “information space”’ (Betancourt, 2013). The informational structure and logic of computational code ‘leaks’ into visual/digital culture, which is then translated into sensuous matter, pregnant with immaterial physicalities.   Whenever the New Aesthetic is mentioned, particularly in reference to the work of James Bridle, we are faced with a direct engagement with the cultural and political outputs of digital networks: their noise, visual identity, clear and/or obscure applications, and agendas. What is artificial blends with what is natural in such an intimate way that the New Aesthetic becomes a perfect example of Donna Haraway’s  naturecultures: collective expressions created and distributed by a network of post-cyborg agents living in a world of pervasive and invisible prosthetics, in which machinic species are equivalent to the material-semiotic assemblages of biological bodies. If ‘the machinic and the textual are internal to the organic and vice versa in irreversible ways’ (Haraway, 2003, 15), then so is their aesthetic. The work of James Bridle is iconic in regards to a New Aesthetic that represents the technological properties of the digital by accumulating virtual traces of material networks. By documenting and curating various examples of immaterial physicalities and bringing the ‘digital’ into the ‘physical’ in an automated and mechanic way, data aesthetics shape reality according to the fluid possibilities of the virtual, organically and meticulously. This article examines the inner workings and cultural implications of data aesthetics, and the ways in which digital curation and network dynamics become tools for a critical engagement with the world, connecting us to our physical landscapes in an unprecedented way, while redefining our perception and experience of time and space.  ",
        "article_title": "Data Aesthetics, Old and New.",
        "authors": [
            {
                "given": "Renata",
                "family": "Lemos Morais",
                "affiliation": [
                    {
                        "original_name": "Deakin University, Australia",
                        "normalized_name": "Deakin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02czsnj07",
                            "GRID": "grid.1021.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "media studies",
            "English",
            "internet / world wide web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " RICHES Mosaic Interface TM (RICHES MI; https://richesmi.cah.ucf.edu ) is the interactive and innovative digital platform for the Regional Initiative for Collecting the History, Experiences, and Stories (RICHES™) of Central Florida, an interdisciplinary project housed in the History Department at the University of Central Florida in Orlando. The mission of RICHES MI is to enable users to  search the database using natural language, tags, topics, and categories to maximize their search results;  analyze the results of their search using the ‘Connections’ module to show the relationship between the returned item and other items in the digital archive using a RICHES™-developed algorithm;  visualize results through digital exhibits, map overlays, and visualizations, and  learn regional history and historical methods as teachers and students use the RICHES™-produced digital modules and source sets. Combining multiple search and analytical tools in an interactive database offers a more effective approach to historical analysis and more closely approximates the process that historians traditionally use in research projects.  As historians moved into digitization of archival collections, the initial excitement of having ready access to rare documents and images took precedence over search and analysis problems that also accompanied this undertaking. Recent conference presentations and white papers demonstrate acknowledgement of these problems and advance solutions that call for greater collaboration between historians and archivists to write more detailed metadata and offer textual analysis tools for understanding larger datasets. A 2011 National Endowment for the Humanities–funded project on changing research practices among historians called for increased digitization of archival sources, the creation of new tools for interactive use of digital sources, and the development of capabilities for connecting smaller archives to larger repositories (Rutner and Schonfeld, 2012). In a discussion on digital history published in the  Journal of American History, Daniel Cohen predicted that the next iteration of scholarship would include ‘methods like collaborative filtering and recommendation systems’ (Cohen et al., 2008). Historians have followed the lead of digital literature scholars in the utilization of data mining and text analysis tools (Nelson et al., 2012), but we believe that historians need to view the results of such tools in context with other documents in order to gain insight into their broader meaning.  RICHES MI addresses a number of issues previously raised by digital historians and moves into the realm of analysis through the Connections tool. Still in its early stages of development, Connections intersects with scholarship on sensemaking that is most frequently associated with intelligence analysis and journalism (Pirolli and Card 2005; Pirolli and Russell, 2011). In the model advanced by Pirolli and Card, ‘The sensemaking process is organized into two major loops of activities . . . a foraging loop . . . [and] a sensemaking loop’. Our system seeks to address some of these challenges posed by the search for relationships between data and apply leverage to some key points of the sensemaking processes.  Designed and developed by an interdisciplinary team of historians and computer scientists, RICHES MI was constructed using several open-source programs, including Omeka, Google Maps, and MALLET (topic modeling). Plugins that permit users to contribute to the database and that enable the mapping of multiple sites for a single item were added. The Connections tool (which can be used in the Search and in the Bookbag) provides users with a tree diagram of related items and enables them to visualize the connections by time, location, tags, and topics. Finally, users can save selected items to a Bookbag, where they can organize the items into folders and annotate them. By following a well-known sensemaking process, our system supports digital research that more closely replicates the archival experience across multiple collections and large datasets. ",
        "article_title": "Using Multiple Strategies To Find Connections In Digital Archives: Making Sense of Historical Data",
        "authors": [
            {
                "given": "Connie Lee",
                "family": "Lester",
                "affiliation": [
                    {
                        "original_name": "University of Central Florida, United States of America",
                        "normalized_name": "University of Central Florida",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/036nfer12",
                            "GRID": "grid.170430.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "historical studies",
            "metadata",
            "natural language processing",
            "interdisciplinary collaboration",
            "spatio-temporal modeling",
            "English",
            "analysis and visualisation",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Histories of early modern European artistic printmaking have often taken a decidedly nationalistic approach. Giorgio Vasari included an entire chapter contrasting the achievements of Italian, Netherlandish, and German printmakers in the 1568 edition of his  Vite (Vasari, 1996, 80; Gregory, 2012). In the north, Haarlem-based art theorist Karel van Mander framed printmaking as a quintessentially Dutch medium, using his 1604 biography of the virtuoso engraver Hendrick Goltzius to rhetorically mirror Vasari’s elevation of Michelangelo in the  Vite, thus arguing that engraving could rival the finest masterpieces of painting and sculpture that Italy had to offer (Miedema, 1994, 385–406; Melion, 1991, 22–23). Modern canonical narratives often focus on the emergence of particular regional or national artistic printmaking communities, such as that in the northern Netherlands around the turn of the 17th century, when Haarlem printmakers Claesz Jansz Visscher, Esaias van de Velde I, and Willem Buytewech began to experiment with new graphic styles that diverged from more conventional international trends, and also turned increasingly towards ‘native’ subject matter, such as views of the Dutch countryside and cityscapes. Scholars have connected this turn in style and content to the coalescing of a Dutch national identity during the course of the revolt against Spain between 1568 and 1648 (Freedberg, 1980; Levesque, 1994)  Less well understood is the role that changing production practices, especially international collaboration, may have played in these shifts. Woodcuts and engravings—and, to a lesser extent, etchings—generally required the collaboration of artistic designers, plate cutters, printers, and distributors (Riggs and Silver, 1993). The sheer quantity and variety of prints from this period, and the number of individual printmakers and publishers involved, challenge traditional models of art historical argumentation. Newly available digitized museum databases offer the chance to analyze large-scale changes in the organizational patterns and artistic strategies of reproductive printmakers and publishers in the Netherlands, and of Europe at large, during the 16th and 17th centuries. This paper draws on the British Museum’s large database of European prints between 1550 and 1750 to infer a dynamic network model of European artistic print production in this period. The group-external / group-internal index of the regional/national groups of printmakers constituting this larger network offers a useful metric for describing the shifting balance of regional/national versus international print production in Europe in this period. The results of this study suggest the importance of structural incentives, under-analyzed in the current literature, that guided the development of regional artistic print production. Data and Methodology This study draws on the digital records of the British Museum’s collections, published as Linked Open Data using the CIDOC Conceptual Reference Model (Oldman et al., 2014). The database describes 53,462 dated prints (including woodcuts, etchings, and engravings) produced between 1550 and 1750. The British Museum has classified at least one associated creator for each of these prints, and assigned a production role such as ‘published by’, ‘print made by’, and ‘made after’. From these dated objects and associated production data, it is possible to construct a dynamic network structure, where designers, engravers, and publishers are interconnected based on the prints they produced. Date properties on edges (print production dates) and nodes (artist life dates) make it possible to derive subsets of the network at different points in time. Using the  igraph package for R, I wrote an analysis script that created 10-year slices of the network using a rolling window (Csardi and Nepusz, 2006).  To evaluate the dynamics of domestic vs. international interaction, I implemented an R version of the group-external / group-internal index. The E-I index measures the coefficient of group-external to group-internal ties for    N  classes of nodes (     N   1   =     n   e   -   n   i       n   e   +   n   i      ) where      n   e     represents all  external links from one class of node to another in a different class, and      n   i     represents all  internal links between nodes in the same class (Krackhardt and Stern, 1988, 127–29; Hanneman and Riddle, 2005, 128–32; Matos et al., 2014, 15). A positive coefficient thus indicates that members of one nationality made a majority of their connections to artists outside that nationality, while a negative coefficient indicates a majority of domestic connections.  Results Figure 2 shows the E-I trend for the six largest regional/national groups of printmakers. The Dutch, French, and English networks all have positive E-I values at 1550, meaning that all these networks were mostly connected to foreign actors. Each of these national networks seems to have experienced their own relatively swift shift towards a majority of domestic connections in the following centuries. While the Netherlands exhibits a sharp inward turn around 1575–1580, the French printmaking network had a more gradual inward turn that was already under way by 1550, but took a particularly sharp negative (i.e., inward) turn between 1600 and 1640. The English network also experienced a sharp turn, but as late as 1650, almost 75 years after the Dutch turn. Conversely, Flemish, German, and Italian printmakers initially connected mostly inwardly, only later shifting towards majority-external connections around the turn of the 17th century. Discussion The recurring pattern of sudden shifts from majority-external connections to majority-internal connections in the Dutch, Flemish, French, and English printmaking networks is striking, and suggests the pivotal effect of historical incidents on print production. For example, the Dutch inward shift around 1570 may have been partly due to the Spanish capture of Antwerp, which precipitated a massive exodus from the city, particularly of Protestants fleeing religious persecution. This migration infused the Dutch provinces with new wealth and talent, and it was recognized even by contemporaries that this wave powered the consumption side of the paintings market, and introduced innovations on the production side of that market. (On the effect of the Flemish migration into the Northern Netherlands, see Montias, 1982, 73; 1987, 459; Vries, 1991, 265; Sluijter, 2009.) However, it also seems to have had an effect on the organization of printmaking. In addition to an influx of new talent and demand generated by a surge in immigration, printmakers may also have adjusted their behavior as conflict made it more difficult to conduct the international business of printmaking, from the early transmission of contracts and drawn designs to the large shipments of finished impressions required for a successful printmaking business. With avenues to international collaboration cut off, and an increasing amount of domestic demand in centers like Haarlem and Amsterdam, the Dutch printmaking network underwent a rapid reorganization in the working relationships among Dutch printmakers to favor more domestic connections than before. However, these shifts did not occur at the same time across Europe. It seems little coincidence that the regional networks that are primarily inward-connecting in the mid-16th century (Flanders, Italy, and Germany) also had some of the longest printmaking traditions, dating back to the late 15th century (Landau and Parshall, 1994). The medium of printing demanded a set of artistic and technical skills, not to mention a set of social connections and financial capital, that presented a barrier to new entrants into the printmaking world. Designers needed to learn what aesthetic effects and compositions would translate well to the printed medium. Printmakers had to learn how to render painted or drawn lighting and color effects with graven lines. Print distribution presented yet another hurdle to new entrants to the market. Publishers needed a large pool of both financial as well as social capital: in addition to purchasing rolling presses and paper, they also had to coordinate with artists as well as with distributors and buyers in domestic markets and at the international book fairs. In the aggregate, these requirements presented a barrier not only to individuals but also to regions and countries. Germany and Italy, the respective origins of woodcut printing in the north and the south in the late 15th century, were able to make mostly internal connections through the 16th century. They would gradually receive an increasing number of foreign connections, as Dutch, French, and English artists sought to connect to expert printmakers. Over time, these externally dependent regions would begin to cultivate more native talent, knowledge, and physical resources, as experienced printmakers trained new students and transitioned from making prints to establishing their own publishing firms. It seems that once a critical mass of designers, printmakers, and publishers had developed within a country, these national networks shifted quickly, rather than gradually, towards increased domestic production. Congruent trends in other countries suggest that it was not the effects of one specific conflict alone that powered the Dutch shift towards more domestic printmaking; it is more accurate to say that war  catalyzed the organizational transformation that had already been made possible, and indeed inevitable, by longstanding international collaboration. This more rigorous and specific measurement of organizational changes will allow art historians to enrich our current histories of style and genre in printmaking, and more robustly characterize the relationship of artistic trends to historical ones.  Figures    Figure 1. Visualization of a small affiliation network inferred from four different print impressions.    Figure 2. Comparative plot of the group-external / group-internal index of French, English, Dutch, German, Italian, and Flemish artists between 1550 and 1750. ",
        "article_title": "Modelling the (Inter)National Printmaking Networks of Early Modern Europe",
        "authors": [
            {
                "given": "Matthew",
                "family": "Lincoln",
                "affiliation": [
                    {
                        "original_name": "University of Maryland, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "libraries",
            "archives",
            "museums",
            "relationships",
            "English",
            "graphs",
            "GLAM: galleries",
            "networks",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We present results of expanding the contents of the China Biographical Database [1] by text mining historical local gazetteers,  difangzhi地方志.[2] The goal of the database is to see how people are connected together, through kinship, social connections, and the places and offices in which they served. The gazetteers are the single most important collection of names and offices covering the Song through Qing periods. Although we begin with local officials, we shall eventually include lists of local examination candidates, people from the locality who served in government, and notable local figures with biographies. The more data we collect, the more connections emerge. The value of doing systematic text mining work is that we can identify relevant connections that are either directly informative or can become useful without deep historical research. Academia Sinica is developing a name database for officials in the central governments of the Ming and Qing dynasties.[3]     Problem Definition and Main Findings Figure 1 shows a scanned page from a  difangzhi,[4] and Figure 2 shows the text of the shown image. As Figure 1 shows, traditional Chinese texts do not have spaces between words or employ punctuation. This feature makes the processing of literary Chinese texts much more difficult than handling alphabetical languages and modern Chinese. The circles in Figure 2 serve as a general delimiter, representing the end of a line, the end of a page, a space, or a transition in text formatting—for example, placing two lines of text in a single column in column 2 of Figure 2.  We would like to algorithmically extract the information about local officials, such as Li Chang 李常in Figure 2. We are interested in the alternative names, such as the style name ( zi 字) and pen name ( hao號), birthplace, entry method, serving office, service time, and so forth. In this abstract, we focus on how we identify a person’s name, style name, and the dynasty. For example, we wish to extract the record Song宋as dynasty, Li Chang李常 as person name, and Gongze公擇 as style name after handling the text in Figure 2.   Local gazetteers record various types of information about local areas; we selected those that are related to local government officials. Not counting the circles in the texts, the current study employed 83 text files containing 901,302 Chinese characters.  We extracted 1,260 records from the files and compared them with the biographical data in CBDB. Table 1 gives an analysis of the results, where a circle indicates a match; a cross indicates a mismatch. Among the 1,260 records, 562 match the dynasty, personal name, and style name of some CBDB records, and 544 (43.2%) match only dynasty and name.             Methods Figure 3 shows the main procedure for extracting the records. In addition to the gazetteers, we used files of previously known names, addresses, entries, offices, and reign period titles from CBDB to annotate the texts.[5] In this study, we also consider the dynasties for names, offices, and reign periods.  We need to consider the ambiguities of a word when annotating the texts. For instance, ‘Li Chang’ 李常 was a person name in the Song, Yuan, Ming, and Qing dynasties, and the four-character office title ‘Guan-cha tui-guan’ 觀察推官 was an office in the Tang and Song dynasties. In addition the second and third characters ‘cha tui’ 察推also represent an office in the Song and Yuan dynasties. Hence, as illustrated in Figure 4, we could generate at least 16 possible label sequences for the following string T1 in Figure 2. T1: 李常字公擇南康建昌人自宣州觀察推官發運使 We sift the label sequences by adopting the principle of favoring longer words[6] and by disambiguating with contextual constraints. In T1, we do not consider ‘cha tui’ 察推 an office for the Song dynasty because the four-character sequence is a longer match for the same dynasty. In addition, it is reasonable to require that all labels in a sequence must be  consistent with the same dynasty. Hence, among the 16 sequences, only ‘李常-觀察推官’ for Song and ‘李常-察推’ for Yuan could survive.   Since there are no known tools for parsing literary Chinese, we employ the concept of language models (Manning and Schütze, 1999) to analyze the texts. We computed, collected, and counted the frequencies of  consistent sequences of six labels.[7]  Aiming at extracting personal names and style names for government officials, we focused on the consistent sequences that have at least one <NAME> label. We then identified and preferred subsequences that include more different labels. We show four such  filter patterns below.  P1: <NAME><ADDRESS><REIGN PERIOD><ENTRY> P2: <NAME><ADDRESS><ENTRY><REIGN PERIOD> P3: <NAME><NAME><ADDRESS><ADDRESS> P4: <NAME><ADDRESS><ADDRESS><ADDRESS> Finally, we selected the consistent sequences that contained the filter patterns and extracted original text segments that corresponded to the consistent sequences. The string T1 was extracted from the text in Figure 2 because it could be annotated with the sequence <NAME><ADDRESS><ADDRESS><ADDRESS><OFFICE><OFFICE>, which contained P4. The <NAME> label is for ‘李常.’ At the annotation stage, our programs did not recognize ‘Gongze’ 公擇as a Style Name for ‘Li Chang’ 李常 because ‘Gongze’ 公擇was not included in the CBDB name list. One of the two annotated results is listed below.  <NAME Song>李常</NAME>字公擇<ADDRESS>南康</ADDRESS><ADDRESS>建昌</ADDRESS>人自<ADDRESS>宣州</ADDRESS><OFFICE Song>觀察推官</OFFICE><OFFICE Song>發運使</OFFICE>  To extract ‘Gongze’ 公擇as a Style Name from T1, we parsed the text segment with a low-level grammar pattern for the task. Specifically, a two-character string that appears after the sequence of a <NAME> label and the character ‘字’ (Style Name) and before an <ADDRESS> label was extracted as the Style Name for the <NAME>. With such syntactic rules, we discovered that ‘公擇’ is a Style Name for ‘李常,’ and obtained two records (Song,李常,公擇) and (Yuan, 李常,公擇).           Results, Evaluation, and Applications We compared the extracted records with the combinations of dynasty, name, and Style Name in CBDB, and Table 1 shows the results. The two records that we just obtained would belong to type 2, because ‘Gongze’ 公擇 is not known to CBDB. All extracted records of type 2 provide opportunities of finding Style Names that were new to CBDB. However, they should be confirmed by asking a domain expert to check the original text segments, which is an operation facilitated by our software platform. Extracted records of type 1 do not provide new information if we are just interested in names and Style Names. Certainly, we are more ambitious than this, and type-1 records are instrumental. They help us find the beginnings of the paragraphs that contain extra information about the owners of the type-1 records. T1 is the beginning of the second paragraph in Figure 1. This paragraph contains extra information about ‘李常’ that we can explore to enhance the contents of CBDB. The third paragraph in Figure 1 and many following paragraphs start with statements that we could identify with the filter patterns.  Records of types 3 through 7 make up only about 12.2% of the 1,260 extracted records. Similar to type-2 records, these records do not match any records in CBDB perfectly. After inspecting the original text segments, we will be able to tell whether these mismatches are new discoveries or incorrect extractions.  Discussion The reported work represents an extension of our work for CBDB that was reported in Bol et al. (2012). In the previous work, experts manually designed regular expressions for specific text patterns. Now, based on prior information about named entities, we are able to compute and analyze the label sequences for the local gazetteer texts to learn useful filter patterns for automatically extracting desired information. We can apply the reported mechanism to extract birthplaces, service periods, offices, and other basic information, as we just did for extracting names and Style Names. In addition, by identifying key opening statements for paragraphs that contain biographical data, the reported procedure opens a new door for algorithmically extracting information about personal career and social networks. We are working toward learning the document structures of local gazetteers. Our work is related to automatic grammar induction in computational linguistics. Hwa (1999) learns grammars with data that were manually annotated with syntactic information, and we automatically annotated data with named entities. Klein and Manning (2005) employed advanced techniques to learn hierarchical grammars for Penn treebank sentences, which may be quite challenging in the case of literary Chinese.  Additional responses to reviewers’ comments are available at http://www.cs.nccu.edu.tw/~chaolin/papers/dh2015blw.online.pdf. Notes 1. The China Biographical Database (CBDB; http://isites.harvard.edu/icb/icb.do?keyword=k16229) is a collaborative project of Harvard University, Peking University, and Academia Sinica. CBDB is an online relational database with biographical information about approximately 328,000 individuals as of October 2013, primarily from the 7th through the 19th centuries. The data is meant to be useful for statistical, social network, and spatial analysis as well as serving as a kind of biographical reference. 2.  Difangzhi—see http://www.chinaknowledge.de/Literature/Terms/difangzhi.html.  3. http://archive.ihp.sinica.edu.tw/ttsweb/html_name/search.php. 4. When written in the vertical style, Chinese paragraphs begin from the right side of a page. 5. Here, ‘names’ include either official names or any alternative names. ‘Addresses’ refer to location names. ‘Entries’ (入仕方式), e.g., ‘進士’ and ‘舉人’, include different ranks and ways of becoming a government official via the Civil Service Examinations (‘科舉’). ‘Offices’ (官職, government positions) include posts in the government. ‘Reign period names’ (年號), e.g., Kangxi 康熙, are names of time periods under a particular emperor.  6. This so-called favoring the longer term 長詞優先principle is commonly adopted when segmenting (or tokenizing) Chinese text strings (cf. Gao et al., 2005). 7. Technically speaking, we are analyzing a 6-gram language model. ",
        "article_title": "Mining and Discovering Biographical Information in Difangzhi with a Language-Model-based Approach",
        "authors": [
            {
                "given": "Peter",
                "family": "Bol",
                "affiliation": [
                    {
                        "original_name": "Harvard University, USA",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            },
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Hongsu",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Harvard University, USA",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "text analysis",
            "English",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Human social life is a repository of symbols in various forms to mark, celebrate, and glorify social groups (Durkheim, 1976). Following the development of modern nationalism, national symbols are devised to identify attachment to specific territories and forge a national unity of particular ethnic and cultural heritage (Smith, 1999). The political, cultural, and ideological identity of an independent country is mostly proclaimed by three symbols: the national flag, the national anthem, and the national emblem (Firth, 1973; Geisler, 2005). The universal adoption of these ethno-cultural identity signifiers provides a great research opportunity to understand the global and regional characteristics of group symbolism in humanity (Butz, 2009; Elgenius, 2011). In this paper, we study the similarity and divergence of symbolic elements in ethno-cultural identity with a focus on national emblems. We apply the data analytic method of social network analysis to a total of 205 national emblems such that relational characteristics of symbolic elements and human communities are examined at multiple levels of composite granularity. We conduct exploratory functionalities of the symbolic analytic framework. The results indicate the potential of fruitful discovery in deciphering symbolism in humanity. As a case of application, this study shows the value of digital humanities research in complementing traditional methods of sampled observation and subjective induction. National Emblems and Data Model  A national emblem is an abstract or representational pictorial pattern that signifies the history, myth, or value of a nation and is regarded as one of the core national symbols. This pictorial pattern is designed to reflect and project a nation’s image with a layout of elementary entities in a set of colors. Every nation and state in the modern world has adopted a national emblem—sometimes also called a great seal or an official coat of arms—of its own for both domestic and international use, such as on the cover of a national passport. Compared to national flags, national emblems, in most cases, seem to contain more elements that encode rich anthropological and ethnic features. For example, the national flag of France (see Figure 1a), is a tri-color layout featuring three vertical bands in blue, white, and red, while the coat of arms of France (Figure 1b) is designed with stronger historical and cultural ingredients and attached with vivid symbolic meaning.    Figure 1a. French national flag.    Figure 1b. French national emblem. A national emblem is a graphical form of expression that encodes information both at the syntactic and semantic levels. At the syntactic level, we observe the surface forms of the elementary entities and the colors. The French national emblem is composed of entities such as an ax, a rod, an oak branch, a laurel branch, a lion’s head, an eagle’s head, a shield, and a monogram, in yellow (golden) and brown. These elementary entities symbolize justice, wisdom, victory, and name at the semantic level. We retrieve descriptive data of the national emblems of a total of 205 countries from Wikipedia. Each record includes the syntactic data of elementary entities with colors and the semantic data of symbolic meaning. The syntactic data of entities are further aggregated into class and meta-class to provide a three-level granularity of data analysis. For example, ‘ax’ and ‘sword’ at the entity level are aggregated into ‘weapon’ at the class level and ‘man-made objects’ at the meta-class level. Similarly, individual countries are also aggregated into region and continent levels. Table 1 outlines the three levels of data classification, the corresponding data size, and partial data content.   Table 1. Syntactic Data Model of National Emblems   Data Type Data Level Data Size Data List   Symbolic Element Meta-class 6 animal, plant, natural/physical objects, human, man-made objects, human ideas    Class 32 amphibian, quadruped, bird, celestial body, crop, tree, text, weapon, . . . , etc.    Entity 308 eagle, lion, sun, moon, star, rice, torch, ribbon, knight, dragon, diamond, . . . , etc.   Color — 9 black, white, grey, red, blue, green, yellow, orange, brown   Human Community Continent 5 Africa, Asia, America, Europe, Oceania    Region 20 East Asia, Southeast Asia, Central Asia, South Asia, Western Asia, North America, Central America, . . . , etc.    Country 205 Malaysia, Cambodia, Venezuela, Finland, Niger, Oman, Fiji, . . . , etc.   Applying Social Network Analysis   Affiliation Network Model  Social network analysis is a data analytic model that encodes relations among a set of entities and reveals the structural characteristics of their interaction pattern (Wasserman and Faust, 1994). For the national emblem data, social network analysis provides a systematic mechanism to explore and analyze the ideological and cultural relations among human communities. The current study focuses on syntactic data and separates the syntactic variables of symbolic element and color. In network modeling, we construct affiliation networks consisting of a set of symbolic elements (or colors) and a set of human communities. A symbolic element (or color) is linked to a community based on membership in national emblem composition. For dual perspectives, this two-mode bipartite network can also be projected or reduced to two one-mode networks (Newman, 2001; Zhou et al., 2007). One is the element (or color) co-membership network where elements are connected to each other when they share membership of a national emblem composition. The other one-mode network is the community affinity network, where ties represent mutual adoption of the same element (color) in national emblems. All ties in either two-mode or one-mode networks are valued by occurrence frequency to reflect the strength of the relationship.  Network Indicators  Social network can be analyzed at different levels for inspecting the embedded relations among entities. Among the many proposed network measurements, we adopt a set of core indicators to observe the essential characteristics of similarity and divergence of self-projected ideology and values among global human communities. The calculation of these network indicators and the network visualization are done with Gephi, a network analysis software package developed by Gephi Consortium.   • Node level:  Degree centrality measures the extent to which a node is involved in extensive relationship.  Betweenness centrality estimates how much a node lies on the paths between any two other nodes. Node ranking by both centrality indicators helps classify a node’s location as core, peripheral, or intermediary in the network.    • Sub-group level:  Modularity is a clustering measurement that detects sub-groups where there are more intensive links between nodes within than among the rest of the network (Newman, 2006). This indicator may be useful in identifying families of ideological elements and human communities at different levels.   Analytical Results The integration of a flexible analytical framework of social network analysis and the size and depth of the emblem symbolic data obviously provide a wide horizon of relational characteristics. For example, based on the syntactic data model of Table 1, we can construct two affiliation networks, color-human_community, and symbolic_ element-human_community, one at 1 x 3 and the other at 3 x 3 composite granularity levels. Each affiliation network can also be projected into two one-mode networks. This will result in a total of 36 networks that encode different aspects and various granularities of relational information. Limited by the space of this paper, we demonstrate two distinct analytic functions and present the results of a few interesting discoveries.  Discovery by Observation We first observe how the nine primary colors are used in national emblems across the world. Figure 2a shows the color-country affiliation network where the size of color nodes is proportional to its degree (frequency of use by country). The top three colors are yellow (149), white (133), and red (129), followed by blue (107), green (73), and black (41). All other colors are significantly minor.     Figure 2a. Overview of color-country affiliation network.    Figure 2b. Unique and strong association of color-region. Next we examine the color and region relations and apply the modularity measurement to approximate the separation of cohesive sub-groups (Barber, 2007). As shown in Figure 2b, several unique associative patterns between region and color are identified that provide interesting information. For example, the color blue is a common symbol of Northern Europe, Western Europe, and the Caribbean, while the color green uniquely represents Western Asia and East Africa. This analytic observation based on systematic data exploration can complement traditional symbolism research in sociology and anthropology (Kolsto, 2006; Podeh, 2011). For entity-country affiliation, it is observed that a total of 308 unique element entities are used 1,624 times in 205 national emblems with a long-tailed distribution. The top 10 entities are listed in Table 2 and account for 38.2% of occurrence frequency. When the one-mode entity co-membership network is constructed, several cohesive subgroups are identified. One of the interesting patterns of entity association is shown in Figure 3, with shield, eagle, lion, cross, crown, and spear as members of the primary core, and seems to indicate a strong European flavor.     Table 2. Symbolic element distribution.   Rank Entity Freq. Perc.   1 ribbon 110 6.8%   2 shield 109 6.7%   3 motto 97 6.0%   4 star 64 3.9%   5 sun 51 3.1%   6 lion 44 2.7%   7 nation name 43 2.6%   8 eagle 36 2.2%   9 crown 34 2.1%   10 mountain 33 2.0%              Figure 3. A sub-group of entity co-membership with European flavor. Discovery by Query Another way to decode the embedded information from the various affiliation networks is to conduct a focused exploration for specific questions. Suppose we start with the questions of how regions share the use of similar symbolic elements and what the overall diversity is across regions. An entity-region affiliation network may provide partial answers with appropriate granularity. Results of applying network indicators are shown in Table 3, where 20 regions are divided into nine subgroups based on common use of symbolic entities. Several families of regions emerge that reveal different aspects of similarity and divergence in symbolic expression of humanity. Sub-group 1 share a common cultural heritage, even though North America is geographically separated from other members. Members of sub-group 2 are both geographically distant and culturally independent. This grouping presents an interesting phenomenon and calls for further investigation. Sub-groups 3, 4, and 5 seem to be both geographically and culturally connected, while the remaining sub-groups are more isolated from others. Figures 4a and 4b show the entity-region networks of sub-groups 1 and 2. Table 3. Regional grouping by common use of symbolic entities.    Sub- group  Region(s) Core Entity Degree Centrality Betweenness Centrality   1 Southern Europe, North America, Western Europe, Central Europe ribbon 18 2313.8     shield 18 2174.9     eagle 15 1310.0     spear 12 1032.0     olive 11 972.4   2 South Asia, Northern Europe, North Africa star 18 2344.3     motto 16 1930.7     lion 17 1831.2     moon 12 990.1     sword 12 927.9     crown 13 819.9   3  Southeast Asia, Central Asia,  East Asia  sun 16 1838.7     nation name 15 1736.1     mountain 13 1280.2     wheat 10 609.3     river 9 512.4   4  West Africa,  East Africa, Central Africa  flag 13 1250.8     palm (tree) 10 875.9     ship 10 734.9     helmet 9 656.6   5 Western Asia, Eastern Europe sea 10 737.0     arrow 7 368.3     oak 8 357.9     horse 7 270.1   6 Caribbean, Oceania chain 7 375.9     stripe 5 194.8   7 Central America triangle 6 260.2     cogwheel 6 258.1   8 Southern Africa man 6 357.9     rifle 4 130.9     sheep 4 77.5   9 South America bow 5 270.1     ax 5 241.2     coffee 5 185.2   Discussion and Conclusion  Symbolism is a fundamental form of humanity expression. While a national emblem forms an integral context of correlated symbolic elements, we believe that a single symbolic element is itself a unit of idea expression and can be examined by an objective data analysis. It is the interpretation of analytic results that needs to take into account the contextual information. In this proposal, our purpose is to show the methodological process of analyzing the inter-relations among symbolic elements while cautiously refrain from making conclusive interpretation.  Through the co-occurrence of the lexical and geographical presence of the emblem symbols, we utilize social network analysis as an effective tool to reveal the emergent information.   This study demonstrates the great potential of employing a data analytic framework for explorative discovery in sociology and anthropology.  Our approach helps unlock the less than obvious information in the symbol-location relationship by presenting a lexical-based concept in a non-linear fashion, opening it up for interpretations that are not so readily available via standard appearance of materials. Furthermore, it allows the lexical-based concept to speak in its multiplicity. The initial results seem to provide rich implications for better understanding global humanity’s similarities and divergences. Our future work includes interdisciplinary research with sociologists and/or anthropologists for more in-depth investigation and interpretation, as well as an extension to analyzing the relations of symbolic semantic meaning in national emblems.      Figure 4a. Entity-region network of sub-group 1.    Figure 4a. Entity-region network of sub-group 2. ",
        "article_title": "A Study of Symbolic Element Network in National Emblems",
        "authors": [
            {
                "given": "Jyi-Shane",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "Natioanl Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Ke-Chi",
                "family": "Ning",
                "affiliation": [
                    {
                        "original_name": "Natioanl Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Tze-Jung",
                "family": "Huang",
                "affiliation": [
                    {
                        "original_name": "Natioanl Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "content analysis",
            "relationships",
            "anthropology",
            "English",
            "graphs",
            "linking and annotation",
            "data mining / text mining",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The point of departure for this paper is the Renderings project  (http://trope-tank.mit.edu/renderings/) established in 2014 and developed at the Massachusetts Institute of Technology in a lab called the Trope Tank. The project is described as concentrating on translations of highly computational and otherwise unusual digital literature into English. Its members ‘not only employ established literary translation techniques, but also consider how computation and language interact. Literary and computational experts worldwide participate’. The current team includes Nick Montfort (the initiator and leader of the project), Patsy Baudoin, Andrew Campana, Sally Chen, Aleksandra Małecka, Piotr Marecki, and Erik Stayton. During the project’s first year, 13 translations or bilingual works, by 12 authors, have been produced in the following languages: Chinese (1), French (3), German (1), Japanese (4), Polish (2), and Spanish (2). The translated works are    • Automation (2013) by Andrew Campana.  • Contemporary Japanese Poetry Generator (2012) by Shinonome Nodoka.  • Dizains (1985) by Marcel Bénabou.  • Hallelujah (2012) by ni_ka.  • MAZ—Mutantist Autonomous Zone (2014) by Mathias Richard.  • Poem 21 (1988) by Amílcar Romero.  • Poet (2003) by Michał Rudolf.  • Sample Automatic Poem (2009) by Féliz Remirez.  • Seika no Kôshô (2013) by Andrew Campana.  • Shanshui by Sally Chen.  • Speeches (1993) by Marek Pampuch.  • Tötan das Gedich (1997) by Johannes Auer.  • Triolets by Paul Braffort. The programming languages of the original works include Basic, Perl, and Java Script. They were selected to represent the wide variety of genres of electronic literature and creative computing, and the productions of cultures/literatures not currently well known in this dominantly English-language field. Thus, the first Renderings set of works includes genres characteristic of specific cultures, such as Japanese ‘monitor poetry’ (a blog that bursts of flowers, hearts, and other graphics dense enough to obscure the screen), a Polish generator of communist speeches, electronic ‘landscape poetry’ from China, and electronic OULIPO texts (France). The selected works also present different approaches to computation in literature. The project itself thus has the aim of describing the experiences of the margins of digital culture and exploring the hitherto overlooked fringes of the digital heritage. The Renderings is not the first project exploring translations of electronic literature. There have already been translations of Michael Joyce’s, Stuart Moulthrop’s, Nick Montfort’s, and Stephanie Strickland’s works from English into other languages. In addition, the Electronic Literature Organization was a co-sponsor of the conference Translating E-Literature in 2012 in Paris. The Renderings project continues these threads, but focusing on the direction from other languages into English, its goal being to give English-speakers access to works from other traditions. The project involves also meetings and brainstorming with literary translators: Robert Pinsky, Marc Lowenthal, John Cayley, and David Ferry. Translating digital works written in code requires the translator to face new challenges in addition to those tackled by the regular translator of literature. It is a type of translation akin to the translation of experimental, conceptual or constrained works. It is not rare that the task requires the translator or translators to reinvent the work in a new linguistic and cultural context, and sometimes also another programming language. The history of literature is already familiar with similar cases, like the translations of works of the French OULIPO group; for instance, Georges Perec’s La dispariton, which is written without the most frequently occurring vowel of the French language, has been be rendered in other languages with the omission of the most frequent vowel in the language of the translator, e in English but a in Spanish. In the case of highly computational digital works there are additional difficulties and challenges, first and foremost, the formal and material properties of the code of the program. If we assume after Noah Wardrip-Fruin that a digital work has three layers: the input, process and output, the task of translation will be operated mostly on the first two layers: the input and process. It will require establishing a lexicon, determining the input data, which may differ given the discrepancies between grammars (inflection, declension, genre) and translating the process, that is the lines of code responsible for producing a given output. Noah Wardrip-Fruin explains to humanities scholars analyzing digital works that they focus mainly on the output level, which he considers a superficial approach. It seems that the work of the translator of a digital work is the ideal activity for performing what this scholar calls ‘expressive processing’. Translating a highly computational work without the knowledge of its inner workings and operation on all the three levels of analysis should not occur. For instance, the novel World Clock, written by Nick Montfort in Python, has 165 lines of code in its original English version. Its Polish translation has an additional 60 lines of code. Indeed it is not unusual for the output of the translated work to be the result of processes different than in the original code. An important category for the translation of digital works is collaborative work, in a team including a translator and a programmer, where translation and programming competences overlap and complete. The translation of a digital work is not only a matter of language, but also requires awareness of the code and the platform for which it was designed. Especially in the case of older works, the translator has to consider porting the work to a platform more accessible to the contemporary reader. The textual generator Poet was written in Perl in 2003 and archived online as a .pl file. It will be thus available to those readers who know how to run the program in the terminal and are willing to download and execute it. Given the goals of the project, it seemed justified to port the program to Java Script in order to publish it online, to provide easier access to readers on the Web. In such a case the translator has to preserve as many aspects of the program’s functioning as possible in the original programming language. Yet another aspect connected to this problem is the change of platform. Platform consciousness and platform description are described according to the methodology developed by Nick Montfort and Ian Bogost in books from the Platform Studies series published by MIT Press. It is a method that ‘investigates the relationships between the hardware and software design of computing systems and the creative works produced on those systems.’ And so when describing the generator of communist speeches translated and published online as part of the Renderings project, an important aspect of its historical, formal and material analysis will be the consideration of the consequences of the fact that it was first written in a popular programming language for the Amiga. The presented paper describes a phenomenon belonging to the broadly understood discipline of creative computing and studies the work of the translator as taking place both in code and language, drawing from the methodology developed by the fields of code studies, platform studies and expressive processing.  ",
        "article_title": "Renderings: Translating Literary Works in the Digital Age",
        "authors": [
            {
                "given": "Piotr",
                "family": "Marecki",
                "affiliation": [
                    {
                        "original_name": "Jagiellonian University, Poland",
                        "normalized_name": "Jagiellonian University",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/03bqmcz70",
                            "GRID": "grid.5522.0"
                        }
                    }
                ]
            },
            {
                "given": "Nick",
                "family": "Montfort",
                "affiliation": [
                    {
                        "original_name": "Massachusetts Institute of Technology, US",
                        "normalized_name": "Massachusetts Institute of Technology",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/042nb2s44",
                            "GRID": "grid.116068.8"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "natural language processing",
            "literary studies",
            "creative and performing arts",
            "including writing",
            "linguistics",
            "interdisciplinary collaboration",
            "archives",
            "translation studies",
            "internet / world wide web",
            "semantic analysis",
            "publishing and delivery systems",
            "text analysis",
            "English",
            "history of Humanities Computing/Digital Humanities",
            "multilingual / multicultural approaches",
            "text generation",
            "programming",
            "sustainability and preservation",
            "philology"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this paper, we propose a fully automatic system for the transcription alignment of historical documents. We introduce the ‘Statuti del Doge Tiepolo’ data that include images as well as transcription from the 14th century written in Gothic script. Our transcription alignment system is based on forced alignment technique and character Hidden Markov Models and is able to efficiently align complete document pages.  * * * Many libraries and archives are currently engaged in large-scale digitization programs to make their documents accessible to larger audiences. However, to make these ancient documents searchable and linkable, it is mandatory to go beyond digital images and encode their textual content. Text transcription is a notoriously complex and time-consuming task, requiring years of scholarly practice, particularly when one has to deal with a range of different scripts.  Machine learning approaches need good training sets to succeed. Luckily, several ancient manuscripts have been already transcribed by paleographers and exist as critical printed editions. These transcriptions could serve as a relevant starting point for developing automatic encoders. However, an additional step must be performed to use these materials as training sets in machine learning approaches: transcriptions must be aligned with digital images of the manuscripts. Most handwritten documents have transcription aligned only at the page level. To be used as a training set, it is necessary to generate a mapping at the line level at the very least. The goal of this paper is to present a full transcription alignment system for historical Latin page documents using Hidden Markov Models (HMMs). In this work, we focus on the Statuti of the Doge Tiepolo, a manuscript transcribed and studied extensively by Prof. Lorenzo Tomasin (2001; 2007). Our system presents many advantages, such as the following; no segmentation into words is needed, the emission probability density functions of the HMMs can be trained to model variations of the character shapes, and the same system can be used for transcription alignment and recognition in an incremental training process. So, the decoding procedure will carry out recognition of words/lines, line/word transcription alignment, and segmentation into words/character models at the same time. Corpus  The  Statuta Veneta of the Duke Iacopo Tiepolo are the principal statutory norms of the Venetian Middle Ages, which were promulgated in 1242.  It is one of the most ancient experiments of translation in vulgar of an Italian normative source, and it represents the act of foundation of a linguistic register. The existence of multiple medieval versions of the  Statuta Veneta in the vernacular has already been verified; the most ancient has been found in three manuscripts held in Vienna and in Venice (Tomasin, 2001; 2007). Prof. Lorenzo Tomasin has already carried out a complete transcription of the Vienna manuscript.  In this work, we use the Venetian version of the manuscript (Senato e collegio Miscellanea Statuta Veneta) with a diplomatic transcription of 72 pages. A diplomatic transcription copies everything it sees as it is. An example of a page image and its transcription is presented in Figure 1.        Figure 1. Page image and transcription from the Statuti del Doge Tiepolo corpus.   System Description   In this section we describe the four main steps of our method: page segmentation, text segmentation, textline detection, and transcription alignment.  Page Segmentation  The first processing step aims at segmenting each page of the scanned document. Each scanned image contains one page surrounded by a black border and a portion of the subsequent page (Figure 2).       Figure 2. Scanned image from the Statuti del Doge Tiepolo corpus. To segment the page we use a binarization-based approach, as follows:   •  Binarization using the technique proposed by  Howe  (2013) (Figure 3).   •  Contour Detection: The binarized image is processed in order to extract all the contours (Figure 4) (Suzuki, 1985).    •  Contour Classification: The contour having the largest area is classified as the one circumscribing the page of the text (Figure 5).    •  Vertical RGB Projection Profile: For each image it is necessary to detect the book binding. Each page is positioned horizontally with respect to the border of the image (Figure 6). The projection profile is processed in order to extract all the local minima. A voting system based on the a priori knowledge of the aspect ratio of the document page is used to elect the best local minima corresponding to the bookbinding separation (Figures 6 and 7).         Figure 3. Howe (2013) binarization.       Figure 4. Contour detection.       Figure 5. Contour classification and image cropping.       Figure 6. Vertical projection profile.       Figure 7. Page cropping. Text Segmentation  The second step aims at segmenting the pixels of the text. For this step we use the adaptive thresholding approach proposed by Howe (2013). In this case we estimate the thresholding parameters by a brute force technique applied to the 2D cost function:        For each iteration of the optimization procedure, the connected regions are recomputed over the binarized image.  Std x is the average standard deviation of the pixel x position of the extracted connected regions,  std y is the average standard deviation of the pixel y position,  area is the average surface of the connected regions, and  num contours is the number of extracted connected regions.   Textline Segmentation  The third step corresponds to the detection of text lines. For this task we have created a novel approach based on the following steps:   • Blurring of the binarized image (Figure 8).  • Binarization of the blurred image (Figure 9).  • Contour detection (Figure 10).  • Iterative contour expansions: each polygon detected in the previous step is expanded iteratively by 1 pixel until it touches another polygon. The procedure stops when each polygon touches at least one other polygon (Figure 11).        Figure 8. Blurring of the binarized image.       Figure 9. Binarization of the blurred image.        Figure 10. Contour and textline detection. Transcription Alignment  The transcription alignment system is HMM-based. This system is inspired by Slimane et al. (2008). Figure 11 illustrates the different phases of the text recognition / transcription alignment system.  In the feature extraction phase, each text line is transformed into a sequence of feature vectors extracted by moving, from left to right, an analysis window with a width of 17 pixels and a shift of one pixel. Sixty-three typographical and statistical features and delta coefficients are extracted from each window. For more details, we refer to Slimane et al. (2008) and Mezghani et al. (2014).  In the training phase, each character model, represented by six HMM states, is trained using labeled textline images. Our system is developed with 53 character models, including white space and punctuation characters. All the observation sequences are used to estimate the emission probability functions of each character model. The training procedure involves two steps that are iteratively applied to increase the number of Gaussian mixtures to a given M value. In the first step, a binary split procedure is applied to the Gaussians to increase their number. In the second step, the Baum-Welch re-estimation procedure is launched to estimate the parameters of the Gaussians.  In the transcription alignment phase, the feature vector sequences of all extracted text lines of one page are first concatenated into a single sequence. Then the page HMM is created using the concatenation of all character models given by the page transcription. Finally, the Viterbi algorithm is employed for forced alignment of page image, resulting in line boundaries. So, for each line image, we will obtain the corresponding transcription.  This system can be used for recognition using a HMM to build an open vocabulary recognizer when all transitions from one character model to the others are allowed.       Figure 11. HMM-based text recognition / transcription alignment system.  This system, evaluated using 72 pages (2,302 text lines), achieved considerable accuracy for transcription alignment: 98.44% of the returned words were correct in terms of word position and label.  Conclusion  In this paper, we propose a fully automatic system that is able to efficiently extract handwritten text lines from images and perform their transcription alignment at the page level.  In the future, we will continue the optimization of the system, and we will also test it on other scripts from the Venice Time Machine project (http://vtm.epfl.ch/).  ",
        "article_title": "Text Line Detection and Transcription Alignment: a Case Study on the Statuti del Doge Tiepolo",
        "authors": [
            {
                "given": "Fouad",
                "family": "Slimane",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Andrea",
                "family": "Mazzei",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Lorenzo",
                "family": "Tomasin",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This presentation will précis the underlying entity relationship model of the Research Environment for Ancient Documents (READ) and will outline some of the research methods enabled by the platform. The READ project commenced in 2013 with development support from a consortium of universities (Ludwig Maximillian University, Munich, the University of Washington, Seattle, and the University of Lausanne), which, together with the University of Sydney, are involved in the study and publication of ancient Buddhist documents preserved in the Gāndhārī language that originate from Afghanistan and Pakistan. READ leverages 10 years of development on gandhari.org by Stefan Baums and Andrew Glass—the site that currently supports the Gāndhārī Dictionary, Bibliography and Catalog, as well as a comprehensive textual corpus. READ model and UI design are nearing completion, and development is on track for delivery in late 2015. READ has been designed as a comprehensive multi‐user research workbench and publishing platform for ancient Sanskrit and Prakrit texts: manuscripts, inscriptions, coins, and other documents. READ is based on open-source software and built-to-open standards. It provides an extensible entity model, TEI support, and a published API for integration with related systems. This presentation will focus on design aspects as encapsulated in the entity relationship model (ERM). An ERM is an abstract way of describing a relational database, most often represented as a flowchart accompanied by precise descriptions of each entity. This approach allows one to model the entities and their relationships and determine the most effective and flexible way of structuring the data to support authoring, storage, maintenance, analysis, reporting, and publishing. The design approach has been to build a comprehensive set of entities, mapped to real-world objects, in order to seamlessly model both physical and textual domains. The underlying design principle is atomisation of data to its smallest indivisible components and the linking and sequencing of these entities. As an illustration, in the physical realm, manuscripts or other inscribed  Items have  Parts,  Fragments, and  Surfaces.  Images of these  Surfaces are segmented to provide a fixed reference system, a  BaseLine much like the grid laid out at an archaeological excavation. These  Segments are then sequenced into  Spans across a  Surface.  Surfaces can then be aligned and the  Spans sequenced into complete  Lines. The mapping between physical and textual is where  Syllables are identified with  akṣara (the graphical unit of the  kharoṣṭhī script these texts are rendered in)  Segments.  Syllables (deconstructed into  Graphemes) are sequenced into  Tokens (words or compounds) that are ultimately sequenced up through textual divisions into full  Editions.  The philological process model adopted is one of defining each entity by applying classifying metadata and progressively sequencing these entities from the smallest upwards. This approach allows for attribution and annotation at every entity to record scholarly contribution at the finest level of granularity. Different editors’ interpretations of  akṣara Segments,  Syllable assignation, and  Token grammatical deconstruction are all attributed, and multiple versions of all entities exist in parallel to support the publishing of any number of alternative editions.  In summary the design approach rests on a small number of basic principles:  1. By designing entities and their relationships to reflect real-world objects in the domain, one can flexibly map system functions to actual philological processes.   2. By completely atomizing physical and textual objects, one can flexibly sequence these components into higher-level entities.  3. The application of metadata and progressive sequencing of entities from the smallest component upwards through scale supports flexibility and granularity. The technology stack includes PostgreSQL with backend development in PHP and UI development using JQuery—all mainstream open-source development environments. A conventional software architecture has been adopted with    • A data storage system built on a relational database where entities are realized as tables with constraints and triggers to implement model logic and integrity.   • A storage abstraction layer implemented as server side libraries to expose entities and entity aggregates needed to support the web services.  • A variety of services including import, export, index, query, and data management. Services are being implemented as using Ajax for complex interactive UI functions.  • A JavaScript User interface using JQuery frameworks.  • A JavaScript API to provide access to libraries and core services to allow for system extensions and integration. A great deal of work has been undertaken on the design and development of a comprehensive system ontology. The ontology provides standardized terms for both physical and textual domains—everything from object shapes, mediums, and types to grammatical categories such as declension and conjugation values. These standard terms manifest as constraints within the system and ensure data consistency and quality. The system ontology has been designed to be extensible and configurable to allow READ to encompass alternative schema and new research objectives. Users can develop their own term sets within the ontology to address their own research questions. For example, my research encompasses the development of a metadata model for the analysis of formula structures in relic inscriptions, others wish to address syntactical structures, and still others questions of metre. A practical example of the flexibility in the ontology is the support for languages other than Gāndhārī. READ has been designed from the ground up to support all languages that have Sanskrit as their primordial. Branching in the system ontology allowing for alternative grammatical deconstructions enables READ to be easily configured for Sanskrit or Pali corpora—indeed, any Prakrit language. Fundamental to the design of READ is support for the import and export of TEI. The method adopted has been the development of services to export a complete XML rendition of stored data with XSLT transformation to EpiDoc TEI specifications. Import is the same process in reverse. A highly sophisticated parser was developed to read the existing corpus of transcriptions from gandhari.org as text strings and shred these into READ database entities maintaining linkages. This parser can be finessed for other corpora that might currently only exist in unstructured formats like Word. Fundamental to READ’s design is support for import of existing datasets and export as XML and specifically TEI. Indeed READ is positioned as a workbench for ancient documents, complementary to TEI-based textual repositories such as SARIT and integrated with existing Sanskrit dictionaries. Whatever format existing transcriptions were developed in, these can be consumed, elaborated upon, analyzed, and then published as research output in TEI or pdf. The data remains open-source and can be exported as a full XML archive. In summary, READ has been designed to function as  • A linked repository of images, transcriptions, translations, metadata, commentary, and bibliographic records.  • A content management system encompassing multiuser editing, maintenance, and version control.  • A collaboration platform with comprehensive access and visibility control to support draft development, workgroup collaboration, and public presentation.  • A research workbench with access to a dictionary, corpus of texts, catalogs, glossaries, concordances, and bibliography.  • A publishing platform for individual transcription renditions or full scholarly editions, both print-ready and online.  • The kernel of an integrated research network interfacing with related dictionaries, repositories of parallel texts, GIS, data visualization, image rendering, and palaeographic analysis systems. READ enables a very granular data structure, and whilst the level of elaboration is dependent upon the research requirement, deriving the most benefit entails an initial investment in the application of metadata at each level of sequenced entities. The dividend is that this enables a range of automated analysis outputs that support palaeographic, phonological, grammatical, orthographical, and morphological research in addition to the opportunities opened up for formulae and syntactical analysis. ",
        "article_title": "Entity Relationship Model for READ",
        "authors": [
            {
                "given": "Ian",
                "family": "McCrabb",
                "affiliation": [
                    {
                        "original_name": "University Of Sydney, Australia",
                        "normalized_name": "University of Sydney",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/0384j8v12",
                            "GRID": "grid.1013.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "publishing and delivery systems",
            "data modeling and architecture including hypothesis-driven modeling",
            "metadata",
            "corpora and corpus activities",
            "bibliographic methods / textual studies",
            "text analysis",
            "English",
            "translation studies",
            "philology",
            "software design and development",
            "asian studies",
            "ontologies",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Folktales and fairy tales are very often figured, in literary studies, as texts with sociohistorical and ideological force; for example, a given study might theorize how a particular discourse of fairy tales replicates gender roles, how fairy tales in American life support a Disnified culture industry, and so on. But significantly, fairy tales have also long been studied as data objects; since the 19th century, they have been conceptualized and categorized in terms of genre, origins and historic-geographic distribution, and form and structure—in essence, there is a strong sense that fairy tale scholarship depends on the cataloging of tales into various systems of ontological understanding.  In addition to existing in the written records and datatables of cataloguers, fairy tales are quite often associated with many other forms of transmission and dissemination. Certainly, it is common to encounter fairy tales (in many forms) in the context of oral storytelling, but tales also have been performed as plays and ballets, turned into cinematic or televised productions, illustrated in picture books and collections, and in our modern consumer culture, made into toys, dolls, video games, and other products. While the status of tales as texts has been long studied, these different medial iterations of tales have received less attention from scholars trained in literary studies and folklore programs. Yet it’s in this space, beyond the literary representation of fairy tales, where our modern comprehension of their power becomes most fully realized, existing and replicating in a way that Werner Wolf has described as ‘intermedial’. Intermediality, according to Wolf’s conceptualization, encourages attention to semiotic modes (speaking, writing, visually representing) and technological channels (printing presses and books; cameras, projectors, and films; broadcast stations, television sets, and TV shows) (2011). This notion of seeing narrative progress along various media channels is an instantiation of what Marie-Laure Ryan advocates. Ryan has suggested that the studying of narrative occurs best across media because narrative study that is focused only on language leaves out the nuances of different communicative modes and the technologies and social relationships that make such communication possible (2004). In other words, our premise is that studying intermedial forms of fairy tales help us better comprehend how these tales continue to work from telling to telling. When it comes to folklore studies, the natural attraction to ontology and catalogue makes the digital humanities an attractive scholarly cohort, and a subfield often labeled ‘digital folklore’ is gaining traction. Because of the inherent ability of digital analysis methodologies to cross media (or deal with texts that exist in multiple media at once), it makes sense that, if we want to study intermedial fairy tales as a new entry point into these texts, then the crossroads of the computational (or ‘data-driven’) and the narrative is the space for fairy tale scholarship in the 21st century. Timothy R. Tangherlini has encouraged the use of algorithmic approaches to help better navigate the complex intermedial spaces of folklore narrative; leveraging the term ‘computational folkloristics’ (as distinct from digital folklore), Tangherlini states the importance of generating ‘algorithmic methods to assist in the interpretation of relationships or structures in the underlying data’ (2012). Indeed, Tangherlini’s efforts in helping give rise to the study of folklore through computational methods is the launching point for our project in looking at intermedial fairy tales; 1 given the rich history of structural, ontological-based (or perhaps more appropriately termed, for our purposes, metadata-based) approaches to fairy tale study, the power of exploring fairy tales in intermedial spaces, and the concept of applying paradigmatic methodologies from the digital humanities as an analytical tool for analyzing fairy tales today, we have begun to identify relationships between tales, media, history, geography, production and reception, and scholarly and popular audiences in a new light.   For the bounds of this proposal (which is one piece of a recently awarded SSHRC collaborative grant to look at intermedial fairy tales), we are concerned with the occurrence of fairy tale storytelling in a very specific intermedial space—that of modern American television. For the past 85 years, fairy tales have found their way into television broadcasts in numerous ways, whether it be quick allusions to fairy tales to advance otherwise non–fairy tale narratives; spectacular retellings of tales as live events or made-for-TV ‘specials’; fairy tale contexts for particular episodes of traditional, procedural television; longer-running arcs of serialized shows; or even premises for entire shows constructed on fairy tale tropes. We have begun with a teleography—a metadata-rich bibliography of television fairy tales compiled by Kendra Magnus-Johnston and recently published in  Channeling Wonder: Fairy Tales on Television. With over 1,000 data points (individual episodes or TV specials related to a fairy tale in some way), our project will approach this metadata in ways similar to how historical fairy tale scholars have: looking for patterns in terms of narrative similarity, common genre, chronological proximity, and so forth, but doing so with tools and experiments that leverage data processing and visualization methods that have become significant paradigms in digital humanities scholarship.  Our goal in beginning with data modeling is to reposition the existing teleography as a sort of intermedial fairy tale corpus that can be mined and analyzed visually, spatially, and temporally. Through automated text parsing, processing, and categorization; aggregating information both from the teleography as well as from other sources (semantic repositories of linked data such as Freebase.org and other relational datasources such as the crowdsourced website and API thetvdb.org); and generation of our own dynamic dataset (serialized in various forms—tabular CSV and JSON right now), the scholars and students involved in this phase of the project have been better positioned to comprehend the scope of the fairy tale data and metadata, and identify the types of interactions and relationships that might be exposed by different types of visualizations. In many ways, this sort of data structuring is a reimagining of the work done by Antti Aarne and Stith Thompson (of the Aarne-Thompson classification system) or by Hans-Jorg Uther’s  ATU Catalogue; in fact, our data model, mastered in a MongoDB instance and published out to a web app, uses a fairy tale’s ATU number as a unique identifier, supplementing missing tales as needed (the current state of the searchable, filterable teleography web app is available at http://fttv.byu.edu/#datatable). This preliminary data work has already demonstrated ways that the medium of television has gone far beyond the traditional animal, magic, and folktales of the ATU ontology; we have found it useful to incorporate into our data such literary “fairy tales” as  Peter Pan or  The Wizard of Oz, Hans Christian Andersen tales such as ‘The Ugly Duckling’, ‘The Little Mermaid’, and ‘The Snow Queen’, or more abstract references such as ‘princess-ness’, ‘grimm-ness’, and so forth. Our plans for the underlying data source are lofty, as we are continually working to incorporate more and more occurrences of fairy tales on television as well as to augment the data with more metadata, such as broadcast dates, actors involved, broadcast and cable channels where they occur, and so forth.  Of course, given that our goal is to approach the study of fairy tale narratives in an intermedial space, we aren’t satisfied with just developing our own indexed data source; rather, we have begun to analyze its structured form by generating a collection of visual representations of the data (see http://fttv.byu.edu/#visualizations). With everything from simple histograms, charted occurrences of fairy tales on TV over time, and force-directed or matrix-based network graphs built with d3.js (that can juxtapose different metadata variables with our expanded ATU tale-type ontology), we are recognizing new patterns and insights about the tales themselves as well as about the medium in which they’ve appeared. For example, one student, seeing the highest frequency of fairy tale appearances in American television to be ATU 510A (Cinderella), is now investigating the relationship between the narrative of the servant-turned-princess and the national discourse of American exceptionalism. Another is looking at the rise of reality television as a replacement for the traditional fairy tale story, while a third is exploring the many references in a single television series ( Fractured Fairy Tales, snippets of tale retellings which appeared in 91 different episodes of  The Rocky and Bullwinkle Show) as an entry point for understanding commodity capitalism and self-valuation during the late 1950s–early 1960s.   Cristina Bacchilega has suggested that modern treatment of fairy tales expresses the evolution of human hopes and fears (1997); television’s ability to appropriate, repurpose, or refigure tales confirms this. Combining multiple tales to tell new stories is also leading scholars to look at intertextuality between fairy tales. Note 1. As far as we can tell, there have not yet been any computational approaches to intermedial fairy tale analysis. There are a few projects in which scholars have sought to apply digital humanities methodologies to traditional fairy tale texts such as  Grimm’s Fairy Tales; notable among these are Declerck et al., who describe a TEI-based markup format and a tool for semiautomated markup of fairy tale text (2011), and Weingart and Jorgensen, who use algorithmic analysis of European Fairy Tales to discuss conceptualizations of the body (2013).  ",
        "article_title": "At the Crossroads of Data and Wonder: Algorithmic Approaches to Fairy Tales on Television",
        "authors": [
            {
                "given": "Jarom Lyle",
                "family": "McDonald",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Jill Terry",
                "family": "Rudy",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University, United States of America",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "video",
            "audio",
            "English",
            "multimedia",
            "folklore and oral history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this paper we present an intuitive and interactive visualisation of comparison results on Early New High German manuscript records supporting the scholar to determine clusters of similar records. Based on detailed comparison results generated by the framework LAKOMP, an easily accountable similarity graph is dynamically generated. By selecting portions of text and manuscript records of interest, the scholar can explore the commonalities and divergences by witnesses linked to the graph elements.  * * * Philologists studying the genetic origin of texts with multiple available records face the challenge of identifying genetically close records. To the end, for every two records the witnesses for similarities or divergence have to be listed. Done manually, a pair-wise comparison of records is a tedious task that would vastly benefit from automatic tool support. Here we present such tool support for Early New High German manuscripts. Searching for similarities and divergences between manuscripts of this stage of the German language is a particular challenge because of the lack of a common orthography. Actually, our tool is a semiautomatic multistage approach that leads to an interactive similarity graph that represents the similarities of records in an intuitive and transparent way. This graph gives an overall view of the relationships between the different records. Together with the possibility to ‘jump’ to the witnesses for similarities, it allows philologists to confirm or confute previously formulated hypotheses about the clustering of records. This similarity graph is based on witnesses for similarities of records identified by our framework LAKOMP (Medek et al., 2014; 2015) that is being developed in the interdisciplinary project Semi-automatic Difference Analysis of Complex Text Variants (SaDA), in which Germanists, Romanists, and computer scientists develop software tools for supporting philological text comparison projects. Existing tools for comparing multiple text records such as Juxta (Juxta, 2014) or collateX (Dekker and Middell, 2011) do offer visualisations for their comparison results also, but lack a good scalability for large amounts of texts and do not make use of annotation data such as lemmata, which is crucial for comparing Early New High German manuscripts.  We present our approach through the example of the 15th-century manuscript ‘Wundarznei’ by Heinrich von Pfalzpaint, a forefather of plastic surgery and one of the most famous surgeons in the late Middle Ages. While the original manuscript has been lost, 11 manuscript records written by different copyists in the 15th and 16th centuries are known. One of these records is presumed lost; the other 10 records are available to science and are now subject to a detailed comparison by Germanists at the Martin Luther University Halle-Wittenberg. Figure 1 shows a facsimile of a page of one of these records. Besides the detailed listing of differences among the records for corresponding text passages as critical apparatus, a goal of the text comparison is the grouping of records according to their similarities.     Figure 1. A facsimile of the manuscript Dresden 292 of the ‘Wundarznei’. Source: SLUB Dresden/Handschriftensammlung, Signature: [Mscr.Dresd.C292, fol. 151v]. Approach  A prerequisite for visualising similarities is a detailed comparison of the records. For an automatic comparison of the manuscripts of the ‘Wundarznei’, a particular challenge arises due to the lack of a common orthography in Early New High German. Because of very different spellings of the same word form even within a single record, an automatic identification of word forms is not possible. A founded identification de facto requires manual intervention of the philological researchers for nearly every word in the manuscript records. We meet this problem by using lemmata of the word forms instead of the word forms themselves for the comparison of the records. An appropriate semiautomatic tool for lemmatisation of Early New High German manuscripts is integrated in our framework LAKOMP  (Medek et al., 2014) . It offers the philologist an intuitive, user-friendly, and very fast interface to enter lemmata for all word forms of a text. 1 Currently, it is applied in multiple philological projects.   After the lemmatisation of the manuscripts, the comparison is done in two steps, covering two levels of detail. In the first step, corresponding text passages among the records are identified. For this purpose, the records are split into small segments, e.g., sentences or parts of sentences. These segments are aligned in a tabular manner such that corresponding segments of different records are placed next to each other. In this table, each column contains consecutive segments of a record identified by its siglum. Figure 2 shows an excerpt of an alignment resulting from the first comparison step computed in LAKOMP using an alignment algorithm that is based on fingerprints of the segments and was developed within our project. 2     Figure 2. Alignment of corresponding text passages. Here segmentation and alignment can be modified. Words marked in dark grey lack annotation data such as lemma or morphological data. A click on a word opens the annotation dialogue. In a second step, the corresponding segments are compared in detail on word level, resulting in a fine-grained alignment as shown in Figure 3. Here, each line contains a segment (as defined above) of a record. As a result, corresponding words of different records are placed in the same column. Every column in a word-level-based alignment in which two records have an entry is called a witness for the similarity of these two records. Counting the witnesses for each pair, the scholar obtains an objective measure to judge the genetic proximity of records. Pairs of records having a lot of witnesses are genetically closer to each other than pairs with only few witnesses. This information can be displayed graphically. An intuitive presentation is a similarity graph, in which each record is represented by a node. The similarities between two records are represented by an edge between their nodes. For each edge, a weight is determined by the witnesses, , e.g., by simply using the number of witnesses or by categorizing the witnesses and prioritising the numbers per category. Having annotation data such as part-of-speech tags, witnesses of word type noun can be weighted heavier than those of type conjunction. Highly weighted edges are drawn stronger and shorter than others, leading to a closer proximity of nodes. Figure 4 shows a similarity graph generated by LAKOMP applied to a large portion of the ‘Wundarznei’. As it is very crucial for scholars to understand the relation between a visualisation and the underlying data, LAKOMP allows interactive exploration and manipulation of the graph in a web browser. Current technology for dynamic web content by jQuery (jQuery, 2014) and visualisation by D3.js (D3, 2014) allow respectable and sophisticated renderings. Moving the mouse on an edge, the number of witnesses leading to this edge is displayed. By clicking on an edge, a list of all witnesses for this edge is presented, allowing the user to directly jump to the word-level-based alignments that contain the witnesses. Additionally, scholars can select the text passages and the records for which the similarity graph should be drawn.     Figure 3. Detailed comparison of records. Preliminary Results  In the case of the ‘Wundarznei’, Germanists are furthermore interested in finding groups of records, such that close records share a group. Until now, the existence of two groups has been assumed, group A containing the records with sigla H, Bu, D3, E, Br, and group B containing the records with sigla B8, B9, D2, P, St. However, the similarity graph shown in Figure 4, generated for a section of the text over all 10 available records, shows the existence of two clusters. Each cluster is a subset of one of the assumed groups, but the three records St, E, and P cannot be assigned to exactly one of these groups with certainty, either because of shared similarities to multiple groups or because of missing text in the considered section. The generated similarity graphs are fundamental for discussions of assumed groups, being an objective measure to support or reject assumptions by the clusters found.  Funding  This work was funded by the German Federal Ministry of Education and Research (BMBF) [grant number 01UG1247] as part of the project Semi-automatische Differenzanalyse von komplexen Textvarianten under the direction of Professor Dr. Thomas Bremer, Professor Dr. Paul Molitor, Dr. Jörg Ritter, and Professor Dr. Hans-Joachim Solms.   Figure 4. A similarity graph with edges linked to witnesses. Notes 1. LAKOMP also supports the annotation with part-of-speech tags and morphological data, which can also be used for comparing the records. 2. There are existing alignment algorithms used in statistical machine translation such as those implemented in tools like Giza++ (Liang et al., 2006) or Berkeley Aligner (Denero, 2007). These approaches compute alignments of two texts on the word level but require a huge training corpus providing pairs of corresponding sentences among the text variants. We did not use them because of their limitation to an alignment of two text variants at once, which leaves the problem of a generalization to more text variants unsolved. Another issue is their expensive training phase, which requires manually created training data for each pair of text variants. ",
        "article_title": "Interactive Similarity Analysis of Early New High German Text Variants",
        "authors": [
            {
                "given": "André",
                "family": "Medek",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Jörg",
                "family": "Ritter",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Molitor",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Sylwia",
                "family": "Kösser",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "german studies",
            "visualisation",
            "corpora and corpus activities",
            "programming",
            "text analysis",
            "encoding - theory and practice",
            "English",
            "linking and annotation",
            "software design and development",
            "lexicography"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper offers a theoretical proposal for understanding the relation between design and the digital humanities with a focus on resource consumption. It proceeds from the position presented by Burdick et al. (2012) that the digital humanities move design to the centre of the research questions it poses. From this central position, design enables an unprecedented reach and capacity for the generation and management of information and the creation of new forms of knowledge. In this short paper I argue that the designed platforms sustaining the digital humanities are not simply enabling; they are agential in the development of new ways of encountering and knowing the world.  The digital humanities depend on design and its objects, and yet in its tool-like practicality and normative intent, design so often evades scrutiny.  Design disappears. As Clive Dilnot (1998, 22) has remarked, ‘If something cannot be made a matter of speech then it tends to disappear as such, i.e., it does not enter discourse as such and therefore remains invisible. Hence of course, one of design’s ironic conditions: that it is everywhere seen and everywhere remains invisible’.   This paper will critically attend to three territories of design that tend to withdraw as objects of critical attention within the digital humanities:  1. Product-systems shaping digital engagement and participation.  2. Material infrastructures sustaining digital data.  3. Image ecologies encoding and communicating digital information. Taking each of these design territories in turn:  Digital  product-systems have very quickly materialised what David Michael Levin presciently called the ‘frontal ontology’ of our age, an interpretation substantiated by the prevalence of mobile screen-based engagement as a new bodily posture, a rise in immediate encounters with worldviews designed by others and attached to these new social practices that extend between online and material environments. I will consider how new ontologies are shaped and activated by digital environments, particularly the proposition of Fry (2005) that ‘our mental ingenuity at finding ever more novel ways to relieve physical effort far outstrips the slower evolutionary time of our physicality’.  Vast, energy-intensive  infrastructures sustain, store, and distribute data at an exponential rate and unimaginable scale. While innovation drives patterns of obsolescence in product-systems, data seems interminable. We lose touch with the material substrate as digital information has a powerful symbolic vitality that manifests independently of how it is being resourced and reproduced. I will consider how big energy consumers such as Google are reporting their resource consumption and what their analytical stories reveal about how ecologies are being understood, interpreted, and communicated.  As rhetorical devices at the forefront of digital systems,  images can be conceptualised as energetic and cognitive materials that are reshaping how we see and understand our living environments. As designed systems become more complicated, big and live consumption data are packaged to promote more ecologically literate and aware ways of being in the world. Yet we might ask to what extent can new visualities of ecology generate the sensory intuitions we need to live more sustainably? There is an anxiety around visualising data without there also being a strong sense of how visualisation shapes both cognition and action. With reference to key living experiments and projects, I will consider how participatory forms of data collection and reporting of resource consumption might inform more reflexive forms of ecological data visualization.   All three territories warrant being made discursive in the context of the digital humanities as they have, to paraphrase Burdick et al., profound implications for what it means to be human in the networked information age. The question of what it means to be human is, of course, a fundamental question for the digital humanities. My claim is that any response to this question must now also interrogate the extensively and intensively designed nature of the worlds in which we live, dream, and build knowledge. In order to perform this interrogation, I mobilise the emerging theory of ontological design. Ontological design is a theory that characterises the nature and agency of design as a fundamental human activity shaping the condition or behaviour of ‘what is’ (Willis, 2006, 81). It is also a method of relational critical analysis. My critique will therefore demonstrate how the unique areas of infrastructure, software, and image design are relationally implicated. The theory of ontological design is based on the hermeneutic phenomenology of Martin Heidegger and Hans-Georg Gadamer, with more explicit design interpretations in the work of Terry Winograd and Fernando Flores, Anne-Marie Willis, as well as Tony Fry and Arturo Escobar. Ontological design understands that ‘our very “being-in-the-world” is shaped by knowledge we pursue, uncover, and embody’ (Fry, 2012, 43).  By bringing a consideration of the ontological implications of digitisation within the remit of the digital humanities, an ongoing critical discourse regarding design’s making and unmaking of material and symbolic worlds is invited.  ",
        "article_title": "The Ontological Designing of the Digital",
        "authors": [
            {
                "given": "Abby Arwen",
                "family": "Mellick Lopes",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "philosophy",
            "user studies / user needs",
            "interdisciplinary collaboration",
            "interface and user experience design",
            "digitisation - theory and practice",
            "digital humanities - nature and significance",
            "English",
            "maps and mapping",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Automated comparison of narrative structure facilitates narrative stylometry, cross-document co-reference, and the alignment of events in the retellings of stories from different perspectives. With attention to the second and third outcomes, we introduce a method for the unsupervised population and comparison of graph representations of narrative texts. Populated with extracted story elements drawn from documents, graph substructure similarity analysis techniques are used to measure the similarity of narrative order across stories and for the similarity of character function across those stories. Designed to scale, our first application of this method is against a test corpus of 10 variations of the Aarne-Thompson type 333 story, ‘Little Red Riding Hood’. Preliminary experiments correctly coreferenced differently named entities from story variations and indicated the relative similarity of events in different iterations of the tale despite their appearing in differing order. Next steps include heterogeneous corpora, non-fiction corpora, and more precise comparison. * * * Navigating graphs of sentence-level grammars has been an achieved goal of natural language processing (NLP), at least in English, since the development of the Penn Treebank and the success of statistical parsers in the mid-1990s (Magerman, 1995). Adapting sentence-level approaches to the problem of higher-level analysis such as cross-document correlations of narrative remains an open challenge. The goal of this research is to advance our prior work in narrative information extraction (Miller et al., 2013) and visualization (Shrestha et al., 2013) for narrative similarity assessment, event alignment, and cross-document coreference. Analytical problems of core interest to humanities scholars are not bound to sentential units. They focus on semantics, representation, ideology, influence, pragmatics, narrative—problematics that frequently bridge, fracture, and co-referentially scatter throughout documents and corpora. Discourse analysis (Jørgensen and Phillips, 2002) and textTiling (Hearst, 1997) are two methods used to circumvent sentential boundaries by segmenting documents into logical blocks. As with latent semantic analysis (Dumais, 2004), these blocks can then be characterized in ways that facilitate comparison, extending sentential navigation to macro-level structures. Our position builds on tree-based textual analysis to argue that comparison of the graph representations of narrative documents using techniques from graph theory advances both NLP and humanistic inquiry. This work scales as extraction, lookup, and comparison operate unsupervised. Identifying events described in a document, producing a graph of actors and locations implicated in those events, and comparing the underlying matrices facilitate various types of cross-document narrative analysis. Graph similarity measures provide our primary comparison method to highlight similarities, as exemplified in Figure1. Additionally, condensing narrative texts in a manner consistent with narratological theory enables testing of narratological schema. Formal analysis of narrative has been a relatively successful aim of many.    Figure 1. Side-by-side comparison of two simplified iterations of the ‘Little Red Riding Hood’ fairy tale. Linked location, time, and agent units are shown, with red edges indicating change between tellings. Structuralist topologies developed by the Russian formalists such as Propp (1968), and later work by Genette (Genette and Lewin, 1983), Bal (1997), and others, yielded many complementary top-down models for organizing narrative. These schema distinguish between fabula and discourse, or events to be retold and the manner in which retelling takes place. Narrative, broadly defined, is the ordering of fabula by discourse. Discourse order is the relationship between the temporality of events and their representation as part of a narrative (Genette and Lewin, 1983). This structural perspective serves humanists well when analyzing single narratives or small corpora. Computational models developed from formalist approaches have been the subject of compelling experiments. Like work by Finlayson (2012) on analogical story merging and Fisseni on story comparison (Fisseni and Löwe, 2014), our work presents a bottom-up method to help test formalist top-down narratological schema. Unlike theirs, our work implements unsupervised comparison of narrative graphs, a proxy of narrative structure, across a corpus. Narrative graphs facilitate (1) stylometric comparison, (2) clustering of documents according to their event-relationship characteristics, (3) cross-document co-reference, and (4) alignment of event descriptions across narratives. To test three and four, we implemented a method as described below. Method Computation of similarity between the substructure of the graphs required extraction of entities, creation of the graph, and finally comparison.    lrrh wolf grandmother woodcutters forest gm house   lrrh 1 0 0 1 1 0   wolf 0 1 0 0 1 0   grandmother 0 0 1 0 0 0   woodcutter 1 0 0 0 1 0   forest 1 1 0 1 0 0   gm house 0 0 0 0 0 1   Table 1. Adjacency matrix created from one version of ‘Little Red Riding Hood’. An edge in the graph or 1 (in the adjacency matrix) between two entities signifies that these entities interacted within the given set of events. 1.  Extraction. Events were automatically marked in the narratives using the NLP tool, EVITA (Saurí et al., 2005). EVITA increments event tag ID numbers; that sequence proxies discourse order. Sequential narrative discourse order generated from EVITA was used since fiction frequently lacks dates and timestamps—necessary features for temporal taggers like SUTime and GUTime. Entity extraction and classification were accomplished using the Stanford Named Entity Recognizer (NER). Each graph includes elements corresponding to event name, event type, actor, location, and discourse order. Extracted events from a given story contain the following information: document  ID, event  verb, event  verblemma, event  verbhypergram, event  type, entity 1, entity  n, entity 1 type, entity  n type, event  number.  2.  Graph creation. Based on changes in topic, the set of events  E was divided into  k parts. A sub-graph  G was created for each event set,  e 0  < e x  < e k and was represented as an adjacency matrix as shown in Table 1. An adjacency matrix is a common way of internally representing graphs for computation such that an element  a ij in the matrix equals 1 if there is an edge between  i th and  j th nodes in the graph and 0 otherwise. In our case, the adjacency matrix represents the entity::entity relation as mentioned in the narrative for the set of events  e x.  3.  Similarity analysis. Many domain-specific algorithms to compute similarity have been developed. Most are based on neighborhood analysis. In this paper, we propose our own similarity analysis method inspired by the work of Blondel et al. (2004).  We begin by modeling each narrative as a 3 D matrix where the first two dimensions represent the adjacency matrix and the third dimension represents the discourse order of the sets of events. Following this model, given two matrices,  A i,j,k and  B p,q,r , for comparison, we construct a 3 D matrix,  X, of dimensions  p× q × φ where  φ is the smallest of  k and  r, initialized to all 1s. We then proportionally shrink the larger timeline to match the smaller one so that  k =  r.  For each  φ, as in the HIT-inspired algorithm (Fackler, 2005) proposed by Kollias et al., we compute   X ←  BXA T +  B T  XA (1)  and normalize  X after each iteration. The even iterations converge to a final similarity matrix. To simplify and speed this process, we use the Kronecker product and the vec(.) operator. This results in   x ← ( A ⊗ B +  A T  ⊗ B T ) x (2)  where  x =  vec( X). These sets of equations give a similarity score frame per scene, which is then aggregated to produce a final similarity score.  Discussion For the purposes of testing our methodology, we used 10 of the 58 known iterations of the Aarne-Thompson type 333 story (ATU333), ‘Little Red Riding Hood’ (Lang, 1891; Grimm and Grimm, 1812; Wratislaw, 1889; Schneller, 1867; Ashliman, 2007; Marelles, 1895; Potter, 1908; Bates, 1883; Thurber, 1983; Carter, 1993; Tehrani, 2013). The purpose of using this corpus was to strengthen the possibility of narrative overlap and focus the method on fine-grain distinctions between re-tellings. The primary purpose of the current work is to test our method’s detection of event and character similarities and differences across a given corpus; however, we do not anticipate that our method is dependent on such homogeneous data. By utilizing categories of variables associated with elements of the fabulaic and discourse levels of narrative, we anticipate our method will function similarly with larger, more heterogeneous corpora. Via this method, 1,384 events were extracted across 10 story iterations. Numbering 8,450 tokens, including titles and authorship information, the overall density of extracted events to tokens is high. Contrasted to event detection methods reliant on temporal expressions (SUTime identified two events), this density of event detection provides a good basis on which to compare narrative structure. Generalizing event types from specific verb tokens to hypergrams of those verbs (e.g., event 41 from Carter, 1993): ‘armed’ lemmatized to ‘arm’ hypergrammed to ‘supply’) generally retains the function of each event within the story, but abstracts sufficiently to allow for variation such as is introduced by translation to not prevent the recognition of correspondences. The automatically produced matrices for this work are exemplified by Table 2.   undergo,EVENT104 grandmother wolf lrrh   bed 1 1 1   perceive,EVENT105 grandmother wolf lrrh   bed 1 1 1   undergo,EVENT106 grandmother wolf lrrh   bed 1 1 1   perceive,EVENT107 grandmother wolf lrrh   bed 1 1 1   undergo,EVENT108 grandmother wolf lrrh   bed 1 1 1   seize,EVENT109 grandmother wolf lrrh   bed 1 1 1   undergo,EVENT110 grandmother wolf lrrh   bed 1 1 1   consume,EVENT111 grandmother wolf lrrh   bed 1 1 1   Table 2. Eight matrix layers from 3 D stack of event matrices.  Table 2 shows eight layers from the 3 D event matrix stack. It, and the following list, a one-line extract, highlight the generalized form and granular detail of the events this method extracts: 10; prospered; prosper; change state; OCCURRENCE; child, grandmother’s house; person, location; 45. The stack and list correspond to the ‘Oh, grandmother, what big ears you have!’ to ‘[a]nd with that he jumped out of bed, jumped on top of poor Little Red Cap, and ate her up’ sequence from Lang (1891).  Results of functions (1) and (2) on the adjacency matrices are exemplified below in Table 3. Column headings correspond to entities from Grimm and Grimm (1812) for event 3, and row headers correspond to entities from Lang (1891) for event 4. Table 3 shows that the measure of similarity between Little Red Riding Hood (‘lrrh’) and Little Red Cap (‘lrc’), is 0 .32. Although low, that score was calculated only based on entity-entity connections, the sequence of those connections, and event hypergrams. These preliminary results are encouraging, as the correlation between those characters is high relative to all correlations found. More importantly, as shown by the diagonal symmetry of the measures, the correlation between events is high relative to correlations between that event and others within the stories. Overall correlation between event 3 from one variation and event 4 from another variation suggests that this method can align scenes across variations of the same ATU. Table 4 shows the potential for this method to align characters from different versions based upon their position within the story.     lrc wolf grandmother huntsman home woods gm house   lrrh .32 .25 0 .25 0 .32 0   wolf .32 .25 0 .25 0 .32 0   grandmother 0 0 0 0 0 0 0   woodcutters 0 0 0 0 0 0 0   home 0 0 0 0 0 0 0   forest .32 .25 0 .25 0 .32 0   old woman’s house 0 0 0 0 0 0 0   Table 3. Character similarity across ‘Little Red Riding Hood’ and ‘Rothkppchen’.    lrrh wolf gm wc home forest owh   lrc .67 .76 .31 .14 .14 .48 .37   wolf .79 .94 .42 .14 .14 .56 .5   gm .35 .47 .31 0 0 .16 .37   hunts .23 .28 .18 0 0 0 .26   home 0 0 0 0 0 0 0   woods .48 .53 .16 .14 .14 .48 .16   gmh .39 .52 .34 0 0 .16 .42   Table 4. Character alignment across all events for iterations Lang (1891) and Grimm and Grimm (1812) of ATU 333. Table 4 sums all event matrices for two variations of the tale. Version 1 occupies the columns (Little Red Riding Hood, Wolf, Grandmother, Woodcutters, Home, Forest, and Old Woman’s House) and version 2 the rows (Little Red Cap, Wolf, Grandmother, Huntsman, Home, Woods, Grandmother’s House). Name-independent character alignment is demonstrated by the 0 .96 correspondence between the two wolves. Interestingly, the event matrix suggests that certain characters function dissimilarly between variations: most notably, Grandmother. The corresponding value between the Grandmother characters is only 0 .31, suggesting that they share some event associations but not as many as are held by other cross-document pairings. That assessment is accurate as, in version 1, the story concludes upon the wolf’s consumption of both Little Red Riding Hood and Grandmother. In story version 2, both survive to boil a second hungry wolf.  Conclusion and Further Work This preliminary work resulted in a viable method for automated narrative stylometrics across iterations of stories, for narrative alignment, and for the cross-document coreference of characters bearing different names but similar story functions. The extraction and matrix comparison methods are implemented and tested. Our next stage of this research is to refine the comparison algorithm, apply it to a corpus of dissimilar narratives, explore the fit of structuralist models of narrative to non-fiction, and assess the method’s ability to cluster stories by narrative similarity. Acknowledgements This work is supported in part by NSF award 1209172. ",
        "article_title": "Automated Comparison of Narrative and Character Function Similarity Using Graph Theory",
        "authors": [
            {
                "given": "Ben",
                "family": "Miller",
                "affiliation": [
                    {
                        "original_name": "Georgia State University, United States of America",
                        "normalized_name": "Georgia State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03qt6ba18",
                            "GRID": "grid.256304.6"
                        }
                    }
                ]
            },
            {
                "given": "Ayush",
                "family": "Shrestha",
                "affiliation": [
                    {
                        "original_name": "Georgia State University, United States of America",
                        "normalized_name": "Georgia State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03qt6ba18",
                            "GRID": "grid.256304.6"
                        }
                    }
                ]
            },
            {
                "given": "Jennifer",
                "family": "Olive",
                "affiliation": [
                    {
                        "original_name": "Georgia State University, United States of America",
                        "normalized_name": "Georgia State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03qt6ba18",
                            "GRID": "grid.256304.6"
                        }
                    }
                ]
            },
            {
                "given": "Shakthidhar",
                "family": "Gopavaram",
                "affiliation": [
                    {
                        "original_name": "Georgia State University, United States of America",
                        "normalized_name": "Georgia State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03qt6ba18",
                            "GRID": "grid.256304.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "linguistics",
            "natural language processing",
            "literary studies",
            "information retrieval",
            "text analysis",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Japanese publisher East Press published a manga edition of Karl Marx’s multivolume  Das Kapital in 2007, and in that same year released 507,000 copies. Several scholars have written on (and created) the practice of transposing difficult philosophy or classic literature into graphic novels and comic books. However, what happens when the manga themselves are transposed into a more interactive art form?   Visual novels are interactive fiction games that incorporate game play and are usually centered on dialogue, non-linear narratives, and multiple perspectives. In this presentation I immerse the audience in  Kapital using Ren’Py, a visual novel engine based on simplified Python scripting. (Please see the game’s information site here: http://campuspress.yale.edu/marxifg/.)  Session Audience Teaching and learning professionals / faculty / instructional designers, frontline practitioners. Session Description The focus of this presentation is to immerse our audience in game play of  Kapital. I anticipate that audience participation will serve as experiential learning for the visual novel as a serious game, and will therefore lead to a robust discussion about the merits and pitfalls of this learning approach. For learners of  Kapital, I am concerned that long philosophical documents that hold importance for current cultural issues are, nevertheless, inaccessible to the majority of students because they are dense, weighty, and of astronomical length. I endeavor through gameplay to have players embodied in the philosophy, to learn theory through doing, and to react in ways that will reveal that they are able to make appropriate decisions using their new knowledge base.   Value of Serious Games and Interactive Fiction for Higher Education  1. This is a method for promoting literacy of and exposure to inaccessible philosophical texts whose ideologies are vogue in culture and criticism.   2. The form of the visual novel is gaining popularity and will find some use value in education (is it serious game, edutainment, or literary?).   3. The issues of digital learning—to code in Python, techniques for storyboarding, and translation issues—are paramount. Increasingly, IT will need to develop these types of games, offer workshops to support student and faculty learning of programming, and mentor groups in project management and theories of game design.  Learning Outcomes  1. Understand and apply critical concepts from  Kapital.   2. Engage in and comprehend the structure and form of the visual novel.  3. Identify the process for designing an IF game.  4. Envision use value for the genre in learning environments. Of interest to the presentation is the unique collaborative background for this project: The presenter is partnering as a staff-faculty member with undergraduate student Alex Lew to create  Kapital. They have successfully presented the game design, history, and practice in two conferences, including the inaugural presentation of Yale’s Performance Studies Working (fall 2014), and in the first session of the INKE (Implementing New Knowledge Environments) Conference (Chicago, 2014). They have also submitted this proposal to Educause 2015 for joint presentation of the interactive demo.   ",
        "article_title": "Kapital: An Interactive Fiction Game",
        "authors": [
            {
                "given": "Dana",
                "family": "Milstein",
                "affiliation": [
                    {
                        "original_name": "Yale University, United States of America",
                        "normalized_name": "Yale University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03v76x132",
                            "GRID": "grid.47100.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "german studies",
            "cultural studies",
            "interdisciplinary collaboration",
            "virtual and augmented reality",
            "video",
            "programming",
            "audio",
            "creative and performing arts",
            "English",
            "multimedia",
            "games and meaningful play",
            "knowledge representation",
            "digital humanities - pedagogy and curriculum",
            "including writing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Both the challenge and the pleasure of poetry analysis originate in nocuous and polysemous ambiguity: Different readers interpret linguistic expressions in a poem based on different contexts. The layers of ambiguity grow exponentially when a reader applies semiotic analysis to a poem to examine linguistic expressions for modalities, paradigms, syntagmatic structures, rhetorical tropes, intertexual elements, and semiotic codes. Poetry scholars might argue, in fact, that to eliminate nocuous and polysemous ambiguity would diminish the poem and defeat the purpose of poetic interpretation. To date, ambiguity analysis generally manifests itself in domains like requirements gathering documents, where ambiguity can lead to misinterpretation, lost money and time, and poor or stunted project outcomes. Approaches that attempt a probabilistic prediction of ambiguity in such documents can be useful for indicating ambiguity when writing these texts, so that authors can be reminded to clarify their writing where it  might be necessary. In poetry analysis, however, it may be less useful for interpretation purposes to probabilistically predict which expressions are ambiguous or not. Orthography petrifies meaning in poetry, some argue; nevertheless, a poem may be infused with multiple meanings by virtue of ambiguities created through wordplay and rhetorical devices that can be discerned only through close reading and puzzle solving to decode the linguistic expressions and structures.  We are curious to discover if reducing ambiguity will limit poetic analysis and what new types of questions may emerge as a result of disambiguation. To that effect, in this presentation we aim to present three facets of what we hope will become a large project on quantifying ambiguity:  a.  Method as Applied to Research in the Humanities: This algorithmic tool will be further refined to provide a mechanism to quantify the ambiguity of a given text by analyzing its lexical and syntactic ambiguity to produce a number that represents the overall ambiguity of the text. As well as gamifying the process of communicating unambiguously (by enabling the dynamic scoring of the author’s progress as she writes), the tool enables the ambiguity of diverse texts to be objectively compared and opens new avenues for text comparison and analysis. Furthermore, the tool should identify ambiguity  within a text by quantifying the ambiguity of individual words and phrases inside the text. An immediate application of this will be to produce an extension that will highlight terms or phrases in a text in colors representing their ambiguity, providing an ‘ambiguity map,’ the equivalent of a ‘heat map’ (see Chen and Wang, 2012) for ambiguity within texts:  Figure 1. Sample output of the tool in current prototype.     b.  Critical Assessment of the Application: Application of the tool to William Blake’s ‘The Sick Rose’. Initially, we set our ambitions too high—to apply the tool to what many consider the most difficult poem in the world due to its high level of ambiguity, Mallarmé’s ‘Coup de dés’ (CDD). Obstacles arose because of the structure and variants in the Littré Dictionary (which Mallarmé used to write the poem), which made it difficult to apply the algorithm in its current form. CDD’s length and visual formatting also render it too difficult to parse manually in order to compare it to the algorithm’s output. Therefore, we decided to work with William Blake’s ‘The Sick Rose’, for its brevity, morphological simplicity, and its ambiguity (scholars refer to it as one of Blake’s ‘gnomic triumphs’ [Bloom, 1987]). We anticipate that this 34-word poem will be more manageable both for the algorithm to analyze, so that we can manually code the poem and compare our effort to the algorithmic output, and to build a proof-of-concept multimodal dictionary with word processor.    c.  Impact in Formulating and Addressing Research Questions: An outcome of our analysis is the realization that dictionaries (whether those used by the poet or current dictionaries used by readers of poetry) provide definitions that may themselves be ambiguous, primarily due to circularity (for example, the definition of a word in a dictionary may contain a word, the definition for which contains the original word), but also due to the general effect of nocuous or polysemic ambiguity in the dictionaries themselves. Additionally, by hand coding one poem we will show that a computer-aided coder (e.g., a special word processor) would greatly speed up the task of specifying the semantics of a text. Therefore, we intend to build and showcase a multimodal dictionary for the poem, which we will discuss in conjunction with a proposed, longer-term project to build a type of word processor that enables authors to specify the meanings of words in-line while writing a poem or to mark up a poem post-hoc.   As dictionaries often provide multiple meanings for a single word, the proposed new dictionary will provide identifiers for concepts as well as linking terms to those concepts (in much the same way that WordNet does [see Miller, 1995]), enabling authors to use the word processor to specify the concept they are referring to when they use a term or phrase (rather than simply including a multi-meaning term in their text), and minimizing the ambiguity of their writing. The dictionary will require all concepts to be defined using data (images, audio, video, other sensor data), semantic primitives / semantic primes, or some combination of these, thus ensuring that the definitions in the dictionary are minimally ambiguous (Wierzbicka, 1972). In order to eventually enable  semantic ambiguity to be quantified, the dictionary will also not limit the referrers that may refer to concepts to only terms or phrases but will allow any text to link to a concept. This effort aligns with current ambiguity analysis theory, where authors state that ambiguity can be avoided by using controlled languages (Fuchs and Schwitter, 1996; Gause and Weinberg, 2011), style guides (Kovitz, 1998; Ryser and Glinz, 2000), and lexicons (Chantree et al., 2006) In this section, we also address concerns that creating a lexicon of domain-specific terminology, even for a 34-word poem, will be a sizeable task. If time permits, we hope to solicit feedback from the audience on the proposed word processor, which would enable authors to create texts with minimal ambiguity by enabling them to specify the meanings of words and phrases as they type (in much the same way as predictive text software works). The word processor would also enable deterministic semantic analysis (in contrast to the mainstream approaches to semantic analysis, which are, for the most part, probabilistic [see Hoffman, 2001]) by capturing a machine-readable version of the meaning of the text being written.   ",
        "article_title": "Quantifying Ambiguity by Gamifying the Writing Process: A Case Study on William Blake’s “The Sick Rose”",
        "authors": [
            {
                "given": "Dana",
                "family": "Milstein",
                "affiliation": [
                    {
                        "original_name": "Yale University, United States of America",
                        "normalized_name": "Yale University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03v76x132",
                            "GRID": "grid.47100.32"
                        }
                    }
                ]
            },
            {
                "given": "Euan",
                "family": "Cochrane",
                "affiliation": [
                    {
                        "original_name": "Yale University, United States of America",
                        "normalized_name": "Yale University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03v76x132",
                            "GRID": "grid.47100.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "drama",
            "content analysis",
            "natural language processing",
            "bibliographic methods / textual studies",
            "video",
            "information retrieval",
            "audio",
            "English",
            "multimedia",
            "software design and development",
            "knowledge representation",
            "lexicography",
            "poetry",
            "genre-specific studies: prose"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper explores a phenomenon that we have both noticed in recent years: that digital humanities programs and progress are driven by the presence of luminaries. A DH luminary is someone who identifies clearly as a digital humanist and is known in the international DH community; they are the people who keynote at DH, DHSI, and similar conferences. The presence of a luminary leads to significant success in attracting grant funding, graduate students, speaking engagements, etc. We are not the only ones who have noticed this phenomenon. In his 2011 blog entry ‘The (DH) Stars Come Out in LA’, Matt Kirschenbaum calls for a more direct engagement by DHers with the effects of celebrity in the field. In discussing luminary figures, it is not our intention to criticize them or in any way to diminish the hard work that has contributed to their status. Instead, we want to discuss the challenges that luminaries present for developing a resilient DH culture that supports training and individual member projects, and which will persist even if the luminary departs. While acquiring a luminary can go a long ways towards ‘making DH happen’ at a school, it can also engender concerns for future sustainability. In this paper, we address a few of the specific trade-offs/challenges that luminaries present, and identify actions that schools wanting to develop a DH initiative could take. By DH initiative, we mean the alchemistic mix of a DH community, program (i.e., formalized instruction, whether certificate or degree), physical centres, and DH-focused research entities that taken in sum represent a given institution’s DH culture. A DH initiative is intended to be a sustained and ongoing investment, rather than the temporary program of a pre-determined span of time. Besides funding and publicity, luminaries contribute direction to a DH program, orienting its content towards a particular area: making, text analysis, software development, pedagogy, etc. Another vital (though intangible) ingredient that luminaries bring is the insurance of having succeeded previously. A luminary’s presence can serve as justification for a new and relatively untested practice. Both of these qualities are especially important in light of the lack of general DH knowledge distributed throughout humanities departments. Arguably, then, the presence of a luminary provides a DH program with a specific identity. Luminaries help to ensure success of a particular kind, easily recognisable in traditional departments, even when the medium of scholarship is different: a project is released, generating responses within the disciplinary community. Without a luminary, a new DH centre will lack the insurance of previous successes as well as the established identity/direction. The work of achieving both components will then fall on the centre’s faculty, staff, and early participating graduate students. This creates an uphill slope for those involved: a substantial part of their work will involve trial-and-error experimentation, and their progress will almost certainly depend on their finding local allies, and their ability to connect with larger DH social networks either in person or via social media. This type of work is undeniably valuable, and applicable to many contexts, within and outside of academia. However, the work that is about producing infrastructure and culture, rather than scholarship, gains the most plaudits—in short, the work that leads to luminary status.  A luminary at the helm is an attractive prospect for any university that wants to make a substantial investment in the digital humanities. Moreover, universities love quick victories that give rise to press releases, photo ops, and instant notoriety. In light of the continuing lack of funding for higher education, and for humanities initiatives in general, acquiring a luminary figure to guide development may seem like the best strategy for ensuring that a new DH initiative will not fail. However, a sustainable DH community does not necessarily follow from a quick victory. More often, such a community is a slow-burn development involving a wide range of people and the formation of collaborative relationships that contribute to student training and career growth for all involved. The advantages that come with luminaries can be accompanied by challenges, specifically:  • The leadership of a luminary figure can generate energetic activity in an initiative—but this activity may reflect the luminary’s charisma rather than the growth of a widespread DH culture within the community.  • Luminaries, while powerful, are not guarantors of continuity: contingency plans are needed for sabbaticals or departures.  • While DH ostensibly transcends disciplinary boundaries, when programs are built around luminaries their disciplinary focus may be overemphasized, excluding other disciplines.  • Luminaries’ successes can become normative, creating challenges for those who want to develop new initiatives. Instead of looking at a particular community’s strengths and needs, the goal becomes reproducing what was achieved with a previous initiative or at another university. The result is that the culture of DH becomes more narrow, rather than becoming more varied and diverse. These challenges can be summed up by saying that luminaries seem to provide instant infrastructure, when in reality, their contribution is superstructure. As we consider the future of digital humanities on a global scale, the question we focus on is how to make a DH space and/or community accessible that allows all interested graduate students, staff, and faculty to be fully involved in shaping the collective identity of their campus DH culture, so that leadership is shared and can easily pass from person to person, with no single individual responsible for carrying the community. Such a community may have luminaries in it—but the sustainability of the program does not depend on their presence.  DH initiatives and programs often face scalability and sustainability challenges. Even the brightest of luminaries is only one person and can mentor only a limited number of students. Currently, luminaries are especially valuable as evangelists for DH, drawing people in, but eventually the number of eager community members will outpace the luminary. How will universities address the needs that arise when this moment arrives? We will close by presenting specific strategies for broadening and diversifying the development of DH initiatives, and for making the development of DH culture the collective work of communities, rather than of a single individual. ",
        "article_title": "The Question of the Luminary: Building a Resilient Campus DH Culture",
        "authors": [
            {
                "given": "Paige Courtney",
                "family": "Morgan",
                "affiliation": [
                    {
                        "original_name": "McMaster University, Canada",
                        "normalized_name": "McMaster University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02fa3aq29",
                            "GRID": "grid.25073.33"
                        }
                    }
                ]
            },
            {
                "given": "Dale",
                "family": "Askey",
                "affiliation": [
                    {
                        "original_name": "McMaster University, Canada",
                        "normalized_name": "McMaster University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02fa3aq29",
                            "GRID": "grid.25073.33"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "digital humanities - institutional support",
            "interdisciplinary collaboration",
            "project design",
            "English",
            "digital humanities - facilities",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper is concerned with comparative analysis of the Synoptic Gospels, or, more specifically, the  synoptic problem. In order to examine the synoptic problem scientifically, average synonymous word groups were first extracted based on the interrelationship between words in the Greek New Testament and various translations thereof. In the next step, those synonymous word groups were used as parameters for factor analysis. Thirty-four factors were identified, most of which pertain to theological concepts. Factor scores were then used to identify the major themes of each Synoptic Gospel. The major theme of Matthew was identified as ‘justice’; of Mark, ‘miracles’ and ‘salvation’; and of Luke, ‘missionaries’ and ‘what disciples should do’.  * * * The so-called  synoptic problem pertains to the complexity involved in editing the first three New Testament Gospels (the Synoptic Gospels of Matthew, Mark, and Luke), largely due to the similarity in content between them. The similarities and differences between these texts have been analyzed extensively in the field of Bible studies using traditional humanities’ methodologies, though descriptive statistics were utilized in some cases (Rosché, 1960). In such studies, the synoptic problem has been mainly divided into two themes: the first regards how the texts are edited, and the second regards the differences between the texts. In other words, there exists both diachronic (process of editing) and synchronic (differences in the texts) research into the synoptic problem in the field of Bible studies.  However, with the rapid development of information technologies, it has become easier to quantitatively analyze texts, and in the digital humanities field, the Bible has become one of the major research targets. Regarding the synoptic problem, for example, Miyake et al. (2005) were able to automatically extract similar parts of the Synoptic Gospels and created Natural Language Processing–based synoptic tables. In another project, Murai (2007) quantitatively extracted background theological focus points about the editorial process based on relationships between similar text parts. Until now, however, such quantitative research into the synoptic problem has largely focused on the diachronic problem.   Research Aim  In light of this research gap, it is necessary to analyze the synoptic problem synchronically using quantitative methodologies. Although superficial differences between the texts, such as word or phrase frequency, have been analyzed manually since the 19th century, no quantitative analysis has been conducted into the differences between theological concepts. One of the major difficulties in analyzing theological concepts is the diversity of expressions and words used in the original Greek New Testament. The same theological concept is expressed using many different words. Therefore, the relationship between synonymous words should be identified in order to conduct intertextual analysis of theological concepts. Moreover, because several theological concepts are mixed in each pericope (small story) in the Bible, it is necessary to use some methodology to separate out these concepts. For the reasons outlined above, theological concepts cannot be extracted directly simply by counting the words. Therefore, in this research, the first step is to develop synonymous word groups and then to develop highly related word groups in the Synoptic Gospels by factor analysis of frequently appearing words. The resultant groups of highly related words would include theological concepts that are composed of co-occurring words. Method and Results  Identifying Synonymous Word Groups  The nuance of word meaning in the Bible has been frequently contended in the field of Bible studies. There are numerous hypotheses and interpretations regarding the meanings of the original Greek words, so it is difficult to propose a neutral thesaurus for the biblical Greek. Therefore, in this research, an average of several interpretations was used to extract synonymous word groups.  The average interpretation of word relationships between the original Greek and subsequent translations was extracted. In order to identify the relationship between the original text and translations, the Asymptotic Correspondence Vocabulary Presumption Method (ACVPM) was used (see Murai, 2010). This method enabled the extraction of multiple word relationships for each word in order to form a bipartite network of original and translational words (see left network of Figure 1). Based on this network, two original words are connected when they share a corresponding translational word; in this way, a network of original words can be created (see right network of Figure 1) that signifies the relationships between original Greek words (Murai, 2013). Moreover, this method enables to compare characteristics of translational languages and background ideologies (Murai, 2012). A separate network of original words can be constructed from each translation of the Bible. If many different networks of original words have a common edge between some two original words, it suggests that the two words are very similar in meaning. Therefore, by combining networks from several translations, and by extracting strong edges, average synonymous word groups are extracted as partial connected graphs of that network (Figure 2). In this research, four English translations (the New Jerusalem Bible, the New King James Version, the New Revised Standard Version, and the New American Bible) and four Japanese translations (the Colloquial Japanese, the New Japanese, the Franciscan Japanese, and the New Interconfessional Translation) of the New Testament were analyzed and edges were extracted when more than four translations had that edge in common. Using this method, 224 similar word groups were obtained.    Figure 1. An example of extracting a network of original Greek words from a bipartite network of original and translational words.    Figure 2. An example of creating combined networks and extracting stronger edges.  Factor Analysis of the Synoptic Gospel  In previous research, several methods have been used to extract concepts from texts—for example, LSA (Latent Semantic Analysis) or the Topic Model (Hoffman, 1999). Because LSA is used to analyze specific words, it is not suitable for whole text analysis. While the Topic Model enables the extraction of an arbitrary number of topics from target texts with a high degree of accuracy, it does not indicate the number of essential topics. Therefore, in this research, factor analysis, which allows indication of the number of essential topics, was used to extract factors composed of synonymous word groups from the Synoptic Gospels.  For this stage of the research, pericopes were adopted as the units of analysis. The Gospels of Matthew and Luke each contain 145 pericopes, while the Gospel of Mark contains 81 pericopes. From these, 175 synonymous word groups were selected, comprising those that appeared in more than 10% of all pericopes. The parameters used for factor analysis were the frequencies of those synonymous word groups in each pericope. The rotation method used was promax. According to the results of Parallel Analysis, 34 factors were identified, and the cumulative contribution ratio of those 34 factors was 0.785. The names of obtained factors and synonymous word groups with greater than 0.5 factor loadings are shown in Table 1. In Table 1, synonymous word groups are represented by one of the included Greek words. The largest factors concern ‘the salvation of sinners’. These are followed by factors concerning ‘the beginning and the end’, ‘bread in the wilderness’, ‘journey to Jerusalem’, ‘hypocrisy of the Pharisees’, and ‘parables’. Apart from Factor 12 (‘numeral’), all identified factors were related to important theological concepts.    Table 1. Factor names and synonymous word groups. In order to compare the major themes of the Synoptic Gospels, characteristic factors were identified based on factor scores. The average factor scores in each Gospel are shown in Table 2. In Matthew, Factors 6 (‘hypocrisy of the Pharisees’), and 23 (‘good and evil’) are found to be influential, which may allow us to consider the major theme of Matthew’s Gospel to be justice. In Mark, Factors 4, 8, 10, 13, 16, 17, 20, 21, 24, 25, 26, 27, 30, and 31 are influential, which may lead to the conclusion that Mark’s major themes are miracles (‘bread in the wilderness’, ‘exorcism’, ‘resurrection’, and ‘awe of people’) and salvation (‘parables of sower’, ‘leading to faith’, ‘salvation for the people’). In Luke, Factors 2, 5, 9, 11, 14, 19, and 29 are influential, so the major theme of Luke may relate to missionaries (‘travel to Jerusalem’, ‘missionary’) and what disciples should do (‘thanks by sinners’, ‘announcements to Maria’, ‘the lord and servant’).    Table 2. Average factor scores in each Synoptic Gospel.  Conclusion and Future Work  In this research, average synonymous word groups were extracted using a quantitative method. These word groups were used to extract theological concepts from the Synoptic Gospels by utilizing factor analysis in a falsifiable way. Moreover, the theological differences between the three Synoptic Gospels were identified based on the average factor scores in each text. Unlike conventional exegetic methods, these results are reproducible, and third-party evaluation is possible because they are based on scientific methodology. For future work, detailed analysis and consideration of obtained factors should be conducted. In addition, other books of the Bible should be analyzed and compared in order to understand the theological concepts of the Gospels and the Bible. ",
        "article_title": "Identifying Synonymous Word Groups in the Synoptic Gospels: A Quantitative Analytical Approach",
        "authors": [
            {
                "given": "Hajime",
                "family": "Murai",
                "affiliation": [
                    {
                        "original_name": "Tokyo Institute of Technology, Japan",
                        "normalized_name": "Tokyo Institute of Technology",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/0112mx960",
                            "GRID": "grid.32197.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "theology",
            "relationships",
            "text analysis",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " It is the case with Buddhist studies—as with most other fields in the humanities—that various types of digital resources have been developed. So far, there have been several core digital resources based on traditional series of Buddhist scriptures, indexes, and dictionaries, which had been published within the long era of paper media. It is, in fact, efficient to use such traditional research tools in order to connect the digital resources with existent secondary sources. In this context, digitization makes us aware of what we have been depending upon; it also helps to reveal issues that have been inherent in our work, even during the era of paper media. We will describe the significance of digitization for Buddhist studies from this viewpoint through a recent case in the SAT Database (SAT DB) (Nagasaki et al., 2013).  The SAT DB is a web service that delivers an integrated research environment for Buddhist studies based on a corpus of digital classical Chinese Buddhist scriptures. The SAT DB has connected with several academic and cultural resources such as the Digital Dictionary of Buddhism, a parallel corpus of the Buddhist scriptures translated into English, and several bibliographical databases and character databases—all linked by implementation of convenient interfaces. The web services are accessed over 300,000 to 500,000 times per month. The SAT DB has recently been linked text by text with several other digital resources, such as the Buddhist Canons Research Database (BCRD) 1 and some other digital repositories. We will briefly describe the BCRD below.  The BCRD is the only comprehensive index to Tibetan Buddhist canonical materials, providing cross-references between primary materials and detailed publication information to secondary literature, while incorporating hyper-text links to online resources. Standard library cataloging systems provide bibliographic information at the ‘item’ level (a monograph, a serial, etc.) only. This level of cataloging information is inadequate for classical collections such as the various Buddhist canons. To illustrate by example, the Tibetan Buddhist canon consists of approximately 5,000 individual works of varying authorship; an individual recension of the Buddhist canon is typically represented in standard library catalogs by either one or two bibliographic records. All other relevant details for accessing individual works are relegated to secondary reference literature. The same holds true for the vast majority of subsequent commentarial literature that has often been published in large collections grouped thematically or by author. The BCRD has compiled complete documentation for the Tibetan Buddhist canon, as well as another cross-linked 3,500 post-canonical works. It also provides the raw bibliographic resources to enable advanced research by fully documenting previous research and to provide constantly updated bibliographic references to print and digital resources in multiple languages. Such a project benefits a broad range of scholars in the humanities by providing detailed and accurate reference information otherwise not readily available, and in a quick and concise manner. In addition, the BCRD provides full-text searching of the entire Tibetan Buddhist canon (15 million syllables). At the present time, it has more than 700 active users and deploys cutting-edge techniques of Natural Language Processing to enable intelligent searching and precise identification of search results. As a result, user feedback has been enormously positive, and users have described the resource as ‘revolutionary’ and ‘a huge advance . . . [and] enormously beneficial to scholars in the field’. The linkage between the BCRD and the SAT DB has been made possible based mainly on a corresponding catalogue (Ui et al., 1934) that was a milestone in the paper media—that is, the two present resources are connected with the past resource. It has streamlined the workflow of browsing corresponding scriptures across the Chinese and the Tibetan, which had been important and inevitable but needed complex procedures in order to explore transition of various aspects of Buddhism. Moreover, each separate database project is now relieved from having to do the various works of building mutual databases. It will also be useful to explore further issues in both corresponding scriptures and to improve earlier catalogues.  The SAT DB also has begun to gather URLs of digital images of primary resources of Buddhist scriptures, such as manuscripts and woodcut printings, which are published in various cultural data repositories. We have tentatively obtained approximately 600 URLs and have uncovered several problems. In the case of secondary resources, the SAT DB has already provided a stable linking service by semi-automatic search by use of Web API in several search engines of journal articles. However, in this case, we were not able to use some popular convenient methods such as Web API due to the fact that the metadata in each repository are not unified and the contents cannot be retrieved by texts. Our collection must be done by manually checking each image. In Japanese cultural web resources, at least two integrated search engines are enabled, but neither provides enough search function for our usage. HathiTrust 2 and gallica, 3 from which we gathered URLs, are also not sufficient. In the case of HathiTrust, the names of scriptures are not yet regularized with our databases. In the latter, as it includes various fragments of manuscripts of the scriptures, we need to identify the location of them one by one in order to provide convenient usage. So far, we have targeted the repositories of HathiTrust, gallica, the National Diet Library of Japan (NDL), 4 the University of Waseda, the University of Ryukoku, Ritsumeikan University, the National Institute of Japanese Literature, and so on (see Figures 1 and 2). As the targets will increase further, we should check URLs continuously. Meanwhile, we should analyze each repository and design appropriate metadata for our database and disseminate these for efficient interoperability according to several standards, because some repositories lack publication data, some lack a method of writing, and some don’t include format of metadata. Moreover, we released a Web API to provide our link data so that other providers can avoid reinventing the wheel. Thus, it will also be useful to provide a model case for interoperability of Eastern cultural resources.      Figure 1. The dialog to link with other resources.    Figure 2. The relationship of digital repositories via SAT DB and BCRD. Notes 1. http://www.aibs.columbia.edu/databases/New/index.php. 2. http://hathitrust.org/. 3. http://gallica.bnf.fr/. 4. National Diet Library Digital Collection, http://dl.ndl.go.jp/?__lang=en. ",
        "article_title": "Significance of Linking between past and present, east and west, and various databases",
        "authors": [
            {
                "given": "Kiyonori",
                "family": "Nagasaki",
                "affiliation": [
                    {
                        "original_name": "International Institute for Digital Humanities, Japan",
                        "normalized_name": "International Institute for Digital Humanities",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/0454arg59",
                            "GRID": "grid.474291.d"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Hackett",
                "affiliation": [
                    {
                        "original_name": "Columbia University",
                        "normalized_name": "Columbia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hj8s172",
                            "GRID": "grid.21729.3f"
                        }
                    }
                ]
            },
            {
                "given": "A. Charles",
                "family": "Muller",
                "affiliation": [
                    {
                        "original_name": "University of Tokyo",
                        "normalized_name": "University of Tokyo",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/057zh3y96",
                            "GRID": "grid.26999.3d"
                        }
                    }
                ]
            },
            {
                "given": "Toru",
                "family": "Tomabechi",
                "affiliation": [
                    {
                        "original_name": "International Institute for Digital Humanities, Japan",
                        "normalized_name": "International Institute for Digital Humanities",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/0454arg59",
                            "GRID": "grid.474291.d"
                        }
                    }
                ]
            },
            {
                "given": "Masahiro",
                "family": "Shimoda",
                "affiliation": [
                    {
                        "original_name": "University of Tokyo",
                        "normalized_name": "University of Tokyo",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/057zh3y96",
                            "GRID": "grid.26999.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "English",
            "resource creation",
            "cultural infrastructure",
            "linking and annotation",
            "and discovery",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Social media sites and mobile phones are routinely used to capture, create, store, and share personal memories (Keightley and Schlesinger, 2014; van Dijck, 2010). As our ownership of digital devices and our use of social media sites increases, our capacity to amass personal memories grows and our need for external memory storage expands. Data centers—also referred to as server factories and data warehouses—are the fixed sites that are increasingly used to store personal memories that have been uploaded to social media and other cloud-based services. This makes data centers among the most important infrastructural installations for the production and maintenance of global connection and for what we refer to elsewhere and in this paper as ‘globital memory’ (Reading 2010; 2015).  The number of data centers has been steadily growing for more than a decade, and one of the drivers for this growth is the uptake of cloud-based services. One data market research company estimates that ‘the number of data centers being built around the world will continue growing until it peaks at 8.6 million in 2017’, while the amount of data center space worldwide ‘will grow from about 1.58 billion square feet total in 2013 to 1.94 billion square feet in 2018’ (International Data Corporation 2014). Despite these quite remarkable growth trends, our knowledge of data centers is often obscured by business rhetoric and commercial secrecy. Very few of us know where our personal data is being stored, who it is being shared with, or the territories through which it moves when we or others seek to access or retrieve it (Blum, 2012; Mosco, 2014).  Beneath the surface and fuelling the growth of data centers is commercial discourse suggesting that digital memory is far more reliable and has a greater capacity when compared to our human memory. The company Memolane, for example, promised to capture ‘your entire online life in one timeline, making it easy for you to travel back in time and re-live great memories. Whether you’d like to re-discover your holiday of a lifetime, re-live that great party last week or get all nostalgic about when the kids were little, now with Memolane it’s easy to keep the memories alive’ (Memolane, 2011). Given these claims, it is ironic that this personal memory storage company collapsed just a few years after it began, and the digital memories people stored on its service were erased on only short notice (Memolane, 2014). Digital memory—as we can see from this example and many others—is far from infallible. The expansion and vulnerabilities of cloud services and remote digital storage industries raise important questions about their political economy and impacts. Recent research in the humanities has highlighted the environmental and social impacts of data centers, particularly in the US and European context (Reading and Notley, 2015; Hogan, 2015; Carruth, 2014; Kitchin, 2014; Maxwell and Miller, 2012; Parikka, 2011). However, there remain many gaps in this limited academic literature, particularly when it comes to examining social and personal impacts in relation to individual and collective memory. In addressing this gap, this paper asks what our ‘globital memory’ means and looks like to data center users and to the different actors that play a role in the data center industry. The globital memory field, which seeks to understand the synergetic combination of digitisation with globalisation, is an emergent concept within the field of memory studies. Globital memory is best understood as a cultural field that involves uneven struggles by memory agents to mobilise and secure assemblages of memory with memory capital through connective and mobile technologies. The memory ‘assemblage’ has multiple nonlinear trans-medial trajectories and connectivities that may be uneven and contradictory; these trajectories traverse conventional communicative binaries that have framed many understandings of cultural and mediated memory, such as body-machine, analogue-digital, public-private. Memory assemblages within the globital memory field are a composition of things and bodies with utterances and expressions. The assemblage’s mobilities and securities may be analysed in particular domains across six trajectories that include its transmedialities, velocity, extensity, valency, viscosity, and modality (Reading, 2010).  Since the data center is now one of the key domains through which digital memory assemblages are mobilised and securitised, in this paper we seek to analyse how globital memory involves the movement of assemblages of data through fixed points, which in turn serve to enable the multiple trajectories and transmedialities within the globital memory field.  To explore this issue, we examine discursive artifacts that illuminate the complex corporate rhetoric that serves to define digital memory, and we compare this with the material realities of data centers. Extending this analysis, our research then asks: What do data center users and workers expect of the data center and cloud service companies that store our personal memories, and how does this correspond to practices and codes of conduct? To analyse the space of server farms, we examine the rhetoric and reality around the development of Western Sydney as a ‘data centre technology hub’ for Australia. In particular, this pilot research, part of a larger research project, analyses the commercial rhetoric surrounding a single large data center situated in Western Sydney. The analysis draws on examples from news reports as well as stories by local business networks and from the company itself. This is rearticulated through examples of on-the-ground experiences and imaginaries of people in the region in Western Sydney gathered through online discussions as well as a discussion group and pilot interviews with Western Sydney residents whose everyday lives and knowledge practices are experientially situated within the new technology hub in Western Sydney.  We argue that, in part, public understanding of our role as memory agents within globital memory is largely determined by metaphors and rhetoric that serve to create digital imaginaries. To see how these imaginaries work, we ask: How do we experience personal memory when it is no longer contained within us or nearby us, but rather is stored remotely in data centers? When we send our data to these places, do we see the physical spaces they inhabit, including the companies and their workers who ‘watch over’ our personal memories? What do we expect of these people and companies, and how does this correspond to their motivations and codes of conduct? Or are the behaviors of these intermediaries mostly rendered invisible to us because, for example, we just don’t care or because these actors are concealed by opaque, vague, or misleading corporate rhetoric? By analysing the discursive rhetoric that exists in relation to globital memory within the grounded context of the emergence of data centers in Western Sydney, this paper offers original empirical material framed through new theoretical insights linking in much-needed ways work within memory studies, media studies, and the digital humanities. By connecting commercial rhetoric with on-the-ground, everyday knowledge and memory practices, the paper highlights the contradictions and frictions that exist between corporate and digital memory rhetoric and the material realities of data centers. ",
        "article_title": "Server Factories and Memory Mediators",
        "authors": [
            {
                "given": "Tanya",
                "family": "Notley",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Anna",
                "family": "Reading",
                "affiliation": [
                    {
                        "original_name": "Kings College, London",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "digital humanities - nature and significance",
            "anthropology",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Applying clustering techniques in modeling text collections is effective for surfacing high-level themes at scales that humans cannot match with close reading. Within-book topic modeling is more difficult to perform, due to noise inherent to small frame sizes and the expectation that a single book can simply be read. In this work, we argue that this form of text analysis is effective as a research aid, presenting a method and tools to visualize the progression of topics through the course of a long text.  This approach focuses on improving human readability of machine-modeled topics and is notable for (a) a sliding-frame topic inference technique that smooths noise and (b) a visualization approach that uses sorting and filtering to help focus attention on clearer topics and their narrative order.  The goal is to use topic modeling as an aid to a reader’s understanding of a work as well as the ability to communicate that understanding. In particular, it appears to have promise as a mnemonic tool, helping past readers recollect plot points and progression. As this work progresses, it will explore better ways of surfacing important topics and measuring the quality of its visualizations. To this end, a tutorial and scripts have been released, enabling researchers and instructors to evaluate this technique for their own uses.  Data  This study was performed using the extracted features dataset from the HathiTrust Research Center (HTRC). HTRC is the research arm of the HathiTrust, developing tools for research access—and particularly large-scale access—to the holdings in the HathiTrust Digital Library. For our purposes, the HTRC’s feature extraction dataset is used less for its breadth than its features. The extracted features dataset includes page-level feature information, including part-of-speech tagged token counts with headers and footers removed. Our approach will work similarly on any clean text documents with page-level information or comparably sized sections—for example, blocks of TEI marked-up sections or paragraphs.  Approach  This study’s method for within-book topic modeling is notable for its approach to training and inference: where the training data is composed of individual page texts, the data it is inferred against is a sliding frame of pages. This lends a coherence to the resulting topic models by offering clean input data while assuming that actual occurrences of topics in a book occur in broader spans of text. We take a conservative view of what a theme in the text is when training, and a liberal view when inferring.  While pages are succinct for training, most of the time they are a physical artifact, disconnected from the content of the book. Language also deviates from page to page: inferring their topics in groups of pages allows for themes at that point in the book to emerge more clearly. Since we do not know where topics start and end, nor can we reliably assume that any particular part of a book is about any one topic, we use a sliding frame of groups of pages.  Latent Dirichlet Allocation (LDA) is used in this approach (Blei et al., 2003). LDA is a generative mixture modeling approach used to estimate probability distributions over correlated data. When used with text unigrams as features, these distributions are often interpreted as ‘topics’. One can think of a topic distribution as a word generator, outputting different words with varying frequencies. A topic about ‘Valentine's Day’, for example, is more likely to generate the terms ‘love’ and ‘February’ than one would normally see in the English language; to guess how likely a document is to be about Valentine’s Day, we can look at the likelihood of the Valentine’s Day topic generating the particular words in the document.  In training a generative model, it is ideal to have each input document represent a minimal number of concepts. While LDA is robust at differentiating different components of a training document, a clearer input improves the possibility of coherent clusters. For this reason, training a classifier on a large chunk of text such as a book can lose the nuance of the various themes within that text. For this study, we train on individual pages of a work. This page-level information is offered in the HTRC Feature Extraction dataset, and modeled using the LDA functionality of the MALLET toolkit (McCallum, 2002).  After a work is modeled against its pages, the resulting topic distributions are then applied to infer the most probable topics of sliding frames of pages in the same work. For example, for a frame size of 5, the work is processed into sections representing pages 1–5, pages 2–6, 3–7, and so on. Figure 1 demonstrates the difference in readability that the sliding frame allows.      Figure 1. Example of topic for  Anne of Green Gables inferred without (top) and with (bottom) sliding page frame.      In pursuing a technique for aiding an individual’s understanding and recall of a work, inferring coherent within-book themes is only as important as their presentation. For this reason, this study pursued visualization of within-book topic models with sorting and filtering.  Figure 2. Topics in  Tess of the D’Ubervilles, shown with a sliding frame of 10 pages. Every third of 30 topics shown, to demonstrate topic sorting with brevity.   For sorting, topics are tagged by their most representative page, and subsequently visualized in chronological order of these pages. This sorting technique has proven to be effective at showing their progression through a work. Not all topics are equally insightful. Particularly, there are usually a few topics that serve as catch-alls: attracting probability mass for difficult-to-assign terms. These ‘noise topics’ serve a useful function, but not for an individual’s analysis or understanding. We attempted to identify these topics and filter them through a number of methods, so that they could be ignored in visualization. This included looking at high-variance distributions, distributions with high peakedness, and topics with disproportionately large pieces of the overall probability mass. Unfortunately, each of these techniques would filter out some useful topics, so we thus far have not found a filtering approach that is worth performing. Instead, the sorting performed at visualization leads with the most likely ‘noise topics’, making them easy for a person to visually assess and ignore if necessary.  Figure 2 demonstrates topic visualization for topics seen in  Tess of the D’Ubervilles by Thomas Hardy. For brevity, only every third topic is shown, showing the sorting progression and the grouping of potential but not necessarily noisy topics at the start. In addition, vertical lines are included to denote the ‘phases’ in the book, allowing a comparison of topics to parts of the book. For example, Phase the Second clearly contains most of the discussion around the protagonist’s birth and subsequent loss of her child, and the language representative of this topic does not recur again in the book.   Next Steps  The code for this project is available online. 1 As this is a work-in-progress, we hope this tool encourages others to use it and provide feedback. Thus far, this study has pursued qualitative improvements based on the judgments of the authors. As we move forward, we hope to evaluate this approach against the satisfaction of domain experts, compared to previous techniques.   Additionally, the goal of filtering uninteresting or noisy topics is still interesting, even if our given approach was not tractable. A more deliberate approach might be more effective in the future; specifically, human judges will be asked for their opinions on the most insightful topic distributions. This will allow us to compare their responses to a number of statistical metrics about the distribution, potentially building a classifier for useful topics.  Note 1.  HTRC-Book-Models, Github, https://github.com/organisciak/htrc-book-models.  ",
        "article_title": "Remembering books: A within-book topic mapping technique",
        "authors": [
            {
                "given": "Peter",
                "family": "Organisciak",
                "affiliation": [
                    {
                        "original_name": "University of Illinois at Urbana-Champaign, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Loretta",
                "family": "Auvil",
                "affiliation": [
                    {
                        "original_name": "University of Illinois at Urbana-Champaign, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "J.Stephen",
                "family": "Downie",
                "affiliation": [
                    {
                        "original_name": "University of Illinois at Urbana-Champaign, United States of America",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "English",
            "text analysis"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Global Outlook::Digital Humanities (GO::DH) is a Special Interest Group (SIG) of the Alliance of Digital Humanities Organisations (ADHO). Its purpose is to help break down barriers that hinder communication and collaboration among researchers and students of the digital arts, humanities, and cultural heritage sectors across political, economic, and regional boundaries (Global Outlook::Digital Humanities, 2013). The origins of GO::DH lie in Melissa Terras’ infographic ‘Quantifying Digital Humanities’ (Terras, 2012). This posting contained a map showing the distribution of physical centres in the digital humanities (as defined by members of ADHO communities) across the globe. Its most striking feature, however, were the absences: apart from a thin band of centres across the Northern Hemisphere and in Australia, New Zealand, and South Africa, the majority of the world appeared as ‘empty space’. While digital work in the humanities was being conducted by researchers, teachers, and scholars in these areas, it remained for the most part invisible to researchers in the ‘Global North’ associated with the largest international digital humanities societies and organisations. There was a literal disconnect between researchers in these (largely mid- and low-income economic) ‘unknown’ regions and those appearing on the map (for the most part found in high-income regions): the researchers were simply not part of the same digital, academic, and social networks. Although the digital humanities as we define it is, relative to the humanities more broadly, a highly international and collaborative endeavour, it remains the case that our internationalisation and collaborative activity are primarily conducted on an East-West basis among a relatively small number of generally contiguous, high-income economies (O’Donnell, 2012; see also Fiormonte, 2012; Galina, 2013). The GO::DH approach to breaking down these barriers is extremely practical: the organization develops complementary strengths, interests, abilities, and experiences of its membership through special projects, events, activities, and publicity (Priego and O’Donnell, 2013; Priego and Gil, 2013). Its core activities are discovery, community-building, research, and advocacy. It helps its members learn more about digital work in the arts, humanities, and cultural heritage sectors; it acts to foster collaboration and cooperation across regions and economies; it coordinates research on and in support of the use of technology in these areas across the globe; and it advocates for a global perspective on work in this sector. This paper discusses in particular approaches that the SIG has taken in the last two years towards promoting multilingualism within the digital humanities, primarily through the ‘whisper campaign’ (first rolled out at DH2014). It concludes with information on the translation toolkit currently under development by authors. The Whisper Campaign The whisper campaign from DH2014 in Lausanne is one of the more prominent GO::DH initiatives. This project, which came together very quickly in the weeks leading up to the conference, involved the distribution of buttons to conference attendees on which they could write languages they were able to interpret for others. The model for this project came from the 2012 INKE conference in Havana, Cuba (INKE, 2012). Although the language of the conference was English, and although most participants at the conference could work in English to a greater or lesser degree, the conference organisers also decided to promote an informal system through which those with stronger English skills could help those with weaker ones. When speakers who preferred to speak in Spanish presented, the same system was used, with Spanish speakers in this case translating for Anglophone attendees. Two members of the GO::DH executive in attendance at that conference, Alex Gil and Daniel Paul O’Donnell, were quite taken with this model and began to wonder whether this could be used in other DH contexts. The need for such an approach was further confirmed at DH 2013, when several members of GO::DH saw examples of presenters or conference attendees struggling with questions or answers in English. The final push for the whisper campaign came in the form of an unconference session on multilingualism led by Élika Ortega at DHSI2014 in which participants were asked to come up with ideas on how to ‘knit an exchange network of DH scholarship in various languages’ (Ortega, 2014). In his concluding keynote, Gil argued that the initiatives proposed at this workshop would help ‘open language to the community, where a translation of the website or any forum post depends on the community itself’ (Gil, 2014). In a subsequent discussion on the GO::DH mailing list, O’Donnell suggested that ‘most people find it easier to receive information in weaker languages than produce it’, while Isabel Galina proposed using the (then) upcoming DH2014 meeting as an opportunity to put together a concrete plan of action: ‘Maybe somewhere where you can sign up and say, “I am willing to help out with the following languages” or “I need help in this language”? A volunteer system for helping out in sessions that require it?’ (Global Outlook::Digital Humanities, 2014). With some buttons ordered by Ortega and some felt-tipped markers, the project was implemented for less than $200. While there were relatively few examples of ad hoc translation at DH2014 (though there were a large number of volunteers), the greater impact is seen in its influence on subsequent meetings: organisers at the Society for the History of Authorship, Reading, and Publishing (SHARP) meeting and OurMarathon project, for example, have sought to either incorporate the same model or use it to inspire efforts to make sure their practice is open to languages other than English. We will also be offering the service again at DH2015.  The Translation Toolkit  The success of the whisper campaign—both in practical terms at DH2014 and in its wider effect at bringing multilingual issues to the fore—has inspired members of the executive to develop a ‘translators toolkit’. This is a collection of tips for best practices and tools that have been compiled with help from the broader DH community. Examples of these practices include promoting the use of multilingual slides during presentations, allowing presenters and attendees to request live translation (via the whisperers themselves), creating multilingual posters, matching multilingual chairs to multilingual sessions, and compiling a multilingual glossary of DH terms. Ultimately, the initiative could lead to the creation of a directory of whisperers and multilingual reviewers. ",
        "article_title": "Psst! An informal approach to expanding the linguistic range of the Digital Humanities.",
        "authors": [
            {
                "given": "Elika",
                "family": "Ortega",
                "affiliation": [
                    {
                        "original_name": "CulturePlex Lab, University of Western Ontario",
                        "normalized_name": "Western University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02grkyz14",
                            "GRID": "grid.39381.30"
                        }
                    }
                ]
            },
            {
                "given": "Alex",
                "family": "Gil",
                "affiliation": [
                    {
                        "original_name": "Columbia University",
                        "normalized_name": "Columbia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hj8s172",
                            "GRID": "grid.21729.3f"
                        }
                    }
                ]
            },
            {
                "given": "Daniel Paul",
                "family": "O'Donnell",
                "affiliation": [
                    {
                        "original_name": "University of Lethbridge",
                        "normalized_name": "University of Lethbridge",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/044j76961",
                            "GRID": "grid.47609.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "translation studies",
            "English",
            "digital humanities - nature and significance"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Ten years ago, Peter Robinson reviewed the state of digital editing at the end of its first decade in the Internet era in a trio of important articles (Robinson and Taylor, 1998; Robinson 2003; 2005). The results were not good. On the one hand, there were, by that point, a large number of digital editions and a great interest in the community of textual scholars in the possibilities of the digital edition. On the other, there was very little evidence that users were turning to digital editions: there were still many print editions being made and evidence of citations suggested that scholars preferred, on the whole, to work with familiar print editions over their newer counterparts (Robinson, 2005). Robinson, for his part, argued that a good part of this problem lay with the lacuna in the digital editions themselves (Robinson, 2003). While it was difficult even in those days to find ‘a single large-scale editorial project in western Europe or America which does not have already have, or is not actively preparing, a digital dimension’ (Robinson, 2003), this work was itself still quite conservative in its use of the technology: there was much experimentation, but the result was very little that could not be found in print contexts (esp. Robinson, 2005)—a problem compounded by difficulties in using the technology of the day (Robinson, 2005; O’Donnell, 2010). Ten years later, the digital is now even more firmly established as a primary method of scholarly communication and humanistic research (Robinson, 2010; Gold, 2012; Thaller, 2012). Moreover, interest in the significance and utility of the digital as a method of editing continues unabated (Mandell, 2010). But what about Robinson’s original point about how these digital editions were being used? While textual scholars, editors, and digital humanists discuss the possibilities of the genre, and the improvements they have brought to our ability to present textual scholarship, is there any evidence that the supposed end users of these editions are recognising the benefit that digital editions offer to their research? A review of contemporary secondary literary scholarship suggests that they are not. Despite the publication of a large number of new digital scholarly editions and great improvements in delivery methods that have all but eliminated difficulties of access experienced by scholars in the 1990s and early years of this century, bibliographic evidence suggests that digital textual editions are still passed over in actual use by scholars in preference for often superseded print editions. In contrast to the early years of this century, when scholars rarely cited electronic editions, scholars now frequently cite but rarely actually use their digital descendents. The actual text cited, as often as not, comes from the print editions that preceded these digital texts. This paper presents the results of an ongoing study of this problem, based on a comparative longitudinal citation analysis of select digital and print editions of the same literary work. As part of this research, the authors have compared the rate at which citations to a ‘newer’ (approx. 10 years old) digital edition of a standard medieval vernacular poem have been disseminated through the secondary literature and compared this to the rate at which previous (print) critical editions of the same work were disseminated within a similar time frame in earlier periods. The results suggest that editorial research may simply disseminate more slowly than was previously thought: it appears to take about a decade before the text of a ‘new’ edition begins to be used as the basis of new critical arguments within the main text of secondary research (as opposed to being mentioned in passing in bibliographic notes and asides); it appears to take about two decades before a ‘new’ edition begins to become the default choice within a given research domain. The final part of the paper turns to consider the implications of these findings for contemporary DH researchers. The first is that the slower-than-expected diffusion rate for digital scholarship places an even greater emphasis than before on issues of longevity and preservation. Ten years ago, O’Donnell demonstrated how few of the early digital editions of medieval texts had survived even a half-decade in an operational format (O’Donnell, 2004). This current research suggests that creators of digital editions need to think instead in terms of decades when it comes to preservation and longevity. A second implication involves the question of delivery format. While this study compares the diffusion rates for digital and print editions of the same text over several decades, the editions are similar in that all were disseminated as discrete bibliographic items: as books in the case of the print editions, as a CD-ROM in the case of the digital text. Evidence from other disciplines, however, suggests that web-based open-access publications have stronger citation rates (and, possibly, quicker dissemination rates as well) than closed-access publications (see, e.g., Laakso, 2014). In this particular case, no comparable web-based editions of this text exist, and none of the few web-based critical editions of comparable texts from the same period survive in nonarchived format. This suggests that while web-based publication may speed up dissemination rates, this speed must be balanced against the evident difficulty such projects have had, historically, in ensuring their survival—especially in the case of a time frame measured in years. And finally there is the question of training. The evidence suggesting that dissemination proceeds more slowly that has been previously anticipated suggests that digital editors also have a different type of training problem than they may have initially thought: instead of focussing on the short-term, early adopters of such technology, editors need instead to think about use cases 10 or 20 years down the road—of future scholars who have yet to begin their undergraduate careers, and current colleagues who may not see a citation to a digital edition for another 15 or 20 years. In discussing the success (or lack thereof) of digital editions in making an impact on the larger domains to which they aim to contribute, it is common to attribute the problems they have experienced in gaining acceptance to problems with the genre, its producers, or its audience. The research in this paper, however, suggests that the real problem may be simply that digital editorial scholarship is still too recent to have had much of an impact on secondary study of the texts they mediate, as a very similar problem appears to have affected earlier print editions of such works in their first few years. But if the ‘problem’ with digital editions is simply that they are too new, then this changes our perspective on a number of other issues commonly discussed in relation to the discipline. Faced with a dissemination rate that suggests it may take 20 or more years before any edition can be considered ‘standard’, digital editors need to rethink questions of preservation, format, and training. ",
        "article_title": "The Old Familiar Faces: On The Consumption of Digital Scholarship",
        "authors": [
            {
                "given": "Daniel Paul",
                "family": "O'Donnell",
                "affiliation": [
                    {
                        "original_name": "University of Lethbridge, Canada",
                        "normalized_name": "University of Lethbridge",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/044j76961",
                            "GRID": "grid.47609.3c"
                        }
                    }
                ]
            },
            {
                "given": "Gurpreet",
                "family": "Singh",
                "affiliation": [
                    {
                        "original_name": "University of Lethbridge, Canada",
                        "normalized_name": "University of Lethbridge",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/044j76961",
                            "GRID": "grid.47609.3c"
                        }
                    }
                ]
            },
            {
                "given": "Roberto",
                "family": "Rosselli Del Turco",
                "affiliation": [
                    {
                        "original_name": "Universita degli studi di Torino",
                        "normalized_name": "University of Turin",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/048tbm396",
                            "GRID": "grid.7605.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "publishing and delivery systems",
            "digitisation",
            "scholarly editing",
            "user studies / user needs",
            "project design",
            "bibliographic methods / textual studies",
            "english studies",
            "English",
            "history of Humanities Computing/Digital Humanities",
            "philology",
            "resource creation",
            "and discovery",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Ireland’s literary communities are thriving. In the face of mounting economic pressures, the island continues to host a relatively substantial volume of prestigious publications dedicated to contemporary literature, all of which are seemingly committed to a high standard of literary quality. Some of Ireland’s most acclaimed contemporary authors are engaged with these journals as editors, with an increasingly wide spectrum of genres and modes being catered to. Ireland is a nation rich in literary tradition, but this is a varied tradition, comprising something of a riot of aesthetics, forms, and ideologies. However, unlike larger countries, it is feasible to identify Ireland’s most prestigious journals, and having done so, produce a whole range of literary extrapolations through the use of computational methodologies. This paper takes a number of Ireland’s longest-running and most prestigious literary journals, and using a variety of computational methods, conducts a series of macro-analytical explorations designed to identify the key trends across its dynamic literary community. In choosing literary journals, I am focusing on a sector that continues to thrive, both nationally and indeed globally (Harmanci, 2011). More importantly, journals and periodicals are reflective of a region’s literary grass roots, in that they largely comprise authors who are an active part of the artistic community. Thus, they arguably provide a more accurate indicator of local and national trends than one would find in an analysis of novels or standalone collections. Methodology and Results This study brings together a unique dataset, containing digitised editions of some of Ireland’s longest-running and most respected literary periodicals. Specifically, this dataset includes complete or nearly complete collections of each of the canonical  journals for contemporary  Irish writing. While a major analysis has been conducted and is still under way, for the sake of brevity only a sample of my findings are presented here. The journals have been analysed using a variety of approaches to textual analysis, as well as topic modeling. In this particular instance, I have elected to outline the results of a subset of my stylometric and textual analyses, as well as my initial topic models. For the purposes of this abstract, I am focusing on the online issues of two of Ireland’s leading contemporary journals, Poetry Ireland’s  Poetry Ireland Review, and  Southword, published by the Munster Literature Centre. The final study will incorporate the results of several other analyses relevant to this exploration of contemporary Irish writing and use a wider dataset.   Cluster Analysis of Poetry Ireland Review  and Southword  Editorial influences on style are explored using multivariate stylometric methods packaged in Stylo (Eder et al., 2013). In this instance, what is of interest is whether there is a clear stylistic separation between the submissions being accepted to  Southword and the  Poetry Ireland Review. A cluster analysis, using Delta (Burrows, 2002; Hoover, 2004) and 100 features, demonstrates a clear separation between the styles found in each journal (see Figure 1).     Figure 1. Cluster analysis of Poetry Ireland Review and Southword.  Principal Component Analysis  Principal component analyses of the journals suggests that content is temporally comparable (see Figure 2), but also editorially separable, an issue that will be delineated in greater detail in the presentation.    Figure 2. PCA of Poetry Ireland Review and Southword.  Distinctive Markers in Poetry Ireland Review  and Southword  A list of the most distinctive words from each journal is generated using Craig’s Zeta (Hoover, 2008; Burrows, 2007), with a text slice length of 5,000, text slice overlap of 1,000, and an occurrence and filter threshold of 2 and 0.1. Unlike Delta, Zeta is a mode of classification, the results of which demonstrate a sample of the ‘unique’ words from each journal (see Table 1).    Poetry Ireland Review Southword    humour criticism oxford dolmen revolution role perception mythology extent stated speak shall original desire pain myth development described fact critical awareness celtic passion importance hearing given matter sight culture historical   father sitting walked minutes revival across distance passed drove completing prisons sandwiches seated shouts angled knocks sideways clock camera cigarette beer pipes pressing shit splash corners trousers muddy crowded meal    Table 1. Most distinctive words from Poetry Ireland Review and Southword.  Topic Modeling in Poetry Ireland Review  and Southword  Topic modeling is a form of probabilistic modeling designed to ‘uncover the hidden thematic structure in document collections’ (Blei, n.d.; 2012). Non-negative Matrix Factorisation was used to conduct the analysis, a fragment of which has been provided (see Table 2).   love time home mr high deep remember war lost died    poetry poems poem work irish poet book poets language verse   life great things made find part power don sense human    black day heart half world trees hair silence dream garden    night light dark sea water sun green room air sky     white man dead time back red blood head leaves bed    na ar de ag la mo agus tuairisc se el      god long face eyes words death hands place earth hand    clarke house years young back church found small ll great    good people man women world Heaney poet political woman life   Table 2. Topic models in Poetry Ireland Review. Interpretations A number of interesting peculiarities have emerged from this study. The fact that there is a consistent stylometric separation of the journals suggests that there is a clear style associated with each journal, which I suggest can be attributed to editorial contexts. To my knowledge, this is the first study that offers quantitative evidence of editorial influences. The topic models are also significant, in that they present some fascinating thematic axes. In  Poetry Ireland Review, we see the alignment of Heaney with gender and politics, and dreams with nature.   In my presentation, I will present the complete set of results from this macro-analysis, as well as the most significant findings from this study, which in an Irish context is the very first of its kind. Key findings and interpretations in relation to literary trends—editorially, temporally, and otherwise—will be presented and discussed. I believe this study of interest to the international literary community, not only because of the global interest in Irish writing, but also because many of these findings may well be generisable, or comparable, across differing literary and cultural contexts.  ",
        "article_title": "Computational approaches to Ireland’s Contemporary Literary Journals",
        "authors": [
            {
                "given": "James Christopher",
                "family": "O'Sullivan",
                "affiliation": [
                    {
                        "original_name": "Pennsylvania State University",
                        "normalized_name": "Pennsylvania State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p491231",
                            "GRID": "grid.29857.31"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "stylistics and stylometry",
            "literary studies",
            "text analysis",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Critics have long examined the ‘multimodal capacity of electronic literature’ (Page and Thomas, 2011, 2). The material semantics of electronic literature are not inherently new, but rather a rejuvenation of established literary practices exemplified by the material modernists and other such movements to have sought control over the paratextuality of a literary work. Yet new media does deliver rejuvenation through enhanced paratextual potential. Materiality, and in turn paratextuality, is concerned with experience: it governs the reception of a work and the ways in which its audience might interact with it. In this paper I will engage with works of electronic literature in an attempt to delineate how some of the field’s most prominent authors and practitioners have used the manipulability of digital paratext for artistic purposes.  Electronic Literature as Avant-Garde When an author selects a digital medium, there are ideological considerations that cannot be ignored—they are making a statement about the aesthetic they desire and what that aesthetic represents. In this respect, we see how it is that electronic literature is very much connected to concepts of the avant-garde, so much so that its works are often criticised as being overtly experimental. Responding to Andrew Gallix’s challenge that electronic literature sacrifices literary quality, Grigar accepts that ‘the hybridity of the forms and technological innovation that artists bring to their work result in a high level of experimentation that may at first obfuscate literary content’ (n.d.), but she is quick to point out that obfuscation should not deny the digital’s claim to the mantle of literary. If electronic literature serves as little more than a jolt in literature’s long history, then surely it has achieved the very thing that literature sets out to achieve? Perhaps electronic literature is more valuable ideologically than it is semantically or aesthetically, but having such political value is in itself a very literary trait. Grigar raises another interesting point in that same article, referring to electronic literature’s brief moment within the spotlight of the press, back when it still held its ‘shock of the new’. She offers this as a rebuttal to the claim that electronic literature, once popularised within the media, has since been relegated to a tertiary note on specialised blogs. On the contrary, electronic literature may no longer be a media darling, but it has found itself a place within the academy, and within the focus of respected authors and critics from across the digital humanities.  In this sense, electronic literature’s ability to disrupt the status quo has faded, but it has traded the ease by which it can shock for an ability to achieve problematisation through defamiliarisation—so much so, that electronic literature has become self-reflective to the point where it is now querying its most essential of properties: being born digital.     Figure 1.  Closed Room, Soft Whispers, by Jacob Garbe. From Pathfinders Exhibit, MLA 2014 convention, Chicago, January 2014.  The Pathfinders Exhibit at the 2014 MLA convention presented a selection of early electronic literature, alongside some more recent pieces. Many of the works on show displayed a new iteration in the juxtaposition between literature and technology, in that digital and physical materiality had been fused beyond hardware and software. What Strickland achieves with her hybrid  V: WaveSon.nets/Losing L’una, for example, has been taken to its conclusion: electronic literature is moving, diachronically, at an immense pace, its authors beginning to deconstruct the very boundaries that they themselves put up.   When code is open access, the reader can pierce the veil in a manner rarely seen before. Readers, and indeed authors, have insights into the worlds of their precursors and contemporaries. Authors in this field began by swerving away from the page, but now they are, while perhaps not swerving away from the digital, certainly seeking its convergence with other, seemingly disparate, materialities. It is re-creation of the re-created, a tension that has been consciously instigated in an effort to achieve further defamiliarisation of that which is already defamiliar. This is precisely what Pressman labels as ‘digital modernism’, a movement that, she argues, interrogates ‘cultural infrastructures, technological networks, and critical practices’ (2014, 10), and in doing so, ‘offers a surprising counterstance to this privileging of newness’ (1).  The Politics of the Screen Within the electronic literature community, there is a marked reaction against the intentions of software’s function. Authors in this realm practice perverse engineering, where a tool is not modified for literary purposes but rather adapted, at a surface level, so that it satisfies some authorial desire. Flash, for example, was never intended for literary purposes, but it is at the very heart of the first two volumes within the  ELO Collection. The exploitation of computer systems is fundamental to contemporary electronic literature in that the majority of works are produced using technologies that were designed for some other purpose. This act, the manipulation of hardware and software for literary purposes, is a fundamental aspect of the movements that promote and produce much of this work. Reducing the process of writing electronic literature to a purpose-built intuitive graphical user interface would be akin to creating a program that structured the metre for nondigital poetry. In this sense, electronic literature is the product of exploited technologies; it is expression through manipulation. This presents an interesting reversal of Adorno and Horkheimer’s view, with technology becoming an instrument in the problematisation of those structures that they argue it enforces. Ideological arguments cannot always account for an author’s technological selections, but there are many instances where both the form and tools adopted for a work of electronic literature are inherently ideological, if not overtly political.  In exploring the influence of digital materiality and the politics of e-lit processes and practices, I will look at  Flight Paths (see Figure 2), by Kate Pullinger and Chris Joseph, as well as Mark Marino’s ‘a show of hands’, a piece inspired by the 2006 immigration reform in Los Angeles.     Figure 2. ‘Previous Contributions’, from  Flight Paths, by Kate Pullinger and Chris Joseph.  Electronic Literature Collection, Vol. 2. Electronic Literature Organization, February 2011, collection.elitarture.org.  Conclusions While we cannot separate ideology from any literature, we can see that electronic forms are married to the politics of process. Each of these works has meaning in its materiality, considerably more than would typically be the case, even amongst the foremost of the material modernists. While Yeats was able to embed the esoteric in his bindings, the screen permits semantic permeations throughout a work as a whole. Even concrete poets, with typographic arrangements, are restricted by the surface of the page. I do not want to represent the page as a media with particularly negative limitations: all forms have limitations that can at times be just as readily considered strengths. Concrete poetry, like the cover of a Yeatsian edition, is manifested as a permanent entity. There is no such permanence in the electronic environment, where works of art, when reduced to substance, are merely the graphically rendered representations of bits and bytes. This impermanence offers something authors that is not found in other forms, and as has been seen, it allows them a freedom of ideological expression that is very much bound to materiality. Process is also essential. Traditionally, authors collaborate with publishers to see their literature produced in book form. I think it fair to assume that the majority of writers do not actually possess a working knowledge of how it is that manuscripts are transformed into physical books. That is not to say that nondigital writing practices are without process, but that process plays a more significant role in the digital arena, where authors must be familiar with processes of production if they are to write. Even where collaboration exists, an understanding of how technology can—and cannot—manipulate the linguistic content of a piece must be possessed. It is not simply about words on the surface but rather about the surface and the underlying code that dictates part of the reader experience. Garbe did not just write  Closed Room, Soft Whispers, he  made it: he composed the surface-level text, he wrote the underlying code, and he built the wooden box. Marino captured his own images; he worked directly with the platform that presents his narrative. In this respect, the politics of the screen are, unlike the page, materially layered, with authors being as much makers as they are writers.  Throughout this paper, I will use these works of electronic literature to delineate how contemporary juxtapositions of traditional and modern literary practices have presented authors with hyper-paratextuality, and their engagement with such is significant. It is easier for an author to put down words on a page than it is, for example, to learn Python and produce a generative poem. Thus, it is always significant when an author chooses digital encapsulation for expression. This act, the selection of an electronic rather than nondigital medium, is an authorial statement from which immediate conclusions can be drawn. The significance of the influence exerted by digital paratextuality may be questioned on the basis that much of that which makes it digital sits behind the interface, but the recognition of digitality is in itself influential. Authors do not complicate the process of writing with additional technical requirements on a whim.  ",
        "article_title": "Electronic Literature and the Politics of Process",
        "authors": [
            {
                "given": "James Christopher",
                "family": "O'Sullivan",
                "affiliation": [
                    {
                        "original_name": "Pennsylvania State University",
                        "normalized_name": "Pennsylvania State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p491231",
                            "GRID": "grid.29857.31"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "English",
            "media studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Academic libraries have long supported, collaborated on, and led digital humanities (DH) research. Evidence of engagement can be found in acquisition and preparation of objects of inquiry amenable to computational analysis, tool development, and data and platform management and preservation. While investment in these activities varies, recent entrants and long established institutional participants share in common a commitment to delivery of library-led DH instruction. Library-led DH instruction expands the breadth and depth of DH research capacity on campus and provides a space in which to spread awareness of data and services that can drive DH research forward. Despite widespread provision by librarians, there exists little critical engagement with the pedagogical challenges librarians face in designing and delivering DH instruction.  As librarians do not typically lead credit-bearing courses, the modes of instruction typically employed by librarians occur via one-shot instruction (e.g., disciplinary faculty requests workshop for class) or a workshop series wherein attempts to sequence content (e.g., data gathering workshop, data cleaning workshop, data visualization workshop) are necessarily loosely joined, given an audience that is not fixed by course enrollment and/or an institutional predilection for enabling target audiences to drop in and drop out of a sequence. Given format and audience constraints it becomes tempting to focus on tools at the cost of disciplinary relevance, let alone missing out on the pedagogical possibilities latent in tools rendered as vectors for enacting theories, methods, and ways of thinking about the world (Lincoln, 2014; Padilla, 2014; Vinopal, 2014).  Basic word frequency analysis of the  Journal of Library Administration issue on digital humanities and the  dh+lib response reveals that out of a corpus of more than 43,000 words, pedagogical terms are minimally represented: pedagogy (7), education (12), teaching (15), learn (20). While this might appear to indicate low commitment to pedagogy, the terms work (208), service (207), and support (135) all fall within the top 15 word frequencies ( Journal of Library Administration, 2013; Coble, 2013). 1 It is likely that pedagogical activities are contained within these more general terms, yet painting with such broad strokes does little to assist the librarian aiming to plan, implement, and deliver effective DH instruction. In order to take steps toward remedying this need, two digital humanities librarians, an information literacy librarian, and a data services librarian from Michigan State University applied for and received a DH Pedagogies Microgrant from the Association for Computers and the Humanities to explore development of a model for guiding design of a library-led DH pedagogy.   On 17 October 2014 the  Library-Led DH Pedagogy: Modeling Paths Toward Information and Data Literacy symposium brought together disciplinary faculty and librarians from across the state of Michigan to discuss and test an approach to designing library-led DH pedagogy that draws on the draft Association of College and Research Libraries  Framework for Information Literacy in Higher Education, the Data Information Literacy (DIL) program competencies, and disciplinary learning expectations (ACRL, 2014; Data Information Literacy, 2014; History Discipline Core, 2012). This approach encourages a move toward library-led DH instruction supported by updated information literacy concepts (interacting with the information ecosystem), leverages advances in data information literacy research (how to work with research data, e.g., metadata, data management, and data curation competencies), and increases relevancy through orientation to disciplinary learning expectations (how to interpret, analyze, and make sense in a manner befitting practice by a disciplinary community). The process of alignment was supported by presentations and discussions that brought the  Framework and DIL Competencies into contact with disciplinary practice. This process was further supported by engaging disciplinary faculty and librarians in group-based activities. Each group designed learning outcomes and created model lesson plans predicated on a single or multidisciplinary audience. Mixed disciplinary faculty and librarian participation in project activities was by design, given a framing that assumes DH pedagogy is best understood as an interdisciplinary as well as interprofessional community of practice (Nowiskie, 2014).   This paper frames and describes the variety of attempts made to implement a pedagogical model for library-based DH instruction that aligns the draft  Framework for Information Literacy in Higher Education, the Data Information Literacy program competencies, and disciplinary learning expectations. The model lesson plans produced over the course of the symposium are intended to help guide library development of DH instruction that optimally cultivates information and data literacies and, where appropriate, maintains disciplinary relevance (Carlson et al., 2011; Calzada Prado and Marzal, 2013). The paper concludes by discussing additional findings that surfaced during the course of the  Library-Led DH symposium, such as the challenge of teaching to single vs. multidisciplinary audiences; advances an approach for designing DH pedagogy in a library context; and reflects upon the use of the library-led DH pedagogy model at Michigan State University during the spring 2015 semester.   Note 1. Hirsch (2012) took a similar approach to indicate the paucity of DH pedagogy engagement in general. ",
        "article_title": "Modeling Approaches to Library-led DH Pedagogy",
        "authors": [
            {
                "given": "Thomas George",
                "family": "Padilla",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": "Bobby",
                "family": "Smiley",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": "Sara",
                "family": "Miller",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": "Hailey",
                "family": "Mooney",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digital humanities - institutional support",
            "interdisciplinary collaboration",
            "English",
            "digital humanities - pedagogy and curriculum",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2011, six academics gathered over 90,000 authentic text messages in French from the general public, in compliance with French law. 1 The SMS ‘donors’ were also invited to fill out a sociolinguistic questionnaire (http://sud4science.org; Panckhurst et al. , 2013). The project is part of a vast international initiative titled sms4science (http://www.sms4science.org/; Fairon et al., 2006; Cougnon and Fairon, 2014; Cougnon, 2015) that aims to build a worldwide database and analyse authentic text messages. After the sud4science SMS data collection, a pre-processing phase of checking and eliminating any spurious information and a three-step semi-automatic anonymisation phase were conducted (Accorsi et al., 2014; Patel et al., 2013). Two extracts were transcoded into standardised French (1,000 SMS) and annotated (100 SMS). The finalised digital resource of 88,000 anonymised French text messages, the ‘88milSMS’ corpus, the extracts, and the sociolinguistic questionnaire data are currently available for all to download, via a user free-of-charge licence agreement, from the Huma-Num web service (http://88milsms.huma-num.fr; Panckhurst et al., 2014).   Why decide to  exclude full transcoding and annotation tagging phases?   Transcoding ‘raw’ text messages into ‘standardised’ French means that morpho-syntactic parsers and other natural language processing tools can ultimately analyse them. Checking spelling and grammar facilitates comprehension, but  no supplementary information should be ‘injected’. What if a texter tries to simulate a certain form of oral French, for instance, by using an apostrophe, or through agglutination (‘j’sais’ = ‘je sais’, ‘chuis’ = ‘je suis’)? Should these items be transcoded or not? What about punctuation, often absent in text messages? Should one re-introduce this systematically? Researchers may have differing theoretical viewpoints.   Another issue is tagging the corpus. After much scientific debate about previous experiences with other sms4science members, eight tags were chosen for ‘88milSMS’: TYP(ography), MOD(ificiation), GRA(mmar), BIN(ettes, smileys/emojis), ABS(ence), LAN(guage), ORT(hography, spelling), DIV(erse). Like the previous transcoding phase, annotation is a source of theoretical disagreement. To highlight this, it may be difficult to decide which tag to use, and double tagging may be necessary:  Bone journé. The ‘scriptor’ may have voluntarily modified the two words (‘Bonne journée’ [have a nice day]) or may have lacked spelling knowledge. So should ‘MOD’ and/or ‘ORT’ be used? In another example, in the statement, ‘Il es rentrer a 22h30 et jai eu ldroii au: jsui fatiguer, jai mal a la tete jvai me coucher’ (He came home at 10:30pm and I got to hear: I’m tired, I have a headache, I’m going to bed), ‘rentrer’ (‘Il est rentré’) could be either a grammatical mistake (GRA), or the scriptor may have preferred using an ‘r’ (MOD) instead of pressing the ‘e’ to access the acute accent (on a smartphone). It is extremely difficult to provide satisfactory standardised tagging.   We decided to limit the processing to two extracts. Our (rare) choice to exclude full transcoding and tagging is a theoretical position: annotation is far from neutral. It is directly linked to an interpretative framework. A true consensus on how to standardise the transcoding and annotation does not exist, owing to differing/varying theoretical, (pluri)disciplinary, and scientific stances. We believe that no additional mark-up initiatives should be imposed upon researchers (other researchers disagree; see Chanier et al., 2014); it seems more relevant to let them conduct their own annotation bearing their specific scientific questioning in mind, without being trapped within a unique theoretical framework.  The 88milSMS resource will provide inspiration for many years to come. Our corpus can be used to analyse contemporary mediated electronic discourse, build knowledge on SMS writing forms (Panckhurst, 2009), and let algorithms learn from this: alignment methods for facilitating automatic transcoding are currently being explored (Lopez et al., 2014), as are methods for classifying ‘unknown’ items for use in automatically identifying lexical ‘creativity’ within 88milSMS and also to improve electronic dictionary approaches. The resource also sheds light on ‘corpus-driven’ and ‘corpus-based’ approaches (Panckhurst 2013; Panckhurst et al., 2015). XML encoding means that the resource will be eligible for long-term archiving with the CINES (https://www.cines.fr/). Perhaps in the future, people will look back and explore these ‘snapshot’ resources and understand more about the evolution of scriptural practices and usages in the 21st century. Note 1. Many thanks to my colleagues, Catherine Détrie, Cédric Lopez, Claudine Moïse, Mathieu Roche, Bertrand Verine; our 13 students, who all contributed to sud4science; Nicolas Hvoinsky, legal expert (CIL); and MSH-M, CNRS, and DGLFLF for funding. ",
        "article_title": "'88milSMS', A New Digital Corpus Resource Of French Text Messages: Why We Chose To Exclude Full Transcoding And Standardised Tagging.",
        "authors": [
            {
                "given": "Rachel",
                "family": "Panckhurst",
                "affiliation": [
                    {
                        "original_name": "Praxiling UMR 5267 CNRS, Université Paul-Valéry Montpellier France",
                        "normalized_name": "Université Paul-Valéry Montpellier",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/00qhdy563",
                            "GRID": "grid.440910.8"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "linguistics",
            "natural language processing",
            "interdisciplinary collaboration",
            "corpora and corpus activities",
            "text analysis",
            "English",
            "french studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  We only see what we have interest in.   —Paul Nougé,  Birth of the Object (The Subversion of Images), 1929–1930  In this paper we discuss the heuristic capabilities that the process of creating cultural heritage linked data may afford and the potential of integrating linked open datasets to facilitate and augment arts and humanities research. More specifically, we address the paths to a linked data-driven investigation in the history of women in jazz. The research context is provided by Linked Jazz (linkedjazz.org), an evolving project that experiments with applying linked open data principles and techniques to cultural heritage materials. The project currently uses oral histories from digital archives of jazz history as the main source of named entities to be represented as linked open data. Interview transcripts are processed through open-source applications developed in-house that employ automated and manual methods for name extraction, matching, and quality control. A linked open dataset has been created that includes over 9,000 proper names of musicians as well as the personal and social relationships connecting them. The dataset supports the creation and visualization of interactive social graphs for representing the highly interconnected community of jazz musicians (Pattuelli et al., 2013). While the Linked Jazz tools and data services are intended to support data discovery and analysis, the development process itself has proven conducive to research inquiry by revealing unanticipated paths to discovery and engagement with heritage data. One of the initial steps in producing the dataset for Linked Jazz involved creating a directory of proper names to be leveraged in automated text analysis and extraction activities. To this end, a domain-specific vocabulary of personal names was built by extracting and filtering data from the US version of DBpedia. In line with linked open data best practices, existing URIs were reused whenever possible. As an RDF export of Wikipedia data, DBpedia is the largest and most popular hub of linked open data and thus encompasses an extensive array of jazz artist entries, including less prominent figures. For further refinement and consolidation, the names were mapped to VIAF, the international aggregation service of bibliographic name authorities including LC/NAF, the Library of Congress Name Authority File. While VIAF is widely used to disambiguate proper names and enrich the vocabulary with their variant forms, including nicknames and stage names, it contains only name instances derived from bibliographic records, a smaller subset of the pool that can be found in DBpedia. Through automated analysis of interview transcripts, names of musicians mentioned in the text were identified and matched to corresponding names in our directory. Instances occurred when names were recognized in the transcripts but did not have a corresponding match in our inventory. This was mostly due to the lack of an entry in Wikipedia, the source of DBpedia data. When needed, alternative linked open data sources were manually searched, including the music-specific encyclopedia MusicBrainz (https://musicbrainz.org). When names were not found in any of the available sources, URIs had to be minted using the Linked Jazz namespace, e.g., http://linkedjazz.org/resource/Lynn_Grissett. Based on the random sample of 54 interview transcripts we have processed to date, a total of 219 name instances were newly coined. The tallying of minted URIs showed that 25 out of 219 minted URIs referred to women, and 18 of those women were involved in the jazz scene in various capacities, from less prominent jazz musicians to people otherwise involved in the music industry (producers, managers, educators, etc.). While these individuals were mentioned in our collection of oral histories, they were missing from major encyclopedic knowledge bases, such as Wikipedia and MusicBrainz, as well as from massive repositories of bibliographic name authorities such as LC/NAF and VIAF.  The bare data encoding, which is provided by the URI syntax, revealed aspects of the data worthy of further investigation. An obvious finding was that women make up only a tiny percentage of the entities with new identifiers (11%). Even a cursory glance at the entire Linked Jazz dataset shows a significant discrepancy in the ratio between female and male jazz musicians. The missing URIs confirm this trend, but also suggests that the morphology of linked data can serve as a fertile clue and provide threads to new lines of inquiry through the lens of linked open data.  These accidental findings have triggered a new stream of development strategies aimed at supporting research in gender representation, an understudied area of jazz (Placksin, 1982; Tucker, 2000). By looking for patterns of the missing data, performing cross-references against interview mentions and conducting data analysis through different data facets including gender, roles, or professional relevance, we expect to be able to expand the array of questions that can be asked of the data. We are currently working to associate additional properties, starting with gender and instrument(s) played, to the person entities in the Linked Jazz dataset. This is achieved through data mashups with suitable sets of data from external sources. A key tenet of linked open data development, the integration of heterogeneous datasets is enabled by the design of RDF, a unifying data framework that relies on common modeling constructs such as URIs (Hendler, 2011). Data integration is the strategy that our project has adopted to add new layers of semantics to our set of triples and thus offer new opportunities for analysis and discovery. A dedicated RDF ontology is being developed to harmonize heterogeneous semantics and ease the process of integrating data. The ontology is created by expanding the set of predicates from the Linked Jazz dataset with predicates from selected data sources, resulting in a cohesive and expressive conceptual model. The data enrichment offered by mashing up datasets will bring together a wide range of information from temporal and spatial data (e.g., time periods, dates, events, geographic locations, etc.) to music-specific data (e.g., professional roles, instruments, recordings, music venues, etc.), opening up unanticipated strains of inquiry. A visual representation of a data integration use case is shown in Figure 1. It is based on a test of mashups centered on Ella Fitzgerald.     Figure 1. Data integration use case. Three domain-specific databases will be leveraged to perform mashups: (1) Columbia University’s J-DISC, 1 (2) Steve Albin’s BRIAN, 2 and (3) Carnegie Hall’s performance archive database. 3 The first two are open-source jazz discography databases 4 that provide granular information including session details with songs, tracks, dates, locations, and artist contributions from the composer to the sideman. The third contains rich and detailed information including types of performer (e.g., singer, instrumentalist, conductor, etc.).  As a condicio sine qua non, however, we have to associate the attribute  gender to the jazz musician names throughout the Linked Jazz dataset. Unfortunately, this won’t be a straightforward integration task as gender information, limited to binary  male and  female, is only recorded in J-DISC, which includes fewer than half of the artists present in BRIAN (and only 313 female jazz musicians). To supplement J-DISC data, we would need to make use of Wikipedia data via DBpedia. Specifically, we will leverage Wikipedia categories  women and  female as they are the only gender-related qualifiers applied to the jazz domain. The symmetric categories  men and  male are not specified, but assumed.   The heuristic capability inherent in the syntax of linked data and the data enrichment derived from knitting together linked open datasets have the potential to shed light on underrepresented contributors to the history of jazz and offer new perspectives to arts and humanities inquiry. Notes 1. http://jdisc.columbia.edu. 2. http://www.jazzdiscography.com/Brian/. 3. http://www.carnegiehall.org/PerformanceHistorySearch/#!. 4. More specifically, Steve Albin’s BRIAN is an application that allows users to create and publish their own discographies. ",
        "article_title": "Accidental Discovery, Intentional Inquiry: Leveraging Linked Data to Uncover the Women of Jazz",
        "authors": [
            {
                "given": "M. Cristina",
                "family": "Pattuelli",
                "affiliation": [
                    {
                        "original_name": "Pratt Institute, United States of America",
                        "normalized_name": "Pratt Institute",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/007m3p006",
                            "GRID": "grid.262107.0"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Miller",
                "affiliation": [
                    {
                        "original_name": "New York Public Library, United States of America",
                        "normalized_name": "New York Public Library",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02eysy271",
                            "GRID": "grid.429888.7"
                        }
                    }
                ]
            },
            {
                "given": "Karen",
                "family": "Hwang",
                "affiliation": [
                    {
                        "original_name": "Pratt Institute, United States of America",
                        "normalized_name": "Pratt Institute",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/007m3p006",
                            "GRID": "grid.262107.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "repositories",
            "museums",
            "creative and performing arts",
            "encoding - theory and practice",
            "GLAM: galleries",
            "linking and annotation",
            "networks",
            "art history",
            "including writing",
            "libraries",
            "archives",
            "semantic analysis",
            "analysis and visualisation",
            "ontologies",
            "digitisation",
            "digitisation - theory and practice",
            "relationships",
            "music",
            "spatio-temporal modeling",
            "English",
            "cultural infrastructure",
            "crowdsourcing",
            "visualisation",
            "data modeling and architecture including hypothesis-driven modeling",
            "philosophy",
            "metadata",
            "programming",
            "sustainability and preservation",
            "graphs",
            "gender studies",
            "resource creation",
            "knowledge representation",
            "and discovery",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Since 2013 the Migrant, Mobilities, and Connection project has consisted of a complementary and interlinked program of research and development between Curtin University (History of Migration Experiences, Sustainability Policy Institute, Perth), the University of Western Sydney (Digital Humanities Research Group), and the Huygens Institute for the History of the Netherlands (Royal Netherlands Academy of Arts and Sciences, The Hague). The backbone of both programs is informed by a digital framework that reconstructs the life courses of migrants through correlating data from both countries within the context of mutual cultural heritage policies and research objectives. The goal is to reconstruct a history that simultaneously intersects with Australia and the Netherlands through accessing or digitizing birth documents; death lists; shipping lists; passport requests; health clearances; alien registration; citizenship papers; school and business records; diaries and letters previously held only in state, regional, national, and international archives; consulates; and other governmental organizations. Indeed, our pilot study involves 51,525 emigration registration records from the National Archives, The Hague, which contain pre-migration demographic facts for over 180,000 Dutch emigrants over the period 1946–1982. This includes ‘hard facts’ on composition of family, dates of birth, addresses, religion, marital status, date of arrival, carriers, port of entrance, and emigration scheme, and ‘soft facts’ on profession. Migration is, by its very nature, a mutual heritage activity since all migrants leave documentary traces of their past in memory institutions from their country of origin or home, and move records of their present and future into archives and libraries of the receiving or host society. By maintaining, managing, using, and highlighting this heritage, we can foster a critical reflection on our shared pasts and acknowledge, integrate, and build awareness of the migration experience. This includes drawing attention to the right of dual belonging, the desire to find common ground and contribute to home and hostland identities, and the need to build knowledge about events that induced individuals—and refugees in particular—to leave their land. This paper will tease out the project’s progress within the context of how to best negotiate mutual heritage records in digital forms and enable access to the intersecting and interdependent histories they represent. Like any form of data migration, which conventionally involves the process of transferring data between storage types, formats, and computer systems, here we have the additional load of mapping old and new datasets into meaningful assemblages that ‘talk’ to each other. This includes not only the differences between the Australian and Dutch provenance of data but also the information that exists digitally and information that is yet to be digitized. How, for example, do we join data in the Australian domain with data in the Dutch domain, or information that is embedded in dusty community-based storage attics with names on the ‘Welcome Walls’ at the Fremantle Maritime Museum in Perth and the Australian National Maritime Museum in Sydney? We bump up against these issues because the solutions are not easy and require a step change in how we conceptualise the digital repatriation of mutual cultural heritage materials.  While much digital research in the library and information field deals with the explosion of user-generated content online, which creates new challenges and opportunities for libraries as custodians of data as well as objects, our project is faced with the exact opposite issue: not the problem of struggling to decide what data to collect and document from the ever-growing content generated by new media environments, but rather the equally tremendous volume of untapped cultural heritage materials and the infeasibility of libraries to collect and save it. Furthermore, from a digital humanities research point of view and with regards to the current spatial turn, around this arises a challenge over whether the ascendance of humanities-focussed GIS mapping techniques that emphasize the stories of a place and their geo-located attachment to said places—otherwise known as sited narratives or histories—can be meaningfully utilised in the context of mutual cultural heritage, in which the human subjects carry the inheritance of two or more worlds with them throughout their life courses. Migrating People, Migrating Data This short paper is part of two interlinked short papers that discuss the archival, custodial, and digital challenges that impact the discovery, collection, preservation, and content management of material and immaterial traces from the past that the Netherlands shares with Australia. (The other short paper is ‘Ruptured Life Courses: Institutional and Cultural Influences in Transnational Contexts’ by Marijke van Faassen and Rik Hoekstra.) In partnership with key institutional and community stakeholders, our pilot study on mobility between these two countries is about developing new understandings of the experience and representation of migration and how this has shaped an evolving sense of Dutch-Australian heritage—and with it, the consequences for the formation of cultural identities.  The deliberate play in the short papers’ linking theme, ‘Migrating People, Migrating Data’, is to signal the thematic content of the parent project, Migrant, Mobilities, and Connection—that is, the sociocultural material traces that append to the historical activity of people moving from one region to settle in another, in which the movement of bodies through space combines with information about their mobility through time. At the same time, it is also to signal the technical and conceptual challenges surrounding the consolidation of different data sources (both hard copy and digital) from a prior generation of technology to successive generations. For example, many Dutch community groups in both countries are actively collecting documents, artefacts, photographs, and maps to pass on to future generations. However, few have developed sustainable workflows to ensure the sustainability of their ‘collections’, and rarely are they familiar with cataloguing and metadata conventions that help describe an item’s provenance, role, and position in the world. Planning for digital preservation therefore is uneven, leading to concerns about a ‘digital gap’ in a community’s history. Mitigating the deleterious effects then of information loss and fading human recollection is an issue central to both the continued accessibility of cultural heritage materials and the digital preservation of historical knowledge beyond technology format lifetimes.  Project Members Dr Nonja Peters, Curtin University and University of Western Sydney Professor Paul Arthur, University of Western Sydney Dr Marijke van Faassen, Huygens ING Dr Rik Hoekstra, Huygens ING Dr Jason Ensor, University of Western Sydney  ",
        "article_title": "Developing a Sustainable Model in Mutual Cultural Digital Heritage",
        "authors": [
            {
                "given": "Nonja Ivonne",
                "family": "Peters",
                "affiliation": [
                    {
                        "original_name": "Curtin University, Australia",
                        "normalized_name": "Curtin University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02n415q13",
                            "GRID": "grid.1032.0"
                        }
                    }
                ]
            },
            {
                "given": "Jason Donald",
                "family": "Ensor",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "cultural studies",
            "historical studies",
            "repositories",
            "libraries",
            "archives",
            "museums",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "resource creation",
            "cultural infrastructure",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2003 Peter Robinson asked ‘where we are with electronic scholarly editions, and where we want to be’; more than 10 years later we may ask a similar question: Where are we, and where do we want to go? Rather than asking exactly this, however, I would like to address the question from a different angle—namely, what has been the impact of the digital in scholarly editing in more than 20 years of practice, and in which ways may this impact the future? This approach will allow us also to reflect in general on the impact of the digital in humanities disciplines more generally, by considering textual scholarship as a case study.  A first caveat on this analysis: textual scholarship is not really a discipline. One could call it an inter-discipline or a super-discipline: we edit, and do so with similar approaches, within many of what we consider to be the traditional disciplines, such as, for instance, history, English, French, German, theology, philosophy, and art history. Most disciplines in fact have to deal with old documents, and therefore they have to edit them to study and make them available for scholarship. In spite of the many national and disciplinary differences, there are similarities in all the approaches adopted; furthermore, all of these disciplines have been affected by the impact of the digital.  Within the past 20 years, all scholarly editions have in a sense become digital: all editions are in fact prepared on a computer; some use a very basic nonspecific setup such as word processor and standard image viewers; others are prepared with the help of sophisticated ad-hoc software, using markup and various combinations of scripts and algorithms. Let us consider the former case first. One cannot overestimate the impact of the availability of affordances such as copy and paste, digital images, and emails on the work and workflow of a textual scholar; these possibilities are transformational on the one hand, but they do not question the substantial heuristics of the textual scholarship—or not a great deal. The same cannot be said for those editors who have experimented with more innovative digital methods such as automatic collation, text encoding, and phylogenetics, among others; instead, the engagement with these methodologies brings into question some of the most established heuristics of textual editing, and it is claimed here that such innovations are building a methodological divide within textual scholars. Let us consider, for instance, automatic collation: this requires the editor first to transcribe all the witnesses of a given textual tradition in order to be able to compare them, a practice substantially unknown to traditional textual scholarship where collation is conducted by transcribing only one witness in full and then recording only the variants of other witnesses in various ways. Automatic collation is not an editorial method per se, but it is one first, necessary step of phylogenetic as well as other editorial digital methods (Andrews and Macé, 2013). The transcription of all witnesses is indeed a costly activity, and perhaps it is not the most exciting either, resulting in a much more detailed collation with respect to the one done manually; both aspects of this constitute a problem for the textual scholarship community: the transcription is stigmatized because it is resource-thirsty, and the (over)accurate collation because it does not distinguish between ‘proper’ errors, innovation, readings, and orthographic variants, with the risk, it is feared, of the reconstruction of an unreliable genealogical network of relations (Howe et al., 2012). According to Andrews (2013), however, the problem for traditional editors is not as much that variants and errors are not distinguished, but the fact that they refuse to consider that with the new digital method they might not need to be distinguished, since they refuse to consider the change in the heuristic of the discipline as worth having. A similar case happens with text encoding: while the use of text encoding is a new introduction to editing as a result of digital methods and it does seem to bring a different approach to the way we understand and edit texts (Cummings, 2008; Price, 2008), it is also refused as ‘too techie’, and the requirement for editors to learn the TEI seems beyond the reach of editorial work (Bree and McLaverty, 2009).  In spite of this skepticism and drawback that have so far prevented the majority of textual scholars from embracing digital methods, new generations of textual scholars are indeed trained in digital tools and are therefore understanding and editing texts in these new ways. Universities, research centres, and various networks organize one event after the other, where mostly graduate students and young researchers are trained in the use of TEI, collation tools, and other formalisms. One example of this trend is represented by the ‘Marie Curie’ DiXiT ITN; within this network, 12 Early Stage Career Researchers (PhD students, more or less) and five Experienced Researchers (postdoc) are being intensively trained in all aspects of digital editing. The network offers courses every six months for the whole duration of their fellowship; furthermore, such trainings have been offered openly also to people outside the DiXiT Network. Once the experience of DiXiT is over, the impact of these 17 researchers (to which we have to add all the external participants in the training and camps) in the academic market for textual scholarship could be very strong: we are talking about small numbers, but in a super-discipline such as textual scholarship that counts only handfuls of practitioners within each national and disciplinary community, that effect could be really transformational. It will also be interesting to see in future years how many of these newly trained digital editors will remain for the longer term in the traditional disciplines where they began, or if, because of this digital experience, they will end up more on the digital humanities job market instead. A survey of the career plans and aspirations of the DiXiT fellows conducted for this paper might give us some indications. What could be the consequences of this situation at a disciplinary level? The methodological gap (people who engage with digital methods and people who do not) will be magnified and enhanced by a generational gap; the result of this situation could bring textual scholarship to the breaking point. This is what happened in the field of linguistics: the impact of digital technologies has transformed the methods and heuristics of linguistics beyond recognition, to the point that a new label had to be created: computational linguistics. Is this the scenario that we envisage for digital editing? A similar disciplinary divide seems to be happening within paleography (Stokes, 2009) and doubtless can be also observed for, say, musicology and history, to name a few.  As I have argued elsewhere (Pierazzo, 2015, forthcoming), digital editing is not a new discipline—not yet, at any rate. It is certainly evident that in digital scholarly editing there are at least as many elements of continuity as there are of innovation. The innovative elements are truly transformative, involving as they do formats, methods, roles, heuristics, and hermeneutics of editing. Yet the purpose of editing remains the same—that is, the presentation of historical documents in ways that are meaningful for a group of users according to a documented and sharable methodology. For this, we may conclude that digital scholarly editing is a radical evolution (but not revolution) of print-based editing, as if in a Darwinian pattern of evolution a few steps have been jumped all at once. However, what is interesting to ask here is if the elements of continuity will be enough to keep the discipline together, so to speak, or will the innovative methods otherwise continue to widen the divide between traditional and digital? The answer to this question depends on the willingness of the traditional disciplines to engage with the new methods, but also from the attitude of digital humanists/editors to keep the dialog open with them. This is ultimately one of the biggest challenges of the digital humanities of the next few years: the relationship with the traditional humanities and their more or less peaceful coexistence. The paper will therefore present an analysis of the impact of the digital within scholarly editing using measurable indicators obtained by surveying the field, outlining patterns and trends that may help us assessing the consequences of such an impact. Such indicators include the number of early career researchers attending annual training events such as the DHSI and MMSDA, the number of digital-oriented papers at conferences like the ESTS and the STS in the past five years, and, in contrast, the number of print editions published by print publishers such as Cambridge University Press, Oxford University Press, Brepols, and Brill during the same period. ",
        "article_title": "Disciplinary Impact: The Effect of Digital Editing",
        "authors": [
            {
                "given": "Elena",
                "family": "Pierazzo",
                "affiliation": [
                    {
                        "original_name": "Université Stendhal Grenoble III, France",
                        "normalized_name": "Stendhal University",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/03yppfm65",
                            "GRID": "grid.440911.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "scholarly editing",
            "bibliographic methods / textual studies",
            "digital humanities - nature and significance",
            "English",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  A Practical Example: Mapping the 2007-2008 Financial Crisis The 2007-2008 financial crisis was a dramatically complex event and the political responses to this event were at least as complex. These responses can be studied thanks to a huge amount of documents produced by various bodies during the crisis and made available since then.  An American initiative aims at studying the response of the American authorities to the crisis through PoliInformatics, defined as “an interdisciplinary field that promotes diverse methodological approaches to the study of politics and government”  ( http://poliinformatics.org/ ). We participated in the first PoliInformatics challenge, as we describe in following.   The organizers of the challenge made available a series of documents on the 2007-2008 financial crisis. The shared task consisted in developing solutions to address questions such as “Who was the financial crisis?” or “What was the financial crisis?”. Of course, these questions are too complex to receive a simple and direct answer. So our strategy has been to provide tools to process and visualize the most relevant data, so that experts can easily navigate into this flow of information and make sense of the data. While we believe in semi-automatic corpus exploration, we do not think it is possible or even desirable to provide fully automatic answers to the above questions. We have as far as possible used available tools to extract and visualize information. More precisely, we have used the Stanford Named Entity Recognizer (Finkel et al., 2005) and the Cortext platform ( http://www.cortext.net/) for information extraction. As for data visualization, we have used Gephi (Bastian et al., 2009) to observe semantic and social networks, and the Cortext platform to observe the evolution of the domain over time. However, these tools are not enough to obtain meaningful representations: for example, new developments are necessary for named entity normalization and linking, esp. to link text with ontologies (Ruiz and Poibeau, 2015). The result should then be filtered following precise, domain-dependent criteria, so as to obtain navigable and readable maps with the most salient information.     Technical Overview   Named Entity Recognition and Normalization The first step was to extract named entities from the different corpora. Named Entity Recognition is a mature technology that has been used in several Digital Humanities projects (see Van Hooland et al., 2013 for a discussion of some recent projects). However, the use of NER in the analysis of political science texts seems to have been limited, e.g. Grimmer and Stewart’s (2013) survey of text analytics for political science includes no discussion of this technology.  In order to perform entity extraction, we used the Stanford NER, based on Conditional Random Fields, with MUC tags (Time, Location, Organization, Person, Money, Percent, Date) (Finkel et al., 2005). Certain entities appear under different forms. For instance, “Standard and Poor” might occur as “Standard & Poor”, “S&P” or “Standard & Poor’s executive board” (this last sequence in fact refers to a slightly different named entity); in a similar fashion, a person as “Mary Schapiro” may appear as “Schapiro”, or “Miss Schapiro” or “Chairman Schapiro”. We implemented a simple normalization method based on the maximization of common sub-sequence between two strings and obtained qualitatively good results when compared to other more sophisticated algorithms (Gottipati and Jiang, 2011; Rao et al., 2011). Entity linking could then be applied on the result.    Visualizing entities We used the Gephi software (Bastian et al., 2009) so as to create graphs for each corpus, such that:  a node corresponds to a cluster of persons or organizations in the corresponding corpus; an edge between two nodes corresponds to the number of co-occurrences of the two nodes within the same sentence in the corpus.  We chose to consider persons and organizations together since they can play a similar role in the event, and metonymy is often used, so that a person can refer to a company (and vice versa).      Figure 1: Visualization of links between entities with Gephi  Some results can be seen on figure 1 (which is hardly readable in small format but can be interactively explored by the experts in the field on a computer). Some links correspond to well establish relations like the link between an organization and its CEO (see for ex. the link between Scott Polakoff and OTS, or between Fabrice Tourre and Goldman Sachs). However, we are also able to extract less predictable links that could be of interest for scholars and experts in the field. As an example, we observe a link between the Fed Consumer Advisory Council and the Board of Governors (for ex. Bernanke, Mark Olson, and Kevin Warsh) since the first group of people (the council) warns vigorously the Board of Governors about the crisis. An interesting methodological issue to consider when elaborating these networks is that different automatic linguistic analyses can affect the complexity of the network: For instance, depending on the strategy to calculate intra-document coreference (e.g. whether we consider pronouns referring to an entity as an instance of the entity or not), the amount of edges in the graph will vary. Rieder and Röhle (2012) have discussed interpretation problems for visualizations, and Rieder (2010) has commented on how different graph layout algorithms can lead to representations that promote opposite interpretations of a network for the same corpus. We are as well interested in exploring how the computational linguistics tools employed in order to assess co-occurrences, even before applying a graph layout, can influence the graphs ultimately produced.    Visualizing temporal evolution The visualizations we produced should be explored and tested by specialists who could evaluate their real benefits. A historic view on the data would also be useful to analyze the dynamics and the evolution of the crisis, through for example the evolution of terms associated with named entities over different periods of time. We tried to explore and visualize the temporal evolution of the financial crisis, more specifically the evolution of the perceived role of organizations over time. To do so, we produced Sankey diagrams of the correlation of organizations and domain related terms in the corpus. With this strategy, Sankey diagrams take into account the temporal evolutions of entities and actions along the crisis.     Figure 2: Evolution of the links between named entities and topics over time  Figure 2 reveals a modification in the data between 2006 and 2008, a period which approximates the start of the financial crisis. For instance, the stream in purple in this graph reveals many co-occurrences of Fannie Mae and subprime loans for the period 1990-2007 while for the period 2008-2010, Fannie Mae is more closely associated with ’bank regulators’, or ’Federal Financial Services Supervisory Authority’. In a more general way, all the streams of data represented in the diagram are dramatically modified after 2007.     Evaluation and Future Work The work presented here has been evaluated by a panel of experts in the field. They assessed the utility of the tools and helped us define ways to improve these first results. Perspectives are thus twofold: on the one hand enhance data analysis so as to provide more relevant maps and representations, and on the second hand work closely with domain experts and provide interactive ways of navigating the data. Concerning interactions with experts, it is clear that end users could provide a very valuable contribution in the selection of relevant data as well as in the way they are linked and mapped. Some experiments are currently being done with a focus group gathering social science as well as information science experts. They will assess that the solution is useful and workable and more importantly, will give feedback so as to provide better solutions.   Acknowledgements This work has received support of Paris Sciences et Lettres (program “Investissements d’avenir” ANR-10-IDEX-0001-02 PSL*) and of the laboratoire d’excellence TransferS (ANR-10-LABX‑0099). Pablo Ruiz is funded thanks to a grant from the Region Ile-de-France.  ",
        "article_title": "Generating Navigable Semantic Maps from Social Sciences Corpora",
        "authors": [
            {
                "given": "Thierry",
                "family": "Poibeau",
                "affiliation": [
                    {
                        "original_name": "LATTICE-CNRS, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "content analysis",
            "natural language processing",
            "relationships",
            "text analysis",
            "information retrieval",
            "spatio-temporal modeling",
            "graphs",
            "analysis and visualisation",
            "English",
            "linking and annotation",
            "semantic analysis",
            "knowledge representation",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " When the Early English Books Online Text Creation Partnership (EEBO-TCP) was proposed by Mark Sandler of University of Michigan in 1999, the intention was to raise sufficient funds to produce accurate, marked-up, full-text transcriptions of 25,000 titles from ProQuest’s EEBO database of images. The main partners in the collaboration were the library at the University of Michigan, the Bodleian Library at the University of Oxford, and ProQuest—which, in return for making the images available to the EEBO-TCP partners, would receive a five-year exclusivity period to exploit the full-text data. Once that period elapsed, these 25,000 carefully produced texts would be released into the public domain for use by anyone in the global scholarly community and beyond. That was the dream. This paper will describe the practical realities and challenges that were faced to make this dream come true. On 1 January 2015, the 25,000 electronic texts produced by Phase I of EEBO-TCP were released into the public domain. They represented the concerted efforts of a large number of individuals employed at both Michigan and Oxford—more than 20 editors who, between them, had contributed over 100 person-years of work towards the objectives of this $9,000,000+ endeavour. In terms of scale and scope of its ambition and its outputs, the Early English Books Online Text Creation Partnership should be recognized as a seminal project in the development of the digital humanities. Perhaps one of the most interesting aspects of the EEBO-TCP was the mixed nature of its underlying funding model. The anticipated cost of $9M over five years was felt to be too substantial to appeal to any single foundation or funding body. Moreover, the formal collaboration with the commercial e-publisher, ProQuest, complicated matters further and, not unreasonably, raised questions about whether this could ever be a collaboration of equals.  In the United States, colleagues at Michigan enthusiastically promoted the benefits of individual institutions joining the EEBO-TCP—putting heavy emphasis on the benefits of partnership. Contribution levels were adjusted to allow institutions of different sizes to join the Text Creation Partnership on an equal footing; they could choose to contribute via a single lump sum (of $50,000 on average) or by five equal annual $10,000 payments. In return for their commitment to the work of EEBO-TCP, these institutions would gain immediate access to the textual resources as they were created—via both the commercial EEBO interface offered by ProQuest and also via a more tailored platform, built on the DLXS system developed at Michigan. They would also be contributing to the production of a corpus of essential texts, published in England between 1473 and 1700, which would subsequently be made available for the benefit of all. Despite increasingly constrained budgets, almost 150 US institutions paid to support the work of EEBO-TCP. In the United Kingdom, we were fortunate to have the Jisc (then known as the Joint Information Systems Committee), a nationally funded service dedicated to acquiring or funding the production of content, tools, and services that would be of widespread benefit to the UK’s scholarly community. The Jisc immediately saw the merits of EEBO-TCP’s innovative approach and committed a single contribution of £1,000,000 on behalf of the UK academic community. This decision also served the Jisc, as they were involved in negotiating the relicensing of ProQuest’s EEBO database to UK universities, and the added value of full-text searching for 25,000 of these important items considerably increased its appeal to library budget holders. The initial successes in fundraising meant that the production work of EEBO-TCP was able to begin in 2000. A keying specification was developed, and text conversion companies were invited to tender for the work. Titles were selected in light of suggestions from an international editorial board and also colleagues at the growing number of EEBO-TCP partner institutions. The digital images of the texts were sent to the chosen keying companies, and the results were subject to robust quality assurance and markup enhancement by trained teams of digital editors based at Michigan and Oxford. Texts that did not achieve the desired quality threshold were returned for rekeying, whilst those that met the standards were fed into a delivery workflow that ensured their timely appearance in ProQuest’s products, and also the TCP’s own delivery platforms. Everything was going to plan. But even the best-laid plans need to account for unanticipated issues and obstacles, and this paper will share the lessons learned from this major international collaborative endeavour. For example, whilst the overall production workflow worked extremely well—thanks to the careful oversight of key individuals at Michigan—there is no doubt that fundraising a work-in-progress raises some additional challenges; yet had we not adopted this approach, it is probably unlikely that we would ever have secured a significant majority of the necessary funding before beginning the work. In fact, having outputs that we could  show to potential partners as the work moved along in many cases helped secure their commitment and enabled them to clearly understand what we were aiming to do. Even so, the hard work of attempting to raise funds to achieve the target of $9,000,000, whilst spending a proportion of that money each month on text production, resulted in the work taking longer than anyone had originally envisaged. We met our production target of 25,000 texts—but it took nearly four years longer than we had planned!  In the course of our work, new questions began to emerge that we had not anticipated at the outset. In 2000, nobody asked us about the employment practices and ethical standards of the keying companies selected to work on EEBO-TCP material. By 2007, some institutions that were thinking about committing to EEBO-TCP, and even end-users of the materials, wanted reassurances that the digital data had been produced in an ethically acceptable environment. Moreover, with the growing awareness of the Google Books Library programme, some users began to question the legitimacy of ProQuest’s five-year exclusivity period to the full-text data, and understandably they wanted clarity on when that five-year embargo would elapse. EEBO-TCP had agreed that ProQuest’s exclusivity period would start from the end of the year in which production was completed (originally anticipated to be 2005), but because production was necessarily extended until 2009, the texts could not be released into the public domain until 2015. As the work of EEBO-TCP neared its end, other questions were also raised. The first and most rewarding for us as a project, was the request to carry on doing what we were doing: to produce  more texts. Whilst this was a clear demonstration that many people valued the work of EEBO-TCP, it also raised new questions about retaining and redefining our production methods and workflows, whether we could continue with the same funding model, how to select further texts, and so forth. It was tremendously rewarding to have the Jisc commit an additional £1,000,000 to EEBO-TCP ‘Phase II’ without hesitation—but this was 2008–2009, and we have undoubtedly been directly affected by the consequences of the global recession ever since.  Perhaps some of the biggest questions about the 25,000 texts produced by EEBO-TCP (‘Phase I’, as it is now known) are around what we meant—and what we now understand by—the term ‘public domain’. Back in 1999 we blithely assumed that we would simply release this corpus of material into the intellectual wilds, and that ‘the community’ would assume responsibility for their ongoing maintenance and enhancement. Nowadays, we are constantly asked to consider the sustainability of digital resources—and to define what this might mean, who will do the work, and most importantly, how it will be resourced?  At the time of writing this abstract, the texts from EEBO-TCP have not yet been released. That will not happen until 1 January 2015. Several leading individuals and groups from around the globe have already expressed an interest in working with some or all of the corpus; for example, they have put forward ideas for how the materials can be enhanced with additional markup, or corrections crowdsourced from a community of volunteers, or their contents integrated into scholarly editions. But we do not know which, if any, of these things will happen—or if the texts will be taken up and used in wholly unexpected ways by communities with which we have yet to engage. However, by the time of the DH conference in 2015, we will be in a position to reflect on both the 15-year build-up to the release of one of the most important collections of digital texts yet to be created, and to summarize what has happened in the six months since their release. Will they have been taken up and used in new and exciting ways, been picked up by just a few people, or been resolutely ignored?  Whatever the impact of the release of the 25,000 EEBO-TCP Phase I texts on 1 January 2015, there will certainly be important lessons to be learned by the global digital humanities community.  ",
        "article_title": "Text + Creation + Partnership: Whatever Happened to the Best Laid Plans of EEBO-TCP?",
        "authors": [
            {
                "given": "Michael",
                "family": "Popham",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "digitisation",
            "licensing",
            "literary studies",
            "project design",
            "digital humanities - nature and significance",
            "encoding - theory and practice",
            "and Open Access",
            "English",
            "management",
            "resource creation",
            "and discovery",
            "copyright"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Manuscripts that went through many editions being revised, augmented, or shortened from time to time are of special interest for historians, sociologists, and philologists. Identifying the text passages that have been changed and presenting them in a synoptic view or in critical editions plays a central role in these fields. In order to support the investigation of text genesis, appropriate information technology tools should be available. In addition to flexible and effective implementations for comparing variants of a manuscript (e.g., see Medek et al., 2015), the text differences found by such tools should be visualized in an appropriate way, allowing scholars to effectively navigate and explore the differences. With the  Colored & Aligned Texts view ( CATview) we present such an interactive visualization tool in this paper. Starting with a general map of the text witnesses’ aligned segments, scholars can search, find, and zoom to specific text passages of interest, which are colored according to their grade of revision or highlighted as striking, depending on the user settings. In collaboration with a synoptic presentation of the text variants,  CATview is a useful add-on both for web-based editions and frameworks for the editing process.  Motivation of the Work  CATview is motivated by an ongoing investigation of Abbé Raynal’s  Histoire philosophique et politique des établissements et du commerce des Européens dans les deux Indes, a text about the negative influence of European civilization during the colonization of the East and West Indies. The first edition was published in 1770 (Amsterdam). After the manuscript was forbidden, Raynal released expanded editions in 1774 (The Hague) and 1780 (Geneva). A last edition appeared post mortem in 1820; Book 6, for instance, consists of 52,372 words whereas the edition of 1770 features 28,451 words.  The work is part of the project SaDA—Semi-automatic Difference Analysis of Complex Text Variants 1 funded by the German Federal Ministry of Education and Research (BMBF). The visualization tool presented in this paper is embedded in an overall framework for text comparison that acts according to the following workflow. First, each of the given witnesses of a text is divided into segments, either paragraphs or sentences. Then, fingerprints of the segments are computed, allowing an assignment between them—that is, an alignment of the segments. Finally, the aligned segments are compared in detail and presented in a synoptic manner with apparatus.   CATview is an additional component built on top of this workflow to lighten the navigation within the synopsis and facilitate the investigation of text differences by preparing the collected data in a clear manner.  Previous Work The visualization of text differences and structure with aspects of navigation is already handled by several web-based tools. One example is the navigation bar of the Perseus Digital Library. 2 It illustrates the text structure by a set of iconic bars, e.g., for books, sections, or verses. The individual bars are divided into segments whose lengths indicate the lengths of the text passages they represent and to which they are linked.  The histogram view of Juxta 3 exceeds the pure aspect of navigation. This small pop-up dialog allows scholars exploring ‘the overall rate of change across the witnesses’ 4 of a manuscript, but only works for the comparison of one specific witness against others.  In Ben Fry’s remarkable visualization of ‘On the Origin of Species’ (2009), the text structure is abstracted for an overall view, and multiple witnesses’ revisions are visualized by colors. This is done in a fixed animation. At any point merely the latest revision is shown for each word. The text itself will pop up as dialog above the mouse cursor without a steady representation. Thus, the text cannot be searched, and frequent mouse moving is necessary for reading longer portions. Besides control elements for the animation there is no user interactivity such as zooming or further options to manipulate the presentation.  Features of  CATview  In the first place,  CATview illustrates the aligned segments in a tabular manner. Each text witness is represented by one row. Their segments are abstractly represented as rectangles. If two segments of different witnesses are aligned by step 2 of the workflow described above, they share the same column. The differences of the aligned segments found in the following step are visualized in an aggregated form by the intensity of the rectangles’ color. To lighten navigation, the rectangles are also links that will scroll the synoptic view to the corresponding segment when clicking on a rectangle. On the other hand the current position within the synopsis is represented in  CATview with a marking. Additional markings to highlight search results and further information with respect to the alignment can be displayed.  Illustrating the Alignment The presentation of the alignment in a tabular manner helps in reviewing the overall structure to see patterns of revisions (Figure 1). It is also useful to evaluate the alignment in order to improve the underlying algorithms as it calls attention, e.g., to falsely aligned segments. To see more details, the user can zoom in and out with the mouse wheel and slide the currently selected excerpt by dragging it with the mouse (Figure 2). Thereby, the option to change the order of the rows—i.e., the witnesses—and a consecutive numbering of the columns lighten the orientation. This basic functionality allows easy identification of relevant portions of text that have been added or removed during the authors’ revisions. Figure 2 illustrates this feature: one paragraph was added to witness H20 (which denotes the 1820 edition) between the aligned paragraphs at columns 59 and 61, where paragraphs of all four witnesses are aligned.    Figure 1.  CATviews’ general map of aligned paragraphs for the four witnesses of the  Histoire des deux Indes (book 6).     Figure 2. A smaller excerpt of the alignment seen in Figure 1 by using the built-in zoom. Coloring of Rectangles The intensity of colors assigned to individual rectangles denotes the degree of similarity between the corresponding aligned segments and can be determined, for instance, by the ratio of the segments’ number of differences to the amount of text. The color ranges from a standard light blue, which indicates similarity, up to a strong dark blue for rather extensive revisions of the text. This additional information effectively helps locating hot spots of revisions. Figures 3 and 4 show  CATview with enabled colors for two levels of zooming.     Figure 3. The general map of Figure 1 automatically enriched by colors for the paragraphs, based on the aggregated information gathered by a detailed difference analysis.    Figure 4. An excerpt of Figure 3 on a higher zooming level: columns in light blue express a strong similarity between the aligned paragraphs, whereas the dark blue at the columns 47, 51, and 54 indicate a strong difference. In column 54 a major revision first appeared in witness of 1780 with siglum H80. Benefits of Additional Markings Another feature of  CATview is to connect rectangles of different witnesses with a line if the corresponding segments are supposed to be similar but cannot be aligned due to a conflicting data situation. This situation occurs if the alignment is blocked by an assignment of other segments, e.g., in case of transposed or merged segments. The additional lines lighten the identification of such cases for a closer look.  Furthermore, the tool can display search results by highlighting single rectangles or full columns that contain the search phrase with a colored background. This feature helps in effectively estimating the distribution of a subject within all witnesses. Figure 5 illustrates the described markings.    Figure 5. Additional markings in  CATview: the bar at column 54 indicates the current scroll position in the corresponding synoptic view. Search results for the key word ‘Colomb’ (French for ‘Columbus’) are highlighted by a colored background (columns 43, 45, 47, 48, 58, 59, 61, and 62). The lines from 45 to 47 show paragraphs supposed to be similar that could not be aligned, as in H80 Raynal split paragraph 47 of H74 into two paragraphs (45 and 47) and added an additional paragraph in between.  Other Planned Features There are further features currently under discussion. The height of rectangles could present the size of the corresponding segments, displaying the original page numbers for each witness on higher zooming levels could lighten the orientation, and multiple aligned segments could be aggregated on lower zooming levels, to name a few. Technical Remarks  CATview is designed for web applications and implemented in JavaScript as a Singleton Object. Thereby the functionality of the JavaScript libraries D3.js 5 and jQuery 6 is used to generate and manipulate the SVG-Image, which contains the graphical elements described above. A publication of the finalized tool as open source is planned.  Summary In this paper we present an interactive visualization tool, a useful add-on for web-based editions and frameworks for the editing process, to effectively navigate and explore the differences of multiple text variants in a graphical overview. Despite its relative simplicity, our tool named  Colored & Aligned Texts view ( CATview) has universal applications in the field of text comparison according to the available data. Starting with a general map of the text witnesses’ aligned segments, scholars can search, find, and zoom to specific text passages of interest, which are colored corresponding to the grade of revision or highlighted as striking, depending on the user settings. In collaboration with a synoptic representation of the text,  CATview effectively supports scholars in the investigation of differences between several variants or the full genesis of a text, as well as improving the underlying algorithms for text comparison.  Funding This research was funded by the German Federal Ministry of Education and Research (BMBF) [grant number 01UG1247] as part of the project Semi-automatische Differenzanalyse von komplexen Textvarianten, under the direction of Professor Dr Thomas Bremer, Professor Dr Paul Molitor, Dr Jörg Ritter, and Professor Dr Hans-Joachim Solms. Notes 1. SaDA—Semi-automatische Differenzanalyse von komplexen Textvarianten, http://www.informatik.uni-halle.de/sada. 2. Perseus Digital Library, http://www.perseus.tufts.edu. 3. Juxta—Compare–Collate–Discover, http://www.juxtasoftware.org. 4. Juxta Commons, http://www.juxtacommons.org. 5. D3—Data-Driven Documents, http://d3js.org. 6. jQuery—Write Less, Do More, http://jquery.com. ",
        "article_title": "_CATview_ - Supporting The Investigation Of Text Genesis Of Large Manuscripts By An Overall Interactive Visualization Tool",
        "authors": [
            {
                "given": "Marcus",
                "family": "Pöckelmann",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "André",
                "family": "Medek",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Molitor",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            },
            {
                "given": "Jörg",
                "family": "Ritter",
                "affiliation": [
                    {
                        "original_name": "Martin-Luther-University Halle-Wittenberg, Germany",
                        "normalized_name": "Martin Luther University Halle-Wittenberg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05gqaka33",
                            "GRID": "grid.9018.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "scholarly editing",
            "content analysis",
            "interface and user experience design",
            "programming",
            "text analysis",
            "information retrieval",
            "french studies",
            "English",
            "internet / world wide web",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As the digital humanities gathers global impetus, with recent initiatives such as Global Outlook: DH and Around DH in 80 Days, seeking to extend the reach of the discipline beyond Anglo-American institutional portals (both virtual and architectural), it might be an appropriate moment to reflect upon what indigenous digital humanities forged in an Indian context might look like. What might it mean to have an Indian homegrown digital humanities? Why is it even a necessity? One obvious answer would include the assumed digital divide between the Western world and the developing world, but also we would argue, in response to the need for a ‘poor theory’—a term Ngugi wa Thiong’o coined in a 2012 essay—a theoretical framework that endeavours to ‘accord dignity to the poor as they fight poverty’. He elaborates: ‘Poor theory and its practice imply maximising the possibilities inherent in the minimum’. To those of us working in an Indian context, this is all too familiar, being borne out in practice by the Indian practice of  jugaad, the endeavour of creating ingenious solutions by yoking disparate technologies or parts together. In recent years, this form of informal iterative working has been gaining recognition in the West, often drawing parallels to hacking. The semantic differences of the two words, however, deserves close attention, alerting us to philosophical differences that are consequences of that digital divide. To ‘hack’ contains within it both the meaning of subverting the authority of proprietary systems through some sort of destructive action as well as to come up with a quick solution, whereas the aim of  jugaad is almost always constructive, often unaware of the capitalist systems it undermines, and truly born out of necessity. The practice and theoretical shape of the digital humanities thus far, almost exclusively determined by wealthy institutions in the global North, has failed to make space for such circumstantial necessity, and it is only with the Maker/DIY movement in the West that an alternative paradigm has been forged. The concept of minimal computing, for example, dwells on the dichotomy of choice versus necessity built on the understanding that computing resources in the developing world are not necessarily high performance, and that much can be done by streamlining low-cost single-board computers such as the Raspberry Pi for use in these contexts.   The inevitable lacunae formed by the absence of this awareness in the Western academy has meant that the discipline has been remarkably tone-deaf to the noise made by cultural criticism in the mainstream humanities post ’68—as Tara McPherson writes in a 2013 essay:  Much of the work in the digital humanities also proceeded as if technologies from XML to databases were neutral tools. Many who had worked hard to instill race as a central mode of analysis in film, literary, and media studies throughout the late twentieth century were disheartened and outraged (if not that surprised) to find both new media theory and emerging digital tools seem indifferent to those hard-won gains.  However, as the discipline matures, Alan Liu advocates that digital humanists should become sharper critics of ‘how the digital humanities advances, channels, or resists today’s great postindustrial, neoliberal, corporate, and global flows of information-cum-capital’. We believe that the local flavours of such analyses, as well as the particularities of our relationship with technology in India as exemplified by the philosophy of  jugaad mentioned earlier, necessitates a particularly Indian approach to this area of critical thinking and doing.   If, as digital humanists, our aims are to create resources that help us perform the act of cultural criticism and to study the objects of such criticism, we must accept that our vision is necessarily circumscribed by cultural specificity and particularity. These concerns operate both at the level of content and interface: for example, much humanities work in Indic languages has been, until relatively recently, impeded by the lack of optimised character recognition software. Katharina Reinecke’s seminal work on how national culture influences our perception of good design has shown how, for instance, Google struggled to get a foothold in the Korean market, as Korean users preferred much more colourful and graphically populated interfaces. This demonstrates how international technology players can be disadvantaged by their failure to tailor their offerings to local tastes and visual habits.  Our research into gaming in India endeavours to demonstrate how these local tastes, habits, and practices are shaped by global forces, but how local context still uniquely shapes digital culture—however homogeneous that culture itself might be—and how questions of access, infrastructure, economic and government policy, technological obsolescence, relevance, and lag might all contribute to more nuanced understandings of what it might mean to practice games studies and the digital humanities in the global South.  The Indian market is growing exponentially, especially with the undeniable popularity of mobile gaming. While India has long been considered a destination for outsourcing animation and visual effects work for games made abroad, the local industry is beginning to take shape. Indeed, the nascent, emergent Indian videogame industry with its growing player base is a marker of how digital technologies are shaping contemporary culture in this developing nation. However, videogames based on original intellectual properties created in India are still few and far between. Commentators have assumed that industry expertise combined with visual vibrancy, narratives rooted in myth and legend, and the multimodal richness of a certain sort of India, embedded in ethnic otherness, could result in a watershed moment for the industry. Indeed, in 2009, eminent game designer and commentator Ernest Adams felt that India’s lack of progress in the field could be rapidly compensated for by relying on adaptations of grand epic narratives such as the  Mahabharata or the  Ramayana.   While this promise is yet to be fulfilled, predictions have been heady and optimistic—with KPMG forecasting a growth of 17 billion INR by 2017, and NASSCOM (India’s flagship computing association) anticipating an eight-fold surge in casual gamers over the 2010–2015 period, with the vast majority (approximately 60%) playing on mobile, approximately 38% on PC, but a paltry 0.3% on console. However, there has been almost no academic research on the business of selling games, game design and how it might be shaped by its Indian origins, nor any rigorous analysis regarding the kind of games that are being enjoyed and played in the country.  To this end, we undertook an online (via surveymonkey.com) and offline survey of game developers and gamers, across sociocultural and linguistic communities in India (covering platform reach, importance of storytelling, gaming practices, problems of accessibility). SPSS, MAXQDA 10, and Wordle were the tools used to yield quantitative and qualitative data analyses of the total of 185 responses, which mostly came from major Indian urban centres. The paper will showcase our findings, which include a range of anecdotal responses to open-ended questions as well as empirical data. The scope can certainly be enlarged, and the final paper will reveal how the data sample is indicative. Our research on videogames very usefully addresses the conference’s theme of ‘global’ digital humanities as it is possibly the most international of contemporary media forms, and as our data demonstrates, individual engagement with such a phenomenon is always mediated through its local context but inextricable from its global condition—and we feel these are significant revelations for the future of storytelling, narrative, and digital cultures. ",
        "article_title": "Press F6 to Reload: Games Studies and the future of the Digital Humanities in India",
        "authors": [
            {
                "given": "Padmini",
                "family": "Ray Murray",
                "affiliation": [
                    {
                        "original_name": "University of Stirling, United Kingdom",
                        "normalized_name": "University of Stirling",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/045wgfr59",
                            "GRID": "grid.11918.30"
                        }
                    }
                ]
            },
            {
                "given": "Souvik",
                "family": "Mukherjee",
                "affiliation": [
                    {
                        "original_name": "Presidency University, Kolkata",
                        "normalized_name": "Presidency University",
                        "country": "Bangladesh",
                        "identifiers": {
                            "ror": "https://ror.org/03vzjn737",
                            "GRID": "grid.443025.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "other",
            "cultural studies",
            "interface and user experience design",
            "digital humanities - nature and significance",
            "English",
            "games and meaningful play"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Media artists are increasingly concerned with creating intense audience experiences or immersive works that are opportunities for participants to engage with spaces, mobility, and embodiment. This research paper asks what it is to  move audiences in mediated environments and offers understandings and insights into creating media art for audiences now attenuated to intense embodied experience in everyday life. Along with illustration of a practice-led component—an installation called  grove—this research paper provides an emphasis on different key concepts for analysis of current and future media art installations. Through the artwork  grove and by analysis of other exemplars, the research argues for a schematic sequence by which a participant can engage with an ecology.   Specifically the research argues for the centrality of movement in perception and in participant engagement in mediated environments by drawing on current and attenuated theories of affordance and affect. It explains how we perceive  through movement—that is, difference and change—in always changeful environments that can be understood as ecologies, comprising flows of information that afford us stimulation across multiple modalities. Movement is core to triggering affects, the precursors to feelings, actions, and cognitions. This research paper hypothesises that to  move an audience in media art installations is to afford them opportunities for affective actions and reactions in a dynamic environment with which they are one—in short, an ecology. Over time, ‘moving’ experiences become significant events, accumulating and predisposing us to similar responses.   The hypothesis was explored in the creation of a major work called  grove, a 10m x 10m installation comprising a dynamic light array in the form of a grove of moving, programmed lights, set in a moat of reflective darkness. Documentation of the installation will be used in the paper. The grove itself is animated by the moving lights in a 25-minute sequence that engages light with light, exploring their characters, diameters, effects, and colour, and which also plays on the grove’s terrain molded from black rubber crumb, criss-crossed by paths of glittery salt. The choreography of light-as-media is rhythmic and gently changeful, exploring the inherent materiality of light, its ‘faktura’—reflectivity, transparency, colour, volume—and their  others: absorption, opacity, and void. Like all groves, the installation space is literally and metaphorically animated and made sacred by light.   The installation aimed to provide audiences with an immersive and contemplative space of refuge from the urban hubbub. Accordingly, it drew on minimalist aesthetics. Light is the medium; its materiality and variables provide the means to animate it. There is no content per se, other than the signification of the metaphor of the grove. By eschewing content and ostensible interactivity and with its minimalist palette (of light and its antonym, darkness),  grove invites visitors into a quiet ecology in which to dwell and be immersed in these core registers of visual and spatial perception:  grove affords audiences a sense of interiority.   The research demonstrates how  grove and other media arts installations can be analysed and interpreted as heterotopia—bounded spaces where time, inclusion, and sense of place are altered. The research argues that heterotopia provides a historic and cultural framework for understanding the capacity for changed relations within  and between audiences. By drawing on contemporary theories of heterotopia as sites of flow, this research provides a nuanced argument for media arts installations as ecologies of flows of opportunities for perception, and embodied and affective engagement. In short, this practice-led research hypothesises that to  move an audience is to afford them affective responses in an ecology of flows with the audience as interdependent participants in that system, integrated across various sensory modalities and physiological scales. The research also explores the rich cultural metaphor and metonymy of the grove, a bounded yet semi-permeable space full of potential aesthetic delights—an archetypal soft architecture that draws one in and offers a reflection space of high cultural value.  ",
        "article_title": "Grove - Creating An Ecology of Flows",
        "authors": [
            {
                "given": "Kate E.S",
                "family": "Richards",
                "affiliation": [
                    {
                        "original_name": "UWS, Australia",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "interface and user experience design",
            "video",
            "audio",
            "English",
            "multimedia"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this work, we use network analysis methods to sketch a typology of fiction novels based on characters and their proximity in the narration. We construct character networks modelling the 20 novels composing  Les Rougon-Macquart, written by Émile Zola. To categorise them, we rely on methods that track down major and minor characters relative to the character-systems. For that matter, we use centrality measures such as degree and eigenvector centrality. Eventually, with this analysis of a small corpus, we open the stage for a large-scale analysis of novels through their character networks.   Character Network Analysis  A character network is a model of a novel’s plot focusing on a single dimension among the different types of narrative entities—that is, the  character or, at the level of the whole novel, the character-system:   [. . .] the arrangement of multiple and differentiated character-spaces—differentiated configuration and manipulations of the human figure—into a unified narrative structure. (Woloch, 2003, 14]) Characters are represented in the network by nodes. The relations among them are determined on the basis of their proximity in the narration: if two characters appear side-by-side more often than a given threshold, then a link (i.e., an edge) is created between them in the network (Rochat, 2014). If two characters never appear close together, or not significantly enough according to the defined threshold, then they are not linked in the character network.  As examples of existing research, Franco Moretti explored the narrative importance of a character by comparing some features of a character network before and after deletion of the said character (Moretti, 2011), while Mac Carron and Kenna (2012) extracted the structures of three mythological works ( Beowulf,  Iliad, and  Táin) and compared them one to another and to real social networks, concluding that they were ‘discernable from real social networks’ and eventually proposing to rank them ‘from the real to the fictitious’ (5).   Les Rougon-Macquart The novels constituting  Les Rougon-Macquart were published between 1871 and 1893, starting with  La Fortune des Rougon and ending with  Le Docteur Pascal. They cover a historical period going from 1852 to 1870. In these, Zola arranged a society of fictional and real characters in dissimilar ways, once focusing on a single character, and at other times dividing the attention between a few complementary protagonists, along with other characters recurring from one novel to another:   I wish to explain how a family [. . .] conducts itself in a given social system. [. . .] I shall endeavour to discover and follow the thread of connection which leads mathematically from one man to another. (Zola, 1967 [E. A. V. Merton, trans.]) In his study of  Les Rougon-Macquart’s character-systems, Philippe Hamon writes that some novels have a main protagonist, while others have more than one protagonist:   Polyfocalisation of the system on a few heroes—rather than unfocalisation, which alternately shares the ‘hero spots’ of the system—polyfocalisation of which  Pot-Bouille,  La Bête Humaine and  La Débâcle are the best examples, processes issued from a network made of marked ‘nodes’ and interstitial light layers, which take distance from a fixed ‘pyramid-like’ hierarchy (a hero, secondary and marginal characters, etc., according to a non-adjustable scale) of classic works. (Hamon, 1998, 320, my translation)  We propose a mathematical formalism to study these questions in the section on typology. The index of  centralisation measures how centralised the network is, i.e., how much more central the most central character is compared to all the other characters, ‘central’ being an open concept thus far. Then,  coreness highlights who the characters at the center of the narration are.  The Index  In order to construct the character networks, we consider an index built on the whole series (Zola, 1967, 1795–884), for which the indexer detailed his/her choices. It is a table compiling the occurrences of characters, from which we extract the co-occurrences that lead to the determination of the sets of edges. Contrarily to an automatic extraction process, we can rely here on the professional work of scholars, which provides exact positions at a page-level by disambiguating characters cited by nicknames, pronouns, or multiple names.  The index contains supplementary information, from which we use the novel names (characters frequently appear in more than one novel) and the characters’ descriptions to distinguish characters with the same name—for example, the six different characters named ‘Rose’.  Eventually, we transformed the index into a table composed of 40,768 entries, each one of them having three attributes: name of character, name of novel, and page. The table contains 1,343 unique characters and 7,290 unique pages.  The Networks  The table is then divided into 20 smaller tables, each one corresponding to a novel. We apply the method developed in Rochat (2014) to include co-occurrences on overlapping pairs of pages in order to take characters appearing in the same sentence but on different pages into account when creating the edges, since they need to be linked together. We build bipartite networks from these tables, with one set of nodes composed of the characters and the other set composed of the pages. Then we compute the graph projections on the sets of characters to obtain the characters’ networks, shown in Figure 1 (see Fruchterman and Reingold, 1991, for the layout algorithm).     Figure 1. The character networks of the  Rougon-Maquart’s 20 novels.  The character networks show significant diversity (Table 1). The number of nodes (i.e., the  order) varies from 16 to 88, and the number of edges (i.e., the  size) from 68 to 1,181. Works like  Le Rêve and  La Faute de l’Abbé Mouret feature few characters and relations: this is consistent with their intimate subjects. In comparison,  Pot-Bouille,  Au Bonheur des Dames, and  Germinal feature many characters and relations: they are composed of a rich crowd along with narrative events involving many characters.      Table 1. Basic network properties.  The  density of a network is the ratio of the number of existing edges by the number of all possible edges. Low density implies that the characters are sparsely connected, while high density means that the characters are more intricately connected to each other. In our case, this property can be used for categorisation, since large ( La Débâcle) and rather small ( La Fortune des Rougon) character networks obtain small density values. However, large density values can also be attained by large ( Germinal) as well as the small ( Le Rêve) character networks.  Typology Based on Major vs. Minor Characters  In this section, we develop two ways to categorise character networks by exploiting the distributions of major and minor characters. The first one consists in studying  centralisation, a global measure based on the centrality measures of all the characters, while the second one measures the  coreness of the network—that is, the size of a particularly dense subgraph that we view as a core of protagonists of the network.  Centralisation  Centrality is a wide concept mathematically expressed by families of measures reflecting particular properties of the network under study. For example, degree is one of them. Here, we use in particular betweenness centrality: it measures how much a character acts as an intermediary at the level of the network. Betweenness centralisation is the global network measure based on betweenness centrality: we sum the differences between the maximal betweenness score and each node’s betweenness score, and then divide it by the theoretical maximal sum (Freeman, 1979). A centralisation index returns a value located between 0 and 1: a value close to 0 means that there is no node playing a central role (e.g., a ring graph), while a value close to 1 implies that there is a centralised structure (e.g., a star graph).  We observe the scores in Table 2: most of the networks have low betweenness centralisation. However, those that rank first are significantly more centralised:  L’Oeuvre, L’Argent, Le Docteur Pascal, and  Son Excellence Eugène Rougon have one and only one protagonist (the main character of  L’Argent appears on every page), and  La Débâcle is the story of two men at the front and their strong friendship.   Table 2. Centralisation scores.  Coreness  In order to delimit the  core of the network (in opposition to the  periphery), we consider the notion of  k-core (Seidman, 1983; Csardi and Nepusz, 2006), that is, the maximal induced subgraph with all its nodes having a degree equal or superior to  k. Normalised by its respective network order, the highest possible  k value in a network is a measure of how compact the main group of characters is. We call it  coreness.   Results are shown in Figure 2, plotted with the networks’ orders.  La Faute de l’Abbé Mouret’s character network is composed of a very dense component consisting of more than half the total number of characters. We remark that among the three ‘polyfocalised’ novels noticed earlier by Hamon, two of them ( Pot-Bouille and  Germinal) have high values of coreness, meaning that the central and prominent characters are well connected among them among themselves and act as interchangeable figures. However, for the third one,  La Débâcle, the coreness is low, suggesting that having strong protagonists in a sparser network diminishes the strength of the core of protagonists.      Figure 2. Coreness. Conclusion In this work, we have shown a descriptive approach to compare character networks. Our results show that it is possible to discriminate them. By iteration, the comparison of character networks leads to the analysis of large numbers of character networks. ",
        "article_title": "Character Network Analysis of Émile Zola's Les Rougon-Macquart",
        "authors": [
            {
                "given": "Yannick",
                "family": "Rochat",
                "affiliation": [
                    {
                        "original_name": "EPFL, Switzerland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "relationships",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the first months of the HUMANIST discussion list a discussion erupted around the value of programming languages in humanities computing courses. Do computing humanists need to know how to program, or is learning expert use of applications enough? What is the point of learning to program? (McCarthy, 1987). This discussion can be traced back at least as far as the 1986 ACH/Sloan Workshop on Teaching ‘Computers and the Humanities’ Courses, where participants discussed ‘most vigorously of all whether programming should be taught, or only package programs’ (Sperberg-McQueen 1986). The issue has really never gone away as it is about disciplinary formation or ‘what it is we want our students to learn, the nature of the world into which we are sending them, and the relationship both of technology and (more fundamentally) the algorithmic approach to problem-solving’ (McCarty, 1987, 1–2). Based on a reading and analysis of a substantial collection of historical documents (journal articles, association newsletters, books, Listservs, etc.) in this paper we will examine how the humanities computing community discussed programming and how that discussion reflects changing views of the field over time. Specifically we will  • Outline a three-part history of programming in the digital humanities starting with a concording phase when programming was not important to using computers.  • Discuss the turn toward teaching programming as a way of teaching computational thinking.  • Look at how the emergence of the Web changed the discussion. We will argue that as the Web became important, scripting skills suitable for building websites replaced older programming languages and it became possible to imagine the digital humanities being defined by the ability to program.   • Conclude with some reflections on how the case for programming has been made more recently.   Humanities Computing as Concording  I’ve tried to stay free of programming. I’m perfectly innocent of any knowledge of any programming language and I feel I must remain that way if I am going to continue to function as a scholar in literature; my heartfelt advice to any of you younger people embarking on this whole venture is to do the same—to ask programmers to do for you what they know how to do and what would be costly and painful for you to learn—to learn to talk their language but not to get involved in programming research or machine research. I know the best of you will not take this advice and will, therefore, break new barriers and so on, but I persist in offering it. (Parrish, 1969, 24–25)  Programming in the early concording years of computing in the humanities was something often left to specialists. Stephen Parrish, who was general editor of the Cornell Concordance Series, saw himself as a ‘scholar in English literature who drifted laterally into the making of concordances’ (1969, 16). He stayed away from programming as a distraction from scholarship. This meant that concording ‘required an organized, teamwork approach, characterized by a substantial budget and a university computing centre’ (McCarty, 1993, 52–53). Parrish is not the only one to worry about programming distracting scholars, and in the presentation we will discuss other examples as a way of teasing out how programming was not considered of scholarly value.   Programming and Reasoning  By the 1980s, things had changed and programming had become important, as can be seen in the way programming was included in a majority of courses. Surveys like Joseph Rudman’s ‘Teaching Computers and the Humanities Courses: A Survey’ (1987) report 175 courses teaching a programming language while 131 were ‘Applications Only’. In that survey, the most popular languages taught were BASIC (in 75 courses reporting), Pascal (30 courses), Prolog (15 courses), Lisp (13 courses), and SNOBOL (10 courses). What is impressive is the number of responses that indicated they had some sort of course for humanities students. We often assume that computing in the humanities is a recent, post-advent-of-the-Web thing, but given the number of courses, the discussions on HUMANIST, and articles in journals like  Computing and the Humanities, it is clear that programming was becoming an acceptable activity, especially for those in support staff positions.   Despite the popularity of programming languages like BASIC and Pascal for courses, the languages about which humanists wrote in the 1980s tended to be languages like SNOBOL and Icon—all languages that were good for string (text) manipulation, as can be seen in the attention they received in books for teaching programming to humanists like John Abercrombie’s  Computer Programs for Literary Analysis (1984) , Susan Hockey’s  SNOBOL Programming for the Humanities (1985) , and Alan Corré’s  Icon Programming for Humanists (1990) . Then there are the discussions on HUMANIST and reviews of languages like Mark Olsen’s self-explanatory ‘Beyond SNOBOL: The Icon Programming Language’.   As an aside, one could argue that SGML (Standard Generalized Markup Language) and later XML (Extensible Markup Language) are also forms of programming—meta-languages with which one can create descriptive languages with which to rigorously describe texts for scholarly electronic editions. These were popular in humanities computing, especially after the Text Encoding Initiative began to provide guidelines for the encoding of texts in the late 1980s. How many humanists were first introduced to computing in the humanities when asked to develop a DTD (Document Type Definition) and encode a text?  It is interesting that many of the discussions about programming in the 1980s and early 1990s circle around the teaching of it and that this issue is reported as contentious. No one believed that the ability to program was essential for a computing humanist, in part because so many couldn’t, but proponents of programming argued for it to be taught as way of teaching computational reasoning or problem solving. They also argued that there were social and ethical issues that could be understood through learning computing. You learned to program so as to understand what the computer could do or to be able to talk to programmers. In the presentation we will look at summative discussions of the issue like McCarty’s (1987) and a lovely balanced essay by Nancy Ide, ‘Computers and the Humanities Courses: Philosophical Bases and Approach’ that came out of the 1986 ACH/Sloan Workshop. We will also speculate as to why there was this shift towards including programming and arguing for its importance. Whatever the reasons, the discourse had changed and computing humanists were beginning to take an interest in programming and teaching it.   Programming the Web   The emergence of the Web changed the languages that humanists were likely to use for programming and, we will argue, made it possible to make programming a defining skill for digital humanists. Moreover, learning to program for the Web provided a more appealing and transferable skillset; web and application development became normalized and desirable. Because the Web provided such a convenient way to show and distribute digital research, it changed which programming languages received attention or were taught. We can see the shift in these histograms of word frequencies in the HUMANIST Archives. The string processing languages popular in the late 1980s taper off and are replaced with languages like PHP with which one can build dynamic websites.        Figure 1. Two graphs showing the frequency of programming languages in the annual archives of the HUMANIST Discussion Group Listserv, produced with Voyant Tools.  Now programming (and other web skills) went from being about teaching reasoning to being a skill students and humanists could actually use to create humanities products, i.e., websites. Programming became creative and expressive. There was also a convenient on-ramp as students could start creating HTML pages and then learn to use scripting languages like PHP that enhanced the web site until you knew enough to code a scholarly web site.  We will further argue that once web development wasn’t something that digital humanists would necessarily be amateurs at, then it could become a defining skill, and a source of some pride. Digital humanists finally had something that they could be good at and a set of competencies unique to the digital humanities. Digital humanists like Stephen Ramsay could provocatively say, ‘Do you have to know how to code? I’m a tenured professor of Digital Humanities and I say “yes”’ (Ramsay 2011). This led to the ‘hack vs. yack’ discussion that followed about programming and other forms of building (see Nowviskie [2014] for more general context).   Conclusions: Programming in the Humanities and Disciplinary Formation   This paper traces the ways programming has been discussed in the digital humanities. We believe that the role of programming was important to the way the field of humanities computing (and later digital humanities) conceived of itself. This is not surprising given the importance of programming to computing in general, but it is interesting to follow the particular ways programming was discussed as the discipline emerged.  We will end the paper by theorizing, or at least speculating, about where programming in digital humanities is going. One direction (already well under way) is towards software studies and the studying of programmed artifacts as works of human art and expression worthy of the humanities. Matthew Kirschenbaum in a fine essay for the  Chronicle of Higher Education talks about ‘procedural literacy’.   All programming entails world-making, as the ritual act of writing and running Hello World reminds us. . . .   Ultimately what’s at stake is not the kind of vocational computer literacy I was taught as an undergraduate, but what a growing number of practitioners in the digital humanities (and related disciplines, like digital art or game studies) have begun to call procedural rhetoric, or procedural literacy. (Kirschenbaum, 2009)  Another direction is data science. The opportunities for new insights through large-scale text mining, or what Moretti (2007) calls distant reading, have made programming languages like R attractive. There is a return to text processing languages, but now languages that can analyze large corpora or visualize results. ",
        "article_title": "Talking About Programming in the DIgital Humanities",
        "authors": [
            {
                "given": "Geoffrey",
                "family": "Rockwell",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Stéfan",
                "family": "Sinclair",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "history of Humanities Computing/Digital Humanities",
            "English",
            "programming"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper examines the use of ‘visual analytic’ techniques to scrutinise and refine current approaches and algorithms for the identification of text reuse and alignment. In particular, we compare text alignment approaches using the ViTA (Visualization for Text Alignment) system we have developed in the context of a Digging into Data project that seeks to identify 18th-century commonplaces in large-scale historical datasets. By leveraging the flexibility of image processing techniques to visualize text alignments in a variety of ways and using multiple parameters, we can improve upon current alignment algorithms in an iterative and integrated process of automated text mining, multivariate visualization, and human-computer interaction.  * * * Recent scholarship has demonstrated that the various rhetorical, mnemonic, and authorial practices associated with Early Modern commonplacing—the extraction and organisation of quotations and other passages for later recall and reuse—were highly effective strategies for dealing with the perceived ‘information overload’ of the period, as well as for functioning successfully in polite society (Yeo, 2001; Blair, 2001; Allan, 2010). But the 18th century was also a crucial moment in the modern construction of a new sense of self-identity, defined through the dialectic of memory (tradition) and autonomy (originality), the resonances of which persist long into the 19th century and beyond (Dacome, 2004; Bénichou, 1996). In the context of a larger Digging into Data Round 3 project, we are examining this paradigm shift in 18th-century culture from the perspective of the commonplace, a nexus of intertextual relationships that we aim to explore through the concerted application of sequence alignment algorithms and large-scale visual analytics.  In our previous work on text reuse (Horton et al., 2010; Allen et al., 2010), we came across numerous examples of textual borrowings and shared passages that exhibited the characteristics of commonplaces. However, traditional text mining and sequence alignment approaches for the identification of text reuse in the sort of large-scale data repositories that interest us entail no small amount of methodological challenges. For this reason, data visualization is a crucial element to this project, and will be used not merely as a means of presenting the end results of text mining but rather as an analytic approach to data at all points in the text-mining process: from the initial stages of data exploration and hypothesis evaluation, to user-driven feedback for refining text-mining rules and parameters, to generating new hypotheses and questions previously unseen in the data. This type of methodological approach is known as ‘visual analytics’, and this paper outlines our attempts to leverage image processing techniques to refine our previous text-mining assumptions, practices, and rules.  The concept of visual analytics was first proposed by Jim Thomas and his colleagues at the National Visualization and Analytics Center in 2004 (Thomas and Cook, 2005). It has become the de facto standard process for integrating data analysis, visualization, and interaction to better understand complex systems. The concept rests on the following assertions (Chen et al., 2011):   • Statistical methods alone cannot convey an adequate amount of information for humans to make informed decisions—hence the need for visualization.  • Algorithms alone cannot encode an adequate amount of human knowledge about relevant concepts, facts, and contexts—hence the need for interaction.  • Visualization alone cannot effectively manage levels of details about the data or prioritize different information in the data—hence the need for analysis and interaction.  • Direct interaction with data alone (e.g., search by queries) is not scalable to the amount of data available—hence the need for analysis and visualization.  ViTA: Visualization for Text Alignment  In order to benchmark and test the boundaries of our current approach to text alignment, we have developed a visual analytic system named ViTA: Visualization for Text Alignment. By comparing output from our alignment system PhiloLine 1 to output from the ViTA system, we can visualize text alignments on a dot-matrix image—alignments that might have been missed and/or misrecognized by our current n-gram-based approach. Using the information gained from this interface, we can then begin to construct a set of refined alignment rules that incorporate these new findings. For example, in Figure 1 we see two alignments found by the PhiloLine system.         Figure 1. The red output was detected by PhiloLine as an alignment. The blue represents what might also be considered a match, but that the algorithm, for various reasons, missed. Given the strictures of our current n-gram matching model, isolated and highly frequent words (such as ‘et’ and ‘en’) are not be considered as part of potentially larger alignments. Using the ViTA visual analytics system, however, our capacity for detecting a matching pattern on a visual plane helps us identify the gaps in these longer sequences and eventually help in fine-tuning the matching algorithm. If we start with simple word-to-word matching, here is what the ViTA output for the above passages looks like, where matching words from each text (the horizontal and vertical axes) are plotted as single pixels in a dot-matrix (Figure 2):        Figure 2. ViTA output: Any word with default settings. We can clearly detect the matching sequence, but the gaps in the alignment show the limits of one-to-one word matching. If we change the system’s matching parameters to look only for trigrams, we allow for more variance within each match (Figure 3):        Figure 3. ViTA output: Any word; +1 word on each side; minimum score of 3; image darkening >.4. We see that much of the noise has been removed, which is to be expected as matching trigrams are relatively rare. The greyscale seen here accounts for n-grams that only have two tokens in common. While this has helped clear the image of unrelated matching unigrams, it still does not constitute the clear alignment that one sees when reading the passages. If we loosen our matching further to look for pentagrams, while also allowing for some variance in the content of matching n-grams, we can allow even more variance in n-gram matching: e.g., two tokens out of five tokens matching still constitutes a match, though not as strong as a true 5-gram, and is therefore represented in a lighter shade of grey (Figure 4):        Figure 4. ViTA output: Any word; +2 words on each side; minimum score of 5; image darkening > .2).  As you can see, the alignment has stretched even further, with very little additional noise. If we try to loosen our parameters further, and use heptagrams, we can allow an even greater variance in the n-gram matching. Since heptagrams will be extremely rare, partially matching heptagrams above a certain threshold will be displayed in a lighter shade of grey (Figure 5):        Figure 5. ViTA output: Any word; +3 words on each side, minimum score of 7; image darkening > .2.  What these examples demonstrate effectively is that the combination of an improbable hypothesis, such as using heptagrams for sequence alignment, with image processing tools such as darkening (a threshold determining what should constitute a match, and shading the strength of the match accordingly) can in fact provide longer visual alignments, regardless of the various gaps that could separate matching patterns. In other words, this is a perfect example of how visual analytic tools can build upon our human capacity to discern hard-to-find matches visually and then leverage this information to refine our algorithms.  This is equally true for smaller matches, or alignments that would have otherwise been missed due to certain trade-offs between match resolution and computing efficiency. One such trade-off is the elimination of high-frequency and short stopwords at the pre-processing stage of PhiloLine alignments. The assumption here is that important and relevant matches are made primarily on content words like nouns or proper nouns. When dealing with dirty OCR data, however, the challenges of pattern matching are quite different, as matches are often missed because these content words have not been properly recognized by the OCR engine. Below is an example of two shared passages that our text alignment system missed (Figure 6):        Figure 6. Dirty OCR matches that were missed by text alignment system. Using the ViTA system, we can see precisely why these matches were missed by matching only on words longer than two characters (left) and/or by looking for matching trigrams (right) in Figure 7:        Figure 7. Matching unigrams and trigrams with small words removed. As we can see in both cases, the matching pattern is very small and would normally be dismissed as insignificant. If we leverage the flexibility of the visual analytic system, however, we can reintroduce stopwords into the mix, extending our matches to both any matching word (left), and then to matching partial trigrams (right) (Figure 8):        Figure 8. Matching all words and partial trigrams.  The above alignments demonstrate forcefully that many of our text-mining assumptions, such as the removal of stopwords before matching, may not hold true when using image-based processing of text alignments. These findings suggest that further analysis will enable us to define and redefine our matching algorithms as we proceed.  Conclusion  To conclude, we assert that the identification of commonplaces is a non-trivial intellectual undertaking. Technical challenges arise not only with the size of our data, but also with the multifaceted nature of computational rules designed to capture the diverse aspects associated with commonplaces. These aspects may include, but are not limited to, word occurrence and co-occurrence, grammatical structure, linguistic properties, etc. Thus, to compile a global commonplace book computationally, one needs efficient and effective means for identifying appropriate rules, determining appropriate parameters, verifying text mining results, and most importantly, gaining an understanding of the relationship between different algorithmic choices (rules and parameters) and the quality of the corresponding text mining results (e.g., in terms of recall and precision). The ViTA system outlined above is designed to facilitate just such a process. We thus intend to move beyond conventional text alignment techniques by adopting an iterative development process that combines complex analytical tasks through an integrated process of automated text mining, multivariate visualization, and human-computer interaction.  Note 1. https://code.google.com/p/text-pair/. ",
        "article_title": "Visualizing Text Alignments: Image Processing Techniques for Locating 18th-Century Commonplaces",
        "authors": [
            {
                "given": "Glenn",
                "family": "Roe",
                "affiliation": [
                    {
                        "original_name": "Australian National University, Australia",
                        "normalized_name": "Australian National University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/019wvm592",
                            "GRID": "grid.1001.0"
                        }
                    }
                ]
            },
            {
                "given": "Alfie",
                "family": "Abdul-Rahman",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Min",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, United Kingdom",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "Clovis",
                "family": "Gladstone",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, United States of America",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Morrissey",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, United States of America",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Olsen",
                "affiliation": [
                    {
                        "original_name": "University of Chicago, United States of America",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "content analysis",
            "literary studies",
            "English",
            "data mining / text mining",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Arguably the relational database has had greater impact on the transformation of organizational cultures and the world economy than the Internet. The materiality of data centres coupled with the computational analytic potentials of databases has produced models of this world without historical precedent. Key here is the question of scale. The knowledge once derived from the transitional technologies of cabinets of curiosities ( Wunderkammer), demographic registries, and Foucault’s ‘great tables’ in the 17th and 18th centuries—later systematised into various epistemic instruments that included Diderot’s encyclopaedia, the periodic table, the museum, and Linnaeus’ taxonomies—were all coincident with the rise of populations governed as statistical subjects. Such instruments can today be understood as proto-databases, foreshadowing what Gernot Böhme (2012) has called our present era of ‘invasive technification’.  The advent of the relational database in the early 1970s marks a particular turning point, we argue, in the ductility and malleability of knowledge concerning technical operations and the governance of society at large. At this point information becomes in a new sense purely programmable, and available for, among other things, forms of ad hoc knowledge production. The logical interrogation of subjects is literalised with the advent of ‘structured English query language (SEQUEL)’ in 1974, which presents as its very first example a resulting ‘relation describing employees’ featuring the barely fictional cast of ‘Jones’, ‘Smith’, and ‘Lee’ (Chamberlin and Boyce, 1974). Thereafter the relational model and its lingua franca, SQL, make possible for the first time entirely new fields of data science: data mining, informatics, business intelligence, real-time analytics. This in turn has led to a technological shift in the processing and logistical operations of modern institutions, with transformative effects in the apparently mundane fields of report writing, insurance assessment, credit checks, and policy development. What were once specialised arts become template-driven forms of analysis and institutional processes designed for replication. Here, knowledge rubs up against the politics of parameters. New uses of data become a constant in the social life of institutional settings, laden with a politics that remains for the most part implicit as it is pervasive. The durability of knowledge practices is coextensive with the persistence of parameters. Political existence contracts into the embodiment of Quine’s dictum: to be is to be the value of a variable.  By the early 1980s, the increasing reliance of all institutions on the relatively hypostatised  form of the database reinforces and reinflects early-20th-century theories of institutionalism. For Weber, the institution was a necessarily constrained artefact of capitalist modernity. In the early eighties, DiMaggio and Powell (1983) revisited the ‘iron cage of bureaucracy’, reconceiving the modern institutional form as an ‘isomorphic’ entity with shared common procedures, structures, and operational norms while at the same time capable of adaptation to geographic, commercial, and industry-specific conditions. We argue that this isomorphism is recognisable by new institutionalist theorists in part due to its historical coincidence with the ubiquity and relatively enduring quality of the enterprise database, already emerging as a necessary part of modern institutional infrastructure. In the same way, the onset of flexible modes of capital accumulation was not a transformation independent of emergent developments in computational architectures. For example, the logistical world of ‘supply chain capitalism’ (Tsing, 2009) has become increasingly governed by the dual and interconnected processes of real-time computationally and just-in-time modes of production and distribution. The agility of the modern institution is, then, contingent upon the combinatory possibilities of relational databases that operate at ever increasing scales. The capacity for institutions to adapt to regimes of flexibilization is augmented, rather than replaced, by novel non-relational systems.   This twin model of data-and-institutional organization extends to the governance of logistical populations. One key example can be found in the recent challenges faced by the Australian Bureau of Statistics, which in 2014 confronted a very public institutional crisis of legitimacy based on a perception of computational failure. This crisis was precipitated by the multiplication of sites and points of data agglomeration: the ‘monopoly of knowledge’ enjoyed by the ABS for many years has now become rivalled by a diversity of institutional actors who also have considerable computational capacity to produce knowledge that bears upon how economies and populations are understood. The era of distributed computing, of virtualised clusters of machines and software that can co-operate to resolve queries over structured data on heterogeneous network and computational topologies, have been paralleled by questions of the sovereignty of singular guardians of population data. Since the 2000s, an array of new paradigms for arranging, connecting, and querying data—the Semantic Web, Linked Data, service-oriented architectures (SOA), and software-as-a-service (SaaS)—continue to bring into question claims over institutional legitimacy. In the Australian context, the cutbacks in the operating budget of the ABS from successive governments needs to be seen in the context of the marketization of grey literature enabled by computational processes. This is not only a case of the state increasingly outsourcing a once-sacrosanct responsibility to private service providers. The multiple diffusions and aggregations of population data throughout a heterogeneous computational and institutional network means that the ‘database’ is no longer physically or conceptually containable within the borders of a single institution. The increasing dependency by policy makers on the generation of numbers by machines is symptomatic of the automation of decision making. Such is the institutional over-reliance on the pure power of computation. No matter how many manual double-checks and regulatory procedures may constitute the repertoire of techniques deployed to guard against the sort of institutional risk exposed by the ABS debacle, the scale and distribution of computational calculation in the production of knowledge will most likely result in an increasing jostling for legitimacy among institutional actors seeking government contracts related to policy development. Implicit in this jostling is a challenge to assumptions of ‘closed worlds’ and non-monotonicity that accompany the traditional relational database form and, by association, the single institution that manages such infrastructure. Rival claims, multiple perspectives, and contradictory or indeterminate datasets form the new territory of informational contestation. The case of the ABS offers an optic through which to view the emergence of such struggles. Our claim is that this is less a story about decentralization and privatization of government within a neoliberal paradigm (although these are without doubt key forces), and more an instance of the technical logic of databases and distributed computing resulting in an unsettling of modern institutional authority. The relatively short history of the relational database has been mirrored, early on, by a recognition of the homologous or isomorphic character of the institutions that deploy them—and, more recently, by a sense that this isomorphism has itself ‘morphed’ or adapted into outright competition in the economy of data. Our central interest in this paper is to consider the role of the database as a technology of governance and the scramble of power as it relates to a capacity to model the world and exert influence upon it. What are the implications for public institutions as they relate to the supply of knowledge on national populations when the technologies of insight have become distributed and increasingly unaccountable across a range of actors? And what affordances does this present for the disruption of parametric politics, or the establishment, at the very least, of alternative parameters though which political life can be constituted? ",
        "article_title": "Statistical Institutions and Database Disasters",
        "authors": [
            {
                "given": "Liam",
                "family": "Magee",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Ned",
                "family": "Rossiter",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "English",
            "history of Humanities Computing/Digital Humanities",
            "cultural infrastructure",
            "knowledge representation",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Data visualization is a rhetorical form of increasing importance, both inside the digital humanities and in arenas of public discourse from journalism to advertising. This paper will argue that the act of revisualizing and representing information whose historical importance was consolidated through visualization offers a useful opportunity for humanists. It lets them fruitfully contribute to and intervene in the professional practice of data visualization, while building new methods of visualization that are specifically grounded in historical reflection on the self-presentation and failures of visualization in a wider historical context. It will do so by constructing  revisualizations of influential historical data from the US Bureau of the Census. This data was first visualized in atlases published between 1870 and 1920 that remain some of the most widely influential works in the history and present practice of data visualization and thematic mapping. Revisualization reveals the elisions, illusions, and labor at the heart of these canonical works.  In the world of data analysis and visualization itself, a recent process of canon formation has made historical visualization a fundamental part of the field’s self-conception and acts as an inspiration (Tufte, 1983).   Inside the digital humanities, the grounding of data visualization as a historically situated practice is somewhat muted. Martin Jessop (2008) and John Theibault (2012) have called for better situating practices of data visualization as historical practices themselves.  On the one hand, there is deep interest in data visualization as a deliverable for digital humanities projects in which humanists implement out-of-the-box practices of visualization  (‘Palladio Project’, 2014) and take the representation and augmentation of historical visualizations for digital modes of presentation as an end goal (Fletcher, 2014).   On the other hand, scholars such as Lauren Klein have called for situating these advances in the history of data visualization; still others, such as Johanna Drucker (2011), call for an authentically humanistic form of data visualization that will register uncertainty among the power relations involved in categorical enforcement.   The practice of data revisualization bridges the divide between enthusiastic application of other tools and critical reflection on the elisions inherent in visualization. This talk will investigate a self-consciously critical practice of data visualization through the re-analysis and re-visualization of two deeply influential cartographic data visualizations produced by the US Bureau of the Census between 1870 and 1920. One shows shifting population densities in recent decades of US history. It was widely discussed across American society, including providing the impetus for the single most influential article in the historiography of the United States, Frederick Jackson Turner’s frontier thesis  (Turner, 1894). The other shows the ‘center of population’ for the population as a whole and for the white and Negro subsets of it. These maps, and the census atlases they come from, are well known both among practitioners of data visualization and scholars of the 19th-century American state (Dandison, 2012; Kinnahan, 2008; Hannah, 2000).   This paper will investigate and deconstruct these maps in two ways: 1. Through the use of archival sources from the National Archives, the paper will better contextualize the maps as products of state construction. The center-of-population maps, for example, are described internally by the Census Bureau in terms that highlight the extreme difficulty of calculating the mean rather than the median of population. Employment records and extraordinary quantities of saved paperwork at the Census Bureau bear this out. These files can help explain the rhetorical origins of data visualization in practices of the state that emphasize the extraordinary labor invested in created data visualization as a way of constituting authority. Archival records also help to clarify the way that these records were used in the early 20th century by other members of the national polity, from advertisers interested in demonstrating the centrality of their state to newspaper editors fearfully tracking the Negro population’s move north after the Great War. 2. Through revisualization of the historical data using the D3 framework for JavaScript, the paper will investigate the elisions in the original data as well as the ways that different choices can open up different perspectives on the data. For example, while Turner treated the closing of the frontier as an objective fact, this analysis will show how it was only possible through an aggressively imposed set of assumptions about population distributions that even the Census Bureau itself abandoned 10 years later, leading a (mostly ignored) restoration of the frontier on official documents. The challenges this poses for data visualization make possible useful interventions in the wider community of data visualization. For example, to adequately represent the shifting frontier line, in all its incarnations, requires extending default svg path behavior to enable smooth transitions in ways that are broadly useful for digital humanities mapping projects mapping with D3. The strictly national character of this history will usefully intersect with the global themes of the conference, because they highlight the fundamental but frequently repressed connection between data visualization and explicitly state-centered ways of seeing. The census data is doubly so; it both relies on state-focused data and historically served to consolidate a nation-centered identity. Yet recent scholarship has tended to focus on the  scientific provenance of data visualization, despite the earlier emergence of both specific forms like the choropleth and the widespread dominance of what Lorraine Daston and Peter Galison called ‘trained judgment’ as a way of seeing data in state circles decades before their hegemony in scientific circles (2007).   In contrast to these state-oriented, bureaucratic forms of organization, this paper will propose a model of humanities visualization as an interpretive, authorial practice that aims to open questions of data integrity and visual design rather than mask them through interactivity. As a single-author visualization project, it will address the ways that claims for the inevitability of collaboration in digital humanities can serve as a backdoor for the introduction of statist forms of argumentation. ",
        "article_title": "Data Revisualization as Critical Humanities Practice: Reinterpreting 19th Century Data with Modern Tools",
        "authors": [
            {
                "given": "Benjamin",
                "family": "Schmidt",
                "affiliation": [
                    {
                        "original_name": "Northeastern University, United States of America",
                        "normalized_name": "Universidad del Noreste",
                        "country": "Mexico",
                        "identifiers": {
                            "ror": "https://ror.org/02ahky613",
                            "GRID": "grid.441462.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "historical studies",
            "English",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " TILT is a Web-service and graphical user interface designed to make it easier to study printed books or handwritten manuscripts. TILT recognises word-shapes in page images, then links them to words in an already existing transcription. Unlike a traditional OCR program, no actual characters are recognised. Instead, the links are used to provide visual cues to the user by highlighting corresponding parts of the image and the text as the user moves the mouse over the words or taps them. Text-to-image linking was ﬁrst explored by Seales et al. (2000) in electronic editions of old English texts at the University of Kentucky. Their technique was to manually draw mostly rectangles around areas of manuscripts and then link them to the text by means of embedded markup in the transcription (Dekhtyar et al., 2005), developing eventually the commercial EPT tool. Other examples include the British Museum’s electronic  Codex Sinaticus (BM, 2009) and, using a different display technique, the ‘zoom topographic’ view of the Samuel Beckett digital manuscript project (Hulle et al., 2011), the TILE project (Reside et al., 2011), and the TextGrid text-to-image link editor (Al-Hajj and Küster, 2013). However, all of these methods were predominantly manual. Rather than automatically processing the text and images, they invite the user to manually draw shapes (rectangles, ovals, and polygons) around words, and to link them one at a time to words in the text. This is very tedious, and the linking methods also lead to problems of overlapping markup that can only be resolved by creating two separate transcriptions: one to contain the text-to-image links and one the formatted text (Middell and Wissenbach, 2011).   The earlier TILT tool (Schmidt, 2013) attempted to address the automation of the text-to-image links by allowing words to be outlined via a single click, and for entire pages to have their shapes recognised and  linked to the corresponding words of the transcription largely automatically. But TILT 1 proved to be limited in the way it could deliver a user interface. The key problem was that in order to automate the linking, it would be necessary to have access to a fully featured suite of image-processing tools. For this, the Java class library was chosen. However, the only way to mediate the automation process was to present the tool as a Java applet, which is notoriously difficult to run in a web context.  TILT 2 attempts to address this problem by separating the front-end GUI (link editing and management of the recognition process) from the back-end (page recognition) part. The back-end is accessible via a test interface (Schmidt, 2014), and is designed to act as a faceless service usable by anyone who has a page image and a transcription, and wants to link the two. The front-end GUI is currently under development and contains tools for revising the automatically generated word-shapes and links using standard web technologies.    Figure 1. Text-to-image links and polygons as invisible GeoJSON overlay. Rather than use embedded markup in the transcriptions to link word-shapes with the corresponding words in the text, TILT uses GeoJSON (Butler et al., 2014) to describe the page and its recognised shapes, and annotates it with the positions of the words in the text. As shown in Figure 1, both the text-to-image links and the polygons that overlay the image are wholly contained in an invisible GeoJSON document, which can be loaded from a database or generated if required. The transcription can be in HTML or plain text formats.  Page Recognition  Given the immense variety of manuscript types, the TILT service is divided into clear stages. Each stage feeds the result into later stages, ending in the generation of the word-level links. The stages are:  1. Conversion to greyscale.   2. Conversion to two-tone.   3. Removal of borders.   4. Line recognition.   5. Word recognition.   6. Linking word-shapes to actual words.  Of these, the first three conform closely to the standard processing steps of OCR and use techniques in the Java class library or the Ocropus toolset (Google, 2014). The fourth step, however, represents a departure from standard OCR techniques. In order to recognise words, they must first be organised into lines. In printed books, recognition of lines is relatively easy. OCR programs use heuristics to determine text blocks and assume even line spacing. Skewed or warped lines must first be deskewed or dewarped. Since lines may curve or tilt naturally in manuscripts, a general line-recognition algorithm is needed that can cope with both printed and handwritten material. TILT’s approach is to first divide the image into narrow vertical strips. The strips are then reduced to a single average column of pixels, which is smoothed via a moving average algorithm. The peaks in the black pixels of each strip then are taken to correspond to lines. Finally, the peaks are linked up across the page using a greedy approach, by linking the closest points in each pair of adjacent columns first, then progressively more distant points, so long as this does not cause the lines to cross (Figure 2) (BL, 2014).    Figure 2. Line-recognition (Court of Oyer and Terminer for Trial of Attained Traitors record book [1796] from Grenada).  Once lines have been recognised they can be subdivided into words. (This assumes that texts  have word divisions, which may not be the case in certain languages.) The current method uses the two-tone image to identify ‘blobs’ of connected pixels in an image, then surrounds them with a polygon, and finally measures the gaps between polygons (Stamatopoulos et al., 2010). So on a page with 230 words on 30 lines, the number of inter-word gaps to be expected would be 229−29=200. So, assuming that the 200 largest gaps are inter-word gaps, polygons not separated in this way can be merged to form word-shapes. Although this works well for printed texts, with success rates often as high as 98 to 100%, variation in the use of spacing in manuscripts limits its success. An alternative technique (Manmatha and Srimal, 1999) tries to determine an appropriate Gaussian blur to join up blobs into the expected number of words, and then split words on that basis, achieving 86% average word recognition on the test documents. Application of this technique to TILT2 is planned but not yet implemented.     Figure 3. Alignment of word-widths in text and image. The final linking step is actually the most reliable part of TILT. Since the numbers of word-shapes and words are now known, the approximate width of a single letter can be calculated. This allows the estimation of the pixel-widths of words in the text. Since the sequence of word-shapes in the image must match the sequence of word-shapes in the transcription, the two can be aligned by modifying a classic diff algorithm, such as Ukkonen’s (Ukkonen, 1985). But instead of matching letters in two texts, TILT matches word-widths and allows either several word-shapes in the page image to match one word of the transcription or several words in the transcription to match one word-shape in the image, as shown in Figure  3. This can result in polygons being split or merged to correct the alignment.   Editing Interface The editing interface aims to visualise the automatically generated text-to-image links and allow the user to adjust them manually. This uses a number of semi-automatic techniques to speed up the editing process (Figure  4):    1. Redistributing words among the word-shapes between any two manually specified anchor points.   2. Merging two selected polygons when a word-shape has been incorrectly split by dragging across the shapes.   3. Splitting of polygonal shapes by dragging a line across merged words.  In addition to these techniques, manually adding and deleting points, and adjusting shapes provide a fallback when automatic methods fail.  Although currently incomplete, the editing interface is anticipated to be ready for demonstration at the conference (Schmidt, 2015).     Figure  4: Semi-automatic editing tools: realignment with anchors (top), merging a split word (middle), splitting two merged words (bottom).   Acknowledgements The TILT2 tool was one of two winners of the British Library Labs Competition 2014. The Center for Biodiversity Informatics at the Missouri Botanical Garden is currently supporting further development of the tool. ",
        "article_title": "TILT 2: Text to Image Linking Tool",
        "authors": [
            {
                "given": "Desmond",
                "family": "Schmidt",
                "affiliation": [
                    {
                        "original_name": "Queensland University of Technology, Australia",
                        "normalized_name": "Queensland University of Technology",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03pnv4752",
                            "GRID": "grid.1024.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "digitisation",
            "video",
            "programming",
            "audio",
            "English",
            "multimedia",
            "resource creation",
            "linking and annotation",
            "software design and development",
            "image processing",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper highlights shared methods, questions, and challenges between research through design (RTD) and digital humanities (DH) through the discussion of an archival research project. In DH, debates continue (Gold, 2012) regarding the impact of digital technologies on epistemology, methodology, and our professional identities as researchers, scholars, academics, and teachers. Our reading of this debate is that there is a tripartite relationship at work between the kind of work we should call DH (and aspire to produce); the nature of knowledge, research, and scholarship (particularly with reference to the role of artefacts produced); and issues of disciplinary orientation or professional identity. We could phrase these as the what, how, and who of DH and, of course, RTD. The discussion of our project is in no sense intended to provide an exclusive answer to those questions, but to give one snapshot of what DH and RTD look like when they come together.  RTD emphasises artefact-led, practice-based research with an emphasis on developing design methods, conceptual frameworks and theories, as well as products (Gaver, 2012). Its speculative orientation and emphasis on making with digital materials aligns it well with some currents of thought in DH (see, e.g., Drucker, 2009). Indeed, Joanna Drucker has pointed to a shared ground with design, generally noting that ‘all forms of design share a propositional orientation that is well-suited to the challenges that come with designing new structures, for design asks, “What if?”’ (Burdick et al., 2012). At the same time she warns that ‘the cultural authority of digital technology is still claimed by the fields that design the platforms and protocols on which we work. These are largely fields in which quantitative, engineering, and computational sensibilities prevail’. We wish to qualify Drucker’s claim by noting a series of more detailed concordances with RTD of a particular propositional, speculative, and explorative orientation. We note that despite Drucker’s valid concerns for the positivist outlook of some design research contexts, within the field of human-computer interaction (despite its background and reputation for some as computing engineering’s heartland), and interaction design, a vibrant dialogue is being sustained concerning the role of designed artefacts in knowledge production (Bowers, 2012), their responsiveness to the values of users (see, e.g., Vines et al., 2013), and the state of critique and criticality (Bardzell and Bardzel, 2013), all of which are directly relevant to DH. In the following paragraphs we will describe aspects of an ongoing project, positioned as DH with roots in RTD methodology. Through it we will position our work in relation to the tripartite relationship we identified between what digital humanities research looks like, what role its artefacts play, and what kind of people do it.  Our project is, typically for digital humanities research, based around an archive. Bloodaxe Books is a small but internationally significant publisher of contemporary poetry, whose archive, consisting mostly of edited manuscripts, was purchased by Newcastle University, UK, in 2013. Our role within a research project as artists and Interaction Designers was to create exploratory and provocative interactions with the archive both online and in physical space. This is the ‘what’ of our project.  The interfaces we designed for the archive respond to its formal and textual specificities, and this flexible and responsive mode of engagement is typical of RTD processes. We think that this kind of approach is well aligned with a view of the role of DH as one that expands the ground of research processes as well as simply augmenting their methods with new tools and techniques. In our project we began with loose research questions (as described above) that were refined alongside the project. Our treatment of the archival material was to be informed by the experience of 30 project participants, each of whom was conducting personal, creative research in the archive. Inspired by previous research through design work, we conceived of a ‘cultural probes’ (Gaver et al., 2004) activity to gain insight into the way the archive was being used by our project participants and to uncover some of the things they found interesting about the materials themselves, the better to inform our ‘what’. Our activity used a bookmark-like insert completed and left by participants in the archive boxes themselves to act as a conversation ‘backchannel’ for participants. A trial of this activity revealed a number of interesting features of the archive that focused our interests and informed future designs. Particularly our later work with archival Marginalia and our related interest in the temporality of the archive was significantly informed by this process. When Drucker asks, ‘Have the humanities had any impact on the digital environment?’, one answer is in exactly this kind of enquiry, which has at least a 10-year history in design research. The probing activity described above was user-focused, not in the sense of a formal ‘requirements’ analysis but as a kind of critically sensitizing activity. For instance, in the final interface three particular areas of our design responded to questions and issues raised by the participants and our literary research colleagues: ‘Shapes’ 1 allows users of our interface to sketch—with the mouse—the shape of a poem and receive matches from the archive. ‘Data’ mashes our own metadata with British Library and Wikipedia data to explore wider the context of the archive. 2 ‘Words’ 3 uses text mining techniques and word distancing to produce network graphs of correspondences between documents and relationships to themes (e.g., flowers, death).  In practice-based research processes, an inevitable question arises concerning the status of the artefact as a bearer or disseminator of knowledge, and researchers in (Ramsay and Rockwell, 2012) and out (Ingold, 2013) of DH have noted the problems this causes for the place of such work within academia. In other words, our ‘how’ is about how we treat the things we make, how we evaluate their contribution and ensure that they are productive. Our perspective, as writers of both code and research papers, is that there is knowledge in both objects and commentary but that the relationship between them should be negotiated honestly and delicately. In our project we adopted a fast prototyping cycle in which we made a series of early visualisations and web interfaces to the digitized material publically available and invited criticism and feedback. We also made a feature of both our ongoing design work and the cataloguing and digitization taking place in the library by producing a Twitter feed of computationally extracted archival marginalia connected live to the work of the archivist and digital assistant and a drawing robot, the Marginalia Machine, which publically re-draws these same editorial notes.     Figure 1. Screen grab from the @BloodaxeArchive Twitter bot. These artefacts acted as kinds of a ‘boundary object’ (Star and Griesemer, 1989), around which we and our colleagues in the library and the English Department could discuss future iterations of design work. Their production afforded what we might call the ‘unimagined interactions’ characteristic of this kind of speculative prototyping. In our project, as is common in RTD, the artefacts were a part of a methodology for integrating facets of our research, working with our colleagues and participants and of course improving future designs. Despite this pivotal function for the ‘things’ (Brown, 2001) of our research, we recognise the value of reflection, in written or other forms, for future work. John Bowers proposes the ‘annotated portfolio’ (Bowers, 2012) as one approach to communicating the value of our work to ourselves and others. We note that however much artefacts articulate a position, their situatedness in a world of stuff means that they will always be poly-vocal. This is both their strength (when we wish to explore) and their weakness (when we want to be specific).     Figure 2. Detail of the Marginalia Machine. Finally we observe that in the picture of the digital humanities researcher as one who can ‘research, write, shoot, edit, code, model, design, net-work, and dialogue with users’ (Burdick et al., 2012), RTD shares a problem educating, finding, and hiring such people. We further note that the role of a designer, even one with such competencies, in a DH project remains problematic for a number of reasons. If we wish to pursue successful interdisciplinary research, then we must decide to what degree our research questions will be common with our collaborators or distinct. One model for the latter sees a humanities researcher provide a problem for a computer scientist to work with—for instance, the development of a corpus analysis tool. The computer scientist produces technical innovation while the humanist integrates the tool into the research. In our project, however, the designer, coming from a humanities background himself, brought his own questions to bear on the material, which were informed by art history, philosophy, and the cultural history of design. A challenge for such designers is how to meaningfully collaborate with other humanities researchers without, first, denigrating their own research questions and competencies to a second-tier status, and second, ignoring the nuance and depth of good humanities research. Strategies for the successful avoidance of these two pitfalls, we feel, should be the focus of future discussion of this relationship.  Notes 1. http://bloodaxe.ncl.ac.uk/explore/#/shapes. 2. http://bloodaxe.ncl.ac.uk/explore/#/data. 3. http://bloodaxe.ncl.ac.uk/explore/#/words. ",
        "article_title": "Research Through Design and Digital Humanities in Practice: What, How and Who in an Archive Research Project.",
        "authors": [
            {
                "given": "Tom William",
                "family": "Schofield",
                "affiliation": [
                    {
                        "original_name": "Culture Lab, Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Kirk",
                "affiliation": [
                    {
                        "original_name": "Culture Lab, Newcastle University, United Kingdom",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            },
            {
                "given": "Mitchell",
                "family": "Whitelaw",
                "affiliation": [
                    {
                        "original_name": "Faculty of Arts & Design, University of Canberra",
                        "normalized_name": "University of Canberra",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04s1nv328",
                            "GRID": "grid.1039.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "repositories",
            "user studies / user needs",
            "libraries",
            "interface and user experience design",
            "archives",
            "museums",
            "digital humanities - nature and significance",
            "sustainability and preservation",
            "English",
            "GLAM: galleries"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This study applies topic modeling to a collection of French crime fiction novels in order to discover topic-related patterns. The results show both expected and unexpected patterns related to authors, subgenres, and time period. Topic modeling proves highly useful for investigating the history of French crime fiction.  French Crime Fiction Crime fiction is a type of narrative prose fiction involving the elucidation of a (usually) violent crime through a (more or less) rational investigation (often) taking place in an urban setting and (typically) involving (one or several) investigators, victims, witnesses, suspects, and criminals. French crime fictionʼs rich history goes back to the 1860s and has many highly prolific proponents. Prototypical detective fiction is easily recognized, but the boundaries of the genre and its internal division into subgenres remain controversial (see Todorov, 1971; Lits, 1993; Colin, 1999; Lavergne, 2009). The abundant material is a challenge to any readerʼs memory but an opportunity for quantitative methods. Research Questions This study addresses the following questions: How prevalent are expected, genre-related topics such as crime and investigation, and which other topics are important? What relations exist between topics and categories like authorship, subgenre, or time period? What kind of groupings of novels does one obtain based on topic similarity? What new insights into the history of crime fiction does topic modeling allow? Data For this study, a collection of 270 French novels published between 1858 and 2012 was created. The vast majority are crime fiction novels, but some non–crime fiction novels have also been included. The collection includes novels pertaining to seven subgenres and written by 14 different authors. It has around 16 million word tokens. Texts in the public domain have been obtained online (from ebooksgratuits.com), while additional texts have been obtained by full-text digitization. Method Topic modeling is an unsupervised method of discovering latent semantic structure in large collections of texts. Technically, a topic is a probability distribution over word frequencies, and each text is characterized by a distribution over topics (see Steyvers and Griffiths, 2006). In practice, the words with the highest scores in a given topic are mostly semantically related words; the topics with the highest scores in a text represent the textʼs major themes or motives.  The most widely known algorithm is Latent Dirichlet Allocation (LDA; see Blei et al., 2003), but several precursors and alternatives exist—for instance, Non-Negative Matrix Factorization (Lee and Seung, 1999). Several tools are available, like MALLET (McCallum, 2002) or gensim (Rehurek and Sojka, 2010), as well as tutorials (e.g., Graham et al., 2012, Riddell, 2014). Topic modeling has proven immensely popular in digital humanities (e.g., Blevins, 2010; Rhody, 2012; Jockers, 2013). For the results reported here, the following parameters have been used: Lemmatization has been applied to the texts, because French is a highly inflected language. After POS-tagging with TreeTagger (Schmid, 1994), nouns have exclusively been selected for analysis. Each novel has then been split into segments of approximately 150 nouns each. MALLET has been run with 60 target topics and 10,000 iterations.  Results and Discussion Topics Obtained The topics obtained can manually be labeled by their dominant semantic trait (see Figure 1). Figure 1. Selection from the 60 topics obtained (with topic ID and 20 top-ranked words). The subjective topic coherence is very high: few top-ranked words do not share semantic traits, and few topics are hard to interpret (but see Chang et al., 2009; Schmidt, 2012). Many topics could appear in any type of novels, such as topics #28, #38, and #01 (labeled ‘family’, ‘money’, ‘train’). Only nine out of 60 topics are related to crime fiction, such as #15, #33, and #53 (labeled ‘investigators’, ‘fire arms’, ‘jewelry’). Judged by topic composition alone, crime fiction appears to be a less distinctive novelistic sub-genre than expected. Note that topics are based on various types of similarity: topic #44 (‘interiors’) is related to a recurrent setting, topic #22 (“informal1”) to a specific register. Authorship, Subgenre, and Time Topic scores per text segment can be aggregated and averages obtained, for instance, at the document, author, genre, or time period levels.  On the author level (see Figure 2), it appears that several (but not all) authors have a distinctive ‘signature topic’: a topic with a particularly high score in comparison both to other topics for the same author and to the same topic for other authors (i.e., across rows and columns).   Figure 2. Distribution of topic scores at the author level (15 topics with the largest variation across authors, measured in standard deviations). Gaboriau ʼs signature topic is very general (#11, ‘bourgeoisie’) while Simenonʼs is more genre-specific (#29, ‘office’) and Maletʼs is not thematic (#22, ‘informal1’). For some authors (Leroux, Manchette, Ponson), no clear signature topic emerges.   Compared to authors, most subgenres have less marked characteristic topics (Figure 3).    Figure 3. Topic scores aggregated to the subgenre level (20 topics with the largest variation, measured in standard deviations, across genres). For example, the classical detective novelʼs most characteristic topic is #29 (‘office’). However, the traditional genre labels used here are problematic, because they tend to refer to periods rather than structural types and correlate strongly with authorship.  When average topic scores are obtained across all novels for each successive tenths of novels, and topic score progressions are compared across subgenres, genre-specific patterns appear (see Figure 4).     Figure 4. Topic score progression for topic #26 (average topic scores per text segment and subgenre). For instance, topic #26 (‘twilight’) decreases over the course of the average detective fiction novel, but remains stable at a higher level over the course of ‘roman noir’. In the former, darkness and uncertainty are being dispelled and order restored, while in the latter, they are not.  The distribution of topics at the level of time period (Figure 5) shows that, as expected, some topics gain while others lose in importance over time. Some very similar topics ‘take turns’, as it were, with successive peaks (‘informal’, right). Others reflect larger societal changes (different means of  communication, left).  Figure 5. Topic scores per decade for two topic groups. For the ‘informal’ topics, each peak is associated with one author and their period of activity: Malet, Dard, and Vargas (see Figure 2). The topicʼs rapid rise in the 1940s underlines how bold Maletʼs use of informal language was, but also shows the (problematic) impact of individual authors on these temporal patterns. Beyond individual topic development over time, the cumulated rate of change in topic scores from one decade to the next gives an insight into the thematic innovation cycles of crime fiction (Figure 6).     Figure 6. Cumulated absolute differences between topic scores from one decade to the next. Values above trendline indicate periods of intense topic-related innovation and, possibly, generational shifts (notably, 1880s–1900s and 1930s–1940s). Values below indicate periods of relative continuity (notably, 1910s–1920s and 1940s–1960s). Such results provide a fresh perspective on periodization in literary history.  Author Similarity Based on the topic scores per novel, author, or genre, Principal Component Analysis (see Joliffe, 2002) yields groupings of topically similar items, independently of preexisting classifications. The PCA plots in Figure 7 are based on topic scores aggregated to the author  level and have been obtained  using the stylo package for R (Eder et al., 2013). The first two components retain large parts of the variation in the data (31.3% and 12.6%, respectively).  Figure 7. PCA plot of aggregated topic scores per author: authors (right) and loadings (left). Not surprisingly for a collection of texts spanning 150 years, the first component is correlated  with time period: authors active before 1930 are located to the east of the plot, those active from 1930 to 1960 close to the middle, and those active after 1960 to the west. A notable exception is Fred Vargas: although writing in the late twentieth century, she appears close to Malet and Dard: the topic-based grouping reveals a link between the now classic ‘roman noir’ and its later reinterpretation by Vargas. Note that although the three authors each have an ‘informal’ topic as their ‘signature topic’ (see above), their proximity remains unchanged even when these topics are removed. Japrisotʼs unique work rightly stands out, but his relative proximity to Simenon remains to be explained.  Conclusions and Future Work  The results obtained here shed a new light on the history of French crime fiction. Authors, subgenres, and time periods each have distinctive topic characteristics. Some well-understood facts about the genre can be confirmed (e.g., author groupings), but new insights into the genreʼs thematic history also become possible (e.g., thematic innovation cycles). Topic modeling proves to be a valuable tool, providing a fresh perspective on literary history and prompting new interrogations about periodization and the contours of subgenres.  Future work will involve two areas: The text collection will be expanded to reduce correlation between authors, subgenres, and time period. Also, the precise relation between topics and certain categories (e.g., subgenres) will be further investigated using supervised/labeled topic modeling (see McAuliffe and Blei, 2008; Ramage et al., 2009).  ",
        "article_title": "Topic Modeling French Crime Fiction",
        "authors": [
            {
                "given": "Christof",
                "family": "Schöch",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg, Germany",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "content analysis",
            "literary studies",
            "text analysis",
            "English",
            "french studies",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ‘What Is Digital Humanities? [. . .] NB: Refresh the page to get a new definition’. —Jason Heppler, http://www.whatisdigitalhumanities.com Answers to the question ‘What is DH?’ abound. One needs only skim several recent editions to gain a preliminary, wayfaring-like  sense of the discursive horizon and its temporality (see  . In such volumes, DH as stabilized institutional entity looms in the distance of a ‘present-future’  . It seems that DH scholars face a set of futures and move (multi-)linearly toward freedom from a ‘turtlenecked hairshirt’ of one form or another  .  Recently, however, a non-linear window has opened. Raley  frames DH as a discursive construction, as a site of potential. That is, while ‘semantic battles about the institutional identity of the digital humanities are a symptom of a discipline [. . .] fixated upon making a permanent space for itself”  , DH may be productively conceived of as a troubling mediator through which scholars can imagine ‘potentials—not what is, but what might have been [. . .], as Geoffrey Rockwell suggests, “what could be”’  . Within this formulation, the present-future orientation of the DH discourse tilts.  In this short paper, I will not attempt a concrete answer to the question ‘What is . . . ?’ Rather, grounded in my own disciplinary situation within a department of informatics, I will ask another question. By considering the temporality and materiality of the ‘digital’ as it applies to the humanities’ textual objects I ask, ‘How long is the “now” in which the potentials of DH reside?’  A Digital Discourse As Foucault  noted, ‘The positivity of a discourse [. . .] characterizes its unity throughout time’, but the ‘ a priori of positivities is [. . .] itself a transformable group’. It follows, then, that the durability of ‘digital’ comes into question as the violent  signifier by which DH distinguishes itself from humanities; as discursive entity, it is subject to change even as it effects change. The presence of the digital modifier, a nebulous source of techno-adjectival power, sketches the uncomfortable mutation of a known category (i.e., humanities), established across hundreds of years of practice. (NB: This is closely related to Alan Liu’s notion of ‘alien’  ]). But how far in time does a mutation reach?  Referential clarity dissolves with the digital modifier as new textual materialities arise and exert influence not only over ‘text that might yet be’, but also texts that already are. At some point, the ‘alien’ of the digital modifier becomes resident across tenses; the digital materiality so emphasized in ‘DH’ yields a potentially pervasive change to the materiality (and therefore interactivity) of the humanities’ textual objects, the products of inscription (see  . As will be seen through a discussion of inscription, a shift toward digital textual materiality fundamentally transforms the archive of textual study  across the tenses. In the context of digital inscription, the ‘now’ of DH extends spectrally across the ‘present-future’ and the ‘present-past’  .  (Digital) Inscription The etymological roots of inscription (the Latin  scribere, to scratch) demonstrate the physicality of the mark, the intentional exertion of force upon a substrate  . Inscription results in the removal of the substrate’s embodied noise, its openness and naturally unbounded potential relations to the objects and agents of its ecology. Thus, inscription constitutes a transformation—both the transformation of object into the written-textual and the entrance of a text into the archive it partly constitutes. It represents the point at which a ‘present’ is materially embodied for future remembering, ‘readied for future use’  . One might further envision inscription as the precursor to textuality, to the ‘woven’ structure of interrelated inscriptions  in a forward-facing present tense.  In the increasing invisibility  of physical inscription that occurs under the influence of the digital modifier, or indeed the presence of analogous ‘writing on’ or ‘writing upon’, wherein meaning is apparently added without the removal of substrate, such marking still connotes absence. The inscription, as either metaphor or actuality, is the marked’s categorical entrance into the archive . The inscription is simultaneously the mark of the archive and the space in which the archive symbolically resides. It is the textuality of the humanities, whether digital or not. As such, the archive of humanities, modified by a digital transformation in the method and mode of inscription, is itself transformed. In this transformation the temporal boundaries of the DH discourse become hazy. The present tense of digital textuality colonizes the pre-digital; the epoch of ‘potential memory’  re-contextualizes extant texts bounded in pre-digital inscription; the ‘digital’ modifier troubles potential pasts as much as it does potential futures. That which has been woven in one materiality is potentially unraveled. Closed work risks openness  ; woven inscription risks Barthesian fragmentation. The present-future of the digital acts upon the present-past.  Pervasive Distance A basic tenet of philology frames the problem of temporality in digital inscription. According to McGann (2014, 44), the philological method necessitates ‘examin[ing] original materials  in situ [. . .]. Surrogates, digital or otherwise, may serve the work at some point, but they cannot substitute for those first hand visits’. But such a statement becomes problematic when the alien becomes resident, when digital (re)inscription colonizes pre-digital inscription. When the potential invisibility of the digital renders pre-digital texts fallaciously  always already digital, the boundaries of text, the difference between original and surrogate, blur. In the event of such a potentiality, one must ask, ‘To what extent can an interpretive knowledge product derived from analysis facilitated by digital transformation refer to anything but itself, to anything but the pervasive present of the digital modifier?’ An analysis of Moretti’s first acknowledgement of ‘distant reading’ will clarify.  The textual-technical  dispositifs that give rise to such methods as ‘distant reading’ render a representational metaphor that filters the texts that they are (theoretically) intended to render transparent. Moretti  calls for ‘a return to that old ambition of  Weltiliteratur’ in order to study the ‘planetary system’ of literature (54). In the move from canon to planetary system, a question of scale becomes immediately apparent. How does one move from the closeness of reading to a distant, telescopic view? The answer is scaffolding:   The study of world literature will somewhat have to reproduce this ‘page’—which is to say: this relationship between analysis and synthesis—for the literary field. But in this case, literary history will quickly become very different from what it is now: it will become ‘second hand’: a patchwork of other people’s research,  without a single direct textual reading.  ( )  ‘Direct textual reading’ is bypassed via scaffolding, the construction of a sociotechnical vantage from which to view the whole ‘planetary system’. The ‘planetary system’  becomes text by way of digital re-inscription. The scaffolding provides the context for knowledge production by way of extending the now-resident digital alien of DH to pre-digital texts. But the scaffolding is more than just context; it is the context  and the knowledge produced, an artificial hypotext referring to itself through the spectrality of digital inscription. It is the sociotechnical assemblage by which linearity can be assigned to ‘a device, a trope, a limited narrative unit’  , but an assemblage that bears an a priori relationship to the otherly inscribed texts it analyzes. It has only itself as its object of study and must methodologically account for itself.  A digital method like distant reading must interpret itself; it is the contemporary tendency to rely on the prima facie validity of pervasive, computerized, and disembodied representationalism that lurks behind the knowledge productive phenomena of DH (and big data) that deserves interpretation. Moreover, it is in this growing invisibility of the digital that Foucault’s historical a priori of the inscribed humanities archive becomes a  pervasive a priori: an artificial and invisible scaffolding presenting with the appearance of an ‘always already’ materiality that retroactively transforms the archival materialities of the past.   In order to approach the question of the ‘now’ in which DH potentials reside, one must first reflexively interpret DH itself. The emergence of DH represents an opportunity to read the potentials of the digital, not to merely read digitally—to critique through a process of abduction the potential development of a pervasive a priori. In considering DH as discursive site of potentials as Raley (2014) suggests, one begins to see unexpected implications of the ‘digital’ modifier for the analysis of cultural objects across time. The future of ‘digital’ in the humanities is of great consequence to the past. ",
        "article_title": "How Long Is Now? The 'Digital' in DH",
        "authors": [
            {
                "given": "John",
                "family": "Seberger",
                "affiliation": [
                    {
                        "original_name": "University of California, Irvine, United States of America",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "digital humanities - nature and significance"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We developed a novel method for adding realistic degradations to color document images in order to generate large sets of training data for computational processing. We extract and process patches of real noise from degraded documents and paste them into the target document image using the gradient domain in order to achieve independence from the hue of the document. We will describe the novel algorithm and present an expert’s opinion on the data generated. * * * Historical documents suffer from different kinds of damages during their conservation, caused by either constant use (e.g., stains, fingermarks, ink strokes or spots, scratches, degradation of ink by abrasion), by inappropriate storage conditions (such as mildew caused by humidity), by historical catastrophes like wars, or by the material itself (such as ink corrosion). These damages can affect the readability of the manuscripts and also pose a severe challenge for digital processing of such documents. Developing robust digital processing algorithms requires machine learning, which relies on huge amounts of realistic training data, i.e., large sets of document images together with their corresponding ground truth (which refers to the expected labeling of the document). The high cost of ground-truthing limits the number of available training and testing documents. This motivates the development of methods that artificially add noise to existing ground-truthed documents in order to increase the number of available training documents. Fischer et al. (2013) presented a method to generate training samples by deforming binarized text lines. They showed that the accuracy of text recognition could be increased by up to 3.23% by their method.    Figure 1. An document image sample (left) and its degraded version (right). Kieu et al. (2012) have presented a local noise generation method for historical document gray-scale images. They evaluated their method with optical characters recognition (OCR) software and found a linear relation between the quantity of the noise and the recognition error rate. Several other methods have been developed, but they all work either on the red, green, and blue values of images, or on binary (black-and-white) images. Methodology The main novelty of our method is, contrary to previous work, to work in the gradient domain of color images, i.e., using the difference of values between pixels, and to use patches of real degradations from existing document images. To insert degradation patches in the gradient domain, we first compute the gradients of the document image and of the degradation patches, then paste the gradients of the degradations on the gradients of the document image, and finally reconstruct the document image from its modified gradients. This method is illustrated in Figure 1. Working in the RGB color space would lead to inconsistencies if the document from which we extracted the degradations does not have a very similar background color as our target document image. Working in the gradient domain to avoid this is the main novelty of our method. Our method is inspired by the Poisson image editing method presented by Pérez et al. (2003). First, we compute the horizontal and vertical gradients of the image. Then we apply the degradations on these gradients. Finally, we construct the result image from the modified gradients. Scholar’s Point of View From a manuscript scholar’s point of view, the method for adding synthetic noise presents advantages and disadvantages regarding its verisimilitude. One advantage is that the inserted patches are based on real degradations appearing in historical documents; thus, they represent realistic historical degradations. This method provides a rather realistic impression for small-sized details of a manuscript page. However, the overall impression of an entire page looks less realistic, because the degradation patches are spread repeatedly, following a uniform random distribution over the page, and can easily be recognized as artificially added. Thus, the gradient method is appropriate for simulating degradations such as stains, scratches, and ink spots, which mostly overlap the script or the background, but doesn’t have a direct impact on the script (as, for example, abrasion and humidity do). It works best on small-scale details. Discussion Our method can be used to generate document images to train machine learning algorithms. The settings of the software that we developed are easy to manipulate by non–computer scientists. However, precautions have to be taken in order to clarify that the resulting degraded documents do not represent the actual state of the real historical document. As future work, we will compute statistics about the position of the degradations while extracting patches in order to generate more realistic distribution models. ",
        "article_title": "Gradient-domain Noise for Realistic Degradations in Historical Documents Images",
        "authors": [
            {
                "given": "Mathias",
                "family": "Seuret",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Nicole",
                "family": "Eichenberger",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Marcus",
                "family": "Liwicki",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Rolf",
                "family": "Ingold",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "interdisciplinary collaboration",
            "programming",
            "English",
            "resource creation",
            "image processing",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2012 Google indexed an estimated 46 billion web pages (staticbrain, 2014). Although the Web of linked data was only 3/10,000s this size in the same period (Simpson and Brown, 2014), 15.9 million pages is not an insignificant number (Cheng et al., 2013). Furthermore, as an emerging and burgeoning technological field, we can be certain that these numbers have grown significantly since then. With such a vast amount of data in these two Webs, the increasingly pressing concern is how to understand what is contained within them, particularly when it comes to seeing relationships within the content of each. This remains a problem even with linked data and, when warranted, the ontologies that connect such data, even though such data was built to make relationships explicit and available for exploration. This is a problem that others have articulated as well. For instance, Loskyll et al. ask, ‘How can one represent huge collections of knowledge (e.g. ontologies with over 10 million concepts) as browsable trees in a scalable manner and with a clear user interface?’ (2009, 385). While part of the solution would include appropriately curating the original data, this is not feasible with large datasets, and thus a solution must be borne by improved visualization tools and techniques. While there is a clear interest in using visualizations to navigate this information, there is no generally agreed-upon method to aid in this task. Should we render information that would otherwise need to be inferred ‘actually explicitly visible’ (Howse et al., 2011, 259), provide ontologists with an ‘effective solution’ for understanding ‘relations between existing concepts’ (Kocbek et al., 2013, 34), or any number of other purposes? Balancing these concerns remains a challenge despite the fact that visualizations have played an important role in assisting the retrieval and dissemination of both qualitative and quantitative information for centuries (Tufte, 2006; 2001). In this paper, we ask,  What features should a Semantic Web visualization tool have to maximize the discovery of new knowledge?  This paper answers this question in two stages: it provides a review and evaluation of 30 existing Semantic Web–related visualization tools, and it reports on the production of a completed network visualization tool that is now entering the user testing phase.   Review of Linked Data and Ontology Visualization Tools  Before reviewing the features of the network visualization tool we have produced, we will summarize the results of a review of 30 existing tools or techniques that target visualizations of either linked data or ontologies. While it is the case that surveys of visualization tools for linked data and ontologies have been carried out before, 1 since 2007 there have been none that we know of and none conducted on this scale. The list of tools or techniques that are included in our review is displayed in Table 1.     CropCircles (Wang and Parsia, 2006)  Cytoscape http://www.cytoscape.org/  D3 (Data-Driven Documents) http://d3js.org/ Gephi  http://gephi.org/  Google Visualization API with RDF   http://data-gov.tw.rpi.edu/wiki/How_to_use_Google_Visualization_API   GraphViz http://www.graphviz.org  HyperTree (Bingham and Sudarsanam, 2000) http://kinase.com/tools/HyperTree.html InfoVis ToolKit  http://philogb.github.com/jit/  IsaViz http://www.w3.org/2001/11/IsaViz/   Jambalaya (Storey et al., 2001)   KC-Viz (NeOn Toolkit Plug-in)   (Motta et al., 2011) http://neon-toolkit.org/wiki/KC-Viz   Knoocks (Kriglstein and Wallner, 2010) http://enomisk.net/knoocks/ LodLive http://en.lodlive.it/ NeOn Toolkit  http://neon-toolkit.org/wiki/Main_Page Networkx http://networkx.github.io/   OntoViz http://protegewiki.stanford.edu/wiki/OntoViz  Prefuse http://prefuse.org/ ProcessingJS http://processingjs.org/  Protégé VOWL Plugin (Lohmann et al., 2014 )  http://protege.stanford.edu/  Protovis http://mbostock.github.com/protovis/   RDF Gravity http://semweb.salzburgresearch.at/apps/rdf-gravity/    RDF2SVG (Rhizomik) http://rhizomik.net/html/redefer/rdf2svg-form/   Redland http://librdf.org/  SIMILE Exhibit / Exhibit 3.0 http://simile-widgets.org/exhibit/     Sp  aceTree   (Plaisant et al., 2002) http://www.cs.umd.edu/hcil/spacetree/  Spicy Nodes http://www.spicynodes.org/   Tableau Public http://www.tableausoftware.com/public   Tom Sawyer Software http://www.tomsawyer.com/home/index.php  TopBraid Composer  http://www.topquadrant.com/product/TB_Composer.html  Visual Browser http://nlp.fi.muni.cz/projekty/visualbrowser/   Visual DataWeb (RelFinder, SemLens, gFacet, and tFacet) http://www.visualdataweb.org/tools.php    Voyage RSS Reader http://rssvoyage.com/   Welkin http://simile.mit.edu/welkin/   Wiki Map Project http://ickn.org/wikimaps/  Yago http://www.mpi-inf.mpg.de/yago-naga/yago/index.html    Table 1. Linked data and ontology visualization tools. A New Visualization Tool In 2014, Lohman et al. pointed out, ‘While several visualizations for ontologies have been developed in the last couple of years, they either focus on specific ontology aspects or are hard to read for non-expert users. The silver bullet would be an ontology visualization that is equally comprehensive and comprehensible. It must be printable but also provide intuitive ways to interactively explore ontologies’ (395). While the focus of this comment is clearly on ontologies, the same is true of visualizations for linked data in general. This is just one set of criteria that a so-called silver-bullet visualization tool ostensibly should have. Others include key concept extraction, a rich set of navigation and visualization mechanisms, flexible zooming into and hiding of specific parts of an ontology, history browsing, saving and loading of customized ontology views, essential interface customization support, such as graphical zooming, font manipulation, tree layout customization, clear representations of hierarchy and predicates, and the availability of multiple geometrical views (Motta et al., 2011; Sivakumar and Arivoli, 2011). Clearly, there are a lot of expectations to be met. In constructing a new visualization tool for ontologies and linked data, our primary goal was a general one: build an interactive visualization tool that can be used by non-expert users to explore linked data. Of course, such general goals are often riddled with complexities when put into practice, and that was certainly the case here as we worked to address many of the criteria felt to be important by other developers (see above) as well as within our own team. The result has been the tool illustrated in Figure 1, which we are tentatively referring to as HuVis (short for Humanities Visualizer). As the figure makes clear, the interface of the tool is broken out into three areas: a central stage; a tabbed command construction history, documentation, and configuration window on the right; and a text-viewing area on the left that displays text to which the triples are linked.    Figure 1. The HuVis tool in action. In the center of the display are all the entities of the dataset arranged in a circle around a central visualization stage. Entities, represented as nodes, are colour-coded by type and may be dragged onto the stage or reshelved in the ring by dragging with the mouse. When dragged to the stage, all the nodes immediately connected within the pre-loaded linked dataset to the added entity are also added. Nodes may be removed from consideration by dragging them to a discard area, here shown by the small circle of blue dots in the lower right of the figure. To aid users with navigation, the default display of labels is limited to those attached to nodes that are either in the center stage area or those within the range of the mouse pointer. A circular area around the mouse pointer also acts as a magnifying glass, making it easy to differentiate entities even if they are crowded into the display area. The command panel in the upper right of the display is a powerful tool for controlling the content of the visualization. With just a few clicks, users may execute commands to display or hide groupings of nodes from the stage area and control the display of those entities. Such selections may be made in conjunction with direct selections of elements in the stage area or outer ring, based on entity type, or based on the predicates by which entities are connected. Other features, such as general instructions for the tool (the Help function) and the controls for the layout of the stage area (Settings), are available in the same window through a tab interface. In the top left of the window is the ‘snippet display’. This area shows portions of the text from which the links in the center display area have been drawn. Snippets are revealed by clicking on a link between the two nodes. These snippets are important so that scholars may examine the criteria by which the connections that they are seeing are justified.  Conclusions Our paper will report on the results of the user testing conducted in early 2015. We will share our insights, as well as a set of recommendations for ontology and linked-data visualization based on the results of this study. Note 1. The review by Katifori et al. (2007) in particular has been highly influential in terms of ontology visualizations. ",
        "article_title": "Building Better Linked Data & Ontology Visualization Tools",
        "authors": [
            {
                "given": "John",
                "family": "Simpson",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; University of Guelph, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Jana",
                "family": "Smith Elford",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Shawn",
                "family": "Murphy",
                "affiliation": [
                    {
                        "original_name": "Semandra, Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Brundin",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Robert",
                "family": "Warren",
                "affiliation": [
                    {
                        "original_name": "Carleton University; University of Guelph, Canada",
                        "normalized_name": "Carleton University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02qtvee93",
                            "GRID": "grid.34428.39"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "data modeling and architecture including hypothesis-driven modeling",
            "interface and user experience design",
            "relationships",
            "data mining / text mining",
            "English",
            "graphs",
            "internet / world wide web",
            "networks",
            "software design and development",
            "knowledge representation",
            "semantic web",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " How can topic model visualizations aid in the exploration of large datasets? The standard method for visualizing topic models involves a two-stage process that begins with the use of a tool specifically designed to construct topic models on a specified corpus. MALLET (McCallum, 2002), a command line tool that implements the Latent Dirichlet Allocation (LDA) algorithm described by Blei et al. (2003), provides output in the form of text files that can be shared directly with other tools and seems to be the default tool of choice for the first stage. Regardless of the topic modeller used, its output is then passed to a separate tool for the second stage in the production of the visualization. These separate tools are either general visualization tools or topic model specific visualization tools. The use of general visualization tools is attractive because it typically requires a lower technical competency and allows the user to leverage skills they already have, but general tools provide fewer targeted affordances for exploration. Still, despite the existence of at least a dozen tools that have been used to visualize topic models, it remains the case that none of them are particularly well suited to exploring large datasets comprising thousands or millions of entries in a corpus distributed over a wide range of time and with a correspondingly large set of candidate topics produced by topic modelling.  This paper provides a solution to this problem by describing a tool that has been designed specifically to address this problem and that is now entering production. Before describing the tool and providing a short demonstration of its functionality, we summarize the current landscape of topic model visualizations and provide recommendations based on various use-cases.   Topic Modelling Visualizations  The visualization methods used across these tools can be fruitfully categorized using a set of modified categories drawn from work done by Katifori et al. (2007) on ontology visualizations:   •  Traditional charts (Bar, line, pie, scatterplot, etc.): D-VITA (Gunnemann et al., 2013); Tableau (Sharkey and Ansari, 2014); ParallelTopics (Dou et al., 2011); NIPS (Iwata et al., 2008); MetaToMATo (Snyder et al., 2013).   •  Network graphs: Gephi (Chen et al., 2013); Topicnet (Gretarsson et al., 2012).    •  Zoomable tools: an unnamed tool by Chaney and Blei (2012).    •  2D matrix: Serendip (Alexander et al., 2014); Termite (Chuang et al., 2012).  The variety of visualization techniques being pursued are evidence of the difficulties related to displaying relevant information from topic modelling, as each visualization method has been developed or chosen with a particular application in mind. The tools that particularly focus on exploring the underlying data and revealing connections are Serendip and MetaToMATo, neither of which is a graph-based visualization, and both of which become overwhelmed by large datasets. Gephi is a general-purpose graph-based visualization tool that has been used to visualize large datasets (Munster, Jockers), but the images it produces are static and it does not easily lend itself to sharing information derived from topic models. Drawing on lessons from a review of all these tools, but Serendip, MetaToMATo, and Gephi in particular, the tool we report on constructing is targeted at exploration using graph-based visualizations of topic models on large corpuses.   Topic Modelling Philosophy Journals  Motivation for this project has been brought about in part by work done over the past year with a corpus of philosophy journals acquired from JSTOR (Simpson et al., 2014). While investigating this corpus we made use of chart-based visualizations with R after preprocessing with MALLET. While there was much to be learned from the changing prevalence of particular topics over time, there were also a number of things that the visualizations were not able to tell us. In particular, our first visualizations made it difficult to easily see connections between the different topics, to see relations between the different journals, and to have this information present itself with increasing detail as required for the investigation. This challenge arose in large part because the dataset covered 27,536 journal articles from 10 different journals published between 1876 and 2008. Datasets of this size and diversity are becoming increasingly common, and so we moved to pursue a tool capable of visualizing the results of topic modelling. Not finding a suitable one for the exploration of large datasets, we set about designing a new one.   Visualizations in 3D  MacEachren et al. (1994) suggest that all visualizations are meant to principally perform one of the following tasks: clearly present previously discovered information, analyze a particular dataset, combine multiple datasets, or explore data for knowledge discovery. It is the last category on which our visualization research team has been particularly focused: visualizations that allow the user to not simply understand or analyze a dataset, but to explore it in hopes of uncovering new information.  As Lauren Frederica-Klein says in her 2013 digital humanities start-up grant application for the Interactive TOpic Model and MEtadata Visualization (TOME) project , regarding Wise et al.’s Galaxies visualization, ‘By forcing all thematic differences into a single two-dimensional presentation, information is inevitably lost’. In an effort to minimize this information loss, we are now creating an exploration tool for the purpose of experiencing a topic-modeled corpus in 3D space, using the free, open-source JavaScript framework  Famo.us.   The  Famo.us framework is designed to help ‘create smooth, complex UIs for any screen’, and will provide us some substantial benefits, first and foremost being that its CSS-like styling code is relatively straightforward to learn and implement. Additionally, using physics-based animations, three orientation axes, and opacity control, it is capable of rendering a touch-screen manipulable, three-dimensional environment, the interactivity afforded by which is something we believe will help greatly foster user exploration. As explained by Card et al. (1999), ‘This additional [3rd] dimension projects from the viewpoint toward infinity, creating a large visible workspace’, a decidedly beneficial quality when visually exploring a large dataset.   Chuang et al. (2012) describe the illegible results of experimenting with topic modelling visualizations where text is displayed directly in the visualization matrix. To reduce the volume of unwanted information, our design implements Healey’s (1996) notion of ‘knowledge discovery’, allowing the user to  filter unwanted data, and what Petre and Green (1992) describe as ‘secondary notation; the use of layout and perceptual cues which are not formally part of the notation (elements such as adjacency, clustering, whitespace, labeling and so on)’. Like Healey, we believe this will help ease any display or perception bottleneck and afford the user a unique opportunity to discover and explore unknown trends and relationships in the data. The user will customize the visibility of elements of the dataset and secondary notation by adjusting a series of controls to make alterations to the sensitivity of, for instance, clustering algorithms, labeling, or the visibility of edges in a network.   Since any single visualization is fundamentally limited in its ability to communicate very complex relationships, like Andrew Goldstone’s DfR browser (2013), and Snyder et al.’s Metadata and Topic Model Analysis Toolkit (MetaToMATo) (2013), our concept merges multiple visualizations into what Snyder et al. term ‘a single faceted browsing paradigm for exploration and analysis of document collections’. Word clouds generated via topic modelling, histograms, line graphs, and network diagramming are all experienced via a zoomable user interface (ZUI). Snyder et al. (2013) explain that ‘effectively exploring and analyzing large text corpora requires visualizations that provide a high level summary’. Figure 1 shows how the viewer will be able to move from that high-level summary, ‘flying’ deeper and deeper into the data, simultaneously experiencing multiple visualizations from unique vantage points, and, as espoused by Chuang et al. (2012), ‘drilling down’ into additional layers of information as desired.     Figure 1. As the user navigates deeper into the dataset, more and different relationships are revealed.  ",
        "article_title": "Exploring Large Datasets with Topic Model Visualizations",
        "authors": [
            {
                "given": "John",
                "family": "Montague",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Simpson",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Geoffrey",
                "family": "Rockwell",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": "Stan",
                "family": "Ruecker",
                "affiliation": [
                    {
                        "original_name": "Illinois Institute of Technology, USA",
                        "normalized_name": "Illinois Institute of Technology",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/037t3ry66",
                            "GRID": "grid.62813.3e"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Canada; University of Guelph, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "philosophy",
            "relationships",
            "text analysis",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper presents an attempt to apply state-of-the-art text mining techniques currently employed by business to the task of literary research. The research in question is the preparatory part of a project called Tolstoy Digital. The main goal of this project is to convert the recently digitized 90-volume collected works of Leo Tolstoy into a novel digital humanities resource. We intend to create the so-called semantic edition of Tolstoy’s works by providing it with a complete semantic markup consistent with TEI schema 1—a common standard of digital text encoding. The markup is supposed to include a wide spectrum of tags, from exact named entities and events to things like ‘editor’s note’, ‘author’s correction’, ‘draft version of the same text’, and so on. Named entities are also expected to be interlinked (co-reference resolution) and have external links to standard knowledge bases like DBpedia or Freebase.  With more than 46,000 pages of text that collectively contain 14.5 million words, Tolstoy is famed as one of the most productive writers ever. The sheer size of the material suggests that some automation of the markup is not only desirable but inevitable. In this paper we demonstrate how the use of an advanced language analyzer helps us extract named entities, various relations between them, and certain facts/events mentioned in Tolstoy’s texts.  The technology we apply to this task is called Compreno. It is a powerful text analysis platform being developed by ABBYY—a software company specializing in OCR, text analytics, and computational lexicography. Compreno converts text into a forest of syntactic-semantic trees that comprise dependency links and constituency structure. The analysis is based on the universal semantic hierarchy—a complex WordNet-like 2 ontological structure that stores meanings rather than words.   The resulting trees contain nodes with all sorts of linguistic information attached to them: semantic classes from the said hierarchy, surface syntax slots, deep syntax slots, as well as sets of grammemes. Here is an example of a tree that the Compreno parser yields for the phrase ‘ 28-го октября Кутузов с армией перешел на левый берег Дуная и в первый раз остановился, положив Дунай между собой и главными силами французов’ ( On the twenty-eighth of October Kutuzov with his army crossed to the left bank of the Danube and took up a position for the first time with the river between himself and the main body of the French) from Tolstoy’s  War and Peace:      Figure 1. Sample Compreno tree (automatically generated, no manual corrections). Semantic classes from the hierarchy are green and capitalized, surface syntax slots are blue, deep semantic slots (they can be compared to Charles Fillmore’s [1968] deep cases) are dark red. Note that just like in English, in Russian there are several meanings for the word ‘силы’ (forces), but the parser performed disambiguation correctly, choosing the ‘FORCES_AS_PEOPLE’ semantic class. The parser is also capable of anaphora resolution (for more information on that, see Bogdanov et al. [2014]). We realize that the example above is but a glimpse of the employed technology. Unfortunately, a thorough description of Compreno is well beyond the scope of this proposal. More details on that technology can be found in Anisimovich et al. (2012). So for now we will just note that for our research we used the information extraction system built upon and powered by Compreno. The system in question allows writing sets of production rules to extract facts and entities from unstructured texts. The main advantage is that deep semantic representation of text provided by Compreno enables us to describe a whole range of different variants of a phrase in a very concise manner. For instance, we do not need to care about the word order (which is flexible in Russian), since the syntactic roles of different words remain the same. And even in case of voice transformation ( He loved her →  she was loved by him), only surface syntax slots change, while deep slots remain unchanged. Here is an example of a simple production:  \"VERBS_OF_COMMUNICATION\"  [Agent: active_side  \"HUMAN\"]   [Addressee: passive_side  \"HUMAN\"]  =>; In this case we demand the system to find any Compreno subtree that has a node with a semantic class ‘VERBS_OF_COMMUNICATION’ or any of its descendant classes (since ‘VERBS_OF_COMMUNICATION’ is a very high-level class within our hierarchy and there are many lower classes that inherit from it) and at least two children nodes—one (or more) with ‘Agent’ deep syntax slot and another with ‘Addressee’ slot. Both children must also belong to / be inherited from a semantic class ‘HUMAN’ (which contains all sorts of subclasses that define people—names of occupations, social roles, relation terms, known proper names, etc.). Despite its obvious simplicity, this rule will extract many examples of communication between people (or, in our case, characters) like the ones below:   Дмитрий, — обратился Ростов к лакею на облучке. — Ведь это у нас огонь? ( ‘Dmitri’,  addressed Rostov to his valet on the box, ‘those lights are in our house, aren’t they?’)   Ну же пошел, — кричал он ямщику. ( ‘Now then, get on’,  he shouted to the driver).   Никаких извинений, ничего решительно, — говорил Долохов Денисову ( ‘No apologies, none whatever’,  said Dolokhov to Denisov)  Formal representation of entities and facts being extracted is performed by means of ontology engineering. We develop ontologies using OWL language 3 developed by the W3C. In the executable right-hand side of a production we can either create a new information object of a certain class of an ontology or modify the existing ones (add a surname attribute to a Person-class object, for instance). After the implementation of the rules, we receive the result of the information extraction process in the form of an XML document consistent with the RDF schema. 4 Facts and entities also have links to their annotation—i.e., exact fragments within text. Here is a description of a ‘SpeechActivity’ fact, one of the many that were extracted from the second volume of  War and Peace:    <BasicFact:SpeechActivity rdf:nodeID= \"bnode53D90FDA-F8F1-4DCB-8E8B-353456BEA164\" >   <BasicFact:theme rdf:nodeID= \"bnode261C3F83-0E71-4D4A-B60B-9268129D59F6\" />   <BasicFact:text rdf:datatype= \"http://www.w3.org/2001/XMLSchema#string\"  xml:lang =\"ru\">Однако я тебя стесняю, </BasicFact:text>   <BasicFact:listener rdf:nodeID =\"bnode99CCE0DA-BDF2-4ECB-8CA1-8222D19F5641\" />   <BasicFact:type rdf:resource=\" http://www.abbyy.com/ns/BasicFact#TOS_Quotation\" />   <BasicFact:speaker rdf:nodeID= \"bnodeF9F72F9B-9C30-43DD-B4F2-9E2EE3BE10DD\" />   </BasicFact:SpeechActivity>  ‘Listener’ and ‘speaker’ attributes contain links to information objects of the Person class. In this case they point to Boris Drubetskoy (Person with surname ‘Друбецкой’ and name ‘Борис’) and Nicholas Rostov (Person with surname ‘Ростов’ and name ‘Николай’), respectively. Text attribute contains a string with the exact text of the speech. Below is the piece of text upon which the fact was extracted:   Ростов сделался не в духе <. . .> Он встал и подошел к Борису.    — Однако я тебя стесняю, — сказал он ему тихо, — пойдем, поговорим о деле, и я уйду.  ( Rostov became sullen <. . .> He got up and approached Boris.    ‘I’ve come at a bad time I think,’ he said to him in a low voice. ‘Let us talk business, and then I’ll leave’.)  So with the help of this system we can easily research the speech characteristics of each hero. Note also that this particular example clearly demonstrates the importance of correct anaphora resolution for the tasks of our in-depth text research.  The information extraction system used for this research has been in development for several years. It contains dozens of rule libraries with hundreds of extraction and identification (i.e., object merging) rules. Therefore, the first stage of our research consisted of applying these already existing rules to Tolstoy’s works (we limited ourselves to  War and Peace at this point) with further analysis of the results.   Even the initial results seem very promising. For instance, the system correctly extracted the age of certain heroes, the parent-child relations between prince Vasili and Helene and many other family relations, the ‘illness’ of Dolokhov when he was wounded in a duel and the ‘Termination’ of the said illness when he was finally cured, the ‘Friendship’ between Dolokhov and Helene, the making of bast shoes by a servant of the Rostov family named Prokofy, and many more facts that to some extent represent the plot of the book. The co-reference relations between Person-class objects were also established with great accuracy: ‘Vasili Kuragin’, ‘prince’, and ‘Vasili’ are recognized as one individual; the same is true for ‘Andrew Bolkonski’, ‘prince Andrew’, ‘Andrew Nikolaevich’, and the diminutive ‘Andrysha’.  We were also able to find out quite a lot about the characters by exploring the positions they occupy in the predicate-argument structure. For instance, our data shows that prince Vasili Kuragin occurs more often in Agent and Possessor positions, while Bolkonskaya demonstrates inclination towards the roles of Experiencer, Object, and Addressee. This might be a reflection of character traits—the cunning and intrigue of profit-seeking prince Vasili versus the sensitivity and timidity of the shy and awkward princess Mariya.    Character Agent Object Experiencer Addressee Possessor   Andrey Bolkonsky 705 183 225 59 83   Pierre Bezukhov 387 101 103 28 50   Nikolai Rostov 330 102 128 19 56   Vasili Kuragin 284 72 68 16 46   Mariya Bolkonskaya 225 92 132 24 39   Anna Drubetskaya 224 39 29 3 15   Mikhail Kutuzov 217 49 49 15 42   Nikolay Bolkonsky (the old count) 164 34 36 8 18   Boris Drubetskoy 157 77 55 15 22   Elisabeth Bolkonskaya (the little princess) 147 55 48 15 27   Anna Scherer 142 24 38 9 26   Natasha Rostova 142 33 25 10 22   Pyotr Bagration 136 27 33 13 16   Anatole Kuragin 113 32 26 8 14   Table 1. Most frequent occupants of different syntax deep slots (first volume only). The resulting table also shows that in the first volume of the epic, princess Anna Mikhailovna Drubetskaya is a much more active character than her son Boris, although he is mentioned no less often than her. This is also clearly the reflection of the plot, where a determined, businesslike mother takes care of the career of her still-shy son (who’d become just as pragmatic later on). Speech activity statistics also promises to be quite informative, though we have not analyzed it enough to come to conclusions yet. Below is some data we have obtained so far:    Figure 2. Speech activity frequency of different characters.  Figure 3. Listening frequency of different characters. The next step of the research is the creation of our own extraction model within the existing system. This model, already in the making, is going to be designed and adjusted specifically to meet the needs of our research and is expected to help us extract much more information about Tolstoy’s characters, their description by the author, and their relations between each other.  Notes 1. TEI: Text Encoding Initiative, http://www.tei-c.org. 2. WordNet, http://wordnet.princeton.edu/. 3. OWL Web Ontology Language Overview, http://www.w3.org/TR/2004/REC-owl-features-20040210. 4.  Resource Description Framework, http://www.w3.org/RDF.   ",
        "article_title": "Automatic semantic tagging of Leo Tolstoy’s works",
        "authors": [
            {
                "given": "Daniil",
                "family": "Skorinkin",
                "affiliation": [
                    {
                        "original_name": "National research university 'Higher school of economics'; ABBYY software company",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            },
            {
                "given": "Anastasia",
                "family": "Bonch-Osmolovskaya",
                "affiliation": [
                    {
                        "original_name": "National research university 'Higher school of economics'",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "xml",
            "natural language processing",
            "literary studies",
            "text analysis",
            "English",
            "resource creation",
            "linking and annotation",
            "semantic analysis",
            "data mining / text mining",
            "and discovery",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The UC CEISMIC Digital Archive (Millar et al., n.d.) was developed to store and make publically available digital content produced during the course of a major earthquake sequence, which hit the Canterbury region in the South Island of New Zealand between 2010 and 2012. The project was presented at DH2012, Hamburg, when its key assets had just gone live (Smithies, 2012). This report on the current state of the archive is the most significant to be made at a digital humanities conference since then. It will report on governance, ethics, and technology but, more importantly, reflect on lessons learned and outline plans for the next decade of operation.  The Canterbury earthquake sequence has been described as a truly ‘large-scale’ event. It included a devastating 6.3 magnitude tremor on 22 February 2011 that resulted in 185 deaths and caused significant damage to large portions of the city. Over 12,000 aftershocks followed, including 31 greater than magnitude 5 on the Richter scale. Recovery is ongoing, with portions of the inner city only recently opened to the public. Significant infrastructure renewal and building remediation and construction are expected to continue across the city and surrounding areas for another decade (Johnston, 2014). The University of Canterbury, where the UC CEISMIC team is based, is planning for over 1,000 construction workers to be on campus during 2015 to repair earthquake damage and build a significant new science and engineering block.  The UC CEISMIC project has been positioned by the University of Canterbury as one of four flagship projects targeted to lead the way towards recovery, and leads a consortium of 11 national organizations that has recently expanded to include scientific as well as cultural heritage agencies. The archive currently stores 80,000 public items with a further 15,000 either embargoed or in process, ranging from audio and video interviews to images and official reports. Tens of thousands more items await ingestion. Significant lessons have been learned about data integration in post-disaster contexts, including but not limited to technical architecture, governance, ingestion process, and human ethics. Team members have recently gained access to almost 1 million historic tweets, which will be subject to computationally intensive analysis (Williford and Henry, 2012).  The depth and breadth of the CEISMIC project, and the innovative work it has prompted, are directly attributable to it being a digital humanities project. The archive was modeled on the Center for History and New Media’s September 11 Digital Archive 1 and conceived, designed, and built by digital humanists. Its operational program office is staffed and managed by humanities and digital humanities graduates. Its core values are derived from principles instilled in all DH graduates: civic responsibility, the value of our digital cultural heritage, deployment of technology to enable research, use of open-source tools, the importance of craft skills and tacit knowledge, the importance of communication and community engagement, and our responsibility to future generations. It presents an excellent model for digital humanities teams faced with the problem of archiving significant post-disaster events.   The archive has implemented a technical architecture optimized to resolve issues with data integration in post-disaster contexts (Spennemann, 1999). The system relies on a bespoke research-oriented repository built using open-source tools and hosted at the University of Canterbury, New Zealand. It sits on virtualized university infrastructure, including access to New Zealand’s national High Performance Computing (HPC) infrastructure and REANNZ high-speed broadband research network. Tiered backup and recovery stores all content on both high-availability disk and off-site tape storage. DigitalNZ, a unit within the New Zealand National Library, performs national metadata aggregation (Oldman et al., 2014). This allows the archive to leverage an extensive range of existing government IT infrastructure; although over 75% of CEISMIC content is hosted at the University of Canterbury, content is contributed from a wide range of government agencies. The federation is bonded at a technical level through DigitalNZ’s modified Dublin Core schema, with each contributing archive responsible for adding additional metadata if possible (Sugimoto et al., 2002). Access is provided through one key and two subsidiary websites, a mobile app, and two Application Programming Interfaces (APIs). Long-term preservation has been outsourced to New Zealand’s National Digital Heritage Archive (NDHA), a government agency responsible for preserving key national digital assets for the long term.  Unusually for a digital humanities project, CEISMIC has grown to become a significant ‘enterprise’-level undertaking, with a mature portfolio of services, relationships with peak government agencies from the chief executive level down, integration into the university’s project and technical change management functions, and strategic concerns that intersect not only with university but local and national government policy. Because of its vocal advocacy for digital cultural heritage archiving in the immediate post-disaster context, the project is positioned as a possible future repository of earthquake-related big data of radical size and scope (Spennemann, 1999). It is recognized as one of the success stories of the national earthquake experience; it reflects the development of post-disaster social capital in fundamental terms (Aldrich and Meyer, 2014).  Despite this, the CEISMIC project is well known as a community-focused effort with minimal staff and resources, struggling to attract external funding and move key operational staff from temporary to permanent contracts. Although there is awareness of the project at the ministerial level across multiple government departments, the scale of the CEISMIC ecosystem has made it difficult to secure a single business owner outside the research community. This tension is perhaps typical of many digital humanities projects: at once creative and vibrant, and under threat.  Despite this, and unlike other digital humanities post-disaster projects (Rivard, 2012), the intention is to keep the CEISMIC project operational for several decades into the future. This is a perhaps radical aspiration that contrasts with the more modest goals of similar projects, but it offers interesting perspectives for the global community. New Zealand has a particular set of cultural values (and institutional characteristics) that make projects like CEISMIC possible. Lack of state boundaries, a small digital GLAM sector, and a tradition of cultural innovation and inter-disciplinary cooperation combine to allow the CEISMIC ecosystem to grow. Unlike other cyber-infrastructure projects, the archive has exceeded its initial goals and is implementing strategies that appear capable of securing it a long-term future. Again, this was due to the application of digital humanities values rather than computer science and engineering values, and achieved because it evolved from the bottom up, rather than being imposed (even with the benefit of significant funding) from the top down (Dombrowski, 2013). For these reasons it presents the global digital humanities community with a useful model that encompasses design, development, ethics, governance, and community engagement.  Note 1. Roy Rosenzweig Center for History and New Media, September 11 Digital Archive, http://911digitalarchive.org. ",
        "article_title": "Building Post-disaster Social Capital: A Current State Report on the UC CEISMIC Digital Archive",
        "authors": [
            {
                "given": "James",
                "family": "Smithies",
                "affiliation": [
                    {
                        "original_name": "University of Canterbury, New Zealand",
                        "normalized_name": "University of Canterbury",
                        "country": "New Zealand",
                        "identifiers": {
                            "ror": "https://ror.org/03y7q9t39",
                            "GRID": "grid.21006.35"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Millar",
                "affiliation": [
                    {
                        "original_name": "University of Canterbury, New Zealand",
                        "normalized_name": "University of Canterbury",
                        "country": "New Zealand",
                        "identifiers": {
                            "ror": "https://ror.org/03y7q9t39",
                            "GRID": "grid.21006.35"
                        }
                    }
                ]
            },
            {
                "given": "Chris",
                "family": "Thomson",
                "affiliation": [
                    {
                        "original_name": "University of Canterbury, New Zealand",
                        "normalized_name": "University of Canterbury",
                        "country": "New Zealand",
                        "identifiers": {
                            "ror": "https://ror.org/03y7q9t39",
                            "GRID": "grid.21006.35"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "licensing",
            "repositories",
            "archives",
            "sustainability and preservation",
            "and Open Access",
            "English",
            "cultural infrastructure",
            "copyright"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This session reports on a significant information infrastructure project in the history of science as the discipline prepares itself to deal with the major scholarly and societal challenges of the 21st century. History of science scholars are seeking new analytical tools and research methodologies and are looking to computer-aided tools that utilise well-structured data. The session is drawn from participants in an international collaborative project between the History of Science Society, the University of Oklahoma, the University of Melbourne, and the Division of History of Science of the International Union of the History and Philosophy of Science. Simply stated, the IsisCB project is taking 100 years of history of science bibliographic data and transforming it into a web or graph of information objects that represent not just the cited works but, more importantly, the people, organisations, events, subjects, themes, and time periods of the world captured in the work of the historians. The project involves digitisation of print-only materials, optical character recognition, computer-aided parsing, semantic transforms, large-scale data transfer, industrial‐scale search and faceted browse facilities, visual analytics, application programming interfaces, and user‐facing design. It involves taking a community and a well-established print-focused way of working on a revolutionary change process to create an open, persistent, citable, high-quality data source that will be amenable to utilisation using linked data protocols and technologies. The history of science is a global, multidisciplinary endeavour that seeks to help us understand one of the most potent forces that has shaped contemporary society—namely, science. Although often represented as a discipline in its own right, practitioners are frequently located in other disciplinary settings, such as history, philosophy, population health, medicine, social science, and media and communication studies. Its themes and studies, while often starting with a local story, inevitably, like the science itself, document networks and interconnections that cover all continents. In the early 20th century, a community of practice began to emerge in Europe as historians of science started to formalise their own networks and communication channels. In 1913 they created the  Isis Current Bibliography of the History of Science, and it has been maintained as an annual print‐based bibliography ever since. The latest edition was published by the History of Science Society in 2014 and included references to over 4,000 scholarly works. The total corpus will comprise hundreds of thousands of records and millions of relationships and will continue to grow.   The three papers will be presented by members of the IsisCB project working group, covering a range of the technical and informatic challenges, and exploring the opportunities for the history of science discipline emerging as a result of the development of the IsisCB web resource.  1. Mapping the History of a Discipline through Curated Bibliographic Data: A Case Study of Social Networks as Found in the Isis Bibliography of the History of Science, 1913–2013  Stephen P. Weldon University of Oklahoma Sylwester Ratowt University of Oklahoma  Birute Railiene Wroblewski Library of the Lithuanian Academy of Sciences  John Stewart University of Oklahoma The 100-year-old Isis Bibliography is a valuable data resource for exploring the development of the field of history of science over the course of the 20th century. The recent collaborative project between the Isis Bibliography of the History of Science and the eScholarship Research Centre (ESRC) at the University of Melbourne has created a robust digital research dataset consisting of over 300,000 bibliographic citation records going back to 1913. The new tool, called the IsisCB Platform, is more than a bibliographic database; it makes use of bibliographic data but supplements that data with links to other content using Linked Open Data protocols.  To create this tool, we have had to ingest new data and rethink how bibliographic data can be used in a big-data, computational environment. Part of the IsisCB project involved digitizing 60 years of data in several volumes of cumulative print bibliographies. We are now parsing and merging this data with 40 years of born-digital data. The combined bibliographic dataset covers 100 years and spans the period in which history of science developed as an academic discipline. By reconceptualizing this dataset as a resource linking publications, institutions, and people, it becomes an information-rich resource that can be used to tell the story of scholarship in this area over the 20th century.  This paper serves two functions. First, it introduces the IsisCB Platform and explains how the bibliographic dataset has been reconceptualized so as to create a scholarly tool with multiple research capabilities. Second, the paper provides a brief example of how a subset of this data can be used to explore the history of the discipline itself. By using computational tools, we can address historical questions about the social, institutional, and intellectual history of the scholars themselves. It will be shown how the IsisCB data can serve as the foundation for digital humanities research when it has been thoughtfully restructured in the right way. Bibliographic records contain extensive information about the institutional and publication information of people. Information scholars working on social network analysis have developed various ways of comparing and understanding multiple kinds of networks that can be produced from bibliographic data (Batagelj and Cerinšek, 2013). We will be employing some of these methods. Recent work in social network analysis has shown the utility of bibliographic datasets for exploring history of disciplinary change in the humanities (So and Long, 2013). Such an analysis has not yet been done to any great extent in the field of history of science, but the field is ripe for this kind of work (Laubichler, Maienschein, and Renn, 2013).  The composition of the Isis Bibliography dataset over the years makes this analysis both possible and exciting. The Isis Bibliography contains full author, title, and citation information for each of its entries. Unlike some bibliographic databases, the Isis Bibliography also has classifications or subject index terms for most of the items, along with descriptive material for many of them. Moreover, because there have been relatively few bibliographers over the years, there has been a great deal of consistency in the categorization and tagging of the records (Weldon, 2013a). Finally, the Isis Bibliography contains the best set of information on the academic network of historians of science. The first installment of the Isis Bibliography was one of the critical moments in the founding of the discipline. George Sarton published his bibliography in 1913 in the first issue of his journal  Isis. Over the next couple of decades, Sarton and several other scholars in Europe and America began producing bibliographies, publishing journals, and holding conferences. These would ultimately lead to the creation of a small but dedicated community of scholars focused on the study of the history of science. As the discipline grew, so did the Isis Bibliography.   We will conclude our paper by looking at how data in the IsisCB can illuminate historical questions. We will build a few network graphs to help us analyze the scholarly genealogy of the discipline. We will try to tease out some of the stories that lie buried within it: Which universities and research institutes have been instrumental to the growth of the discipline? How so? In what ways has gender played a role in the development of the discipline? Do the data help us identify national styles? In the end, the paper will showcase preliminary results of our analysis to show how bibliographic data can be applied in new and unique ways when the data are made available in accessible ways. References  Alfonso-Goldfarb, A. M., Waisse, S. and Ferraz, M. H. M. (2013). From Shelves to Cyberspace: Organization of Knowledge and the Complex Identity of History of Science.  Isis, 104(3): 551–60, doi:10.1086/673274.   Allen, C. and the I Group. (2013). Cross-Cutting Categorization Schemes in the Digital Humanities.  Isis, 104(3): 573–83, doi:10.1086/673276.   Alsukhni, M. and Zhu, Y. (2012). Interactive Visualization of the Social Network of Research Collaborations.  2012 IEEE 13th International Conference on Information Reuse & Integration (IRI), doi:10.1109/IRI.2012.6303017.   Anderson, R. J. (2013). The Organization and Description of Science Archives in America.  Isis, 104(3): 561–72, doi:10.1086/673275.   Batagelj, V. and Cerinšek, M. (2013). On Bibliographic Networks.  An International Journal for All Quantitative Aspects of the Science of Science, Communication in Science and Science Policy, 96(3): 845–64, doi:10.1007/s11192-012-0940-1.   Coscia, M., Giannotti, F. and Pensa, R. (2009). Social Network Analysis as Knowledge Discovery Process: A Case Study on Digital Bibliography.  2009 International Conference on Advances in Social Network Analysis and Mining, doi:10.1109/ASONAM.2009.65.   Huang, Z., Yan, Y., Qiu, Y. and Qiao, S. (2009). Exploring Emergent Semantic Communities from DBLP Bibliography Database.  2009 International Conference on Advances in Social Network Analysis and Mining, doi:10.1109/ASONAM.2009.6.   Laubichler, M. D., Maienschein, J. and Renn, J. (2013). Computational Perspectives in the History of Science: To the Memory of Peter Damerow.  Isis, 104(1), 119–30, doi:10.1086/669891.   Mattison, D. (2010). The Twittering of the Search World (LiveLinks).  Searcher, 18(7): 24.   Mullins, N. C. and Mullins, C. J. (1973).  Theories and Theory Groups in Contemporary American Sociology. Harper & Row.   Schatten, M. (2013). What Do Croatian Scientists Write About? A Social and Conceptual Network Analysis of the Croatian Scientific Bibliography.  Interdisciplinary Description of Complex Systems, 11(2): 190.   Sidiropoulos, A. (2012). Finding Communities in Site Web-Graphs and Citation Graphs, http://arxiv.org/pdf/1208.5464.pdf.   So, R. J. and Long, H. (2013). Network Analysis and the Sociology of Modernism.  Boundary 2, 40(2): 147–82, doi:10.1215/01903659-2151839.   Tang, J., Zhang, D. and Yao, L. (2007). Social Network Extraction of Academic Researchers.  Seventh IEEE International Conference on Data Mining (ICDM 2007), doi:10.1109/ICDM.2007.30.   Weldon, S. P. (2013a). Bibliography Is Social: Organizing Knowledge in the Isis Bibliography from Sarton to the Early Twenty-First Century.  Isis, 104(3), 540–50, doi:10.1086/673273.   Weldon, S. P. (2013b). Introduction.  Isis, 104(3): 537–39, doi:10.1086/673272.   Yan, E. and Ding, Y. (2012). Scholarly Network Similarities: How Bibliographic Coupling Networks, Citation Networks, Cocitation Networks, Topical Networks, Coauthorship Networks, and Coword Networks Relate to Each Other.  Journal of the American Society for Information Science and Technology, 63(7): 1313–26, doi:10.1002/asi.22680.   2. Why Standards Are Critical: Graphing Knowledge by Building on Well‐Established Entity‐Relationship Standards Can Look Well into the Future  Ailie Smith University of Melbourne Marco La Rosa University of Melbourne Through its history as the Australian Science Archives Project (1985–1999), and the Australian Science and Technology Heritage Centre (1999–2007) (eScholarship Research Centre, n.d.), the eScholarship Research Centre (ESRC) at the University of Melbourne has well‐established links with the international history of science community. Since 2014 the ESRC has been collaborating with Dr. Stephen Weldon from the University of Oklahoma on an Alfred P. Sloan Foundation–funded project, the Isis Document Indexing Platform: A Curated and Community‐Based Resource for History of Science. The ESRC has been using the World Wide Web to publish and disseminate history of science and other humanities datasets—including bibliographic data, archival descriptive data, and contextual information—since the mid‐1990s. Examples of these are web resources such as the  Frank Macfarlane Burnet Guide to Records (McCarthy et al., 2001) and the  Encyclopedia of Australian Science (eScholarship Research Centre, 2012), which includes bibliographic records from the ‘Bibliography of the History of Australian Science’, published annually in  Historical Records of Australian Science (Cohn, 2014).  These resources are created from data captured in two systems developed by the ESRC: the Online Heritage Resource Manager (OHRM) and the Heritage Documentation Management System (HDMS). Both of these systems are based on international archival descriptive standards (ISAD[G] and ISAAR[CPF]) (International Council on Archives, 2000; 2004) and have long been used for the generation of HTML outputs for dissemination of information as web resources. However, these outputs are not readily sharable with other organisations or other resources operating in a similar sphere, resulting in limited reuse of this data and increased potential for duplication of work. The increasing use of Extensible Markup Language (XML) schemas by information professionals to enable system interoperability, data sharing, and data reuse has provided the ESRC with greater opportunities to collaborate, to rethink the way we present information online, to share data with and harvest data from other systems, and to represent and reinterpret data through the use of a variety of search and analytical tools. By generating Metadata Object Description Schema (MODS), Encoded Archival Description (EAD), and Encoded Archival Context–Corporate Bodies, Persons and Families (EAC‐CPF) (Library of Congress, 2013a; 2013b; Staatsbibliothek Zu Berlin, 2012) XML outputs for these resources—in parallel with the web presentation HTML outputs—the ESRC can make better use of the data underpinning these resources. This includes not only the applications and services developed to display and navigate individual datasets but also the capacity to build services that run across a range of datasets, and to harvest and share data with external sources at national and international levels to improve collaboration and extend the use of the data beyond individual research projects. It was a natural progression for the ESRC to begin working with Dr. Stephen Weldon from the University of Oklahoma on the Isis Document Indexing Platform: A Curated and Community‐Based Resource for History of Science project. The IsisCB web resource brings together 100 years of bibliographic records from the  Isis Current Bibliography of the History of Science (‘About Isis CB’) with the technology base of the ESRC to create a rich, searchable, contextualised online resource for exploring publications relating to the history of science. Working with researchers from the University of Oklahoma, the ESRC is helping to create XML records, develop ingests of XML and free‐text content, and build on existing technology platforms to create search and browse interfaces for interacting with the bibliographic data. This work will take resources that have previously been available in static print formats and allow them to be explored and recompiled in new ways, providing a valuable tool for historians of science.  The work on the IsisCB Platform builds on a range of technology developments that the ESRC has undertaken and collaborated on over recent years, making use of XML data. Internally, the ESRC has been developing indexing, search, and faceted browse and filtering capabilities using XML data and Solr/Lucene technology, supported by high-availability, large-scale server infrastructure that has been developed and continues to be supported in-house (La Rosa et al., 2014). In addition, XML data are being used by the ESRC to develop flexible online presentation of humanities datasets that are targeted at the needs of specific user communities. Externally, two of the ESRC’s web resources began being harvested in 2010 by the National Library of Australia’s Trove service (Dewhurst, 2010), which had adopted the EAC‐CPF schema for harvesting information about people and organisations and their related records and publications (Dewhurst, 2008) . XML data from a number of ESRC resources have also been harvested by the Humanities Networked Infrastructure (HuNI) project (Deakin University, 2014), and the underlying data structure has been mapped to the Registry Interchange Format–Collections and Services (RIF‐CS) schema used by Australian National Data Service (ANDS) for their Research Data Australia registry (Australian National Data Service, n.d.). Through collaborations with other professionals—within and beyond information professions—the eScholarship Research Centre is contributing to and developing research infrastructure, information resources, and public knowledge spaces that widely communicate humanities research data.  With a focus on the development of the Isis Document Indexing Platform, this paper will investigate the ESRC’s experience in the development and utilization of MODS, EAD, and EAC‐CPF XML data exported from its systems, and the collaboration with other researchers to develop XML outputs from their own systems. It will also look at a range of tools and services that leverage the standards‐based XML outputs to allow researchers to explore, interrogate, and present the data in order to reinterpret and derive new understandings of it, and the technologies that underpin these services. The paper will also discuss the potential to better engage with public search interfaces such as Google by including schema.org markup metadata in web templates in order for resources to reach a wider audience (Starr, 2014). The paper will explore the way the standards‐based approach will help to create the Isis Document Indexing Platform as a sustainable history of science resource well into the future. References  ‘About Isis CB’. (n.d.). ISIS Current Bibliography, http://isisbibliography.org/about/ (accessed 3 November 2014).   Australian National Data Service. (n.d.). Research Data Australia, http://researchdata.ands.org.au/.   Cohn, H. M. (ed.). (2014). Bibliography of the History of Australian Science, No. 34, 2013.  Historical Records of Australian Science,  25(1): 123–41.   Deakin University. (2014). HuNI Virtual Laboratory, https://huni.net.au.   Dewhurst, B. (2008). People Australia: A Topic-Based Approach to Resource Discovery.  VALA2008 14th Biennial Conference and Exhibition, http://www.valaconf.org.au/vala2008/papers2008/116_Dewhurst_Final.pdf.   Dewhurst, B. (2010). Encyclopaedia of Australian Science Available via Trove. ARDC Party Infrastructure Project Blog, National Library of Australia, 7 May 2010, https://wiki.nla.gov.au/display/ARDCPIP/2010/05/07/Encyclopaedia+of+Australian+Science+available+via+Trove.   eScholarship Research Centre. (2012).  Encyclopedia of Australian Science, http://www.eoas.info.   eScholarship Research Centre. (n.d.). Our History, http://www.esrc.unimelb.edu.au/about?us/our?history/ (accessed 3 November 2014).   International Council on Archives. (2000).  ISAD(G): General International Standard Archival Description. 2nd ed., http://www.icacds.org.uk/eng/ISAD(G).pdf.   International Council on Archives. (2004).  ISAAR (CPF): International Standard Archival Authority Record for Corporate Bodies, Persons and Families. 2nd ed., http://www.icacds.org.uk/eng/ISAAR(CPF)2ed.pdf.   La Rosa, M., McCarthy, G. and Smith, A. (2014). Nothing Can Ever Be Lost: Infrastructure for Archival Research and Dissemination. eResearch Australasia 2014, http://eresearchau.files.wordpress.com/2014/07/eresau2014_submission_29.pdf.   Library of Congress. (2013a). Encoded Archival Description: Version 2002 Official Site, http://www.loc.gov/ead/ (accessed 3 November 2014).   Library of Congress. (2013b). Metadata Object Description Schema: Official Web Site, http://www.loc.gov/standards/mods/ (accessed 3 November 2014).   McCarthy, G., Manhal, O., O’Sullivan, L., Tropea, R. and Sherratt, T. (2001).  Frank Macfarlane Burnet Guide to Records, Australian Science and Technology Heritage Centre, Melbourne, http://www.austehc.unimelb.edu.au/guides/burn/burn.htm (accessed November 2014).   Staatsbibliothek Zu Berlin. (2012).  Encoded Archival Context Corporate Bodies, Persons and Families, http://eac.staatsbibliothek?berlin.de/ (accessed 3 November 2014).   Starr, B. (2014). Demystifying the Google Knowledge Graph.  Search Engine Land, 2 September 2014, http://searchengineland.com/demystifying?knowledge?graph?201976.    3. The Case of the Missing Records: The Long Journey of the Correspondence of Ferdinand Von Mueller into the World of Digital History of Science  Gavan McCarthy University of Melbourne Within the context of the development of the Isis Cumulative Bibliography Online (IsisCB) as a global information infrastructure service for the history of science community, this paper presents a case study that demonstrates how a long-term research activity, through the creation of a scholarly edition of scientific correspondence, can be liberated from its print paradigm strictures to join the 21st-century world of interconnected knowledge. The Von Mueller Correspondence project has produced a corpus of over 15,000 digitally transcribed letters from the 1840–1896 period. These are complemented by materials in a range of forms that refer to Mueller dating from 1814 to 1931. Mueller was a prolific correspondent and established links with hundreds of fellow botanists and biologists across the globe; most of these, and certainly the most notable, will be registered in the IsisCB as Authority Records with links to publications about them and in some cases publications by them. The plan is to systemically interlink the Von Mueller Correspondence digital corpus and the IsisCB and develop the synergies that will drive digital humanities analysis and future scholarly endeavour. In 1987 Professor Rod Home of the Department of History and Philosophy of Science at the University of Melbourne embarked on a project to locate, edit, and publish the correspondence of Baron Ferdinand von Mueller. Mueller was Victorian government botanist (1853–1896) and one of Australia’s most well-known 19th-century scientists, not just in Australia but across the globe. Mueller was also director of the Melbourne Botanic Gardens (1857–1873) and played a major role in the establishment of the Herbarium at the Gardens as a centre of research and the dissemination of Australian plants and botanical knowledge. These roles remain the foundations of the Gardens and the Herbarium today, over 160 years later. The major challenge of this project, which included collaborators from the United Kingdom and Germany, was to locate as much of the outgoing correspondence as could be found in archives, universities, museums, and herbaria across the world. This need to reconstruct the Mueller ‘archive’ resulted from the destruction of the locally held letter-copy books, inward correspondence, and associated files about three decades after Mueller’s death in 1896. Why these records were destroyed is still shrouded in mystery. Subsequent research revealed that Mueller may have sent out well over 100,000 letters during his career. The research team, over the last 25 years, have managed to locate, copy, transcribe, and prepare for scholarly publication over 15,000 items. Over 750 selected documents have been published in print form, with some made available on CD-ROM (Home et al., 1998; 2002; 2006). This work still continues today due to the continuing commitment of the scholars, despite the initial funding having been exhausted for many years. New letters and related documents continue to appear, and new knowledge that helps in the understanding and annotation of the existing materials continues to be unearthed. This process of steady accumulation and explication of knowledge sits at odds with the original print-based paradigm that configured the project at its commencement in the late 1980s. It would appear that an online edition capable of batch-processed updating could provide both the research team and scholars from all nations with a reference and analytical resource geared to the expectations of 21st-century digital humanities. In 2011 Professor Rod Home approached the eScholarship Research Centre (ESRC) at the University of Melbourne for ‘advice on databases’ in relation to the Mueller Correspondence project. However, the subtext was both more revealing and ultimately more worrying. The Mueller project team had been using Apple Macintosh computers since 1987 and had been transcribing the photocopied source materials into Word for Mac Version 1, which had originally been released in 1985. The request in 2011 was prompted by two events: the Mac used to hold the corpus was showing signs of not booting up correctly, and the upgrade to the latest edition of Word for Mac revealed that a few hundred items were in the earliest forms of Word and could not be brought forward without the loss of the styling and annotation that the scholars used to mark up the letters. Although the scholars were interested in and aware of mark-up languages, such as SGML, at the outset of the project, they had not conceived it as a digital humanities project but as a traditional edited scholarly correspondence project with a print form being the final output. The scholars used digital technology that was readily available, improved productivity, and helped them achieve their print publications. They had not anticipated time frame of the project, now entering its fourth decade, nor the changes in technology that would occur over that period. To deal with the ‘crisis’ facing the project—that is, the potential loss of the 15,000 items—a full copy of the corpus and related materials was copied to the ESRC project ‘file share’ (a structured and managed digital object storage platform) and protocols enacted for batch updates as required. The original folder structure and file-naming protocols were problematic in a generalised digital world, so changes were instigated. However, the next key issue was the upgrading of all old format files into a recent format that would enable batch processing of the corpus in the future. Research by Chris Kirk at the ESRC revealed that Microsoft had no records of the internal proprietary formats used in the original Word for Mac releases, which meant that programmatic conversion was not technically feasible without significant expense. In the end it was cheaper and quicker for the scholars to re-style the few hundred items by hand. Once the uniform corpus had been established, a new goal was set to transform the .doc files into TEI P5 (Test Encoding Initiative) XML schema form, a non-proprietary form suitable for long-term digital preservation; a form that could be ingested into search, retrieval, and analytical engines such as Solr/Lucene; and a form that could be readily transformed to HTML for web presentation. Although the .doc form was not amenable to these types of transforms, it was found that the styles and annotations employed by the scholars were adequately preserved in the .docx form, and that using Oxgarage these versions of the items could be successfully transformed into TEI P5. At the time of writing the pipeline processes for the full transforms are being prepared. In the meantime the lead scholars, now in their seventies, are still able to keep working on the corpus, adding and refining their knowledge in technologies they are comfortable using. Despite what appeared to be two cases of the destruction of records that could have resulted in considerable loss of historical knowledge, the dedication of a few scholars will result in a digital corpus of significance to the history of science in Australia. In addition it will also reflect on the social life of the Australian colonies in the 19th century, contribute to research into Australian flora and the study of our natural environment, and contribute to the characterisation of Australian colonial administration and the relationship of Australian scientists to the national and international scientific communities. Moreover, with a suite of over 15,000 uniquely and globally identified and citable XML objects, the next generation of scholars using annotation and mark-up tools will be able to continue to add value to the collection. The transform of the Isis bibliographies into an informatically similar form will enable the two systems to share data and create many intersections. The new world of interconnected datasets and computable social networks is not something that the Mueller scholars could have envisaged in 1987. For Mueller himself, it was his world, and those networks and connections were held in his mind and externalised for posterity in his records. With a little care and attention from digital humanities curators and the emergence of robust digital preservation and publication platforms, the knowledge conceived in the critical scholarly print form will have its value multiplied many times over. References  Home, R. W., Lucas, A. M., Maroske, S., Sinkora, D. M. and Voigt, J. H. (eds). (1998).  Regardfully Yours: Selected Correspondence of Ferdinand von Mueller. Vol. 1:  1840–1859. Peter Lang, Bern.   Home, R. W., Lucas, A. M., Maroske, S., Sinkora, D. M. and Voigt, J. H. (eds). (2002) . Regardfully Yours: Selected Correspondence of Ferdinand von Mueller. Vol. 2:  1860–1875. Peter Lang, Bern.   Home, R. W., Lucas, A. M., Maroske, S., Sinkora, D. M., Voigt, J. H. and Wells, M. (eds). (2006).  Regardfully Yours: Selected Correspondence of Ferdinand von Mueller. Vol. 3:  1876–1896. Peter Lang, Bern.  ",
        "article_title": "The History of Science in the Age of Networked Digital Humanities",
        "authors": [
            {
                "given": "Stephen P.",
                "family": "Weldon",
                "affiliation": [
                    {
                        "original_name": "University of Oklahoma, Norman, Oklahoma, United States",
                        "normalized_name": "University of Oklahoma",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02aqsxs83",
                            "GRID": "grid.266900.b"
                        }
                    }
                ]
            },
            {
                "given": "Sylwester",
                "family": "Ratowt",
                "affiliation": [
                    {
                        "original_name": "University of Oklahoma, Norman, Oklahoma, United States",
                        "normalized_name": "University of Oklahoma",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02aqsxs83",
                            "GRID": "grid.266900.b"
                        }
                    }
                ]
            },
            {
                "given": "Birute",
                "family": "Railiene",
                "affiliation": [
                    {
                        "original_name": "Wroblewski Library of the Lithuanian Academy of Sciences, Vilnius, Lithuania",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ailie",
                "family": "Smith",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "La Rosa",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": "Gavan",
                "family": "McCarthy",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "repositories",
            "xml",
            "corpora and corpus activities",
            "archives",
            "bibliographic methods / textual studies",
            "relationships",
            "standards and interoperability",
            "sustainability and preservation",
            "English",
            "graphs",
            "resource creation",
            "digital humanities - facilities",
            "and discovery",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " 1. A Global Benchmarking Study of Expertise in Digital Scholarship Cawthorne, J.  West Virginia University Lewis, V.  McMaster University Spiro, L.  Rice University  Wang, X. University of Cincinnati There is increasing awareness of the geographical and linguistic diversity of digital humanities (Russell, 2014), but insufficient knowledge of what shape expertise takes around the world. Supported by the Andrew W. Mellon Foundation, we conducted a global benchmarking study to understand the skills and competencies necessary for digital scholarship. We interviewed faculty, research staff, administrators, and graduate students at leading digital humanities and digital social science organizations in eight countries/regions, asking about the expertise important to their work, how they acquired this expertise, and how the organization nurtures it. This panel brings together leaders of five organizations that we visited to offer their perspectives on the human dimensions of digital scholarship.  We will begin the panel by giving a 10-minute summary of key findings and lessons learned. Through our site visits, we witnessed different models for supporting digital scholarship, including centers, labs, departments, and networks. We learned that many organizations face common challenges in recruiting and retaining staff and examined strategies for contending with such challenges, including facilitating a community of practice and offering dedicated research time. We hope that our study will contribute to a deeper understanding of the diversity of digital scholarship.  Brief presentations by leaders of DH organizations will provide examples of this diversity. Each DH center leader will give an approximately five-minute talk, raising a few key questions or provocations. The rest of the session will be devoted to discussing these contributions and engaging with questions and comments from the audience. 2. Fellows No More: DH Centers and Post-Project-Centered DH  Fraistat, N. University of Maryland The digital humanities center has been a crucial institution for helping humanists develop digital competencies, primarily through offering fellowships for project development. The fellowship/project model, still prevalent in today’s centers, originally was based on the widespread model that humanities centers used so successfully: fellows received a year off from teaching to work on a book project, in return for which they would be resident at the center. As transferred to DH centers, especially the University of Virginia’s Institute for Advanced Technology in the Humanities (IATH) in the early 1990s, the ‘fellows model’ was extraordinarily successful and influential. It served well to meet the needs of a historical moment in which the great majority of humanists were uninterested at best and suspicious at worst of digital scholarship. We are now in quite a different moment, when there is widespread interest among humanities faculty and students in developing digital competencies. The fellowship model now constitutes an over-investment in the few at the expense of the many. Rather than developing digital competencies in faculty fellows, it has often resulted in faculty relying almost completely on the competencies of center staff without developing any new ones of their own. This, in turn, leads to all sorts of problems in the life of the project once a fellowship period has ended. I will turn to our experience at the Maryland Institute for Technology in the Humanities (MITH) to argue that DH centers need to think beyond the ‘fellow’ and the ‘project’ as the best means for developing these competencies, discussing our DH Incubator program as one means of doing so.  3. Building Digital Archive Systems for Scholarly Use   Hsiang, J.  National Taiwan University In 2007, computer scientists founded National Taiwan University’s Research Center for Digital Humanities (RCDH). RCDH focuses on creating archives of Chinese language materials (which pose research challenges) as well as building the system methodology and tools for scholarly use of digital archives. This differs from the model where a humanist brings her problem and data to a DH center and develops solutions with technologists. Our emphasis on system design grew out of observing humanities scholars’ frustration in using digital archive systems, which are based on the conventional precision/recall model. Since historical research can be construed as a study of contexts, a digital archive system for historians should provide an environment for exploring relationships as well as retrieving documents. Developing such a methodology required understanding how historians use a digital system. We built a digital archive, observed how historians used it, and developed new features based on their feedback. After two years of iteration, we revised the Taiwan History Digital Library to their liking. Through this process, we developed a new methodology: digital archive systems with context discovery. Using this methodology, we have built 33 digital archive systems that host about 4 million metadata records, 30 million images, and 400 million Chinese words, as well as over 20 tools designed for specific collections. They are widely used by historians working with Chinese materials. We view a good scholarly digital archive system as an ongoing, close collaboration among system builders, domain experts (in our case, historians and anthropologists), and data curators. RCDH draws its staff mainly from the fields of computer science, library science, geology, history, and anthropology. The ability to work with people from different disciplines is our top priority. We also conduct workshops and provide scholarships to attract young scholars to use our systems and provide feedback.  4. Reflections toward Building Collaborative, Methods-Centred Community in the Digital Humanities  Siemens, R.  University of Victoria Discussion around establishing the Electronic Textual Cultures Lab (ETCL) at the University of Victoria, before I took up its Canada Research Chair in Humanities Computing in 2004, pointed in several directions. One was that U Victoria already had a small research computing support unit. Another was that the fellows model of IATH, MITH, King’s College London, and elsewhere was demonstrating success in building champions of a digital approach to humanities teaching and research as well as internationally significant research interventions. Further, the field was starting to reap considerable benefit from the early investment made by groups at Toronto, Princeton, Oxford, and elsewhere in training institutes, instilling computational skills in local constituents and enabling those from elsewhere to participate. In the context of such considerations, the most important decision we made in establishing ETCL was to view the end of what we were doing not so much as a creating a physical entity as creating something viewed in terms of its function and facilitative potential—including, but beyond, any single physical space. From that focus away from a physical/institutional imprint flowed many other decisions that have shaped the growth of digital humanities at the University of Victoria. The extant support group continued providing local humanities-specific support, at the same time as ETCL assumed a broader mandate across teaching, research, and service activities, with cross-disciplinary local, regional, national, and international impact understood. Further, the Digital Humanities Summer Institute (DHSI) was embraced locally not only as a venue for on-site training but also as an international meeting point for understanding DH-pertinent tools, techniques, and technologies; our undergraduate minor in DH, joint with computer science, builds on this, as does our recent DHSI-based graduate certificate in DH, offered in partnership with the DH Training Network and beyond. So, too, with projects such as Implementing New Knowledge Environments, a network of researchers at over 20 institutions working with a similar number of invested research partners—having a centre in ETCL, but ultimately comprising spaces across the larger research community. This approach also facilitated embracing more than one physical locus for community- and practice-based work, the most significant being the MakerLab and the shared sense of enterprise between it and ETCL. My contribution will focus on these elements, and this approach, toward building collaborative, methods-centred community in the digital humanities. 5. Digital Humanities Center of Wuhan University  Ma, F.  Wuhan University The Digital Humanities Center of Wuhan University (DHWU) focuses on knowledge service, knowledge networking, semantic technologies for cultural resources, ontology, semantic publishing, information retrieval, etc. DHWU researchers come from diverse disciplines, including library and information science, computer science, natural language processing, GIS, literature, and art. Most have rich experience in interdisciplinary research. As a virtual research center, DHWU plays a key role in scientific communication. To strengthen researchers’ academic abilities, DHWU holds workshops and seminars in which they can share research achievements and collaboration experience, inspire new ideas, and establish partnerships.  6. Digital Humanities as an Academic Department Spence, P. King’s College London Digital humanities has a long tradition at King’s College London, but its evolution from a service centre to a research active unit was formalized in 2001–2002 when it was granted status as an academic department (DDH) within what is now a Faculty of Arts & Humanities. Over the years, DDH has lived through many debates about the best model to facilitate innovative but sustainable engagement between the humanities and digital culture, and the result has been a unit that corresponds more closely to the makeup of sister departments within the Faculty. This includes a stronger emphasis on teaching (DDH operates or co-manages four MA programmes, a PhD programme, and is now launching an undergraduate programme in digital culture), in addition to its historic strengths in performing funded research. This greater diversification has strengthened the department but has also presented challenges in career development, academic progression, and credit. I will explore the opportunities and tensions in the formal academic departmental model from a DH perspective, focusing on how growth can be driven by arts and humanities students’ increasing appetite for a close engagement with digital technology, which is theoretically grounded but also provides hands-on experience, and how this in turns influences the academic fabric necessary to sustain it. ",
        "article_title": "Global Perspectives on Digital Humanities Expertise",
        "authors": [
            {
                "given": "Lisa",
                "family": "Spiro",
                "affiliation": [
                    {
                        "original_name": "Rice University, United States of America",
                        "normalized_name": "Rice University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/008zs3103",
                            "GRID": "grid.21940.3e"
                        }
                    }
                ]
            },
            {
                "given": "Jon",
                "family": "Cawthorne",
                "affiliation": [
                    {
                        "original_name": "West Virginia University, United States of America",
                        "normalized_name": "West Virginia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/011vxgd24",
                            "GRID": "grid.268154.c"
                        }
                    }
                ]
            },
            {
                "given": "Vivian",
                "family": "Lewis",
                "affiliation": [
                    {
                        "original_name": "McMaster University, Canada",
                        "normalized_name": "McMaster University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02fa3aq29",
                            "GRID": "grid.25073.33"
                        }
                    }
                ]
            },
            {
                "given": "Xuemao",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "University of Cincinnati, United States of America",
                        "normalized_name": "University of Cincinnati",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01e3m7079",
                            "GRID": "grid.24827.3b"
                        }
                    }
                ]
            },
            {
                "given": "Neil",
                "family": "Fraistat",
                "affiliation": [
                    {
                        "original_name": "University of Maryland, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jieh",
                "family": "Hsiang",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University, Taiwan",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            },
            {
                "given": "Ray",
                "family": "Siemens",
                "affiliation": [
                    {
                        "original_name": "University of Victoria, Canada",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Feicheng",
                "family": "Ma",
                "affiliation": [
                    {
                        "original_name": "Wuhan University, China",
                        "normalized_name": "Wuhan University",
                        "country": "China",
                        "identifiers": {
                            "ror": "https://ror.org/033vjfk17",
                            "GRID": "grid.49470.3e"
                        }
                    }
                ]
            },
            {
                "given": "Paul",
                "family": "Spence",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digital humanities - institutional support",
            "interdisciplinary collaboration",
            "digital humanities - nature and significance",
            "English",
            "digital humanities - pedagogy and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The dates of medieval documents are often not precisely known and so are catalogued with labels such as ‘early 12th century’. While these labels are useful and meaningful for a medievalist, they present difficulties in a digital context in terms of searching, sorting, and aggregating existing descriptions. This paper will examine these challenges, propose an alternative way of modelling dates, and then make some suggestions for representing these in user interfaces. It draws on work for Models of Authority, a new project on Scottish charters of the 12th century that is funded by the UK Arts and Humanities Research Council and that uses and extends the DigiPal framework (DigiPal, 2010–14).  The Problem   Scholars have developed conventions for recording the various degrees of uncertainty in dates of manuscripts and documents. 1 To take an example from Models of Authority, National Library of Scotland GD55/32 was written between 1189 and 1196; by convention this is indicated ‘1189×1196’ (POMS document no. 3/14/3). However, one or both dates in this range may be approximate or uncertain (‘circa 1192 × 24 March 1201’: POMS 3/486/8), or different date ranges may be possible (‘A.D. 670 × 671 [? A.D. 681]’: eSawyer S.1168). Alternatively, only a general date might be known (‘late 12th / early 13th century’: POMS no. 3/590/9). Other possibilities include ‘early’, ‘mid’, or ‘late’ in the 12th century; the first or second half of the century; and sometimes the second quarter, the first third, and so on (see further examples in DigiPal, 2010–14; eSawyer, 2010; as well as DigiPal, ‘Glossary’).  This system of dating has served medievalists well. However, there is no obvious way of searching for or ordering material labelled in this way. When exactly does ‘late 12th century’ begin and end? Does it include the last quarter of the century? Does the first quarter of the 12th century come before or after the ‘early’ 12th century? Where does ‘A.D. 670 × 671 [? A.D. 681]’ fit on a timeline? We can decide answers to all of these questions in any given application, as indeed has been done (Stokes, 2012; see also the ‘search by date’ function in Manuscripts Online, and DigiPal’s faceted search, among many others). However, the result will necessarily be arbitrary and therefore difficult for others to understand. It will also inevitably be inconsistent with practice in other projects, and this in turn will lead to problems with linked data or aggregating sites (examples of which in this context include Biblissima, MESA, and Manuscripts Online). One could try to recover the intentions of the original cataloguer(s), but this is often lost and, even if recoverable, is unlikely to be consistent from one source to the next. Models of uncertainty do exist that go some way toward capturing these formulae, such as that in the TEI Guidlines (§§13.1.2 and 21.2), and those described by Grossner (2013), as well as projects such as TopoTime (Meeks and Grossner, 2013–) and PeriodO (Rabinowitz et al., n.d.). However, they still leave open these problems of searching and presenting the material, are very early in development, and/or do still not seem to adequately capture the specifics of manuscript studies. Instead, an alternative is required.  The Model  It is argued here that asking if ‘early 12th century’ includes the year 1100 is legitimate in a digital context, but to medievalists the question is in many ways meaningless, since if cataloguers knew that the manuscript was written after 1100, then they would have specified this. ‘Early 12th century’ does not mean ‘no earlier than midnight 1 January 1100 and no later than midnight 31 December 1115’, but rather something closer to ‘probably some time in the first 15 years or so of the century, but perhaps a little later or earlier’. To indicate this difference more concretely, it is useful to use probability density functions. Summarising crudely, a probability density function (pdf) represents the likelihood that a given variable has a given value. For instance, if we know when a document was written, then it has 100% probability of being written then and 0% probability of being written any other point of time; the resulting pdf is the Dirac delta function and is conventionally represented as shown in Figure 1. The assumption implicit in many search interfaces that ‘early 12th century’ has a fixed and firm beginning and end—say 1100×1115—and can be represented using the rectangular distribution shown in Figure 2. In contrast, judging intuitively from my own experience with manuscripts, ‘early 12th century’ is probably captured more accurately by something like a normal distribution (Figure 3). We can also combine these for more complex cases like ‘A.D. 670 × 671 [? A.D. 681]’ (Figure 4). Indeed, the range of possible curves is essentially limitless, and the model is sufficiently general to allow for any curve that best represents the particular case at hand.    Figure 1. Dirac delta function representing ‘1107’.    Figure 2. Rectangular distribution representing ‘1100×1115’.    Figure 3. Normal distribution representing ‘early 12th century’.    Figure 4. Hybrid distribution representing ‘A.D. 670 × 671 [? A.D. 681]’. It is important to emphasise that these curves do not necessarily represent the mathematical likelihood of the date of writing, or even of the cataloguer’s judgment, since accurately quantifying human impressions is difficult and problematic. Indeed, the resulting curves need not even necessarily be true pdfs. They may be mathematically valid and numerically accurate in a given implementation, but this assumes a reliable statistical model and so seems only appropriate for those projects that are sufficiently quantitative that such a model is available (examples may include DEEDS and work described by Smit [2011] or Wolf [2015]). However, it would be incumbent on people using this approach to demonstrate the validity of their statistical representation (for cautions against which, see Sculley and Pasanek [2008]; Stokes [2009]; Hassner [2013]). The point is in fact the opposite: rather than providing exact figures, they are intended instead to represent more meaningfully in digital form when the scholar in question considered the document to have been written. For instance, if I want to communicate the approximate frequency of a given letter-form in time, then instead of using a simple timeline like that shown above, I can instead calculate the sum of the distribution functions of all the scribal hands that show this form. Figure 5 shows the resulting curve for all occurrences in the DigiPal database of the tall- e form of the letter  æ, and this seems to effectively capture the received view that the letter-form was common early in the 11th century but went out of use soon after (Ker, 1957; Stokes, 2014). Alternative representations could include lines of varying colour, adjusting the value or saturation according to the value and thereby allowing easier comparison of different categories, or perhaps even transparency, where users adjust the date and, according to their distribution, images of the relevant letters fade in and out of view. The curves could also be used to provide a significance value for search results by taking the integral of the curve across the time interval that the user has specified, or again adjusting the transparency of images representing these forms.     Figure 5. Distribution curve for occurrences of the tall-e form of æ. Conclusion As noted above, the highly quantitative nature of this model does not bring any more certainty, nor does it address all of the concerns raised at the start of this paper. However, it does seem to promise a more intuitive and meaningful way of interacting with the content. It has been observed that the computer should not be used to provide firm answers or proving the truth of hypotheses regarding historical fact, but should instead provide means for interaction, visualisation, and knowledge creation (Clement et al., 2009; Jessop, 2008; Sculley and Pasanek, 2008). Paradoxically, then, the benefit of the model proposed here could lie not in its mathematical accuracy but the opposite: by highlighting the ‘fuzziness’ of the content it may help to break down the illusion of certainty that the computer typically brings, and may instead present a more useful and meaningful interface. Initial experiments and informal surveys with medievalists to date have suggested very strong support indeed for this approach. No doubt that other, better models will be developed by others in due course, but the point remains for now that the representation of dates in databases of manuscripts and documents is too narrow, and more imagination is required if we are to make the best use of what we have. Funding This work was supported by the Arts and Humanities Research Council [AH/L008041/1]. Note 1. This section of the proposal draws extensively from Stokes (2012). ",
        "article_title": "The Problem of Digital Dating: A Model for Uncertainty in Medieval Documents",
        "authors": [
            {
                "given": "Peter Anthony",
                "family": "Stokes",
                "affiliation": [
                    {
                        "original_name": "King's College, London, United Kingdom",
                        "normalized_name": "Bansomdejchaopraya Rajabhat University",
                        "country": "Thailand",
                        "identifiers": {
                            "ror": "https://ror.org/03e0h3d81",
                            "GRID": "grid.443695.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "medieval studies",
            "English",
            "historical studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Written texts are both abstract and physical objects: ideas, signs, and shapes, whose meanings, graphical systems, and social connotations evolve through time. Beyond authorship and writer identification or palaeographical dating of textual witnesses, the materiality of text and the connexion between the ideas and their written instantiations are a matter of cultural history, historic semiology, and history of communication and representations. In the context of large, growing digital libraries of texts and digitized medieval manuscripts, the question of the cultural significance of script and the ‘dual nature’ of texts may at last be addressed.  Several research projects, interfaces, and software allow for a closer text-image association during the editing process (TILE, 1 T-PEN, 2 MOM-CA 3) and for data visualisation (Mirador, 4 DocExplore 5), with interoperable annotations schemas (SharedCanvas 6). But in most cases, the finest granularity is at line-level with an alignment being done by hand on small amounts of text (a few pages).  In this paper, we present a new method, derived from Handwritten Text Recognition, to automatically align images of digitized manuscripts with texts from scholarly editions, at the levels of page, column, line, word, and character. During the Oriflamms project funded by the French National Research Agency and Cap Digital, 7 it has been successfully applied to two datasets: ‘ Graal’ (130 pages, 13th c. manuscript, online edition at http://catalog.bfm-corpus.org/qgraal_cm) and ‘ Fontenay’ (104 pages, 12th–13th c. charters). Partial results of alignment at word and character level are online: http://oriflamms.a2ialab.com/Charsegm/.   Our contention is that such an alignment method is the only way to gain access to new questions on a large-scale basis and massively transfer the results of traditional humanities and textual scholarship into digital humanities. The automation causes not only a change of scale (larger corpora) but also a change in granularity (page-by-page or line-by-line alignment to word and character alignment). It avoids the tedious task of drawing boxes by hand around characters and allows a systematic analysis of the data. It outmatches the preceding attempts made to more closely associate both aspects of text and also opens perspectives on automated transcription. Methodology Text-image alignment from existing transcripts is a newly opened, specific challenge. It has relevance for the humanities, as scholars work with a large set of already transcribed texts from manuscripts (handwritten texts). In other fields, the priority was given to pure (handwritten) text recognition. OCR-free methods have been proposed by our team (Leydier et al., 2014) and others (Hassner et al., 2013), which requires a precise transcript at line level, with either lesser results or no metrics on evaluation. In the present method, we use Handwritten Text Recognition (HTR), which aims to automatically transcribe the image of a handwritten text (‘text image’) into an electronic text, and implement it as an alignment method.  ‘Forced Alignment’: From HTR (Handwritten Text Recognition) to Handwritten Text Segmentation  Most of the current HTR models are based on the Hidden Markov Model (HMM) associated with a sliding window approach to segment the input image. These models may produce a precise character segmentation of the ‘text image’ as a by-product. When the transcript of a line image is known, the HMM focuses on the retrieval of characters’ positions and forces the output to correspond to the actual sequence of characters (so-called forced alignment). If only the whole document transcript is available, and not the positions or transcript of the words or lines, we can use line detection methods and map the transcripts to the detected lines. Such methods also relax the annotation effort needed to produce character segmentation. A method to automatically segment and annotate handwritten words from line images using forced alignments was proposed by Zimmermann and Bunke (2002). The problem has then shifted to mapping the whole transcription of historical documents to segmented words or lines (Kornfield et al. 2004). When the word or line segmentation is not known, a global forced alignment of the full transcript is possible, as proposed by Fischer et al. (2011), or at different levels (word, line, sentence, page), as proposed by Al Azawi et al. (2013). Our model is similar to the last one, but is based on both on A2iA proprietary image processing libraries and on Kaldi, an open-source toolkit for speech recognition, which has been adapted for the task of text-image alignment. This system adds yet another level of complexity since it also deals with abbreviations and handwritten cursive text with no blank space between the letters, for which the character segmentation cannot easily be found.   Proposed Method for Handwritten Text Character Segmentation  The character segmentation results of an incremental process (Figure 1): (1°) we convert the image to gray scale and remove black borders; (2°) we apply a text line segmentation algorithm, adapted from Zahour et al. (2007) to the full page; (3°) we keep the pages with a correct number of detected lines; (4°) we assign the line transcript to the line images and use them to train a first Hidden Markov Models based on Gaussian Mixture Models (GMM-HMM) recognizer, which is (5°) used to align the line transcription with the line images as described in Bluche et al. (2014). (6°) Based on this result, we train a new recognizer. This process is repeated until all text lines are correctly transcribed. Afterwards, we train a final text recognizer based on deep neural network HMMs. This model is trained with a discriminative criterion and yields better transcription results and segmentation accuracy than the standard GMM-HMM (example of forced alignment at word and letter level in Figures 2a and 2b).  Evaluation of Alignment Accuracy ( Graal ,  Fontenay )  Two methods were applied to evaluate the automatic alignment at word level. First, a tabular view is used to validate/reject each occurrence of a word, evaluate average accuracy, and spot problematic lines. Then a complete validation is performed by a palaeographer line by line. The accuracy is computed by the distance in pixel between the automatic segmentation and the ground-truth word boundaries (distribution in Figures 3 and 4). In  Graal, 95% and 89%, respectively, of the left and right boundaries are correct with a 10px (0.84mm) tolerance. In  Fontenay, the results are 91.8% and 89.4%, respectively, with a 30px (3.58mm) tolerance and 85% and 74.4%, respectively, with a 15px (1.79mm) tolerance, less than half of the average character width (23px, i.e., 1.94mm in  Graal and 45px, i.e., 5.37mm in  Fontenay): this is a great achievement.  The alignment was also performed at a character level on  Graal and  Fontenay. The immense number of characters makes the validation difficult. Samples show a very high accuracy rate for complex graphic structures (e.g., 100% for st-ligature), but further tools are needed to measure the accuracy. The results of evaluation partly depend on the validator’s skills in reading ancient scripts.   Evaluation of Transcription Accuracy ( Graal )  The HTR-based system was also tested for recognition and evaluated according to the word and character error rate criterion (WER/CER) by splitting the corpus into a training set (101 pages) and a test set (29 pages). For the recognition, we used a lexicon of 7,005 unique words and 4gram statistical language model estimated on the train set. The standard GMM-HMM achieved 23.0% WER and 6.9% CER, whereas the hybrid deep neural network HMM achieved 19.0% WER and 6.4% CER—that is, more than 80% of words and 93% of characters are accurately recognised. Granularity and Scalability This method unlocks a new level of granularity and allows modelling of different letterforms (‘allographs’, e.g., s/ſ/S). In  Graal, palaeographers provided the analysis of the graphical chain, and the system had to choose between identified possible solutions (Figure 5). In  Fontenay, the system had to create several character models without any previous knowledge, resulting in allograph clustering.   Even at this fine level of granularity, this system is scalable to large corpora.  Graal includes 130 pages, 10,700 lines, 114,268 words, and more than 400,300 characters;  Fontenay includes 104 pages, 1,341 text lines, 22,276 words, and more than 99,900 characters. In comparison, the historical databases ‘Saint-Gall’ and ‘Parzival’ comprise, respectively, 60 pages, 1,410 text lines, 11,597 words; and 47 pages, 4,477 text lines, and 23,478 words. The four-year DigiPal project produced a database encompassing 61,372 manually annotated images of letters, without text transcriptions.   The system is furthermore robust (book hands and diplomatic scripts), and the data format is fully TEI compliant. Human in the Loop: Evaluation, Verifiability, and Ergonomics In interdisciplinary research, humanities and computer sciences scholars must articulate their respective systems of proof and uncover underpinnings and pre-assumptions, in order to produce efficient systems that present data in a way that scholars on all sides can understand, evaluate, and trust (Stutzmann and Tarte, 2014). During this research, we observed many times that the so-called ground-truth was not 100% accurate or did not correspond to what the system was expected to produce (e.g., transposed words are edited in the order one should read them, while the alignment can only match the words in their order on the line), so that we had hesitations and explored new paths. This is a challenge for future developments: large resources will all contain inaccuracies or not automatable information. Likewise, some inconsistencies in evaluation may appear. Increasing the interactivity of software tools is a solution, not only to overcome shortcomings of strictly automatic approaches, but also to correct the ground-truth and improve the tools and the data models. Therefore, this project also developed a user-friendly software (Leydier et al., 2014). Agile development and interoperability concern software creation, but also corpus enhancement, to use humanist, computer scientist, and machine competences at their maximum. The alignment was performed in three person-months. This is obviously less than by manually drawing boxes around words (or even letters). Our tools and system open a large-scale, standardized, interoperable approach of historical scripts. The human in the loop is part of an interdisciplinary work and process, avoiding tautological approaches and allowing better results, user-friendly tools, and a better understanding on all sides.  Notes 1. http://mith.umd.edu/tile/. 2. http://t-pen.org/. 3. http://www.mom-ca.uni-koeln.de/mom/home. 4. http://showcase.iiif.io/viewer/mirador/. 5. http://www.docexplore.eu/. 6. http://iiif.io/model/shared-canvas/1.0/index.html. 7. http://www.agence-nationale-recherche.fr/projet-anr/?tx_lwmsuivibilan_pi2[CODE]=ANR-12-CORP-0010.   Figures         Figure 1. HTR-alignment.       Figure 2a. Forced alignment at word level.       Figure 2b. Forced alignment at character level.       Figure 3.  Graal alignment accuracy (number of occurrences / correction on left or right boundaries, in pixels).        Figure 4.  Fontenay alignment accuracy (number of occurrences / correction on left or right boundaries, in pixels).        Figure 5. Modelling allographs and graphical connexions. ",
        "article_title": "From Text and Image to Historical Resource: Text-Image Alignment for Digital Humanists",
        "authors": [
            {
                "given": "Dominique",
                "family": "Stutzmann",
                "affiliation": [
                    {
                        "original_name": "Institut de Recherche et d'Histoire des Textes (CNRS), France",
                        "normalized_name": "Institut de Recherche et d'Histoire des Textes",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/05evznf71",
                            "GRID": "grid.450127.5"
                        }
                    }
                ]
            },
            {
                "given": "Théodore",
                "family": "Bluche",
                "affiliation": [
                    {
                        "original_name": "A2iA, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alexei",
                "family": "Lavrentev",
                "affiliation": [
                    {
                        "original_name": "ICAR, Interactions, Corpus, Apprentissages, Représentations (ENS de Lyon – UMR 5191), France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Yann",
                "family": "Leydier",
                "affiliation": [
                    {
                        "original_name": "LIRIS Laboratoire d’Informatique en Image et Systèmes d'information (INSA de Lyon – UMR 5205)",
                        "normalized_name": "Laboratoire d'Informatique en Images et Systèmes d'Information",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/04dv4he91",
                            "GRID": "grid.482747.a"
                        }
                    }
                ]
            },
            {
                "given": "Christopher",
                "family": "Kermorvant",
                "affiliation": [
                    {
                        "original_name": "Teklia, France",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "linking and annotation",
            "linguistics",
            "xml",
            "interdisciplinary collaboration",
            "data mining / text mining",
            "medieval studies",
            "digitisation",
            "content analysis",
            "corpora and corpus activities",
            "text analysis",
            "English",
            "software design and development",
            "standards and interoperability",
            "visualisation",
            "data modeling and architecture including hypothesis-driven modeling",
            "scholarly editing",
            "digital humanities - nature and significance",
            "resource creation",
            "image processing",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper uses the Visual First Amendment (VFA) project to highlight the use of data visualization, user experience studies, and computational methods in the study of law. VFA presents visitors with interactive displays that allow them to explore the interrelation of issues, cases, courts, and justices over time, and to consider the broad social and legal changes that have impacted the United States Supreme Court’s ruling on the First Amendment freedoms of religions, speech, press, assembly, and petition. We begin by situating our project in the emerging field of empirical legal studies and describing the open data sources used to create our visualizations. We also discuss several usability tests conducted to improve our visualizations, and the development of the project cycle from concept to dissemination. Explicit attention is given throughout to digital humanities aspects of the project.  * * * In 1897, future U.S. Supreme Court justice Oliver Wendell Holmes Jr. observed, ‘For the rational study of the law the black-letter man may be the man of the present, but the man of the future is the man of statistics and master of economics’ (Buckler, 2012). Holmes was addressing the need for law to look beyond historical examples to the social realities of the time. His methodological suggestion, however, has found new life in the era of digital humanities.  This paper presents the Visual First Amendment (VFA) project, which applies empirical methods and visualization techniques to the study of the First Amendment of the U.S. Constitution. VFA aims to   • Provide a deeper understanding of the American constitutional freedoms of religion, speech, press, assembly, and petition.  • Engage citizens in a discussion of rights, as interpreted now and in the future.  • Raise awareness of First Amendment issues from a historical and legal perspective.  • Racilitate research surrounding the First Amendment across a range of education levels and information tasks.  • Serve as a model for representing Supreme Court data generally.  We begin by giving an overview of empirical legal research and similar projects that have been attempted to date. Following that, we describe the data sources and visualization tools behind several VFA interfaces. Finally, we present the results of user experience studies, as well as future directions.   Background  Traditional legal scholarship consists of detailed analysis of primary sources (e.g., legislation and court rulings), supplemented by secondary sources such as legal scholarship. Empirical legal research, first employed in American legal scholarship in the late 1990s, ‘uses data analysis to study the legal system’ (Georgetown Law, 2014). While this approach is still not widespread, it has been described as ‘the next big thing’ in legal intellectual thought (George, 2006). Cane and Kritzer (2010) describe empirical legal research as involving ‘the systematic collection of information (“data”) and its analysis according to some generally accepted method’ and the (possibly numeric) coding or tagging of units of text. This research is often interdisciplinary—examining questions in the intersection of law, society, and economics, as well as judicial behavior and politics—and uses both quantitative/statistical and qualitative methods. Most data used in empirical legal studies are ambient in the sense that they are generated through judicial processes rather than through experiments, which would raise ethical risks in the case of law (Epstein and Martin, 2010).  The proliferation of law-related datasets and the increasing availability of data to the general public represent an important step toward increased civic engagement. The U.S. Open Government Initiative makes available over 200 datasets, which are used by journalists, researchers, and the general public for a variety of purposes, among them government oversight and accountability. The Sunlight Foundation, for example, has supported several projects that draw on publicly available data to promote government oversight practices such as tracking the link between campaign contributions and congressional votes on legislation and public policy.  To date, however, few attempts have been made to visualize court rulings, and none are focused on presenting data about the First Amendment for a wide variety of audiences:   • One of the earliest projects, Visualizing Legal Information (http://www.cs.umd.edu/hcil/west-legal), was created in the late 1990s in the Human Computer Interaction Lab at the University of Maryland. It is no longer updated, and most of the visualizations are no longer functional.   • A group of researchers from the University of Oslo is working on an interdisciplinary Law and Visualization project (http://www.jus.uio.no/english/research/areas/law-history/projects/law_visualization.html), which focuses on the relations between law and visualization as expressed in art, architecture, and film. Though the project is described as ‘empirical’, it does not employ data-driven methods, instead examining the experience of aestheticians and their concepts of law.   • The Judicial Research Initiative (http://artsandsciences.sc.edu/poli/juri) from the University of South Carolina aims to ‘provide a comprehensive access point to the most recent and cutting-edge research on law and judicial politics.’. This project only has datasets available for download and does not include visualization.  Visual First Amendment is unique among these projects as it applies visualization techniques to First Amendment data for a wide range of users. The information presented by the site is also highly contextualized both historically and thematically.  Project Design  Visual First Amendment draws on three freely available data sources, which have been selected for their recognized excellence in the field:   • First Amendment Timeline is compiled by the First Amendment Center, an operating program of the Freedom Forum located at the Newseum and Vanderbilt University. The Timeline is a chronological list of 150-plus American historical events, Supreme Court rulings, and legislation relating to the First Amendment from 1215 to 2011, with narrative descriptions of each event.   • Supreme Court Database (SCDB) is a National Science Foundation project hosted by the Center for Empirical Research in the Law at Washington University in St. Louis with nearly a dozen contributing law schools. SCDB contains data on 8,407 cases, 655 of which are classified as First Amendment cases, with over 200 pieces of information coded about each case. While this schema has received some criticism (Shapiro, 2008), it is highly regarded and used by scholars in various publications (Buckler, 2012; Sharma, 2013; Stearns, 2013), including a recent book on behavior of federal judges (Epstein et al., 2013). The project currently covers cases between the 1946 and 2012 terms and has a timetable established for the addition of other years.   • Supreme Court Citation Network Data, compiled by James H. Fowler (UCSD) and Sangick Jeon (Stanford), contains 202,167 citations to and from 30,288 Supreme Court majority opinions over the 1800–2002 period. The initial VFA prototype contains a range of visualizations based on these data sources. A complete list of these pairings is given in Table 1, and each type of visualization is described below.   Visualization Type Software Data   timelines TimelineJS First Amendment Center Timeline  statistical charts and graphs Tableau Public SCDB  networks Gephi SCDB, Citation Network Data geographic maps Leaflet, QGIS, Tableau Public SCDB  Table 1. Timelines  Timelines are the simplest and most easy-to-read displays used in the project. The current prototype includes a timeline of 100-plus major cases and historical events, as well as curated timelines around specific First Amendment stories. Users can swipe through the timeline using their mouse or touchscreen, and events are divided into a number of categories, including world/national events and various First Amendment freedoms. Visual First Amendment Timeline Browser        Figure 1. http://visualfa.org/timeline/. Statistical Charts and Graphs  VFA includes several statistical charts and graphs about First Amendment rulings as well as Supreme Court justices. These visualizations are interactive, allowing the user to select, filter, and manipulate the information presented, and can be shared or embedded across blogs, websites, and social media.  Justice Voting Patterns by Ideology        Figure 2. http://visualfa.org/statistics/justices-by-ideology/. Network Maps  Networks are among the most complex yet frequently discussed visualizations in the initial prototype. These graphs illustrate the relationships between court opinions and agreement between justices. Each dot represents a case; hovering over the dot calls up the name of the case, and clicking on it opens a sidebar with full case details. Cases are searchable by name or by topic.  Citation Network of Majority Opinions by First Amendment Issue        Figure 3. http://visualfa.org/citation-network-maps/network-maps-a-guided-tour.  Geographic Maps  Geographic maps are some of the first visualizations that young users learn to read, and they bring a new dimension of spatial reasoning to historical and thematic content. Several maps in our initial prototype illustrate the behavior of circuit courts, visually investigating whether these courts merit their reputations as being conservative or liberal in their rulings. A separate map examines the number of cases that arise from each state, filterable by issue, petitioner categories, respondent categories, and more.  Circuit Court Map by Decision Ideology        Figure 4. http://visualfa.org/circuit-court-map/.  User Experience  In spring 2014 the Visual First Amendment team conducted an initial round of user experience tests (Pratt IRB #189/2-12-14) at the American Civil Liberties Union and New York Civil Liberties Union offices in downtown Manhattan. Five users were tested using a talk-out-loud method in which they were asked to complete tasks and to narrate their decisionmaking process as they go. The tests were recorded using the Silverback Screen Capture software so that user behavior could be examined later. Users were also asked for general feedback on the site, including its potential audiences. We also spoke more informally with law students, law faculty, and law librarians, who have a special interest in the content of the project. This research has led to a number of site revisions, including   • Site navigation with multiple points of entry.  • Contextual information and details on demand within visualization interfaces.  • Curated datasets and stories that introduce more advanced interfaces.  Future Directions  At present, we are working to collaborate with humanities scholars, legal advisors, and media experts on content and visualizations that reflect general/public interest. To further public engagement, the project would benefit from additional user testing with broader audiences as well as the creation of additional tutorial videos and other instructional materials for the website. Finally, we hope to create an integrated user experience across multiple types of visualizations and to link with external resources, such as news stories and reference sources.  ",
        "article_title": "Visual First Amendment: A Case Study in Empricial Legal Studies, Visualization, and User Experience",
        "authors": [
            {
                "given": "Chris Alen",
                "family": "Sula",
                "affiliation": [
                    {
                        "original_name": "Pratt Institute, United States of America",
                        "normalized_name": "Pratt Institute",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/007m3p006",
                            "GRID": "grid.262107.0"
                        }
                    }
                ]
            },
            {
                "given": "Debbie",
                "family": "Rabina",
                "affiliation": [
                    {
                        "original_name": "Pratt Institute, United States of America",
                        "normalized_name": "Pratt Institute",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/007m3p006",
                            "GRID": "grid.262107.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "visualisation",
            "user studies / user needs",
            "project design",
            "law",
            "English",
            "data mining / text mining",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " As the world’s third-largest democracy, Indonesia has experienced fundamental changes since the country progressed from a repressive authoritarian to a democratic regime in 1998. Competitive, direct elections have been held four times; freedom of speech and assembly are legally protected; and citizens have a lively and outspoken public sphere. Freedom House, the U.S.-based non-governmental organization, labels Indonesia as a free and democratic country (Antlov and Wetterbeg, 2011, 3). Indonesia’s Ministry of Communication and Informatics reports that there were 82 million Internet users in 2014, and Indonesia has the eighth-highest number of Internet users in the world (Kominfo, 2014). The Internet is perceived as a media that can potentially promote greater democracy due to it being a freer and more open form of communication compared to traditional media, therefore opening more opportunities for freedom of speech and other communicative functions than traditional media communication. Bennett (2008, 9) argues that the future of democracy is in young people’s hands. It is crucial for young people to learn how to use digital media in order to develop civic and political actions. Democracy and participation in public culture require not just a willingness to consume information but a willingness to create, share, and use information. Internet use has increased rapidly in Indonesia, particularly among Indonesian youth (Kominfo, 2014). This provides an opportunity to use Internet and social media to enhance democracy in Indonesia. Nugroho, Putri, and Laksmi (2012, 7) found that the Internet has encouraged space for citizens to communicate without restriction. Also, Indonesian citizens can create their own public sphere and engage freely with others by blogs, social media, and micro-blogging. Digital media literacy is an important life skill that is increasingly necessary to develop democracy in Indonesia; therefore, this research aims to investigate whether young Indonesian people have been able to understand and meaningfully use digital media to support democratic institutions and practices. There will be two stages in this research. In the first phase, the focus group method will be suitable for researching Indonesian youth’s digital media literacy and capacity to use digital media to support democracy. I will conduct two focus group discussions to identify and compare their digital media practices. The first focus group will be conducted with young Indonesian urban men and women, middle class, ages 18 to 25 (as university/college students), and Internet heavy users (defined as using the Internet a minimum of once a day (ACMA, 2009a, 6). The second focus group will be conducted with a group of the same background as Group 1 but with a different Internet habit, namely Internet medium users, defined as using the Internet one to seven times a week (ACMA 2009a, 6). Both focus groups will be recruited from faculties of politics/communication in the top 10 university/colleges in Jakarta, and respondents will be drawn from leadership positions in campus organisations (student union/guild). Focus group discussion topics will include how the student makes responsible choices and accesses online information, and also how the student analyses and creates information in a variety of digital media forms. The second phase, semi-structured interviews, will be conducted relating to the digital media practices of young Indonesian urban men and women, middle class, ages 18 to 30, active members in non-profit organizations or the community as a leader or participating on a committee, Internet heavy user and living in Indonesia’s capital, Jakarta. The findings will measure the role of digital media literacy to enhance democracy in Indonesia. ",
        "article_title": "Digital Media Literacy in Indonesian Youth: Building Sustainable Democratic Institutions and Practices",
        "authors": [
            {
                "given": "Fiona",
                "family": "Suwana",
                "affiliation": [
                    {
                        "original_name": "Queensland University of Technology, Australia",
                        "normalized_name": "Queensland University of Technology",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03pnv4752",
                            "GRID": "grid.1024.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "digital humanities - nature and significance",
            "English",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " One of the most onerous responsibilities a citizen may have to perform is jury duty: listening in silence to days, even months, of people talking at you, then trying to reach an unanimous decision with a group of strangers. This task violates the normal rules of communication: the decision-makers do not engage with the speaker; they are forbidden from any form of personal enquiry; rather than being experts they are chosen precisely because they have no knowledge of the subject; and further, confronted with a confusing set of facts, they are expected to make a simple decision.  Into this complicated communication task enters digital evidence. Criminal trials that were once characterised by oral testimony in the form of eyewitness statements and confessions now include displays of scientific diagrams, photographs, interview transcripts, witness statements, intercepted phone calls, and video evidence. For more complex trials, courts typically give jurors a DVD containing some of this evidence to take into the jury room. The next step in the process of digital transformation, under active consideration by courts, is giving jurors individual copies of digital evidence on a tablet.  Having information on a tablet may improve both individual and group processes. For individual jurors, technological aids can prompt juror memory, and enhance comprehension and engagement; for the jury as a whole it may improve the thoroughness of the deliberation. Providing each juror with his or her own copy of the evidence may encourage critical discussion and healthy debate among jurors, which in turn can challenge prejudices and lead to fairer outcomes. Combining oral discussion with visual display could produce the jury with an efficient way of managing cognitive load. Alternatively, giving jurors tablets could increase prejudice against the accused, undermining the right to a fair trial, making visually memorable images more significant, and inhibiting the free flow of debate as individual jurors are diverted from their collective task. Because courts have been unsure which of these two scenarios is more likely, there has been some hesitation about giving jurors tablets. Empirical research will therefore assist in making the relevant policy decisions. This paper reports on an experimental study at the University of Queensland using 106 mock jurors and a written scenario, with six-person juries deliberating for 15 to 30 minutes with visual evidence provided to them either on paper or iPads. The study shows that jurors who deliberated with iPads were significantly more likely to find the accused guilty than those who deliberated with paper. Before deliberation both groups had virtually identical conviction levels, but after deliberation the ‘paper’ group had shifted twice as far as the ‘iPad’ group. Explanations for this include undue weight given to prosecution evidence amongst the tablet users, less attention to group processes, greater willingness to disagree, and less pressure to reach a compromise. The paper reports on a subsequent, more elaborate study with an expanded scenario and evidence list, plus collaboration software that allows the jurors to post images to a shared screen in the jury room. Collaboration technologies are of considerable interest to digital humanities scholars. This study tests how a one-to-many display facilitates collaboration in the highly constrained jury deliberation process. These constraints include deliberating as a single group, complying with judicial directions about the weight to be given to different pieces of evidence, and chairing of discussion by a single foreperson.  The study is funded by the Canadian Social Sciences and Humanities Research Council as part of the Cyberjustice consortium based at the University of Montreal and headed by Karim Benyekhlef. The study has been developed by a team including David Tait (Digital Humanities Group, University of Western Sydney), Christian Licoppe (Paris Tech), Meredith Rossner (LSE), and Blake McKimmie (University of Queensland). The cyberjustice consortium brings together scholars from several countries with an interest in the impacts of emerging technologies on justice processes.  ",
        "article_title": "Digital Evidence in the Jury Room: the Impact of Tablets on Communication and Decision",
        "authors": [
            {
                "given": "David",
                "family": "Tait",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Karen",
                "family": "Gelb",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "interdisciplinary collaboration",
            "mobile applications and mobile design",
            "video",
            "audio",
            "English",
            "multimedia",
            "resource creation",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2006, Gregory Crane posed the provocative question, ‘What do you do with a million books?’, a question that captured the increasing anxiety in the humanities that accompanied the rapid digitization of many library collections, and the distribution of more and more books in digital form (Crane, 2006). Far more books than could ever possibly be read by a single person were now machine actionable, requiring a response from the intellectual community. In the ensuing years, that response has emerged from the digital humanities community, and DH projects have focused on a broad range of approaches to the study of literature at scale, operationalizing the ‘distant reading’ that Franco Moretti had already proposed in his well-known article, ‘Conjectures on World Literature’ (Moretti, 2000; 2013). Yet what has been overlooked in many of these studies is that, alongside the explosion in the digitization of world literature, there has been an equally large explosion of readers commenting on books in online forums and other easily accessed electronic venues. The analysis of these responses allows for the consideration of reader response at scale. The goal of our work is to provide a preliminary answer to the question, ‘What can one do with a million readers?’ At the very least, we want to know what types of information one can extract automatically from thousands of reader posts about a particular work of literary fiction, and what this can tell us about how people (or classes of people) read. The target data for our study are reader reviews of sixteen works of fiction, five of which we focus on in this presentation ( The Hobbit;  Gone with the Wind;  The Life of Pi;  Frankenstein;  Of Mice and Men). The works were chosen from the list of the most frequently rated books on the Goodreads site (number of ratings > 500,000). From these books, we downloaded the maximum allowed 3,000 reviews. The sixteen works we ultimately chose were selected on the basis of the broad disparity in their narrative structures, number of characters, and character relationships. The initial goal was to develop a review-based summary of the novel in the form of a character graph (Figure 1), with dramatis personae and pair-wise connections between them based on actions, events, or other relationships. These graphical representations can be seen as an abstraction of what the reviewers on Goodreads collectively imagine who the main characters and what the main events to be, and the relationships between these characters and events.  The reviews were harvested using a crawler specifically designed for this project. Readily available information on the reviewer was retained as metadata for use in addressing second-order questions related to classes of reviewers (e.g., gender, age, frequency of reviewing). To evaluate reader reviews at scale, we devised two metrics related to plot: completeness and accuracy. We used SparkNotes as a basis for ‘gold standard’ summaries, a choice motivated by SparkNotes’ high degree of completeness and accuracy for target features (entities and relationships), and the brevity of the summaries.  We devised several progressively more difficult challenges to test the type of information we could derive from the approximately 48,000 reader reviews we crawled from Goodreads. First, without any training data, could we successfully discover the main dramatis personae in each novel? Second, could we automatically discover the action-based relationships between characters as represented by the reviewers? Third, could we discover the ‘events’ in which dramatis personae played a role? And fourth, could we develop a visualization that captured these relationships in a clear and engaging fashion that also represented the varying degree or strength of relationships between these entities?   To achieve reproducible results, we devised a simple workflow that can be applied to other similar sites. This workflow consists of a preprocessing step, a statistical entity ranking step that surfaces the main dramatis personae in any given work, a pair-wise relationship discovery step, and a visualization step. As with most blogs and crowdsourced data sources, the Goodreads reviews are ‘noisy’. Consequently, preprocessing focused on reducing noise. We also ran language detection on the reviews to eliminate non-English reviews, and a stemmer to aggregate inflected words.  In our efforts to discover the dramatis personae for any target work, we experimented with three main approaches. LDA proved to be successful at separating the works; we intend to explore this approach more thoroughly in future work for classifying reviews that are not preclassified as they are on Goodreads. Traditional NER (named entity recognition) approaches proved to be less accurate, given the broad variance in orthography that typify these reviews. Ultimately, the most successful approach was based on a statistical ranking of tokens between the review corpus and the individual subcorpus of target work reviews. The next challenge was to discover the relationships between dramatis personae as represented in the reviews. Here, we focused on the verbs, extracted using POS tagging, between two entities, after discovering all sentences with pairs of entities. In future work, we hope to refine these pair-wise relationships by collapsing verbs into a series of higher-level representations of the entity-relationship space. Even without this processing, the current approach discovers important relationships. For example, by looking at the reviews for  The Hobbit, the relationship between ‘Tolkien’ and ‘Hobbit’ is dominated by the verb ‘write’ with a directional relationship in which ‘Tolkien’ inhabits the subject position and ‘Hobbit’ the object position. Similarly, ‘Bilbo’ has a relationship with ‘ring’ characterized by ‘find’, and a relationship with ‘adventure’ of ‘go on’, while the dragon ‘Smaug’ has a relationship with ‘treasure’ of ‘guard’. Interestingly, the reader reviews also generated a relationship between ‘Bilbo’ and ‘Smaug’ characterized by ‘kill’, an inaccurate depiction of the actual events (Bard the Bowman slays Smaug). Our automatic method is able to build a surprisingly large number of relationships between discovered entities without any training data, nor any preexisting lists of entities or relationships.  For evaluation, using only the SparkNotes plot summaries, experts created a list of nouns including (1) names, (2) locations, (3) objects, and (4) concepts that are explicitly mentioned in the summary; these were considered the true dramatis personae. Completeness was quantified by computing the proportion of this list also produced by the algorithm. We measured both Precision (proportion of main characters produced by the algorithm) and the False Detection Rate (proportion of produced characters not in the relevant set). Similarly, using only words explicitly found in the SparkNotes plot summary, experts derived relationships based on verbs that connected pairs of characters mentioned above. We again measured the Precision and the False Detection Rate (FDR) for these relationships. As an example, applying this to  The Hobbit reviews produced a measure of Completeness (or precision) for dramatis personae of 0.6 and Accuracy (False Detection Rate) of 0.13.   The final challenge was to present these relationships in a visually engaging manner (see Figures 1 and 2). We have developed directional, multicolored graphs that represent the strength (or confidence) of a relationship by an edge of varying width. These graphs are easily compared with the ‘gold standard’ ground truth graphs, and they provide us with a visual representation of our Completeness and Accuracy measures. What is immediately clear is that reader reviews have significantly lower Completeness than a resource dedicated to providing comprehensive summaries, while the Accuracy of described relationships is good (Bilbo’s dragon-slaying feats notwithstanding). The comparison raises intriguing issues about memory—for example, why is it that certain events disappear from the user-driven graphs, while others become accentuated? Other graphs are generated for classes of reviewers: e.g., female reviewers vs male reviewers of  The Hobbit, which allow for a different type of comparison. Here the question is on which aspects of a story different types of reviewers tend to comment. Additional refinements could include metrics that reveal the number of reviews that mention particular entities or particular relationships. Currently, a missing component is a dynamic representation of reviewers’ concepts of plot (dynamics), which we are reserving for future work.   The approach we describe here is widely applicable to other crowdsourced response sites. Of particular interest are movie review sites such as Rotten Tomatoes that, much like Goodreads, allow viewers to present their own reviews of popular films. An intriguing aspect of many of these review sites is the propensity of reviewers to provide ‘plot summaries’ as opposed to critical engagements of more sophisticated thematic analysis. While this may drive many literary scholars toward the brink of insanity, it does allow us to consider questions regarding the popular engagement with literature and other forms of artistic production. Given the responses that people do post, can we use the scale of these sites to derive insight into how people (or groups of people) not only read but also remember? It is our contention that what people remember, and what people forget (or choose to leave out), can be very telling indicators of popular engagement with art.      Figure 1. Character/relationship graph of  The Hobbit.      Figure 2. Character/relationship graph of  Frankenstein .  ",
        "article_title": "What Do You Do With A Million Readers?",
        "authors": [
            {
                "given": "Roja",
                "family": "Bandari",
                "affiliation": [
                    {
                        "original_name": "Twitter Inc., United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Timothy Roland",
                "family": "Tangherlini",
                "affiliation": [
                    {
                        "original_name": "UCLA, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Vwani",
                "family": "Roychowdhury",
                "affiliation": [
                    {
                        "original_name": "UCLA, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "content analysis",
            "natural language processing",
            "literary studies",
            "text analysis",
            "English",
            "crowdsourcing",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Although digital humanities continues to expand and become more inclusive, little is known about the extent to which its published knowledge is integrated. A longitudinal co-citation analysis of publications in digital humanities was conducted to examine the degree of cohesion of its published knowledge over time (1989–2014). The measurement of cohesion was performed at source and individual article levels. The results show that, while the publications in digital humanities continue to grow, its diversity and coherence, two hallmarks of interdisciplinarity, remain robust. Betweenness centrality was used to identify the nodes that contribute most significantly to the cohesion of the source and individual networks. * * * One crucial aspect of Interdisciplinarity is the integration of different bodies of knowledge into a coherent enterprise (Rafols and Meyer, 2010; Porter et al., 2007). As an emerging field that draws research interests from multiple fields, DH provides a fertile ground for the study of whether and how different bodies of knowledge are integrated. The cognitive integration of concepts, theories, methods, and/or results from diverse fields is considered as the hallmark of interdisciplinary research. Such knowledge integration is much easier to observe at the micro level, where the diversity of authorship and cited references present within an individual article are often taken as evidences of interdisciplinarity. The level of integration at the macro level, which can be measured by the degree of interconnectedness among published works within a field, is not as readily accessible. While research initiatives in DH often share the common methodological outlook, there is little empirical study regarding whether DH as a research field has consolidated or remained fragmented. Taking a bibliometrics approach, this study aims to fill the gap. We used a co-citation network to examine the degree of interdisciplinarity in the published knowledge in DH. Three types of bibliographic elements resulting from the co-citation network were examined: keywords, publication sources, and individual articles. Social network analytical methods were applied to measure the diversity and interconnectedness of these elements, as well as the most prominent actors in these networks.   Data Collection Procedures   The publications of DH were identified by both keywords search and journals with an explicit digital humanities orientation. Keyword search with Scopus resulted in a set of 1,967 articles and book chapters. As publications in DH do not necessarily have those keywords, a complementary set of articles was created by retrieving all the articles published in the six journals published by the members of  the Alliance of Digital Humanities Organizations (ADHO). The union of the two sets constitutes the target set (N = 2509) from which article co-citation networks were formed and analyzed.   To generate the co-citation networks, pair-wise matching of all the articles’ received citations has to be conducted. This is done by using Google Scholar’s citation tracing function. The citations received by every article in our target set were identified and downloaded; then pair-wise matching was performed to identify shared citation. To study the co-citation network over time, a five-year overlapping time slice was used to divide our target set into 22 co-citation networks.  Results Figure 1 shows the growth of articles and number of nodes in the co-citation networks over time. The graph indicates that the interests in DH took off in early 2000 and has continued to grow to this date. Notice also that there is a significant gap between the number of total articles and nodes in the co-citation network in our target set, indicating a large proportion of isolates in the whole network.          Figure 1. The growth of publications and size of the co-citation network over time. We used author assigned keywords as the representation of subjects treated in the articles. The keywords thus extracted were then normalized manually. Gini index was then applied to measure the balance of the subjects covered in our target set over time. High Gini index of the keyword distribution signals the existence of few dominant keywords. Figure 2 shows a gradual rising of Gini indexing over time, which might be interpreted as a gradual consolidation of research efforts. Yet notice that even with the gently rising slope, the degree of concentration remains low at below .35, which suggests rather disperse research interests in DH.        Figure 2. Gini index of keyword distribution over time.  Figure 3 shows the long-term trend of cited articles remained in the giant component. A jump of the percentage of nodes in the giant component can be observed in the early 2000s; it then gradually levelled off. The dip in recent years might be more likely due to the citation window for recent publications than a sign of disintegration. Another two measures, average geodesic distance and clustering coefficient that often associate with the ‘small world’ phenomenon, also showed similar patterns. Figure 4 shows a rather short average distance between nodes in the giant component, hovering around 4. The cluster coefficient stayed steadily high at 60 percent, indicating dense local clustering, which, coupled with the short average distance, fit a typical small-world model.        Figure 3. Percent of nodes within the giant components.       Figure 4. Clustering coefficient.       Figure 5. Average geodesic distance. Another way to assess the cohesion or integration of a network is through degree centralization, as it measures the degree to which network cohesion is hinged on particular focal importance (Borgatti et al., 2013). High degree centralization signals inequality of node importance. Figure 6 traces the trends of degree and betweenness centralization of nodes in the co-citation main component over time. Both degree and betweenness centralization leveled off gradually from their peaks in the mid-1990s, when the idea of DH was first introduced by a few seminal works. The degree concentration, however, has since declined gradually, which echoes our previous findings, suggesting the presence of many local clusters that represents a wide variety of specialized interests.        Figure 6. Degree and betweenness centrality of the giant components over time.  Among all the centrality measure, betweenness centrality is arguably the most closely related to network cohesion. Table 5 ranked the top 40 sources in our target set by their betweenness centrality, accompanied by their degree centrality. Of particular interest are the sources able to achieve high betweenness centrality despite relatively low degree centrality as they play a significant ‘bridging’ role in the network that contributes the most to the network cohesion.     Table 1. Top 40 sources with highest between centrality.  We visualized the source co-citation network using MDS where the distance between nodes signifies the similarity of their co-citation profiles (Figure 7). The size of the nodes represents betweenness centrality, while different colors group frequently co-cited sources using fraction algorithm.        Figure 7. MDS representation of source co-citation profiles. Stress value = 0.417. To identify individual works that contribute most to the cohesion of the network, we performed the betweenness centrality on the article co-citation network. The works that have a particularly strong bridging character relative to their degrees were highlighted.       Table 2. To 40 works with highest betweenness centrality. Discussion and Conclusion  In this study we applied social analytical methods to measure the degree of knowledge diversity and cohesion of the bibliographic elements extracted from the article co-citation network. The results show that the degree of knowledge diversity is high, as demonstrated by the evenness of the keyword distribution, as well as the presence of publication sources from diverse disciplines. It was shown that DH has gradually evolved into a diverse yet cohesive area of research, where, despite high degree of local research interests, the reachability remains high globally.   A longitudinal approach allows us to observe the evolving of the knowledge integration and the shifting of research topics in an area of research. There are obviously also several limitations to our study. First, even though we have tried to be inclusive in our selection to target set, some of the important works are inevitably missed, especially those published in non-English languages. Using Google Scholar as the source of co-citation data also entails inherent bias. Currently our interpretation of the visualization of the field is quite preliminary due to the lack of domain knowledge in DH. In the future, author co-citation network will also be constructed, which, with the help of expert inputs, will help us better interpret the sub-domains and their relationships in DH.   ",
        "article_title": "A Longitudinal Analysis of Knowledge Integration in Digital Humanities Using Co-citation Analysis",
        "authors": [
            {
                "given": "Muh-Chyun",
                "family": "Tang",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University, Taiwan, Republic of China",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            },
            {
                "given": "Yun-Jen",
                "family": "Cheng",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University, Taiwan, Republic of China",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            },
            {
                "given": "Kuang-hua",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University, Taiwan, Republic of China",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            },
            {
                "given": "Jieh",
                "family": "Hsiang",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University, Taiwan, Republic of China",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "relationships",
            "bibliographic methods / textual studies",
            "English",
            "graphs",
            "networks"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Many collections of medieval manuscripts are now being digitised and showcased online, but in order to do so, a complete pipeline of identification, cataloguing, imaging, and delivery is needed, which can be prohibitively expensive and time consuming. Teams seldom have all the skill sets in place to deal with the identification of complex historical texts. It is also rare to attempt any advanced imaging techniques to aid the reading of manuscript material. We report here on an interdisciplinary collaboration to analyse fragments of unknown medieval manuscript material held in UCL Special Collections. We demonstrate that a centre for digital humanities can provide the catalyst to undertake such a small, yet intensive, digitization project within an institutional context, and, working in joint collaboration, can aid in the process of digitization, communicate with a wider online audience to help with identification of material, and promote interdisciplinary working to prepare material for further research in heritage science.  The Collection  UCL Special Collections, 1 one of the foremost university collections of manuscripts, archives, and rare books in the United Kingdom, hosts fine collections of medieval manuscripts, early printed books, and archival material. A collection of 157 medieval manuscript fragments have been in the possession of the Library for decades, but have never been fully described and are generally unknown to the wider community. They were purchased in the early 20th century in an initiative led by a professor of German studies at UCL, Robert Priebsch, who had long been trying to secure a group of manuscripts for palaeographic study (Munby, 1960, 38). Money was raised by subscription amongst UCL staff (Munby, 1960), and it is thought that this collection was purchased at auction in Bonn in 1921; unfortunately relevant records did not survive the war (Munby, 1960, 42). Although mentioned briefly in a catalogue of UCL’s holdings in the 1930s (University College, London and Coveney, 1935), the manuscripts are yet to be fully described and researched. Mostly originating from Germany, they have been found to contain rare examples of early musical notation as well as legal, religious, medical, and other texts from a range of dates (mainly 10th–14th centuries), styles, and different languages (including Latin, German, Hebrew, English, French, and Greek). Consistent with having been used in bindings for later printed works, the majority of the fragments have significant damage, most usually remnants of, or loss of text removed by, adhesive. They are variously cut, torn, and faded, with pest damage and various accretions.   Digital Humanities and Digitisation  Although most effort in the digital humanities is focussed on the production, analysis, and visualization of text, there is a growing interest in the community towards digital imaging. Digitisation can produce adequate scholarly surrogates of historical documents (Terras, 2008). However, improvements in digital image processing and analysis have allowed the development of a number of techniques that can reveal a greater wealth of information about the originals, beyond traditional digitisation technologies (MacDonald, 2006). Scholars have been able to image, analyse, and recover more information from historical texts, primarily using a technique called multispectral imaging (Chabries et al., 2003; Salerno et al., 2007; Tanner and Bearman, 2009). Over the past few years, UCL has been building up its expertise in cultural and heritage imaging, working on a range of projects that involve developing and analyzing capture methods for primary historical texts (for example, see Pal et al., 2014; MacDonald et al., 2013; Giacometti et al., forthcoming).  We conceived the Fragment Project as the inaugural scholarly programme of the recently opened UCL Multi-Modal digitisation suite, 2 a facility for teaching and research in digitisation technologies jointly supported by UCL Faculty of Arts and Humanities, 3 UCL Faculty of Engineering Sciences, 4 and UCL Library Services 5 coordinated by UCL Centre for Digital Humanities, 6 completed in September 2013. In working together on the digitisation of the manuscript fragments, we aimed to make the fragments available to a wider scholarly audience for the first time, help in their identification and classification, but also to categorize the physical properties of this set of primary source material held locally in UCL Special Collections, so that we can use the fragments in our research aimed at developing new imaging techniques for recovering text in deteriorated historical manuscripts.   The UCL Medieval Manuscript Fragment Project  This project was given a small amount of seed funding from both the Library and UCL Centre for Digital Humanities (totaling £3000) to allow the classification, identification, digitization, and cataloguing of the collection. The first phase of the programme, undertaken between November 2013 and April 2014, involved surveying all fragments in the collection, to identify—where possible—the language, content, and date of the fragments, and to quantify the physical condition of each (for example, Cracks, Flaking, Fading, Abrasion, Ink Water Solubility, Loss, Ink Offset, etc.). We were lucky to employ, part time, a recently finished PhD student with wide-ranging linguistic and palaeographic expertise to undertake this overview. Fifty detailed descriptions of selected fragments were produced and used as the basis for a full archive catalogue, which was produced in accordance with international standards using CALM software. 7 Two hundred seventy-nine high-resolution images of the collection were produced.         Figure 1. An example of a fragment identified and digitized. Thirteenth-century leaf from a hymnal, parchment. Text in a gothic script in two columns interspersed with musical notation in a German gothic ‘hufnagel’ style on a 4-line stave with C marked. UCL Library Services Special Collections MS. Although we were successful in identifying the source of most of the fragments (which are varied: a fragment of St Paul’s Second Epistle to the Romans, a miniscule portion of the Greek play  Medea from the 4th or 5th century AD, a Gregorian chant based on Psalm 65), the digital humanists in our group also encouraged the use of social media to connect with a wider community of scholars (UCL does not have a music department, and we had no expertise in our team to help with identification of the music fragments). Others have had success in the identification of medieval fragments via crowdsourcing (Erwin, 2013), so we turned to the hive mind. Liaising with groups such as the Plainsong and Medieval Music Society on Facebook (https://www.facebook.com/groups/108654282490212/) and Musicologie Médiévale (http://gregorian-chant.ning.com/) allowed us to communicate with other manuscript experts and gain further insights, identification, and classification.        Figure 2. Successful outreach via social media to aid in further identification of fragments. Many of our fragments are now online and can be seen at bit.ly/uclmedievalfragments. This phase of the programme will have several additional benefits for teaching, research, and public engagement. The manuscripts will be showcased in exhibitions and disseminated online, forming a unique set of teaching and research materials. However, the innovative approach of this activity is in being able to prioritise which manuscripts will be most useful to focus efforts on, in order to help read damaged and deteriorated texts via advanced multi-modal imaging techniques, and at the time of writing we are moving into the final phase of the project, quantifying best practice in using multispectral imaging on damaged and abraded text. We now have a range of locally held, categorized medieval fragments with which to work as we perfect our techniques before visiting other archives to image historically important manuscripts.       Figure 3. MS FRAG/MUSIC/15, an antiphonal featuring Gregorian chants. Text and square musical notation in black with 4-line staves ruled in red. Only 40% of text legible: an example of a fragment we can work on with multispectral imaging to improve legibility of the text. Conclusion Framing the project as a digital humanities one opened up access to financial and physical resources provided by UCL Centre for Digital Humanities, gave easy access to institutional infrastructure for setting up and maintaining digital projects (such as data backup for digitisation), allowed identification and employment of staff to aid in the surveying and identification of the fragments, aided in reaching out to a wider online community to help identify fragments of unknown provenance, and allowed a large part of the digitization of the collection to be carried out. In doing this work we are now prepared to undertake novel imaging research with the collection (demonstrating the groundwork that is necessary before undertaking advanced cultural heritage imaging). It should be stressed that we could not have undertaken this project without the full support of the Library, whose own investment allowed refining of the collected data, cataloguing, mounting of the digitized material into the institutional content management system, and further digitization. By pooling resources and expertise, we have undertaken identification, cataloguing, and digitization of a collection, for a mere few thousand pounds, whilst preparing for a more advanced phase of imaging research. The project demonstrates, then, that centres for digital humanities, working in equal collaboration with institutional partners, can provide complementary resources and expertise to support complex digitization processes, whilst this mutual relationship can lead to future novel research endeavors. Notes 1. http://www.ucl.ac.uk/library/special-collections. 2. http://www.ucl.ac.uk/dh/facilities/digitisation-suite. 3. http://www.ucl.ac.uk/ah. 4. http://www.engineering.ucl.ac.uk/. 5. http://www.ucl.ac.uk/library/. 6. http://www.ucl.ac.uk/dh. 7. http://www.axiell.co.uk/calm. ",
        "article_title": "Collaborative Digitisation: UCL’s Medieval Manuscript Fragment Project",
        "authors": [
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Helen",
                "family": "Graham-Matheson",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Gillian",
                "family": "Furlong",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Steven",
                "family": "Wright",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Katy",
                "family": "Makin",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Adam",
                "family": "Gibson",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "medieval studies",
            "organization",
            "digitisation",
            "repositories",
            "project design",
            "archives",
            "sustainability and preservation",
            "English",
            "management",
            "resource creation",
            "image processing",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Slade School of Fine Art, 1 an internationally leading art school based at University College London, has an intriguing but underused archive relating to students and staff, and their teaching, artworks, and experiences. The Slade Archive Project, jointly undertaken by the Slade and UCL Centre for Digital Humanities, 2 is an interdisciplinary, highly iterative, exploratory research collaboration, investigating how digital tools and techniques can open up the archive to the public and increase engagement with its contents. We demonstrate that collaborative work with a centre for digital humanities informs and enhances the use and understanding of digital methods available to art historians—a field that has not, to date, made much use of computational research methods—and encourages and supports new archival research methods.    The Slade and Its Archive  Since 1871 the Slade School of Fine Art has educated and trained generations of world-renowned artists. 3 The Slade has an extensive archive, including papers, photographs, class lists, student records, audio recordings, films, prospectuses, death masks, and other artefacts, providing rich evidence of the culture and activities of the college. However, this archive is difficult to access, and no attempt has been made to present it to a wider audience. Over time, the archive has been consolidated and dispersed across the university, with records now held by UCL Art Museum, 4 UCL Records Office, 5 UCL Library Special Collections, 6 and within the art school itself. Cataloguing is incomplete, and the documentation systems are not interoperable.      Figure 1. The main body of the Slade Archive, held at UCL. The art school is both the context and the subject of the archive: any project derived from this archive will be, at least in part, art historical in nature. However, the archive is also of great interest to alumni, family historians, filmmakers, and authors who request access and want to contribute because of an interest in the personal histories  beyond their conventional scholarly value. In this complex web of priorities, interdependencies, and responsibilities, digital technologies can provide the means to engage with archival content in unprecedented ways.    Digital Humanities and Art History  There has been curiously little research that applies digital methods within an art historical context (beyond simple digitisation of collections). Although scholars are starting to question the relationship of digital methods to art history (Rodriguez Ortega, 2013) and are exploring new tools (Rodriguez et al., 2012), these are not yet embedded into art historical methods. The John Paul Getty Trust has been urging art historians to utilize more digital technologies (Dobrzynski, 2014), but it has been suggested that the digital needs of art historians can be successfully met only through the work of many support organizations (Long and Schonfeld, 2014): we have undertaken to do this in the Slade Archive Project, which is inherently collaborative.   The Slade Archive Project as Digital Humanities Collaboration  The Slade Archive Project was launched in summer 2012 as a joint initiative driven by a shared curiosity of what could be done with the unique archive materials within the digital space. At the outset, our small team 7 working from different areas of specialization had yet to learn what the archive even held. The project was conceived as a flexible and collaborative frame under which various sub-projects could be developed, driven by the specific research interests of those working at the Slade and the wider university, and by available resources or discreet funding opportunities. UCL’s provision of seed funds 8 for the initial scoping study of the archive gave us permission to work speculatively, to explore the archive in part by testing ways in which we might activate it, and the consequences and opportunities of doing so.   Framing the project as a digital humanities one opened up access to resources maintained by UCL Centre for Digital Humanities (such as the multi-modal digitisation suite 9), gave access to institutional infrastructure for setting up and maintaining digital projects, and allowed us to embed activities in teaching delivered as part of the MA/MSc in digital humanities. 10 Digital activities across the life span of the project are many and varied, and have included establishing a digital presence for the archive 11 (branding, hosting, and guiding social media activities to encourage public engagement), crowdsourcing the identification of individuals within annual class photographs 12 (prototyping a platform to allow the identification of individuals within digitised historical photographs), publishing archived oral history interviews 13 (providing guidance on digitisation), and mapping the diaspora of Slade students (using GIS to geographically plot their career pathways).      Figure 2. We digitised a selection of Slade class photographs and, using our own prototype face recognition and crowdsourcing software created by using cheap, off-the-shelf tools, 14 asked alumni to help us identify individuals. Visitors to the website can also add comments and add their recollections, and draw connections between students, tutors, and their historical context.   Another subproject is the Transnational Slade project 15 in which we identify international students who studied at the Slade in the 1950s, in order to trace the impact they went on to have internationally after graduation. The 1950s were a key period of change between Britain and its former colonial territories, and art historically it was a time when modernism began to enter the work of artists who would play a more visible role in the Independence movements of their countries in South Asia, Africa, and the Middle East (Dadi, 2010; Enwezor and Achebe, 2001). Yet how art school education has affected the history of art in different parts of the world has not been fully explored. We again used crowdsourcing identification of those within archival photos and gathered input on encounters, memories, and understandings of both the context of the Slade in the 1950s and the international students who were in attendance. This research has already resulted in new art historical understandings: we now know that approximately 10% of students at the Slade during the 1950s came from outside the United Kingdom, which shows how important international students were to its culture and the resulting diaspora. The online audience engaging with this aspect of our project have provided new accounts, information, and archival material that have helped us understand further the Slade’s role in international art contexts, particularly in Sudan and Pakistan.   In undertaking this research through the public dissemination, display, and crowdsourcing of otherwise dispersed and inaccessible archives, we are, as curator Matthew Teitelbaum wrote in 1996, ‘learning in public’ (40). As tasks and roles once associated with different professional and disciplinary arenas overlap, our range of activities have expanded beyond the familiar art historical activities of researching in and extracting from the archive, to encompass the collaborative, digitally iterative, and publicly situated work of ‘enabling, making public, educating, analysing, criticizing, theorizing, editing, and staging’ (Weski et al., 2012, 8). The digital aspects of the project provided the means to approach, refine, and choose ways in which to interrogate and understand the nature of the archive, whilst challenging conventional epistemological and disciplinary frames as it brings methods, practices, and theories together in new configurations (Cook, 1997).   Conclusion  The Slade Archive Project, as well as enabling us to explore what digital technologies can do to support, encourage, and expand research and outreach activities centred around a world-leading art school’s archive, allows us to conceptually rethink the remit and scope of such archival projects, and the role that digital humanities centres have in fostering and exploring new research techniques within traditional humanities disciplines. Involving digital humanities in this essentially art historical pursuit enabled us to articulate questions that incorporate disciplinary concerns (for instance, the writing of art historical narratives, or the validity of our crowdsourcing efforts), alongside questions of how the archive not only  connects people, but how people connect and contribute to the archive, and by extension, to our scholarly pursuits. The Slade Archive Project is, then, a practical example of the collaborative endeavour necessary to embed digital methods in art history research, as described hopefully by Long and Schonfeld (2014), and demonstrates the role that a digital humanities centre can have in helping colleagues exploring different aspects of their research. Employing the staff, expertise, and resources of a digital humanities centre does not suggest the need for some overarching authoritative digital humanities role to override the knowledge of archivists, curators, artists, and art historians. Rather it asks if and how the integration of digital humanities methods into the art historical landscape might add to our already rich disciplinary toolkit, and remind us of what we already know from our respective practices, in order to assist us in critically navigating our project together, where new convergences of the museum, the academy, the library, the archive, and the digitised spaces between continue to form new configurations of risk and opportunity in art historical research.   Notes 1. http://www.ucl.ac.uk/slade. 2. http://www.ucl.ac.uk/dh. 3. Famous alumni include Gwen and Augustus John, Stanley Spencer, and Ben Nicholson around the turn of the 20th century and early 1900s; Richard Hamilton and Eduardo Paolozzi in the 1940s; and Derek Jarman, Paula Rego, Euan Uglow, and Craigie Aitchison in the 1950s and 1960s. More recent Turner Prize–winning alumni include Martin Creed, Rachel Whiteread, Antony Gormley, and Douglas Gordon.  4. http://www.ucl.ac.uk/museums/uclart.  5. http://www.ucl.ac.uk/library/about/records-office.  6. http://www.ucl.ac.uk/library/special-collections. 7. Those listed as authors represent the core project team, but we would also like to thank Emma Chambers, Stephen Chaplin, Alexandra Eveleigh, Andrea Fredericksen, Gill Furlong, Colin Penman, Gemma Romain, Frederic J. Schwartz, Tony Slade, Alan Taylor, and Robert Winkworth for their input. 8. The Slade Archive Project was funded initially in 2012 by a UCL internal small research grant in the Arts and Humanities faculty, and then with a small research grant in 2013 from UCL Grand Challenges for the Transnational Slade component, which gave resources to employ a part-time research assistant and to carry out basic digitisation of specific items in the collection. In addition, sponsorship from the Andor Charitable Trust allowed digitisation of archived oral history recordings. 9. http://www.ucl.ac.uk/dh/facilities/digitisation-suite.  10. http://www.ucl.ac.uk/dh/courses/mamsc. 11. http://blogs.ucl.ac.uk/slade-archive-project/. 12. http://sladearchive.github.io/.  13. https://soundcloud.com/slade-archive-project. 14. Digitised class photos of the Slade Archive are processed through a face recognition algorithm using OpenCV (Bradsky, 2000). This process produces a location mapping of all recognised faces on the larger images. This information was used to crop image portraits of each person recognised by the algorithm and created a static HTML page for each face identified. With the help of the D3 library (Bostock et al., 2011), we also built an interactive visualisation that highlights each portrait within the class photo. We then asked the public to help identify the sitters using the Disqus (https://disqus.com) commenting system, which has been integrated into each portrait page. Periodically, we collected the public’s comments and integrated the names of the people identified into the visualisation. Our prototype gathered a surprising amount of the public’s interest, partly with the help of some media attention (Weaver, 2013).  15. http://blogs.ucl.ac.uk/slade-archive-project/category/transnational-slade/. ",
        "article_title": "Digital Humanities as Catalyst for Digital Art History: The Slade Archive Project",
        "authors": [
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Liz",
                "family": "Bruchet",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Amna",
                "family": "Malik",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Susan",
                "family": "Collins",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Beavan",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Jo",
                "family": "Volley",
                "affiliation": [
                    {
                        "original_name": "University College London, United Kingdom",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Alejandro",
                "family": "Giacometti",
                "affiliation": [
                    {
                        "original_name": "King's College London, United Kingdom",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "digitisation",
            "repositories",
            "interdisciplinary collaboration",
            "project design",
            "archives",
            "sustainability and preservation",
            "English",
            "management",
            "resource creation",
            "crowdsourcing",
            "and discovery",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper discusses a project that is encoding archival vocabularies of Australian Indigenous languages recorded by Daisy Bates (White, 1985; McGregor, 2012) in the early 20th century. With this material we aim to create new research outcomes by exploring the text using novel techniques. We advocate a methodology in which primary research materials should be citable and accessible so that derived analyses can be verified and the primary data enriched by the process of research. This work will allow identification of the languages represented by these vocabularies and the internal variation in those languages. It will also make the data searchable for the first time, and linkable to other datasets, with the long-term goal of creating rich records for Australian Aboriginal languages, and, at the same time, embedding research practice in established international methodologies. This is the first application of the Text Encoding Initiative (TEI) to Indigenous-language material in Australia, and one of the few uses of the TEI with Indigenous languages that we know of (see Czaykowska-Higgins et al., 2014).  Some 23,000 pages of these manuscripts at the National Library of Australia were digitised as TIFF images from a microfilmed copy and named according to NLA conventions, and 4,300 pages have so far been keyboarded, geocoded, and TEI encoded. These pages represent three kinds of material: (1) a handwritten questionnaire, containing some 2,000 prompt words and sentences; (2) a typescript version of each questionnaire; and (3) other manuscript pages. We have identified over 150 speakers and on the order of 40 languages in the collection. One of the outcomes of the project will be to determine the relationships between these wordlists and later sources, and to identify more securely what languages are represented. The Barr Smith Library has been producing pdf/text versions of some of these manuscripts that increases their availability, 1 and the collection has been in the public domain since the 1950s. It has been used in various projects, including Native Title claims, but, in general, the knowledge gained by these processes is not reflected in the collection itself. The promise offered by new DH methods is that any enrichment of the sources should then be reflected in them. We acknowledge that Indigenous people have a strong interest in the Bates vocabularies, but, given the number of speakers involved, the many descendants they would now have, the geographic spread of the sources (covering most of Western Australia), and the resources available to us, we are unable to consult widely in the way that, for example, the Laves project did, working as it was with archival material from a single geographical area (as reported in Henderson, 2008).  The TEI texts will remain the repository of the full data model of the project, representing the corpus of texts in a number of different dimensions. At one level, the TEI texts will constitute a set of facsimile editions by linking page facsimile images to the textual transcriptions of those images. At another level they will include explicit markup of person, language, and place names, to provide further points of entry to the data, and allow linking to external authority files (where possible). This multi-dimensional data model will be built up step by step and encoded in the TEI corpus by a combination of careful manual markup and the application of algorithmic methods.  The manual data-entry work was initially restricted to a typographical encoding of the texts simply as two-column tables with headings. The right-hand column of these tables, generally containing a comma-separated list of Indigenous words, was then marked up by an automated XSLT transformation. This naturally leads to an iterative data cleaning workflow based on a software development ‘build’ process, in which a time-consuming batch process crunches through the documents, gradually enhancing them, performing automated validation checking, and generating visualisations for manual review. We found these data visualisations a potent force for quality assurance. It is easy to spot interpretive errors made by the automated parser, and that correction can feed back, either as a refinement of the automated process or as a manual correction of the document markup, leading to a gradual improvement in the data quality. The dataset exists in a number of forms: a paper archive; a set of photographic images in TIFF format, and derived JPEGs; a set of TEI transcriptions that encodes the text and the tabular layout of the forms; and links to the JPEG page images. A gazetteer of places is maintained as a spreadsheet. The workflow is managed via a spreadsheet, and the source files are stored in a bitbucket repository. One important requirement is the ability to track the provenance of the individual words. A dataset of this scale and type has scope for numerous transcription errors, and it must be possible for a reviewer to query any word, to see the manuscript from which it came, and to view images of the source materials to establish the context in which the term appeared, and thus to check that the contextual metadata in the TEI is correct. The solution is to begin by building a model of the texts, rather than attempting to directly model the lexicographic knowledge they contain. The model of the texts can then be enriched with the lexicographic knowledge in the form of embedded metadata, and from this a simplified model can then be derived to serve the purpose of linguistic computation. We build the data model by parsing the text, but rather than parsing the text and populating a data model, we embed the parsed version into the text in the form of XML markup. This makes the interpretive act explicit—something that can be questioned and altered. We classified each word as either English or Indigenous, and hyperlinked each to the English to which it corresponds. Given the size of the encoding task, it was essential to minimise the amount of manual work, and reduce the scope for human error, by automating the markup process as much as possible. This XSLT script parses the lists into distinct words and inserts the appropriate hyperlinks to relate each Indigenous word to the English word or words to which it corresponds. Some Indigenous words have multiple corresponding English words, separated by commas or sometimes semicolons:   Ankle Kan-ka, jinna werree, balgu   Occasionally, the word ‘or’ is used at the end of a list:   Blood Ngooba or yalgoo   Sometimes the right-hand column contains additional English language glosses—generally to indicate a narrower or otherwise related term, usually written in parentheses, following the corresponding Indigenous word, e.g.,   Kangaroo Maloo (plains), margaji (hill)   Sometimes an additional English gloss is written before the corresponding Indigenous term and separated with an equal sign (or hyphen):   Woman, old Wîdhu; old man = winja   An XSLT script is easily able to handle all these cases and rapidly produce markup that is semantically correct. However, as the forms were filled out by many different people, inevitably there are some inconsistencies in the text, which can lead the XSLT script into error. Sometimes, for instance, the Indigenous words are in brackets rather than the English words, or the text is written in a more explanatory style, which is not amenable to parsing with a simple script:   Bandicoot  Jiriwardu, bardari and bira - two species like a bandicoot. Bardari - like a bandicoot, only with long ears and nose.    In these exceptional cases the easiest thing to do is apply some human intelligence and mark up the text by hand. To support this ‘markup by exception’ style of work, the automated markup XSLT is programmed to simply pass over any text that already contains the TEI <term> and <gloss> markup.  If we find something that looks odd through an analysis of the lexicographic data, how are we able to check whether it is a genuine language oddity or the result of an error? It is essential to be able to track the linguistic data back to the archival source, and that means maintaining links from the derived data that point back to the source questionnaires, and that in turn means having a data model that is able to accommodate all aspects of the data.  The value of the TEI is that it can link those aspects together in a single set of source files. Then the ‘normalised’ version can be extracted as needed from the fully modelled data.  A number of issues will be explored as we develop this project, including the possibility of automated regularising of orthographic differences between word-lists, further permitting comparison of similarities via a Levenshtein distance measure. As the underlying material is in a reusable format, it will be possible to publish electronic editions of the manuscripts and output sets of word-lists for use in schools, and to do comparative analysis of the vocabularies to infer relationships and locations (as in Nash, 2002, and more recently Embleton et al., 2013). We have used XSLT to extract a graph database in RDF-XML format from the TEI source data, and we use SPARQL to query this derived dataset to produce visualisations, and to expose it as Linked Data.  Funding This work was funded by a research grant from the Faculty of Arts at the University of Melbourne and supported in part by ARC grants DP0984419 and FT140100214. Note 1. https://digital.library.adelaide.edu.au/dspace/handle/2440/69252. ",
        "article_title": "Encoding Vocabularies of Australian Indigenous Languages",
        "authors": [
            {
                "given": "Nick",
                "family": "Thieberger",
                "affiliation": [
                    {
                        "original_name": "University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": "Conal",
                "family": "Tuohy",
                "affiliation": [
                    {
                        "original_name": "University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "linguistics",
            "corpora and corpus activities",
            "encoding - theory and practice",
            "English",
            "resource creation",
            "lexicography",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital technologies are facilitating a renaissance of large-scale oral history projects (Gallwey, 2013; Thomson, 2007). Big oral history projects enjoyed a heyday in the 1970s and 1980s. They were often led by sociologically trained historians who sought to create interview samples of sufficient size and diversity to enable ‘the larger insights or generalizations’ that we might gain by finding patterns across many interviews. But large oral history projects ‘receded from view’ by the 1990s as funding dried up for the expensive task of producing and transcribing multiple interviews (Armitage and Gluck, 2006). Just as important, as Armitage and Gluck acknowledge, was the redirection of researcher energy to the cultural or narrative ‘turn’ in oral history (and in history more generally), in which oral historians focused on the  meanings of oral narratives and mostly found it easier, and cheaper, to find and interpret those meanings if they worked with a small number of interviews.   Twenty-first-century technologies have made large oral history projects feasible and attractive again, by offering new tools for creating, managing, and accessing large collections of interviews. Digital technologies are transforming the creation, preservation, presentation, and use of oral history, whilst generating methodological challenges and stretching old ethical dilemmas in new directions (Boyd, 2011; 2013; Frisch and Lambert, 2011; Lambert and Frisch, 2013; Larson, 2013; High, 2010; Zembrzycki, 2013. See also the resources and essays on the Oral History in the Digital Age website, http://ohda.matrix.msu.edu/).  The Australian Generations Oral History Project, an ARC-funded collaboration between university historians, the National Library of Australia, and ABC Radio National, has pioneered digital technologies for oral history (see http://artsonline.monash.edu.au/australian-generations/). A team of nine researchers and 25 interviewers has recorded 300 life history interviews with 50 people born in each decade from the 1930s to the 1980s. We have produced 1,221 hours of digital audio recordings that are searchable via TROVE, the NLA’s aggregation and discovery system, and which will be available for online research, subject to consent agreements, by anyone, anywhere, forever.  Our presentation will focus on how we are preserving and documenting our interviews, and presenting and interpreting them through writing in online formats that integrate aural material. We consider the opportunities and challenges posed by an internationally cutting-edge timed summary system linked to hundreds of hours of digital audio recordings and searchable online via TROVE (Bradley, 2014); an interviewer Google site that collected interviewer discussion forum postings about each of 300 life history interviews and offers an essential finding aid to the interviews; a ZOTERO database that combines the different types of data produced by the project, which grows as researchers add notes about each interview and which will, in due course, be available to future researchers; and an aural history book that enables readers/viewers to listen to interview extracts as they read them, and to discover the aural meanings of recorded interviews. Each technological innovation has posed significant ethical issues, which we will examine in our presentation (for example, each interviewee completes an NLA Rights Agreement that specifies conditions for use of their interview, but in some cases we have closed interviews that interviewees wish to place online because of contents that are potentially defamatory or that reveal illegal activity). ",
        "article_title": "Making Digital Aural History",
        "authors": [
            {
                "given": "ALISTAIR",
                "family": "THOMSON",
                "affiliation": [
                    {
                        "original_name": "MONASH UNIVERSITY, Australia",
                        "normalized_name": "Monash University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02bfwt286",
                            "GRID": "grid.1002.3"
                        }
                    }
                ]
            },
            {
                "given": "KEVIN",
                "family": "BRADLEY",
                "affiliation": [
                    {
                        "original_name": "NATIONAL LIBRARY OF AUSTRALIA",
                        "normalized_name": "National Library of Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04yvjg468",
                            "GRID": "grid.453984.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "repositories",
            "historical studies",
            "libraries",
            "archives",
            "museums",
            "video",
            "audio",
            "sustainability and preservation",
            "English",
            "multimedia",
            "GLAM: galleries",
            "resource creation",
            "data mining / text mining",
            "and discovery",
            "folklore and oral history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Making Histories is a level 2 History undergraduate unit taught as a collaboration between historians at Monash University and curators at Museum Victoria. Students research their own history project, drawing upon the Museum’s rich collection or their own family or local history resources of images, documents, and recordings. They develop a history project proposal based on primary and secondary research, write a film script based upon their research, and produce a three-minute digital video history. Students present their work at a Showcase event at Museum Victoria, which includes family members whose history they may have researched, and curators who may add their video histories to the Museum’s online collection. For student video histories produced in 2014, see https://vimeo.com/groups/makinghistory/albums/10825. For the Museum Victoria Making History website, a resource for both school and university students, see http://museumvictoria.com.au/discoverycentre/websites/making-history/.  After the Showcase event the final assessment for the unit is a class test for which students have known the question from the first day: What have you learnt about making history from making history? Thus the students not only develop a range of historical skills; they are also encouraged to become reflective practitioners and to better understand the processes and issues of historical research and production (see Cullen, 2009; Curthoys and McGrath, 2009; Gunn and Faire, 2011). Initial research using the exams suggests that the students have developed a wider understanding of what constitutes history (intimate personal histories as well as big-picture history), and that histories are always constructed though not fictional. More specifically, while they develop expertise in historical writing through the production of their research proposal, they also develop the sort of transferable skills—like succinct writing for an audience and digital video production—that are now essential tools of the trade in public history and indeed in many professions. The course draws upon the methodologies of digital storytelling (see Lambert, 2013; Burgess and Klaebe, 2009). Digital storytelling offers tremendous potential for historical pedagogy and for the production and dissemination of student history research. Students are engaged by the opportunity to make histories that many people will view, unlike the essays they write for one or two examiners. They are excited by seeing their own families and localities as sites of history; they enjoy gathering family photos, documents, and interviews; they see their histories in new light as they research the secondary literature on their topic; they bring extraordinary digital skills to the classroom but adapt and develop them to historical production; and they feel immense satisfaction and pride as they realise that they are history makers and not just history consumers (for historians’ use of digital storytelling see Benmayor, 2008; Coleborne and Bliss, 2011; Coventry et al., 2006). Though the three-minute digital story seems worryingly short for many students when they start the course, the time length and format does work well within the limits of a single-semester course when students have to conduct original research  and produce a history video.  In this paper we argue, however, that the digital storytelling model advocated by Joe Lambert and others can also be a limited and problematic approach to historical research and production, because it does not require extensive research or critical thinking, and because the storytelling model and emotive intentions that underpin standard approaches to digital storytelling frame good stories that are not necessarily good histories (see Poletti, 2011). In this presentation we assess the opportunities and challenges of teaching historical skills and understandings through the production of digital video history, drawing upon our own experiences of teaching the course and student reflections upon their own learning. We argue that digital storytelling can be adapted for history research and production but with important provisos and qualifications: topics need to be underpinned with deep and extensive research; intimate stories need to be connected to larger historical forces and issues; the analysis and evocation of emotion needs to be developed with both intellectual rigour and due care. ",
        "article_title": "Making Digital Histories: Developing Students as Historians",
        "authors": [
            {
                "given": "ALISTAIR",
                "family": "THOMSON",
                "affiliation": [
                    {
                        "original_name": "MONASH UNIVERSITY, Australia",
                        "normalized_name": "Monash University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02bfwt286",
                            "GRID": "grid.1002.3"
                        }
                    }
                ]
            },
            {
                "given": "JOHNNY",
                "family": "BELL",
                "affiliation": [
                    {
                        "original_name": "MONASH UNIVERSITY, Australia",
                        "normalized_name": "Monash University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/02bfwt286",
                            "GRID": "grid.1002.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "libraries",
            "archives",
            "museums",
            "video",
            "audio",
            "English",
            "multimedia",
            "GLAM: galleries",
            "folklore and oral history",
            "teaching and pedagogy"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In recent years, digital/analog archives for photographs have been created in various places, and photos from different time periods and regions have been exhibited. 1 However, one faces several challenges when trying to use the original photo materials as data.  The first challenge is that it is very difficult to precisely determine the specifics of the subject of the photographs, such as people, buildings, time period, etc., due to there not being enough descriptions in writing, as the photographs are iconographic materials.  The second challenge is that it is rare for the inherited photographs to remain intact, and they are often damaged after natural disasters, wars, disagreements, etc. Therefore, it is necessary to grasp the information about their history and state of preservation. In addition, as a third point, in the analysis of photo materials it is rare that the frequency with which a particular subject is expressed has been quantified before the analysis, so in many cases the target analysis is performed from a subjective impression. However, in an objective analysis, it is desirable that the analysis is based on main quantitative data, such as frequency of the subject (Togiya and Kawashima, 2013). For this reason, in photo materials, it is necessary to provide basic data, such as frequency with which the particular subject is expressed. Outline of the Research Project In this study, we propose basic elements and an implementation plan for a ‘Research Profile’ of the photo documentation in order to objectively use photographic materials as academic materials. First, the ‘Research Profile’ of the photo archive proposed in this study includes the following three elements:  1. Documentation on provenance and original condition (related documents, footage of interviews with old warehouse owners, visual material of the old warehouse space, distribution of the material, and a graph showing inheritance status).  2. Materials that became a rationale for the content of the material for cataloging purposes, such as video interviews, official records and discussions by experts (video and online discussion data), personal information of the subjects, genealogical data, information on the buildings, etc.  3. Frequency with which names that can be recognized are expressed in the subject, frequency with which multiple subjects are expressed together in the photo documentation group. Figure 1 is a diagram showing the actual configuration of the screen shown in the previous section ‘Research Profile’. Video and audio materials that contain detailed information about the corresponding material are stored in the profile after publishing the photo material of interest. These are stored in EPUB format, and text, voice, and images are integrated and stored in one set of files, enabling access with various e-book viewers.    Figure 1. The sample of research profile consisted of photo, movie, and graph. Summary The main point in this presentation is that since the number of photo archives is expected to increase in the future, it is important to collect data that can be browsed and analyzed from a more objective viewpoint and open it to the public as a ‘Research Profile’. In addition, another feature is that data, such as provenance information, interviews with witnesses, and discussions with experts—which became a reference at time of cataloging and until now was out of focus—can be systematically collected as objective data on quantitative analysis of the subject, and can be aggregated and published as material for scientific use. Additionally, these texts, pictures, audio, still images, and other media are aggregated in e-book format, and by using tablets, etc., they can be viewed in one set of files, browsed, and viewed in parallel in PC photo archive format or paper book format. Furthermore, persistent management is possible by storing them in an e-book format into e-book deposit systems, which are becoming popular these days. By making public ‘Research Profiles’ of photo archives, like the one in this study, it will be possible to use photo materials more objectively as scientific materials. An awareness that the testimony of the people involved in the photo is ‘documentation’ will develop, which will provide a base for developing the budding ‘Photograph Material Science’ in the future.  Note 1. Library of Congress: Prints & Photographs Online Catalog, http://www.loc.gov/pictures/; Smithsonian American Art Museum Photograph Archive, http://americanart.si.edu/research/programs/archive/. ",
        "article_title": "Proposal for Creation of \"Research Profile\" of Photo Archives Using Digital Book Format",
        "authors": [
            {
                "given": "Norio",
                "family": "Togiya",
                "affiliation": [
                    {
                        "original_name": "Kansai University, Japan",
                        "normalized_name": "Kansai University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/03xg1f311",
                            "GRID": "grid.412013.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "repositories",
            "archives",
            "video",
            "sustainability and preservation",
            "audio",
            "English",
            "multimedia",
            "art history"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This Digital Humanities GIS (DHGSI) model cross-pollinates literary and social media practices to engage in a participatory, performative, and augmented reality survey of the relations between James Joyce’s novel  Ulysses (1922) and digital eco-system productions of dialogical and social space (Goodchild, 2009; Sieber, Wellen, and Yuan 2011; Priem, 2011; Young and Gilmore, 2013; Graham and Zook, 2013; Lin, 2013). Joyce famously boasted that the aim of  Ulysses (which kaleidoscopically relates the urban journeys of student Stephen Dedalus and advertising salesman Leopold Bloom on 16 June 1904) was ‘to give a picture of Dublin so complete that if the city one day suddenly disappeared from the earth it could be reconstructed out of my book’ (Budgen, 1972, 69). The model integrates a  Ulysses schema outline with live geo-spatially enabled Twitter, Flickr, and YouTube posts to map the language operating in a Bloomsday-generated digital eco-system to re-create the discourse of a virtual Joycean Dublin during the annual celebration of the novel. Consequently the ‘Joycean’ neologism  tweetflickertubing was coined to describe the ontological shift indicated by the HGIS model’s methodology.    Creating the Model  In 1920 Joyce drafted a schema outlining  Ulysses’ eighteen Homeric episodes for the Italian critic Carlo Linati, to whom he wrote, ‘in view of the enormous bulk and the more than enormous complexity of my damned monster-novel it would be better to send . . . a sort of summary-key-skeleton schema’ (Ellman, 1974, 187). The schema’s grid designates episode title, time, color, people, science/art, meaning, technic, organ, and symbols. To create the model an Excel spreadsheet (Figure 1) was populated with this data and geo-coded according to GPS-designated decimal degree locations of the 18 Homeric episode sites in Dublin. These sites were identified through the 1904  Thom’s Map of Dublin, Ian Gunn and Clive Hart’s  James Joyce’s Dublin: A Topographical Guide (2004), and GIS ‘ground-truthing’ methods, and checked against the ‘what’s here’ function of the Google Maps app.   Site centroids were approximately identified to concrete locations described in  Ulysses because the various characters’ movements and locations within the novel (such as the Wandering Rocks episode) occur simultaneously and at multiple sites within and beyond the geographical and temporal boundaries distinguishing each episode in the schema. The Excel database was imported into Google Fusion and visualized through its Google Maps function. The database was also converted to a CSV file and imported into the ArcGIS Online platform and integrated with a live social media map layer.         Figure 1: Fragments of the Carlo Linati schema, Google Fusion map, and Excel/CVS Geo database.  Surveying Bloomsday Digital Ecosystem  Surveys were taken on 16 June 2014 at hourly intervals, based upon the chronology from the Linati schema, and were divided into two categories: local (Dublin) and global, weighed by the indices of site and time, respectively. Several keywords—such as ‘James Joyce’, ‘Ulysses’, and ‘Bloomsday’, as well as character and episode names from the novel—were tested in the model’s Twitter, Flickr, and YouTube search engines. ‘Bloomsday’ received the highest number of hits and became the main survey keyword.  The local survey focused on activity around Homeric episode sites in Dublin, and found that Davy Byrne’s Pub on Duke Street and Joyce’s Martello Tower associated with the  Lestrygonians and  Telemachus episodes attracted the most posts. Tweet postings did not, however, correspond with the chronology of  Ulysses’ narrative outlined in the schema, illustrating that social media activity aggregated around site location rather than novelistic time. A Tweeted image (Figure 2) captures throngs of people in funny hats assembled around Byrne’s pub, and it seems that Joyce’s identification of the ‘Oesophagus’ as the body organ symbol for this episode was indeed apt. The National Library site was originally geo-coded as the centroid of the  Lestrygonians episode; however, survey results suggest that perhaps because of the social gravity indicated by the number of social media posts, the centroid should be re-located to the site of the pub, illustrating the iterative process integral to Neogeographical mapping practices. In the case of the global survey, Tweets blossomed across Europe, the Middle East, Asia, Australia-Pacific, North America, and Latin America during the entire chronology of the Linati schema. ‘Orphan’ Tweets (corresponding outside of the hourly periods not included in the schema) were placed either in preceding episode time slots or in the Penelope episode—whose time indices encompass ‘Infinity’ (see Bloomsday Tweet Schema in poster).         Figure 2. Live social media map integrated with the Linati schema geo-database.  Surveys taken before, on, and following Bloomsday 2014 illustrate that Flickr and YouTube postings with time lags, and reflecting activity over the course of a year, exhibited the most aggregated social media activity. However, over the course of the 16 June 2014 survey, it became apparent that dominant social media ecosystem activity on Bloomsday occurred in Twitter. The following verbal snapshot reflects a parsing of language activity (see Bloomsday Tweet Schema in poster) articulated in this digital eco-system: ‘People in Dublin are wearing funny hats because it is Bloomsday and elderly ladies are getting rowdy in Davy Byrne’s Pub; a wedding anniversary is observed in Glasnevin, North Dublin, while a Spanish tweeter celebrates with a Domino’s Pizza and the latest  X-Men film. Individuals in Dublin, Paris and Washington D.C. resolve to again attempt to read  Ulysses, and a tweeter in Uruguay mentions Bloomsday to her Irish boyfriend, who asks if the day has anything to do with flowers. A few literary minded types post Joycean lines from the novel, while two individuals from Dublin get suited up in Edwardian clothes to face the day; one tweeter reflecting on the day after the night before, asks if Bloomsday was a joke brought up in a drinking session. A tweeter from Mexico City advertises online translations of Joyce’s “lascivious” letters to his wife Nora Barnacle. The celebration of  Ulysses converges with perhaps a larger global event to provoke a Dublin tweeter to state that there is “Nothing like a combo of World Cup and bloomsday to hear people who don’t like either Joyce or football talk about both”. One wry observation from the Bronx asks if “Bloomsday is Paddy’s Day for posh people?” And two more tweets from the USA proclaim “I’m pretty sure Joyce would love hashtags”, and “To paraphrase Laurie Anderson:  Ulysses? Never read it”’.   A corollary can be made between Joyce’s writing technique in  Ulysses and the use of language in this Twitter-based digital ecosystem. Joyce’s stream-of-consciousness technique mimicked the various ways in which the human mind ‘speaks’ to itself, through complex fluid patterns, random interruptions, incomplete thoughts, half words, and tangents (Norris and Flint, 2000, 126). Tweets, limited to a certain number of characters, reflect Joyce’s technique by conveying both focused and random thoughts, and illuminate Graham and Zook’s (2013, 78) contention that the ‘digital dimensions of places are fractured along a number of axes such as location, language, and social networks with correspondingly splintered representations customized to individuals’ unique sets of abilities and backgrounds’. The DHGIS model illuminates how literary and historical tropes can aid in contextualizing and mapping social media activity through the creation of interpretive schemas to study interactions between language, behaviour, time, and place.    DHGIS Implications   Re-conceptualizing J. B. Harley’s observation that ‘text’ is a better metaphor for maps than the mirror of nature through the lens of the digital humanities, it can be seen that HGIS-generated ‘maps are a cultural text. By accepting their textuality we are able to embrace a number of different interpretative possibilities’ (1989, 4). The DHGIS model enabled digital inter-textual relationships between  Ulysses, the Linati schema, and the dialogical and social space reflected in the Bloomsday social media eco-system. Subsequently, digital humanities methodologies of  deformance and  ergodicity were applied as interpretative techniques. By translating one ontological form of discourse to another,  deformance applies  scientia to  poeisis and seeks to explain unitary and unique phenomena (such as the language activity in the Bloomsday social media ecosystem) rather than establish a set of general rules or laws (McGann and Samuels, 1999).  Ergodicity involves an interactive type of labor between the GIS practitioner/author/coder, reader/viewer, and mapping subject to create the potential multiple narrative paths composing a digital text.  Ergodic applications of GIS create non-linear narratives that converge and disrupt both quotidian and epochal chronologies of time and space. As a literary tool, this DHGIS model synchronizes the resulting layers of images, words, and vectors into contrapuntal, multi-dimensional digital narratives, providing the means to ‘reconnect the representational spaces of literary texts not only to material spaces they depict, but also reverse the moment’ (Staley, 2007; Thacker, 2005, 63). Lastly, DHGIS, digital, spatial, and heohumanities modelling techniques, integrated with radical statistics holds the potential to engage Qualitative & Critical GIS / GIScience studies with reflexive epistemologies to address the situatedness and positionality of Big Data as it relates to sustainability initiatives, smart city planning, transport and public health, disaster preparedness, and social and regional conflict (Cope and Elwood, 2009; Aitken and Craine, 2009; Bodenhamer et al., 2010; Dear et al., 2011; Meir, 2011; Elwood et al., 2012; Kwan and Schwanen, 2012; Leventala, 2012; Kitchin, 2014; Travis, 2014).   ",
        "article_title": "A Digital Humanities GIS Ontology: Tweetflickertubing James Joyce’s \"Ulysses\" (1922)",
        "authors": [
            {
                "given": "Charles Bartlett",
                "family": "Travis",
                "affiliation": [
                    {
                        "original_name": "Long Room Hub, Trinity College Dublin, Ireland",
                        "normalized_name": "Trinity College Dublin",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/02tyrky19",
                            "GRID": "grid.8217.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "media studies",
            "literary studies",
            "mobile applications and mobile design",
            "interdisciplinary collaboration",
            "geospatial analysis",
            "bibliographic methods / textual studies",
            "internet / world wide web",
            "analysis and visualisation",
            "data mining / text mining",
            "ontologies",
            "historical studies",
            "content analysis",
            "agent modeling and simulation",
            "text analysis",
            "social media",
            "interfaces and technology",
            "spatio-temporal modeling",
            "English",
            "crowdsourcing",
            "maps and mapping",
            "text generation",
            "visualisation",
            "data modeling and architecture including hypothesis-driven modeling",
            "interface and user experience design",
            "virtual and augmented reality",
            "databases & dbms"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A recent study supports the claim that Aboriginal Australians are the oldest continuous living culture on the planet ( Science, 2011), their being directly descended from the first people to leave Africa up to 75,000 years ago. Aborigines developed a rich culture involving custom, lore, and value systems based on the sustainability of their spiritual connection, belonging, obligation, and responsibility to care for their land, their people, and their environment (Andrews et al., 2006). Knowledge of this fascinating culture is still strong within Aboriginal communities and has adapted to live in two worlds, which in part is due to the recent radical change in lives of Australian Aborigines, the arrival of British colonists in the 18th century, and consequent European domination of the Australian continent. Since then, the relationship between the original Aboriginal population and more recent arrivals changed from initial curiosity, through mutual aversion, to ignorance.   In recent years, both parties are trying to improve this unfortunate situation by dealing with consequences of historical actions and better understanding each other’s cultures. Various initiatives aim to explore and preserve the oldest surviving culture in the world and to disseminate it to the broader Australian population. These efforts include developing First Peoples perspectives in mainstream institutions. Other resources on this matter include organisational web pages, multimedia, and digital archives.  The Aboriginal connection with the world is hard to capture, since time—particularly the dreaming or dreamtime—is difficult to express in static forms of presentation. This is where interactive workshops and focus groups can have a profound impact on the way content is consumed. Non-interactive multimedia, such as movies and videos, are a viable substitute, but their creation is expensive and, once made, the content becomes static and unchangeable. Carefully crafted web pages can combine both experiences, using multimedia for presentation and also allowing feedback from presenters. Yet web pages usually provide content for already interested parties rather than attracting a new public.  The main attraction of today is modern technology, enhanced with artificial intelligence, interactive multimedia, games, and virtual reality (VR). Currently, the VR hardware price has dropped down to $8 for Google Cardboard and $199 for Samsung VR, becoming available to the wide public. Computer games and virtual reality combine the interactive possibilities of workshops with multimedia. It allows us to capture and simulate the transient nature of First Peoples culture, the dreamtime, and to present these from various perspectives, either as an observer or a direct participant. Therefore, in our project, we focus on creating an immersive computer simulation, using a computer games approach and virtual reality, focusing on various aspects of life and events that have occurred throughout the history of aboriginal tribes living in Australia.  The benefits of our project are threefold:   1.  We seek permission from Aboriginal cultural authorities (i.e., Elders/Traditional Knowledge Keepers) to capture and record Aboriginal perspectives.   For the purposes of our simulation, we collaborate with Aboriginal Elders to gain perspectives on the philosophical thinking of surviving in two worlds. Aboriginal people have a holistic worldview and belong to the land, viewing the world from a different perspective—for example, in an Aboriginal voice, ‘We belong to the land and have a responsibility to care for that land’. In this project we recorded what life was like in community pre-contact using motion capture technology. We have also recorded various stories of recollections of what life was like back then and, in collaboration with professionals, co-created with Aboriginal Elders educational plots that motivate simulation participants to explore and understand the perspectives of living in two worlds. We aim to record and digitise Aboriginal languages and place them in the situational context. This not only preserves many already disappearing languages but enables interested parties to learn them efficiently in the immersive manner.  Along with this we are continuously co-creating with Aboriginal people, protecting their intellectual property by appropriate attribution with inputs into a large public database of Aboriginal resources, such as texts, images, videos, and references to be used throughout our project and by any other interested party (available at http://gok.scem.uws.edu.au). 2.  We engage and co-create with the Aboriginal community to understand their perspectives by using simulation technology.   During the development of our simulation, we initially engage the Aboriginal community to help us co-create the content for our simulation. First, we engage and seek permission from Aboriginal Elders to access their intellectual property and record their perspectives, which can subsequently be presented in a simulation. Later, consulting with Elders and in collaboration with Aboriginal artists, we created the 3D assets of the simulation. These assets concern the landscape of the simulation, design of the original population, and wearables and objects they used. Last, we approach youth from various communities to help us co-create the 3D content and writing new stories, allowing others to re-live their experiences in the simulated environment.  3.  Raise appreciation and appropriate attribution for Aboriginal intellectual property within the non-Aboriginal population.   Using our simulation, participants can learn by playing a game with a carefully crafted educational plot, which incrementally raises their knowledge about the presented topics. During this experience each party is able to re-live the experience from the perspective of the other party, helping in understanding both points of the view. Moreover, using virtual reality and the gaming approach attracts the ‘digital’ generation or ‘social web’ generation, allowing them to perform in collaborative environments, providing for the deep social experience.  In the first phase of this project, we simulate an aspect of Aboriginal culture in the Parramatta basin in the year 1770, pre-contact and before colonisation. The simulation is presented in two formats.  First, an interactive 3D video game takes the participant on a quest to explore the life of a clan in the Parramatta basin. A spiritual mentor and guardian in the form of an Aboriginal Elder gradually introduces the participant to the daily life of aboriginal clans—the knowledge they possessed, rituals they performed, protocols they kept, and their connection to dreamtime. The Elder presents the participant to clan members, who are performing various tasks such as spear making, tool making, painting, fishing, or preparing food. During these interactions, the participant also learns about aboriginal medicine, astronomy, arts, as well as ceremonies, such as the smoking ceremony, thus learning about their spiritual values.  For the purposes of this simulation we have collected information from secondary and primary sources, captured it in text and video, and used motion capture technology to record animations. In close collaboration with Elders, we chose the best form of presentation, preparing the plot of our game (simulation). A problematic issue was the amount of dialogue we planned to use. Here, it was advised to use as little as possible and to depend mostly on non-verbal language and visual inference, so maintaining the historic accuracy. Yet, for the format of our presentation, we decided to use dialogue and written word to communicate with participants, but we are continuing to explore possibilities for meeting this requirement.  Second, we built a virtual reality experience, using Oculus Rift and a historical narrative to build an immersive exhibit of aboriginal life. This part of the experience no longer depends on written texts and graphical user interfaces; it fully relies on virtual interactions and dialogue. Due to various responses from our test subjects, which varied from very pleasant experiences to slight motion sickness to vertigo, we have limited the duration of the VR experience to five minutes. We are working to extend this time, better understanding the problems related to the VR simulations. Yet we already acknowledged that the biggest drawback of VR is that it will not suit everyone, and therefore we plan to maintain both 3D and VR experiences.  In the consecutive phases we plan to extend our simulation to re-live history through Aboriginal perspectives, from the first contact up until modern times. The goal of this simulation is to put into perspective views from both parties, to understand them and learn from them. Apart from simulating historical events, co-creating the content will open possibilities to include many different life stories and allow for interpretation of historical contexts.  The demo of the simulation can be found at https://www.youtube.com/watch?v=aT_mwlobwK8.  ",
        "article_title": "The Aboriginal Dreaming meets Virtual Reality",
        "authors": [
            {
                "given": "Tomas",
                "family": "Trescak",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Williams",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Terry",
                "family": "Sloan",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Anton",
                "family": "Bogdanovych",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            },
            {
                "given": "Simeon",
                "family": "Simoff",
                "affiliation": [
                    {
                        "original_name": "University of Western Sydney, Australia",
                        "normalized_name": "Western Sydney University",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03t52dk35",
                            "GRID": "grid.1029.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "cultural studies",
            "historical studies",
            "interdisciplinary collaboration",
            "digitisation - theory and practice",
            "agent modeling and simulation",
            "video",
            "social media",
            "audio",
            "anthropology",
            "English",
            "multimedia"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Saikaku Ihara (c. 1642–1693 ) is one of the most famous writers of the Edo period (1603–1868) in Japan. 1 After publishing the maiden works of  Kousyoku ichidai otoko (The Life of an Amorous Man, 1682), he became the leading author of  Ukiyozoushi, 2 which was a realistic literature from the Edo period. Saikaku’s works are known for their significance in developing Japanese novels today (Emoto and Taniwaki, 1996).   It is said that he wrote 24 works in 10 years. However, with the exception of  Kousyoku ichidai otoko, those achievements have not been fully verified, due to some doubts in their authorship. For instance, Saikaku only wrote  Kousyoku ichidai otoko, while the other works were written by either Saikaku’s student Dansui Houjyou (1663–1711) or a collaboration of Dansui and Saikaku (Mori, 1955).  Saikaku researchers have tried to identify his works by investigating their history, content, format, and so on. However, it remains unclear which works are really written by Saikaku. Accordingly, we decided to use a quantitative approach to inspect Saikaku’s authorship problems, because the potential of quantitative analysis of textual data has dramatically advanced. That method can provide new knowledge about the authorship problem of Saikaku’s works. Moreover, this research will be a good example in using a quantitative approach for the Japanese classical literature research domain because the quantitative approach is not common in that domain.  Purpose of This Study In this paper, we focus on Saikaku’s posthumous works because many of Saikaku’s researchers have raised questions about their authorship. Saikaku’s posthumous works were edited and published from 1693 to 1699 by his student Dansui (Table 1). Therefore, there are claims that Dansui may have modified Saikaku’s work.  We have compared Saikaku’s posthumous works and other Saikaku works for differences (Uesaka and Murakami, 2013; 2014). If we try to resolve the authorship problems of Saikaku, Dansui is the most suspect writer of Saikaku’s work; therefore, Dansui’s text should be analyzed also.  Database of Saikaku’s Works  Since Japanese morphological analyzers are not applicable for early modern Japanese texts (Ogiso et al., 2013), we developed a database of Saikaku’s works with his researchers, who are editors of  Shinpen Saikaku Zenshu (Shinpen Saikaku Zenshu Henshu Inkai, 2000). Figures 1 and 2 show a page from the book. Moreover, we used Dansui’s database for an analysis, which was developed by Banno and Mizutani, who are Saikaku and Dansui researchers, based on  Shinpen Saikaku Zenshu. In this research, we use  Shikidou otsuzumi (1687),  Chuya youjin ki (1707), and  Budou hariai okagami (1709) , because these works’ digital text and database were finished developing.         Figure 1. Saikaku’s publication.       Table 1. Saikaku’s posthumous works.       Figure 2. Modern form of Japanese language. Table 2 shows a list of works in our database and the number of words in each work. According to our database, there are 583,934 words contained in 24 of Saikaku’s works and 55,504 words contained in three of Dansui’s works.       Table 2. Work name and the number of words. Table 3 is a part of the database from Saikaku’s works used for this analysis. Since Japanese sentences are not separated by spaces, we added spaces between the words in all of the sentences. In addition, information was added for the analysis.        Table 3. Database of Saikaku’s works.  Analysis and Results  We compared Saikaku’s works ( Kousyoku ichidai otoko, which has been verified to be a work of Saikaku, and five posthumous works) to Dansui’s three works ( Shikidou otsuzumi,  Chuya youjin ki, and  Budou hariai okagami) using principal component analysis (PCA). PCA reduces the dimensionality of a dataset consisting of a large number of interrelated variables, while retaining as much as possible of the variation present in the dataset (Jolliffe, 2002). When applied to the frequencies of high-frequency items in texts, PCA often successfully reveals the authorial structure in a dataset (Kestemont et al., 2013).  At first, we examined the appearance rate of the seven principal grammatical categories: nouns, particles, verbs, auxiliary verbs, adjectives, adverbs, and adnominal adjectives. Grammatical categories were the basic information for authorship attribution. The study of  Tale of Genji (Murakami, 2002) and  Sandai hiho bonsyoji (Itou and Murakami, 1992) adopted the stylometry research using grammatical categories, and these help to identify the author.   Figure 3 shows the results of the analysis on grammatical categories, using the PCA with a correlation matrix. The horizontal axis shows the importance of the first principal component, and the vertical axis shows the second. The proportion of variance of the first principal component is 0.4031, while it is 0.29088 for the second; the cumulative proportion up to the second principal component is 0.69398. In this figure, indicating differences are revealed by PCA, Saikaku’s works are on the lower left and Dansui’s works on the upper middle.",
        "article_title": "A Quantitative Analysis for the Authorship of Saikaku’s Posthumous Works Compared with Dansui’s works",
        "authors": [
            {
                "given": "Ayaka",
                "family": "Uesaka",
                "affiliation": [
                    {
                        "original_name": "Doshisha University, Japan",
                        "normalized_name": "Doshisha University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/01fxdkm29",
                            "GRID": "grid.255178.c"
                        }
                    }
                ]
            },
            {
                "given": "Masakatsu",
                "family": "Murakami",
                "affiliation": [
                    {
                        "original_name": "Doshisha University, Japan",
                        "normalized_name": "Doshisha University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/01fxdkm29",
                            "GRID": "grid.255178.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "authorship attribution / authority",
            "English",
            "data mining / text mining"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The focus for this project is the life courses of migrants. It is acknowledged that the life courses of all migrants have a rupture at the point of migration. As a result, the lives of migrants differ structurally from the lives of most other people, as their life courses are divided in a period before and a period after migration. Our central research question is about the questions of continuities and discontinuities in life, habits, and lifestyles. As people migrated from the Netherlands to Australia in the post–World War II period, we consider the influences of both the sending and receiving states, the emigrants’ institutions of both countries, their own communities, and the churches on their enrollment for migration. It is a largely unexplored question whether and up to what extent these influences were continued or taken over by new agents after their migration and contributed to the way their lives were shaped in their new country (Green, 2005, 263–65; Green and Weil, 2007, ix; Schrover and Van Faassen, 2010, 4; see also Elich, 1987).  The backbone for the research program is the development of a database that seeks to capture and aggregate data on the life courses of the migrants. The Dutch National Archives hold an archive with some 50,000 emigrant cards, corresponding to ca. 180,000 persons (as the cards register an emigrant unit, usually consisting of a family) or about 90 percent of all emigrants to Australia from 1945 to the mid-1980s (http://www.gahetna.nl). The Australian National Archives also have a collection of files about the same people, who for Australia were immigrants. These cards and files contain the core facts of the migrants’ life courses. Both collections are made accessible by databases. Explorative research has made it clear that it is possible to link the Dutch and the Australian records (http://resources.huygens.knaw.nl/emigratie). The database in development combines records from the Netherlands and Australia and in this way demonstrates a resource with data relating to the life courses of virtually all Dutch-Australian emigrants from 1945 to 1980. This makes it possible to link and understand aspects of migration that could previously only be studied separately. The actual parent project, Migrant, Mobilities and Connection, started in 2014 and is still in an exploratory phase. Results are preliminary.  Life courses are not a new field of study, having long drawn the attention of historians and social scientists alike (Maas et al., 2008, 7–8). However, studying lives has tended to either focus on the micro level or on the macro level. The micro historical approach is mostly qualitative as it works out case studies for individuals; the macro historical method is serial in nature and distinguishes patterns in the lives of groups of people. An important example is the Historical Sample of the Netherlands (HSN) that contains a representative sample of all persons born in The Netherlands between 1812 and 1922, corresponding with ca. 77,000 people (http://www.iisg.nl/hsn). However, the HSN only contains information about the facets of life of the people who are contained in its database. The Migrant, Mobilities and Connection database of life courses (the ‘backbone’ of both projects) is suitable for similar research as the HSN, but that is only the starting point. As it has links to the actual Dutch and Australian files, it is possible to study groups of migrants in much more depth than only the records in a database would allow. The organization of the research around a new, comprehensive dataset (that will be enriched by digitized policy and case files in anonymized form) and using the computer-assisted heuristics of the digital humanities make it possible to connect the micro and the macro approaches into what we would call  serial qualitative research. This method makes it possible to find patterns while keeping access to the details, to make representative selections for case studies, and to generalize and quickly test representative coverage of the findings from in-depth case studies.  For all conceivable groups and selections it is possible to investigate the life courses of each member in detail. In this way it becomes possible to identify social networks of and around the migrants and follow their evolution as they migrated from a Dutch setting to the environment of their new homes in Australia. As the social networks evolved, it is possible to identify the different influences on their development. In this way we can combine pattern recognition of diverging lives and Dutch community formation and forms of assimilation in Australia with the pressures from the social context (Tilly, 2011; Brettell and Hollifield, 2014, 14–19; White and Houseman, 2002). We can research which institutions were involved and what their influences were on the lives of the emigrants both during their time in the Netherlands and after their emigration to Australia. The institutions include the Dutch and Australian governments, both consular services, but also the church, employers, and trade unions (Van Faassen, 2014, 79–121, 162–68). Another question is how did the cultural background of the people, such as the places of provenance and their religion, contribute to the lives they led in their new country? This is complemented by the medical history and group differences that are made possible by medical records archives kept at the Australian National Archives (privacy regulations permitting). The emigrant files also contain links to heritage institutions containing cultural heritage. The additional materials enrich the life courses and extend the fields of analysis. For example, the same people who are in life courses are also on the member lists of Dutch immigrant associations. Previously it was only possible to study the cultural lives in the Dutch Australian immigrant societies as a whole, but the life courses backbone links them to the people involved and their contributions. It is our objective to complement the research project by a cultural heritage project that uses the same life courses backbone as its structuring device for building a site for the Dutch-Australian community. Its members will be invited to identify their families in the life course and to enrich its contents by contributing all sorts of personal materials like photos, pictures, diaries, letters, and memorabilia. It is the intention that the site contributes to the community formation as its members may exchange experiences and come into contact with one another and with their Dutch past (Official Report, 2012). The community site is an objective per se, but can also contribute to the research project, as the materials of the community members may be used for further research into the Dutch-Australian experience and the memories of and the (changing) image of their former homeland.  Last but not least: reconstructing such a backbone dataset of life courses of migrating people and embedding it in both their communities could serve as a template for other countries as well. In order to be able to test this template statement, our research project has a comparative perspective, building on the work already done on German migrants by the Australian digital historian Dr Kristy Kokegei (Kokegei, 2012). Theme: ‘Migrating People, Migrating Data’ This short paper is part of two interlinked short papers that discuss the archival, custodial, and digital challenges that impact the discovery, collection, preservation, and content management of material and immaterial traces from the past that the Netherlands shares with Australia. (The other short paper is ‘Developing a Sustainable Model in Mutual Cultural Digital Heritage’ by Nonja Peters.) In partnership with key institutional and community stakeholders, our pilot study on mobility between these two countries is about developing new understandings of the experience and representation of migration and how this has shaped an evolving sense of Dutch-Australian heritage and, with it, the consequences for the formation of cultural identities.  The deliberate play in the short papers linking theme of ‘Migrating People, Migrating Data’ is to signal the thematic content of the parent project, Migrant, Mobilities and Connection—that is, the socio-cultural material traces that append to the historical activity of people moving from one region to settle in another, in which the movement of bodies through space combines with information about their mobility through time. At the same time, it is to also signal the technical and conceptual challenges surrounding the consolidation of different data sources (both hard copy and digital) from a prior generation of technology to successive generations. For example, many Dutch community groups in both countries are actively collecting documents, artefacts, photographs, and maps to pass on to future generations. However, few have developed sustainable workflows to ensure the sustainability of their ‘collections’, and rarely are they familiar with cataloguing and metadata conventions that help describe an item’s provenance, role, and position in the world. Planning for digital preservation therefore is uneven, leading to concerns about a ‘digital gap’ in a community’s history. Mitigating the deleterious effects, then, of information loss and fading human recollection is an issue central to both the continued accessibility of cultural heritage materials and the digital preservation of historical knowledge beyond technology format lifetimes.  Project Members Dr Nonja Peters, Curtin University and University of Western Sydney Professor Paul Arthur, University of Western Sydney Dr Marijke van Faassen, Huygens ING Dr Rik Hoekstra, Huygens ING Dr Jason Ensor, University of Western Sydney ",
        "article_title": "Ruptured Life Courses: Institutional and Cultural Influences in Transnational Contexts",
        "authors": [
            {
                "given": "Marijke",
                "family": "van Faassen",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Rik",
                "family": "Hoekstra",
                "affiliation": [
                    {
                        "original_name": "Huygens ING, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "cultural studies",
            "historical studies",
            "repositories",
            "libraries",
            "archives",
            "museums",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "resource creation",
            "cultural infrastructure",
            "and discovery"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Writing is often inspired by external source texts. Including an author’s personal library is a crucial aspect of mapping creative invention at work, which is the aim of genetic criticism. In the field of genetic criticism (the study of modern manuscripts), Raymonde Debray Genette made a useful distinction between endogenesis (the part of a composition process that involves the writing of draft versions) and exogenesis (the part of the genesis that relates to external source texts—for instance, when an author consults an encyclopedia or makes notes on a book s/he is reading). Most digital genetic editions focus on the endogenesis, but it is also possible to incorporate the exogenesis. In the context of digital scholarly editing, the combination of these two aspects of a work’s genesis involves a form of digital editing that takes into account at least three approaches: (1) a documentary, (2) a textual, and (3) an intertextual approach.  Recent developments in genetic editing (notably including the work of the TEI) have increasingly drawn due attention to the importance of the ‘document’, rather than the ‘text’, as the core of the edition. In the meantime, the textual and intertextual aspects should not be neglected, though, especially if one considers the collation of versions as a central element in any scholarly edition.  The specific challenge faced by scholarly editors collating modern manuscripts is the complexity of the documents. Autograph manuscripts typically contain numerous cancellations, substitutions, and additions, which complicate the comparison. To examine the possibility of digital collation of modern manuscripts, this paper studies the case of the Beckett Digital Manuscript Project (www.beckettarchive.org), a project that is made possible thanks to an ERC Grant (‘CUTS: Creative Undoing and Textual Scholarship’) and whose digital infrastructure is developed as part of the Marie Curie ITN ‘DiXiT’ (Digital Scholarly Editions).  Documentary Approach  Modern manuscripts (notes, sketches, drafts) are not ‘texts’ but ‘protocols for making a text’, as Daniel Ferrer describes them in  Logiques du brouillon: Modèles pour une critique génétique (2011). This has consequences for scholarly editing, as Hans Walter Gabler notes in ‘The Primacy of the Document in Editing’, because ‘it is documents that we have, and documents only. In all transmission and all editing, texts are (and, if properly recognised, always have been) constructs from documents. For to edit texts critically means precisely this: to construct them’ (2007, 199). As a consequence, the inclusion of digital facsimiles has almost become a sine qua non in digital scholarly editing. This trend has been a considerable help in making modern manuscripts more accessible, especially when the facsimiles are accompanied by a transcription. But if this transcription is conceived in terms of a document-oriented approach, it often treats every word as a separate ‘island’ (linked to a particular set of coordinates on the facsimile), not as part of a syntactic entity. This is where a textual approach becomes useful.   Textual Approach  In addition to the documentary approach, a textual approach can make modern manuscripts more accessible by offering a plausible interpretation of the ‘protocol’ (to employ Ferrer’s term). If a word is crossed out and an alternative is written above the line, the chronology of these writing acts can—in many cases—be reconstructed, and this editorial interpretation can be usefully employed to feed into the alignment table of the collation software. In collaboration with the Huygens ING Institute (The Hague), we integrated the automated collation software tool CollateX in the Beckett Digital Manuscript Project (BDMP) and developed a model to incorporate all of these writing acts into the collation (Haentjens Dekker et al., 2014), to be included in a next version of CollateX.  In the meantime, a new bout of intense collaboration between the lead developers of the BDMP and CollateX (using the existing version 1.5 of CollateX) has resulted in a working alternative to this theoretical model. Deleted and added words are fed into the algorithm with an added property declaring them as such, collated against the text of other witnesses and visualized in the resulting alignment table in the same way as they are visualized in our transcriptions. Complex substitutions can push the software to its limits, and to that end corrective mechanisms were developed to guide the automatic collation to produce improved results.   Intertextual Approach In addition to the documentary and textual approaches, a digital genetic edition may also be expected to map the relations between endo- and exogenesis. To that end, it is possible to integrate a writer’s personal library. But in many cases, only a fraction of what an author has ever read is still extant. In addition to the ‘extant’ library (based on digital facsimiles of the marginalia in the books preserved in Beckett’s apartment in Paris), the BDMP therefore tries to reconstruct and integrate a ‘virtual’ library (based on Beckett’s reading notes in notebooks). Exogenesis defines the contours and the specific form of intertextuality that will be discussed in the paper.  * * * These three approaches enable users to compare (1) the digital facsimiles of modern manuscripts with the transcriptions, (2) the multiple versions of the drafts (by means of a digital collation tool to mark the variants), and (3) the author’s manuscripts with the source texts to which they refer or allude. Working with these three dimensions, the paper proposes a model for digital scholarly editions that map the interaction between endo- and exogenesis, formulating a set of criteria for the inclusion of various types of source texts to address the issue of a virtual library’s boundaries as one of the main challenges of exogenesis in digital editions.  ",
        "article_title": "Genetic Criticism and Digital Editing",
        "authors": [
            {
                "given": "Dirk",
                "family": "Van Hulle",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Vincent",
                "family": "Neyt",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp, Belgium",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "English",
            "scholarly editing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We argue that the humanities and digital humanities need to consider certain forms of code as scholarly object and certain types of code authorship as a scholarly activity. It will be essential, therefore, to develop a scholarly mode of evaluating and criticizing these scholarly contributions in code. We argue this position from a theoretical perspective, and we present a case study to consider the boundaries of scholarship in code and coding, and how this scholarship in code may be evaluated. Software, code, and algorithms are becoming an inherent part of the humanities, and not just because they are tools within digital humanities. Rather, code and software have resulted in the softwarization of society as a whole (Berry, 2014). The process of the digital becoming ubiquitous—of code and algorithms permeating culture and society in a radical manner—is not a new phenomenon (cf., e.g., Coyne, 1995; Capurro, 2010). However, in the last decade the emergence of digital technology in every aspect of culture and society, and in every associated workflow, seems to have accelerated vastly with the advent of mobile computing and social media—so much so that Steve E. Jones (2014) in his recent history of the digital humanities compares this development with the ‘eversion’ envisioned in the 2007 novel  Spook Country by William Gibson: a turning inside out of digital technology intertwining with the physical world and real life to the point of complete and indistinguishable mergence. Inter alia, David M. Berry (2014) concludes that society as a whole has therefore moved into a post-digital era. For the unsuspecting or impartial user, this means that a perfectly transparent layer of omnipresent digital technology sits between digital resources, data, and information and the uses or experiences generated from these data and resources. The invisibility or—if one prefers—covertness of this transparent layer creates an illusion of neutrality of this very layer, arguably reinforced by code and software being grounded in logic and mathematics that lend digital technology an aura of impartiality and not seldom even infallibility. However, code and software are non-neutral and fallible (cf., e.g., Coyne [1995] and Berry [2014]). Being manipulable, being engineered, and being authored by people, code has meaning, intent, assertions of truth, and performative aspects of and by itself.  Given the radical ubiquitousness of a digitality that is seemingly transparent but potentially non-neutral, transformative in virtually every aspect of culture and society as well as in scholarship, one would expect the humanities on principle to be heavily involved in research towards the properties and effects of the innards of this digitality. In other words, one would expect the humanities to be interested in the scholarly methodological merits and effects of code and code engineering. Until now, however, code and code engineering have hardly been researched as to their scholarly significance and impact. Code as scholarly object and code authoring as scholarship have been less a point of attention of the humanities than the application of resulting software. This focusing away from code is also rather mainstream within the digital humanities. The debate on code literacy—does one need to be able to code to call oneself a digital humanist?—is undecided but leaning mostly towards asserting that code skills are not a necessity for digital humanists (Ramsay, 2011). The propensity to foreground the results of applying digital or computational analysis and to discuss especially those results on a scholarly level, rather than making algorithms or code the object of scholarly discussion, is also apparent in the writings of practitioners of code in the humanities: mostly what has been done with software and what the results might mean are highlighted, far more than the problematizing of the very digitality of method (cf., e.g., Ramsay and Rockwell, 2012). Because of the argued increase in prominence of digital technology, however, neither the humanities nor the digital humanities can forgo finding a genuine scholarly mode to disclose code and coding objects, and to develop scholarly critical mechanisms for such objects. Thus, because of the ubiquitous role of digital technology both in society and scholarship, we argue that humanities cannot evade taking code and code authorship into its domain as objects of study and criticism. Often in answer to this problematic issue, one hears a hand-waving argument made that one does not need to know how a combustion engine works to drive a car. We argue that this is a false and, worse, dangerous metaphor. Ian Hacking (1981) uses the same metaphor when arguing that one needs no deep theoretical understanding of optics and the physical laws that predict the refraction of light or the behaving of electrons when operating an optic or electron microscope. In the case of microscopy this is valid, as these laws of physics have been widely theorized and heavily empirically supported. Given our scientific consensus expectation that our theory of optics will thus hold, the technology of a microscope can be blackboxed in a true Latourian sense (Latour, 1988). Software code, however, is not governed by laws of physics. Although code has to answer to certain logic to be executable, it need itself not even be logical or mathematically precise in effect, reasoning, or operation. Certainly, as long as code applications in the (digital) humanities remain a ‘parade of prototypes’ (Wouters and Beaulieu, 2007), code engineering will hardly be ruled by generic laws. Even if the mathematics of underlying analytic software may be profoundly formally tested, one is hard-pressed to find software applications in the humanities that are accompanied with formal test suites guaranteeing the correct integration of these mathematical components. Especially the heuristics of purpose-built software should be subject to rigorous automated testing, and to scholarly scrutiny and review. We argue that to improve the situation it is essential to develop a techno-scholarly mode of evaluating and criticizing scholarly contributions in code. Some work has been done in this area, most notably by Montfort et al. (2012) and, e.g., Mark Marino (2006). The literature so far, however, considers code as object external to the humanities. But how do we respond to Jean Bauer’s (2011) echoing of the call that ‘the database is the theory!’? Confirming code as a scholarly object, evaluating its functions, and acknowledging its production as scholarly activity require a theory and critical framework that have yet to be formulated.  Such a framework should be built on the basis of practical approaches to the criticism and the evaluation of code as scholarly object. The remainder of our paper will therefore underpin our theoretical exposé with a concrete case study of the development and the code base of  CollateX.  CollateX is a successful text collation engine developed in a digital humanities context (Haentjens Dekker et al., 2014). The development, algorithm, and logic of  CollateX marries code engineering skills with mathematical methods, computational algorithm design, and decidedly scholarly heuristics. We demonstrate how certain scholarly decisions of collation can be expressed as mathematical problems and logically programmable code, but that in other cases it is essential to hand off decisions to scholarly interpretation. Determining the precise cutoff point between the application of mathematics or logic and the application of scholarly expertise is, as we demonstrate, a skill that is informed by both high-level computational as well as high-level scholarly expertise. We thus also demonstrate from practice that code has intrinsic scholarly purport and intent. Lastly we consider how this purport can be systematically scrutinized by applying both automated testing and a scholarly critical approach.   We then return to the overarching concern of evaluating the scholarship in code, and we argue that ignoring the scholarly nature of code renders humanities willfully illiterate of a technology that is re-inscribing scholarly content and practice. ",
        "article_title": "When Is Coding Scholarship And When Is It Not?",
        "authors": [
            {
                "given": "Joris Job",
                "family": "Van Zundert",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands - Royal Dutch Academy of Arts and Sciences, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Ronald",
                "family": "Haentjens-Dekker",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands - Royal Dutch Academy of Arts and Sciences, Netherlands, The",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "programming",
            "digital humanities - nature and significance",
            "English",
            "software design and development"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Recent computing developments have created several visualization techniques and tools for building interactive visualizations that can be used to explore data in new ways. These techniques and tools play a central role in many novel digital humanities applications, ranging from social network visualizations to 3D models of historic buildings.  In this paper, a wide range of visualizations that have been presented at the digital humanities conference 2014 (DH2014) is analyzed. The research contribution of this paper is twofold. First, an analysis of prominent research in this domain is presented, focusing on the following research questions:   • RQ1: Which visualization techniques are used by current prototypes and systems?   • RQ2: Which interaction techniques are supported by current prototypes and systems?   • RQ3: How are visual approaches to digital humanities research evaluated?  Second, based on an analysis of existing applications that were presented at DH2014, opportunities for future research in this area are outlined.   Analysis of Visual Approaches in the Digital Humanities    RQ1: Which visualization techniques are used by current prototypes and systems? Out of all DH14 contributions, 58 presentations report on significant work on the use of visualizations. Fifteen techniques for different data types are deployed 74 times (see Figure 1):    •  1-dimensional: Word clouds (four occurrences) and histograms (three occurrences) are used to represent 1-dimensional data. Word clouds and histograms are, for instance, used to understand the main themes.    •  2-dimensional: Scatterplots (one occurrence) and timelines (four occurrences) are typical examples of data that are represented on two dimensions.    •  Multi-dimensional: 3D models are used by 12 out of 58 applications. In most cases, these 3D models render historic buildings to support virtual exploration. Geographic maps are also commonly used (13 occurrences). In all cases, a third variable is represented in addition to longitude and latitude data to support data analysis. The size of a dot on a map and different colors are, for instance, used to represent the number of occurrences of entity types in their geographic context. Parallel coordinates and infographics have the potential to represent more than three dimensions—but are not used often. An example of the use of parallel coordinates is represented in Figure 3 and enables exploration of correlations or trade-offs between more than two variables.    •  Relations: Most common are uni- or bi-directional relations represented by graphs. Graphs are present in 23 out of 58 prototypes. Other relations are less common and include hierarchical relations represented by trees, relations between sets represented by a cluster map, Sankey diagrams, matrices, and heatmaps.    •  Animations: Three out of 58 applications employ animation techniques to enhance understanding of complex data.          Figure 1. Visualization techniques used by work presented at DH14.  RQ2: Which interaction types are supported by current prototypes and systems?  For analyzing the level of interactivity, Yi et al.’s classification (2007) of interaction types was used. Out of the 58 presentations, 35 visualizations are interactive. Twenty-three presentations represent a static visualization, or a visualization at a conceptual level without specifying the level of interactivity. Of interest for this analysis are the 35 visualization prototypes that implement interaction. Figure 2 presents the interaction types that are supported:   • The most common interaction type is  abstract/elaborate that enables users to show more or less detail in a visualization. An example is a drill-down operation in a tree representation that allows the user to examine a particular sub-tree. A zooming operation to see part of a geographic map in more detail is another interaction technique that is supported by many prototypes. Another common example is a tool-tip interaction technique that provides detailed information when a mouse cursor hovers over a data item. These interactions have been defined as  details-on-demand operations. Abstract/elaborate interactions are supported by 35 out of 58 prototypes that implement visualizations.    • A second common interaction type is an  explore operation. Such operations enable the user to see a different subset of the data in the visual representation. A typical explore operation is a panning action that enables the users to see a different part of a graph representation or a different part of a geographic map. Thirty-four out of the 35 interactive prototypes support this type of interaction.    •  Filter interaction techniques enable the user to change the set of data items that is presented based on specific conditions. Data items not satisfying the condition are hidden or shown differently. Sliders to specify a range of dates and check boxes to filter categorical data are typical examples. Some visualizations use input boxes to filter data elements such as names through keyboard interaction. Fifteen out of 35 interactive prototypes provide filter operations. Such interactions are key when working with large datasets and enable users to refine research questions on the fly.    • One prototype uses an  encode interaction technique to enable users to show a different representation. Figure 3 presents an example of a visualization that enables users to select a different representation technique (table, graph, etc.) to analyze data.    • To the best of our knowledge,  reconfigure interactions that enable users to see a different arrangement, such as different variables on the X- and Y-axis of a scatterplot,  connect interactions, and  select/focus interactions to mark information are not supported.         Figure 2. Interactions supported by DH14 applications.  RQ3: How are visual approaches to digital humanities research evaluated?  Most contributions presented at DH14 present proof-of-concept prototypes that implement research ideas in prototypes or full systems. These prototypes or systems are then used by the authors to answer specific research questions. Only four out of 58 applications have been evaluated in case studies to assess their value for other researchers. In these case studies, applications have been made available to target users who have used the applications to conduct a research task. Such case studies provide valuable insight into the usefulness and usability of different visualization and interaction techniques.        Figure 3. Visualization to interpret biographies (Booth and Martin, 2014) Conclusion  In this paper, visualization and interaction techniques in visual applications presented at DH14 have been analyzed. From this analysis, a few take-away messages and opportunities for future research can be highlighted:   1.  Visualization techniques (RQ1). DH applications employ a wide variety of visualization techniques. An earlier study by Montague et al. (2014) identified  trees, histograms, graphs, word clouds, and  scatter plots as key visualization techniques to support text mining analysis. Results of our study identify a broader range of visualization techniques that have been applied successfully in digital humanities prototypes, beyond the scope of text analysis. Our analysis indicates that  graphs,  geographic maps, and  3D models are the most prominent techniques. Trees, histograms, word clouds, and scatter plots are also employed, although less often. In general, our analysis identifies a wide variety of techniques that have been used successfully, including advanced multidimensional techniques such as parallel coordinates.    2.  Interaction techniques (RQ2). A subset of 35 out of 58 applications employs interaction techniques to enable users to drive the analysis. Most common are  abstract/elaborate techniques that enable users to retrieve details on demand.  Explore interactions are also common in interactive prototypes.  Filter operations are key when dealing with large data collections and enable retrieval of a subset of the data that is relevant. Such interactions are supported by 15 prototypes. Other interactions that enable users to choose different data types ( reconfigure) or different representation methods ( encode) are not used often: such techniques are more advanced, and can play a role in interactive interfaces that support data analysis from different perspectives. The use of such techniques to steer visual data analysis processes constitutes a further line of research.    3  Evaluation (RQ3). Our analysis indicates that case studies that evaluate the usefulness and usability of visualization techniques to support digital humanities are still rare. Four out of 58 prototypes have been evaluated in significant case studies. Whereas such case studies have been identified as key to gain insight into actual use of visualization techniques, only a limited number of research contributions report on insights derived from such studies. Results of our own work indicate that case studies are key to identifying suitable visualization and interaction techniques: a cluster map technique to represent relations turned out to be complex for novice users, whereas a traditional Venn diagram was more successful in a case study that we conducted with researchers. Similar experiments are needed to gain insight into the merits and drawbacks of representation and interaction techniques, and how they can be successfully deployed in useful applications.   ",
        "article_title": "On the Use of Visualization for the Digital Humanities",
        "authors": [
            {
                "given": "Katrien",
                "family": "Verbert",
                "affiliation": [
                    {
                        "original_name": "KU Leuven, Belgium",
                        "normalized_name": "KU Leuven",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/05f950310",
                            "GRID": "grid.5596.f"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "English",
            "spatio-temporal modeling",
            "analysis and visualisation"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Over the last three years, Stanford University’s Center for Spatial and Textual Analysis (CESTA) has jointly led a research project in partnership with Historypin to examine the potential for leveraging crowdsourced information about photographic, map, and textual content for humanities research. The project was funded by the Andrew W. Mellon Foundation and sought to study elements of user interface, user interaction, community engagement, and collaborative partnerships. Our work mirrors the findings of others utilizing crowdsourcing in cultural heritage, in that there is an important distinction within the realm of crowdsourcing between more straightforward knowledge gathering and more complex and collaborative tasks and processes that revolve around what we termed ‘knowledge communities’: small networks of neighbors or enthusiasts representing a group of people that could be systematically organized to share and participate in research for a common aim. Engaging with these communities often requires longer time frames than simpler task-driven crowdsourcing may allow, and is necessarily much more collaborative than extractive. Furthermore, there are important social benefits for the institutions and communities alike that fulfill the new missions of 21st-century cultural heritage and academic institutions. Throughout our research, we found that not only will there be a much more collaborative future among academic humanists, memory institutions, and knowledge communities, but that only through such collaborations can some questions relative to the humanities be explored. In this presentation, we summarize the methodology of the project and present key learnings and examples that we hope will assist academic and cultural heritage institutions that are interested in engaging with knowledge communities. In addition, we will provide a way to practically approach such projects with realistic expectations and ideas for successful implementation. Methodology In this section we describe in some detail the methodology used in each of the three projects and the different approaches and communities we were working with. We had three sub-projects: the Year of the Bay, Living with the Railroads, and the Emotions of London. Together, these projects allow us to offer a much broader range of findings and practices than we could have with a single one.  Crowdsourcing and Knowledge Communities  We’ve seen that digital-transcription projects on the humanities side, or ‘citizen science’ on the scientific side, tend to blend a combination of ‘crowd’ and ‘community’ (Clauser and Wallace, 2012). In the former, loosely associated individuals perform autonomous and relatively simple tasks, while the latter often requires collaboration and a more social and coordinated element (Haythornthwaite, 2009). In Living with the Railroads and Year of the Bay, we were not dealing with questions of transcription or necessarily easily defined tasks, but rather more complex questions of contextualization and even contribution with a specific goal in mind, still meeting definitions of crowdsourcing but leaning toward more complex tasks requiring an engaged community (Dunn and Hedges, 2012). In both projects, there was a collaborative approach between the participating institutions and the individual participants toward a common aim, however broadly defined—be it exploring the human and ecological history of the bay or tracing western expansion through life around the railroads. This also speaks to the potential for strengthening and measuring community ties through this work, which Nick Poole and Nick Stanhope describe as a new approach not only to digitization, ‘but of a new role for museums as places of engagement and participation in which the disciplines or curatorship, digitisation, collections management and documentation are shared with the user in a joint effort to develop shared cultural capital’ (Poole and Stanhope, 2012).  As these projects progressed, it was helpful for us to understand and define the various communities we were increasingly in partnership with, not as users or as a crowd, but as ‘knowledge communities’. Whether they were local residents or history enthusiasts who were knowledgeable about their area, or in some cases obsessed with solving a history mystery that was presented to them, or who had specific topical knowledge about details pertaining to one railroad line, these small networks of neighbors or enthusiasts represented a group of people who could be systematically organized to share and participate in research for a common aim. After two years of working with specific individuals and groups of individuals, we came to see this as an important distinction within the realm of crowdsourcing, and an approach that is most importantly collaborative in intent and design rather than extractive and focused on the needs of an institution or a researcher. This approach has also been termed community-sourcing (Sample Ward, 2011). Literature on crowdsourcing generally supports the process (which remains frustratingly variably defined) on the basis of whether some sort of utilitarian goal is met: entries are added (Wikipedia), materials are transcribed (Transcribe Bentham), data is classified (Galaxy Zoo). In this, understandings of crowdsourcing, both definitional and in terms of results, remain wedded to a market model. Can you get data or analysis? Can you get it more quickly or more cheaply? These are the primary questions. The question that motivated our project—whether crowdsourcing can be useful to humanities researchers pursuing humanities projects—found a very different answer, though utilitarian goals remain. Our three sub-projects engaged three different types of crowds: the amorphous public of people interested in the Bay Area, the expert community of train enthusiasts, and the paid community of Amazon Mechanical Turkers. All three were, unsurprisingly, only partially successful in terms of the research questions initially posed. There is usable and interesting data (images and metadata uploaded to Historypin for the first two, analysis of literary passages as to emotionality in the latter). But it isn’t perfect, and in some cases doesn’t quite meet the goals of the various PIs. But the conclusions we have drawn thus far ask us—and, by extension, other researchers—to look beyond the data to the process itself. There is increasing segmentation of crowdsourcing approaches in cultural heritage (Ridge, 2014), with great import for the academy as well as memory institutions. Crowdsourcing offers academic humanists a different way of engaging the public, especially in collaboration with non-academic organizations. We suggest that, if researchers have flexibility in their projects, building or collaborating with knowledge communities can result in original research, and an engagement with the community that is typically missing from university-based research. Without this flexibility, crowdsourcing possibilities become more limited, though still possible, as our work with the Turkers demonstrated. Furthermore, we feel that museums, libraries, archives, and academic institutions alike increasingly embrace the opportunities they have to positively impact communities, not just in terms of cultural programming, but also as partners in strengthening a range of societal measures (Van Thoen, 2014). Meaningful engagement of knowledge communities can have a range of outcomes in addition to the traditional measurements of crowdsourcing for the institutions as well as the communities, such as increasing interest in and sense of ownership of institutions, strengthening community ties and associational life, and decreasing isolation among frequently marginalized sectors of society such as seniors (Thomson and Chatterjee, 2013). Practical Applications This section will be presented in detail, as it’s the heart of the practical takeaways for institutions interested in engaging knowledge communities. These are broken into three sections, including (a) design and expectations; (b) methods of engagement; and (c) skills, training, and technology.  Future Research Here we will look at key areas of future research that we’ll be pursuing to continue our work in this field. Acknowledgements We have so many people to thank for their roles in this project over the last year that it’d be impossible to name them all. The Crowdsourcing for Humanities Research project was funded by a grant from the Andrew W. Mellon Foundation and led by Zephyr Frank at Stanford University, who served as the principal investigator. You can see all of the people who worked on the project on our behind-the-scenes blog,  http://crowdsourcingthehumanities.blogspot.com. We’d also like to thank the growing community of practitioners and scholars sharing their experience in this rapidly evolving field, not all of whom are cited below.  ",
        "article_title": "From Crowdsourcing to Knowledge Communities: Creating Meaningful Scholarship Through Digital Collaboration",
        "authors": [
            {
                "given": "Jon",
                "family": "Voss",
                "affiliation": [
                    {
                        "original_name": "Historypin, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Gabriel",
                "family": "Wolfenstein",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Zephyr",
                "family": "Frank",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Ryan",
                "family": "Heuser",
                "affiliation": [
                    {
                        "original_name": "Stanford University, United States of America",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Kerri",
                "family": "Young",
                "affiliation": [
                    {
                        "original_name": "Historypin, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Nick",
                "family": "Stanhope",
                "affiliation": [
                    {
                        "original_name": "Historypin, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "organization",
            "visualisation",
            "user studies / user needs",
            "content analysis",
            "project design",
            "interface and user experience design",
            "libraries",
            "archives",
            "museums",
            "bibliographic methods / textual studies",
            "English",
            "GLAM: galleries",
            "linking and annotation",
            "crowdsourcing",
            "data mining / text mining",
            "maps and mapping",
            "management"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper will outline the approach taken to encoding historical information in the Dictionary of Sydney project and, in particular, will assess five years of use of one of its core data structures—known to the project as ‘Factoids’. The Dictionary of Sydney (http://dictionaryofsydney.org/) The Dictionary of Sydney project arose from an initiative at the City of Sydney Council in 2004–5. A trust was set up to support and manage the establishment and operation of the Dictionary, and it was subsequently the subject of an Australian Research Council linkage grant from 2005 to 2010 involving the University of Sydney, the City of Sydney, the State Library of NSW, the State Archives of NSW, and the University of Technology Sydney (UTS). The author was project manager of the ARC project from 2006 to 2010 and has worked intermittently with the Dictionary since. The Dictionary was ‘born digital’ and aimed to create a flexible repository of information on the history of the area now encompassed by ‘Greater Sydney’. A website, intended to be only the first of various digital products, went live in 2009. The Dictionary continues to operate with the backing primarily of the City of Sydney Council and some other institutional and philanthropic support.  Text and Data Structures  The use of the term ‘Dictionary’ was debated and somewhat at odds with similar initiatives more often titled ‘Encyclopaedias’ (Te Ara [n.d.]; Encyclopedia of Chicago [2005]). The Dictionary was always intended to include narrative entries of varying length and was by no means expected to be confined to a list of defined terms or to be simply a gazetteer of names and labels (as ‘Dictionary’ might have implied). However, the obvious potential for a strong set of terms and labels to link and aggregate digital resources (text, multimedia, and maps) encouraged an approach that integrated narrative text and structured data. This gave rise initially to two distinct sets of terms encompassing Entities (‘things’) and more abstract Subjects. The emphasis in all these deliberations was on simplicity and ease of use rather than the final word in academic rigour. So, in assessing useful categories of urban Entities, listings on popular directory websites such as Sensis (2014) and even those at the back of street directories were consulted along with more scholarly efforts at categorisation. Hierarchies were kept shallow, with generally no more than three levels. The final list of Entity types was Artefacts, Buildings, Events, Natural Features, Organisations, People, Places, and Structures. Digital resources were identified as Entries, Maps, and Multimedia, and the Dictionary’s set of components was completed with Subjects, Contributors, and Roles. By the time of writing, at least 50,000 of these elements have been created (alongside over a million words in Entries), and several years of experience and practice has been built up.  Linking Entities and Defining Roles—The ‘Factoids’  The Entities provided a solid framework to which could be attached various digital resources. This ensured that the Dictionary was not ‘entry-centric’ and could have multiple Entries attached to a single Entity—important in ensuring that the Dictionary was  not confined to a single interpretative voice but instead did justice to the many opinions and discourses that a complex urban history demands.  There remained requirements that Entities alone did not satisfy:  • Linking entities.  • Consistently recording location spatial and temporal extent.  • Recognising that the ‘role’ of Entities can change (e.g., a building used for different purposes over time).  • Handling different names (Mark Foys aka The Downing Centre, Pyrmont Bridge aka Anzac Bridge).  • Finer-grained categorisation where the eight basic Entity types were not adequate. There was reluctance to define different data structures for different Entities, thereby leading to a complex and inflexible relational ‘database of everything’. It was also recognised that the Dictionary could not (and should not) take over the role of every specialist information source; for example, the Dictionary might record the location, designer, builder, and perhaps function of a bridge and key dates, but it could be left to an engineering heritage resource to record—for the true aficionado or expert—the materials used, the number of rivets, the exact dimensions and load-bearing characteristics, etc. The recognition of the temporal element and the need to record change also significantly influenced the data structure used to handle these requirements. The key element for encoding change was the Role. Different Entities could take on different Roles over time—a Building might be a gaol, then a cultural centre, and then a residential block; a Place might be a quarry and then a park; a Structure might be a railway viaduct and then an elevated park. Roles could be created flexibly as needed (with due attention to duplication and ambiguity). Roles could similarly describe relationships, most obviously between People (spouse, child, mentor) and between People and Organisations (member, CEO). A Person could be assigned a Role to indicate a profession—a doctor—or a position—a doctor  at a hospital. In this sense, some relationships were transitive (requiring a source and target on either side of the Role), others intransitive (only requiring a source). Some relations are clearly directional—e.g., child-of—and some are symmetrical—e.g., spouse-of.  A single data structure called a ‘Factoid’ (an unsatisfactory term and only used internally) was fashioned to handle all these requirements. The structure was as follows:   Field Purpose Example   Source An Entity Francis Greenway   Role Describes the relation or adds information Architect   Target A ‘target’ Entity Hyde Park Barracks   Type The type of Entity (see below) Position   Target-text A text substitute for the target Entity where it is not appropriate to create it as a full record    Start date Date on which the Role commenced 1819   End date Date on which the Role ended 1822   Location A geo-spatial encoding (point, line, or polygon)    The Type field was used to distinguish Factoids created for different purposes and to group them meaningfully. The types were as follows:   Type Purpose   Name  for recording different names that Entities had over time   Sub-type  used to subtype the main Entity types, e.g., Hostel as a sub-type of Building   Milestones  key events for an Entity—birth, death, construction, demolition, opening   Occupation  normally held by People, e.g., architect   Position  an occupation applied to a specific Entity, e.g., architect of Hyde Park Barracks   Relationships  most commonly between People—brother, spouse, mentor, etc.   Property  recording occupation or ownership   Time-Space  a special type of Factoid used only to record ‘existence’ and location, commonly used for Natural Features and Places   Factoids can be seen on most Entity pages on the Dictionary website: Figure 1.  The Entity page for Hyde Park Barracks with four groups of Factoids.  Factoids in Use The Factoid is one of many attempts to encode this sort of historical information (HEML [n.d.]; Vicodi [2002–4]; CIDOC-CRM), and it is reflected in other initiatives such as the Humanities Networked Infrastructure (Huni, 2014). The Factoids have been in use now for over five years. This provides a valuable opportunity to assess their utility in a real-world, growing resource. This paper will report the results of an assessment undertaken with the Dictionary of Sydney staff in the first quarter of 2015, and an analysis of the use of the Factoids in the Dictionary and through a comparison with recent developments in historical encoding. Areas to be assessed will include the following:  • Limits the expressivity of the Factoid structure—At what point does the information being encoded become too complex and need to revert to narrative description?  • Has the original structure of the Factoid been amended or new types added?  • Has custom and practice developed around the use of the Factoid that was not originally anticipated?  • Has the Factoid (as defined in the Dictionary of Sydney) been deployed in any other projects?  • Has the Factoid had advantages in processes not originally envisaged (e.g., fact/error-checking)?  • How has the divide between useability and rigour been navigated—Has the Dictionary’s position as a public (rather than academic) history project influenced this process?  • The Dictionary Factoids are not referenced or attributed—Has this affected their credibility or reliability?  • Who has been able to contribute Factoids effectively, and how has the process been managed and quality-controlled? Have amateurs contributed or only professional staff? (Reference may be made here to emerging models for managing ‘cyber-infrastructure’ such as that proposed by Lynch [2014]).  • What challenges and/or opportunities do the Factoids present for sustainability and preservation of the data they contain?  • Has it been possible to link the Factoids to other data standards?  • ‘Factoid’ was a term of convenience for the Dictionary but it has been used in other areas, such as prosopography—Has its use in other disciplines influenced its use in the Dictionary? While it may not be possible to cover all these questions in depth, an assessment of the Factoids in practice may provide useful insights into the utility of data structures of this sort for digital humanities projects. ",
        "article_title": "Recording Historical Connections In The Dictionary Of Sydney.",
        "authors": [
            {
                "given": "Stewart McAdam",
                "family": "Wallace",
                "affiliation": [
                    {
                        "original_name": "University of Melbourne; University of Sydney",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "historical studies",
            "repositories",
            "metadata",
            "knowledge representation",
            "interface and user experience design",
            "archives",
            "sustainability and preservation",
            "English",
            "internet / world wide web",
            "cultural infrastructure",
            "ontologies",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This talk concerns itself with the binarization of British and German Western Front maps during the Great War and their modelling using Linked Open Data (LOD) ontologies. Through the use of image processing algorithms, the scanned images of British and German trench maps were binarized into vector representations that allow us to compare each actor’s perception of the other as well as infer military intelligence activities by tracking the provenance of toponyms.  * * * As with many armed conflicts, the initial intentions and plans of the belligerents were quickly changed when the outcomes of their actions were unsatisfactory. All sides expected a war of maneuver using the classical arms of previous conflicts: infantry, cavalry, and artillery. Commanders from the previous centuries would have been comfortable with the pre-war planning of both sides. However, terrain and innovations in artillery and small arms rate of accurate fire constrained the parties to a war of attrition where movement was no longer possible. Military thinking moved from planning large strategic maneuvers to limited tactical engagements that required local planning. Having to deal for the first time with large numbers of tactical situations requiring good maps, the different groups of belligerents began looking for mapping solutions. The British Expeditionary Forces fighting on Belgian and French soil benefited from Belgian cartographic knowledge from the onset that was then augmented from other French sources (Jack, 1920). The Germans were faced with the problem of invading a hostile country with uncooperative officials and having to build a mapping system from the ground up using opportunistically captured material (Boelcke, 1920; Hinks, 1919; Great Britain War Office, 1917) as well as previous large-scale maps. This talk will cover the geometry and cartography of the  Battle of Vimy Ridge in 1917 as a case study using both British and German cartographic material.   Merging Geometries The military maps were created from an amalgam of Belgian and French sources augmented by field surveys and aerial photography. Overlaying these maps allows us to determine what each belligerent believed about his opponent at that point in time. In some cases, friendly trench details were omitted from maps to avoid leaking information in case of capture. However, what trenches are shown represent an armies’ ability to keep track of its own operational activities. Several editions of each map exist, and some instances of these dated editions survive and have been scanned in digital form. We binarize trench representations by separating the colour of the trench from the image while using Thiessen polygons. Registration of the digitized map is done using OpenCV’s implementation of  Hough Line Transform driving a web service implementation previously described in Warren and Evans (2014).   Multiple Linked Open Data vocabularies were used to record this information, including GeoSparql (OGC, 2012),  NeoGeo, and W3C geo:Points. By recording the individual points within the polygon in a manner similar to that of Linked Geo Data (Stadler et al., 2012), we are able to annotate individual locations within the polygon. This effectively creates polygons formed by a series of geo:Points that can each record not only the shape of the trenches but also calculated positional errors, linkages to other trenches, and trench width.  Depending on the specific type of trench, its width could be anywhere from two feet, six inches to one foot, six inches, depending on the application, and the depth about five feet, six inches (Great Britain War Office, 1914). The depth information is not recorded in the maps, but the width of the trench is sometimes traced from aerial photography, which allows for the measurement of high-traffic trenches. When the scaled trench trace exceeds the minimum one-foot, six-inch width, the width calculated from the pen trace on the map is used instead. A recent improvement brought about by GeoSPARQL is the differentiation between Feature and Geometry. The Geometry is the materialized physical location of the Feature (thing), and the separation between thing and location allows for greater flexibility, for example, in stating the existence of a place without necessarily defining where it is. It also allows us to link different representations of the same trench across maps by assigning the same trench (Feature) multiple Geometries as represented in multiple source documents at different times. This also allows us to reduce redundant information by merging similar geometries across maps where no change has occurred. Small variations in locations (due to wear and tear as well as minor survey errors) occur, and these are taken into account by the merging process by calculating the theoretical accuracy of both map scales against each other. For large variations of friendly trenches, the change can be assumed to be a change in the layout of the trench, while for large variation in the observed enemy trench, the change is assumed to be done through a better intelligence estimate. Comparing both these sets of trench geometries over time is a means of obtaining a quantitative measure of the quality of both military operation and intelligence activities. Toponyms and Intelligence The toponyms used on the maps varied greatly, and translated Belgian and French town names were generally used when known. Other visually prominent features such as farms, woods, or isolated trees (e.g.,  The Lone Tree) were named on an ad-hoc basis based on local events or the cultural baggage of the unit involved in the survey work (e.g.,  Regina Trench).  A common practice for all sides was to re-use their opponent’s toponyms when these were known for printing on maps issued to frontline officers. For example,  Prinz Arnolf Graben, a trench near modern-day  Beaumont-Hamel, France, is shown on  British maps using the  German Army name and spelling. This was done for both frontline tactical purposes and making the navigation of enemy trenches easier during an attack or raid. The re-use of names also marks a turning point in British and German attitudes to military intelligence. Intelligence is no longer seen as a cloak-and-dagger craft involving shady individuals but as a core component of military operations. At the onset of the war, cavalrymen still carried a sketch board onto which they were supposed to draw enemy dispositions for a commander’s review. By 1916, the information flowed from frontline operations back to the printing sections that created maps on an as-needed basis for local tactical use.  Through the use of algorithmic methods the language of trench names is isolated and their source identified across different maps. Within this specific application, this allows us to cross-reference the same trench across the maps of multiple belligerents while establishing the source, or earliest known use, of the toponyms. Previous recording of the name of things, or gazetteers, tended to record objects as a single name, occasionally with a localization (Chasseaud, 2006). Through the use of the SKOS eXtension for Labels ( SKOS-XL) vocabulary, we improved on this by adding label-specific provenance, which allows the tracking of the original owner of the name. This recording of the nomenclature provenance can be used as a proxy that identifies the intelligence processes of the belligerents.  Conclusion These military maps are an important source of historical locations that have been long forgotten or whose importance has changed over time. Through the binarization of scanned trench maps, the locations of smaller engagements can be extracted and the location of named features can be placed in a modern context. Some of these features, such as the location of former ammunition dumps, remain important to this day. In closing this presentation, the use of the semantic web will be reviewed in managing a historical gazetteer of geometries, features, and names during the Great War on the Western Front. The careful tracking of provenance information will be explained, and the novel use of existing semantic web standards allows for the discovery of both the quality of the cartographic work done by both sides and the cultural influences between belligerents. ",
        "article_title": "Language, Cultural Influences and Intelligence in Historical Gazetteers of the Great War",
        "authors": [
            {
                "given": "Robert",
                "family": "Warren",
                "affiliation": [
                    {
                        "original_name": "Dalhousie University, Canada",
                        "normalized_name": "Dalhousie University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01e6qks80",
                            "GRID": "grid.55602.34"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "repositories",
            "archives",
            "geospatial analysis",
            "encoding - theory and practice",
            "sustainability and preservation",
            "interfaces and technology",
            "English",
            "data mining / text mining",
            "semantic web",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The spaces and places we inhabit and interact with on a daily basis are composed of layers of cultural activity that are, quite literally, built up over time. While museum exhibits, historical and archaeological narratives, and public archaeology programs can communicate this heritage, they do not generally allow for rich, place-based, and individually driven exploration by the public. In addition, public heritage programs rarely explore the binary nature of material culture, the preserved record of human activity, and heritage: the presented information about the heritage in question and the process by which scholarly research has reached those conclusions. In short, the scholarly narrative of material culture, heritage, and archaeology is often hidden from public understanding. Further, traditional public heritage programs often ﬁnd it difﬁcult to support rich and vibrant multivocality.  In recent years, mobile devices as well as the development and maturation of augmented reality (broadly construed) have offered both platforms and models for mobile heritage applications to at least partially address these issues (Casella and Coelho, 2013; Haugstvedt and Krogstie, 2012; Davies et al., 2013). Mobile applications such as the Museum of London’s Streetmuseum Londinium (http://www.museumoﬂondon.org.uk/Resources/app/StreetmuseumLondinium/home.html), Florida Public Archaeology Network’s Destination: Civil War (http://www.ﬂpublicarchaeology.org/civilwar/), Histories of the National Mall (http://mallhistory.org/), and the CHESS Acropolis Museum mobile application (http://www.chessexperience.eu) successfully facilitate public interaction with heritage and archaeology in a place-based context. Unfortunately, most mobile heritage applications do not support multivocality, nor do they expose and explore the scholarly narrative of the process by which cultural, heritage, and archaeological information was uncovered and information was generated.  It is within this context that this paper will introduce and explore mbira (http://mbira.matrix.msu.edu). Developed at Michigan State University’s MATRIX: The Center for Digital Humanities & Social Sciences (http://matrix.msu.edu) in collaboration with the Cultural Heritage Informatics Initiative (http://chi.anthropology.msu.edu), mbira is an open-source platform that is purpose built to address critical shortcomings in many mobile heritage applications.   ‘Space and Landscape as Museum’  Based on the metaphor of ‘space and landscape as museum’, mbira lets users create mobile experiences in which locations and areas are organized into curated exhibits displayed within a rich, interactive map interface. Each exhibit location contains information and rich media (video, audio, and imagery) about that location as well as the narrative about the associated scholarly work (excavation, survey, historical and heritage research, etc.).   Cloud-Based Content Management   mbira projects are created and managed using a cloud-based digital repository platform (discussed below). All content (exhibits, locations, and location content) are added, edited, and updated from within an open-source digital repository platform. When app creators or editors add new project content or edit existing project content, changes dynamically appear in the project’s public native mobile app or mobile website.   A Constellation of Open-Source Tools   mbira leverages a series of open-source software and tools, both existing and purpose built, with which individuals, projects, organizations, and institutions can use to author, deploy, and sustain mobile heritage experiences:   KORA. Developed by MATRIX: The Center for Digital Humanities and Social Sciences, KORA (http://kora.matrix.msu.edu) is a self-installed, open-source digital repository platform. KORA serves as the backbone for the mbira platform—managing all underlying content and data management and delivery. KORA, originally developed with funding from the National Science Foundation Digital Library Initiative, IMLS, and NEH, has several strengths in relation to other digital repository platforms. Designed for small and medium-sized cultural heritage institutions with limited technological resources, KORA facilitates data ingestion, data export, data preservation, and data management. KORA also facilitates easy delivery of complex digital objects (i.e., groups of objects of different media displayed together for a speciﬁc use) through a robust RESTful API.    mbira plugin. The mbira plugin (which is installed on top of the KORA digital repository platform) is the core authoring environment in the platform’s constellation of open-source tools. The mbira plugin facilitates the creation of mobile projects and their associated exhibits, as well as all locations and areas (and their associated content). The mbira plugin allows creators and editors to manage all social aspects of their mobile applications. In addition, the plugin lets users manage all of the device-speciﬁc deployments of their mobile heritage experience (iOS, Android, mobile web).    Mobile templates. mbira includes elegantly designed (and well-documented) native mobile (iOS and Android) and mobile-web stock templates that individuals, projects, or institutions can use as is or modify as they see ﬁt. While the templates are designed primarily for those with minimal programming experience, they can also serve as a project jump-start for more seasoned developers. The templates are built speciﬁcally to dynamically display content (exhibits, places, spaces, explorations) authored in the mbira plugin. As with all of the other components of the mbira platform, the mobile templates are available to users for free under an open-source license.   Data Portability  Project administrators can easily export data from their mbira installation (KORA + mbira plugin) as structured data (XML and JSON) for the purposes of backup, preservation, and migration. Data can also be imported from other CMSs and digital repository platforms into an mbira installation.   Strongly Social and Multivocal  The mbira platform allows heritage professionals, projects, and institutions to create mobile experiences in which users can interact with one another and domain experts around heritage places and spaces. The mbira platform also includes tools to recognize the expertise and experience of community members, thereby adding a critical dimension of multivocality to mobile heritage experiences.  Building Sustainable Mobile Heritage Experiences  Beyond addressing the aforementioned issues with current models of mobile heritage experience design, one of the core goals of the mbira platform is to empower individuals, projects, organizations, and institutions to build more sustainable mobile projects. All of the tools in the platform (as well as associated technical documentation) are speciﬁcally designed to lower the oftentimes signiﬁcant technical barrier associated with building mobile experiences. The result is that individuals, projects, organizations, and institutions without existing technical expertise are not forced to contract with potentially costly third-party vendors to develop their mobile projects. The fact that all of the tools in the mbira platform are free and designed to run on the open web further reduces the cost associated with deploying and maintaining mobile projects. Because all of the tools in the mbira platform are available under an open-source license, those with existing technical experience are able to copy, modify, adapt, and redistribute the source code without any ﬁnancial or signiﬁcant copyright barriers. Finally, mbira’s cloud-based content management model means that individuals, projects, organizations, and institutions can easily add new content, thereby extending the effective life of their mobile heritage projects.  ",
        "article_title": "Mbira: A Platform to Build, Serve, and Sustain Mobile Heritage Experiences",
        "authors": [
            {
                "given": "Ethan",
                "family": "Watrall",
                "affiliation": [
                    {
                        "original_name": "Michigan State University, United States of America",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "historical studies",
            "mobile applications and mobile design",
            "anthropology",
            "archaeology",
            "English",
            "cultural infrastructure",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Methodology and Results Craig’s Zeta (Hoover, 2008; Burrows, 2004) was the primary methodology deployed, as was the case in Hoover’s original study. Our Zeta analysis was conducted using Stylo (Eder et al., 2013), with a text slice length of 2,000, text slice overlap of 1,000, and an occurrence and filter threshold of 2 and 0.1, respectively. Initially, we produced standard most frequent wordlists using Delta, with a list of stopwords applied. Comparing these results, we saw little separation between the genders. Zeta, as a measure designed to detect distinctive words, yielded more fruitful data. When comparing authors from across all periods, our results confirm Hoover’s in that there are some stereotypical words that do emerge. On the female side, most notable are ‘household’, ‘smile’, and ‘mother’, while on the male side, ‘country’ and ‘famous’ stand out. ‘America’ is also present in the distinctive male words. The male words are arguably reflective of a dataset that contains slightly more male American authors than it does female American authors—suggesting that Hoover is also correct to point towards nationality as a further potential classifier—but the difference is not so great that it should yield significant skewing. However, while there is a clear presence of stereotypical markers, there is also some significant crossover between authors. This is less pronounced across different periods, but when author sets are considered as a whole (see Figure 1), the macro-separation is not as pronounced as in Hoover’s findings.    Figure 1. All authors. As suggested by Hoover, we also analyze authors chronologically, comparing male and female authors across their respective periods. Once again, there are substantial correlations with Hoover’s wordlist, with stereotypes appearing in all sets, across all periods. But, as noted, the crossover between genders fluctuates throughout periods, with contemporary authors sharing the greatest similarities (see Figures 2, 3, and 4). This would suggest that gender distinctions between authors fluctuate across literary epochs.     Figure 2. Victorian authors.    Figure 3. Modernist authors.    Figure 4. Contemporary authors. Our methodological implications do require brief commentary. A Zeta analysis introduces its own inherent limitations, forming as it does a list of words preferred and avoided by one dataset (e.g., female literature) insofar as they relate to another dataset (e.g., male literature). In Delta, the data speaks for itself, whereas in Zeta we use a mode of classification, so the separation we receive is partly a result of our dataset preparation. Rather than the data establishing its own distinctions, we (in a way) privilege and impose the gender split in our separation of the male and female datasets. There are literary and mathematical justifications for this, but it is worth acknowledging, and certainly worth discussing in the context of a gender-specific study. However, taking into account the data from our Delta analysis, the results of which turned up inconclusive, the subsequent Zeta can be seen to merely refine an already existent pattern. That being said, this particularity is something we will focus on further as we fine-tune our analysis for presentation. In doing so, we will computationally compare results across all of the lists, including Hoover’s, to see if any correlations may have been missed in this initial stage, which will ultimately allow us to conduct a closer analysis of the distinctive gender markers.   Conclusions Worth noting is that the ongoing dispute in literary studies concerned with gender and writing style is wide and varied, and the scope of our research does not immediately affect the debates of gender theory or the potentiality of a distinct form of  écriture feminine. In the parameters of such literary discourses, however, our project does provide a quantitative means to approach an overwhelmingly qualitative discussion. Specifically, our preliminary analyses lend evidence to the claims that such gender differences are evident in writing across periods, but that these differences are less distinguishable depending on the period in question.  Studies that have included multiple genres or mediums in their stylistic analyses and text classifications have concluded that stylistic features often depend more heavily upon genre than gender (Herring and Paolilo, 2006; Janssen and Murachver, 2004; Argamon et al., 2003). Yet what these studies have consistently shown is that gender and genre both seem to reveal stylistic and thematic qualities historically associated with male and female language application. Our goal in limiting the mode of our corpus to fiction novels (and novel-length collections of short stories) is to refine the results of prior studies with more generalized scopes (e.g., Koppel et al., 2002; Argamon et al., 2003) and place similar conclusions within a periodized context, ideally providing ample opportunity for literary application. While we follow in the methodological footsteps of such studies, we have shifted the focus of our investigation away from style, in the macro-analytical sense, to period and its relation to gender-differentiable terminology. A number of projects (Argamon et al., 2003; Burrows, 2004; Schler et al., 2005; Pennebaker, 2011) have concluded that the use of function words—pronouns (taken customarily as female markers) and determiners and prepositions (taken customarily as male markers)—provide a reliable basis for gender identification in writing. None of them, markedly, has sufficiently addressed the issue of modernity and the evolution of language and its changes over time, though Hoover has suggested that this is the way forward. Our research separates from such work in that we aim to do just that: distinguish how gender differences have evolved over and between selections of specified, canonical literary periods, focusing on distinct, rather than functional, word choices. Admittedly, this is a first step, and akin to Hoover in his initial study, we acknowledge the need for a larger corpus and more refined dataset—this, of course, is the constant nature of the computational beast. But we can at least view this is a next step.   ",
        "article_title": "Gender Markers: Distinctive Words in Male and Female Authorship",
        "authors": [
            {
                "given": "Sean G.",
                "family": "Weidman",
                "affiliation": [
                    {
                        "original_name": "Pennsylvania State University, United States of America",
                        "normalized_name": "Pennsylvania State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p491231",
                            "GRID": "grid.29857.31"
                        }
                    }
                ]
            },
            {
                "given": "James",
                "family": "O'Sullivan",
                "affiliation": [
                    {
                        "original_name": "Pennsylvania State University, United States of America",
                        "normalized_name": "Pennsylvania State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p491231",
                            "GRID": "grid.29857.31"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "text analysis",
            "English",
            "gender studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " According to the latest panel discussions at the Historical Image Processing (HIP 2013) 1 and Document Analysis Systems (DAS 2014) 2 conferences, historical documents are among the most challenging types of documents for automatic processing. This is due to the vast variety of challenges they pose to document image analysis (DIA) systems. In the pipeline of automatic DIA, layout analysis is an important prerequisite for further stages, such as optical character recognition and information retrieval. Layout analysis aims at splitting a document image into regions of interest and especially distinguishing text lines from other regions.  For automatic layout analysis methods, an essential prerequisite is the ground truth (GT), i.e., existing labels (text line, decoration, etc.) annotated by human experts for the corresponding regions. An example of historical document image 3 and its ground truth are shown in Figure 1. The importance of GT is twofold: first, automatic methods could learn knowledge represented by GT from the experts, and then predict the layout of unseen images. Second, it is used to assess the performance of an automatic method by the accuracy of the prediction of the method against the GT. However, the generation of GT is time-consuming.  We propose a novel web-based interface called  D IVA DIAWI, in order to assist the user to semi-automatically generate ground truth for large numbers of historical documents. The name  D IVA DIAWI stands for a Web-based Interface for Document Image Analysis of DIVA group (Document, Image and Voice Analysis). 4 D IVA DIAWI incorporates two parts: automatic processing and manual editing. In the automatic processing part, the system automatically draws polygons representing text lines for which manual generation of GT is quite time-consuming. In parallel or sequentially, the user can manually edit the polygons by modifying them or directly drawing new polygons.  D IVA DIAWI greatly accelerates the generation of GT. It is robust to generate GT for historical document images of a diverse nature.      (a)  (b) Figure 1. (a) An image example and (b) its ground truth. In (b), the cyan, blue, red, green, and purple polygons represent the regional contours of the page, text block, text lines, decorations, and comments, respectively.  State of the Art  In recent years, several tools for GT generation have been developed. Aletheia (Clausner et al., 2011) is an advanced system to generate GT for printed materials such as newspapers. It aids the user with a top-down method using split and shrink tools as well as a bottom-up method to group elements together. WebGT (Biller et al., 2013) is a web-based system for GT generation for historical document images. It provides a semi-automatic strategy enabling instant interaction between the user and the system. The Java-based  D IVA DIA tool (Chen et al., 2015) was developed by our group. It provides a user-friendly user interface to generate GT. The tool has been proven effective and flexible by performing a real task of GT generation. Other state-of-the-art tools include AGORA (Ramel et al., 2006), PixLabeler (Saund et al., 2009), TRUEVIZ (Lee et al., 2003), and GEDI (Doermann et al., 2010).  Compared to the state of the art, the novelty of  D IVA DIAWI is twofold. First, the high level of automation greatly reduces the human effort. The user needs to do only a few simple modifications after the automatic processing. Second,  D IVA DIAWI doesn’t require the user to install any software. It works on all modern browsers, e.g., Firefox, Chrome, and Internet Explorer.   System Overview   Ground Truth Representation  As it is shown in Figure 1(b), we define five types of regions of interest:  text line, text block, decoration, comment, and  page (Chen et al., 2015). The  text line is the type of region where the main text is written. The  text block incorporates text lines and document background between text lines. The  decoration represents decorative elements such as figures, drop capitals, and decorative initials. The  comment represents the annotations and inserted text in the margins. Finally the  page outlines only the document part within the scanned image. The contours of the five regions are represented by polygons. All information—including the polygons, the document name, and the name of the author who generated the GT—are saved in XML files of the PAGE format (Pletschacher and Antonacopoulos, 2010), a widely used image representation framework for layout analysis. Furthermore, the GT also has the potential to be exported to the widely used TEI-P5 format for manuscript description 5   System Workflow   D IVA DIAWI is now publicly accessible at http://diuf.unifr.ch/diva/divadiawi/. A screenshot of  D IVA DIAWI is shown in Figure 2.  D IVA DIAWI integrates automatic processing and manual editing. The workflow is illustrated in Figure 3 and detailed below.  Among the five region types, the regions of text lines cost most of the working time for GT generation. 6 The automatic processing of  D IVA DIAWI is developed to accelerate the GT generation specifically for text lines. The user needs to manually draw a rectangular polygon representing a text block that contains some text lines. As soon as the rectangular polygon is drawn, the automatic processing starts. We use Gabor filters, widely used for text extraction (Jain and Farrokhnia, 1991; Raju et al., 2004), to detect regions of text lines from the rectangle. After being detected, text lines are represented by polygons and are then drawn. The automatic processing performs well on our datasets. Most of the regions of text lines are properly outlined. However, there are still some minor mistakes. In small areas where two adjacent text lines intersect or are very close to each other, the two text lines are wrongly grouped into a single line in some cases. In addition, for some strokes lightly written near the boundary of a text line, automatic processing fails to detect them. Thus manual modification follows.  The manual editing has two modes: manual drawing and manual modification. Since automatic processing specifically deals with text lines, the user needs to manually draw regions of the page, text blocks, decorations, and comments. The user can select the shape (polygon or rectangle) and region type to draw. The drawn polygons and rectangles have different colors depending on their region types. After drawing, the user can modify polygons or rectangles by dragging a vertex to a new position. A new vertex can also be added to a polygon. In addition, a polygon can be deleted. Finally, an XML file storing the GT of the page is generated.    Figure 2. A screenshot of DIVADIAWI.    Figure 3. System workflow of  D IVA DIAWI.   (a)     (b)  (c)   (d)  Figure 4. The process to draw polygons of text lines by automatic processing and manual modification. (a) A part of original image. (b) Rectangular polygon of text block drawn. manually by the user. (c) Polygons of text lines obtained by automatic processing. (d) Final result obtained by manual modification. Evaluation IAM Historical Document Database (IAM-HistDB) 7 is used for the evaluation. The database contains a variety of historical document images, which are color or gray, single-column or double-column, and date from different centuries.   D IVA DIAWI works well on the database. Thanks to the automatic processing, it greatly accelerates the GT generation. We show an example to draw polygons of text lines on a color image 8 in Figure 4. Most pixels within regions of text lines are included in the polygons obtained by automatic processing (see Figure 4[c]). However, the result of this step is still not perfect. For example, the bottom part of the big character ‘G’ in the first text line is very close to the second text line, leading automatic processing to fail to separate them. The problems could be solved by manually dragging the problematic vertexes of the polygons to proper positions (see Figure 4[d]). In general, with the automatic processing, the user needs to do only a few simple manual modifications for each text line. This manual work is much less effort than completely manual editing from scratch.   System Usability Scale (SUS; Brooke, 1996), a measurement to assess the global performance of system usability, is used to assess  D IVA DIAWI. Researchers from the humanities and computer sciences are both invited to participate in the assessment. Quantitative evaluation details will be provided in the full paper.   Conclusions and Future Work  We propose  D IVA DIAWI, a web-based interface for the GT generation for historical document images.  D IVA DIAWI is efficient to accelerate the GT generation, thanks to the integration of automatic processing and manual editing. It is robust to work on different kinds of historical document images from the IAM-HistDB.  In the future we will keep improving  D IVA DIAWI. For example, automatic processing could be better designed to even decrease errors. Techniques to automatically outline other region types—e.g., page and text block—could also be applied.   Notes 1. Workshop on Historical Document Imaging and Processing, 2013, http://www.cvc.uab.es/~vfrinken/hip2013/. 2. 11th IAPR International Workshop on Document Analysis Systems, 2014, http://das2014.sciencesconf.org/. 3. Saint Gall DB: Cod. Sang. 562, Abbey Library of St. Gall (SG30). 4. http://diuf.unifr.ch/main/diva/. 5. http://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html. 6. In a process of manual GT generation of 100 document pages, about 80% of the time was spent on text lines. 7. http://www.iam.unibe.ch/fki/databases/iam-historical-document-database. 8. Parzival DB: Cod. 857, Abbey Library of St Gall (PAR23). ",
        "article_title": "DIVADIAWI - A Web-based Interface for Semi-automatic Labeling of Historical Document Images",
        "authors": [
            {
                "given": "Hao",
                "family": "Wei",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Kai",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Mathias",
                "family": "Seuret",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Marcel",
                "family": "Würsch",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Marcus",
                "family": "Liwicki",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Rolf",
                "family": "Ingold",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "historical studies",
            "xml",
            "interface and user experience design",
            "English",
            "internet / world wide web",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In this paper, we use a new technique, called Concepts Through Time (CTT), to trace concepts in newspaper discourse. CTT makes use of sequential semantic spaces to follow semantic shifts of concepts diachronically. The semantic spaces are based on the extensive digitized newspaper corpus of the Dutch National Library. 1 As part of our approach, concepts are modeled as words related to each other in a semantic space. A key aspect of CTT is that the vocabulary used in the debate on a concept may change over time. In fact, the set of words used to discuss a particular concept might not show any overlap at all between different periods of time. As such, our method bears a resemblance to the rhizomatic structure as described by philosopher Gilles Deleuze. The conceptual implication of this method approximates the traditional scholarly methods of conceptual history. We test our method through three case studies: consumerism, globalization, and economic models. These domains are inextricably linked to the manifestation of modernization and Americanization within Dutch public discourse throughout the 20th century. 2 Tracing the genealogy of these manifestations helps us rethink the variety of meanings of modernization and Americanization in the Netherlands during this period. Essential to the development and success of this method is that, for every step of the process, decisions have been motivated from both the perspective of computational linguistics and cultural history. This approach makes this research project a genuine collaborative digital humanities effort. 3  Traditionally, the field of conceptual history has tried to combine micro and macro perspectives by studying the emergence and transformation of concepts, ideas, and thoughts over larger periods of time. Historians have increasingly used digital tools for the purposes of conceptual history. However, researchers in these studies regularly employed pre-defined and ahistorical definitions of concepts. Full-text search and n-gram viewers, for example, require a workable definition of a concept or a range of words that cover the subject in order to, subsequently, analyze them within certain contexts and periods. 4 The necessity of pre-defining terms is a serious drawback of working with these tracking tools. The research done in this way runs the risk of ahistoricity. The same goes for top-down approaches, in which a specific language model allows for the recognition of specific semantic information, such as entities via Named Entity Recognition (Grover et al., 2008; van Hooland et al., 2013) or via word classification lists (Klingenstein et al., 2014). Topic modeling approaches partly circumvent this limitation by modeling groups of words rather than single words. Topic modeling tools such as Mallet and TMT allow users to find topics in a corpus of texts without having to provide the algorithm with any prior information. 5 With approaches such as ‘Topics over Time’, researchers could also track topics diachronically (Wang and McCallum, 2006). Other research projects have used topic models to develop contextualized dictionaries that helped, e.g., to define the concept of ‘strikes’ in 20th-century Netherlands (Bogers and Van den Bosch, 2008) or the Chicago School of ‘neoliberalism’ in postwar Germany (Wiedemann et al., 2014).   The methods described above all share the limitation that concepts—whether defined by hand or automatically—are static. Either the user provides a fixed list of words or a fixed list of topics is inferred from the data in these topic-tracking approaches. However, historians commonly use texts to get a grip on the continuities and discontinuities throughout time. This involves the recalibration of their hermeneutical framework as they go along—that is, the list of words they work with. We propose that digital tools should also be able to provide argumentative evidence for such a methodology in constructing historical narratives. For this purpose, we introduce a new technique, called CTT (Concepts Through Time), that aims to resolve the deficiencies of earlier methods. The technique is explicitly developed to account for historical changes in the semantics of concepts, thereby approximating the way historians traditionally have worked within conceptual history. CCT enables historians to trace concepts over large periods without having to manually select appropriate terms for the entire time span and without being dependent on a fixed set of topics. This allows for a greater sensitivity to semantic changes and an increased interactive heuristic approach to concepts within their discursive context. 6  Methodologically, CTT is based on word embeddings, as inferred by a neural network trained on a large body of data, to monitor semantically related words. These are created using word2vec (Mikolov et al., 2013), a computational technique that does not depart from a top-down model of language. Rather, a semantic space is inferred from the input data. Word2vec uses a continuous bag-of-words or a skip-gram architecture in order to produce a multi-dimensional word-vector space. This space contains semantic and linguistic regularities that can be used for the analysis of discourse (Baroni et al., 2014; Wijaya and Yeniterzi, 20111). A positional shift within the vector space can been established as an indicator for chronological shifts on a semantic and syntactic level—for example, in the use of the words ‘gay’ and ‘cell’ (Kim et al., 2014). 7 This is not done using traditional means of analysis, such as part-of-speech tagging, but by merely observing the geometry in the vector space. However, the authors who have provided the mentioned examples have worked with single words of which the common meaning has shifted almost diametrically. We will be looking at clusters of words and shifts of meaning both large and small.  The data we use to trace concepts are the circa 500,000 newspaper issues between 1890 and 1990 that are available in the Dutch National Library’s digitized newspaper archive. We train subsequent semantic model spaces in which we trace groups of terms, rather than individual words, diachronically and synchronically by keeping track of semantic relations between terms per period. For this approach, no pre-established and fixed topic sets are needed. We will use a single seed set of terms, merely as an entry point into the cluster to then find semantically related words within the semantic model. We will automatically update the set of related words over time, utilizing the subsequent semantic spaces. A key aspect of this procedure is that the original seed words might disappear from the cluster of words over time.  The way our word embedding technique clusters words holds surprising similarities with philosopher Gilles Deleuze’s notion of the rhizomatic structure (Deleuze, 1987). 8 Important features of this structure are that it allows for multiple entry and exit points, and that the semantic meaning of a concept is determined through its ‘relations of exteriority’ (DeLanda, 2006, 10–11). Translated to our approach, the meaning of a term can be considered in terms of the geometric relation in a semantic space. 9 Starting from this theory, we aim to focus on the stabilization and destabilization of relational vectors between words, i.e., the emergence and disappearance of words within a subset, as well as the shifting position of words in relation to one another. The latter could be the effect of forces within the cluster or changing vector relations outside of the cluster. We argue that the semantic space allows us to analyze ‘the processes that historically produce the identity of a given whole, but also the processes [coding and decoding] that maintain that identity through time’ (DeLanda, 2006, 10). Using this approach, we hope to determine words central to a topic and avoid concept drift caused by ambiguous words.   The development of this tool stems from a close collaboration between information retrieval experts and cultural historians. Every single technical or methodological choice has been motivated such that it complied with both sound computational linguistic research and critical historical methods. Consequently, we have high hopes of the applicability of this technique in historical research. We will evaluate our approach through three historical use cases in the context of the historiographical debates on modernization and Americanization in the Netherlands. 10 In the first case, we will trace popular consumer goods, such as cigarettes, alcohol, and fast food. This case study sets out to show which consumer goods appeared in newspaper discourse in specific periods. Moreover, it might show us in what wider discursive contexts these consumer goods were discussed—e.g., as luxury goods or as unhealthy products. The second use case maps out businesses in newspaper discourse. This might shed light on processes of globalization in which local businesses are substituted by multinationals. The final use case deals with the notion of efficiency. The concept was introduced in the Netherlands after the First World War (Bloemen, 1988); we will trace the genealogy of this concept to see whether it was already present in earlier discourse without being denoted as ‘efficiency’. In our final paper, we will show a formalization of the search strategies used to trace concepts through time. Furthermore, we will reflect on the representativeness of the newspaper archive as well as the applicability of the method to other corpora.   Notes 1. This collection includes over half a million newspapers for the 1890–1995 period. See www.delpher.nl. 2. This paper is part of the research project Translantis, which looks into the role of the United States as a reference culture in Dutch public discourse. Therefore, the chosen case studies are related to issues of Americanization within the field of consumer society and economy.  3. Collaboration between computational experts and humanities scholars is an elemental part of the digital humanities. See Burdick et al. (2012), pp. 15–17. 4. This method was adopted to study eugenics by Huijnen et al. (2014). 5. For an explanation of topic modeling, see Blei and Lafferty (2009). For uses of topic modeling in historical research, see Newman and Block (2006); Mimno (2012); Blevins (2010); Wittek and Ravenek (2011). 6. The importance of context for historical research is eloquently expressed in Snickars (2012). 7. In Kim et al.’s paper, the authors show how ‘gay’ has shifted from the connotation with an emotion to sexuality, and ‘cell’ has shifted from the denotation of a prison cell to the cellular phone.  8. For a clear description of rhizomatic thinking and assemblage theory, see ‘Assemblage Theory’ (2010). An interesting implementation of Deleuze’s notion of assemblage and rhizome has been performed by DeLanda (2006). 9. Michel Foucault makes a similar point when he argues that the meaning of an expression cannot be simply deduced from syntactic and semantic meaning. Rather, one should look into the conditions in which expressions appear and change—in his words, ‘by the analysis of the relations between the statement and the spaces of differentiation, in which the statement itself reveals the differences’. See Foucault (2012) . 10. These use cases stem from the Translantis research project. See  www.translantis.nl and van Eijnatten et al., 2013.  ",
        "article_title": "Concepts Through Time: Tracing Concepts In Dutch Newspapers Discourse (1890-1990) Using Word Embeddings",
        "authors": [
            {
                "given": "Melvin",
                "family": "Wevers",
                "affiliation": [
                    {
                        "original_name": "University of Utrecht, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Tom",
                "family": "Kenter",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam, The Netherlands",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Pim",
                "family": "Huijnen",
                "affiliation": [
                    {
                        "original_name": "University of Utrecht, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "philosophy",
            "content analysis",
            "corpora and corpus activities",
            "text analysis",
            "English",
            "semantic analysis",
            "data mining / text mining",
            "semantic web"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Succession (http://mtchl.net/succession) is a web-based work that uses combinatorial techniques to provide an engine for investigating the complex fields of meaning latent in digital cultural collections. This paper introduces this practice-led research project and investigates the potential of combinatoric generative techniques for imaginative, interpretive meaning-making with digital cultural collections.   Context, Aims, and Approach  Digitisation is turning out corpora that rapidly exceed human interpretive scope: the Internet Archive recently published some 2.6 million machine-extracted images to the Flickr Commons (Miller, 2014). Techniques for dealing with non-text collections at scale are developing, such as Manovich’s work on macroscopic analyses (2012). Another emerging strand of practice takes a more poetic and playful approach, offering serendipitous samples and chinks of algorithmic insight. Tim Sherratt’s Trove News Bot tweets archival news articles based on daily headlines (Sherratt, 2013); the British Library’s Mechanical Curator posts random images from the library’s digitised books (Baker, 2013). Sherratt’s Eyes on the Past (2014) harvests faces from digitised newspapers and has their eyes peer out through the interface, inviting investigation. These approaches reflect an emerging interest in collections as active sites of meaning-making and experimentation with how we might encounter such collections in an everyday digital environment.  Succession addresses the industrial and pre-industrial heritage of Newcastle-Upon-Tyne and surrounds, reflecting my encounters with the city during an extended visit, and the thoughts (and affects) on energy, history, industrialisation, and capital that the place provoked. Newcastle was a key Roman settlement, later one of the crucibles of the Industrial Revolution, now finding its way in a ‘de-industrial’ Britain (Chakrabortty, 2011). While the work is highly localised, it reads this place as a cypher with far wider implications. The work’s title draws a term from ecology, alluding to continuous and ongoing change and adaptation.  Succession aims to mine the city’s industrial past in order to fuel consideration of our possible futures.   In practice the work is a web application that draws on a corpus of some 2,000 image records harvested from the Flickr Commons and combines these elements into new composites (or ‘fossils’). Each fossil comprises five randomly selected source images arranged, composited, and potentially repeated. Sources are cited; thus, while composites often radically obscure, transform, or juxtapose their elements, the sources and their attendant contexts remain navigable and intact. Composites may be saved, acquiring a permalink to become a new citable online object. The generative process of composition is performed live, in the browser; each viewer will encounter a series of unique composites. The system allows for around 2.5x10 15 combinations of elements (ignoring spatial and blending variations). At a rate of one per second, it would take around 8 million years to show all permutations.  Generative Digital Humanities Schnapp and Presner called for a ‘generative digital humanities’ in their 2009 manifesto; here ‘generative’ means hands-on, makerly, productive, and integrative. This project fits their proposal for a DH that is ‘qualitative, interpretive, experiential, emotive [and] generative in character’. I use ‘generative’ more specifically to describe formal and computational techniques; as such, my approach also reflects Berry’s call (2011) for DH to focus on the attributes of computation and their implications for knowledge. The work described here aims to be generative in both senses: a computational system that provokes experiential, interpretive modes of encounter with digitised history and culture. This is what Drucker and Nowviskie (2004) term  speculative computing: ‘speculative approaches to digital humanities engage subjective and intuitive tools’ while enlisting computation for augmentation, rather than simply automation. A speculative DH pursues computation that is ‘dynamic and constitutive’, ‘creating programs that have emergent properties’.  As Drucker and Nowviskie show (2004), generative procedures have long been used in the arts to augment and extend thought, creating aesthetic and conceptual provocations.  Succession uses combinatorics, the procedural combination of specific formal elements. Permutation gives combinatorics a form of quantitative leverage: small sets of elements proliferate into vast numbers of possible outcomes. When the permuted elements are symbolic or textual, combinatorics becomes a machine for knowledge or meaning. Ramon Lull’s  Ars Magna (1305) permuted divine principles into sets of theological assertions; these conceptual machines marked out domains of knowledge for investigation (see Gardner, 1958). In the 1960s, poets Brion Gysin and Raymond Queneau used combinatorics to pursue unforeseen or inconceivable meaning using fixed and finite textual means. Bill Seaman’s ‘recombinant poetics’ seeks ‘emergent meaning’ within digital arrays of textual and audiovisual elements (2001). Ross Gibson and Kate Richards’  Life after Wartime applies combinatorics to an archive of 1940s crime-scene photographs, showing how a generative ‘story engine’ can prompt speculative interpretations of digital heritage (Richards, 2006).  Authored Spaces and Generative Artefacts  Succession consists of two related domains common to any computational generative process. The first is the generative system: the set of source elements, rules, and procedures that make up the work; the second is the field of actual and potential digital artefacts produced by that system. These two domains have distinct characteristics and specific discursive modes, as outlined below.  Bill Seaman describes his combinatoric generative system as an ‘authored electronic space’, emphasising that it is expansive, but not arbitrary (2001, 426). Similarly,  Succession’s space is authored, in part through its corpus: sources were selected for content relevance, as well as visual potential. The content base grew and was pruned around the conceptual focus in a slow process of subjective evaluation, exploratory search, and tangential investigation. In Seaman’s words, this authorship seeks out a ‘resonant unfixity’ (Seaman, 2002).  In  Succession the rules for compositing elements are also authored, tailored around the idiosyncrasies of the sources and the poetic aims of the work. Layered composition is both formal machine and metaphor: to address a city built on coal it seemed necessary to combine and compress, to obscure while at the same time hopefully intensifying the energy latent in those sources. Thus the image blending modes are biased to overlay dark elements; this treatment resonates with the engraved illustrations in the Internet Archive and British Library collections. Process and corpus (algorithm and data) are interdependent. Where (default) Cartesian space is inert, extensive, and homogeneous, here authored space is co-constituted by an active assemblage of media, concepts, and computational processes.   Figure 1.  Succession composite 1413513552860 (see http://mtchl.net/succession/#/saved/1413513552860).  This saved composite in Figure 1 shows how the generated artefacts can also operate as what Drucker and Nowviskie (2004) call ‘aesthetic provocations’ while enlisting the contexts and referents of their source elements in speculative juxtapositions. This composite is dominated by a 1993 photograph of Wearmouth Colliery in its final week of operation—a poignant image of the last days of Newcastle coal. But a spectral water-bird (from Gould’s 1837  Birds of Europe) seems about to splash down in those desolate puddles. Faintly in the background is the bustling River Tyne itself, circa 1880, and on the left of frame appears a carriage destined for Newcastle’s Metro system, under development in the mid-1970s. Almost imperceptible at bottom left is the HMS  Opal, a torpedo destroyer, under construction at the Sunderland shipyards in 1915. So this composite encompasses not only 150 years of urban history but a latent portrait of 20th-century capital, the rise and (UK) fall of extractive industry, war, urbanism, and pre-Industrial naturalism and the non-human lives it records.  This example shows how formal and visual transformations inflect narrative or historical interpretations, and thus that these generative artefacts are not simply bundles of citations but speculative visual propositions. Layering emphasises simultaneity and atemporal juxtaposition rather than chronology: Gould’s duck, about to dive into the colliery puddles, or perhaps swimming on the 1880s Tyne? Faded traces evoke the presence of the lost; visual collisions prompt an interpretive search for coherence, patterns of connection in the authored space of the system. Thus generative systems like  Succession can be both prompts for interpretation and humanistic ways of speaking in themselves. In addressing complex topics (or ‘wicked problems’) such as extractive industry, capital, and urban change, it seems necessary to respond in kind. Generative techniques provide a way to speak complex multitudes, as well as an engine for unforeseeable combinations, the seeds of something new. Seaman terms this ‘unfixity’; Gibson calls it a ‘restlessness’ that prompts imagination through ‘artful imbalances and implied possibilities for completion or patterning’ (Gibson, 2006). This active imagination is essential, Gibson argues, in understanding and potentially altering ‘the continuous tendencies that are making us as they persist out of the past into the present’.   Generative systems have a long history as formal tools for generating (and navigating) large fields of potential meaning in the arts. This paper argues that such systems are also relevant in the digital humanities.  Succession draws on networked digital heritage to create a ‘restless’ generative system that produces speculative propositions anchored in specific historical contexts, shaped by specific authorial themes and metaphors. It shows how generative techniques can support the creation of new composite cultural objects, and how these composites can use historical fragments to speak richly to the complexity of our present moment.  ",
        "article_title": "Succession: Generative Techniques, Speculative Interpretation and Digital Heritage",
        "authors": [
            {
                "given": "Mitchell",
                "family": "Whitelaw",
                "affiliation": [
                    {
                        "original_name": "University of Canberra, Australia",
                        "normalized_name": "University of Canberra",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/04s1nv328",
                            "GRID": "grid.1039.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "repositories",
            "libraries",
            "corpora and corpus activities",
            "archives",
            "museums",
            "creative and performing arts",
            "sustainability and preservation",
            "English",
            "GLAM: galleries",
            "including writing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In some of the countries where there has been a rapid increase in the use of online music distribution technologies, analysts have reported declining sales of local music repertoire (e.g., Nordgård, 2013). The analysts are concerned about such tendencies since local music repertoire accounts for a sizable share of an average country’s total recorded music sales (e.g., IFPI, 2012). This paper searches for empirical evidence that may confirm these reports in a number of music markets in North America, Europe, and Australasia. The paper makes a contribution to the literature on the digital transformation of the music industry since it combines and analyses data sources that previously have not been used in this context and gives a new perspective on changing user consumption practices in the music industry. The paper also examines the variation of geographic diversity over time among international acts that become commercially successful in the countries covered by the study. In order to measure local repertoire share (LRS) and international repertoire concentration (IRC), weekly singles chart data is collected for seven countries: Australia, Austria, Belgium, Norway, Sweden, the United Kingdom, and the United States. In total, data on more than 270,000 chart items, 50,000 tracks, and 30,000 acts have been mined, processed, and analysed. By combining the chart data with location data from the music technology company the EchoNest, it is possible to attach country information to more than 93% of the data points included in the study. The local repertoire share (LRS) for each country is calculated for each year by dividing the number of chart items by local acts with the total number of chart items during that year. The IRC for a certain country during a certain year is calculated as the Herfindahl-Hirschman index, where the size of the total ‘market’ is defined as the number of chart items by international acts. The index is then calculated based on each country’s share of that ‘market’. Figure 1 shows how average local repertoire share varies over time. It shows that LRS remained relatively stable from the beginning of the period until the turn of the millennium. It then increased from approximately 30% and peaked at about 40% in 2006. From 2007 onwards, LRS has decreased and has been relatively stable at 33% since 2011.  The second variable (IRC) is presented in Figure 2, which shows that the geographic concentration within international repertoire (IRC) remained relatively stable until the turn of the millennium. It then increased until 2009, and finally decreased towards the end of the period. There are similarities between how LRS and IRC vary over time. The paper notes that after an initially relatively stable period that ended at the turn of the millennium, music consumers listened more to music performed by local acts, and the international music that nevertheless was able to reach the charts came from a shrinking number of countries. A previous study (Wikström, 2013) has examined this period (1999–2006) based on another type of data, and showed that the increasing local repertoire share was a consequence of record companies’ changing marketing strategies. In 1999, the recorded music industry entered a period of rapidly declining sales (primarily caused by online piracy), which put severe pressure on record companies’ financials and required a number of music companies to reduce their marketing budgets and to increase their risk aversion (Wikström, 2009). An efficient way for a record company to reduce marketing budgets is to focus on a limited number of countries and to reduce the number of artists that are given the support required to promote their music beyond their local markets. As a consequence of this change, local artists’ relative commercial success increased and the number of countries with the capability to export acts internationally decreased (as illustrated by Figures 1 and 2).  From 2006 onwards, music listening patterns changed once again. Music listeners returned to music performed by international acts (Figure 1) and somewhat later they also increasingly listened to international acts from a growing range of countries (Figure 2). While the dynamics during the first phase can be explained by music companies’ changing marketing strategies, one has to look elsewhere in order to explain the dynamics during this period. By 2006, the Internet and online piracy certainly had affected the music industry during a number of years. Revenues from recorded music sales had shrunk by more than 30%, but while this in many ways had severe consequences for a number of industry players, the fundamental structures for promotion and distribution of recorded music largely remained untouched: First, music promotion in 2006 still was confined to traditional channels, primarily broadcast radio and television, meaning that music promotion was still structured by national boundaries (see, e.g., Wikström, 2009). Almost a decade later, in 2014, Facebook and particularly YouTube have significantly weakened these structures and have emerged as the most important tastemakers in the music industry—especially among younger music listeners (see, e.g., Chmielewski, 2013). This is a fundamental transformation of how music is promoted by producers and discovered by fans. National boundaries certainly still matter, but to a much lesser extent than in the past, and the cost of promoting an act to an international market has diminished. Second, even though there were a number of legal online music retailers available for music consumers in 2006 (e.g., Apple’s iTunes), the recorded music market was still dominated by physical distribution (IFPI, 2007), which means that there were still significant barriers that made it difficult for a music listener to discover new music without significant costs. In 2014, music services such as SoundCloud, YouTube, Spotify, Deezer, etc., make music instantly available and allow music listeners to make treks into musical territories that are far away from their own musical backyard. More than half of the global recorded music market is online, and in some of the countries in this study (such as Norway and Sweden), the digital market share is closer to 80% (Figure 3). The paper is able to provide empirical evidence that is able to confirm the observations about declining local music repertoire sales as promotion and distribution of music move online. The paper also shows that, at the same time this happens, international repertoire concentration declines as well. In other words, the music industry has become increasingly international in its character: Local audiences may listen less to local acts, but overseas opportunities open up for acts from countries that previously were unable to launch their music internationally. ",
        "article_title": "Local Music Repertoire and the Digitization of the International Music Industry: An Empirical Analysis, 1994 - 2013",
        "authors": [
            {
                "given": "Patrik",
                "family": "Wikstrom",
                "affiliation": [
                    {
                        "original_name": "Queensland University of Technology, Australia",
                        "normalized_name": "Queensland University of Technology",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/03pnv4752",
                            "GRID": "grid.1024.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "digitisation - theory and practice",
            "music",
            "English"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The ways in which geographic space is used in literature and in other textual materials is a topic of keen interest to researchers in the humanities (Dimock, 2006; Fetterley and Pryse, 2003; Giles, 2011; Heise, 2010; Hsu, 2010; Thacker, 2005). Yet until recently, it has been impossible to study the problem at scales beyond a handful of books or, occasionally, somewhat more broadly via bibliographic metadata and other paratextual information (Ogborn, 2012). The newest work in the field, however, uses natural language processing techniques in combination with geographic information systems to extract and analyze geographic references directly from large textual collections, a development that enables much more comprehensive treatment of historical and contemporary materials (Blevins, 2014; Cooper and Gregory, 2011; Wilkens, 2013). Nevertheless, there remain difficulties in applying such methods to truly large textual corpora (Rupp et al., 2014) and in making sense of the data they generate to answer properly literary, historical, and cultural questions. The present work describes the application of computational georeferencing to the 12 million volumes (approximately 2 trillion words) of the multilingual, international HathiTrust corpus of digitized texts, as well as a preliminary use of the generated data to analyze the relationships between patterns of literary attention and demographic and economic factors in the United States between 1800 and 2010. The findings are novel in the size of the underlying dataset, in their incorporation of modern and contemporary in-copyright texts for large-scale computationally assisted literary analysis, and in their use of statistical modeling to assess the social conditions shaping literary production.  Georeferencing of the source material is performed in two steps. First, named entities are identified in each volume of the HathiTrust corpus via the HathiTrust Research Center’s non-consumptive data capsule infrastructure (Borders et al., 2009; Zeng et al., 2014). The capsule infrastructure allows research code to be executed safely against the full corpus, including in-copyright volumes, without the threat of leaking reconstructable texts. The NER algorithm is adapted from Stanford’s linear-chain conditional random field method (Finkel et al., 2005; Lafferty et al., 2001) using language-specific training data for English, French, German, Spanish, and Chinese as appropriate at the page level. For English-language texts, a second, period-specific language model is used for volumes published before 1900. Identified location strings are then associated with detailed geographic information via Google’s geocoding API, tuned to the country of source publication. Disambiguation is performed in limited cases according to co-occurrence patterns in gold-standard texts. Overall accuracy ( F 1) approaches 0.8, but varies by language, historical period, OCR quality, and other factors.   Locations are then aggregated and mapped at the national and city levels, optionally consolidating by era of publication, source country and language, and/or source genre. This process produces cartograms of the types shown in figures 1 and 2.     Figure 1. Log-scale nation-level aggregated locations in 19th-century U.S. fiction.     Figure 2. City-level locations in the same dataset.  These figures are useful because they provide an unprecedented overview of geographic usage in a corpus that, while still subject to limitations, approaches comprehensive representation of formally published literary output in recent centuries. Notable features include a heavy preponderance of locations in Europe, the eastern United States, and the Mediterranean rim, reflecting the composition of the corpus, as well as, more interestingly, a posited undercurrent of historical and cultural conservatism within literature that leads its distribution of geographic attention to lag significantly changes in economic output, population centers, and other social structures.  The observation concerning literary-geographic conservatism leads to the second area of investigation, an attempt to assess the viability of predictive modeling of geographic attention as a function of socioeconomic variables in the United States (and, in work not reported here, elsewhere in the world). Historical data are drawn from U.S. Census Bureau records, including state- and county-level measures of population, literacy rates, educational attainment, manufacturing output, agricultural production, racial and ethnic composition, immigration history, newspaper publishing activity, and so on. These data are then used to build a predictive generalized additive model of geographic attention by performing penalized regression analysis against the geolocation results reported above. The results for a limited case are shown in Figure 3, though one is less interested in ‘predicting’ geolocation results already in hand than in identifying outlier cases (geographic locations that are under- or over-represented relative to the model’s expected values) and evaluating the significance of the various socioeconomic factors as predictors of literary attention. A two-dimensional geonormalized plot of the prediction data in the same case is shown in Figure 4.     Figure 3. Observed vs. fitted state-level location counts in U.S. fiction. Adjusted  R 2 = 0.76.      Figure 4. Predicted geographic location intensity in the same dataset. White = high; red = low.  In the example shown here, which considers 19th-century U.S. fiction, significant predictors of geographic attention include total population, non-white and immigrant population, and degree of urbanization. Surprisingly insignificant as independent predictors are literacy rates, per capita newspaper publication, and manufacturing output. Three classes of locations are especially prominent in the model: cities, battle sites, and in the second half of the century, far Western locations. These facts may ultimately be assimilable within established critical narratives about the trajectory of American fiction in the 19th century, but note that they align badly with both the regionalist and Puritan hypotheses (Fetterley and Pryse, 2003; Bercovitch, 1975), while being perhaps better suited to hemispheric and transatlantic views. In any case, the results suggest a particular need to revisit issues related to urbanization and westward expansion in the period.  For lack of space, I’ve presented here only one brief example. But these methods are straightforwardly extensible to the other periods and national contexts included in the HTRC data. In recent decades, for instance, literary-geographic attention (in all studied languages) has once again lagged urbanization outside the Western world and correlates only weakly with economic growth, facts that align in interestingly imperfect ways with intensifying critical consideration of neoliberal tendencies in contemporary fiction. The French and British cases show striking similarities in the degree to which their domestic attention is allocated to their capital cities, though with different development curves in response to industrialization and with different allocations of foreign attention structured by their lingering colonial investments.  The results presented in this paper represent some of the largest-scale computational humanities work carried out to date. It has been made possible in part by the generous support of the American Council of Learned Societies, the Andrew W. Mellon Foundation, the Social Sciences and Humanities Research Council of Canada, the HathiTrust Research Center, and the University of Notre Dame Office of the Vice President for Research. Dr. Michael Clark provided substantial help with the statistical modeling work. Vienna Wagner, Angela Lake, and Jessen Baker performed much of the work to develop period-specific English-language NER training data.  ",
        "article_title": "Mapping and Modeling Centuries of Literary Geography across Millions of Books",
        "authors": [
            {
                "given": "Matthew",
                "family": "Wilkens",
                "affiliation": [
                    {
                        "original_name": "University of Notre Dame, United States of America",
                        "normalized_name": "University of Notre Dame",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00mkhxb43",
                            "GRID": "grid.131063.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "data modeling and architecture including hypothesis-driven modeling",
            "literary studies",
            "geospatial analysis",
            "English",
            "interfaces and technology",
            "data mining / text mining",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper will present the prototype of EMNON, the Early Modern Network Of Networks, an open-access research resource using social network analysis in tandem with digital visualization techniques to enable modern scholars to access, explore, and participate in the reconstruction of the social network of scholars working in Europe and America between 1500 and 1750 whose intellectual discoveries fuelled literary and historical developments throughout this period. I will discuss how EMNON expands on the work of Findlen et al. (2008–), Elson, Dames, and McKeown (2010), and Piper (2014) by pioneering the combination of NodeXL and VisualEyes to create digital visualizations of different facets of the Early Modern Network Of Networks. In using digital techniques to re-discover early modern social networks, EMNON aims to enable our modern global knowledge network to re-engage with our early modern counterparts in an interactive digital environment, so that we reach new understandings of how social relationships drove intellectual change in this period on a global scale.  Critical Background for the Project Findlen et al. (2008–) are using network analysis to reconstruct correspondence networks, whilst Elson, Dames, and McKeown (2010) used these techniques to investigate social networks of fictional characters. Turkel’s (2011) use of network analysis examining the NiCHE community is a representative example visualizing a modern knowledge network, whilst Klein’s (2012) engagement with Protovis to explore familial interactions documented in  The Papers of Thomas Jefferson provides insight into the utility of network analysis in animating historical social dynamics. One of the most exciting recent applications of network analysis in literary studies has been Piper’s (2014) research creating network graphs visualizing a variety of connections between the literary works of J. W. Goethe. In treating the texts themselves and their lexis as network nodes, Piper’s work is closely aligned to the approach used by EMNON in which not only people but also specific intellectual terms act as nodes within network diagrams.   Work conducted by the Centre for Early Modern Mapping, News, and Networks and the Centre for Early Modern Exchanges provides crucial early-modern-specific critical contexts. Ahnert’s 6 Degrees of Francis Bacon is a reference point, though EMNON differs from this in reconstructing a very specific early modern network of logicians and their readers. Aranda’s prosopographical study of scientific networks in Spain is also a useful comparison, as, like EMNON, he uses archival research to trace early modern intellectual relationships linking people with one another via the ideas that they promulgated. Likewise, Suzanne Sutherland’s research (2000–) exploring and visualizing the correspondence network of Athanasius Kircher via Palladio offers crucial blueprints for effective data collection and analysis from early modern manuscript materials in particular. These projects are allies with my work in transforming textual data into network visualizations, and by the time of DH 2015 it is my hope to have live dynamic network mapping capabilities on the EMNON website to provide a new approach for this kind of work. I concur that James Secord’s (2004) concept of ‘knowledge in transit’ is key to my project, and I will engage with his. EMNON’s Approach: Aims and Method Logic was the fundamental ‘ ars artium, scientia scientiarum’ (art of arts, science of sciences) in the early modern period, providing the tools for creating all forms of written expression and textual analysis, and governing the selection of subject matter and the structuring of discourse. A plethora of logic textbooks were produced across Europe and America from 1500 to 1750, and these texts were used ubiquitously in schools and universities in this period to teach students to read, analyze texts, and write new works. Howell (1956), Kneale and Kneale (1962), and recently Mack (2011) have employed traditional approaches to establish a bibliographic printing history of this field. EMNON expands on this work to contend that intellectual developments in this fundamental ‘art of arts’ were propelled by an extensive global knowledge network of scholars working throughout Europe and America. These scholars operated in diverse confessional, geographic, chronological, and linguistic contexts, and yet it is through their intellectual exchanges and friendships that the early modern ‘art of arts’ evolved.   Whilst much work is being done using tools specifically designed for historical network analysis, there is a notable lacuna of engagement with software designed to tackle the social elements of social network analysis. EMNON is innovative in highlighting the importance of the sociability of early modern knowledge networks by using software explicitly created for the analysis of social networks via social media, NodeXL. This open-source graphing software enables complex multi-faceted analysis of relationships in a large dataset. NodeXL’s flexible, extendible format allows EMNON to visualize relationships between people and ideas in the early modern network in numerous different ways. In conjunction with the use of NodeXL to reconstruct social relationships within the early modern network of networks, EMNON also engages with the open-source tool VisualEyes developed at the University of Virginia, and used experimentally by Bill Ferster et al. to visualize Jefferson’s travels to England. EMNON is among the first projects to adopt VisualEyes as a tool for the visualization and animation of historical social networks, and in this paper I propose to discuss the benefits of doing so, furnishing examples from the prototype EMNON website, which will be live at DH2015.   Data Collection  Working in rare book libraries for five years, I transcribed 1.5 million words from logic textbooks and literary works from 1550 to 1700. 1  Sources include publications by writers and thinkers involved, their manuscript discussions of early modern logic with one another, correspondence, marginalia, pedagogical exercises, university class lists, and records of book loans from private libraries, which combine to illuminate distinctive edges connecting people and ideas in the network. EMNON at Work: Examples In this paper I wish to show several examples of the utility of using NodeXL and VisualEyes in creating dynamic visualizations that capture the intricate layers of connectivity in a social network. In this proposal I wish to furnish some brief examples to illustrate the early results of the efficacy of these dual approaches, creating graph-based visualizations of the Early Modern Network Of Networks, and animated visualizations of this data bringing digitized primary source materials about this subject to life.   The following example documents the birth, genealogy, and migration of the logical term ‘procatarctic’. This term refers to an ‘external’ logical cause, and was coined and adopted by a very specific but diffuse network of scholars whose links had not been appreciated prior to their documentation and discovery within EMNON’s NodeXL graphs. Bartholomaeus Keckermann first uses the term ‘procatarctic’ in 1600. He identifies Galen as the intellectual progenitor of this word, and this engenders two of the first critical connections in the ‘procatarctic’ network: almost half a century earlier, also writing on ‘external cause’, Petrus Ramus references the same part of Galen in his 1543  Dialecticae Libri Duo, although without adopting this terminology. This small triadic relationship between two of the titans of early modern logic had been   unappreciated until discovered via NodeXL’s graph visualizations as follows:  Figure 1. NodeXL visualizes a triadic relationship in EMNON.  Yet the network of intrigue does not end with these three nodes. The term was promulgated through a subset of scholars working in Europe in the 17th century, from English Puritans Zachary Coke and John Milton to Dutch scholars Burgersdijck and Heereboord, who passed the torch to English Presbyterian dissenting academies via Samuel Jones and Charles Morton. Morton in turn used this term and its corresponding logical precepts to teach pupils including Daniel Defoe at the dissenting academy at Newington Green in the 1670s and 1680s, before immigrating to the Massachusetts Bay Colony where he introduced it to students at Harvard and later Yale. NodeXL enables users of EMNON to visualize this network in many different ways, including chronologically (Figure 2) and as a web of influence (Figure 3).  Figure 2: A chronological graph of usage. Figure 3: A graph showing the web of influence and connections between users of this term. Alongside these graph representations, EMNON is pioneering the application of the VisualEyes software to bring this early modern social network to life.  VisualEyes enables me to animate the spread of ideas through the early modern network, displaying its results in video format with multiple different available views. For example, Figure 4 is a still from EMNON’s moving map, which has traced the geographic spread of the same term ‘procatarctic’ as it moved through Europe and eventually to America:    Figure 4. EMNON uses VisualEyes to create a moving map tracing the geographic spread of the term ‘procatarctic’. In this paper I will also showcase the ways in which EMNON uses the interactive capabilities of VisualEyes to allow users an enriched engagement with this early modern social network. For instance, Figure 5 illustrates what happens if a user clicks on one of the ‘clickable’ waypoints on EMNON’s moving map tracking ‘procatarctic’:    Figure 5. Interactive map whose ‘clickable’ labels display pictures and text about that particular user of the term ‘procatarctic’. VisualEyes enables users to discover multiple pages of additional information about any of the waypoints on EMNON’s moving map, including digitized primary materials and research resources. VisualEyes also has its own graphing capabilities that EMNON can draw upon to enable users to visualize specific subsets of the extensive early modern social network that it documents (Figure 6):    Figure 6. Concept map showing the nexus of users of the term ‘procatarctic’. In pioneering the use of NodeXL and VisualEyes in the context of historical network analysis, EMNON aims to bring the early modern global knowledge network to life so that, in turn, a network of modern scholars can make new discoveries about the accomplishments, trials, and tribulations of the Early Modern Network Of Networks on a pan-European and transatlantic scale. Note 1. Thanks to the following institutions for fellowships that enabled this research: the Rare Book and Manuscript Library at the University of Illinois at Urbana-Champaign for the John ‘Bud’ Velde Fellowship, the International Society for the History of Rhetoric for their fellowship award, and the University of St. Andrews for their Russell Trust Award. ",
        "article_title": "Building The Early Modern Digital University: Using Social Network Analysis and Digital Visualization Tools To Bring The Early Modern Network Of Networks (EMNON) To Life",
        "authors": [
            {
                "given": "Emma Annette",
                "family": "Wilson",
                "affiliation": [
                    {
                        "original_name": "University of Alabama, United States of America",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "visualisation",
            "historical studies",
            "philosophy",
            "literary studies",
            "relationships",
            "English",
            "graphs",
            "renaissance studies",
            "networks",
            "maps and mapping"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Our proposed paper will provide an overview of the theory and methodology driving the creation of Distant Reading Early Modernity (DREaM), a digital humanities project that has made a massive corpus of early modern texts amenable for use with macro-scale analytical tools. Key focus areas include the technical challenges deriving from non-standardized spelling, the philosophy of our tutorial program, the argument for our approach to the early modern archive, and the potential benefit to early modern scholarship of distant reading techniques.  From Microfilm Library, to EEBO, to EEBO-TCP, to DREaM  The foundational work for DREaM began in 1934, when Eugene B. Power used parts of two movie and still cameras to create one of the world’s first microfilm bookcameras, a device he used to photograph thousands of texts in British libraries (Anderson and Power, 1990). In 1998 Power’s microfilm library became the basis for Early English Books Online (EEBO), a database that comprises the images for some 125,000 texts from 1475 to 1700, and has profoundly expanded the horizons of early modern research. 1 To date, approximately one-third of the documents on EEBO are available as transcribed, full-text editions. Researchers for the EEBO Text Creation Partnership (EEBO-TCP) are currently working to transcribe the remaining 85,000 documents, which are as yet only available as digitized microfilm images. 2  Although completion of the transcription work is still at least 10 years in the future, the prospect of a full-text library of all documents from the first 225 years of English print points to the need for some careful re-thinking about the relation between scholarship and archival sources. As it now stands, the EEBO-TCP corpus amounts to 8.02 gigabytes of XML-encoded text and contains nearly 45,000 documents, for a grand total of well over a billion words (1,155,264,343 by our count). Confronted by the sheer expanse of a corpus several magnitudes larger than anything one could hope to read in a lifetime, early modern scholarship must now work to incorporate digital methodologies that enable a bird’s-eye view of large corpora, an approach that Franco Moretti has dubbed ‘distant reading’ (Moretti, 2007). DREaM has begun the work of making such a view possible.  Unlike EEBO, DREaM enables batch downloading of custom-defined subsets rather than obliging users to download individual texts on a one-by-one basis. In other words, it functions at the level of ‘sets of texts’ (sometimes called  worksets) rather than ‘individual texts’. Examples of subsets one might potentially generate include ‘all texts by Ben Jonson’, ‘all texts published in 1623’, or ‘all texts printed by John Wolfe’. A user-friendly interface makes subsets available as either plain text or XML-encoded files, and gives users the option to automatically name individual files by date, author, title, or combinations thereof (this file naming flexibility can be useful when interoperating with other tool suites).   The ability to generate custom-defined subsets is important because it allows researchers to explore the early modern canon with distant reading techniques, and to capture otherwise intractable data with visualizations such as graphs, charts, or other forms of graphic representation. On this note, another key feature of DREaM is that it allows users to transfer specially tailored subsets directly to the analytic interfaces of Voyant Tools (voyant-tools.org), a suite of textual visualization tools that collectively constitute the leading platform for open-access digital humanities research. 3 In fact, DREaM is actually implemented within Voyant Tools (version 2.0, not yet released, which provides much better support for very large text collections). DREaM thus provides a compelling example of a bridge between massive full-text repositories (that typically provide faceted searching) and more specialized analytic and visualization environments. By enabling simple transference between the EEBO-TCP archive and Voyant, DREaM has significantly expanded the range and sophistication of technologies currently available to researchers who wish to gain a broad sense of printed matter in early modern England.   Notably, however, DREaM does not aim to replace EEBO, or to supplant conventional forms of research. Rather, our goal is to simply add a new item to the scholar’s toolbox, and to increase transferability between distant reading methodologies and more fine-grained forms of analysis.   Negotiating the Complexities of Non-Standardized Spelling  Standardized spelling had yet to emerge in early modernity: writers had the freedom to spell however they pleased. To take a famous example, the name ‘Shakespeare’ has 80 different recorded spellings, including ‘Shaxpere’ and ‘Shaxberd’. As one might imagine, variance on this scale presents a serious challenge for large-scale textual analysis. How is it possible to track the incidence of a specific word, or group of words, if any given word could have an unknown multiplicity of iterations?  To address this problem, we enlisted the assistance of VARD 2, a tool that helps to improve the accuracy of textual analysis by finding candidate modern form replacements for spelling variants in historical texts. 4 As with conventional spellcheckers, a user can choose to process texts manually (selecting a candidate replacement offered by the system), automatically (allowing the system to use the best candidate replacement found), or semi-automatically (training the tool on a sample of the corpora).   After some preliminary training, we ran the TCP-EEBO corpus through VARD using the default settings (auto normalization at a threshold of 50%). Rather than using the ‘batch’ mode—which proved unreliable for such a big job—we wrote a script that normalized the texts on a one-by-one basis from the command-line. This process took about three days on a commodity machine. VARD normalized 80,676 terms for a grand total of 44,909,676 changes overall.  A careful check through the list resulted in 373 term normalizations that we found problematic in one way or another. The problematic normalizations amounted to 462,975 changes overall, or only 1.03% of the total number of changes. These results were satisfactory: our goal was not to make the corpus ‘perfectly normalized’ (an impossibility, not least because perfection is debatable in this context), but, more pragmatically, to make it generally normalized, which is the best one can reasonably expect from an automatic process. On this point, it is important to note that VARD encodes a record of all changes within the output XML file, so scholars will be able to see if the program has made an erroneous normalization.  Some of the problematic VARD normalizations seem to have derived from a dictionary error. For example, ‘chan’ became ‘champion’ and ‘ged’ became ‘general’. In other instances, the problematic normalizations were ambiguous or borderline cases that we preferred to simply leave unchanged. Examples include ‘piece’ for ‘peece’, and ‘land’ for ‘iland’. There were also cases where the replacement term was not quite correct: ‘strawberie’ became ‘strawy’ rather than ‘strawberry’, and ‘hoouering’ became ‘hoovering’ rather than ‘hovering’. We fixed as many of these kinks as we could by making adjustments to the VARD training file and running the entire corpus through the normalization process a second time.  Of course, it is not difficult to imagine scenarios wherein a researcher may prefer to work with original spellings rather than normalized texts. With such projects in mind, we have kept both normalized and non-normalized versions of the EEBO-TCP corpus.   The DREaM Tutorial Program  As noted above, one of the central objectives of DREaM is to create an interface that will maximize user-friendliness, allowing scholars with a minimal level of technical expertise to quickly and efficiently create subsets tailored for whatever specific research question they wish to pursue. We are building DREaM for our own research, but we also have a much broader pedagogical perspective in mind. To meet this objective, we have launched a pilot tutorial program, currently under way, that will teach scholars how to use DREaM, but will also point to ways in which DREaM could more effectively serve the demands of scholarly investigation.  In a series of tasks that build toward the production of a short case-study report, pilot users must articulate a detailed research question and provide a description of their argument. In addition to establishing a valuable feedback loop for the project, this assignment aims to nudge new users toward a more comprehensive, more practical understanding of how macro-scale textual analysis can complement scholarly practice. The key conceptual challenge, as we see it, hinges on new users’ ability to understand, and learn to negotiate, the gap between distant reading and more conventional means of engaging archival sources.  Our pool of pilot users derives from the membership of our parent project, Early Modern Conversions, a five-year interdisciplinary research initiative that has brought together a team of more than 100 scholars, partners, and graduate student associates from universities in Canada, the United States, England, New Zealand, and Australia. 5 Early Modern Conversions provides a propitious testing ground for DREaM because it is at the vanguard of early modern research, and because it entails a rich diversity of disciplinary approaches. Our presentation for DH2015 will report on the results of the tutorial program and on the progress of the project overall.   Screenshots     Figure 1. The DREaM interface. Search fields in the middle of the screen enable users to define a subset of EEBO-TCP texts by keyword, year, author, and publisher. Below the search field, an ‘Export’ button opens a dialogue box that offers the option of sending the subset directly to Voyant-tools.org, or downloading it as a ZIP archive. Users may also choose to download subsets as either plain text or XML-encoded files. A drag-and-drop mechanism (bottom) enables automatic naming of files within a subset by date, author, title, or combinations thereof.        Figure 2. A sample subset transferred to Voyant Tools. Beginning in the top left corner, one sees a word cloud representing the frequency of keywords in terms of font size. At a glance, it shows that the highest frequency words in the subset are ‘good’ and ‘come’. Below the word cloud, there is a summary that lists statistics for basic categories such as word count, vocabulary density, word frequency, etc. In addition, the summary lists words that have a notably high frequency for each year: ‘Rome’ and ‘death’ appeared with particular frequency in 1594, while ‘virtue’ and ‘envy’ stood out in 1612. Moving to the bottom left corner, one sees an ordered list of frequencies for each word in the corpus accompanied by a thumbnail graph that tracks the frequency of words over the 40-year delimitation. At a glance, the tool shows a significant spike for the word ‘knight’ in 1624. In the middle of the screen, a ‘Corpus Reader’ tool enables users to drill down into the corpus to examine the context for particular terms.  Notes 1. See http://eebo.chadwyck.com. 2. See http://eebo.odl.ox.ac.uk/e/eebo/. 3. See http://voyant-tools.org. 4. See http://ucrel.lancs.ac.uk/vard/about/. 5. See http://earlymodernconversions.com. ",
        "article_title": "DREaM: Distant Reading Early Modernity",
        "authors": [
            {
                "given": "Stephen",
                "family": "Wittek",
                "affiliation": [
                    {
                        "original_name": "McGill University, Canada",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            },
            {
                "given": "Stéfan",
                "family": "Sinclair",
                "affiliation": [
                    {
                        "original_name": "McGill University, Canada",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Milner",
                "affiliation": [
                    {
                        "original_name": "McGill University, Canada",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "media studies",
            "repositories",
            "content analysis",
            "interdisciplinary collaboration",
            "literary studies",
            "corpora and corpus activities",
            "archives",
            "bibliographic methods / textual studies",
            "english studies",
            "sustainability and preservation",
            "English",
            "renaissance studies",
            "data mining / text mining",
            "digital humanities - pedagogy and curriculum"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper presents an open-source web service providing researchers in all fields with state-of-the-art computational methods for several Document Image Analysis (DIA) steps. Research on automatic DIA focuses mainly on developing and refining automatic processing steps, e.g., text line extraction, binarization, and layout analysis. While many state-of-the-art methods perform satisfactorily, the algorithms applied to obtain the results are not easily accessible for other researchers. Just making the source available online is not sufficient, as it typically requires a cumbersome installation of required libraries and reading long manuals about the usage. Our approach is to directly make the methods available as web services that can be accessed via RESTful HTTP requests, the current state of the art in online web communication. Thus, the resulting services can be integrated easily into document processing workflows by any software engineer without specific knowledge of the mathematical and algorithmic details of DIA. We will build on standards for result presentation, such as the Text Encoding Initiative (TEI) 1 and the International Image Interoperability Framework (IIIF). 2   State of the Art  Our research is motivated by the availability of many different web-based tools for researchers with a humanist background wanting to do document image analysis (e.g.,  SALSAH [Schweizer and Rosenthaler, 2011], Transcribe Bentham [Causer and Wallace, 2012], or the Genizah project [Wolf et al., 2011]). Most of these tools were either developed to solve a specific task or lack the inclusion of (semi)-automatic methods. Several projects using web services for DIA have been proposed in recent years. One such example is the Document Analysis and Exploitation (DAE; Lamiroy and Lopresti, 2011) that provides different algorithms as web services and allows for workflow creation. Our aim is to expand this research with a focus on making the algorithms available for researchers with only little computer science knowledge by providing them also with simple web interfaces as showcases building on the web services and demonstrating how to use them to integrate computational methods into their research.   Methodology We propose an open-source framework for providing algorithms to the public. For this we designed a RESTful web service architecture exposing all information using the JavaScript Object Notation (JSON). The intention is to include a wide assortment of services for different tasks:  • Image processing and enhancement in order to make the desired content more easily visible or to make the processing of further automatic analysis simpler. Those methods include, for example, binarization methods (Otsu 1979), Laplacian of Gaussian (LoG), Difference of Gaussian (DoG).  • Document layout analysis methods allowing automatic extraction of texts, text lines, or images. These methods include pixel- (Wei, 2013) and interest point– (Garz et al., 2011) based approaches.  • Optical Character Recognition (OCR) to support the transcription of the documents.  • Methods for palaeographic analysis, such as script identification (Ghosh et al., 2010), writer identification (Fiel et al., 2013), and water mark analysis.  • Methods for feature extraction and feature selection, so that computer scientists can directly work on extracted meta-information without any specific knowledge in DIA. For example, the following methods are included: Local Binary Patterns (LBP; Nicolaou et al., 2014), Scale-Invariant Feature Transform (SIFT; Lowe, 1999), Gabor features (Chen et al., 2010), standard feature search algorithms, as well as several feature selection methods (Wei et al., 2014).  • Machine Learning algorithms: Support Vector Machines (SVMs), k-nearest neighbor algorithm (k-NN), Gaussian Mixture Models (GMMs). 3   •  Evaluation metrics for the automatic assessment of results and to allow computer science researchers to compare their systems. There we will build on the standards laid out in DAE .  Figure 1. Conceptual overview of the proposed D ivaServices framework. Access to the provided methods and tools would all be standardized using HTTP requests and JSON as input/output format.  Besides a large set of own implementations we will integrate several open-source software like Tesseract 4 and OCROpus. 5 This enables fast integration of many available image processing algorithms that have been in development for years and proven to produce reliable results.  A high-level overview of the proposed framework is provided in Figure 1. Access to the provided tools and algorithms would be standardized across all possible end-user applications using simple HTTP requests and JSON as input/output format. For accessing the methods we will follow the proposed URL format for RESTful services. The current state of D ivaServices is available at http://divaservices.unifr.ch. Using GET requests allows for browsing the available services. We are in the process of developing a web front-end that will allow for automated prototype creation of all available algorithms in order to allow for experimenting with them.  Since we only provide algorithms, creating specific workflows is left to developers designing client applications and can therefore be designed targeting the specific need of end users. At a later stage we aim at directly implementing some of the more common workflows directly into D ivaServices.  As proof of concept 6 a simple histogram-based line segmentation method was exposed using the proposed framework. This service was then integrated into the D ivaD iaWT, a web-based interface that allows for the creation of transcriptions. The user interface of the D ivaD iaWT is shown in Figure 2.     Figure 2. Overview of the DivaDiaWT. The original image is displayed on the left side, transcriptions on the right side. Transcriptions can be displayed in Layout mode, where they are aligned with the original image.  The line segmentation service is used on a user-marked region and automatically extracts lines from there. In Figure 3 a user created a box using his mouse around a region that he wants to have automatically processed into text lines. The result of the automated text line segmentation is shown in Figure 4.  We have set up a web server on which we run our RESTful web services. When the user wants to automatically segment a text area, a POST request is made to the server containing the following JSON: {  \"url\": \"http://www.e-codices.unifr.ch/loris/bbb/bbb-0360/bbb-0360_001r.jp2/full/pct:25/0/default.jpg\",  \"top\": \"300\",  \"bottom\": \"500\",  \"left\": \"190\",  \"right\": \"750\" } Listing 1. Example of a JSON sent together with the POST request for automatic line segmentation. The url points to the source image; the four location values mark the region within the image that the user selected. [  {  \"bottom\": \"180\",  \"left\": \"95\",  \"right\": \"469\",  \"top\": \"156\"  },  { …} ] Listing 2. The JSON the server sends back to the client application containing all bounding boxes of the detected text lines. The web service downloads the image and processes the region marked by the user. This can lead to several detected text lines, and the server responds with a JSON file containing the bounding box of each detected text line: [  {  \"bottom\": \"180\",  \"left\": \"95\",  \"right\": \"469\",  \"top\": \"156\"  },  { …} ] The D ivaD iaWT parses this information and presents it as shown in Figure 4.      Figure 3. The user marked a box (visible as grey shaded area) that should be automatically divided into text lines using an algorithm provided by D ivaServices.      Figure 4. The result of the automated segmentation process.  Notes 1. http://www.tei-c.org.  2. http://www.iiif.io.  3. See  for an overview of classification algorithms in script identification.  4. http://code.google.com/p/tesseract-ocr.  5. http://github.com/tmbdev/ocropy.  6. Code of the proof of concept is available at https://github.com/lunactic/Diva-WebService. ",
        "article_title": "DIVAServices – A RESTful Web Service for Document Image Analysis Methods",
        "authors": [
            {
                "given": "Marcel",
                "family": "Würsch",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Rolf",
                "family": "Ingold",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            },
            {
                "given": "Marcus",
                "family": "Liwicki",
                "affiliation": [
                    {
                        "original_name": "University of Fribourg, Switzerland",
                        "normalized_name": "University of Fribourg",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/022fs9h90",
                            "GRID": "grid.8534.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "programming",
            "English",
            "internet / world wide web",
            "information architecture",
            "image processing"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " I propose to study prosody of modern poems by analyzing recordings of their authors’ voice reciting their poems and producing forms similar to a music score. An example is shown in Figure 1. In the following, such a notation will simply be called the music score of a poetry reading.     Figure 1. First I will review developments in three partly overlapping areas related to this study: relationship of poetry recitation to prosody, studies of recorded voice, and prosody of free verse. Then I will describe the computer-assisted method for producing the music score by analyzing recorded voice. Some influential papers have asserted that prosody can’t be studied by analyzing the voice. I will discuss their arguments and show some results indicating that, for many modern poets, analyzing their voice can be an effective way of studying their prosody.  Until about half a century ago, linguists, prosodists, psychologists, and sometimes poets actively participated in prosodic studies using human reading of poems. In the mid-1910s, Patterson developed a device called a ‘voice recorder’ that seems to have been a voice amplitude recorder, and poet Amy Lowell used this equipment to measure, in effect, time between stressed syllables in readings of her own and others’ poems. In 1956, Seymour Chatman analyzed recordings of eight people, including the poet himself, reading Frost’s ‘Mowing’. However, those studies were severely attacked by critics W. K. Wimsatt Jr. and Monroe C. Beardsley (1959). They believed that there is an ‘abstract metrical pattern’ in a poem that cannot be studied by analyzing a performance or a set of performances: ‘There are many performances of the same poem—differing among themselves in many ways. A performance is an event, but the poem itself, if there is any poem, must be some kind of enduring object’ (587). Shortly afterwards, linguist Roman Jakobson (1960) concurred, stating that ‘meter, or in more explicit terms, verse design, underlies any single line or, in logical terminology, any single verse instance’ and ‘A variation of verse instances within a given poem must be strictly distinguished from the variable delivery instances’ (365–66). After this, according to Paul Kiparsky (2010), ‘Generative metrics has turned Jakobson’s distinction into an exclusion: Delivery (recitation and text-setting) is not considered to be in the province of metrics’ (slide 6). Among literary prosodists, too, not many have ventured into serious analysis of poets’ recorded voice.  In the 1970s, however, Reuven Tsur started to radically change the notion of ‘prosody’. He has formulated an extensive ‘Cognitive Poetics’, encompassing phonology, prosody, and philosophy. More recently, Paul Kiparsky has proposed to expand metrics into ‘broad metrics’, which includes ‘delivery’ (2010). Among literary prosodists, Harvey Gross (1964) has used T. S. Eliot’s recordings for the analysis of his prosody. Poet and scholar Charles Bernstein edited the important book  Close Listening (1998a), wherein Bernstein stressed the importance of performance (vocal and print) in prosody. More recently, Marjorie Perloff and Craig Dworkin edited another important book,  The Sound of Poetry / The Poetry of Sound (2009).  In the related area of sound study, Jason Camlot (2003), Derek Furr (2010), and Matthew Rubery (2011) have been studying audiobooks and other collections of recordings. Recently, an inter-disciplinary area of ‘voice studies’ seems to be forming. Although Wimsatt and Beardsley expressed hope that traditional metrical analysis would be applicable to modernist and later poems, it turned out to be difficult. Several prosodists, including Derek Attridge (1995) and Alan Holder (1995), have been developing prosody for non-metrical verse. Literary critics Helen Vendler (2007) and Marjorie Perloff (2009) have also analyzed prosody in several modernist and, in the case of Perloff, postmodernist poems with non-metrical rhythms, sometimes listening to the poet’s recorded voice.  I hope the method described in the following will be usable in the above fields of study. First, the recorded voice is analyzed by using a combination of Wavesurfer and PRAAT speech-analysis software packages. The reason for using two rather similar packages is to take advantage of the user-friendly human interface of Wavesurfer and also the superior pitch analysis and scripting capabilities of PRAAT. The human operator first segments the recorded voice data into syllables and silent/breathing segments, and inputs them into Wavesurfer. The data is imported to PRAAT, and the human operator inputs stress pattern (whether the syllable is heard as stressed or not) and also the line number, in the poem, to which the syllable belongs. After this the analysis is almost automatic, and the following data are obtained from PRAAT: the starting time and duration of each syllable or silence, and the vocal power (loudness) for a short time segment. A perl program was written to convert this into an input program for LilyPond, a software for producing scores. The average over the syllable duration of fundamental formant f0 is taken as the pitch, which determines the position of the note. The duration is converted to the note length (assuming MM=120). The longtime average of power is converted into dynamics. A stressed syllable can be represented by an accent mark next to the note, by a colored note, or both. The resultant music score of poetry recitation will be shown, synchronized with the voice, at the conference. The music score obtained as above is, of course, only an approximation of the original recorded voice. It is known that ‘pitch’ is a psychological quantity and does not always correspond to f0. Also, f0 of talking/reciting voice usually changes continually, and its average over a time segment is not an accurate representation. The perception of a stress on a particular syllable is also known to be a psychological (physically difficult to measure) phenomenon, and its presence or absence is sometimes quite difficult to judge, and can be controversial. However, by using music score notation for representing the human voice reciting poetry, much relevant information (not only the stress pattern) is conveniently concentrated in a small space, facilitating discussion. The above-mentioned arguments by Wimsatt and Beardsley (1959) and by Jakobson (1960) rely on two assumptions. First, they assume that poetry reading is a performance, and a performance can change arbitrarily and cannot be trusted, even if the performer is the author. Second, they assume that for a poem there is one correct ‘absolute meter’ or ‘verse instance’, which is, in Wimsatt and Beardsley’s terms, ‘something that can be read and studied with the help of grammars and dictionaries and other linguistic guides . . . The meter is something which for the most part inheres in language precisely at that level of linguistic organization which grammars and dictionaries and elementary rhetoric can successfully cope with’ (588). So it can and should be elucidated easily, without going so far as to study any ‘performance’.  To see if the first assumption holds, I have studied stress patterns in multiple recordings of the same poem by poets such as W. B. Yeats, Robert Frost, William Carlos Williams, Ezra Pound, T. S. Eliot, Kenneth Rexroth, W. H. Auden, Allen Ginsberg, and Galway Kinnell. I also analyzed stress patterns of non-authors reading works of those poets and compared them with the author’s reading. The result indicates that the author’s stress pattern is quite stable over years (though there are exceptions), while non-author readers (including poets) tend to read the poem in quite different stress patterns (especially in modern poems that do not have classical metric patterns) from the author’s and from each other’s. That stands to reason, as the stress pattern perhaps is one of the important characteristics in composing a poem, even when (or perhaps the more when) the poem does not have classical meter. In other words, for many authors, the stress pattern may be too important to change arbitrarily (though sometimes change may occur due to the change of heart of the author). On the other hand, the reader often fails to guess the author’s intended rhythm (or find another rhythm) in a modern poem. In short, the first assumption does not seem to hold, at least for many poets reading their own poems.  The second point, the assumption of the presence of one ‘absolute meter’ per poem, is difficult to apply to modern poems such as free verse and syllabic verse. For them, modern prosody and sound/voice studies will be able to contribute much, although as Bernstein notes, quoting Peter Quartermain, ‘The poet’s voicing of a poem should not be allowed to eliminate ambiguous voicings in the text; nor should the author’s performance of a poem be absolutely privileged over that of other readers and performers’ (11).  The method of analyzing the voice of the poet and the other readers given in this paper will, I hope, contribute to those studies. Relevant software developed in this study (perl programs and PRAAT scripts) will be made available to researchers in the near future. ",
        "article_title": "Music Score Representation of Poetry Reading: Can Prosody Be Studied by Analyzing the Author’s Voice?",
        "authors": [
            {
                "given": "Takeo",
                "family": "Yamamoto",
                "affiliation": [
                    {
                        "original_name": "Graduate School of Literature and Sociology, The University of Tokyo, Japan",
                        "normalized_name": "University of Tokyo",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/057zh3y96",
                            "GRID": "grid.26999.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "literary studies",
            "video",
            "english studies",
            "audio",
            "English",
            "multimedia",
            "prosodic studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " One key issue in the study of humanities and social sciences is to figure out clearly the formation of certain important ideas and concepts. Traditionally, this kind of investigation is usually carried out based on informed speculations at best. Thanks to the new methods informed by digital humanities study, the elusive process of idea formation can now be examined more accurately. By means of these methods, we can observe the trajectory of idea formation, tracing important development of ideas in terms of key word analysis. As a result, we can not only pinpoint the origin of ideas but also realize how their meanings develop and change in different historical contexts.  It is almost impossible for any visitor to today’s China to fail to notice the popularity of the term ‘wenming’ (civilization/civility) in public life. Various kinds of posters commonly invoke the term ‘wenming’ in their slogans to exhort citizens to have manners, to behave properly in public life. For example, even in the public restrooms, salient slogans such as ‘a little step forward, a giant step toward wenming’ are used to remind people of urinating in the right place. Indeed, as is well known, since the late 19th century, the term ‘wenming’ had been widely used in China to honor the European ‘modern/advanced’ ways of life and institutions. Many Chinese intellectuals had come to describe the European nations as ‘wenming guo’ (civilized nations), which are paradigms for China to emulate. To be sure, ‘wenming’ was used as a prefix to describe explicitly many things coming from the West. For example, the Western gentleman’s cane was translated in China as ‘wenming gu’ (civilized stick). In short, the popularity of the term ‘wenming’ is very indicative of the enchantment of the modern (Western) way of life in modern China, especially from the late 19th century to the mid-20th century.  However, after the Chinese Communist Party took power in 1949, many Western things were denounced as ‘capitalistic’ and ‘imperialist’ and therefore no longer appreciated, according to the official ideology. On the other hand, nationalism and patriotism are emphasized more to boost national pride. Still, intriguingly, under such circumstances, the term ‘wenming’ did not lose its allure and became even more popular. PRC government propaganda has adopted this term to strongly promulgate ‘civic orders’, to stress the importance of proper manners in public life. In other words, ‘wenming’ has come to stand for ‘civility’ rather than ‘things explicitly Western’. In a way, that many behavior codes originating from the West are now promoted as ‘wenming’ ways of life—i.e., universal, civilized ways of life—symbolizes a cultural policy and strategy aiming to maintain a balance between modernization and westernization.  My study of the changing meaning of the concept ‘wenming’ (civilization/civility) in modern China is based on the rich resource provided by the database of Modern Chinese Intellectual and Literature History (1830–1930), which includes more than 120 million words from original important historical texts. In addition, the whole content of  People’s Daily (1946–present) is now available online, which can provide a lot of related information concerning ‘wenming’ in the modern period. In order to explore deeply the historical significance of this idea in modern China, I aim to study it with the help of the digital humanities. First of all, I will utilize the ‘pat-tree’ and ‘key word search’ methods to analyze the discourse contexts in which ‘wenming’ was used in various historical situations. Moreover, I will investigate the frequency of the appearance of this key term in important journals during the period between the late 19th century and the early 20th century, a crucial period for the construction of modern China’s collective identity. Third, I will try to analyze the significance of its appearance with other important terms, such as ‘modernity’, to further explore how these ‘family terms’ were utilized in shaping the modern Chinese collective identity and consciousness.   My initial investigation of the origin of the modern meaning of ‘wenming’ has revealed that modern Japan might be the source. In the late 19th century this term was transmitted to China from Japan, where the Western nations were also highly regarded as the models to follow. Indeed, the historical fact of this development is strongly supported by the findings based on digital methods. By surveying the aforementioned database in terms of the key word ‘wenming’, I have found that the high frequency of using the term ‘wenming’ in its modern meaning appeared in a number of famous journals published after in Japan by the exiled Chinese intellectuals and students there. By more clearly pinpointing the trajectory of the idea ‘wenming’ transmitting from Japan to China, my study will illuminate further the complicated relationship between China and Japan in terms of their modernization and Westernization processes.  By historicizing and denaturalizing the key term ‘wenming’ in the discourse of modern China, my study will not only closely probe into a new set of issues, shedding new light on our understanding of modern Chinese identity, but also demonstrate the power and importance of digital methods in the study of intellectual and cultural history. ",
        "article_title": "The Enchantment of Civilization/Civility: the Digital Humanity Study of the “Obsession” with “Wenming”(文明) in Modern China",
        "authors": [
            {
                "given": "Jui-sung",
                "family": "Yang",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University, Taiwan, Republic of China",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "cultural studies",
            "historical studies",
            "English",
            "translation studies",
            "asian studies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The History of Medicinal Plants from the New World Case Study As proof of concept for semantic interoperability, our work focuses on cultural heritage data relating to the history of medicinal plants in the Low Countries roughly from the moment that natural drug components from the New World started penetrating Europe until the introduction of chemical and synthetic drugs in the nineteenth century (i.e., 1550–1850). The resulting system aims at enabling a historian to investigate the trajectories of colonial drug components in the Low Countries, and the patterns of correspondence, trade, and knowledge exchange that lie underneath it. Our case study is meant to provide the aggregated data required for new information to surface, which an individual researcher is unable to produce by means of traditional historical research alone. The factors leading to the adoption of a given drug component are revealing of the dynamics of the medical market at that historical period of time. Recent research showed that drug adoption has only partly to do with therapeutic qualities. Equally important are non-medical factors such as public acceptance, marketing, and availability (Pieters, 2004; Gijswijt-Hofstra et al., 2002; Friedrich and Müller-Jahncke, 2009). A special focus on the trajectories of individual drugs has proven itself a valuable research method in this respect. For colonial botanical drugs in the early modern period, several publications have paved the way for this novel approach in the history of tropical medicine (e.g., Vöttiner-Pletz [1990] on guaiacum, used to treat syphilis; Foust [1992] on rhubarb; Jarcho [1993] on Peruvian bark). Another interesting aspect of our case study relates to the circulation of knowledge and goods in the early modern period with particular attention to exotic botany and pharmacy in the Netherlands (as in, e.g., Wilson, 2000; Schiebinger and Swan, 2005; Cook, 2007; Dauser et al., 2008; Dupré and Lüthy, 2011; Snelders, 2012; Van Gelder, 2012). Knowledge about the vicissitudes of individual drugs across time and place may enrich current debates about the circulation of knowledge and goods. Moreover, sequenced data about colonial drugs may reveal trajectory patterns and the mechanisms of trade and exchange in the early modern period.  These aspects of our case study make it thus an excellent ground for digital research methods experimentation and a resource for other humanities areas, such as pharmacy, ethno-botany, philosophy, and science of the Enlightenment period, and socio-economic studies of the respective colonial trade period.    Creating Time Capsules 2  Our data sources are of two main categories: structured database data, referring to linguistic, pharmaceutical, and botanical data, such as the Dutch Chronological Dictionary and the Economic Botany Database; and unstructured free text corpora collections, such as pharmacopoeias, medical books, and other Early Dutch text corpora. The complete list of our current data sources is illustrated in Table 1.   Figure 1. Time capsule architecture overview.  An important consideration and challenge in our database data lie in their various formats (relational database, Excel tables, etc.) and in our text data (OCR errors and transcribed data in Latin or Early Dutch). Most importantly, a serious issue lies in the spelling and semantic variation of both drug component as well as plant names and in the ambiguity of their taxonomic classification, which introduces fuzziness into our data classification. For example, a drug ingredient may or may not originate from a certain plant, which may or may not belong to a given plant taxonomy class and may or may not correspond to a given contemporary name. For the semantic integration of our data, we adopt a linked-data approach whereby our data sources are first converted into RDF and then linked to our Historical Drug Components Ontology, so as to enrich existing information of the respective KB, as illustrated in Figure 1. In order to address the issues associated with variation, we incorporate historical lexicon resources, and we further enrich these by application of automatic spelling variation detection (Reynaert, 2014). The issue of uncertainty in our data classification is currently addressed by adopting a fuzzy classification approach, whereby an ambiguous instance may be classified into more categories—e.g., a name can be an instance of both a plant concept and a drug component concept. Finally, our structured data sources are used as a resource in detecting relevant information about drug components and their potential use in text documents. The resulting semantic annotations in these historical text corpora are in turn also converted into RDF and automatically linked to our structured sources, so that original historical written evidence is provided to the researchers.     Structured Data     Botanical, pharmaceutical, historical, and image resources     Ontology of Historical Drug Components  Historical drug components information found in historical pharmaceutical sources   National Museum for the History of Pharmacy    Economic Botany Database  Metadata of objects in the Economic Botany Collection   Naturalis Biodiversity Center    BRAHMS  Metadata for 1.2 million records of plant collections   Naturalis Biodiversity Center    Snippendaalcatalogus  Inventory of plants of the Snippendaal’s 1646 Amsterdam botanic garden    Hortus Botanicus Amsterdam     IrisBG  Current information about the plants of the Amsterdam botanic garden, including pictures    Hortus Botanicus Amsterdam http://dehortus.gardenexplorer.org/     Dutch Nature Images Collection  Images of flora and fauna in the Netherlands and the Dutch Antilles     Netherlands Institute for Sound and Vision—Stichting Natuurbeelden  http://www.natuurbeelden.nl/     RADAR  Geographical and research data about botanical macro remains collected during archaeological excavations on Dutch territory   Cultural Heritage Agency (RCE)   Lexical resources    PLAND  Plant names in various Dutch dialects, including sources, dates, locations, and name distributions    Meertens Institute http://www.meertens.knaw.nl/pland/     Chronological Dictionary  Historical/etymological Dutch dictionary including first observed sources and dates     Meertens Institute http://dbnl.org/tekst/sijs002chro01_01/     GiGaNT  Diachronic Dutch lexicon (6th century–present), including spelling variations, proper names, and morpho-syntactic information    Institute for Dutch Lexicology (INL) http://www.inl.nl/onderzoek-a-onderwijs/projecten/gigant    Free-text data    Looted Letters  Transcriptions and metadata of c. 3,500 letters, taken as loot from Dutch ships during the Anglo-Dutch wars (1652–1805)    Meertens Institute http://www.gekaaptebrieven.nl/     Letters as Loot  Linguistically analysed subset of c. 1,000 letters from the Looted Letters collection    Institute for Dutch Lexicology (INL) http://brievenalsbuit.inl.nl/     CKCC corpus  Transcriptions and metadata of c. 20,000 letters from the correspondence of 17th-century scholars in the Netherlands    Huygens ING (and partners) http://ckcc.huygens.knaw.nl/     DBNL corpus  Subset of DBNL 16th- to 19th-century texts, including literary, medical, biographical, and other texts     DBNL–National Library of the Netherlands (KB) http://dbnl.org/index.php     Pharmacopoeias  Collection of scanned images of apothecary data   Google Books   Apart from data aggregation, our time capsules should allow for information re-contextualisation. This issue is partially addressed by our semantic data integration, thus providing the researcher with different but related information sources and aspects (botanical, linguistic, historical). Another important aspect in contextualisation lies in making explicit the relation of data to space and time, thus allowing the virtual spatio-temporal ‘reconstruction’ of the knowledge transfer trajectories. For this purpose, we exploit a combination of spatio-temporal information in our data sources with spatio-temporal reasoning using OWL-Time (W3C, 2006) and SOWL (Batsakis and Petrakis, 2011), an ontology framework for representing and reasoning over spatio-temporal information in OWL.  In the current version of our system, our structured and free-text sources are being processed and integrated, while a demo interface is gradually developed for querying the aggregated data. Our next steps include the development of an exploratory search interface and visualisations of data overviews. A particular challenge lies in the development of a user-friendly web interface for querying our RDF data using SPARQL (W3C, 2013). Future work finally includes the application of our system to a different case study and different types of data (text and images), so as to test its portability and its functionality in a different domain. Notes 1. From the early ’70s until his death in 1987, Warhol selected items from correspondence, newspapers, gifts, photographs, and other material and preserved them in sealed boxes, which he marked with a date or title. These so-called time capsules provide a unique view into Warhol’s private world, as well as an enlightening window on the interrelatedness of culture, media, politics, economics, and science in the 1970s and 1980s.  2. http://www.timecapsule.nu/.  ",
        "article_title": "Creating Time Capsules for Colonial Botanical Drugs in the Early Modern Low Countries",
        "authors": [
            {
                "given": "Kalliopi",
                "family": "Zervanou",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Wouter",
                "family": "Klein",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Peter",
                "family": "Van den Hooff",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Marc",
                "family": "Bron",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Frans",
                "family": "Wiering",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": "Toine",
                "family": "Pieters",
                "affiliation": [
                    {
                        "original_name": "Utrecht University, The Netherlands",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Paul Arthur, University of Western Sidney",
        "date": "2014-12-19",
        "keywords": [
            "digitisation",
            "historical studies",
            "xml",
            "metadata",
            "natural language processing",
            "English",
            "resource creation",
            "internet / world wide web",
            "linking and annotation",
            "knowledge representation",
            "and discovery",
            "semantic web",
            "ontologies"
        ],
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]