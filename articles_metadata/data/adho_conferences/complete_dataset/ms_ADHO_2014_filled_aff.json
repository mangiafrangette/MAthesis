[
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction  Building on the Derrida’s archival theory (1998) through engagement with critical making (Ratto, 2011), reflective design (Sengers, et al., 2005) and empirical investigations into social networking sites (SNS) as emerging archival forms, this paper introduces the FeverBook project (FB). In doing so, it provides evidence of novel distributions of archontic power, or the ordering powers afforded to the actor-network responsible for the construction and maintenance of the archive, in SNS contexts. This paper points to the need for an archival theorization of SNS that extends beyond traditional concepts of the archive. Broadly operationalized as representative collections of cultural materials appraised and preserved in a standardized way for unspecified future use, traditional archives are central to the production of historical knowledge. For cultural histories, it is archives ‘not compiled with an eye towards history,’ (Farge 2013: 7) but compiled with an eye towards the mundane from which histories of daily life are written. However, massive databases like those found underlying SNSs such as Facebook are recreating what once was stored in traditional archives similar in form and function to the French judicial archives (Farge, 2013). Here the personal digital archive (PDA) (Marshall, 2008) extends the constitution of its actor-network to include myriad users, appraisers, programmers, database managers, graphical user interfaces (GUI), media formats, and external institutions involved in the use of archival materials.  Despite recent shifts of archival theories from the pre-digital realm (Derrida, 1998; Foucault, 1982; Ridener, 2009) into the digital (Ernst & Parikka, 2013; Garde-Hansen, et al., 2009; Manoff, 2004; Manovich, 1999; Mayer-Schönberger, 2009; Sacasas, n.d.), scholars have only begun to consider the applicability of extant archival theories to SNS-platforms. What work has been done (Acker & Brubaker, 2014; Mitchell, 2014; Garde-Hansen, et al., 2009) comprises the beginning of a lengthy discourse about emerging forms of digital archives. As an entry into this discourse, this paper leverages Facebook’s user affordances to explore the applicability of Derrida’s archival theory to SNS-platform-based PDAs (Derrida, 1998).  The exploration of archontic power distribution in SNS-platform-based PDAs was occasioned by critical making, defined as ‘a mode of materially productive engagement that is intended to bridge the gap between creative physical and conceptual exploration,’ (Ratto, 2011, p. 252). Such critical making resulted in the production of the FB exhibit and webpage, which present an empirically grounded fictive SNS profile through which users can explore and rethink core aspects of Derrida’s archival theory (1998) in the context of SNSs.   2. Methods  To generate narrative material for the fictive profile at the center of FB, the FB research and design team[1] conducted two rounds of interviews with a total of twelve informants. Initially, four Facebook users were recruited for pilot participation through convenience sampling, leading to semi-structured interviews lasting 60 to 90 minutes. Eight additional informants were subsequently recruited through similar methods. Analysis of data generated during interviews, including photographic data from participants’ Facebook profiles, took the form of content analysis and occurred concurrently with close readings of Derrida’s Archive Fever (1998). Following completion of first-round interviews, FB construction began via iterative design (Ishii, Kobayashi, & Arita, 1994). Subsequent to each design meeting, the team engaged in journaling to understand their relationship to proposed interface changes. Journal content then served as a basis for reflection and future design decisions (Sengers, et al., 2005). Upon completion of the initial prototype, the design team then conducted two rounds of user testing.  3. New archival characteristics: Generative troubling  The ‘archontic’ is a key term for Derrida.  The etymological origins of the term ‘archive’ (arkheion) are rooted in ancient Greek social hierarchy and the domesticity of those residing at the hierarchy’s pinnacle (Derrida, 1998). The arkheion refers to the domain, literally the residence, of the archons or ‘those who commanded’ (Derrida, 1998, p. 2).  Thus, the archontic refers to the ordering powers afforded to what might be called the actor-network responsible for the archive – powers to include and exclude, to align and alienate, to consign or fragment.  Findings from the present research, including the user-practice of what is herein coined ‘generative troubling,’ evidence a direct relationship between the interior and exterior of the archive that exists in stark contrast to the a priori nature of the archive proposed by Derrida (1998).  For Derrida, the archive is inherently violent. The mechanism by which an artifact enters the archive is a process of filtering artifacts-in-the-world through the ordering and inscribing function of the archon (e.g., through the act of appraisal). Such filtering yields an archive only theoretically related to the world it is intended to represent, related purely via the filters of archontic power. Moreover, in working with the archival subject – the object of violence – the historian or archival researcher troubles, perturbing the archival body and allowing narrativity to arise from archived artifacts. Troubled in one way, a given archive, a set of artifacts representing but unattached to the world, yields narrative x; troubled in another way, the same archive yields narrative y, where x and y are topologies of the same set.  In any troubling event, the traditional archive yields only narratives capable of being constructed from that which has already been archived.  However, the a priori nature of the archive and its limited potential narrativity break down in the SNS-platform-based PDA as a result of its capacity for generative troubling. Such breakage occurs at the level of direct communication within the Facebook user’s timeline – when the archive itself simultaneously constitutes a repository of potential narrative elements and a site of active discourse, the purely a priori is transformed through the addition of a posteriori characteristics. The archive constituting an active site of discourse no longer bears only a theoretical relationship to the world it archives – the communicative act is simultaneously both act-in-the-world and archival artifact. Instead, the SNS archive of Facebook breaks a ‘fourth wall’ between archive and world, necessitating reconsideration of archontic violence. (For further discussion of the fourth wall, see the coverage of Diderot’s grand mur in Bender & Marrinan, 2010). Here the impetus for narrativity doubles as the impetus for the generation of new elements in the archival set, thus expanding the set from which archival narratives may be derived. The following excerpts from separate interviews with Stephanie and Alexandra (pseudonyms) illustrate the point: Alexandra: “I went really dark […] because people respond to more personal stuff, I was hoping to get more encouraging words and it, I guess it was dark enough that people didn’t want to respond at all.” Stephanie: “Which is why I put it up [posted it to my timeline]. Just to see what people come up with. Because you know, a whole bunch of words come in and if one of them happens to … you know … make me feel way better for whatever reason, then great.” Both Alexandra and Stephanie discuss the communicative function of Facebook’s timeline.  In Alexandra’s case, she describes the execution of a communicative act intended to generate new archival content in hopes of alleviating emotional distress. Although Alexandra’s attempt was unsuccessful (which itself warrants further investigation), the successful communicative attempt Stephanie describes is similar: both participants actively generated content and posted to their timelines, archived in the Facebook databases, with the goal of generating situation-specific responses. Through Alexandra and Stephanie’s acts of appraisal (deciding that a generative communication warrants inclusion in their SNS-based PDA), other users are prompted to add related content to the SNS-archive. The act of generative troubling indicates that the relationship between SNS-based PDAs and the world from which the archival set is gathered is no longer strictly a priori. When otherwise worldly, or extra-archival, communication occurs directly in and through the archive, the wall between a priori and a posteriori is dismantled. Through the ability to leverage Facebook’s timeline for direct communication, the archontic power underlying the construction of SNS-based PDAs is distributed among all users responsible for content generation at the GUI level. As such, the responsibility for appraising material representative of the mundane is no longer solely held by traditionally defined archivists; no longer is the archon a homogenous body of standardized intent distributed among a defined body of archivists. Instead, as Alexandra and Stephanie have illustrated, the ‘user’ in SNS-based PDAs becomes the ‘user-appraiser,’ or both an archival subject and the archivist. Although the archival act of preservation remains in question in the context of SNS-archives, such user-appraiser empowerment fundamentally separates the SNS-platform-based PDA as an archive of the mundane from the traditional form of mundane archive described by Farge (2013). The archive-theoretical differences resulting in such separation will affect the epistemology underlying the future production of cultural historical narratives – a change in the ‘how’ of mundane archival construction implies a change in the ‘how’ of cultural historical knowledge production.   ",
       "article_title":"Generative Troubling in Emerging Archival Forms",
       "authors":[
          {
             "given":"John",
             "family":"Seberger",
             "affiliation":[
                {
                   "original_name":"University of California, Irvine, United States of America ",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "digitisation - theory and practice",
          "literary studies",
          "sustainability and preservation",
          "repositories",
          "archives",
          "social media"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The results of what has been termed the scientific method infuse every aspect of modern life.  Science, as a contested enterprise, affects and is affected by political, economic, technological, social, cultural, religious, and ethical factors. Sensational cases illustrate that people are interested scientific data and the process that produces scientific knowledge.1  Scientific data, and evidence about how it was created, shaped, and interpreted, serve as resources for humanistic studies in disciplines including journalism,2  anthropology,3 history,4 medicine,5 and literary studies.6  If scientific information is to be useful for current and future scholarship, we must answer three fundamental questions: (1) Which information is preserved and made accessible? (2) What evidence does it provide? and (3) How can it be used?   Future users of scientific records will provide conflicting answers: Archivists cannot anticipate the precise nature of future research, and selecting or arranging materials with the expectation of one particular use may preclude transformative uses. If the goal of preservation is to make accessible a record of science that is amenable to disparate analyses and interpretations, information professionals must identify and preserve evidence regarding the process of scientific production, not just the results of the process.  Given that each lab operates differently, preservation programs must use interdisciplinary modes of analysis and techniques, as must users of archives.  The fields of digital humanities, digital curation, data curation, and digital preservation have long been interdisciplinary, and are becoming increasingly hybrid in terms of the disparate theoretical frameworks and tools of analysis that they utilize. It may well be true that the lines between archives, libraries, digital humanities, and publishing are becoming more fluid, yet we believe that archives are well positioned to serve as centralized sites where these intersections lead to greater collaboration and knowledge dissemination.7   2. Academic Archives (AA) Academic and research institutions have traditionally played a leading role in preserving the ‘papers’ (i.e. personal archives) of faculty, including scientists.8.  Archival guides devoted to scientific and technical documentation emphasize that reports, correspondence, photographs, scrapbooks, lab records, and supplementary documentation hold as much continuing research value as formal research products (publications and processed datasets).  Ideally, archivists act upon the basis of the core archival doctrine, respect des fonds: the idea that grouping records by the function or activity that led to their creation best preserves their value as evidence.9 To accomplish this, archivists draw upon concepts and techniques from other disciplines, to protect the records’ provenance and original order.   3. Digital Preservation (DP) DP’s core concepts extend archival doctrine and practice.  DP establishes requirements and processes for maintaining authentic and trustworthy digital objects.10  The Open Archival Information System Reference Model (OAIS) and Trusted Digital Repositories Framework provide recommendations concerning system design, policy development, and institutional commitments.11  Research regarding digital personal papers (including faculty archives) recommends particular acquisition, arrangement, descriptive, and access practices that preserve contextual information regarding the creation and use of digital records, such as documentation about the research environment and the use of communication/dissemination technologies.12   4. Data Curation (DC) DC techniques complement DP and help scholars manage datasets of continuing informational value, by making them preservable, reusable, and computationally reproducible;13 and by developing “high-functioning” metadata.14  While DC provides a rich data-preservation toolkit, it focuses more attention on data than on ancillary documentation, which provides evidence of the ways in which that data was created, used, or interpreted. If the goal is to document the research environment and process, DC methods are but one (albeit, crucial) element in a broader archival strategy.    5. Anthropology Documenting scientific activity, one of many social arenas in which knowledge is constructed, is best accomplished by direct observation. In the absence of ethnography, archives play an important documentary role by providing insight into the context underlying scientific fact creation. Anthropology’s focus on praxis and its reflexive methodological and epistemological underpinnings offer meaningful points of departure for archivists seeking to capture a nuanced record of scientific processes and activities.15 One goal of science is to produce authoritative, published documents: a materialized fact.16  The archival challenge lies in documenting social factors at play from data generation through the various stages of processing and interpretation, to the final “point of stabilisation.”17  Archives can lay bare the social factors that are all too easily stripped out when the fact is reified, allowing us to read against and along the scientific grain.18  As anthropological surrogates, archives must capture the processes and events that destroy and create facts.19   6. Digital Humanities (DH) Network analysis, text mining, and information visualization provide an algorithmic supplement to the pursuit of past, present, and future research questions, enhancing analysis of digital objects.20  Recent work with digitized historical records demonstrates how the algorithmic approach offers a lens with which to gaze upon and ascertain meaning from large and unwieldy bodies of data.21  As the corpus of materials amenable to computational analysis grows in archives throughout the world, approaches of this type are becoming indispensable. While great promise lies in the analytic potential of the DH, its practitioners are increasingly aware of the challenge to making the results of their research preservable and in the optimal case, reusable.22  For their part, the familiarity that archivists have with the lifecycle of data prepares them well for having conversations with digital humanists.23  Archivists can become digital humanists themselves as they utilize approaches like topic modeling to enhance the ways in which they, and their users, interact with archival materials.    7. General Application An archival processing strategy that is informed by anthropological principles and that uses DH tools will facilitate reflexivity between the archivist’s professional practices and the scholar’s interpretive possibilities. Evidence of the knowledge production process can be better preserved by integrating DP, DC, and DH concepts and tools as essential elements of a systematic archival processing workflow covering analog, born-digital, and digitized records.24 Archivists should be particularly attuned to six elements of scientific work: inscription, circumstantiality, noise, conflict, credibility, and reification.25 To document these factors, archivists must modify how they appraise, preserve, arrange, and describe scientific records.  The challenges to the archival task are many, and include:  Recognizing the potential of archival sources as anthropological surrogates. Weighing the value of records in terms of the insight they reveal in terms of the six elements of scientific work that are listed above.  Using tools that preserve authenticity, while also providing scholars the ability to understand how the scientific process played out within the social networks and environment supporting scientific research.  Controlling costs and sustaining the archives over time.    8. Case Study The strategy being used by the University of Illinois at Urbana-Champaign is illustrated by our ongoing work with the records of Carl Woese (1928-2012), 2003 winner of the Crafoord Prize in Biosciences.26  While a full description of the project is beyond the scope of an abstract, we are integrating DP, DC, and DH concepts and tools into three particular points in our workflow.27    Appraisal/Acquisition   Professionally photograph Woese’s laboratory to document microsocial environment. Transfer materials while maintaining original order and recording original placement in lab. Create forensic image of Woese’s laptop to capture records in a consistent, verifiable manner and avoid unintended data modification.28 Extract user files with disk analysis reports to use as surrogate during processing and topic modeling. Used topic modeling and network analysis tools to help develop processing plans.   Processing   Arrange analog records into functional series representing Woese’s activities. Preserve reprint files in original order, including correspondence regarding Woese’s methods and conclusions. Generate preservation metadata for born-digital content, including genomic datasets.29 Use open-software and to identify and remove private/confidential records, and to identify documents speaking to the six factors, and to assist in generation of access copies, including topic model as alternate access point.30   Access   Create a summary online description for all analog and digital files, with file-level inventory.31 Use file conversion and normalization tools to create access copies of the digital and digitized files. Provide access copies in zip format, facilitating application of data analysis tools by scholars. Present topic-modelled view of data as alternative access point.32 Deposit preserved records in Library’s digital preservation repository (Medusa), ensuring long term integrity and accessibility.33 Undertake migration assessment and planning for at risk file formats.  The process described above is surprisingly cost effective when integrated into the “More Product, Less Process” framework for achieving archival efficiency (more information and examples will be provided at the conference and in a full paper, if selected for inclusion in the proceedings).34    9. Conclusion  In the Woese project, we seek to test one means of preserving evidence about knowledge production in scientific archives.  We believe that techniques like those described above can and should be applied to the archives of important scientists, if we wish for those archives to support humanistic research, but--as with all areas of human knowledge--we realize that our own conclusions are subject to revision after being viewed in the bright the light of experience.  ",
       "article_title":"Constructing Scientific Archives that Support Humanistic Research",
       "authors":[
          {
             "given":"Christopher",
             "family":"Prom",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Bethany",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"Padilla",
             "affiliation":[
                {
                   "original_name":"Michigan State University, United States of America ",
                   "normalized_name":"Michigan State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05hs6h993",
                      "GRID":"grid.17088.36"
                   }
                }
             ]
          },
          {
             "given":"Angela",
             "family":"Jordan",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"John",
             "family":"Franch",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Andrea",
             "family":"Thomer",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Tracy",
             "family":"Popp",
             "affiliation":[
                {
                   "original_name":"University of Illinois at Urbana-Champaign",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "classical studies",
          "linguistics",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Casta painting was one of the most popular non-religious artistic genres in New Spain (present day Mexico) during the XVIII century. They come in series of up to sixteen scenes, each one showing an interracial couple and their offspring usually carrying out daily activities in everyday settings. In these paintings, artists depicted the three main ethnic groups making up Mexico’s colonial population: Españoles, Indios, and Negros, and the process of mestizaje. Produced mostly in Mexico City and Puebla, Casta paintings reached a peak in production between 1770 and 1780, disappearing at the beginning of the 19th century as the War of Independence began and a generalized rejection of colonial structures took hold. In New Spanish society casta referred to race, both in biological and social terms 1. This notion was the basis of a caste system that pervaded New Spanish life at the time; a form of colonial control informing the kind of jobs people could do, where they could live, the civil liberties they had, and whether they paid taxes. According to Edward Long, the caste system had three general purposes: “first, to guarantee that each race occupy a social niche assigned by nature; second, to offer the possibility of improving one’s blood through the right pattern of mixing; third, to inhibit the mixture of Indians and Blacks, which was deemed the more dangerous to the Spanish social order” 2. Blood mending–a process that was believed to make the offspring of interracial couples return to a pure European bloodline–was in the Spanish elite’s eyes a way to assert their prominence. A control mechanism serving colonial concerns, blood mending was commonly staged in Casta paintings and, along with hierarchical and structured serialization, appears to suggest an ordered and stable social system 3. Critics have suggested that Casta paintings served as souvenirs–postcards–of the new world and, thus, showed a functioning and harmonious society 4. As a matter of fact, Magali Carrera has stated that “as visualizations of race, Casta paintings stabilize the ambiguity and complexity of physical race by locating the meanings of race in the confluence, interactions, and mediations between and among physical, social, and economic spaces” 5. Because of the composition and titles of Casta paintings, much literature has focused on their function as documents cataloguing the existence of the main races and the many resulting combinations –upwards of fifty according to Nicolás León 6. We take the painting’s titles as metadata rooted in the hegemonic perception of the groups depicted and explore their inconsistencies; for example, how in thecasta system ‘casta’ simultaneously refers to both a specific mix like ‘Barcino’ or ‘Coyote’ and to the whole set of possible mestizaje instances. For Margarita de Orellana, the proliferation of terms and the representation of many castas showed the discontinuity of the mestizaje phenomenon. Furthermore, by dividing and fragmenting it, the complexity of mestizaje was over simplified 7. The fact that many of Casta paintings were commissioned by and for Spanish patrons, and even meant to be sent to Europe 8, explains the intent to reduce the complexity of mestizaje and to show the stability of life in the Americas. Moreover, the notion of blood mending in the caste system, observable in the paintings, sought to put an end to “the widely held notion in Europe that everybody in the Americas was hopelessly mixed” 9. We argue that by artificially emphasizing the division of castas, these paintings sought to reassure the Spanish who feared their demise in New Spanish society, but ultimately failed to do so. 2. Overview and Methodology In this study we challenge the hierarchical catalogue view embedded in Casta paintings. Instead of looking at castas in their rigid classifications, we have examined them as a group making up the larger and more complex figure of the mestizo. In that sense, we take casta as a group category referring to all mestizos resulting out of Español, Indio, and Negro. We base this assertion following Octavio Paz for whom, “among all the groups making up New Spain’s populations, mestizos were the only ones embodying that society, its true children… Furthermore, they were those who made it not only new, but another” (our translation) 10. In addition, instead of looking at particular series we have built a database with descriptions of over two hundred paintings. Painting description included extracting a total of 618 characters and marking them by casta (as assigned to them in the paintings’ title), gender, as “parent” or “child”, and the activity they are doing. Casta painting critics have often pointed out that far from being a harmonious reflection of the caste system, they highlight the tensions and complexities underlying it. We build upon this through an extensive data approach and analysis. We believe this global approach has the capacity to show how, far from pacifying Spanish fears and concerns, Casta paintings confirm them and suggest prevalent mestizaje and the loss of Spanish prominence. 3. Results Data analysis has shown that the majority of the characters depicted in Casta paintings are either Españoles or Indios, followed by Mulatos and Mestizos (Fig. 1). Most interesting is a composite view of casta and gender in which Español males are the most represented group throughout and are present in almost half of the paintings we have described. The second largest group is Indio females, present in over seventy paintings (Fig. 2). The recurrence of Español male and Indio females signals the basis of racial mixing resulting in the particular figure of the Mestizo, which can be extrapolated as the general view of casta as any of the mixes.   Fig. 1: Overall casta frequency.    Fig. 2: Casta frequency by gender.  A look at the offspring sheds much light into the issues of blood mending and Spanish prominence. When we look at the offspring as separate castas, it is indeed the Español children who are most recurrent. The division of castas, thus, fulfills the objective of showing Spanish prominence among the interracial diversity. Furthermore, as there are no couples composed of both Español father and mother, the presence of Español offspring highlights the possibility of blood mending through the right combinations (Fig. 3). In contrast, when we look at offspring after casta grouping, the outlook is quite different. In Fig. 4, we show the proportion of two non-mixed offspring (Español and Indio as there are no Negro offspring in the corpus) and the two biggest casta groups (Mestizo and Mulato). This grouping highlights how, even though the Españoles are the larger minority, their prominence quickly dwindles in comparison to casta children. Fig. 5 reinforces the sweeping presence of mixed-race offspring in comparison to the Español. Our data on the offspring depicted suggest not only that blood mending and a return to whiteness are rather marginal, it also highlights that further mixing is the standard depicted in Casta paintings.   Fig. 3: Overall offspring casta frequency.    Fig. 4: Main castas and casta grouping.    Fig. 5: Español, Indio, Negro and Castas outloook.  Additionally, we have identified forty activities being depicted in Casta paintings, and explored the three most recurrent ones in detail: carrying and holding children (Fig. 6), portrait posing (Fig. 7), and working (Figs. 8 and 9). We have looked at these activities with regards to casta frequency. In terms of carrying and holding a child, females are seen to be the ones who carry out this activity the most; however, it is actually Indio women who have the highest frequency. For the males, it is seen that Español take charge and hold children over Español women, and over every other male. Español males are most frequently depicted as portrait posing, which sets them apart from the other characters in the corpus for their lack of association with different types of labour. In contrast, when we look at work activities, such as sewing, selling, shoe mending, and making pulque, it is Indios and Negros who have the highest frequency of working depictions. In ratio terms, almost 40% of Indios and almost 50% of Negros are shown carrying out a work activity. These results indicate that the Español characters were represented as prosperous figures, though largely passive, especially when compared to Indios, Negros, Mestizos, and Mulatos who are shown being in charge of most of the basic economic activities.   Fig. 6: Holding and carrying children by casta and gender frequency.     Fig. 7: Posing by casta frequency.    Fig. 8: Working by casta frequency.    Fig. 9: Percentage by casta of working individuals.  4. Conclusions By taking a general castaapproach, this study argues that the notions of blood mending and Spanish prominence that informed and even produced Casta painting are fragile. These two principles of the caste system in place in New Spain at the time are consistent only as long as castas are seen individually. Conversely, when viewed as a group, castas not only outnumber the Español, Indio and Negro characters, they also signal that miscegenation was the standard. Furthermore, the contrast between the depiction of the Españoles’ apparent prominence through non-laborious activities and the frequency of representations of Indio, Negro, Mulato, and Mestizo characters working, suggests an economic tension revealing a loss of Spanish relevance in the changing social landscape. Finally, Casta paintings not only fail at remedying the Spanish elite’s anxieties, but, rather, stage the inevitability of miscegenation and even anticipate the demise of Spanish colonial rule.  ",
       "article_title":"The Landscapes of Casta Paintings: Depictions of Social Anxieties in XVIII Century New Spanish Art",
       "authors":[
          {
             "given":"Natalia ",
             "family":"Caldas",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, UWO",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Élika",
             "family":"Ortega",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, UWO",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Antonio ",
             "family":"Jiménez Mavillard",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, UWO",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Brown",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, UWO",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Juan Luis",
             "family":"Suárez",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, UWO",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Content Analysis",
          "Art History"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  The concept of public participation was brought onto the agenda of urban design and planning prominently after the May events of 1968 (Jencks, 2011). Arnstein (1969)1was the first to identify various ways of participation: manipulation, therapy, informing, consultation, placation, partnership, delegated power and citizen control. After this study, it became more evident that facilitating participation practices do not necessarily grant planning power to the citizens; they may manipulate them as well. Following the Arnstein's ladder, the understanding of participation shifted towards the greater democratization of the processes and deeper involvement of citizens. Connor (1988)2, Dorcey et al. (1994)3 and Rocha (1997)4 have proposed their updated versions of the participation ladder, each focusing on slightly different aspects. Connor (1988)'s point of view was oriented more towards conflict resolution whereas Dorcey et al. (1994) suggested ongoing involvement and consensus building as the highest level of participation. Rocha (1997) placed political empowerment at the top and atomic empowerment at the bottom of her version of the participation ladder.  Senbel and Church (2011)5 linked various forms of empowerment and visualization media while proposing a more \"enabling\" version of Arnstein's ladder. Their ladder involved six \"instances\" of design empowerment.The highest level on this ladder is independent design, when ordinary citizens gain the capacity to create their own plans and visions; reaching autonomy. Overall, the brief review above illustrates the theoretical shift or the \"communicative turn\" from rational planning to deliberative planning. From the perspective of geospatial participatory technologies, it is possible to track similar layers of transformation regarding the production and dissemination of geographic information. From top-down to bottom-up, referring to the public participation GIS (PPGIS), from \"requested production\" to \"voluntary production\", and finally, towards the wikification of GIS and Web 2.0-based social-geographic applications (Geoweb 2.0)(Roche et. al., 2012)6.  Relying on a combination of social software and information aggregation services, Geoweb 2.0-based participatory planning practices stand as a strong alternative to the traditional linear and hierarchical knowledge production methods. These  are loaded with constructivist learning and production principles embedded in the ways they enable social knowledge construction (Pak and Verbeke, 2012)7.  In this context, we would like to critically address the following questions in our study:  Which inclusion strategies and tools are used for design empowerment in popular Geoweb 2.0 supported participatory  planning practices? To what extent do these practices facilitate participation in urban planning?  Motivated with the questions above, we made an evaluation of relevant practices through an online survey. We will share the method and results of this survey and discuss our findings in Section 2. This discussion will be followed by the conclusion (Section 3), in which we summarize the findings and discuss their possible implications for future developments.   2. Evaluation of Design Empowerment Strategies Employed in Practice  We grounded our survey on Senbel and Church's (2011) theoretical framework for \"design empowerment\". As briefly described in the introduction, the authors proposed six instances of citizen involvement in design (Figure 1). In this framework, independent design is depicted as the highest level of empowerment, followed by integration which involves the coproduction of plans. Inclusion of the thoughts of the participants among other priorities, ideation about the plans and inspiration triggering response to an alternative and information are the relatively lower instances of design empowerment.   Fig. 1: Senbel and Church's (2011) Instances of Design Empowerment   Based on the instances and forms of design empowerment above, we prepared an online survey to analyze the inclusion strategies and tools used for design empowerment in the existing Geoweb 2.0 supported practices. In January 2013, we distributed the survey to thirty organizations listed by the crowdsourcing.org8 directory as related to urban design and planning. These organizations were contacted via three different communication channels: email, phone and their facebook pages. Eleven organizations have accepted to attend our survey (OpenPlans, Nextdoor, CitySourced, Neighborland, LocalWiki, Spacehive, MindMixer, LocalData, mySociety, Ideavibes, CommunityPlanIt). At the time of the survey, these organizations represented dominant North America and UK-based practices which operate globally, including the Continental Europe. 64 percent of the participants were private organizations. The remaining 36 percent were NGOs, Social enterprises and University laboratories. In relation to Senbel and Church (2011)’s instances of design empowerment, we asked the participants to rank the priorities (Table 1) of their practices. A legal representative of each organization answered our survey.    Fig. 2: Average rankings of the priorities of the Geoweb 2.0 applications (higher levels of design empowerment are indicated as darker colors)  According to the participants, Ideation (empowerment level 3) was the most important priority, followed by Information (level 1) and inclusion (level 4). The two highest levels of empowerment -Independence and Integration- were ranked as the two least important priorities (Figure 2). Following the ranking, six of the participants chose to answer an open question on the design empowerment potentials of their Geoweb 2.0 applications. One participant expressed that their application \"can be leveraged by people trying citizen design\". According to another participant, the organization \"had lots of broad efforts around planning, driven entirely by citizens. But it had little official use by city planners or professional planners\".  One of the other participants indicated that their \"toolkit is less about formulating citizen-designed plans, but it rather provides a more efficient method for data collection already taking place\". Similar to this comment, another wrote that their application was\"designed more for reporting problems with the local area (e.g. potholes, broken street lights) than for any integration with urban planning”.   In addition to the observations above, we made a brief analysis of the provided functions (Table 1). The most common ones were: commenting on other users' contents (91 percent), followed by adding a placemark and descriptive text (82), tagging content based on predefined categories (64) and uploading a document (55 percent). Only one of the Geoweb 2.0 applications supported annotated drafting and drawing tools, which are necessary for the empowerment of citizens in the independent and collaborative design of plans and projects.      Provided Functions     Percentage      Commenting on other users' contents    91%     Adding a placemark and descriptive text    82%     Tagging content based on predefined categories    64%     Uploading a document    55%     Adding a geolocated photo    45%     Editing other users' contents    45%     Tagging content based on user-defined categories    55%     Forum    36%     Internal Messaging    27%     User controlled thematic layers    18%     Timeline    18%     Other: Search, Video, Organizer moderation, Civic profile, Email notifications, shapefile/kml; data management; survey creation   18%     Drawing polygons on the map and adding a description    9%    The last finding was on the intended target audience of the Geoweb applications. According to the participants these were Neighborhood Organizations (100 percent), Community Residents/inhabitants (100 percent), Governmental Administrations (91 percent), Governmental Planning Organizations, NGOs and Umbrella Organizations, Urban Designers, Research Organizations  (73 percent), Property and Land Owners and Project, Real Estate Developers  (55 percent), Architects (36 percent), Others (18 percent) and Financers (9 percent).   3. Conclusion and Discussion Our analysis results suggest that the strategic positioning of the sampled set of Geoweb 2.0 applications was less towards higher levels of design empowerment and more towards data collection, information and ideation. This finding was evident in the individual rankings of empowerment intentions as well as the provided tools and functions. As a response to the open question, participants reported a specific scenario in which the authorities and experts were empowered through the collection of information from the citizens. The intended levels of design empowerment of the citizens were indirect and limited.  Only 9 percent of the practices provided drafting-drawing tools which are evidently essential for the citizens to create their own plans/visions and reach autonomy.   When combined with the self-reported target audiences, our findings suggest that the sampled Geoweb 2.0 applications were primarily intended to be used as a single-sided communication channel between the citizens and the planning organizations. None of them included convincing mechanisms to guarantee the consideration of the data collected from the citizens and the inclusion of these into the design and planning processes.  Furthermore, according to the survey results, the majority of the practices (64 percent) were controlled by private organizations. Reflecting on the negative experiences of Facebook and Google (Bucher, 2012)9 and (Habermas, 2006)10 we can claim that public opinion on urban planning cannot be formed in a truly democratic manner without separation of tax-based state from market-based society. Unregulated private social networks may encourage disempowerment due to the commodification of personal and sensitive information on citizens, triggering counter results. Therefore, for better practices in the future, it is of utmost importance to construct self-regulating and independent systems which can:   Operate as a mediating interface between the planning authorities and the society, Enable inclusion and equal opportunity for participation in innovative ways, Ensure the privacy and security of the participants, Mobilize discussion on relevant topics and claims and planning actions, Promote critical evaluation from different perspectives.  In this context, the potentials of Geoweb 2.0 to empower ordinary citizens to develop their own plans are yet to be harnessed. Reporting potholes can raise awareness but is only a small step in the empowerment ladder.   Acknowledgements This paper is partially based on  post-doctoral research project supported by INNOVIRIS, The Brussels Institute for the encouragement of Scientific Research and Innovation.  ",
       "article_title":"Geoweb 2.0 and Design Empowerment: A Critical Evaluation of Eleven Cases",
       "authors":[
          {
             "given":"Burak",
             "family":"Pak",
             "affiliation":[
                {
                   "original_name":"KU Leuven Faculty of Architecture",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Johan",
             "family":"Verbeke",
             "affiliation":[
                {
                   "original_name":"KU Leuven Faculty of Architecture",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The face and its proportions have always captured our attention and produced fascination. Even newborns have been reported to dedicate more time to attractive faces than others1. How these proportions are meant to be the guidelines that define facial beauty, has been an object of study since the times of Plato. However, absolute approaches, such as Hogarth's serpentine line2, the Vitruvius' \"well-shaped man\"3, divina proportiones, or mathematically based ratios such as golden, Fibbonacci, or 1:1.6, have proven insufficient to explain how beauty actually works4. As Francis Galton said5, \"The general expression of a face is the sum of a multitude of small details, [...].\" We can now afford to extend this concept and say that the attractiveness of a face is also the sum of a varied set of distinct features. Recent research on evolutionary psychology and neuroaesthetics suggest the same. Beauty of unknown faces seems to include aspects from averageness, symmetry, sexual diphormism, pleasant expressions, and youthfulness6789.  In this study we take state-of-the-art research results on attractiveness and beauty and extrapolate them to the analysis of faces in world painting. We first collected a data set of over 120,000 paintings, and applied industry standard face recognition algorithms to extract facial traits. Furthermore, based on meta-analysis of symmetry and averageness10, we established clues on whether faces across time could be considered more beautiful, and when these trends occur. 2. Method 2.1 Data-set The data-set was obtained from pintura.aut.org, a nonprofit organization working on autism11. The whole set of 120,000 images of paintings was narrowed down to 25,000 images by removing the paintings with no faces. The number of faces found is about 47,000. The distribution of the number of faces per paintings follows a power-law that fits into the Pareto principle. For the current study only 5,800 faces were taken into account: frontal faces no smaller than 150 pixels in height, with pitch and yaw angles between 10º and -10º with respect to the vertical line, and with valid information for at least the following traits: eyes, nose, mouth, height, width, and center of the face. Face rotation or roll was fixed geometrically. Our analysis covers the period between 13th and 19th centuries.    Fig. 1: Distribution of the number of male and female faces per century  2.1. Averageness For each century an average male and female face has been computer-generated (see Figure 2), in addition to a non-gender-specific one that combines both genders. In order to produce this averaged composite face, we first centered the faces according to the center point given by the face recognition algorithm. Faces were then resized to make them fit into a PNG canvas of 500 by 500 pixels at 300dpi resolution, and given a height of 200 pixels (faces with height lower than 150 pixels were excluded to avoid blurred pixelation of the average face). This process was achieved by using affine and projective 2D transformations12 from the original painting to the desired canvas. Every face standardized by size was converted into a 3D numerical matrix representing each of the layers of the RGB color model. A regular statistical mean was then calculated over the set of faces of each century in order to obtain the average value for each pixel. Once the average matrix was calculated, it was converted back into a PNG image.   Fig. 2: Average 20th Century female face from 1058 faces (left), and male from 1017 faces (right).  Resulting quality and averageness of the composite rely on the number of faces used in each century for generating the averaged face. The same face recognition algorithm used in the data-set is then applied to averaged composites. This allows us to measure the averageness of a face as the difference between its symmetry and the symmetry of the average face for that particular period. Averageness refers to the degree to which a given face resembles the majority of faces. In our study averageness values go from the most average, 0, to the least, 1. 2.2 Symmetry Calculation of symmetry is commonly based on Grammer and Thornhill's early work 13 . Their method makes use of 12 different points (one more for averageness): 2 for each eye, 2 for the nose, 2 for the mouth, 2 for the cheekbones, and the last 2 for the jaw. With those, they create lines for each pair and calculate their midpoints. In a perfect symmetrical face, all midpoints must lie on the same vertical line. Although we did not work with those 12 points, our algorithm still used 3 points for the mouth (left, center, and right), 1 for each pupil, and 1 for the nose. This number of traits proved enough for symmetry calculation; therefore, even though our methodology is slightly different from the one proposed by Grammer and Thornhill, the main idea remains unchanged.   Fig. 3: (a) Example of face and detected points for eyes, nose, mouth and center. (b) Vertical line, H, to divide the face into two hemifaces, and numerated points for all the features. (c) Lines for calculating distances between midpoints and hemiface line.  Additionally our algorithm also gave us the centroid or geometric center of all detected features (Figure 3a), which can be taken as the center of the face. From it, we can set a straight line that splits the face into two sides or hemifaces. Figure 3b shows points 1 to 6 (P1 for left eye, P2 for right eye, P3 for nose, P4 for mouth center, P5 for left mouth corner, and P6 for right mouth corner), as well as the line H, that we assume to be the axis of face symmetry. We now trace segments: D1 between P1 and P2, and D2 between P5 and P6 (Figure 3c). For these segments we calculate the midpoints M1 and M2. Symmetry is now obtained as the sum of the distances in pixels of M1, M2, P3 and P6 with respect to the line H. Only lateral symmetry is therefore estimated. For perfect symmetrical faces this value adds to zero; all symmetry values are normalized between 0 and 1. 3. Results Figures 4 and 5 summarize the averages calculated per century for averageness and symmetry values, respectively.   Fig. 4: Distribution of average values of averageness per century for female and male faces conforms to their own gender-specific averageness (solid lines), and to the combined averageness with both genders (dashed lines).  Figure 4 shows the distribution of averageness for male and female faces compared to their gender specific averaged composite. In dashed lines we can also see the same distribution but in regards to non-gender-specific average face. A quick two-sample Kolmogorov–Smirnov test allows us to see that there is no significant difference between the two male distributions (p=0.92) and the two female ones (p=0.51). For male faces, we observe that the levels of averageness are low in the 13th Century, but then begin to decrease until the 17th Century, which leads to a gradual increase until the 20th Century. Averageness, difference between faces and the averaged composite face of each century, can give clues on how similar faces are to each other. Therefore, the peak seen in the 17th Century may be explained by the recovery of the Greek style in Neoclassicism where the faces depicted were following the same pattern, resulting in a closer distance for each one to the average face.   Fig. 5: Distribution of average values of symmetry per century for male faces (blue), female (magenta), and both (dashed gray).  Average values of symmetry per century are shown in Figure 5 for male, female, and both. We see that most symmetrical female faces were found in the 15th Century and male faces in the 18th Century. After that, all faces rapidly become asymmetrical during the 19th and 20th Century. This might be explained by the then new art styles, such as Rococo, that rejected the concept of symmetry from previous styles, such as Baroque and Neoclassicism. 4. Discussion and Further Research As reported by previous studies (e.g. ), there might be a link between averageness and attractiveness evaluations, which suggests that the more average the face, the more attractive it is perceived to be. Therefore, representations closer to the mean tendency of a population are preferred rather symmetry. Extrapolating these ideas to our results (see Figure 4): male faces were more attractive in the 17th Century; unlike female faces, that experimented a decrease in averageness for the same period, hence, so did their perceived beauty. Results from the symmetry analysis seem to support the same trend: faces were more symmetrical, and allegedly more attractive, between the 14th and 17th Century. These results would be further supported by means of rating experiments. We have found differences between genders that merit more research. Finally, while the purpose of this study was to establish when faces were seen as more attractive, some researchers have noticed a weak link between beauty and health that should be explored in the future. Skin tone in relation to attractiveness is also a topic that seems to be gaining more interest in the last years.  ",
       "article_title":"The Changing Canon of Beauty: Facial Attractiveness in the Representation of Human Faces in World Painting",
       "authors":[
          {
             "given":"Javier",
             "family":"de la Rosa",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, Western University",
                   "normalized_name":"Western University",
                   "country":"Cambodia",
                   "identifiers":{
                      "ror":"https://ror.org/02agqkc58",
                      "GRID":"grid.443228.b"
                   }
                }
             ]
          },
          {
             "given":"Natalia",
             "family":"Caldas",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, Western University",
                   "normalized_name":"Western University",
                   "country":"Cambodia",
                   "identifiers":{
                      "ror":"https://ror.org/02agqkc58",
                      "GRID":"grid.443228.b"
                   }
                }
             ]
          },
          {
             "given":"Nandita",
             "family":"Dutta",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, Western University",
                   "normalized_name":"Western University",
                   "country":"Cambodia",
                   "identifiers":{
                      "ror":"https://ror.org/02agqkc58",
                      "GRID":"grid.443228.b"
                   }
                }
             ]
          },
          {
             "given":"Juan Luis",
             "family":"Suárez",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab, Western University",
                   "normalized_name":"Western University",
                   "country":"Cambodia",
                   "identifiers":{
                      "ror":"https://ror.org/02agqkc58",
                      "GRID":"grid.443228.b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "art history",
          "content analysis",
          "image processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Quite a lot of ink has been spilled, or, at least, quite a lot of keys have been hammered on the definition and the history of Digital Humanities. This has also led to a, sometimes heated, debate on “who’s in” and “who’s out”. Regardless of whether all parties concerned are entitled to call themselves “digital humanists”, the consensus nevertheless seems to be that Digital Humanities are in essence a collaborative activity (see, e.g., Siemens 2009 1 and Spiro 2013 2), involving academic staff, students, computer programmers, librarians, project coordinators, administrative staff, and others. It is clear that academic librarians should get involved, not only because they are “as much a part of the (Digital Humanities) plan as faculty are” (Pannapacker 2012 3), but also because they should take to heart the goals for practitioners of Digital Humanities as they were identified by Lisa Spiro (Spiro 2011 4; see also Vandegrift and Varner 2013 5), namely: to provide wide access to cultural information, to enhance teaching and learning, to transform scholarly communication, to enable the manipulation of data, and to make a public impact. Even in a minimal, unambitious definition of the mission of any research library, several of the named goals should form its core business, its heart and soul, and its reason for being. And although the ways in which research libraries have translated these goals into actual activities may have changed considerably during the past decades, the essence still remains the same: at the very least, these libraries aim to provide access to cultural and scholarly information and to enhance teaching and learning at universities and beyond; and are condemned to either continue to do so in the digital age or to become obsolete (Verbeke 2013 6).  Naturally, the exact nature and form of the contribution of the university library and its staff to Digital Humanities projects at a particular academic institution will differ, depending on who else is already involved, on the available staff and infrastructure, on the wishes and willingness of faculty and administration, and on the ambitions and priorities of the institution in question. Possibilities range anywhere from providing basic information about existing tools for Digital Humanities research and teaching as well as the organization of training sessions so that academic staff and students learn how to use these tools, to the creation of library-based skunkworks – or semi-independent, research-oriented software prototyping and makerspace labs (see, especially, Nowviskie 2013 7). Whatever the exact form, however, it seems natural for research libraries and their information professionals to focus on a contribution which is identical or very similar to the roles which are traditionally expected of them anyway, such as discovery and dissemination of data and knowledge, data management, digitization and preservation (Showers 2012 8 and Vandegrift 2012 9). Of the latter, William Kretzschmar and William Gray Potter even stated – not without a sense for drama – that “collaboration with the university library is the only realistic option for long-term sustainability of digital humanities projects in the current environment. … If digital humanities projects stand still, they will indeed die, and the library is the only part of our institutional structure that can keep them moving enough to save them” (Kretzschmar and Gray Potter 2010 10).   This short paper not only evaluates the recent scholarship on the role of libraries and their staff in Digital Humanities projects, but also documents the efforts of the University Library at KU Leuven (Belgium) in general, and the Arts Faculty Library in particular, to maintain, in a Digital Humanities context, its role as an important partner in research. It discusses the various initiatives to transform the ways in which the libraries concerned have supported learning and teaching for decades, and presents various projects to which staff members of the library contribute. Examples include efforts to develop OCR applications for early printed Dutch and Latin texts, the creation of virtual research communities for international projects involving KU Leuven faculty, the building of an integrated reference database and collaborative platform for the study of Patristic, Medieval and Byzantine texts, and a strong involvement in Europeana and Linked Data initiatives. Finally, this short paper also briefly presents the current state of the plans to found a Digital Humanities Library Lab @ Leuven (DH3L), offering a frank discussion of the common challenges (see, especially, Posner 2013 11) encountered over the past months and the ways in which faculty, administration and library staff at KU Leuven have tried to overcome them.   ",
       "article_title":"The opportunistic librarian: A Leuven confession",
       "authors":[
          {
             "given":"Demmy",
             "family":"Verbeke",
             "affiliation":[
                {
                   "original_name":"KU Leuven",
                   "normalized_name":"KU Leuven",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/05f950310",
                      "GRID":"grid.5596.f"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The New York Public Library for the Performing Arts holds in its collection over one million programs documenting a large number of the major theater, music, and dance events performed around the country since the end of the Civil War. Although the collection grows each month, the Library estimates that it currently holds approximately 125,000 dance, 400,000 music, and over one million theater programs. These programs are valuable as individual artifacts, of course, but as an aggregated collection they serve as a sort of analog database of performing arts history.  Unfortunately, querying this “database” is, at present, very inefficient.  The materials are available only to researchers who come to New York to view them in person, and can only be viewed one at a time. Further, many are printed on crumbling paper that may not survive many more examinations by even careful researchers. In early 2013, motivated both by our responsibility to preserve these artifacts and out of a desire to better expose the data they contain, we launched an effort to create digital images of our program collection and organize a crowd-sourced effort to transcribe and structure the information contained within it.  The project, launched in beta under the name Ensemble1 in June of 2013, is now part of a new NEH-funded Digital Humanities Implementation grant to create tools for crowd-sourced transcription projects2.  This paper will discuss the lessons the team learned from the beta release as well as the modifications we are planning for the upcoming full release in 2014.   2. Behind the Beta Although it is our goal is to scan and transcribe every program in the Library’s collection, for the beta release we scanned 5 reels of microfilm containing 200 programs connected to theatrical productions in New York City performed between 1860 and 1930.  We selected this content for several reasons:  The relatively low cost and high efficiency of microfilm scanning allowed us to add a relatively large number of programs to our initial set very cheaply and quickly.  Although in most cases we would prefer to digitize originals, the programs preserved on these reels no longer exist in our collections. Performing arts events from this period are not well-documented by other online databases (such as the Internet Broadway Database3 or Playbill Vault4). These programs are almost certainly in the public domain; therefore they can be scanned and published online in their entirety. (If programs printed between 1923 and 1950 ever were in copyright, they were not likely not renewed and so passed into the public domain 28 years after publication). This period was an especially fertile time in the development of the American performing arts; Carnegie Hall was built5, and Vaudeville and American Musical Theater both developed during these decades.    3. What we learned The beta release of Ensemble has, as of this writing, produced almost 11,500 transcriptions of data from our initial test set of 200 programs.  Although this is significant, it falls far short of the activity seen by other crowd-sourcing projects released by NYPL Labs.  In its first three years, the menus transcription project has had over 1.2 million dishes transcribed. Over 60,000 buildings were checked in the first days of the “Building Inspector” app6. By comparison, participation in Ensemble is very low. In part, these lower numbers may reflect the relative difficulty of the task Ensembleassigns to its users.  Rather than asking for a simple transcription (as the Menus project does), users of Ensembleare required to identify relationships among text on the page (and occasionally to bring to it their own understanding of the theater industry).  For instance, a program in our collection purports to be a record of “Jesse L. Lasky’s Aristic Novelty: Fleurette.” A user assigned this program must determine whether Jesse L. Lasky is the playwright, the producer, or perhaps the director. In some cases, our interface may not even have an appropriate category.  Lacking a consistently adopted schema for performing arts data, the user is required to engage in a bit of amateur taxonomy. In our first official release, we plan to revise and publish our schema, and make it easier for novice users to perform less demanding tasks while saving more challenging assignments for “advanced” levels of the game. Zooniverse's transcription project, Old Weather7, has had success with a similar approach.  Following the model of the citizen sciences like Zooniverse’s Galaxy Zoo, Ensemble requires “agreement” by several users before accepting a crowd-sourced transcription as correct. The level of agreement among different transcriptions of the same text is processed by our systems and will eventually be used to determine what assertions are stored in the database that users of Ensemble construct. In our initial version, we attempted to expose the quality assurance/”user-agreement.”  Our hope was that those who were suspicious of the accuracy of any database constructed by the “crowd” would be somewhat reassured after they understood how the process worked.  More often, though, we found that users who were the first to transcribe a fact, and then saw that the system had a low “degree of confidence” in the work they had just submitted (since no one else had yet “agreed” with them) misunderstood what they were being told and felt either insulted or disheartened. We quickly removed these visualization (although we may want to find a way to incorporate them in a more clearly contextualized way in the final version of the tool).   4. Potential uses of the data Of course, the reason for engaging the crowd to produce this dataset in the first place is based on the assumption that it will be useful to future researchers.  Some initial use cases have imagined include: Aggregating archives: At present, researching historic performing arts events can be difficult as most of the primary sources are held in collections centered on a particular person. For example, if a scholar is researching the George M. Cohen musical Little Johnny Jones he or she will quickly discover that there is no large Little Johnny Jones collection at any major library. Once all of the data in our programs is available, however, a researcher could write a computer program that, given a title of a show, could generate a list of people associated with it and automatically search Worldcat for libraries that hold archives related to these people. Discovering untold biographies: The lives of star performers and successful writers are often studied, but the careers of those members of a production whose role is less visible, but no less vital, often go unchronicled. Ensemble will enable researchers to track, for instance, which stage managers are most often associated with successful plays, which celloists were featured in the best orchestras of the 1920s, and, which constellation of artists and technicians is most often associated with the success or failure of the production of a Shakespeare play. Mapping the arts: Where in New York City in 1920 would one mostly likely find an opera performed? What about a burlesque? A jazz concert? By opening up the data in the programs, it will be possible for software developers and geographers to combine the performance data with our digitized historical map collection and plot the kinds of performing arts events performed in particular regions of the City during a defined time period. This data may confirm or overturn scholarly assumptions about the geographical history of the city. It is possible that the most illuminating and exciting uses of the data in these programs have yet to be imagined because, at the moment, surprisingly little of this information from this period is available at all.  It is our hope that Ensemblewill soon become the backbone of an extensive, linked, open set of performing arts data that will allow researchers of all kinds to discover new information about the rich history of the performing arts in New York.  ",
       "article_title":"Crowdsourcing Performing Arts History with NYPL's ENSEMBLE",
       "authors":[
          {
             "given":"Doug",
             "family":"Reside",
             "affiliation":[
                {
                   "original_name":"NYPL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction From the earliest e-Text centers at Oxford1 and the University of Virginia 2, through the development of the TEI in the late 1980s3, to the publication of image-based editions in projects like The William Blake Archive 4, digital editing projects have long been a core activity of the Digital Humanities. Until very recently, though, the limitations of reliably available technology and complicated intellectual property laws have kept all but the most adventurous editors from venturing beyond the media of text and image to multimodal editions that incorporated video and audio recordings. Over the past year, however, the New York Public Library for the Performing Arts has released two new digital editing projects: an online tool for producing editions of dance video and a mobile app for publishing multimodal, variorum editions of musical theater libretti. This paper will examine both projects and what they imply for the future of critical edition building in the 21st century.   2. Antecedents Although most digital editions produced by digital humanities scholars have presented only text and image, over the past two decades, a few pioneers attempted to produce multimodal texts of various kinds. In the 1993s the Voyager Company produced a series of Hypercard and CD-ROM based multimodal editions (such as a Companion to Beethoven’s 9th Symphony) 5. A 2002 project led by Janet Murray produced a critical digital edition of the film, Casablanca 6.  More recently, the Maryland Institute for Technology in the Humanities, Music Theatre Online, produced with an NEH Digital Humanities Startup grant by the Maryland Institute for Technology in the Humanities, linked several libretti of the 2008 musical Glory Days to mp3 files from live performances7.  Although each was an interesting experiment that advanced the field of digital editing, the Voyager CD-ROMs were never financially successful 8 , copyright restrictions prevented the widespread release of Murray's Casablanca edition, and Music Theater Online never found a large audience. Over the last five years, though, several new technologies have emerged which have made the production and dissemination of multimodal editions much easier. The now widely adopted, video-and-audio-friendly HTML 5 specification and code libraries like Popcorn.js have made it much easier to embed and control audio and moving image recordings on web pages without relying on external plugins like Adobe's Flash or Microsoft's Shockwave. Cloud-based streaming services with public APIs such as Rdio9 and YouTube10 allow digital editors to make use of content hosted by third parties (making rights clearance less of a concern). Further, the ever accelerating migration of users from desktops and laptops to mobile devices and their app-based ecosystem makes it possible to publish these editions on hardware that has become a comfortable technology for long-form reading (thereby overcoming some of the ergonomic obstacles that prevented wide-spread adoption of many earlier digital editions).   3. Our projects   Fig. 1: Two video synchronized and shown together    Fig. 2: The editing environment  In October of 2013, the New York Public Library for the Performing Arts released a collection of over 1000 hours of video of dance performance via a new web-based collections portal. In addition, we released a web-based video editor, based on the JavaScript library Popcorn.js, which allows users to synchronize different videos of the same event (or work) together to be watched side-by-side (thereby creating a kind of multimodal variorum edition) [see figure A].  The tool also allows segments of multiple videos from various sources (e.g. NYPL Archives and YouTube) to be edited together into a playlist and annotated (either with text or additional videos) to create a video critical edition [see figure B]. The individual elements of any \"edition\" created with this tool will only be played in locations where rights agreements allow (for instance, in one of the 88 branches of New York Public Library), but any publicly viewable content and textual annotations can be seen anywhere in the world.   Fig. 3: Libretto: The Android App  In February of 2014, the Library will also release an NEH-funded eBook app (provisionally titled Libretto) for Android operating systems capable of presenting variorum editions of texts linked to audio recordings. Readers will be able to switch among variant versions of the same text, and (through an implementation of the new ePub 3.0 ebook standard) click portions of the text to hear associated audio (e.g. music associated with lyrics in an opera libretto) [see figure C].  In most cases, the music should be bundled with the text, but, in cases where obtaining a license to redistribute the music proves impossible, the editor may link the reader to an online store (such as Amazon.com) to purchase recordings which will automatically be synched to the text based on timestamps included in the editorial metadata.   4. Analysis It is our hypothesis that scholarly users of the multimodal editions created for these environments will find the multichannel approach we have taken useful. However, relatively little research has been done in exactly this context. The Voyager company's CD-ROMS and the Casablanca edition were similar, but designed only for desktops and optical media, and so represented a different experience than we hope to provide with both our mobile eBook reader and the web-based video editor. However, by July, the video editing tool will have be public for nine months and Android app will have been public for about five.  During this period we will track usage of both, and in this short paper will report on the response of both editors and “readers” of these new tools for multimodal editions and what it suggests for the scholarly editors of the future.  ",
       "article_title":"Two new tools for multimodal editions",
       "authors":[
          {
             "given":"Doug",
             "family":"Reside",
             "affiliation":[
                {
                   "original_name":"NYPL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The orthography/identity hypothesis proposes that a speaker’s motivation for selecting between available orthographic variants in a language (e.g. between British colour and American color in English) is to some extent informed by the speaker’s desire to express a certain identity 1 2 3.  In the Canadian context, where a mixture of American/British variants are used – often with non-categorical preference 45 – Heffernan et al. 6 developed a method to qualify the orthography/identity connection in terms of ideology and show that during periods of increased “anti-Americanism,” specifically during unpopular American-led wars, American variant use declines relative to the British.  Heffernan et al.’s data cover the years 1921 to 2004, and are derived from the student newspaper The Gateway at the University of Alberta in Alberta, Canada.  Their method involved locating expressions of national sentiment for each year of the data, rating “anti-American sentiment” on a 7-point Likert scale (255 ratings over 85 years performed by each author) and correlating this with the relative frequencies of 15 orthographic variables (Table 2, though color / colour is my addition): the negative correlation obtained was quite high, with Pearson-r -0.715, p = 0.001.  However, follow-up work by the present author, using data in the same timeframe from the archive of the University of British Columbia’s student newspaper The Ubyssey (~50 million words) in the neighbouring province of British Columbia, failed to find similar short-term diachronic changes in variant use correlated with periods of increased “anti-American sentiment” 7.  Following Heffernan et al.’s method, an insignificant correlation was obtained: Pearson-r -0.434, p = 0.064.  Historical relative orthographies do differ between Canada’s provinces 8 910 but, assuming the strong connection between orthography and identity, no clear explanation remains for the lack of correlation in other Canadian data.  Without dispensing with the orthography/identity hypothesis, I hypothesize that proximal linguistic contexts are also motivating factors in variant selection, and propose to integrate a more context sensitive model into this top-down, language-external theory of linguistic identity performance. To test for contextual differences, I treated the problem as one of word sense disambiguation, where the goal is to distinguish lexemes using a set of features and a computational language model—a technique most often used to distinguish between ambiguous meanings of homonyms (such as judge, bank, bow, etc.) 11.  Features used were a window of words surrounding each variant (8 words either side of the target was found optimal, excluding other instances of variables if present) and the model was Naïve Bayes.  If orthographic variants can be discriminated based on surrounding context, we can assume that those words are in some way unique—with the interesting implication, in the extreme case, that spelling variants might not just diverge orthographically but semantically, as well 12.  Maybe they mean different things.  My experiments attempted to disambiguate variants in each variable from one another using unsupervised and supervised classification, in both cases using the Naïve Bayes form.  Though Naïve Bayes makes the linguistically improbable assumption of feature independence, it has been noted for its precision in classification problems in spite of this simplification (i.e. its ‘naiveté’) 13.  For unsupervised classification I used my own Python implementation of a Naïve Bayes classifier where the parameter estimates are learned through Estimation Maximization (EM), as described in e.g. Manning and Schütze 14.  As Pedersen 15 observes, testing the results of unsupervised classification is complicated by the fact that the algorithm does not assign labels to inputs, instead clustering them, but accuracy can be represented as the proportion of the dominant variant in each cluster.  My classifier outputs two ‘sense groups’, which would ideally correspond to the American or Canadian variant.  After performing 10 trials and averaging the results, I found that only three variables out of 16 (Heffernan et al. exclude color / colour—I include it) produced significant results, in that their prediction accuracies departed from – or improved upon – their ‘lower bound’ accuracies, where the lower bound is the relative frequency of each variant and therefore the accuracy one would achieve simply by assigning each variant to a category based on its occurrence.  For brevity, only these three are represented in Table 1.     American variant   accuracy     lower bound     Canadian variant     accuracy     lower bound      color 55.5%  54%  colour  65.6%  46%     gray 23.2%  17%  grey  86.2%  83%     jewelry 51.3%  49%  jewellery  55.4%  51%    Unsupervised classification for colour improves accuracy by 19.4% over its lower bound, but increases for other variables are marginal and – like colour – generally apply to one variant only. For supervised classification I used the Naïve Bayes classifier in the Python library Natural Language Tool Kit (NLTK)16, a similar method to Mahowald’s 17 recent study in which y- and -th- pronouns were disambiguated in a corpus of Shakespeare’s plays based on context.  Whereas unsupervised classification performed poorly, supervised classification obtained surprising accuracy for multiple variables after 10 validation trials.  These results are summarized in Table 2, ranked by accuracy, with an asterisk denoting significance at the p = 0.001 level.  In this experiment, the lower bound for each variable is 50% because a random subset of the tokens was evaluated and counts were set equal for each variant during testing.               variable                        accuracy    total count    jewelry / jewellery   81.6%*   564     gray / grey   79.3%*   3112     color /colour   74.5%*   5312     program / programme   70.1%*   5704     honor / honour   62.0%*   2538     enrollment / enrolment   61.2%*   2086     humor / humour   61.1%*   2862     neighbor / neighbour   60.7%*   494     defense / defence   58.6%*   5640     judgment / judgement   56.8%   928     offense / offence   56.3%*   1568     centered / centred   55.7%   488     marvelous / marvellous   55.6%   316     fulfill / fulfil   54.7%   312     labeled / labelled            54.4%   270     kilometers / kilometres   40.0%   72    It would seem that we are able to predict, sometimes with high accuracy, whether certain variables will realize their American or British variant based on context.  But why?  If orthographic variations are simply different graphemes of the same lexeme, decided rather capriciously by an American lexicographer in the nineteenth century 18, why should this be possible? The Naïve Bayes module in NLTK provides output for identifying features most useful in making its decisions, and can help answer this question.  For gray / grey, the case is clear, since the terms most likely to indicate British grey are Pt. and Point (Point Grey is the name of the land on which the University of British Columbia lies), and terms indicating American gray are proper nouns like Bob, John, and Stuart (Gray is a common surname).  A revealing result, but only so far as it reveals a highly restrictive context in non-compositional forms, and might suggest this variable be excluded from further testing.  Contexts for color / colour are more interesting, however, and fall into two large subjective categories: ‘cultural’ and ‘technological’ (Table 3).       variant       category       informative features     colour  cultural   diversity, women, people, racism, queer    color  cultural   people    colour  technological   connected, jet, print, modem, monitor    color  technological   cartoons, TV    As a collocation analysis reveals, in the ‘cultural’ category phrases such as women of colour, people of colour, andqueers of colour occur often with colour – 225 total instances, its most frequent collocate – but hardly ever with color (11 instances).  In the ‘technological’ category, computer terms appear with colour and entertainment terms with color, where these terms are often found in advertisements and the site of these interactions tend to be local for colour and global for color (a local transaction for a colour monitor, at least prior to the expansion of current global markets, but the international consumption of color television).  However, these phrases are easily recognizable as historically specific (to around post-1980).  Indeed, the unsupervised classification of colour backs up this historical selectivity: significantly more of the items grouped at 65.6% accuracy are from this decade—context and history are intertwined, of course, and it exceeds my scope to disambiguate these here.  But the more a-historical distribution of terms predicting jewelry / jewellery suggests historical clustering is not inevitably the rule: local activities like piercing and repairs, and localities denoted by West and Point (i.e. the location of a shop in WestPoint Grey) predict British jewellery, but generic sales terms accessories, fine, place, and giftware predict American jewelry.  In sum, advertisements, or, more generically, ‘solicitations’, are the dominant vehicle of these variants and prefer the British when the activity is local (both economically and socially—these will be further described) and the American generally.  I will also further discuss how accounting for genre affects classification accuracy.  Overall, British variants are more uniquely contextualized, and therefore more easily discriminated, than American.  Qualitative sociolinguistic approaches like Heffernan et al.’s (2010) locate identity as an exterior motivating condition for language, with the necessary assumption that orthography is selected independently of linguistic context.  And though this paper finds that this assumption does not hold, the ability to disambiguate orthographic variants based on context is interesting, but not explanatory in its own right.  These contexts are also motivated, and computational techniques take us full-circle back to considering ideological – but more interactional – motivations for linguistic context.   ",
       "article_title":"Does colour mean color?: Disambiguating word sense and ideology in British and American orthographic variants",
       "authors":[
          {
             "given":"Dustin",
             "family":"Grue",
             "affiliation":[
                {
                   "original_name":"University of British Columbia",
                   "normalized_name":"University of British Columbia",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03rmrcq20",
                      "GRID":"grid.17091.3e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "sociolinguistics",
          "word sense disambiguation",
          "Classification",
          "linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction One of the most challenging issues in the analysis of large collections of historical manuscripts or handwritten fragments is the \"joining\" of fragments, either piecing together torn parts of a mutilated folio – which may have disintegrated because of wear and tear over the ages – or reconnecting two or more folios originating from the same original manuscript, but which have since been separated and dispersed to diverse locations – perhaps on account of trading activities between institutions. A striking case in point is the Cairo Genizah, discovered towards the end of the nineteenth century in the attic of an old synagogue in Cairo, which contains more than 320,000 fragments (deriving from tens of thousands of individual documents, almost all in Hebrew characters – though not necessarily in the Hebrew language) spanning a thousand years of writing and copying. Various parts of the Genizah finds are currently located in more than sixty university collections and public libraries spread out on different continents all over the world. Until just recently, a Genizah researcher holding half a page in hand and seeking its other half, did not have any resources with which to achieve that goal beyond erudition, a few catalogs, a gifted memory, and a fair measure of luck. In recent work 1, we described an automated scheme for image analysis and processing that culminates in enabling the computer to compare two images and compute a similarity score based solely on the individual handwriting style. A series of benchmarks and tests convinced us of the reliability and utility of this metric. Moreover, many hundreds of \"joins\" of interest to humanities scholars have already been identified 2. Here, we describe the design and implementation of a follow-up plan devised to integrate the matching scheme into a coherent and efficient system that can help scholars find the best candidates for potential joins for any given fragment. A system with somewhat similar goals for processing and joining fragments of frescoes is described in 3. 2. Joins and Jigsaws The following steps have been implemented: A – The basic idea is to match each of the Genizah fragments with one another so as to obtain a similarity score for each pair of  fragments. We used a combination of local descriptors (SIFT) and learning techniques (OSS 4, SVM, and others). Out of an estimated total number of 320,000 fragments, about 230,000 fragments were available to us, represented by 450,000 digital images, with two images per fragment (recto and verso). For every fragment, a numerical signature vector was computed, encapsulating aspects of its writing style. With a specially designed software component that measures the readability of every fragment, we eliminated from this scenario most fragments with poor legibility, those that most likely would not contribute true joins but rather would deteriorate the effectiveness of the system. These included blank or almost-blank pages, illegible or very dark texts, minute fragments, etc. After eliminating these problematic items, we were left with a total of 158,000 fragments to be compared with one another. That gave a total of 12.4 billion pairs that needed to be measured for similarity, a huge number indeed. Some twenty different similarity scores were computed and stored for each pair. These were generated by using four different algorithms to represent the handwriting style of each document and by using different similarity measures between documents. The different similarity scores can be \"stacked\" together to achieve higher accuracy. B – Twenty CPU's from the Computing Lab of the Blavatnik School of Computer Science at Tel Aviv University ran together continuously for 37 days (the equivalent of some 18,000 computing hours), and the task was accomplished. This computer run is probably one of the most intensive ever implemented in a digital humanities context, in terms of computing resources. Four terabytes of output were generated in the process. C – An efficient and compressed database was built to preserve these results in a structure that is easy to manipulate within a reasonable on-line response time. For each fragment, the top 300 similar fragments were precomputed. D – A simple program, Propose Joins, was then integrated in the operational software of the Genizah website, available at http://www.jewishmanuscripts.org. Any user can input an image number, and the system will respond immediately by giving a list of the best 100 candidates that might qualify as joins for the given fragment, sorted from most similar to least, accompanied by the actual images of these candidates. See Fig. 1. A user can then mark some images as worthy of further investigation as potential joins, passing them over to a second program, called \"Jigsaw Puzzle\", described below.  It is our assumption – backed up by Genizah researchers' expressed attitudes – that a competent user would not mind spending an hour or so examining these images, even if he or she does not end up finding any join in the set, since this is the only way to systematically look for such a join were there indeed any. Our experience shows, in fact, that if there is a join in the Genizah world for the given fragment, it will be found – almost always – in this set.  E – The Jigsaw Puzzle program displays the additional images designated by the user together with the original image on the screen, each image already restricted to the fragment's physical contour, and, using the mouse and a few tabs, a user can magnify or reduce each of them, move any image, rotate it by any angle in any direction, \"flip\" the image over to display the verso (say) of the fragment instead of its recto, calibrate the images at their original proportions in order to check if the geometric features and the running text of the various pieces indeed fit neatly into a join. See Figs. 2-4. If a join is found, it may be incorporated in the website for all users to be made aware of it, with the identifying scholar's name inscribed as the join's composer.  F – To make the system even more user-friendly and intuitive to researchers, even if they are not completely at ease with computers, a large 42\" touch-screen was installed in the Genazim lab, as a prototype, with an attached PC on which the website and the software were installed, all completely transparent to the user. See Fig. 5. Using a virtual keyboard, the user approaches the system by inputting an image number, receives back the images of the 100 best potential candidates, marks some of the relevant ones, passing them over to Jigsaw, where they can be easily manipulated – moved, rotated, flipped, calibrated – by just touching the screen with one's fingers, much like what one is used to doing nowadays with smartphones, and as naturally as one might arrange a jigsaw puzzle spread out on a table. See Fig. 6. Discussion We expect the overall scheme, with all the steps detailed above, to be of relevance in many other similar contexts, although, admittedly, the Genizah case is rather unusual in its scope and complexity. The join-matching tool is quite sophisticated and is constantly undergoing improvement. The jigsaw tool is relatively simple but has already proved very appealing to scholars. We are currently applying the join tool, more or less as-is, to other corpora, including the Dead Sea scrolls and papyri 5 and Tibetan manuscripts and xylographs. Other potential applications include the 2,000,000 images of 70,000 pre-1900 Taiwanese deeds and court papers from the Taiwan History Digital Library 6 and Yad Vashem’s now publicly available Holocaust archives (www.yadvashem.org/yv/en/resources/index.asp). Furthermore, we hope to make the jigsaw tool more widely available. In addition, we have begun to use machine-learning tools based on the same signature vectors to help answer palaeographical questions for such corpora 789. We are still left with the major problem of trying to reconstruct the original state of the entire Genizah collection, that is, to find, once and for all, the entire network of true joins in this collection, with a reasonable level of completeness and precision, and to do this efficiently and in a relatively short time. A crucial step in achieving this goal is to find effective methods for recognizing and eliminating large quantities of false joins and non-joins, even if that may be at the cost of losing a few correct ones. This is currently a topic of intense investigation, involving elements of graph theory, clustering techniques, data mining and related methodologies.   Fig. 1: After the user gave the system the i.d. number of the fragment being studied, the system responds by displaying that image on the left side, and 100 suggested joins, sorted by decreasing order of similarity, on the right.    Fig. 2: The given fragment and two of the suggested joins.     Fig. 3: Two fragments have been “glued together” by the Jigsaw program; the third is on its way.     Fig. 4: The final join (verso).    Fig. 5: The large touch screen mounted on a specially designed chassis, showing the fragment at hand together with 2 suggested joins.    Fig. 6: One of the authors (Y.C.) using the Jigsaw program to manipulate the suggested join.   ",
       "article_title":"Where is my Other Half?",
       "authors":[
          {
             "given":"Adiel",
             "family":"Ben-Shalom",
             "affiliation":[
                {
                   "original_name":"Friedberg Genizah Project",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Yaacov",
             "family":"Choueka",
             "affiliation":[
                {
                   "original_name":"Friedberg Genizah Project",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Nachum",
             "family":"Dershowitz",
             "affiliation":[
                {
                   "original_name":"Tel Aviv University",
                   "normalized_name":"Tel Aviv University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/04mhzgx49",
                      "GRID":"grid.12136.37"
                   }
                }
             ]
          },
          {
             "given":"Roni",
             "family":"Shweka",
             "affiliation":[
                {
                   "original_name":"Friedberg Genizah Project",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lior",
             "family":"Wolf",
             "affiliation":[
                {
                   "original_name":"Tel Aviv University",
                   "normalized_name":"Tel Aviv University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/04mhzgx49",
                      "GRID":"grid.12136.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Classification is a vexing problem in folkloristics. Indexing collections that often include tens of thousands of records is essential, but neither fully manual nor fully automated methods are adequate. In this work, we combine human notions of genre and topic classification with computational classifiers and topic analysis to produce an indexing that is both appropriate for scholarly goals and robust in the presence of ambiguity. Traditional scholarly indexes have been limited by time and technology. Although broad genre classifications such as “ballad”, “folktale”, and “legend” are well established, these formal classifications are coarse and do little more than sort the materials into large, internally diverse groupings. Most standard classification schemes assign each record to a single classification and do not allow for cross-genre classification (e.g., a ballad and a legend about the same murder will be in different categories).1234 The inadequacy of these classification schemes has significantly constrained research on verbal folklore, particularly because such categorizations are often the only available topic index for any given collection. New unsupervised machine learning methods offer scalability but lack human intelligence. Clustering algorithms partition a corpus into groups of documents that are similar. Topic modeling is more flexible, allowing each document to express multiple automatically detected themes. But such methods usually rely on simple bag-of-words representations that miss aspects of a text that are clear to readers familiar with the corpus. In addition, patterns found by algorithms may be statistically valid but uninteresting to scholars.  We explore the problem of classification in a large corpus (~35,000 records) of nineteenth-century Danish folklore and suggest possible solutions to these problems through classification and topic-modeling strategies that combine human labels with machine learning. We consider two classification schemes for the collection: in the first, each document receives one label, whereas the second assigns multiple labels to each document. One label per story The original collector assigned each story to exactly one of 36 labels, but we are most interested in “borderline” stories that could fit in many classes. These “liminal” stories not only reveal the challenges to classification that arise when a system can only accommodate a single label—as in the original index—but also help researchers to discover stories that are anomalous.  An excellent example of such an anomalous story appears in our target corpus, Danske sagn [Danish Legends]:5  DS_I_056:   Per Overlade was out one evening shooting hares. It was up on Kræn Møller’s field. Kræn was in the process of moving his farm, and the old farm had not been completely disassembled yet, and Per intended to hide amid the old frame that was still standing and shoot a hare or two. But when he gets there, he sees an old man who is sitting in there with a red cap on who nods to him. Per gets scared and doesn’t dare go in there, and so he doesn’t catch any hares.  Originally labeled as a story about “mound dwellers/hidden folk,” the story could just as easily be classified in several other categories: poaching, household guardian spirits (nisse, suggested by the old man’s red hat), and law breaking, to name but three. The story also touches on shifting agricultural practices and the significant reorganization of the Danish landscape in the early 1800s, when farms were routinely dismantled and moved out onto the newly reapportioned fields.  Where else could the editor/archivist have placed this story? To answer this question, we train a Naïve Bayes classifier by estimating a word-frequency histogram for each label. We then measure the similarity of a document to each of the resulting histograms, taking care to remove the word counts for the “query” document from the histogram for its original label. For many stories, the “true” label is the closest, but not in this case. Its top five labels in order are:   ID Story label    36 Our forbears' way of thinking and spiritual life    35 Outdoor life     29 Witches and their sport     27 Being in league with the Devil      1   Mound dwellers/hidden folk    Although the first assignment is so broad as to be of little use—emphasizing the inadequacy of the original index—the association of the story with topic 35 highlights its affinity to stories about hunting and poaching, while topic 29 indicates the story's connection with hares—animals most commonly associated with witches. Additionally, we can use this classification scheme to initialize a 36-topic model, creating one topic per original label. We assign each word token to the same topic as the label of its document. We then resample topic assignments for each word token in turn. Given the topic assignments of the tokens in a document, we can rank the topics for that document. After one sweep through the entire corpus, the “Mound dwellers” topic still accounts for more than 80% of the tokens in the story of Per Overlade, but after 10 sweeps, only 21% of the words remain in that topic. “Our forebears' way of thinking” and “Being in league with the Devil” instead account for a greater proportion, with the “Devil” topic triggered by words about shooting hares. Overall, the original topic class now accounts for the majority of tokens in 74% of the stories in the collection. As we increase the number of sweeps through the corpus, the relationship between the topics of the model and the original labels becomes attenuated. At 100 sweeps, the majority of tokens remains in the original class for only 39% of the stories. In our sample story, the prominent topics are “From the time of villeinage”, “Wiverns and small creepy-crawlies”, “Our forebears' way of thinking”, and “Death portents”. Words about shooting and hares are now assigned to the “Wiverns” topic, indicating that we should be careful in using these labels. The “Death portents” topic is represented by the words forskrækket (scared) and sidder (sitting).  Finding anomalous stories is not simply a question of precision and recall: the very fact that a story is “missed” in a given classification makes it particularly interesting. One of the jobs of the folklorist is to reconstruct the imaginary boundaries of the belief world, so stories that question or test those boundaries are the ones that are most important. Computationally cross-validating a traditional human-generated index, as described above, is an effective way to discover such liminal cases. Multiple human-generated labels We can also construct computational story classifiers when editors assign more than one label to each document. Human experts have catalogued a subset of the documents in our target corpus by assigning multiple labels to each document from a modern ontology that includes aspects of stories such as people, locations, and events. We would like to know how these labels map to the words in the documents, but simply counting the words in every document assigned to a label may result in noisy histograms. To improve our ability to interpret the results, we use a labeled topic model to learn which words are associated with which labels. Multiple labels add complexity but allow us to make stronger assumptions. Since each document has more than one label, we cannot easily translate these labels into word-level assignments as in the previous experiment. On the other hand, we can be reasonably certain that the absence of a label implies that it is not relevant. Similar to LabeledLDA6, we can therefore estimate word-topic assignments under the constraint that words can only be assigned to one of the labels for the document, or to a “Background” label that can absorb frequent words not related to any label. We then re-estimate topic-word distributions given these assignments, and repeat the process as needed. To evaluate the resulting word distributions, the original creator of the ontology marked individual words that are highly relevant to each label. At each stage of the algorithm, we have a ranked list of words for each label. Given relevance assignments, we can compute mean average precision (MAP) for the model at each stage. Under the initial noisy distributions, MAP for precision up to rank 20 is .26. After the first iteration, MAP increases to .33, but then begins falling in subsequent iterations, indicating that the model may be overfitting. Consistent differences in ranking quality provide insight into labels. We are more successful at finding words related to concrete themes such as people, animals, and objects. More abstract labels, such as story resolutions and actions or events, were mostly unsuccessful. But there are exceptions: we identified no words related to the label “Farmer”, despite the fact that this is a very common label, while events such as “Disease” and “Death” identified many specific words. Conclusion We demonstrate that classification and topic modeling methods can be used to improve existing manual annotations in a collection of Danish folklore. We find that incorporating human labels into machine learning methods—even when the labels are noisy or incomplete—produces indexes that have the benefits of both scholarly domain expertise and data-driven analysis. We believe that these results are applicable for many corpora both in digital humanities and the wider document analysis community.   ",
       "article_title":"The Telltale Hat: LDA and Classification Problems in a Large Folklore Corpus",
       "authors":[
          {
             "given":"David",
             "family":"Mimno",
             "affiliation":[
                {
                   "original_name":"Cornell University Department of Information Science",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          },
          {
             "given":"Peter M.",
             "family":"Broadwell",
             "affiliation":[
                {
                   "original_name":"UCLA Library",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Timothy R.",
             "family":"Tangherlini",
             "affiliation":[
                {
                   "original_name":"UCLA Scandinavian Section and Department of Asian Languages and Cultures",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "ontologies",
          "corpora and corpus activities",
          "concording and indexing",
          "content analysis",
          "data mining/text mining",
          "folklore and oral history"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The Digital Curator Vocational Education (‘DigCurV’) project was funded by the European Commission’s Leonardo da Vinci lifelong learning programme1.  It aimed to establish a curriculum framework for vocational training in digital curation. DigCurV brought together a network of partners2 to address the availability of vocational training for digital curators in the library, archive, museum and cultural heritage sectors, with a particular focus on the training needed to develop new skills that are essential for the long-term management of digital collections.   1.1. Overview In 2013, the DigCurV collaborative network completed development of  this Curriculum Framework for digital curation skills in the European cultural heritage sector. Drawing on a variety of established skills and competence models in the digital curation and cultural heritage sectors, DigCurV synthesised such expertise with input from those in the digital curation professions to develop a new Curriculum Framework. As a result, the Framework can help develop digital curation training offerings, provide a benchmark against which to map and compare existing offerings, and motivate training providers to continue to develop and refresh training. As the range of digital humanities stretches across disciplines, such frameworks and lenses are essential for understanding the skills and competences of individuals and for describing roles. Our paper will describe the salient points of this work, including how the project team conducted the research necessary to develop the Framework, the structure of the Framework, the processes used to validate the Framework, and three ‘lenses’ onto the Framework. Our paper will also provide suggestions as to how the Framework might be used, including a description of potential audiences and purposes.  As such, this paper draws on various DigCurV project deliverables. The contributions of members of the network to these deliverables is gratefully acknowledged.   1.2. Background A critical and often sidelined issue within digital humanities, and the cultural heritage sector more widely, is the ability of those undertaking research in the arts and humanities to care for their data and other digital material over time.  Digital humanities research creates rich digital resources 3 but also the challenges of sustaining and managing these objects. Other professionals in the cultural heritage sector also have the responsibility of stewardship of digital material over time.  But are those now professionally obliged to perform digital curation receiving the training they need? And what exactly constitutes those training needs? Another pedagogical dilemma in digital curation is whether all staff in the digital humanities and cultural heritage sector should become more proficient in the curation of digital assets, or whether specific training should be developed to enable a distinct strain of specialists to emerge. As digital humanities scholars should we be skilled to care for as well as to create? The Italian economist Vilfredo Pareto argued at the turn of the twentieth century that a society grown wealthy enough would cease to foster general knowledge in individuals and increasingly encourage individual ability in tightly specified and increasingly complex skills. Each worker would become increasingly proficient at one element of the work of a larger project or process. We are currently at a similar point of decision with digital curation training.  It is in the context of these debates that DigCurV operated.   1.2 Demand from Cultural Heritage Sector The EC has encouraged the growth of digital information professions with the 2005 launch of its i2010 strategy and a subsequent Digital Agenda initiative, launched in 2010.4. This investment is justified by the importance of the cultural heritage sector in the European economy. Specifically, in addition to the thousands of universities, libraries and archives across Europe, there are also more than 19,000 museums and art galleries, which employ around 100,000 staff 5.  Traditionally, museums and gallery staff have been trained in physical object care by well-established professional and vocational training courses, but as digital technologies infiltrate every aspect of society, digital objects are increasingly making their way into the collections held by memory institutions. In 2004, the Digital Preservation Coalition and JISC established the need for digital preservation skills training in multiple sectors in the UK JISC and DPC Training Needs Analysis 6, and DigitalPreservationEurope research has also echoed the need for these skills to be regularly refreshed by professionals as digital curation practice develops and evolves 7.  In 2009, the New York Times recognised the growing demand for digital archivist skills in the USA 8.   In 2010, Gartner Research identified four new roles needed by IT departments to remain effective 9 – one of these was ‘digital archivist’, and it was estimated that fifteen percent of businesses would employ in this role by 2012. And yet, at the 2011 JISC ICE Forum in the UK 10, fewer than half a dozen UK institutions were listed as providing digital curation training as part of their profession library and archive courses. The Digital Preservation Coalition is running again in December 2013 its popular course on ‘Getting started in digital preservation 11’ and indication of the need and requirement from the sector for basic, easy access training. The existence of such courses evidences that it is not enough to trust new recruitment into the cultural heritage sector to face the challenges of digital curation.  Research conducted by DigCurV confirms that at least in the experience of our respondents, investment is not always channelled towards creating new staff to take on the emerging digital curation duties increasingly required by heritage institutions. There is a need for existing staff to adapt to the emerging digital cultural sector.   2. Methodology Our paper will describe the research activities of two European wide surveys, focus groups and skills analysis that developed an evaluation framework which was a basis for the curriculum framework. We will focus on the final version of this framework and discuss the research findings that underpin the three lenses, including concept map and model. The DigCurV Curriculum Framework was developed to compare, describe, and inform the development of training offerings. Useable directly by the individual learner, it can also assist with direction-setting for CPD. The Framework draws on knowledge, expertise and research developed within DigCurV and related initiatives in order to synthesise a matrix of core digital curation skills and competences and, where appropriate, pathways of skills progression between one type of professional role and another. The DigCurV Curriculum Framework was iteratively developed through extensive testing and evaluation: in the first place, through a series of workshops organised in several locations across Europe; then through a panel of experts in vocational training at a multi-stakeholder workshop, supplemented by targeted interviews and small focus groups with individual professionals.  The content of the Framework has been elicited from the professions it describes in accord with its ambition to be genuinely useful to professional practice. In this way, the Framework in its current form provides a robust description of the digital curation professions at the time of publication. To this end, the Framework comprises three interrelated parts:  a core Curriculum Framework model, which provides in a cogent, relevant and approachable manner the constituents and interactions of different layers involved in digital curation training;  three ‘lenses’, or views, one each for three broad types of professional role: Practitioner; Manager and Executive;  a technical specification which outlines the groundwork for the Framework, defines the Framework’s terminology and identifies the interactions between the Framework and lenses 12.   In the DigCurV context, the cultural heritage sector is understood to comprise museums, libraries, galleries, archives plus relevant departments of HEIs – critical collaborators in digital humanities.  The types of training relevant to the project were vocational training for those aiming to enter the profession (including Master’s-level qualification) or those already in post (such as in-house skills training, CPD). The Curriculum Framework has the capacity to be useful to various audiences, including those working in digital humanities and cultural heritage professions who would like to increase their expertise in digital curation.   Current Use Cases Various institutions in the higher education sector have found the Framework useful to date. Amongst them, University of London Computer Centre (ULCC), providers of the vocational Digital Preservation Training Programme (DPTP) mapped their curriculum to the Framework, helping to review and reflect on programme content and delivery style. The Department of Information Studies at University College London found the Framework helpful as a tool for skills auditing with those Master’s students who had undertaken an option in Digital Curation. The Framework has also been useful to the University of Aberystwyth in devising its MSc Digital Curation programme. The Professor of Library Science at Purdue University Libraries reported that the Framework has been helpful in understanding the impact of various aspects of the curriculum and the importance of understanding the needs of various professional audiences13. This case work in HEIs, rich in digital humanities activities, embeds the DigCurV framework firmly within the DH context.  ",
       "article_title":"DigCurV: curriculum framework for digital curation in the cultural heritage sector",
       "authors":[
          {
             "given":"Ann",
             "family":"Gow",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Laura ",
             "family":"Molloy",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Leo",
             "family":"Konstantelos",
             "affiliation":[
                {
                   "original_name":"University of Melbourne",
                   "normalized_name":"University of Melbourne",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/01ej9dk98",
                      "GRID":"grid.1008.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "data management",
          "cultural heritage",
          "digital preservation",
          "digital curation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This paper reports on the interdisciplinary project VisArgue which is concerned with the automatic linguistic and visual analysis of political discourse with a particular focus on the concept of deliberative communication 12345. According to the theory of deliberative argumentation, stakeholders participating in a multilog, i.e. a multi-party conversation, should justify their positions truthfully and rationally and should eventually defer to the better argument.  Automatically measuring the deliberative quality of a multilog calls for an identification of linguistic cues that shed light on issues such as objective vs. subjective argumentation, invocation of  the common good or democratic notions as part of the argument. Notions  such as speaker stance, speaker belief/certainty are also immediately  relevant, as is an analysis of rhetorical devices known to trigger  conventional implicaturs 6. In short, a promising way of arriving at an operationalization of the  indications for the deliberative quality of a multilog is a linguistic  analysis of the linguistic cues present in the multilog.    This paper presents on-going work on analyzing strategies for argumentation via automatic means, using public data from a German arbitration. In addition to a thorough linguistic analysis of the relevant parameters, we provide a computational implementation that automatically annotates  the corpus with respect to pragmatically relevant features.  This  implementation combines a rule-based system that reflects deep  linguistic analysis with a visual analysis system that also provides results with respect to more shallow natural  language processing methods such as keyword identification, topic  modeling, and standard calculations with respect to length of  utterances, amount and type of turn-taking, etc.  2. Data In Germany, the method of deliberative discourse has been increasingly applied to the resolution of large-scale public conflicts since the early 1990s. One recent well-known example is the public arbitration process on \"Stuttgart 21\" (S21), a new railway and urban development  project in the city of Stuttgart. In response to massive public criticism of the project, a public arbitration procedure was established. The data for our initial investigation are the transcribed minutes of the  S21 arbitration process, which consist of nine days of sessions, each  lasting for around seven hours, with a total of about 70 different  speakers. The transcripts consist of spoken German conversation between  mediator, experts, project supporters and opponents and are converted  into an XML-readable format in order to facilitate later processing and  annotation. Based on the information contained in the web transcripts,  the XML trancripts are annotated with speaker information and the  general topic of the session. Overall, the transcripts contain around  265.000 tokens in 1330 utterances.  In order to arrive at a more fine-grained analysis of the discourse, all utterances have to be split up into elementary discourse units (EDUs) 7. Although there is no consensus in the literature on what EDUs are, in general, each DU is assumed to describe a single event (e.g., 8). In the case at hand, we approximate this by treating all lexical items between two punctuation marks as belonging to one DU, a method that is commonly assumed in discourse parsing. 3. Linguistic background A central aspect of our work is a linguistically motivated operationalization of features that indicate the deliberative quality of a multilog, important parameters being the realization and the communicative function of arguments in the discourse as well as speaker stance and speaker belief. We have decided to initially focus on just two of the relevant linguistic parameters found in German: the interaction between causal discourse connectors and modal particles.    Causal discourse connectors (e.g., da, weil, denn, zumal 'because (of)/due to/as') generally introduce a justification of a speaker's statement (e.g., 9). These connectives and the justification they indicate can be extracted automatically. However, the precise shade and force of the argument being made, including  speaker stance and speaker belief are modulated in spoken German by a  heavy use of modal particles (e.g., 1011).  For instance halt or eben indicate a conventional implicature that the speaker believes the argument to refer to an immutable constraint imposed by the outside world, exemplified in (1). The particle ja in (2), in contrast, signals that the speaker assumes that the content of the argument is part of the common ground of the multilog participants.               (1) [...] weil halt      in  dem  Bereich  die   meisten   Autos   unterwegs    sind.              [...] as    HALT    in  Art    area      Art   most       car.Pl    underway    be.3.Pl              '[...] because most cars are underway in this area.' (Dr. Heiner Geissler, S21, Nov. 4th 2010)                        (2) [...]  da  Sie                  ja     gesagt           haben,       dass  [...]               [...]  as   Pron.2.Sg.Pol   JA    say.Past.Part   have.Inf     that     [...]                '[...] as you JA said that [...]' (Tanja Gönner, S21, Nov. 4th 2010)               3.1. Ambiguity Ambiguity presents a serious problem for the automatic extraction and identification of both causal connectors and modal particles. For example, especially da 'as' presents a challenge for automatic processing, because of its multi-functional usage as either the temporal or locative pronoun 'there' or as a connector meaning 'because'. However, such ambiguities can be largely resolved by taking linguistic factors such as the position of the connector, its neighboring elements and the general structure of the carrier sentence into account. In (3), we schematize the identification rule for the German causal connector da 'as'.   (3) IF da not followed by verb AND               da not preceded by a particle or another causal connector AND               final verb is an infinitive THEN                    da is a causal connector.    The same procedure is followed with respect to modal particles such as eben, which can be a focus particle, a temporal adverbial meaning 'just', or a modal particle that indicates the speaker's resigned acceptance of a fact due to an immutable constraint 12.  3.2. Inference rules While these two dimensions are by themselves important for the interpretation of a given discourse, the additional benefit for measuring deliberation results from a combination of the two dimensions. Taking the example in (1), the inference rule in (4) yields the annotation in Figure 1. (4) IF causal connector found AND causal connector followed by particle denoting immutable constraint THEN annotate the DU start tag with <DiscRel=\"justification\" CI=\"immutable_constraint\">    Fig. 1: Annotation of example (1).  On the other hand, the rule in (5) deals with the combination of the  causal connector da and the modal particle ja, rendering the  annotatation of example (2) in Figure 2.         (5) IF da is used as causal connector AND                da is followed by particle denoting common ground THEN                    annotate the DU start tag with <DiscRel=\"justification\" CI=\"common_ground\">.   Fig. 2: Annotation of example (2).  These inferences, which perform context-sensitive linguistic annotation of discourse units, help to interpret the  whole discourse and shed light on the way speakers and listeners  interact, incorporating detailed linguistic knowledge about syntax and discourse pragmatics. Despite the comparatively small corpus, it is nevertheless difficult to see  overall patterns of  argumentativity at a glance, while still  maintaining a detailed view on single annotations. In order to overcome  this drawback, we introduce a  visualization system which encodes those  annotations visually and makes the patterns more accessible.  4. Visualizing argumentativity  The visualization of linguistic patterns has been shown to shed light on a  number of phenomena, from theoretically motivated topics like  phonological patterns 13 and lexical semantic  change 1415 to machine learning issues with  respect to clustering 16. The goal of visualizing the structure of argumentativity across the discourse is twofold: First, patterns of argumentation that have been identified through the linguistic inference rules can be analyzed in their context. Second, the distribution of arguments over the course of the conversation may reveal additional knowledge on  the deliberative quality of different parts of the overall discourse.  Figure 3 shows a visualization of parts of the S21 arbitration session on Nov. 4th 2010, each sentence occupying one line, each speaker turn contained in a grey square. The bars marked in yellow represent discourse units  containing justifying statements. The tool is interactive in that the  user can zoom in and out of the discourse and can investigate the  relevant discourse units in detail without loosing the overall  distribution. A detailed view on the data is shown in Figure 4.    Fig. 3: Visualization of justifying statements in the S21 arbitration on November 4th, 2010.    Fig. 4: Detailed visualization of justificational discourse units.  5. Summary and future work This paper presents an approach  of operationalizing the notion of deliberation using discourse  connectors and modal particles in order to shed light on the way  arguments are exchanged and how speakers and listeners relate to them.  By using a visualization approach, the annotated data can be inspected  over the whole discourse, allowing for an interpretation of the role  that argumentativity plays in the arbitration. In the future, we will  incorporate more linguistic cues that are relevant for deliberative  communication and also deal with multiword instances that are relevant  on a number of levels. With  the increasing number of annotation levels, the visualization will be extended to show the interactions between  different levels, allowing for more insights into discourse structure  and eventually deliberation.  Acknowledgements We thank Mennatallah el Assady, Manuel Hotz and Rita Sevastjanova for their help with implementing the discourse visualization and the German Ministry for Education and Research (BMBF) for their funding under the eHumanities research grant 01UG1246.  ",
       "article_title":"Towards visualizing linguistic patterns of deliberation: a case study of the S21 arbitration",
       "authors":[
          {
             "given":"Tina",
             "family":"Bögel",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Valentin",
             "family":"Gold",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Annette",
             "family":"Hautli-Janisz",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Christian",
             "family":"Rohrdantz",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Sebastian",
             "family":"Sulger",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Miriam",
             "family":"Butt",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Katharina",
             "family":"Holzinger",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          },
          {
             "given":"Daniel A. ",
             "family":"Keim",
             "affiliation":[
                {
                   "original_name":"Universität Konstanz",
                   "normalized_name":"University of Konstanz",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/0546hnb39",
                      "GRID":"grid.9811.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Discourse analysis has for the past half century been a staple of the text-based historical and social sciences. Part and parcel of the ‘linguistic turn’ of the 1960s, French discourse analysis was furthermore one of the first disciplines to embrace computational text processing when Michel Pêcheux developed a computer program to identify ideological processes in textual corpora 1. Steeped in contemporary linguistic theory, Pêcheux and his team sought an automated method for uncovering hidden ideological meanings in text corpora. That same year, Michel Foucault’s L’Archéologie du savoir broadened the conception of ‘discourse’ and of the underlying power politics at play in its formation 2. Diverging significantly with Pêcheux, Foucault’s analytical model of ‘archaeology’ brought with it a less strictly-linguistic approach to the discursive. By “loosening the embrace [...] of words and things”, discourses are understood “as practices that systematically form the objects of which they speak” 3. This expanded notion of discourse would go on to exert a strong influence on French historical studies, and in particular, on the historiography of the French Enlightenment and Revolutionary periods, as is evident in the work of François Furet, Lynn Hunt or Keith Baker 456. More recently, Sophia Rosenfeld and Dan Edelstein have re-introduced the specifically linguistic elements of discourse analysis back into the historian’s and literary scholar’s toolbox, most notably through the analytical use of historical and linguistic databases  78. With the rapid growth of digital text collections, a revisiting of Pêcheux’s earlier notion of an ‘Automatic Discourse Analysis’ approach would seem warranted, particularly given recent developments in information retrieval such as topic modeling 9. This paper is thus an attempt at reconciling the computational and the discursive, using topic modeling to uncover Enlightenment discourses in the Encyclopédie of Diderot and d'Alembert. Moreover, Foucault’s concept of archaeology is used to justify topic modeling’s ‘bag of words’ analytical model, as it frees us from exclusive interest in language structure, and what that structure conveys, and orients us more towards the association of the various ideas or ‘topics’ that form a discourse. 2. Topic Modeling as Discourse Analysis Topic modeling is a machine learning approach that was originally designed as a way to classify large amounts of text with minimal human intervention 10. David Newman and Sharon Block have furthermore demonstrated through their use of pLSA (Probabilistic Latent Semantic Analysis) that such unsupervised algorithms can provide a unique overall picture of the contents of a corpus by organizing the data in a manner that gives researchers an objective and wholly original perspective on the texts being analyzed 11. Of course, categorizing texts with no human interaction is not something humanists can accept without question, and this critical point forms the basis of our previous experiments with supervised classification algorithms such as Naive Bayes and Vector Space 1213. For this project, we employ the Latent Dirichlet Allocation (LDA) algorithm as it is built upon the important premise that documents, however focused, are never about one single topic, but are the result of multiple topics bound together in a single text unit 14. Consequently, the documents analyzed by this algorithm will be identified by a unique signature: a distribution of topics that represents the variety of things discussed in them. As these clusters of words do not necessarily map onto what humanists consider a ‘topic’ or theme, we judge their coherence based less on their thematic consistency and more as the representation of a particular discourse, where closely related concepts are used together in a given context. One could imagine, for instance, finding a discourse that never seems to be the main subject of any document, but that nevertheless runs through a significant number of them. 3. Use Case: Topic Modeling the Encyclopédie As a use case, we have chosen to examine one the Enlightenment’s exemplary texts, the Encyclopédie of Diderot and d’Alembert 15. Our aim here is to use LDA to go beyond the disciplinary boundaries of the editors’ original classification scheme, which was designed (along with the cross-references) to connect articles amongst themselves across the whole work, but in reality did little to provide guidance to its readers. The physical structure of the text, which caused articles to be read in relative isolation from others, made obtaining a full dialogic perspective of any given class or article unrealistic. By using topic modeling as a discourse analysis tool we aim to highlight each article’s unique discursive makeup. This will allow us to generate a more transversal view of the encyclopaedia and its contents. Whereas David Blei has asked: “What is the likely hidden topical structure that generated my observed documents?” 16; we likewise ask, “what are the non-obvious discourses than span across multiple disciplines in the Encyclopédie?”  From a methodological perspective, we are using the well-known machine learning toolkit MALLET with several Python wrap-arounds 17. Since our goal is to uncover discourses across the entire Encyclopédie, we settled on a relatively low number of topics (between 280 and 360) compared to the total number of classes of knowledge (2,900), but this number was consistent with our previous machine classification experiments 18. Once our topic model was generated, we stored the results in a SQLite table, along with all available metadata. We then wrote a web interface to visualize this database and run queries against the original metadata. Using the above interface we were able to identify many of the Encyclopédie's disciplinary vocabularies in our topic lists, which were verified using article metadata. Not surprisingly, the ‘chemistry’ topic was found most in chemistry articles, the ‘botany’ topic in botanical articles, mathematics in mathematics, etc. What interests us, however, are topics that are both distinct in nature -- i.e., identifiable with a particular ‘discourse’ -- and that span multiple disciplinary boundaries. Mapping these discourses through the various classes and articles in which they are prevalent leads to a greater understanding of the dialogic and discursive elements at play in the seemingly innocuous encyclopaedic classification system.  The topic we have identified with the discourse ‘droit naturel’, for instance, is present in more than 60 grammar articles, almost double that of its own class (Figure 1).   Fig. 1: Topic #56: \"Droit Naturel\"  Among the top grammar articles we find the small unsigned article ‘Inviolable’ that has since been attributed to Diderot. In it, alongside the grammatical definition of the term, we find a usage example that reads: “La liberté de conscience est un privilege inviolable” (8:864) -- a reference that subtly places freedom of thought amongst other ‘natural’ and unalienable rights. We find a similar treatment in the article ‘Supplanter’, which contains a thinly-veiled condemnation of tyranny as an unnatural state of governance. Other classes function in much the same way as Grammar, allowing the philosophes to smuggle controversial opinions into articles of a seemingly neutral scope. By tracing the presence of various discourses in an inter-disciplinary manner we can begin to uncover the various subversive, discursive, and ideological practices in play over the entirety of the Encyclopédie. The discourse around morality, for instance, is found in no less than 94 articles from the ‘géographie ancienne’ class (Figure 2).   Fig. 2: Topic #227: \"Morale\"  Diderot is here again exemplary in his discursive acrobatics. Whilst describing a tribe of ancient Thracians in the article ‘Dranses’, he quickly turns the discussion towards moral relativism (in a move the prefigures his later work, Le Supplément au voyage de Bougainville), with the assertion that: “Ce n'est pas la nature, c'est la tyrannie qui impose sur la tête des hommes un poids qui les fait gémir & détester leur condition” (5:106).  A similar deployment of the discourse around ‘le culte religieux’ -- a subject on which the encyclopédistes were forced to tread lightly -- can be found in the more than 100 articles labeled as ‘histoire moderne’ (Figure 3).   Fig. 3: Topic #242: \"Culte Religieux\"  The article ‘Schooubiak’, for instance, is another unsigned (but later attributed) article by Diderot, in which he describes an Islamic sect that practices an unusual form of religious tolerance. This seeming incongruence with the accepted cultural stereotypes of the time allows Diderot to raise the issue of religious intolerance thus using the sect as a proxy for the philosophes themselves, and condemning those who would oppose them: \"les prêtres étant les mêmes par-tout, il faut que la tolérance soit détestée par-tout\" (14:778).  As we have seen above, many of these encyclopaedic discourses were deployed subversively in order to move the narrative of Enlightenment forward, and indeed, the discursive nature of the Enlightenment in France has recently been brought to light 19. By extending the reach of our topic modeling approach to other Enlightenment texts we can begin to identify the discursive practices of texts and authors on an even greater scale and with a greater level of systematicity. As a philosophic war machine, as well as a contemporary reference work, the Encyclopédie is an ideal starting point for this sort of work. Many of the discourses we find therein may have been lost to the modern reader through the classification process itself, and still others may prove useful in uncovering interdisciplinary connections that would have otherwise gone unnoticed.  ",
       "article_title":"Discourses and Disciplines in the Enlightenment: Topic Modeling the French Encyclopédie",
       "authors":[
          {
             "given":"Glenn",
             "family":"Roe",
             "affiliation":[
                {
                   "original_name":"The Australian National University",
                   "normalized_name":"Australian National University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/019wvm592",
                      "GRID":"grid.1001.0"
                   }
                }
             ]
          },
          {
             "given":"Clovis",
             "family":"Gladstone",
             "affiliation":[
                {
                   "original_name":"The University of Chicago",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          },
          {
             "given":"Robert",
             "family":"Morrissey",
             "affiliation":[
                {
                   "original_name":"The University of Chicago",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "ontologies",
          "literary studies",
          "french studies",
          "cultural studies",
          "text analysis",
          "information retrieval",
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Reinvention of the Publishing Industry Having developed his commercial expertise as an advertising executive, James Patterson went on to forge a publishing empire that, according Forbes, earns him in the region of $91 million each year.1 Leaving his position as an adman to become a full-time writer in 1996,2 a decade later his work had already grossed $1 billion.3 To date, his name has appeared on over 100 books that have sold a combined 300 million copies.4 Patterson is the driving force behind the approach to marketing that has allowed him to overtake his competitors at the top of the bestseller lists, from dictating the advertising campaigns for his titles, to the manner by which they are distributed.5 Patterson’s method is described as “a literary assembly line”,6 an observation which he seemingly embraces: “I look at it the way Henry Ford would look at it,” is his response to criticisms of his approach.7 His vision is such that Little, Brown, Patterson’s publisher, has restructured its organisation in an effort to meet the author’s requirements. In a lengthy piece for The New York Times, Jonathan Mahler quotes former Little, Brown publisher, Sarah Crichton: “To have one writer really start needing, and even demanding, the lion’s share of energy and attention was difficult. There were times when some of us resented that. When Jim felt that resentment, he roared back. And he was too powerful to ignore.”8 In his case study of Patterson’s marketing methods, John Deighton outlines how the author commissioned his own studies in an effort to identify what potential readers want from a novel, so that he could deliver his titles to the widest possible audience: “The Little, Brown publishing group recognized that Patterson was a most unusual author, one who could teach them a lot about selling.”9 Patterson is, according to Time-Warner Publishing’s Larry Kirshbaum, “the first real brand-managed author”.10  As a reader himself, critically acclaimed writers such as James Joyce and Gabriel García Márquez are amongst Patterson’s preferred authors.11 Patterson admits that there is a distinction between the aforementioned literary works and his novels: \"These books are entertainments. It's a very different process than if you're trying to write Moby-Dick, or The Corrections. That's painful. That's different from very simple, plot-oriented storytelling.\"12 Patterson is frequently criticised for his approach, but writing in the fashion of Joyce or Marquez, is a task which he suggests is beyond his ability as an author: “After reading Ulysses, I knew I couldn’t write anything that great. I don’t have it in me.”13 Thus, producing a high volume of entertaining novels has been his objective.   2. Patterson's Collaborative Process To achieve the prolific output that we see today, Patterson enlists the support of numerous collaborators: “It was Patterson who first showed that television advertising could work for books. More radically, he has demonstrated that working with co‑writers can dramatically multiply sales.”14 Patterson sees little difference between his approach to collaborative writing and those practices that have long been central to other sectors within the culture industry: “It isn’t terribly groundbreaking … The newspaper business, the movie business—they’re full of teams. A lot of art was done by teams …”15 “My short answer to the question as to why work with other people is Gilbert and Sullivan, Rodgers and Hammerstein, Woodward and Bernstein, Lennon and McCartney and it goes on,” he offered in The Guardian. Patterson is quite forthright when detailing his collaborative process: “I’ll write an elaborate outline, maybe 70 pages, very detailed, clear, and focused. The co-author will write the first draft, and I’ll see the work every few weeks. I’ll do two to seven more drafts.”16 Described as “a natural born writer”, many of Patterson’s collaborators have used the opportunity as a platform from which to launch their solo careers.17 Gaby Wood comments in The Guardian that the sentences in Patterson’s novel “are not designed to be lingered over”, “[t]hey are more or less all plot”.18 But, while it seems that Patterson revises each chapter as they are drafted, as well as making all of the final revisions alone,19 the extent to which he contributes authorial material directly, in terms of writing, remains unclear. Patterson has stated that his “name on the cover is the assurance of a good read”20 – using computational methods central to Digital Humanities scholarship, this paper seeks to determine the volume of writing that Patterson contributes before offering such an assurance.   3. Methodology & Results We evaluated the relative contributions of Patterson and his collaborators using versions of Burrow’s Delta, a widely-used lexical measure for English texts.21 We selected the collaborators Peter de Jonge and Andrew Gross for this investigation. Patterson, by his own account, allocates most of the actual writing to his junior partners. It was in such a respect that we formed the working hypothesis that the collaborative works would be stylometrically more similar to texts written primarily by Patterson’s co-authors, than to any of the novels attributed to Patterson alone. We expected the lexical features that we employed to pick out the primary writer. This would correlate with much of the field's existing research, for instance by Patrick Juola,22 who suggests that, in attempted forgeries, the lexical signature of the forger overrides the semantic content which might associate it with the impersonated party. However, this is not a foregone conclusion. Jan Rybicki has demonstrated that in literary translations, the original authorial signals dominate that of the translator when using Burrow’s Delta cluster analysis. In other words, the semantic imprint survives the translation process although all of the lexical features examined are written by the translator.23 Our second hypothesis was that, given the former, Patterson’s contribution would be strongest at critical moments in the text. Given the plot-driven genre, we believed that these would typically be present at the beginning and end of the novels. To test our first hypothesis, we employ a \"bootstrap consensus tree\" cluster analysis over maximum frequency words ranging from 100 to 1000, in intervals of 100, with the Burrow’s Delta metric, using the \"stylo\" package for R.24 25 We avail of a consensus strength of 0.5, meaning that we formed a tree showing proximity wherever this occurred in 50% or more of the 10 maximum frequency clusterings described.26 27 For our second hypothesis, we use the Rolling Delta technique.28 2930 31 To provide a general intuitive description of this method, Burrow’s Delta distances are measured between the collaborative text and single-author texts for each participating author. However, distances are measured to \"windows\" of the collaborative text, allowing for estimation as to which sections carry the stylistic fingerprint of one contributor over another. Sample single-author tests are then plotted over the baseline of the collaborative text, where greater proximity to the baseline indicates greater stylistic similarity, as defined by the delta distance metric. In this paper, we examine the following collaborative texts:  Patterson & De Jonge:  Beach House (2003) Beach Road (2006)  Patterson & Gross:  2nd Chance (2002) 3rd Degree (2004) Judge and Jury (2006)  Our solo texts, by author, are as follows:  DeJonge:   Shadows Still Remain (2009) Buried On Avenue B (2012)  Gross:   The Dark Tide (2008) Don't Look Twice (2009) Killing Hour (2011) 15 Seconds (2012) No Way Back (2013)  For Patterson, we used this fixed set of nine solo works:  First to Die (2001) Four Blind Mice (2002) The Lake House (2003) London Bridges (2004) Maximum Ride: The Angel Experiment (2005) Maximum Ride: Saving The World And Other Extreme Sports (2007) I, Alex Cross (2009) Fang (2010) Nevermore (2012)  The following visualisation displays our bootstrap consensus tree over the entire dataset:   Fig. 1: Boostrap consensus tree  As predicted, the collaborative works all cluster with the respective junior writer. Within both the De Jonge and Gross clusters, the collaborative works form a distinct sub-cluster. Within the Patterson cluster, the Maximum Ride series of novels are separated from another cluster consisting of Alex Cross novels and the Patterson novel, The Lake House. One surprise result is that First to Die, a solo Patterson text, is clustered with the subsequent works in the Women’s Murder Clubseries, which he wrote with Andrew Gross. This could simply represent a limitation of the delta metric over these texts, or alternatively, it could indicate that Gross was so influenced by the particular style that Patterson manifested in this work that he imitated it more exactly than Patterson managed in any of the other works under examination. We discount a third possibility, that the new collaborative series, Women’s Murder Club, opened with a solo Patterson work to kick-start sales. While such an interpretation would align with the marketing ingenuity of both Patterson and Gross, it is without sufficient empirical foundation. Our full study comprised rolling deltas for all collaborative texts, under a number of different setting. For the purpose of this abstract we include just two rolling delta studies, First to Die and its sequel in the series, Second Chance, respectively, both with pronouns deleted from an initial most frequent word count of 1,000:   Fig. 2: Rolling Delta analysis    Fig. 3: Rolling Delta analysis  For Second Chance, Gross’ texts are closest throughout, apart from First to Die, which, as we have already discussed, is attributed solely to Patterson. The model in First to Die is more interesting as the work appears like a true collaboration in which the authors have shared the task of writing passages or sentences with Patterson intervening at critical junctures.   4. Conclusions The quantitative data suggests that Patterson’s collaborators perform the vast majority of the actual writing. Therefore it seems that, unlike translation, the semantic signal from Patterson is dominated by the lexical signal of the other writer. We think the explanation for this difference is the density of the semantic message in each case – when implementing the outline of a general plot, there is more freedom for sentence and passage construction. A full stylometric study of Patterson would also need to measure his contribution to the abstract entity called the plot  in the works under examination. As Patterson says: “above all my brand stands for story. I became successful when I stopped writing sentences and started writing stories. Editors think it's about style. It's not. It's all story”.32 On the one hand, we note that for Patterson, this is just as well, since our analysis shows that his stylometric fingerprint is sometimes weak, even in his solo works. On the other hand, we recall Aristotle also believed, in his analysis of tragedy, that plot (μῦθος) - the “arrangement of the incidents (ἡτῶνπραγμάτωνσύστασις)” is the most important element of the work.33 In the Arcades Project, Walter Benjamin collects several references to writers who brought industrial methods of production to bear upon the process of literary creation in 19th Century France. While the 19th Century “witnessed an institutionalization of the split between technology and art to a degree previously unknown in history”.34 Benjamin pays particular attention to those who found new ways of conjoining the two, such as Eugene Scribe, popularizer of the “well made play (piece bien faite)”, and Alexandre Dumas, who both industrialised the writing process. Dumas was contemporaneously described as running a “factory of novels,”35 and, like Patterson, simultaneously worked on multiple novels, with an output of 400 novels and 35 dramas in 20 years.36 Lucas-Dubreton recounts how accusations of plagiary by one De Mirecourt’s eventually led to Dumas publicly recognising his co-authors, and arranging better terms of payment for them.37  In language which foreshadows the descriptions of Patterson as a “brand-managed” author, Lucas-Dubreton caricatures the allegations against Dumas thus: “Dumas did not exist at all, he was only a myth, a trademark invented by a syndicate of editors to dupe the public”.38 Patterson and Dumas employ modes of authorial production which echo the economic advancements of their times. Unlike Dumas, Patterson has never been questioned in relation to his collaborative process, and such anomalies as we have detected probably indicate most of all the immaturity of stylometry as a method. Regardless, it seems Patterson, an epitome of benign capitalism, has offered sufficient accreditation, tutelage and financial reward for those who work for him. In this paper, we will discuss our analysis in the context of these publishing practices, both new and old, and how literary collaborations might be approached from a stylometric perspective, using Patterson and his reinvention of the industry as a useful test case.  ",
       "article_title":"Beyond Style: Literary Capitalism and the Publishing Industry",
       "authors":[
          {
             "given":"Simon",
             "family":"Fuller",
             "affiliation":[
                {
                   "original_name":"National University of Ireland, Maynooth",
                   "normalized_name":"National University of Ireland",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/00shsf120",
                      "GRID":"grid.9344.a"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"O'Sullivan",
             "affiliation":[
                {
                   "original_name":"Pennsylvania State University / University College Cork",
                   "normalized_name":"Pennsylvania State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04p491231",
                      "GRID":"grid.29857.31"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Literary Studies",
          "Stylometry"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Accounting documents seem to be are well suited digital reproduction: they contain a significant number of highly structured information which was meant to be calculated already in its original context . Creating a database for executing statistical and numerical analysis of the data seems to be a promising method to analyse these documents. This assumption is not reflected in the practice of critical scholarly editions of accounting books. The digital humanists creating resources of accounting documents have not succeeded to build a common form of representation apart from the preference of spread-sheet style data storage. This approach neglects important information in the original documents which become in particular clear when studying medieval account books:  The recent scholalry edition of the Luxemburg city accounts 1 shows that accounting is close to everyday language and thus an important source for research on the history of a language. A digital resource concentrating on numbers, short descriptions of bookkeeping entries and classifications excludes that information. Producing an account in the middle ages was a process which included several steps: collecting the information from informal documents into a fair copy, reading the listed transactions to a supervising committee or to a person using the abacus, deleting debit entries with their fulfilment, entering debits for tax or rent collection and updating the entries during money collection while tax collection or in manorial administration. Medieval accounts thus can be a protocol of acts of accounting and controlling. The layout of medieval accounts changed in time, i.e. for example in administrative accounting the layout developed from protocols in simple text blocks to sophisticated tabular representations2. The early forms of double entry bookkeeping are based on the position of the entry on the page 3. The development of the visual form of accounting documents is part of the research on the history of accounting. Account books give an insight in everyday life and its economy which can be researched only when the variety of existing entries is transfered into a taxonomy. Medieval and early modern accounts used variety of taxonomies to structure the data. All this can only be accessed by researcher with a thorough encoding of the “content”.  Traditionally this multiple interest in the account books led to a decision on one preferred approach. On paper the text could be printed economically only either as tables or as full transcriptions. The current state of digital edition theory promises to solve this problem: an edition as a digital resource can include several layers of interpretation and leave the decision on the presentation to the user. But a brief survey done on relevant digital resources 4 shows these possibilities are rarely exploited in full. The paper discusses some reasons for this situation, in particular the influence of the dominant schools in digital scholarly editing and the resulting models which focus on textual variance, representation of text and documentary edition. Syd Baumann and Kathryn Tomasek have already suggested changes in the de-facto standard for encoding of historical documents the TEI 5. They propose an element to describe transactions by referring to the textual parts describing the business facts. An alternative standard for the encoding of business facts can be found in XBRL6 which offers a flexible methodology and in the “Global Ledger”-module a taxonomy of basic business facts which can be transferred to many historical documents. While the TEI guidelines are good to encode the accounts on the documentary and linguistic level, XBRL seems to offer a good base to model the economic content. A simple bookkeeping ontology would have to include the following facts: gl-cor:entryDetail(gl-cor:account, gl-cor:signOfAmount(bk:i, bk:d), gl-cor:amount(tei:measure(tei:quantity, tei:unit), gl-cor:debitCreditCode), bk:price(tei:measure[x], tei:measure[y]), bk:transaction(bk:when, bk:where, bk:who, what, gl-cor:entryDetail[bk:debit], gl-cor:entryDetail[bk:credit]) which represents the single entry (gl-cor:entryDetail), their organization in bookkeeping (bk:account, gl-cor:debitCreditCode), the central economical informations (amounts, increase/decrease, prices) and the social act of transaction (bk:transaction). Departing from the theoretical considerations presented by Manfred Thaller at the DH2012 7 the paper tries to develop a RDF-model which integrates the multiple layers how an account book can be conceptualised in a digital scholarly edition (text as image, as trace, as language and as meaning). The model is based on references between an URI for the physical object, the images of the object, the transcription and the bookkeeping facts (see fig. 1).   Fig. 1: conceptual model for stand-off markup of digital scholarly editions  The model can be aligned with upper level ontologies like the CIDOC-CRM 8  as the basic entities relate to linguistic objects (crm:E33, text as language), to images (crm:E38, text as image), to inscriptions/marks (crm:34/crm:37, text as trace) and to facts which can be described as events (crm:E5, business transactions as the meaning of the accounts), although substantial intermediate steps have to be included (e.g. price relations, measurements and monetary values as subclasses of propositional objects, crm:89). It allows like XBRL the inclusion of taxonomies for commodities or for types of transaction. The model can be serialized with the help of the feature structures in TEI and converted into explicit RDF with simple XSL which includes the transformation of DOM relations (e.g. ‘contains’) into ontology statements (e.g. <list ana=”bk:d”><item ana=”bk:entry”>For <seg ana=”bk:account”>wood</seg> <measure ana=”bk:amount” quantity=”10” unit=”lb”>x lb</measure></item></list>). First experiences in a project to create a digital edition of a whole series of early modern city accounts show that the model can be used efficiently when the encoding of this kind of structures is supported with a TEI customisation which helps the transcribers to replace repetitive and verbose code with simple XML tags (like <r:lb> for <measure type=”currency” unit=”lb”> or <r:e> for <item ana=”bk:entry”> and when repetitive tasks like the transformation of roman numbers can be included in the XSL transformation from customised code to TEI.  The paper will demonstrate the application of the model to several late medieval account books in a TEI serialization9. The integration in the GAMS repository infrastructures10 allows showing possible functionalities of digital editions based on the model. This includes joint operations on the XML text stored in a Fedora Commons repository and the RDF representation stored in a triple store.  ",
       "article_title":"Modelling digital edition of medieval and early modern accounting documents",
       "authors":[
          {
             "given":"Georg",
             "family":"Vogeler",
             "affiliation":[
                {
                   "original_name":"Zentrum für Informationsmodellierung - Austrian Centre for Digital Humanities, Universität Graz",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "medieval studies",
          "ontologies",
          "metadata",
          "content analysis",
          "linking and annotation",
          "encoding - theory and practice",
          "semantic analysis",
          "scholarly editing",
          "historical studies",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Participatory and collaborative sense-making of complex phenomena is central to productive learning and knowledge work in today’s information-rich world.1 The Lacuna Stories Project creates an exploratory, interactive, and collaborative online space where users can research and discuss significant historical events like 9/11. Lacuna Stories draws together primary source documents, fiction, scholarship, wikis, and user-generated forums and blogs. The online space extends current digital annotation software with functionality that encourages skills such as historical thinking, close reading, and comparison of media and sources concerning 9/11. When approached independently, individual sources, genres, and media inevitably fall short of stitching together the “whole story.” The Lacuna Stories Project’s diverse, multimedia environment provides tools for instructors, students, and the general public to “mend” the gaps in our knowledge of major historical events in order to develop their own narratives. User data generated through the site’s design also will allow researchers to compare and better understand reading and engagement behaviors of students; along with other forms of user experience research and assessment, this research also provides directions for improving the platform and instructional materials.   The Lacuna Stories Project is a cross-disciplinary collaboration that includes faculty whose research interests span literature, pedagogy, historical thinking, public humanities, and platform studies. This short paper will describe the research and pedagogical goals of the Lacuna Stories Project as well as the technological innovations developed to support these goals. By the time of the conference, Amir Eshel (PI) and Brian Johnsrud (Project Manager) will have taught a course during Stanford's Winter Quarter in which students use the Lacuna Stories Platform in and outside the classroom. Johnsrud and Michael Widner (Technology Director) will also have taught a course in the same quarter on building in the digital humanities that will use Lacuna Stories as its primary example. This paper will touch upon how the prototype encouraged student learning and collaboration by presenting our data gathered from student use, interviews, and focus groups. By the time of the conference will also have piloted Lacuna Stories for a test-group of up to 30 public users without a college degree to compare experiences of different kinds of users in and out of the classroom, with and without formal academic training in approaching different kinds of historical texts and media.   Compared to other annotation and archive projects, the Lacuna Stories platform provides three key innovations. First, it creates an integrated multimedia environment that encourages the development of core skills for learning and knowledge work: navigation, critical reflection, linking, synthesis, and collaborative sense-making. Second, no existing digital annotation tool connects multiple types of media together to compare and generate new narratives. We are coordinating with MIT's HyperStudio to add this functionality to Annotation Studio (www.annotationstudio.org) and incorporate it into an ecosystem of digital tools for collaborative learning. Third, Lacuna Stories will provide a novel, curated set of diverse 9/11 resources for users to engage with and connect in innovative ways.   Fig. 1: Annotation Functionality  Lacuna Stories is also a platform that fosters good habits of close reading and thinking historically, whether the users are students, researchers, or members the general public. Within the humanities specifically, the interactive, multimedia functionality of Lacuna Stories goes beyond simply replacing print reading and viewing practices; rather, it creates new and innovative experiences for engaging with various texts and media that reflects the networked state of knowledge today. The platform thus builds upon the work done by Sam Wineburg, another member of the project team and Margaret Jacks Professor of Education and (by courtesy) of History at Stanford, to promote historical thinking (sheg.stanford.edu). Wineburg's work to date, however, has been focused on print resources in high school classrooms. Lacuna Stories will bring Wineburg's deep engagement with these matters to digital resources and make them available in the university setting. The project also seeks to develop an inclusive, empowering, and engaging open-source platform to gather and encourage these responses in a generative and reparative mode. The site aims not to develop a fully coherent or conclusive “truth” of the event, but to encourage the cognitive and imaginative work that inspires responses to and stories about the event, its complexity, and its diverse meanings. Lacuna Stories’ subtitle, “mend the truth,” refers to site’s ability to connect in novel ways the different text, media, and user-generated content. One of our primary contributions to the development of Annotation Studio will be to enable all aspects of the available resources—from images to individual words, lines, or documents to user-generated content—to be archived or collected by registered users into their personal “sewing kit,” which provides users a workspace for the collection, connection, and annotation of materials relevant to their learning and scholarship.   We will incorporate and extend pre-existing open-source projects for the platform, most notably Annotation Studio from MIT’s Hyperstudio group. Annotation Studio is an exemplary, user-centered tool for digital humanities work, currently under active development in conjunction with a wider set of projects to develop shared standards for annotation of multimedia content, a key requirement for widespread adoption both in and outside of classrooms. We are working closely with the Annotation Studio team to ensure that the innovations developed for Lacuna Stories make their way back into the central code base and are thus available to the broadest possible number of users and institutions. One of the core technologies powering Annotation Studio is the javascript library Annotator.js (okfnlabs.org/annotator), a project of the Open Knowledge Foundation that is quickly becoming one of the most popular annotation technologies. By focusing first on developing extensions to Annotator.js, the Lacuna Stories Project ensures that our work will be useable in Annotation Studio and by any other projects that use Annotator.js. We are also working to bring the functionality provided by Annotator.js into the Drupal platform that powers much of the rest of the Lacuna Stories site. Once this work is complete, users will be able to annotate not only texts available through Annotation Studio, but also blog posts, wiki entries, and any other content in a single, integrated environment. In our talk, we will discuss some of the challenges in the prototype phase for this work and our reasons for using Annotation Studio as a replacement until the Drupal work is complete. O’Malley and Rosenzweig argue for the growing importance of the web generally because it allows for communication and exchange of divergent interpretations of the past. The web demonstrates how “meaning emerges in dialogue and that culture has no stable center, but rather proceeds from multiple ‘nodes’” (154).2 Being able to create links between annotations and sources and annotate the quality of those connections is central to the academic process of synthesizing information across documents and reflects the natural associative mechanisms that are central to deep learning.3 4 This functionality, however, does not exist in any current digital annotation tools. Lacuna Stories seeks to change this fact, with a tool that is scalable for use in a multitude of sense-making settings. Lacuna Stories will allow users to create categories and links between items in their kit, such as connecting a line from a novel with a paragraph from a user-submitted story, a forum discussion thread, and a section from the 9/11 Commission Report. These links can additionally be connected to a larger theme as described by the user; there can also be a shared set of themes developed collectively by the group or by an instructor. Social learning will be enabled through opt-in sharing functionality, where other learners can view and extend links and notes. Such open linking from users’ digital “sewing kits” exemplifies the idea that connections among narratives can be made quickly and simply, empowering users to “mend,” create, or share meaningful associations. Moreover, this aspect of the project responds to the work done by Fred Turner (Associate Professor of Communication), another faculty member of the team, to create digital humanities projects that are public-facing and that encourage community engagement. Lacuna Stories is, then, a platform driven by the complementary research interests of a cross-disciplinary team of faculty made possible through technological innovation based on existing, open source tools. Although this paper will focus primarily upon the technology used and plans for future work, a secondary focus will be how the tools and innovations are grounded in research and pedagogy and how these interests influenced our technology choices and strategy.  ",
       "article_title":"Lacuna Stories: Building an Annotation Platform for Historical Thinking",
       "authors":[
          {
             "given":"Michael",
             "family":"Widner",
             "affiliation":[
                {
                   "original_name":"Stanford University Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Brian ",
             "family":"Johnsrud",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "annotation",
          "pedagogy",
          "historical thinking"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This short paper describes how a small-scale implementation of big data text analysis can be used for reading single texts and testing algorithmic processes. By using a small Hadoop cluster, distributed computing methods can be used to parse typographically experimental texts and delineate even minute units of meaning. This experiment takes Ron Silliman’s 26-volume collection entitled The Alphabet (1979-2008) as an object of study because it anticipates and resists quantitative analysis techniques. Silliman describes his brand of “language writing” as a kind of “composition as investigation” that requires active participation of the reader to make meaning.1 The process of running resistant texts through an algorithmic system also exposes the interpretive biases of computational methods of reading poetry. Large scale text analysis can reveal promising texts for further inquiry, but these systems can also be used on a small-scale to complement qualitative human readings with quantitative results. Matthew Jockers has recently warned in Macroanalysis (2013) that “from thirty thousand feet, something important will inevitably be missed. The two scales of analysis, therefore, should and need to coexist.”2 This paper shows how a proximate use of “distant reading” can render a text productively unfamiliar and also show how distributed computing systems can be used to structure highly experimental literary texts. Background The core of my method is animated by two observations: First, Google has become a constant touchstone for DH scholarship. The proceedings for the 2013 meeting of DH in Lincoln, Nebraska includes 156 references to Google Search or other Google products, including Anna Jobin and Frederic Kaplan’s attempt to reverse engineer Google’s autocomplete algorithms. 3 However, the scale and sophistication of Google’s systems make them impenetrable for all but the most experienced Google employees responsible for building these proprietary systems. Second, there has been a recent boom in avant-garde or experimental writing that is fuelled by the growing awareness of large scale text analysis. I argue, therefore, that experimental literature has begun to function as a resistant dataset that can test and even react to algorithmic methods. This new experimental corpus thereby engages in a reciprocal critique of both literary and technical systems. My experiment is grounded in contemporary cultural and technological contexts, while seeking to explore the relationship between corporate analysis tools and literary production. The technologies that support this “Big Data Revolution” were developed by many of the most important technology companies in the US, with Google, Yahoo, and Facebook among them. 4 Hadoop was derived from Google’s MapReduce and Google File System white papers. Doug Cutting (Yahoo!) and Mike Cafarella (Google) spun off an open source implementation in 2005 that is now released by Apache. 56 Hive was written by Facebook to streamline the process of writing MapReduce jobs in Java by allowing queries to be written in SQL. A secondary purpose for this research explores the capabilities and weaknesses of proprietary systems from their open source derivatives.  Implementation  A small-scale implementation of Hadoop and Hive on Amazon Web Services Elastic Map Reduce (EMR) platform is a highly reliable and cost effective means of working with distributed computing methods. As both a pedagogical and rapid experimental tool, EMR automates the networking between data nodes throughout the system and opens the scalability of Hadoop to an extremely broad user base. Hive allows for queries to be written in HiveQL, which does not follow the full SQL-92 standard but rather retains many of the features of MySQL. 7 While the distributed nodes are often virtualized on EMR and the user has no way to determine the full composition of the cluster, the costs remain extremely low and do not require complex discussions with system administrators in a restrictive institutional environment. 8 Hadoop’s speed and flexibility is the result of its very simple order of operations and physical architecture that allows for scaling from just three to potentially thousands of nodes. 9 As the corpus size expands, this system can scale rapidly with only modest additional investment of research funds. Analysis The term “distant reading” first articulated by Franco Moretti has come to describe the process of quantitative analysis. 10 It arose out of a very human inability to read all the texts that compose World Literature. A remarkable number of issues emerge from this very simple state of affairs. Firstly, we now have a kind of writing that acts like reading. In other words, the SQL queries used to parse and structure vast strings of data represents a readable trace of reading. Coding commands for this system represents a “composition as investigation.\" 11 Secondly, the scalability of these systems allows for machine reading systems to perform human lifetimes worth of reading tasks in mere minutes or hours. The computer’s ability to perform repetitive tasks at speeds also means that the interpretive experience of texts is now outside the domain of human perception. “The Hadoop Distributed File System” white paper described the pace of operations through the “heartbeats” that guide the operations of the distributed machines by explaining how the TaskTracker “can process thousands of heartbeats per second.” 12 Thirdly, this technological moment represents the collapse of the false dichotomy between philosophical, subjective, and speculative analysis, and scientific, objective, empirical analysis. Quantitative methods have the potential to erase the long held doctrine of the “two cultures” that divides the sciences and humanities and presumes that “Intellectuals, in particular literary intellectuals, are natural Luddites.” 13 Conclusion The use of the word “experimental” carries a strategic significance in this context. It is a word that simultaneously accesses literary, scientific, and technological discourses. My experiment is primarily animated by the “aesthetic provocation” of avant-guard writing.14 Because computation relies upon symbolically stable inputs, making computational sense of non-sense characters becomes a central challenge to overcome in the study of contemporary experimental literature with computational methods; in order to read these non-sense characters, the “stop list\" for this topic model may need to comprise the entire corpus of proper words. Rather than treating literature as strictly quantifiable data that algorithmic analysis can simply glean information from, I propose a methodology that assumes that literary information is profoundly resistant, reactive, and unpredictable. Kenneth Goldsmith claims, in Uncreative Writing (2011), that “digital media has set the stage for a literary revolution.” 15 While Goldsmith is thinking here about distribution methods on the Web, there is little doubt that literature is responding to the technological context into which it is published. It is now time to include the algorithm in this history of the avant-guard. Acknowledgements I would also like to thank the generous support of the Electronic Textual Cultures Lab at the University of Victoria and Implementing New Knowledge Environments group. This work is supported by the Social Science and Humanities Research Council of Canada.  ",
       "article_title":"Small-Scale Big Data: Experimental Literature and Distributed Computing",
       "authors":[
          {
             "given":"Aaron",
             "family":"Mauro",
             "affiliation":[
                {
                   "original_name":"University of Victoria, Electronic Textual Cultures Lab",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "algorithmic analysis",
          "distributed computing",
          "Google",
          "topic modelling",
          "distant reading",
          "Hive",
          "Hadoop",
          "experimental literature",
          "big data",
          "Facebook"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction 1.1 Overview Scripts are usually seen as simple carriers of languages. Research on scripts until recently has been minimal and niche, except for the field of paleography. Scripts are an important part of the cultural heritage of humanity and its analysis and study requires more research. Fortunately, there is a growing interest in analysis of scripts. Altmann 1 published a volume titled “Analyses of Scripts: Properties of Characters and Writing Systems” to explore various properties of writing systems and scripts such as complexity, ornamentality and distinctivity. Changizi et al 2 discuss the various contour configurations of written symbols and their similarity to the environment in which they were produced.  They also study the distribution of the configurations of various scripts. They 3 further discuss the character complexity and the redundancy of stroke combinations of various writing systems in human history. It is to be noted that analysis in [2] [3] and most methods in [1] were performed manually. Traditionally, analysis and study in paleography have also been done manually. Digital paleographic methods are at present making more inroads into the field. However, applying quantitative analysis on paleographic data is not yet popular and standardized 4. This is partially due to the difficulty of quantifying paleographical features, and partially due to the lack of defined metrics with theoretical and qualitative underpinnings. 1.2 Proposed Framework We propose here a quantitative analysis framework for scripts that is largely computational and requires minimal user interaction. We do not particularly aim at providing a completely autonomous framework, but rather to aid the user as much as possible, with the ability to manually intervene/override as required. The framework is based on various methods and techniques employed in the field of graphonomics. Our framework is grounded on the principles of handwriting production and handwriting analysis. We also explore various features used in the related area of gesture design and recognition and its application to the analysis of scripts. Through this, we attempt to find relevant metrics (with qualitative significance) with sufficient evaluation that might be used for glyphs and scripts for various purposes such as classification, visualization etc. The computed quantitative features could serve as descriptors for scripts, and  be used for comparing and analyzing scripts. This is especially applicable to the field of paleography, where such quantitative features are much needed.  2. Quantitative Analysis Framework Our proposed framework consists of the following modules.   2.1 Spline Conversion  The characters of scripts are externally represented as B-splines. B-splines are very efficient in preserving the shape and curvature of glyphic segments. Additionally, they can be manipulated without significant effort. Rather than representing glyphs as pixelated data, converting them into splines eases analysis. This conversion of the glyphic shape of a character can be done automatically or manually. In a manual process, the user defines each shape of a character directly using a set of B-splines, or explicitly draws the shape, which is then internally converted into B-splines. An automatic conversion of glyphs involves thinning and then its conversion into splines.   2.2 Trajectory Reconstruction  The shape of a character is static and does not contain all information required for analysis. Dynamic information relating to pen movements are not present in the shape. Trajectory reconstruction attempts to recover this temporal information 5. This kinematic information is essential in defining the character. With paleographic scripts reconstructing dynamic information is necessary as the trajectory is usually unknown. Also by altering the trajectory, the changes in dynamic features can be observed.  The recovery is performed by conducting a global search using a set of heuristics, such as length minimization and curvature minimization 6. Especially in case of paleographic scripts, the algorithm is able to provide several alternative viable writing trajectories.  2.3 Stroke Segmentation  Characters are best analyzed as sequences of natural strokes. Breaking them down into basic strokes is the optimal way for analyzing written characters. It also enables us to understand the process of handwriting. Stroke segmentation retrieves the structure of a character based on its trajectory. This is performed by segmentation of the character at various important landmark points of the recovered trajectory such as the extrema of curvature 7.  2.4 Character Representation  Writing is usually considered to consist of two fundamental types of strokes – up-strokes and down-strokes 8. This distinction is necessary since both these two types behave differently. It has been proved that down-strokes usually do not show a lot of variation in handwriting compared to up-strokes 9. A character is internally represented as a set of strokes. This is consistent with the way that the character is internalized and produced by humans. This allows us to derive better features that are more natural and descriptive. Later, it will be possible to apply handwriting modelling to generate alternative scribal variants.   2.5 Feature Extraction  For quantitative analysis, features need to be computed from the characters. These serve to quantify several aspects of the characters. We considered various features used in the field of gesture recognition 1011 1213 and found the features listed below to be relevant to the analysis of characters. We also propose some features in addition to those found in the literature. These features/metrics could additionally serve as descriptors for the scripts. As quantitative features these can be widely used in analysis and/or visualization.   2.5.1 Production Features  The effort that is required to realize and produce a character is an important element of its analysis.  It is related to the number of velocity inversions, number of velocity breaks, number of pen lifts, etc., which are computed from the stroke representation of the character. These features are calculated from the temporal information that was re-constructed at the earlier stage. These are relevant as they quantify the dynamic handwriting behavior present in the character.   2.5.2 Geometric Features  Geometric features throw more light on the visual aspect of characters. These are essential for the study of the judged (visual) complexity 14. The features that relate to visual appearance are compactness, openness, number of crossings, average curvature, sum of internal angles, bounding area, etc.  Some of these, like compactness and openness, are ratios of several parameters such as length of strokes, distance between first and last points,  while others, like average curvature, are derived directly from the glyph structure.  2.5.3 Cognitive Features  Though cognitive features cannot be directly measured, some cognitive features could be interpreted from the geometry of a character. The number of unique landmark points required to plan the trajectory is a possible feature that has correlation with the cognitive load of the glyph. An additional measure is the number of minimal points required to recreate the character.  2.5.4 Stroke Features  Stroke based features such as the primary direction of the glyph, ratio of upstrokes to downstrokes, direction change, histogram of inter-stroke angles etc. are also computed for a character.   3. Prototype Implementation   A prototype of the framework has been implemented in Python with the modules discussed above. We are planning to analyze the development of Indic scripts using the framework. The source code will be released under an open source license when the project reaches maturity. Its repository will also include a complete a set of Indian paleographic scripts. Below, we briefly describe functionality yet to be implemented in the prototype. From the perspective of paleographic scripts, the available glyphs in the literature are usually very noisy. Scanning and importing them would require several layers of pre-processing and noise-removal. For modern scripts, importing the Bezier curves from the respective fonts could be done directly.  Trajectory reconstruction has been implemented only for single stroke characters. A primitive implementation exists for multi-stroke characters. This needs to be made more rigorous and accurate. Based on the stroke structure of a glyph several additional features such as entropy of writing could be calculated as required. Also, the features are to be used for normalized glyph shapes. The behavior of the features with respect to various scribal variants needs to be analyzed further.  Proper visualization of the various quantitative features would help to better study and understand the characters within a script and also compare several scripts. Such visualization is particularly helpful in studying paleographic scripts and analyzing the changes that took place over time. Various statistical analyses of the features and visualization techniques would be built into the implementation.    Fig. 1: Spline Conversion and Trajectory Generation    Fig. 2: Stroke Segmentation & Feature Extraction  4. Similar Projects There are other digital paleographical projects, such as Hand Analyser by Peter Strokes, which work on pixelated images. The current project is more focussed on using strokes to derive various features. Integration/Adaptation of those techniques needs to be further looked into.  5. Summary We have presented a computational  framework for quantitative analysis of scripts. The framework requires minimal user interaction, and is based on the principles of handwriting analysis and handwriting production. We also present a prototype implementation of the proposed framework.  We believe this framework and its implementation would facilitate more quantitative study on scripts.  ",
       "article_title":"Framework for Quantitative Analysis of Scripts",
       "authors":[
          {
             "given":"Vinodh",
             "family":"Rajan",
             "affiliation":[
                {
                   "original_name":"University of St Andrews",
                   "normalized_name":"University of St Andrews",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02wn5qz54",
                      "GRID":"grid.11914.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "classical studies",
          "historical studies",
          "linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction/Motivation Also in the age of electronic publishing print publications often remain the points of reference. While many humanities' projects finally build on XML and in particular TEI 1 and embrace electronic publications, they still want or need to target print publications as one or even the main form of sharing the results of their scholarship with the community. Paper remains the principal scholarly format accepted in many circles and in spite of all activities on long-term digital archiving 23, it remains a central medium to disseminate and conserve patrimonial content over the decades and centuries.However, how can you combine an XML-based workflow with the need to publish high-quality, multilingual print output respecting the often arcane typesetting requirements of scholarly texts in humanities publishing and notably in the realm of critical editions? While there are commercial 45 and free 6 78 products out there that can help for some parts of the job, they are too expensive or too difficult to use in most humanities projects. This was the starting point of the DFG-funded XML-Print, an Open Source project that tackles the typesetting requirements for multilingual critical editions while offering a user-friendly frontend.XML-Print has already been presented to the DH community in two well-received short papers 910 on specific aspects of the project's progress and technical challenges. In this long paper we present the project’s overall results. Typesetting Features XML-Print consists of two components:  an interactive graphical user interface (GUI) based on Eclipse, to define the rules for typesetting the XML texts in direct interaction with the source XML text a command-line typesetting engine, written from bottom up in F#, to actually transform the XML text into pdf for print   Normally scholars will interact with the GUI to map their XML structures on layout rules. The typesetting engine is then transparent to them. However, more automated workflows can integrate the engine directly. In section 5 we present examples for both scenarios. Beyond most standard typesetting functionalities of XSL-FO 1112 XML-Print supports in particular the following three requirements specific for publishing in humanities scholarship: Columns A page always consists of different rectangular regions to add header/footer, marginals and the main text. However, this main text is often not limited to a one-column layout, but rather flows in multiple columns. As XSL-FO lacks in this requirement, XSL-FO+ adds a special interface to set-up arbitrary complex column-layouts, even mixed on one page. Each column has its own width and writing direction (left-to-right vs. right-to-left) allowing even “exotic” layouts to be applied within XML-Print. Cross references When using cross-references we must use placeholders, not only in the main text, but also for the header and footer of a page, where page number and sectioning information are commonly used, and for apparatus’ entries, where typographic information like referenced line numbers can change during the editorial process.   Fig. 1: Two user-defined reference systems for an edition  XML-Print incorporates a concept of “reference fields” to define structural and typographic elements to be counted. This way the user can even combine these two types, e.g. for having a global line count and a local one being reset at a specific XML structure. In addition the corresponding “title” of an reference field can as well be made available. Apparatuses Based on reference fields users can define “referencing schemas” to be used in apparatuses. Predefined typographic elements like a global page, column and line numbering can be combined with user-defined fields, arbitrary fixed strings and special characters, e.g. a non-breakable space.As the concrete output of the schema might depend on previous apparatus’ entries, exceptions with regard to repeated items, e.g. same chapter, can be formulated as well. General Architecture and Technologies Standards Modern functional programming languages reusing established frameworks and libraries allow to build a high-quality, multilingual typesetting engine generating archiving-ready PDF/A-1 13  much faster than even a decade ago. With a comparatively small development team we have been able to meet the project’s major objectives within the funding period.  XML-Print builds on Open Standards, especially on XML as the input language, XSL-FO to express formatting and XSLT to transform data from XML to XSL-FO. The project has extended XSL-FO to cover features such as apparatus and advanced referencing not currently supported by the specification (XSL-FO+).For the typesetting engine the project uses the .NET functional language F# 14, running on the cross-platform Open Source .NET implementation mono. To handle OpenType 15 fonts and generate pdf we have settled on the Open Source library iText 16 that exists for both .NET and Java. We contributed to the library's support for some of advanced OpenType features such as “real” small caps and aspects of bidirectional scripts. Algorithms A major advantage of functional programming languages is the lack of mutable variables and states. Algorithms are commonly more compact and easier to parallelize without mutable variables to share across multiple threads. The XML-Print backend for example parallelizes the parsing of certain XSL-FO elements and the rendering of chapters respectively page sequences.   Initially the rendering module was mainly based on the iText library. Now we are replacing all iText algorithms by our self-developed algorithms. They are specialized on the requirements of XML-Print and so are more efficient and easier to extend. We also decided to develop an own line-breaking algorithm. Going beyond the algorithm by Knuth and Plass 17, we take advantage of today’s hardware capacities.  The line breaking algorithm creates a tree structure for all possible line combinations of an entire paragraph. The best path in this tree structure is calculated by taking several characteristics into account, e.g. interword spaces, hyphenations, etc. The final implementation will be parallelized and produce a tree structure with “cross-connected” nodes, i.e. nodes that represent identical paragraph sections are reduced and replaced by additional arcs, thus increasing the efficiency by avoiding redundant line combinations. Figure 1 illustrates the process of line breaking. Each node represents a possible line. The numbers at the arcs represent the processing order. Equal numbers on the same level mean a parallelized section. The bolded path represents the final paragraph, consisting of the nodes Line_1^2, Line_2^5, Line_3^2 and Line_4^4.   Fig. 2: Line breaking algorithm  Graphical User Interface XML-Print addresses beginners as well as expert users. For the latter the GUI has to offer enough details while the former should not be overwhelmed by too many information at first. To achieve this goal XML-Print categorizes functionality and provides a basic and an expert layer.   Fig. 3: GUI for XML-Print  We face, however, one inherent problem. To guarantee a high-quality output the typesetting incorporates a rich repertoire of typesetting logic and features which the user expects to appear somewhere in the graphical user interface. It is not always possible to shield users from these inherent complexities, while mapping all possible options onto GUI elements. Apart from defining formats and declaring \"mappings\" between XML elements and corresponding formats, the GUI offers several possibilities to modify aspects of typesetting, from preprocessing the XML source to altering the PDF output format, from configuring the page size to influencing hyphenation. Use Case Examples Edition “Kurt Schwitters: Wie Kritik zu Kunst wird” During the starting phase of the XML-Print project, staff members of the editorial project “Kurt Schwitters: Wie Kritik zu Kunst wird” 18 already used recent version of the software to proofread their XML transcriptions. At a later stage, formats and mappings for critical and commentary apparatus were added. Dictionaries: The “Trierer Wörterbuchnetz” The “Deutsche Wörterbuch von Jacob und Wilhelm Grimm” is a digitized version of the leading German Dictionary with more than 300.000 entries stored as XML inside a database. The pdf is a byproduct, creating pdf files on the fly is the only effective approach. The XML-Print typesetting engine was wrapped via a simple webservice interface, allowing remote access. Decoupling typesetting engine and GUI improves on flexibility and scalability, as it adds the options of cluster-processing and batch processing to the standard interactive processing.  Outlook To ensure the long-term viability of the project beyond the end of funding in May 2014 XML-Print integrates with TextGrid 19 to complement the current stand-alone mode. In parallel we build up a community on SourceForge http://sourceforge.net/projects/xml-print/, reaching even now more than 50 monthly downloads on average.Further development of XML-Print is also intimately linked to the needs of its customers, especially the critical editions using it, evolving in response to specific requirements.XML-Print is there to play a key role in creating, sharing and preserving our digital and non-digital textual cultural heritage and humanities digital resources on one of the most durable media yet known to humankind – paper.  ",
       "article_title":"XML-Print.  Typesetting arbitrary XML documents in high quality",
       "authors":[
          {
             "given":"Lukas",
             "family":"Georgieff",
             "affiliation":[
                {
                   "original_name":"UAS Worms",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marc Wilhelm",
             "family":"Küster",
             "affiliation":[
                {
                   "original_name":"UAS Worms",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"Selig",
             "affiliation":[
                {
                   "original_name":"UAS Worms",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Sievers",
             "affiliation":[
                {
                   "original_name":"University of Trier",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In a recent overview of issues in the digital humanities, Manfred Thaller notes the importance of: “(1) access to the information needed to tackle a research question, (2) the analysis of that information by tools reflecting the methodological requirements of the specific discipline and research problem and (3) the publication of the new information gained by the analytical process.” (Thaller 2012:11)1 For most of the world's 7000 languages there are few records available via the internet. Efforts to increase the documentation of these small languages have led to the development of tools and repositories over the past decade. I suggest that Thaller’s desiderata are reflected in the language documentation activities of the creation of archives, metadata systems and the ability to locate, store, retrieve and re-use language records. The network of language archives represented by the Open Language Archives Community (OLAC) has adopted a common metadata system that each archive serves for OLAC’s aggregation, allowing more specific searches than can be provided by google, for example. However, not all digital language archives currently provide metadata to OLAC, rendering their collections invisible to the aggregated search. While their webpages may be accessible to web- searches, they do not allow the targeted search by language that is the focus of OLAC’s aggregator. Other repositories (including many institutional repositories—national libraries and archives, mission archives and so on) have language content that is not noted in the collection’s catalog, and the catalog may not be available for web-harvesting. Finally, there are collections still held by their creators and not in a repository at all. This paper discusses two approaches to making collections of primary language material locatable and accessible. While the methods are generalisable to any discipline, this paper describes an index of records of language material for collections that have no such metadata and for which no other mechanism is foreseeable. The first approach builds a traceable index of a researcher’s discoveries in existing repositories, for example, a state library or archive, using established aggregation services. The second is a survey that aims to locate and digitise smaller collections that are currently outside established institutions, typically still in the care of the researcher.   The language index The language index provides metadata in an Open Archives Initiative-compliant form, allowing records2 to be found in generic language searches3. Not all repositories can provide metadata using ISO-639-3 language codes, so it is useful to provide a mechanism whereby researchers can build this resource as they discover new material. In general, repositories are just unaware of standards rather than being reluctant to share data, hence the need for them to either change their metadata system (which is unlikely) or for an index of the kind described here, that points to their collections. The paper will demonstrate the index as it is currently implemented in the catalog of the Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), and will discuss the issue of persistence of the links provided and future possible alternatives to exposing collections that are otherwise invisible to aggregation.    The survey In an effort to locate what I have called invisible collections I launched a survey4 in 2012 asking respondents to identify language collections that needed to be either (or both) digitised or described using OLAC’s service. I tried to keep the questions as simple and easy to answer as possible. In this paper I will report on the findings of this survey, in particular noting that they point to the need for training; for simple metadata entry tools; for standards-compliant metadata repositories; and for recognition of collections of primary material as a form of scholarly output.  The survey questions were as follows: 1. Do you know of recordings of small or endangered languages that are not yet digitised? These could be in personal collections or in established repositories that do not plan to digitise their collections. If so, please provide as much detail as you can about the number and type of recordings (reel to reel, cassette, DAT etc), the content, and the state of their current storage. Can you provide information about who to contact about these collections? 2. Do you know of collections whose catalogs are not available through federated searches (that is, they are only available if you visit their website and not anywhere else on the web) and for which we could provide a reference to make it easier to find them?  3. Do you know of repositories of manuscripts that have received little attention from linguists but which are likely, in your opinion, to have linguistic records in them? These may include, for example, missionary archives or State administrative archives. 4. Please include your name and contact email so we can follow up with you if necessary (email addresses will not be added to any lists). (Please indicate if you allow us to publish an anonymised version of your response). The survey form was publicised among linguistic networks. It is now nominated as a future activity of the international network of language archives, DELAMAN5 which should ensure wider coverage. As a first step, it has revealed an interesting variety of collections, each with characteristics that are significant for the effort of making such collections available. At a time when funding for digitisation is difficult to obtain, it is important to recognise that unique cultural heritage recordings such as these are at risk of being lost. A summary of some responses and an observation about the broader significance of each is given below. (1) 22 tapes of a Sudanese language held in Washington DC by a retired linguist – how to get them digitised and where to store them then? 22 tapes are sort of manageable. There are also a large number of notes that need to be scanned. For a retired researcher it may not be easy to access the equipment needed to do this work.  (2) Several hundred cassettes in a Solomon Islands language, particularly valuable as they are recorded by a speaker, so capturing lots of natural speech. Digitising such a collection is a serious undertaking needing significant funds. (3) The tapes are in Stockholm, stored in a box but the recorder is based in Chicago and is still an active academic. A basic problem of access of the collector to their own material. (4) Colorado, USA, a dozen reel-to-reel and two dozen cassette tapes with a senior linguist concerned to make the collection safe and not being sure what to do. (5) Tapes were deposited with a national Cultural Centre in a small Pacific country that may or may not have the resources to look after them. It does not publish its catalog (if it actually has one) so it is not clear if these tapes need to be digitised or not, or what conditions may be placed on access to them. (6) A recent MA in Linguistics at one of the PARADISEC consortium universities, tapes stored in boxes. Paper transcripts may have been thrown out. Shows lack of communication even within our own departments. (7) A collection of [language] tapes stored in a Harvard University repository which may not prioritise digitising it (but could if funding were made available).  (8) Researchers who have small collections, less than twenty tapes, and digitise them themselves by connecting a tape player to a digital recorder. Problem of methods used in digitisation, may damage the tape and not result in the best digital file. One reason that these collections are not digitised is clearly the lack of importance placed by academia on the re-use of primary research materials. If it were an acknowledged research output to create archived and accessible collections of primary data, counting towards promotion and tenure, then it is more likely that cases like those listed above would no longer occur. It is clear that much remains to be done to extend the reach of digital language archives, assisting in locating legacy collections, describing and digitising them, connecting with source communities/individuals, creating a means for online annotation (crowdsourcing) and of valuing the collections (both monetarily or academically). I conclude by discussing an online service for providing small metadata snippets pointing to these otherwise invisible collections. This paper presents these efforts based around the digital archive Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC).  ",
       "article_title":"What remains to be done – Exposing invisible collections in the other 6500 languages and why it is a DH enterprise.",
       "authors":[
          {
             "given":"Nick",
             "family":"Thieberger",
             "affiliation":[
                {
                   "original_name":"University of Melbourne",
                   "normalized_name":"University of Melbourne",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/01ej9dk98",
                      "GRID":"grid.1008.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  HuNI (Humanities Networked Infrastructure) is a major new digital service for humanities researchers. Developed in Australia, with funding from the NeCTAR (National eResearch Collaborative Tools and Resources) programme, it aggregates data from 28 different cultural datasets from a variety of disciplines, makes them available for external re-use through an API and as Linked Open Data, and provides a set of tools for researchers to work with the data. HuNI is a virtual laboratory application which will be of great value to anyone interested in understanding Australia’s rich cultural heritage. It is also the single largest aggregation of networked humanities data in Australia – a new national data service which is of cultural significance in its own right, and accessible to all. HuNI is planned to go into production in the second quarter of 2014. We reported on the initial design stage of this project at a previous Digital Humanities conference.1 This new paper will tell the full story of HuNI and will analyse, describe and evaluate its first full public release. It will include a demonstration of the full functionality of the service, and will report on its uptake by researchers and the wider community. We will also discuss the lessons learned from this large-scale project over its two-year lifespan, and the measures taken to ensure the sustainability of the HuNI service beyond the life of the project.  The paper will focus, in particular, on the ways in which HuNI is changing the nature of humanities research in the areas of data sharing, collaboration, community involvement, and the creation of socially linked data. Socially-derived linking of data is one of the key features of HuNI. Researchers are able to make assertions about relationships between entities represented in the aggregated data.  If, for example, they search the data aggregate and identify two entities in their result set which are related in some way, they can add a link between the two records and define the nature of the relationship. The linking statement may be drawn from an existing vocabulary of relationships, or may use free text entered by the user. The virtual laboratory also allows a researcher to assert that two entities are not related, in recognition that this kind of statement is also a key characteristic of humanities research.  To help visualize these social links, each entity has its own network graph, showing up to six degrees of separation, resulting in a growing network of dynamic connections, or the “networked effect”. These “social linking” assertions are visible in the HuNI data aggregate. They may also appear in virtual collections assembled and published by individual users of the HuNi virtual laboratory. These virtual collections can be published for other users to see and re-use. Some links between entities have also been imported from the source datasets as part of the harvesting and ingest process. HuNI users can annotate these links with their own assertions as well. Crucially, the provenance of all these “social linking” statements is also captured, enabling subsequent researchers to see who made each assertion. HuNI is an aggregate with a sense of its own history, in which researchers can trace how records have changed over time. Humanities research not only involves making connections between entities; it also involves assessing any changes in cultural flows and network relationships through time. So each HuNI record is time-stamped, meaning that researchers will always see the current view of a record, with its related records and assertions, but will also have the option to view how the record has changed since it was first harvested. The provenance information for each record, together with any curated assertions, is captured, so that researchers can see when the records were harvested and by whom. A link to the originating data record at source is also provided in the user interface. “Social linking” is a crucial feature of the HuNI virtual laboratory. Instead of relying on a pre-coordinated mapping to a detailed ontology, we are relying on researchers and community users to establish most of the connections within the heterogeneous data aggregate. This enables HuNI to capture the different disciplinary perspectives of users, rather than trying to fit all the data into a single normative framework. It also acknowledges the productive differences that both define and link specific domains through a form of generative knowledge transfer. The opportunity to link data socially encourages HuNI users to share their knowledge and research findings in the form of specific assertions, and to discuss these statements with each other. In the paper, we will report on the ways in which this feature is being used by researchers and community users, and the extent to which it is enabling a new approach to data sharing in the humanities. We acknowledge the contributions of Dr Marco La Rosa (Solution Architect) and Dr Anne Cregan (Semantic Lead and Business Analyst) in developing the HuNI infrastructure.    ",
       "article_title":"Socially-Derived Linking and Data Sharing within a Virtual Laboratory for the Humanities",
       "authors":[
          {
             "given":"Toby",
             "family":"Burrows",
             "affiliation":[
                {
                   "original_name":"University of Western Australia",
                   "normalized_name":"University of Western Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/047272k79",
                      "GRID":"grid.1012.2"
                   }
                }
             ]
          },
          {
             "given":"Deb",
             "family":"Verhoeven",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Alex",
             "family":"Hawker",
             "affiliation":[
                {
                   "original_name":"VeRSI",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "humanities infrastructure"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction In this paper we investigate the possibility to adapt existing lexical resources and Natural Language Processing (NLP) methodologies related to Sentiment Analysis (SA) to the historical domain.  Sentiment analysis aims at the computational treatment of opinion, sentiment and subjectivity in texts .1 Current research in SA mainly focuses on the identification of sentiment and opinions in areas such as social media 2,  news 3  4, political speeches 5 , customer and movie reviews 6 78. To our knowledge, SA in the context of the humanities has been rarely explored 9 10 11. Many SA tools often take advantage of polarity lexicons, i.e. a lexicon of positive and negative words and n-grams. In a polarity lexicon, each word is associated with its prior polarity, i.e. the polarity of the word out of the context. A SA system uses these lexicons to evaluate the polarity of a whole text, a sentence or a topic within a text. The availability of a sentiment lexicon is thus a crucial step toward the creation and training of any SA application. Unfortunately, the majority of existing SA lexicons are for English (e.g. Harvard General Inquirer 12) while no lexicon for Italian has been developed yet. The polarity of a word can however be different according to its context of use. A word can be negated and change its polarity (‘ice-cream is good’ vs ‘ice-cream is not good’) or have different usages (‘they fought a terrific battle\"’ vs ‘I loved the film, it was terrific!’). To account for these differences, a system must be able to handle the contextual polarity of a word, i.e. the different  polarity of a word according to its syntactic, semantic or pragmatic context 13 1415 16. Apart from manual annotation or automatic mapping from English, crowdsourcing methodologies can offer a viable solution to collect a polarity lexicon17 and to annotate a large dataset 18. The need to explore the application of SA to historical texts has emerged thanks to the collaboration between the authors and the Italian-German Historical Institute (ISIG) in Trento. This collaboration is aimed at developing tools that can help historians access and understand textual data through the adoption of NLP methods. In particular, SA has been identified as notably relevant to quantify the general sentiment of single documents, to track the attitude towards a specific topic or entity over time and across a large collection of texts, and to allow specific search based on sentiment. This is crucial, for instance, to research on the history of ideology, evolution of political thought, etc. The dataset used for our research is the complete corpus of writings of Alcide De Gasperi, one of the founders of the Italian Republic, made of about 3,000 documents and 3,000,000 words. Using this corpus as a case study, two experiments have been carried out and are described in this paper. The aim of these experiments is the evaluation of i) how existing lexical resources for SA perform in the historical domain and ii) the feasibility of a sentiment annotation task for historical texts either with expert annotators and crowdsourcing contributors.   2. Prior Polarity Experiment The first experiment on De Gasperi's corpus has been carried out using two existing polarity lexicons, namely SentiWordNet 19 and WordNet-Affect 20, to calculate the prior polarity of lemmas and measure the general sentiment of each document within the corpus. The goal was to test how resources built on contemporary languages can deal with historical texts. SentiWordNet and WordNet-Affect have the great advantage of being extensions of a well-known resource called WordNet 21. This allowed us to map the word senses (called synsets) with a positive, negative or neutral polarity in SentiWordNet and WordNet-Affect to the corresponding Italian synsets in MultiWordNet 22, in which Italian synsets are aligned with WordNet ones. At the same time, lemmas were automatically extracted from De Gasperi's corpus using the TextPro tool 23: the total of 70,178 lemmas was reduced to 36,304 after excluding lemmas that can’t have a polarity score (e.g. numbers, articles). Each lemma was then automatically associated with the most frequent synset in MultiWordNet and its polarity score: this association covered 14,874 lemmas (40.97%) among which 9,650 were neutral. This process, followed by a manual check of the scores, produced a list of 5,224 lemmas with a polarity score: 449 with an absolute positive score (e.g. 'giubilo'/rejoicing), 576 with an absolute negative score (e.g. 'affranto'/broken-hearted) and the others with an intermediate score. The general sentiment of each document in the corpus was finally calculated summing up the polarity scores of the lemmas appearing both in the documents and in our list, and visualized through a gauge diagram in the A.L.C.I.D.E. web platform [dh.fbk.eu/projects/alcide-analysis-language-and-content-digital-environment] (Figure 1).   Fig. 1: Document visualization: sentiment and key-concepts  Historians’ evaluation of the results was positive for most of the documents but a more specific need emerged: historians are indeed more interested in the polarity of a specific topic and in its evolution over time, rather than in the global polarity of a document that can give us indications only about the general sentiment conveyed in it. However, as historical texts are complex documents in which several topics can be identified, the global polarity of the document is not enough to identify the polarity of a single topic. To address these requirements, we performed the experiment presented in Section 3 aimed at annotating SA at the level of topic in De Gasperi's corpus, following a contextual polarity approach.   3. Crowdsourcing Experiment for Contextual Polarity In order to perform a pilot experiment, we identified two topics which were relevant in De Gasperi's writings, namely \"sindacato'\"(trade union) and \"sindacalismo\" (trade-unionism). A corpus of 525 sentences was automatically extracted from De Gasperi's corpus, where each sentence contained at least one of the two lemmas “sindacato” and “sindacalismo”. The previous and the following sentence were added as a context as well. Each sentence was annotated by two expert annotators, while a third annotation was collected through the crowdsourcing platform CrowdFlower [www.crowdflower.com] after performing a majority voting over 5 judgements. The two expert annotators were asked to create gold standard data (GS), i.e. a set of sentences on which both annotators gave the same judgements, from a subset of the corpus (60 sentences, 11% of the whole corpus). Both expert annotators and crowdsourcing contributors were then asked to annotate the contextual polarity of the two topics in the sentences with one of the four possible judegments (i.e. positive, negative, neutral, unknown) given a simple set of instructions and some annotation example.  In addition to the manual annotation, we also calculated the prior polarity for each sentence using the same algorithm applied to the documents and described in Section 2. 1. The feasibility of this task was then evaluated calculating:   2. the accuracy of the crowdsourced annotation over GS (figure 2), i.e. how well non-expert contributors performed the task;   3. the accuracy of the prior polarity for each sentence over GS (figure 2), i.e. how well the Italian prior polarity lexicon performed on the sentences in comparison to the contextual polarity approach;   the inter-annotator agreement (IAA) with the Fleiss's kappa measure (figure 3)24 , i.e. the level of consensus between the annotators.    Fig. 2: Accuracy scores      Fig. 3: IAA results  The overall accuracy score for the crowd-collected judgements in Figure 2 (68.3%) indicates the general complexity of the task. In particular negative and positive polarities are more difficult to identify (55.5% and 46.6%) than neutral polarity (80%). Considering the prior polarity scores in Figure 2, we observe that accuracy is always lower than in the crowd annotation setting, except for the positive judgements (86%).  The IAA agreement in Figure 3 confirms that SA is a a challenging task 25 . The highest kappa-score is found if we consider the two expert annotators (0.46), but it is not much higher than the situation in which we consider 3 annotators (0.39) or one of the two experts and the crowd judgement (0.35). In general, the type of documents have a great influence on the agreement scores: past works report that news stories can achieve an agreement of 0.81 26, whereas social media (tweets) can be as low as 0.321 27.   4. Conclusions and Future Works This paper presented two experiments related to SA and involving a corpus of historical texts. In the first one we created a new Italian lexical resource for sentiment analysis starting from two existing lexicons for English and we applied it to measure the polarity of an entire document using a prior polarity approach. In the second experiment, the use of crowdsourced annotation to obtain contextual polarity of a specific topic was exploited. The long term goal of our ongoing research is to create a system to support historical studies, which is able to analyze the sentiment in historical texts and to discover the opinion about a topic and its change over time. In the near future we plan to perform domain adaptation of existing annotation schemes developed for SA 28 29 and of the Italian lexical resource we created. Particular attention will be devoted to a step-by-step evaluation by historians in order to tailor the results of our work to their needs.  ",
       "article_title":"Sentiment Analysis for the Humanities: the Case of Historical Texts",
       "authors":[
          {
             "given":"Alessandro",
             "family":"Marchetti",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          },
          {
             "given":"Rachele",
             "family":"Sprugnoli",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler / University of Trento",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sara",
             "family":"Tonelli",
             "affiliation":[
                {
                   "original_name":"Fondazione Bruno Kessler",
                   "normalized_name":"Fondazione Bruno Kessler",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/01j33xk10",
                      "GRID":"grid.11469.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Workspace for Collaborative Editing is a project funded by the AHRC (UK) and DFG (Germany) between September 2010 and December 2013. It has the goal of creating an online workspace to support the production of the Editio Critica Maiorof the Greek New Testament by teams based in Birmingham, Münster and Wuppertal and collaborators dispersed all over the world.1 The edition has been in progress since the 1990s, but the obsolescence of key tools and encodings have led to this ambitious project to connect all the different stages of the editorial process through online interfaces and shared databases.   The production of a critical edition involves the identification and selection of manuscripts to be included, the acquisition of images, the creation of full-text electronic transcriptions (which are themselves published as separate electronic editions, linked to the electronic apparatus, enabling further research in related fields), the automatic comparison of these transcriptions to generate a critical apparatus of all variant readings, the editing of this apparatus by scholarly editors to filter out ‘noise’ and prepare the data for analysis using genealogical tools, the addition of evidence from early translations and biblical quotations and the publication of the material in electronic and printed form. The aim of the Workspace project has been to adopt existing standards and open-source solutions in order to create a lightweight architecture capable of being easily renewed and updated, so that both the data and software created may be reused by other projects. The result is an open-source browser-based environment written in Python and Javascript. The core software consists of a MongoDB database and the asynchronous web application framework MAGPY. Data is stored in JSON and made available via a RESTful interface. On top of this is a layer of applications which call the relevant data objects for the individual editing processes. The goal of transparency at every level of the editing process means that a record is kept of each object at each stage of the process, and any modifications introduced are treated as additional records rather than replacing existing data.2 The Greek New Testament provides a very specific use-case, with a large amount of data already created and highly developed editorial principles. In addition, ongoing work by existing editorial teams offers the opportunity for immediate testing in real-life situations. Developing in these circumstances can be a challenge, with the evolution of guidelines, changes of editorial practice and 'creeping featurism'. The system needed to make existing legacy data compatible with the much more detailed XML encoding developed by the project and cater for as many known and potential scenarios as practicable. The dispersed team of editors was often called upon to codify their procedures and reach a common mind on problems presented by live data, including agreeing changes in policy. As a result, the creation of the Workspace has proceeded hand in hand with the development of different stages of the edition as a whole.   The two principal areas in which the Workspace meets a pressing need are the development of a transcription editor, which produces and allows the editing of valid XML in a WYSIWYG environment, and a collation editor which enables the scholarly creation of a critical apparatus. Both of these are browser-based, in order to enable dispersed collaborators to work with differing operating systems and contribute directly to the central data store.  The Transcription Editor has been created by team members at the Trier Center for Digital Humanities and released as open-source at the end of the project.3 Its basis is the platform independent TinyMCE package.4 A set of options for mark-up was then developed through a series of menus and shortcuts (cf. Figure 1). The aim is to allow student and volunteer transcribers not familiar with XML to work in an environment which matches as closely as possible the format of the transcriptions already published in the system. The mark-up in the browser uses HTML encoding. An export function converts this into XML matching the specifications developed by the project.5 Likewise, an import function is required in order to support the editing of existing transcriptions. Some of the problems include the encoding and display of paratextual information, normally located in the margins of a manuscript. The dialogue box for entering this information has to have the same functionality as the main transcription interface for recording unclear or supplied text, corrections and so on. The concept has therefore been developed of the “editor-within-an-editor” which makes this possible. A problem with the import of existing transcriptions is the sequence within which elements were nested within the XML. As a result, it has been necessary to establish a system of tag sequences supported by the editor. The standalone nature of the Transcription Editor and its use of an agreed set of TEI encoding means that it can be installed as a plug-in to different environments, including the New Testament Virtual Manuscript Room (NTVMR 2.0)6 as well as the Workspace for the production of the critical edition.   Fig. 1: The Transcription Editor in the NTVMR environment. Based on the selection different menu options for breaks, corrections, deficiency, ornamentation, abbreviations, marginals, notes and punctuation are offered. Mouseovers and different colours help the users to identify different structures.  The Collation Editor provides an interface to the CollateX engine developed by the INTEREDITION project, the successor to the COLLATE program by Peter Robinson.7 This software performs one of the most mechanical and error-prone tasks in an edition, namely the comparison of all witnesses in each variation unit to build up a critical apparatus. Each file is aligned using an algorithm taking into account not just spelling variations, additions, omissions and substitutions, but also transpositions within each block of text. However, the output still requires considerable input from scholars in order to clean up the raw data for publication as a critical apparatus. The first stage is regularisation, the elimination of insignificant variations such as spelling errors. The variant readings are set out underneath a base text, with the witnesses attesting each reading visible in a mouseover box (see Figure 2). An interface built using the redips drag-and-drop library allows editors to drag-and-drop the readings for regularisation onto the correct form.8 For each regularisation, a dialogue box requires users to state the scope of the regularisation and also its nature. Once this is completed, the regularisation is marked in grey and a rule is saved to the database. The ‘recollate’ button sends the data back through CollateX, preferring the regularised token to the original form where present. This means that a different configuration of readings may appear in each column, as the data is cleaned up and a better match is made by the collation algorithm. The second stage involves setting the length of each variant unit, again implemented through a user-friendly drag-and-drop interface for combining or splitting neighbouring columns. One of the dangers with this interface is changing the overall sequence of words in a manuscript by combining different units and repositioning readings. A checking mechanism has therefore been developed which warns the user as soon as the sequence of any manuscript has been disrupted. On some occasions, the data is best displayed as two units of different lengths. By right-clicking on the relevant reading, it can be sent to a line below as an “overlapping variant”, which can then be combined and manipulated like the other columns. One further complication is that an overlapping variant such as a lengthy transposition of words may also contain a reading which should cited in the main sequence. The system therefore makes it possible to duplicate such readings. The final stage is the ordering of variant readings within each unit and assigning the appropriate reading identifier. From here, the apparatus can be output in a number of forms, such as a positive or negative plain text apparatus, an XML encoded apparatus, or a set of values for incorporation into a database for phylogenetic analysis. The information added in the regularisation dialogue box makes it possible to generate automatically the lists of original forms for orthographic variants and erroneous readings which are printed in an Appendix in the Editio Critica Maior.   Fig. 2: The regularisation interface with the dialogue box displayed.  The presentation will briefly demonstrate the Workspace, especially the two interfaces described above. We will discuss some of the problems encountered during its development, along with their solutions. Although the scope of the original project was specifically to support an edition of the Greek New Testament, a pilot project to customise the environment for an edition of Avestan texts will be outlined: from here, we hope that it will be possible to develop the Workspace for use with other textual traditions.      ",
       "article_title":"The Workspace for Collaborative Editing",
       "authors":[
          {
             "given":"Hugh",
             "family":"Houghton",
             "affiliation":[
                {
                   "original_name":"Institute for Textual Scholarship and Electronic Editing (ITSEE), University of Birmingham",
                   "normalized_name":"University of Birmingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03angcq70",
                      "GRID":"grid.6572.6"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Sievers",
             "affiliation":[
                {
                   "original_name":"Trier Center for Digital Humanities (Kompetenzzentrum für elektronische Erschließungs- und Publikationsverfahren in den Geisteswissenschaften, Universität Trier) ",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          },
          {
             "given":"Catherine",
             "family":"Smith",
             "affiliation":[
                {
                   "original_name":"Institute for Textual Scholarship and Electronic Editing (ITSEE), University of Birmingham",
                   "normalized_name":"University of Birmingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03angcq70",
                      "GRID":"grid.6572.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Editing is one of the most important activities we have to perform as digital humanists: with the entire literary and historical production facing remediation, the need for a theoretical as well as practical understanding of what it is at stake and what does it mean to create a digital (scholarly) edition is of crucial importance. Many contributions in the past have dealt with the issue of what a text is, what a document is, how they relate to each other and which are the implications of their ontological status with respect to the work they manifest1 2 3456 78 910 1112 to name a few). The topic has been tackled from different points of view: sociological, cultural, psycholinguistic, philosophical, historical and computational. Why, then, is it necessary to return to the same topic once again? Because while some if not most of the previous contributions have touched upon it, none has tried to account for the whole concept of digital editing, and if a model is “a representation of something for purposes of study, or a design for realizing something new” (p. 21) 13, a new purpose of study will require a new model. In order to perform an activity with the help of a computer, in order to digitize a workflow, such an activity (i.e. editing) needs to be modelled, as there is “the fundamental dependence of any computing system on an explicit, delimited conception of the world or ‘model’ of it” (p. 210)14.  The only existing model of text that has been developed explicitly for computational purposes is represented by the so-called OHCO model15, the limitations of which are widely known 16 17 1819 20. However, as it reflects upon the fundamental way of functioning of the most important technology used so far for the production of digital editions, i.e. XML, in spite of its inadequacy, it still represents a fundamental approach for editorial endeavours: if one edits using XML and TEI, then one will have to adopt some sort of model which strongly relates to the OHCO model. But besides all the issues already pointed out by earlier critics, there is a series of facts and entities which are outside the scope of the OHCO model but which are nevertheless of fundamental importance in scholarly editing; namely, what is an edition? What is a work? What is the relationship between an edition and the text it edits, and the work the text represents? What is the function of the reader and of the editor in establishing the text and the edition?  The edition of a text, any text, embodies a model of the work which the text represents. In the same way that a map can be considered a model of the earth built for a specific purpose, an edition of a text can be considered as a model of the text itself, because it represents a selection of the infinite features of a text according to particular point of view, scholarly or not. Selecting also mean simplifying: a model is necessarily a simplification of a real life object, which makes it more apt to analysis and manipulation, In a contribution from 2009 Michael Sperberg-McQueen declared that there are three things to consider when we edit a text (p. 31)21:  There is an infinite set of facts related to the work being edited. Any edition records a selection from the observable and the recoverable portions of this infinite set of facts. Each edition provides some specific presentation of its selection.  In saying this he declared an edition to be a model of a work, where the act of selecting features from the uninterrupted continuity of the reality is the defining act of modelling, the purpose of which in turn is to provide a discrete selection of facts to be interpreted. The present research stems from this consideration and proposes a new, comprehensive conceptual model of the editorial domain, which could be called the Digital Editing Model, or DEM. The conceptual model deals with the following entities:  Documents, i.e. physical objects that contain some sort of information; therefore a book is a document, as is a leaf with some writing on, a stone, and so on. More generally, a document is a physical object that has some text on it, or more formally, a Verbal Text Bearing Object, or VTBO. This definition willingly and knowingly omits non-verbal documents, as the object of the present research is to analyze and model written and verbal texts with the purpose of editing them.[1] User-function: any type of human interaction with the documents. The entity represents set of functions, more than human beings, such as, for instance, reading, editing, collecting, preserving, transcribing, analyzing etc.  As seen before, documents present an infinite set of facts. A user-function selects a subset of these facts and, according to an organizing principle, groups them into dimensions. As the dimensions that are potentially observable in a document are defined by the user-function’s purpose, consequently it is impossible to draw a stable and complete list of such dimensions; however, for the purpose of exemplification, such a list could include linguistic, semantic, literary, genetic, iconographic, codicological, and palaeographical dimensions. The interaction of user-functions with documents generates  Document models: the meaning(s) that user-functions give to the subset of dimensions they derive from a document and that they consider interesting. If the subset of dimensions considered by the user includes the verbal content of the document, such a document’s model is defined as a text. Works: an editorial statement of the fact that a number of documents aim to contain more or less the same verbal content. The sum of all possible texts derived from such documents, in a one-to-one or many-to-one relation, constitute the work.  The model does not necessarily need an author-function, but it may, if the user-editor postulates it. If present, the author-function performs two main sub-functions, one in posse and one in esse, where the latter represents the activities of producing some of the facts present in the documents, especially, but not exclusively, the ones concerning the verbal content of the documents. The function in posse concerns instead the authorial intention, namely what the author-function wanted to produce but did not, or, if it did, the evidence for this is lost. It is in posse because it is unachieved (or perhaps is achieved but with no way of knowing this). The DEM model sketched out here, will be supplemented by a second one on text transmission (how text migrates from one support to the next), where emerging theories of transcription will also fit very well 22 23242526.  However, these latter theories can also be thought as an instance of the user-function, encompassing the building of texts from the infinite set of facts available from the documents, and therefore are integrated into the DEM. In its full version, DEM will also deal with concepts such as versions and derivative works. In conclusion the proposed DEM contributes to the elaboration of a set of models required by the renewed work and workflow of digital editing, in dialog with previous scholarly elaborations, providing a holistic and agnostic base for the understanding of the digital editorial endeavour.   [1] A similar but more generalized definition is given by Huitfeldt and Sperberg-McQueen, who prefer to speak of ‘marks’ on a document, rather than of ‘verbal text’: ‘By a document we understand an individual object containing marks. A mark is a perceptible feature of a document (normally something visible, e.g. a line in ink)’ 27.  ",
       "article_title":"Modelling digital editing: of texts, documents and works",
       "authors":[
          {
             "given":"Elena",
             "family":"Pierazzo",
             "affiliation":[
                {
                   "original_name":"King's College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Geoffroy",
             "family":"Noël",
             "affiliation":[
                {
                   "original_name":"King's College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction 1.1. Overview    Digital Humanities (DH) is becoming an increasingly global community of practice 1 with international initiatives such as centerNet 2, Global Outlook::Digital Humanities 3, the Alliance of Digital Humanities Organizations (nd), the Digging into Data Challenge (2013) and many others.  With advances in telecommunications and information technology, these types of collaborations are no longer bound by geography.  However, as documented elsewhere, challenges stemming from geographical distance must be managed to ensure that teams are work together successfully.  One of the primary challenges is finding ways to facilitate communication and coordination across distance and time (Olson & Olson, 2000; Siemens, 2010b; Siemens & Burr, 2013). Skype and other internet-enabled tools provide some potential to accomplish this; however, little knowledge exists on the best way to use these tools within a geographically dispersed collaboration. This paper will contribute to this discussion with an examination of the experiences of a DH lab using an open communication channel through Skype to connect team members located in different sites.  As Olson and Olson (2000) outline, despite advances in information and telecommunications technology, distance between members still impacts on a team’s functioning at the task and personal levels.  As they suggest and confirmed by others (Kennedy, Vozdolska, & McComb, 2010; Kraut, Galegher, & Egido, 1987), some amount of social presence or visible awareness of others is needed to allow for the sharing of advice, feedback and support among each other and the teams as a whole. Team members build on this social presence to task coordination.  For co-located teams, face-to-face formal meetings and informal interactions in common rooms and around the proverbial coffee pot is the primary way to create and reinforce social presence. (Belanger & Allport, 2008; Kennedy et al., 2010; Warkentin & Beranek, 1999).  For dispersed teams, the challenge to the creating this awareness exists because fewer channels and fewer personal cues in the communication exist.  Without these, individuals are less likely to pay attention to each other (Short as cited in Warkentin & Beranek, 1999).  Less personal forms of communication such as email tends to produce an “out of sight, out of mind” effect.  (Hiltz as cited in Warkentin & Beranek, 1999).  So, the question is how might computer-mediated communication like an open audio and video communication channel, through something like Skype, overcome distance and sustain social presence so that the team can achieve its tasks? 1.2. Methodology    This project grew from the desire of a DH lab that wanted to connect members who were split between two locations.  As one initiative, the lab wanted to experiment with an open audio and video communication channel between the two offices.  Using Skype, cameras and monitors would be installed and operating during work hours, allowing for communication between the two sites.     In order to understand the effectiveness of this communication channel in facilitating task and personal relationships, lab members were interviewed on two occasions.  The interview questions focused on the participants’ understanding and experiences of this type of technology to facilitate collaboration between geographically disbursed sites (Marshall & Rossman, 1999; McCracken, 1988).  The first round occurred before the cameras and monitors were installed (pre interviews).  The second happened after the communication channel had been in place for several weeks (post interviews). 1.3 Findings    At the time of writing this proposal, final data analysis is being completed, but clear patterns are emerging and, after final analysis, these will form the basis of my presentation.   As found in the pre interviews, these participants had little to no experience with this type of communication channel and were not sure what to expect.  At the same time, they could see its potential for increased collaboration because they would be able to see their colleagues who located elsewhere and could more instantly communicate, much like if the person was seated next to them.  While some participants expressed some concerns about privacy, as a whole, they were more curious about the ways in which the communication channel would actually work.  Some of the questions focused on the location of the monitors and cameras, hours of operation, camera sight lines, and the ways in which communication would be facilitated.  Overall, they were intrigued and excited to see how this open communication would work and support the lab’s work.   After the cameras and monitors had been in place for several weeks, the lab members relayed that they became quickly accustomed to the presence of the cameras and monitors.  In fact, when asked, they had difficulty recalling the day that when these were installed and the communication channel was opened.  As several noted, the cameras and monitors were perceived to be “just there”.   In terms of challenges, noise was an issue from the outset.  The microphones amplified all sounds that caused much distraction.  As a result, the microphones were turned off after several days.  Participants also noted that uneven coverage of team members due to camera placement.  Some were very visible while others sat outside the sightlines.   Despite this, the participants were very positive about the experience and the benefits produced.  Many noted that this open communication channel reinforced the feeling of collaboration by providing an “extension of the existing space” and a “hole in the wall” to the other members.  As a result, the lab felt less divided by distance.  The interviewees noted several benefits.  First, because they could constantly see each other, members were reminded of the presence of those in the other office.  In some cases, those in the other office might come join conversations that they had seen on the monitor.  Second, an opportunity was created to model professional and academic work habits, such as reading, writing, thinking and discussing, which reinforced these for the others.  And while the lab is a professional space, the participants also had a sense of play with each other.  They would wave at each other and make visual jokes.  1.4 Conclusion While this paper reports on the experiences on a small DH lab, it suggests potential for other geographically dispersed collaborations.  Open audio and video communication channels can create the sense of social presence by reminding members that they are part of larger efforts, even working at a distance.  These tools also complements the other well-established online ones, such as basecamp, github, email, and others, as well as face-to-face meetings for project coordination and decision-making (Ruecker, Radzikowska, & Sinclair, 2008; Siemens, 2010a; Siemens & Burr, 2013; Siemens, Cunningham, Duff, & Warwick, 2011).   ",
       "article_title":"The potential of open computer-mediated communication channels to facilitate collaboration in geographically distributed collaborations",
       "authors":[
          {
             "given":"Lynne",
             "family":"Siemens",
             "affiliation":[
                {
                   "original_name":"University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction One of the marks of a \"mature science\"1 is the development of \"standards\" of analytic practice, based on shared \"key theories, instruments, values, and metaphysical assumptions\"2 that scholars work with.  This concept has been incorporated into US Law as a mark of reliable evidence.3   One of the weaknesses of authorship attribution is the absence of such standards of practice. For example, fifteen years ago Rudman estimated4 that more than 1000 different feature sets had been proposed for this task. This of course creates controversy about the appropriateness of methods and even the possibility of cherry-picking feature sets to a specific task to get a desired answer. The solution demanded by Daubert is the use of a specific analytic technique, with standards controlling its operation and an established error rate.   We offer a relatively simple protocol for such analysis in the hopes that it may provide a base for the eventual development of such a standard.  We illustrate the application of our protocol with three case studies from the recent literature. 2. Methodological Overview These cases involve the early writings of Edgar Allan Poe5, the anonymized case of an asylum seeker (cited as \"Bilbo Baggins\")6, and, more famously, the pseudonymous author of The Cuckoo's Calling7, revealed to be J.K. Rowling of Harry Potter fame.  All three cases share several characteristics which may therefore be regarded as \"typical\"; unlike many literary studies of authorship, these are \"verification\" problems in which there is really only one candidate author of interest, and therefore available samples. No information is readily available to exclude anyone plausible from authorship (unlike, for example, the Federalist Papers, where scholars readily accepted that authorship was confined to the small group of Hamilton, Madison, and Jay).   In each case, the candidate author was an established writer and a baseline of writings by that candidate could be easily obtained and validated.  Previous work has shown89 that authorship attribution can be performed with relatively high accuracy using a variety of methods.  Typical performance on small, closed-class problems is around 80% accuracy.1011 Using a ensemble methods such as \"mixture-of-experts\" can boost performance above the baseline of any individual method.  Our proposed protocol, then, is to solve this verification problem by running a number of independent studies as elimination tests against an ad-hoc distractor set, to see whether any features set can definitively eliminate the author of interest.   Using multiple independent tests provides strong protection both against false acceptance and false rejection errors. 3. Protocol Details 3.1 Ad-hoc distractor set Most stylometric methods formally choose the most likely author from among a fixed and finite set of candidates based on similarity of writing.  While this set is normally chosen based on authors who may actually have had the opportunity to write the disputed document, this is not a formal requirement.   From the point of view of stylistic similarity, any two authors or documents can be usefully compared.   Koppel et al.12 noted that randomly chosen authors from the same general field and genre would work as well given repeated measures: \"The known text of a snippet’s actual author is likely to be the text most similar to the snippet even as we vary the feature set that we use to represent the texts. Another author’s text might happen to be the most similar for one or a few speciﬁc feature sets, but it is highly unlikely to be consistently so over many different feature sets.\"  Juola [6] applied the same technique, using newspaper articles scraped from the Web as a baseline against which to compare Baggins' writing.   3.2 Multiple independent elimination tests The key insight here is that, quoting Koppel, any given wrong author \"is highly unlikely to be consistently [similar] over many different feature sets.\"  This insight can be formalized mathematically as follows:  If a technique is X% accurate, the chance of it being wrong is (1-X).  (I.e an 80% chance of being right yields 20% chance of being wrong).   If two independent techniques are X% accurate, the chance of them both being wrong is (1-X)^2. If K different techniques are each X% accurate, the chance of them all being wrong is (1-X)^K, which becomes arbitrarily small as K increases.  Thus using multiple independent analyses will reduce the chance of false acceptance error to as small a value as desired. Similarly, false rejection errors can be handled by using a relaxed acceptance criterion, and essentially treating the top few candidates as \"successful.\"  This again can be demonstrated rigorously.   If our technique is 80% accurate among a set of distractor authors, there is a 20% chance that the most similar author will not be the correct one.  But in this case (and with suitable independence assumptions), there will also be an 80% chance that the most similar author among all other authors studied will be the correct one (by assumption), and hence only a 4% chance that the correct author will not be among the top two in the original set.  (This chance drops to 0.8% for the top 3.)  Thus we can say with high probability that any author not among the top few most similar has been eliminated as a plausible candidate author.   3.3 The proposed protocol formalized We can thus formalize the proposed authorship analysis protocol as follows:  Gather an ad-hoc collection of three to five authors other than the author of interest.   Run a number of independent tests of different feature sets to determine which author is most similar to the questioned document on that specific feature.  (JGAAP13  14  provides a huge number of feature sets from which to choose, and is designed to be extensible to enable people to add additional sets of interest).   Any author not in the two or three most likely candidate authors is eliminated as a potential author.   If, after enough experiments have been run, the only author not eliminated is the author of interest, his or her authorship of the questioned documents is deemed confirmed. 3.4 An example (Rowling) The Galbraith/Rowling case is instructive.  In this case, I was provided a distractor set of three authors, all contemporary female British crime writers, so their writings would be comparable to \"Galbraith's.\"  Tests were run on four separate feature sets: word lengths, character 4-grams, word pairs, and the 100 most frequent words.   Of the four authors, only Rowling was not eliminated by at least one feature set.   We can determine the likelihood of error as follows:  Assuming that Rowling was not the author, the probability of her appearing in the top half (top 2 of 4) in any list of candidate authors would be 50%; thus she would have one chance in 16 (approximately 6%) chance of not being eliminated through this procedure. 4. Discussion and Conclusions Perhaps obviously, there are some caveats to the proposed protocol.  The most key is, of course, the implicit assumption of independence.   Is it is reasonable to believe that the distribution of word lengths is independent of their use of common function words?   More importantly, can this belief be validated empirically and justified theoretically?   Similarly, there are some numbers in the protocol that may need tightening -- is three to five distractor authors enough?  Are five better than three?  Can these numbers be justified?   We will discuss this further but invite commentary on this point. It should also be clear that this paper does not ipso facto establish a mandatory standard for authorship studies.   We invite discussion and even competing proposals, in addition to further studies to establish not only what other protocols might be more accurate, but also which ones are easier to apply, or even more likely to generate useful information (beyond simple authorship). One key aspect of this proposal is that it relies primarily on rank-order statistics and does not take into account the degree of variation; a more sophisticated protocol might use parametric statistics for greater power, at the possible cost of increased complexity.  From a practical standpoint, however, this protocol may represent a substantial maturation of the field.  Not only have we used it ourselves, but it has also been used by third parties [5].  The results have been validated by reference to independent ground truths (Rowling acknowledged authorship on July 12, 2013.15)   The results have even been accepted in courts of law.  We are thus confident that the proposed protocol will provide a relatively clear-cut way to reduce controversy regarding stylometric authorship attribution and increase its uptake and credibility.    ",
       "article_title":"The Rowling Case: A Proposed Standard Analytic Protocol for Authorship Questions",
       "authors":[
          {
             "given":"Patrick",
             "family":"Juola",
             "affiliation":[
                {
                   "original_name":"Duquesne University, Juola & Associates",
                   "normalized_name":"Duquesne University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02336z538",
                      "GRID":"grid.255272.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Working on scholarly edition in general can be seen as a chain of working steps. Each step leads to an intermediate result, which other steps may build upon. At the end of this chain stands the published edition as the final result. Traditionally these steps, thus the editorial process, are described in form of methodological guidelines and editorial principles. Digital scholarly editions not only add new aspects like the separation of data and presentation or the chance to include additional materials 1234, they might also change the way we document editorial processes. This  seems particularly important for new forms of digital scholarly editions like open source editions5, work in progress editions 6, editions 2.0 7 or social editions8. Such editions are evolving over time, they are possibly incomplete  (work in progress), they might have been created in a collaborative way  by many editors, or they provide not only canonical texts but also  accompanying materials or results from intermediate stages like  graphemic transcriptions, pre-normalized texts etc. as raw data for  further enrichment and research. As a consequence it is questionable if traditional ways of documenting how an edition was made are still appropriate. If we want to deal with the fact that digital scholarly editions are no longer static resources but collections of evolving results, we must put a stronger focus on process aspects. For evolving scholarly editions we should consider to understand formal process documentation as an essential part of such editions. Problems First we have to define what we mean with editorial processes. Creating a scholarly edition includes a large number of activities like searching, collecting and evaluating sources, other textual witnesses, literature and images, the review of existing and preliminary editorial work, making transcriptions, collations, normalized texts, abstracts, indexes, glossaries. It also implies tasks like dicscrimen veri ac falsi, stylometric, paleographic, prosopographic, and sphragistic research. Some of these tasks have to be done only once, but the majority of editorial activities is either recursive and/or has to be repeated over and over again, e. g. for each charter of a collection. Documenting such processes seems easy, but if we take a closer look, we find that the documenting of single processes requires a clear and formalized concept of processing steps based on the definition that a process is a directed activity which leads from one state to another. I assume that editorial activities can be separated from each other and can therefore be handled as addressable and formally describable units of action. This is a basic prerequisite for process documentation, because we need distinct entities to refer to. Each process has to be defined with its starting point, a defined ending point and a set of activities which lie in between. This means that we do not only have to describe activities, but we also have to create relations between a specific activity and the data sets which represent the starting and ending points of the activity. Another issue is how we should describe editorial processes. This includes three basically inseparable sub-questions: (1) what should be documented to what extent (2) how should this be put into practice and (3) which data formats are feasible. The extent of documentation depends on the objectives of an editorial project, but a minimal set should include timestamps, actors (persons or algorithms), a formalized description and possibly an additional free-text description of the action and, as mentioned before, pointers to the initial and resulting data.  The second question deals with the problem that documenting single editorial activities seems to put a massive overhead to editorial work. But for the greater part this data can be generated automatically if we use appropriate working environments.  The question of feasible data formats is twofold: we have to distinguish between process data used within a project and data which becomes a formal part of the edition. The former needs little discussion, as it depends on technical environments. But the latter is still an open problem which requires further discussion and research although some solutions already exist. It seems that graph based models are particularly suitable for the problem 910, but also XML-based standards for process descriptions like PREMIS must be investigated as possibly adaptable models. The third and most fundamental question is what can we do with this data or more pragmatic: why should we start collecting process data at all? Formalizing editorial processes and the availability of such data has multiple benefits. From a project management point of view this data can be used to get an overview of the project status at any point in time. One can use this data to find out for example how long a task takes on average, which might become important for future project planning. Process data also can be used to trigger actions like sending a message to co-workers if a certain state has been reached or even to move a document automatically along a predefined workflow based chain of tasks. In quality control process data can be utilized to identify systematic errors.  From a user point of view quality control will become more important with new forms of scholarly editions. If an edition has many contributors, or if an edition has no final result, it is partly up to the user to decide if a particular element of an edition is trustworthy. Peter Robinson has claimed that every act of editing should be attributed to the person who did it, because this allows attribution and trust 11. Process metadata describing when and by whom an element was added or changed can therefore be an important indicator for the reliability.   Open ended and collaborative editions consist of an ever increasing number of representation forms and versions, which constitute multidimensional knowledge spaces. Process data can be used to construct references and therefore contexts between the single components of such a multidimensional scholarly edition 12. Hence, process data can give important directions when users want to leave the paths predetermined by editors. Approaches The problem of process planning and process documentation is of course not unique to scholarly editions. One example is software development, where many contributors can work on the same project, continuously creating new and refactoring existing code. This requires the use of software versioning and control system like Apache Subversion or Git for the creation and organization of process metadata which document different versions of files and development branches. In science there is a long tradition of hand written log books with the purpose to keep laboratory work reproducible and transparent. Nowadays these log books are replaced by electronic systems which generate and log great parts of process metadata automatically 13.  Most advanced in this domain is business process management (BPM) as a discipline of business informatics 14. It is focusing on process optimization which is not the main problem with scholarly editions, but BPM has developed a domain specific terminology, conceptual frameworks, software tools and standards like WfMC, WSFL, XPDL, BPEL or BPMN, which turn out to be very useful for understanding our topic. Concerning the problems they try to solve document related technologies15, especially (enterprise) content management 16 17 seem to be more applicable to digital scholarly editions than BPM. Content management describes the life cycle of a document through a chain of states in an abstract way. Content management systems implement these formalized life cycles by providing states, workflows, users and roles. They are able to keep track of stages and versions, are able to automatically route documents through defined paths and to collect metadata for every stage and action. Enterprise content management must not be confused with web content management, as web content management systems are mostly simplistic, monolithic and proprietary applications, which are not suitable in terms of long time availability. Conclusion  New open forms of digital editions require a stronger focus on editorial processes. This affects the planning of work, needs support for process specific workflows and requires process specific metadata which has to be seen as an essential part of an edition. Other fields, particularly (enterprise) content management has requirements similar to those of scholarly editions. Upcoming software tools for digital editions should investigate CMS with regard to process aspects and adopt useful features. Special focus has to be put on the development of an appropriate, interchangeable format for edition-related process metadata.  ",
       "article_title":"Process Data for Digital Scholarly Editions",
       "authors":[
          {
             "given":"Gunter",
             "family":"Vasold",
             "affiliation":[
                {
                   "original_name":"University of Graz",
                   "normalized_name":"University of Graz",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/01faaaf77",
                      "GRID":"grid.5110.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "scholarly edition"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The project of the BFM collection of digital edition was born as a result of over 20 years of developing a large corpus of medieval French texts for research purposes. This corpus called Base de français médiéval (BFM, http://txm.bfm-corpus.org) currently includes over 130 texts, totalling approximately 4.7 million of words. The essential part of the corpus is composed of digitized paper scholarly editions selected for their philological quality. Digitizing paper editions was the only way to build a substantial corpus in a relatively short time and with limited funding. However there are serious copyright issues related to printed editions, as publishers tend to require exclusive rights on the books they print, and they are often reluctant to authorize digitization and re-use of the data in text corpora. Before 2000, publishing contracts rarely included explicit clause on digital distribution, so it can be argued that scholarly editors (or their heirs) still hold copyright for this medium, but more recent contracts include long lists of digital products and distribution modes. Even though possibilities for open-licensed publishing on the web exist, scholars have to give up all their rights if they want to publish their works in a prestigious collection recognized by the academic community. The BFM team aims at providing scholars with a possibility to publish medieval French texts under an open license (like CC BY-SA) in a collection with editorial quality guaranteed by the expertise of the reading committee including leading specialists in medieval French language and literature, in text editing techniques and in digital philology. 2. Editing principles In addition to the open licensing, the BFM collection marks itself out by innovative editing principles. These principles have been elaborated in the project of the Queste del saint Graal digital edition1 and include a multi-layer transcription of primary sources (at least normalized and diplomatic), particular attention to punctuation and word segmentation, careful and clearly marked correction of scribal errors, linguistic annotation (part-of-speech and direct speech tagging). The \"bedierist\" method of the \"best witness\" is generally applied, but transcriptions of additional aligned witnesses are encouraged.  Whenever possible, an edition should include a digital facsimile of the primary source which allows verifying the quality of the transcription. The presence of a modern French translation is optional but may be very useful to increase the range of potential readers and uses. All these principles are described in detail in the Introduction to the Queste del saint Graal edition and most of them were presented and discussed at the International Congress of Romance Linguistics and Philology (CILPR) in 20132.  3. Workflow and publication platform At the first stages of the editing process text editors like Microsoft Word or Libre Office Writer may be used for the convenience of scholarly editors. A small number of special characters and character or paragraph styles are defined to facilitate future processing. For instance, a hash symbol before a letter indicates that a small letter from the primary source should be capitalized in the normalized transcription. Once the primary editing complete, the text is converted to XML-TEI, which is the pivot format for all markup and editorial products in the BFM collection.The BFM corpus preparation chain automatic tools for tokenization, morphosyntactic annotation and direct speech markup. Whenever possible, the morphosyntactic annotation is verified by experts, as the automatic tagging of Old French produces inevitably a certain number of errors due to the high level of orthographic and morphological variation. The BFM web portal built on the TXM platform3 will be used to publish the collection on the web. The advantage of this portal is that it combines the possibility to render the edition in a convenient form for reading (including parallel browsing of multiple transcription layers, digital facsimile and translation) with powerful tools for qualitative and quantitative text analysis (including frequency lists, KWIC concordances, specificity, factorial analysis, etc.). The editions of the BFM collection will be included in the BFM main corpus, and the BFM registered users will benefit from additional services, such as creating a subcorpus or recording queries. However, the possibility to read the text and to download XML-TEI source files or a PDF printable version will be provided without registration requirement. 4. Current state of the project The edition of the Queste del saint Graal is currently complete from the philological point of view (although additional manuscript transcriptions will probably be produced in the future). All the major components of this edition (multiple layer transcriptions, modern French translation, manuscript images, introduction, proper name index and glossary) are available on the BFM portal (http://txm.bfm-corpus.org) through a special \"GRAAL\" corpus. However, a more convenient interface for browsing the edition and for direct access to its components is still under development. More editions are being prepared at a more or less advanced stage. These include the Psautier d'Arundel (edited by C. Pignatelli and A. Lavrentiev)4, the Vie de saint Alexis (edited by C. Marchello-Nizia and T. Rainsford) and the first French texts, Serments de Strasbourg and Séquence de sainte Eulalie (edited by C. Guillot, A. Lavrentiev, C. Marchello-Nizia and T. Rainsford). All these editions should be published in 2014.   ",
       "article_title":"BFM Collection - Open-Source Digital Editions of Medieval French Texts",
       "authors":[
          {
             "given":"Alexei",
             "family":"Lavrentiev",
             "affiliation":[
                {
                   "original_name":"ICAR - CNRS",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "corpora",
          "French studies",
          "licensing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The paper will present the digital genetic edition of the notebooks of the Austrian conceptual artist Hartmut Skerbisch (1945-2009). The edition is currently emerging as part of a PhD project which explores the use and advantages of applying semantic technologies to art historical source materials. Therefore, the central research question is how a digital genetic and semantically enriched edition can support the appreciation of an artist’s concepts and associations during the process of a work’s creation. Such insights should facilitate the interpretation and comprehension of his work.  Hartmut Skerbisch dealt conceptually with an extended notion of sculpture and space and applied these ideas to his works. He explored the relation between these entities, especially how space in general is generated or changed through the interaction of sculpture with audience and setting (e.g. how can electricity generate or compress space) and how electronic media change our perception of space.12 In his notes, he mentioned different authors, philosophers and novelists like James Joyce, Franz Kafka, Kathy Acker or Rudolf Steiner, as well as musicians and bands, ranging from classical to rock to blues music. He frequently cited passages from their works, commented on their ideas and theories, and consequently reflected them in his artistic expressions.    The 35 notebooks originated between 1969 and 2008 and have a scope of around 2100 pages. They include handwritten texts, calculations, formulas and sketches. The major editorial challenge regarding this source material is dealing with painted over or torn out passages, deletions and corrections. Skerbisch’s notes do not follow a linear construction, they are often fragmentary and lacking formal notation. Additionally, the interconnection between text, graphics, annotations and references to other text passages itself generates an artistic composition in the two-dimensional medium of the notebook. At first glance, texts and sketches seem to be randomly arranged on the surface: Closer inspection, however, reveals an underlying principle of the different building blocks, whose form, shape, size and placement give the entire text additional meaning. This feature of the notebooks calls for special attention to a document-oriented view in addition to a work-oriented view.3 Consequently, the paper will discuss three key issues:   The need for editions of (hand)written sources in art historical research is evident. The recent editions of the sketchbooks by Max Beckmann4 as well as the class notes on form and design theory by Paul Klee5 show that traditional philological edition methods are not entirely sufficient for art historical needs. Issues of text-image relationships - especially for these types of sources, where images are of equal value as texts or text passages themselves become graphic elements – have not been adequately resolved.  Writing a notebook is a process manifested in time and space, therefore the evolution of a text over time is an important aspect. A genetic edition focuses on the document aspect and attempts to trace the origination process with a view on corrections, deletions and additions of the text to envision the author’s original intentions.  Especially modern manuscripts, with fragmentary and cursory notes, often exist as unfinished drafts and put different demands on the editors.678 The difficulties with digitally encoding such handwritten sources of visual artists with TEI are addressed: The paper will report on the benefits and drawbacks of the already implemented elements and attributes for genetic editions in the TEI guidelines and demonstrate their application on the material at hand. Finally, the paper focuses on how semantic technologies could help to uncover the cultural and intellectual background of Hartmut Skerbisch’s creative work, through relating textual references to concepts. Recent research projects like the Theodor Fontane’s notebooks 9 pursue a similar approach and emphasize the importance of linking concepts for semantic analysis of the source material.  Thus, in addition to the content-related and formal structure of the text, recurring concepts like people, places, works of literature, music and film are annotated. For uniquely referencing such entities, controlled vocabularies as well as authority files such as VIAF, GND and GeoNames are used. Beyond that, the aforementioned concepts are further classified. The identified concepts are extracted from the TEI document, mapped to a RDF model and stored in a triple store to facilitate reasoning.10 In this manner, implicit relationships between these concepts are established which are not explicitly present in the texts. For example, a quotation from a novel is linked to that novel’s author and other works of the same author. Thus, it is possible to compare and juxtapose the concepts used which allows for a variety of different analyses of the content.   The advantages of these methods are a) bringing the fragmentary entries of the notebook into a content-related sequence, b) visualizing unexpected combinations and associative chains of concepts to show the many-faceted influences on the artist’s work, c) establishing a relation between the notebook entries and Skerbisch’s realized works of art and finally d) tracing the creative process to promote the understanding of his work. The insights gained in the process will provide a basis for further research questions and analysis in art history and digital edition.  ",
       "article_title":"Hartmut Skerbisch – Envisioning association processes of a conceptual artist",
       "authors":[
          {
             "given":"Martina",
             "family":"Semlak",
             "affiliation":[
                {
                   "original_name":"Centre for Information Modelling, University of Graz",
                   "normalized_name":"University of Graz",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/01faaaf77",
                      "GRID":"grid.5110.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "art history",
          "encoding - theory and practice",
          "scholarly editing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Ontology is a semantic scheme which comprises the main classes (concepts) of the given domain of knowledge, their properties, inter-relationships and instances 1. The basic ontological relationship types are IS-A (hyponymy) and instance-of (a concrete type-of) which are the basis of the conceptual taxonomy, and other thesauri-like (e.g. \"part-of\", \"related-to\") or content-driven relationships (e.g. \"located-at\", \"produced-by\") are allowed as well. One of the standards for formal encoding of ontologies is RDF (Resource Description Framework) 2. RDF-based knowledge representation consists of triples (relations) of the form: (concept1 – relationship type – concept2). For example, in the domain of Jewish cultural heritage the following relations could be included in the ontology: (Passover IS-A Jewish Holiday), (Holiday part-of Cultural Heritage), (Orthodox Jew preserves Jewish Tradition). Every triple corresponds to a certain statement or fact on the domain.  Nowadays, ontologies are widely used as a formal domain vocabulary for content-specific agreements in a variety of knowledge-sharing activities, such as, information organization, retrieval and tagging 3. However, in the current state of the Semantic Web for many domains there are multiple diverse ontologies rather than one standard vocabulary. This is due to the fact that they are typically constructed by different experts who often possess contrary viewpoints especially in cases of controversial domains. These domains include cultural heritage, economy, politics, history, religion, art and even medicine. Apparently, when using several ontologies in a common application, mismatches can create incoherent results for the users. Therefore, reaching maximal agreement between these ontologies is necessary to standardize and unify the domain vocabulary. Hence, building unified consensual ontologies has become a big research challenge.  The objective of this work is to explore ways to maximize the inter-ontology agreement for controversial domains. Particularly, we experimented with the case of the Jewish life style domain which comprises cultural, religious and political aspects. We also aim to explore whether it is possible to identify consensual ontological relations from diverse ontologies and construct a maximal subset of consensual vocabulary for the controversial domain.   Research Methodology   To overcome the semantic heterogeneity problem in ontologies a variety of ontology matching algorithms were presented in the past decade 4 , 5. These systems usually focus on mapping individual concepts (and/or their taxonomic structures) of one ontology to similar concepts in the other one. However, the level of inter-ontology agreement assessed by the automated approaches is limited by the following major factors:   The algorithm's ability to recognize semantically similar concepts, which are frequently conveyed by different terms; Matching of isolated concepts which does not reveal the maximal potential for semantic similarity of ontologies unless all the direct and indirect relations (triples) binding these concepts can be consistently matched as well.  The low overlap between the explicit terminologies of diverse ontologies (for both, concepts and relations) due to the viewpointdiversity of their composers (especially for controversial domains).     Some partial solutions were lately proposed in the literature. Thus, in order to reduce the impact of the first factor, a recent study by 6  proposed to employ \"wisdom of crowds\" for detecting similar concepts in two different ontologies. To resolve the second limitation, similarity between ontologies should be computed for relations rather than for individual concepts, as implemented by 7. He counted exactly and partially matching triples from a pair of given ontologies. This methodology is adopted in the current research which further focuses on matching relations rather than individual concepts. However, the third and the most crucial factor, a relatively small amount of common relations, still remains unresolved.  The main question is how to reveal and assess the potential maximal agreement between ontologies despite the low overlap between them. The essence of this problem is the underlying assumption that relations, which are present in one ontology, but are missing from the other ontology, are automatically considered as unmatched and increase the ontology disagreement level. Nevertheless, it can be observed that if the ontology composer did not choose to add a relation to his personal ontology, it is unclear whether he agrees or disagrees with the truth of this relation. To this end, we introduce a new collaborative approach where independent ontology composers can explicitly express their opinions on the others' relations. Thus, after completing the construction of their own ontologies the participants are exposed to the relations of the others and are asked to decide for each of them whether it is true or false. Then, the \"real\" exhaustive inter-ontology agreement can be calculated based on these votes rather than by counting the common relations in the original ontologies.  We distinguish between two levels of ontology agreement:  The local agreement between a pair of ontologies that can be calculated as follows:    Fig. 1: The local agreement measure     The global agreement definition between all the ontologies for the domain:Fig. 2: The global agreement measure  These relations constitute the consensual part of the ontologies. The other relations will be considered as controversial. The threshold descriminating the consensual and controversial relations can be computed by applying machine classification on the composer's votes as features for each relation as described in the next section.     Experimental Setting and Results  Based on the above collaborative scheme for relation evaluation we conducted an experiment with 21 ontology composers (students of the Semantic web course in Information Science Department). At the first step, the group has chosen a set of 130 concepts which are the most representative of the domain of Jewish life style. Then, every participant was required to construct up to 100 RDF-style triples (relations) with the above concepts and a set of 15 predefined relationships (such as, IS-A, instance-of, part-of, disjoint-with, entails, located-at, antonym-of) independently from the other members of the group. The relations were inserted into the web-based system implemented for this purpose. Further, each one of 1175 distinct ontological relations, created by all the participants at the first step, was consecutively displayed by the system and independently evaluated as true or false by every participant of the group. The analysis of the results shows that the initial local agreement between the diverse ontologies (the CR component of the measure) was very low (0-22%) reflecting the controversity of the domain. This is despite the fact that all the participants used the similar set of concepts and relationships for relation construction. The exhaustive local inter-ontology agreement assessed after applying the collaborative evaluation procedure appears to be much higher (39-90%), as demonstrated in Fig. 3. Thus, our collaborative scheme substantially enhances the local agreement level between ontologies.   Fig. 3: The local agreement rates computed by the amount of overlapping relations in the original ontologies as a baseline (the blue skew) vs. by the votes after applying the collaborative evaluation procedure (the red skew). Axis X represents pairs of different ontologies in our corpus.   To create a golden standard evaluation required for the global agreement calculation, two experts were asked to annotate all the statements as correct (ground truth) or controversial (depending on one's personal beliefs). First, they worked independently and then reached full consensus through a discussion. Overall, 885 (out of 1175) relations were judged as correct facts (TRgoldstandard), and 290 as controversial viewpoints. For example, the correct relations (like, Passover – is-a – Jewish holiday; Reform Jew – disjoint-with – Orthodox Jew; Western Wall – located-in – Jerusalem) are expected to obtain the large majority of agreement (\"true\") votes in the collaborative procedure, while the controversial relations (such as, Ultra-Orthodox Jew – resists – Scientific progress; God – created – Universe; Bible – written by – Man) are supposed to gain intermediate scores for both agreement (\"true\") and disagreement (\"false\") voting categories.  The |TRoverlap| component needed for the global agreement calculation was rather low 29% (232 out of 885) even with the threshold of 2,  as only 256 relations appeared in at least two ontologies. 919 (almost 80%) of the relations appeared in one out of 21 ontologies), while only one relation was present in 9 out of 21 ontologies.  Then, in order to estimate the global agreement after applying the collaborative evaluation procedure, we utilized the WEKA environment 8 to choose the optimal machine classification algorithm. Eventually, the best 10-cross validation results were achieved by the Multilayer Perceptron algorithm which yielded 90% average accuracy. As a result |TRcollaborativevoting| of 876 was obtained. Interestingly, all 232 relations of TRoverlap were included in TRcollaborativevoting. Overall, 99% of the correct relations according to the golden standard were classified as correct by the automatic classifier. The classifier used as features the true and false voting scores. Most of the errors in classification were controversial relations probably reflecting some common viewpoint among the members of the group. So in the future research we intend to conduct crowdsourcing microtask-based experiment (like in 9) with a much larger number of participants. In summary, our collaborative method significantly increases the baseline agreement that can be achieved manually or automatically from the explicitly overlapping/matching relations. This methodology further leads to construction of a reliable large consensual ontology for controversial domains which seem impossible to achieve from the small overlap of the original ontologies.   ",
       "article_title":"Collaboratively maximizing inter-ontology agreement for controversial domains: A case study of Jewish cultural heritage",
       "authors":[
          {
             "given":"Maayan",
             "family":"Zhitomirsky-Geffet",
             "affiliation":[
                {
                   "original_name":"Information Science Dept., Bar-Ilan University",
                   "normalized_name":"Bar-Ilan University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/03kgsv495",
                      "GRID":"grid.22098.31"
                   }
                }
             ]
          },
          {
             "given":"Eden Shalom",
             "family":"Erez",
             "affiliation":[
                {
                   "original_name":"Computer Science Dept., Bar-Ilan University",
                   "normalized_name":"Bar-Ilan University",
                   "country":"Israel",
                   "identifiers":{
                      "ror":"https://ror.org/03kgsv495",
                      "GRID":"grid.22098.31"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "ontology matching",
          "ontology construction"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction 1.1. Overview In this paper I use topic models to identify the most common discourses used by Russian newspapesr to discuss Russia’s relations with the North Caucasus region. I then use grammatical features of news texts to identify three styles or genres of news report. Pro-Kremlin texts about Chechnya are overwhelmingly factual  news reports; oppositional texts tend to be analytical or emotional. Using the example of Beslan I show that this divergence coincided increased political control over Russian news media   1.2. Methodology 'Some themes are endemic to particular nations or to particular author genders; other themes cross both geographic boundaries and gender lines'  -- Matthew Jockers, Macroanalysis (2013, p. 146) Jockers, writing about how topic modelling can be used in literary studies, argues the digital humanities can tackle big questions, such as how linguistic style and topic selection varies depending on nationality and gender. My research achieves this goal by comparing how thematic and grammatical preferences vary depending on the political orientation of Russian media sources. Most broadly I explore the hallmarks of Russian oppositional and pro-Kremlin discourse, and more specifically I trace how symbols and historical events are mobilised as part of political debates within a database of 1 million news articles. I use topic models to identify the most common discourses used to express Russia’s relations with the North Caucasus region. I then use grammatical features of news texts to identify three styles or genres of news report (factual, analytical, opinionated), and show that coverage of the Caucasus is divided more strongly along generic lines than by subject matter.    2. Getting Started Texts about Chechnia and/or legal cases involving Chechens in pro-Kremlin papers are overwhelmingly news reports, while in oppositional texts they tend to be more analytical. The distribution of texts in different genres and across papers is strongly specific to texts about the Caucasus region. The use of a factual register with a high density of nouns may in some cases function as a cover or device for ignoring and overlooking sensitive issues. While the pro-Kremlin newspapers spoke of the Beslan incident within a narrative of international terrorism, the opposition framed Beslan within a narrative of official incompetence and cruelty. A feature of opposition texts generally, and exhibited in particular by the case of Beslan, is that some news stories almost become symbolic as they extend their shelf-life beyond that of the mainstream media. In the case of Beslan, oppositional longevity was made especially clear due to editorial changes in Izvestiia, where, under stricter political control, coverage of Beslan did not only virtually cease, it also changed dramatically in tone.   In September 2004, Raf Shakirov, editor of Izvestiia was dismissed from his position. Shakirov, though never an oppositional figure, did at times allow critical material to be printed, and according to a number of sources Izvestiia’s graphic coverage of suffering caused as the security services lifted the siege at Beslan was the final drop.According to a source cited by Polit.ru, Shakirov was dismissed due to publishing necrophilic content (pictures of dead and wounded children) (Polit.ru, 2004). Whether or not the pictures were in poor taste, they certainly portrayed human tragedy with great immediacy and pathos, an aspect that was notably absent from pro-Kremlin coverage of Beslan, and one which, following Shakirov’s dismissal, was notably absent from Pro-Kremlin papers. In June 2005, Gazprom-media bought Izvestiia from Prof Media, and in November 2005 Vladimir Mamontov, was made Editor in Chief. In April 2006 control over Izvestiia was made more overt when Il’ia Kiselev, formerly head of United Russia’s Press Centre was made deputy editor of Izvestiia. Kiselev and Mamontov brought rapid change to Izvestiia: now the newspaper printed a larger number of shorter, more opinionated texts. Coverage of the after effects of Beslan, though, moved in the opposite direction: it became dry and factual.    noun density over time, colour coded according to genre: news articles (2), analyses (1a) and opinion pieces (1b). Izvestiia is the middle plot on the top row.   All newspapers printed fewer articles as the story lost news value, but all publications except Izvestiia maintained a relatively constant tone. I present illustrations showing how Izvestiia’s tone changed, and how this pattern was repeated for related subject matter. Compared to each newspaper’s average publication patterns, the two oppositional newspapers printed a relatively larger amount of opinion pieces and analyses and virtually no ‘plain’ news reports.    The two more oppositional papers, Novaia Gazeta and Gazeta.ru are on the right-hand side.   I use Euclidean distance of genre ratios to calculate which subjects are covered most similarly to the pattern above. These tended to be about one of three subjects: Chechnya, legal cases, or opposition politicians. Articles about Chechens generally (but not Chechnya, the Republic) follow this pattern, as do texts about Shamil Basaev, the Chechen militant who claimed responsibility for taking the hostages, Aleksandr Dzasokhov, the president of Northern Ossetia who resigned as a consequence of the incident, and [Aslambek] Aslakhanov and Ruslan Aushev – State Duma deputy from Chechnya 2000-2003  and president of Ingushetia from 1993 to 2003 respectively – who were sent to negotiate with the terrorists.  Additionally Nur-Pashi Kulaev (the single surviving hostage taker at Beslan and subsequently sentenced to life imprisonment in December 2006), Ruslan Gelaiev (Chechen separatist leader, killed leading a raid in Dagestan in 2004), Alkhan Khala (A Chechen village and insurgent stronghold, ‘cleansed’ by the Russian army during 2001) all feature amongst the top twenty most similar subjects.  Very few pro-Kremlin texts explicitly set out to portray Chechens as terrorists and nie-liudi (inhuman). Instead there is a systematic tendency to cover these incidents with a news genre that by its nature obscures the humans behind the story and in a style where the content is easily forgotten.    ",
       "article_title":"Beslan and the disappearance of opinion",
       "authors":[
          {
             "given":"Rolf",
             "family":"Fredheim",
             "affiliation":[
                {
                   "original_name":"University of Cambridge",
                   "normalized_name":"University of Cambridge",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/013meh722",
                      "GRID":"grid.5335.0"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "media studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction A community of Digital Humanities in Spanish and Portuguese (HD*) has  been consolidating over the past few years. Due to a series of events  gathering numerous colleagues and taking place in diverse latitudes,  2013 has been a turning point. A milestone was the first DíaHD, which brought together about a hundred practitioners and showed that HD scholarship is highly active and eager to build a cohesive community.  The purpose of the event was “to identify and establish or improve  networks and collaborative work among the community of digital humanists in Latin America, the Caribbean, and the Iberian Peninsula, as well as  digital humanists in other regions of the world whose work is done in  Spanish or Portuguese” (translation ours)1. Our project, MapaHD originated that day and has embraced the bilingual profile established  by that event. MapaHD is an exploration of the features and  intersections among those who self-identify as HD practitioners and  their characteristics beyond language affiliation. In our paper, we  provide insights into issues of temporal development of HD, geographic  location, interdisciplinary practices and approaches, and how  progressively a community of digital humanists has been taking shape.  The development of MapaHD has been made public from its beginnings at  mapahd.org where we have gathered visualizations and preliminary  results. Simultaneously, the data collected has been used to build an  interactive and exploratory map using DARIAH-DE Geo-Browser that is also available through our website. 1.1. Overview MapaHD is a direct address to the question launched by Domenico Fiormonte, “Is there a non Anglo-American Digital Humanities, and if so, what are its characteristics?”2. In this project we have gathered and analyzed practitioners’ data that evidences not only the existence of a thriving DH community in Spanish and Portuguese languages, but more importantly what its features are. The diversity of the characteristics we have observed sheds light on the HD community’s institutional and project affiliations, area of research, geographic location, research approaches, and temporal data. 1.2. Methodology In order to tackle these issues, we have carried out three research phases: 1) Data collection gathered entirely online through a survey, answered voluntarily by 85 participants. The survey was available during a four-month period from June 10th to October 10th, 2013. Questions included gathered data on participants’ institutional, project, and disciplinary affiliations, research approaches, location, among others. The survey was distributed through mailing lists and Twitter. Links to the projects’ survey were tweeted using hashtags used by similar events and communities such as #DíaHD, #HDH2013, #DH2013, #ThatCampBaires, #HDBr, #RedHD, #arounddh, #HumanidadesDigitales, and #dhpoco. The aim of this distribution model was to catch the attention of as many participants in as many locations as possible. This approach to data collection sought to allow anybody who identified himself/herself as a digital humanist in the two languages to self-report their characteristics, rather than send out invitations to those we might consider to fall even under a “big tent” definition of digital humanities.3 2) Using the available data, we built a graph database organized according to the semantic network schema in Fig 1. Rather than looking for person to person connections, this analysis sought to shed light on non-obvious and non-personal connections among participants. The links joining researchers and students among them are disciplines, approaches, work spaces, and geographic proximity. The data was subjected to frequency, central tendency, and network analysis. Several results emerged from the data contained in the database and are presented in the next section.   Fig. 1: Database Schema.  3) An interactive map visualization built on DARIAH-DE Geobrowser. This resource is a second contribution of this project to the field as it seeks to serve not only as a visualization of the data collected, but also as a reference tool. Finally, aside from providing a glimpse of the state of the field, MapaHD hopes to strengthen and expand the sense of community and connection among Spanish and Portuguese speaking digital humanists initiated by DíaHD. 2. Results   2.1. Discipline Outlook More than half of the 85 participants reported working in at least two disciplines distributed as shown in Fig 2. Literary studies was the discipline most participants reported. However, out of the 56 participants who reported working on the literary fields, 32 also reported working in other disciplines.   Fig. 2: Discipline distribution of participants.  In Fig 3 we show the most common combinations of literary studies and other disciplines. Aside from the recurrence of literary studies together with other disciplines, the only other recurrent disciplinary combination was History and Visual Art. The rest were mostly unique combinations. This information confirms the fact that, not unlike DH, HD is also “a hybrid domain, crossing disciplinary boundaries and also traditional barriers between theory and practice, technological implementation and scholarly reflection”4.   Fig. 3:  Combinations between Literary Studies, the most common discipline in the database, and other fields.  Our analyses offer qualitative insights as to what different disciplines might be bringing in into the mix. For example, the recurrence of  interdisciplinary work attached to a “root” field of Literary studies  suggests that the field is not necessarily the gravity centre of HD as  has been suggested by Azofra5 but, as observed in Fig 4, a hub where other expertises converge,  shedding light upon each other. In contrast, network analysis has shown  that the second and third best connected disciplines are Information  Sciences and History.   Fig. 4: Network visualization showing the prominence of Literary Studies by measuring degree, and History and Information Science as the next two best connected disciplines.  The relevance of these two fields in the network resides not so much in how many participants reported them, but how variably combined they are. As a matter of fact, the cluster formed around Information Sciences brings together a few other disciplines such as Education and Computer Science. Even though they have key links joining them to the rest of the network, disciplines such as Philosophy, Film and Media Studies, and Linguistics retain a certain level of isolation. From our data it is possible to see both the exposing of disciplines to other fields of knowledge, on the one hand; and on the other, how in opening up, disciplines flood other fields too. While some disciplines, by sheer numbers seem to be exercising a larger influence, connecting fields like History and Information Sciences might be providing a common foundation of porous perspectives through which these dynamics take place. 2.2. Geographic Outlook The geographic location of MapaHD participants (Fig 5) was a foundation for the initial concept of the project. Through the interactive map visualization, we also explored the participants distribution. In total 41 cities located in 11 countries were identified (Fig 6). As a reference, this is close to 50% of the total number of countries represented in ACH membership set at 23, as Bethany Nowviskie reported via Twitter in October 20136.   Fig. 5: Screenshot of MapaHD, built on DARIA-DE Geobrowser, showing the spread of the HD community around the world.  Interestingly, close to half of the locations are found in the UK, USA, Canada, Germany, and Italy where, though common, neither Spanish nor Portuguese are official languages. Although some of the participants’ location can be seen as a ‘diaspora’, our results have showed that this is only a small portion of them (22%) and not the sole distinctive of the analyzed group.   Fig. 6:  Geographic distribution of participants.  The issue of location and problems of centrality and periphery in the context of Digital Humanities have been better expressed by Domenico Fiormonte, who stresses the fact that DH have not “succeeded in either strengthening the field of humanities or putting some balance into the power relationships between humanities and computer science” 7. We believe that, as Isabel Galina proposes, this lack of balance “can also help us think about DH from a different perspective… [and] pushes the limits of our creativity and our capacity to solve problems”8. The clear ties with the Anglo-American branch of DH, and to a lesser extent with the continental European one, seem to imply that as an identifiable community HD is porous and prone to cross-polination in terms of approaches, academic practices, and language. The international spread of HD practitioners might be the cause behind the particular diversity in this community. Nevertheless, the HD community maintains a sense of cohesion that can be traced perhaps to the shared lack of visibility, institutional similarities, and linguistic coincidences – Rafael Alvarado’s “network of family resemblances”9. Furthermore, HD ties with other DH communities, both geographic and linguistic, have set up a communication channel through which approaches and projects may travel back and forth. 3. Conclusions Much has been said about the characteristics of DH on a global scale and, especially, of the Anglo-American branch. MapaHD constitutes the first data-driven approach to the HD community and we provide insights into its disciplinary and geographical particularities. Not discussed in this abstract but included in the paper are the issues of collaboration and where it takes place, as well as an outlook of the connections among the research approaches undertaken, and the historical development of our participants’ trajectories.  Endnotes *We use HD to distinguish the Spanish and Portuguese speaking branch of digital humanities from the Anglo-American one commonly referred to as DH.  ",
       "article_title":"MapaHD: Exploring Spanish and Portuguese Speaking DH Communities",
       "authors":[
          {
             "given":"Élika",
             "family":"Ortega",
             "affiliation":[
                {
                   "original_name":"CulturePlex Lab / University of Western Ontario",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Silvia",
             "family":"Gutiérrez",
             "affiliation":[
                {
                   "original_name":"Würzburg Universität",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "spanish and spanish american studies",
          "relationships",
          "graphs",
          "digital humanities - nature and significance networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction We are approaching a golden age in the study of visual art and photographs. Many museums, libraries, and universities have digitized large portions of their collections and have made the images, and associated metadata, available for study.  This process has been a major boon to art historians, collectors, and other researchers. Instead of calling individual items one at a time in library reading rooms or digging through old, expensive, incomplete, and often out-of-print art catalogs, a researcher can simply type a query into a library or museum database and retrieve large sets of images.  However, while the rate of digitization has lately increased, this has, at times, come at the expense of detailed cataloging.   Even before the era of mass digitization, catalogers struggled to identify certain works of art (sometimes due to a lack of collaboration between institutions).  Today, simple digitization is often faster and cheaper than expert cataloging, and so many works of art and photos appear in repositories with limited metadata using inconsistent schema or vocabularies1.  The result is that while there are more works of art online than ever, it is still difficult for researchers to find the images they seek.  However, advances in automated fuzzy image recognition may allow researchers to discover relevant content even when available metadata is limited.  In this paper, we will examine two test cases--photographs of theatrical performance with unidentified actors, and mis-attributed Japanese woodblock prints--to demonstrate how image search algorithms can be used both to locate content and help researchers understand it better.   Overview of the test cases Woodblock prints In December of 2012, John Resig (Visiting Researcher at Ritsumeikan University and creator of the jQuery JavaScript library) released Ukiyo-e.org: a database of Japanese woodblock print images with metadata harvested by traversing the publicly-accessible digitized collections of prints at the targeted institutions.  The images were copied and saved to a separate server for faster access (a technique which avoids overburdening the institutions by loading the images directly from their websites).  The information on the website is organized broadly by artist and time period on the homepage, but is primarily designed to be used as a search engine allowing users to search both by text and by images.  The database currently contains over 213,000 prints from 24 institutions collected from from late 2011 to late 2012.    One of the most important features of the Ukiyo-e.org2 website is its ability to do real-time analysis on the images it holds for comparison and searching.  There is frequently disagreement among major institutions regarding the attribution, dating, titles, and other information associated with a print.  Because of this incongruous metadata, it becomes virtually impossible to find similar prints among multiple institutions.  The one piece of information that is never under contention, however,  is the image of the print itself.  The image that is presented by most institutions usually includes a full, straight-on photograph of the print (or prints, if it’s a diptych, triptych, or similar).  By ignoring the metadata provided by the institutions and comparing only the actual contents of the images, it is possible to find similar-looking prints at different institutions.   Theater Photographs Along with works of art, many libraries and archives have recently begun to publish large sets of photographs, often depicting unidentified people.  In some cases, the lack of any additional information makes identification of the faces in the photographs all but impossible.  However, images from performing arts collections often feature well-documented events with widely recognizable people appearing alongside lesser known figures.  A rehearsal shot of a musical comedy, for instance, might feature a star in front of a chorus of anonymous extras.  Metadata for such photographs may identify the star, and the title of the piece in which he or she is performing if it is known, but the supporting cast is generally left unidentified.  In early 2012, Doug Reside, Digital Curator for the Performing Arts at New York Public Library, began a series of experiments to attempt to identify these performers.  Over 90,000 theater photographs are now available on the Library’s website with varying degrees of metadata3.  In most cases, the work being presented is identified.  Information about the cast and crew of these productions often can be harvested from other online databases such as Playbill Vault4, the Internet Broadway Database5, and DBPedia6.  Given infinite time, a human investigator could theoretically identify many of the anonymous people in the Library’s photographs by finding all instances of the face in any online photograph, and then using additional datasets to determine the most likely name associated with it.  An otherwise anonymous person, might, for instance, be identified in a newspaper photograph or in a headshot in theater program.  Similarly, it might be possible to identify an otherwise anonymous actress if the shows in which her face appears uniquely match her resume as constructed from published cast lists.   Methodology Both test cases would benefit from computer vision algorithms capable of searching a corpus of images for a set of very similar (but not necessarily identical) images.  Although research in this area began decades ago, implementations capable of comparing thousands to millions of images from various sources simultaneously have only emerged very recently7. The general availability of this technology, however, has been mixed. Tools such as imgSeek8 have made rudimentary image comparison technologies available for use in Open Source projects, but at present commercially-available tools with public APIs (such as TinEye’s MatchEngine9) provide faster image analysis with a greater level of clarity.  Neither tool, however, is exactly suited for facial recognition, which requires the ability to identify a face pictured at different angles, under different lighting, in front of varying backgrounds, and at varying sizes (depending on the distance of the subjects for the camera).   The MatchEngine tool, while a commercial service, is well suited for finding images that are close matches of one another, or even partial matches embedded inside a larger image (as in the case of triptychs).  Like imgSeek, MatchEngine was able to find images by upload and quickly process newly-added images.  Resig tested both MatchEngine and imgSeek during the development of Ukiyo-e.org and found that MatchEngine was much better at finding exact matches, ignoring differences in color, and finding prints (or portions of prints) inside other print images. With an effective image similarity engine it became possible to develop many new tools to aid woodblock print researchers.  Using the tools available on Ukiyo-e.org, researchers can now look for a print not just by a title, description, or artist name (there is generally little agreement on the metadata between institutions) and instead find a print by providing just a photo.  Additionally, scholars who are researching the manipulation and reuse of the physical woodblocks over time can now more easily locate prints that are derived from the same block but have different imagery.  Finally, a tool has been constructed to automatically provide institutions with corrections for their metadata, made possible by finding similar prints and then automatically comparing their associated metadata, looking for differences.  All of these tools are able to provide unprecedented improvements to researchers, scholars, and institutions. The theater photographs work is at a somewhat earlier stage in its development. The project’s early experiments with face recognition in the computer vision library OpenCV 10 identified the location of faces within a photograph reliably, but could not be used to suggest whether a face in one photograph belonged to the same person as a face in another.  More promising has been the OpenBR11 library from MITRE Corporation which, after “registering” a library of photographs, can quickly return a set of photographs from the library containing faces that most closely match one depicted in a new photograph.  For faces displayed at similar angles and under similar lighting, it performs relatively well, but when the angle changes and additional faces appear in the picture, mistaken identification is more common than success. Nonetheless, the mistaken identifications are sometimes usefully provocative.  False positives reveal similarities among physical characteristics, costumes, and makeup that may not be obvious.  For instance, a search using a photograph of young Roddy McDowall as Mordred in the original 1960 Broadway production of Camelot returned (with 100% certainty): Steve Lawrence in a 1967 production of Golden Rainbow, a headshot of actress Sybil White, and a photograph of George C. Scott in Plaza Suite.  To the most observers, these faces bear relatively little resemblance, however, to the face recognition algorithm, which looks mostly at the shape and position of the eyes12, the faces appeared identical.  As earlier investigations by Jerome McGann have revealed13, this “deformance” of the image by the algorithm may reveal new ways of interpreting the objects.  Are there any other similarities (not just visual) among the performers (or the characters they are portraying), that may not have been noticed without the provocation of the algorithm.  What do these “fail cases” suggest about the casting practices or makeup design on the mid-20th century Broadway stage? This paper explores both the promising successes and provocative failures of image analysis tools for humanities research, and suggests future avenues of research the technology makes available to scholars.  ",
       "article_title":"Using Computer Vision to Improve Image Metadata",
       "authors":[
          {
             "given":"Doug",
             "family":"Reside",
             "affiliation":[
                {
                   "original_name":"NYPL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "computer vision",
          "facial recognition",
          "woodblock",
          "image matching",
          "theater"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Background  One core practice that has traditionally supported humanist research process has been the practice to annotate the object of interest in some way. The handwritten additions to books in dustry shelves of libraries are documenting this very well. Although in digital humanities the concept of object is more problematic, annotations are still a fundamental and even increasingly important principle. For example annotations are used for data enrichment and harmonisation of massive digital collections, to model overlapping semantics and structures in presentation systems, and as the key means of connecting online material in Linked Open Data projects. Annotations are one of the core scholarly primitives of humanities research independent of discipline or media.   The more tools with annotation capabilities that are developed and the more annotations that are published, the more interoperability will be critical. The W3C Open Annotation Community Group and the definition of the Open Annotation (OA) Ontology are important steps to ensure interoperability among digital annotations. Nevertheless some important aspects remain to be done. The OA model describes a comprehensive way of rendering the structure of digital annotations; but its goal is not to reflect the methodological or hermeneutical context of digital annotations beyond what is needed to make the model generic. Moreover, the model claims that every situation where a web-resource can be linked to another web-resource can be modelled as an annotation.     Problem  The more annotations are published in a consistent way as web resources - a process that is promoted by the Semantic Web framework of the OA model - the more the question of interoperability shifts from the representation of structure to context. The problem here is not the use of a predicate to refer to the annotation target rather than to the annotation body; that can be easily resolved by the inclusion of any concept which is dereferenceable between oa:hasBody and oa:hasTarget. The problem is more that annotations which refer to the same object and which use the same concept may have meanings depending on the research context different from those for which the annotation were originally created. Consider the following examples:   For humanities researchers an algorithm represents an object of interest as much as any other cultural object. That is to say, annotations created in a context where the algorithm is investigated should not be used as expressions about the objects on which this algorithm was tested. As a result, modifications that are made during the investigation ought to reveal something about the algorithm not about the object. Annotations about the authorship of Shakespeare that are made in a stylometric analysis proving which works should be attributed to Shakespeare ought to be processed and interpreted differently from authorship annotations that create a catalogue of texts in a project. An annotation created in a crowdsourcing context without the use of a formal ontology is suitable for other research questions than the same annotation created by a disciplinary expert who applies a related ontology.   These cases demonstrate that the efficient reuse of published annotations needs a formalized representation of the research purpose for which the annotation was created in the first place. On the other hand such metadata would also provide a valuable information source in the digital environment for traditional humanities research questions. As Claudine Mouline has put it, contextual data about annotations ought to be an invaluable resource for investigating the social or cultural structures out of which these annotations were made. In one of her examples she gets insights about missionary work in Anglo-Saxonia as well as insights about the social structure of monasteries not only from the content of annotations but also from the context. Annotations made by high-priests can be compared with annotations written by monks if these can be distinguished at the first place. Moulin describes how this knowledge is derived from knowing in which monastery a book was annotated and which tool was used for the annotation because some tools were only used by high-priests. Different purposes for annotations were revealed giving the annotation a specific meaning.   It is true that the first draft of the Open Annotation Ontology attempts to address this issue by introducing the predicate ‘oa:motivatedBy’, in order to describe the motivation or intention associated with the annotation apart from its content. Nevertheless, the exact use of this predicate is still an open question, especially its relation to the type of annotation (Highlighting, Sticky Note, etc.) . Indeed, the community group emphasizes the importance of a general approach for using this predicate and encourages the definition of vocabularies in communities so that proliferation of terms can be avoided. Yet, it is still unsure if the oa:motivatedBy predicate will be kept in the final version of the Open Annotation model as was expressed by Timothy Cole at the DH2013 in Nebraska.    Approaches  The value of maintaining the concept of an oa:motivatedBy predicate may be seen by considering large European infrastructure projects like DARIAH and EUDAT which are developing annotation tools as generic components of their infrastructure. The generic structures and models adopted by these projects strongly demand that digital annotation practices and purposes ought to be systematized. At the same time the massive scale of these projects, resulting from their integrated services, offers a variety of possibilities of defining a vocabulary that can be used for oa:motivatedBy in the Digital Humanities. Specifically we suggest three sources:   The results of the DARIAH-DE experts workshop on interoperable annotations for the arts and humanities The efforts around the definition of the Scholarly Methods Ontology in DARIAH-EU The Scholarly Domain Model presented by Stefan Gradmann at the DH2013   Each of these sources represents a different approach to how the motivation of an ontology can be formalized. The methodological part of the June 2013 \"DARIAH-DE Experts Workshop on interoperable annotations\" built a classification upon the evaluation of practices in which digital annotations are used (to collaborate, to review, to enrich, etc.): a DH motivation vocabulary ought to be grounded in general practice and from the annotation point of view. The Scholarly Methods Ontology tries to classify digital research activities according to a methodological framework. Therefore it represents the best possibility of creating a more abstract vocabulary grounded upon theoretical reflections about how the field of DH is organized. The Scholarly Domain Model differs from the former approach because it does not focus on any research activity but on the areas in which humanities research activities take place: these areas are Input, Output, Metadata and Social Context and Research.    To put it more simply: the first classification model relates to the annotation object itself, the second tries to reflect fields of research, while the third introduces an abstract view on knowledge production itself. It would be misleading to select any one of these perspectives as the annotation model, since all of them have advantages and disadvantages. For example, the Scholarly Domain Model is currently too general in terms of semantics to help overcome interoperability issues for annotations. Besides, these models do not pertain to mutually exclusive ideas: enrichment by annotation in the first model is also suitable to describe the metadata class of the Scholarly Domain Model, while reviewing is located along the input/output axis. Thus, rather than prioritizing one perspective over another, we suggest an ontology concept for oa:motivatedBy that integrates these three models through their overlaps and interconnections, which in turn respects the idea that any ontology should relate annotation patterns to specific research fields.    Perspectives  The problems and approaches presented in the context of annotations and the Open Annotation Ontology address more complex ongoing discussions, such as the need to have a sustainable and expressive concept of provenance for digital research objects. This task is especially difficult in the case of humanities since the frequently non-processural and non-linear elements of humanities research do not sit well with the purely event- and agent-based provenance models created in the e-Science realm. The case of annotation is a good example. The meaning of a result for a humanities-related research question is not made totally transparent by the process or the broader lineage in which it ‘happened’. The Open Annotation Ontology suggests the use of the Provenance Ontology which is modeled along these concepts therefore not offering additional help. Our contribution, then, aims not only to show the importance of the oa:motivatedBy predicate and make the first steps towards developing a concept for a related ontology; we also hope to encourage the development of a discriminating idea of provenance in the humanities that is rarely developed at present time.   ",
       "article_title":"Digital Humanists Are Motivated Annotators",
       "authors":[
          {
             "given":"Niels-Oliver",
             "family":"Walkowski",
             "affiliation":[
                {
                   "original_name":"Berlin-Brandenburgische Akademie der Wissenschafte",
                   "normalized_name":"Berlin-Brandenburg Academy of Sciences and Humanities",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05jgq9443",
                      "GRID":"grid.420264.6"
                   }
                }
             ]
          },
          {
             "given":"Elton T. E.",
             "family":"Barker",
             "affiliation":[
                {
                   "original_name":"Open University",
                   "normalized_name":"Universidade Aberta",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02rv3w387",
                      "GRID":"grid.26693.38"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "Annotation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Project Summary In the field of numismatics there have been many attempts to create overarching reference works for ancient Greek and Roman coins, with different degrees of success. Challenges include the sheer amount of coins to be documented and the difficulty of accessing them, being spread throughout the world. A virtual collection however opens up new possibilities and promises to come closer to this aim through collaboration and linked data.    The Ancient Coins of Thrace concentrates on coins originating from a specific ancient region (Thrace—today's Bulgaria, northern Greece, and European Turkey) digitally collected from around the world. A web portal of this character is quite unique within the numismatic community. The project hopes to provide a method which others can emulate, perhaps leading to a group of online resources which together accomplish that which a printed edition could not. Funding comes from a three year DFG (Deutsche Forschungsgemeinschaft) grant. The project is located at the BBAW (the Berlin Brandenburg Academy of Sciences and Humanities) and is currently starting its second year. Its main partner is the Bode Museum Coin Cabinet, part of the Berlin State Museums. The project has two goals. The first is the virtual collection itself: a web portal where ancient Thracian coins can be found. This is accomplished in three ways.  The BBAW: The academy has its own extensive collection-- numbering almost 33,000 plaster casts of about 16,500 Thracian coins. The coin data is being entered into the project data base along with scans of the plaster casts. Cooperation with museums and institutions: The project's first and main partner is the Bode Museum Coin Cabinet. They are providing data and photos from about 4,000 Thracian coins. Other museums and institutions have expressed themselves as ready partners, such as the ANS (American Numismatic Society). Individual entry: This is one of the project's more special features. Users will be able to register on the web portal and enter their own Thracian coins. This is especially for smaller museums that do not yet have data bases or for private collectors. Smaller museums would then have an online presentation of their Thracian coin collection.  The second goal is research-oriented: it involves the typification of Thracian designs (the pictures on coins) and legends (the texts on coins), together creating the 'type' of a coin. Through standardized design descriptions and legends, the identification of dies2 is greatly furthered. All coins found in the portal will thus be linked to these standardized descriptions, and as a result dies can be identified and given a fixed number. This is an essential step for numismatic research.3   What challenges arise from such an undertaking? Digital standards: One important focus of the project is implementing and promoting numismatic digital standards. To this purpose nomisma IDs4 are used for all relevant fields, and lists are being sent to nomisma for new IDs for those which do not yet exist. The nomisma data base standards (NUDS) were also used wherever possible. Such standards are essential for data exchange but will only become truly useful when implemented by other institutions. The Bode Museum Coin Cabinet is participating by entering nomisma IDs for Thracian coins. The ANS, being the main partner of nomisma, have nomisma IDs for their coin data as well. It is hoped through example to encourage the use of these standards in other numismatic data bases.  Import and web interfaces: the original concept for the portal was to import coin data from other institutions into its  own data base. This is currently the method in process for the ca. 4,000 coins from the Bode Museum Coin Cabinet. The Bode Museum is however a very involved partner, working actively and closely for a successful import. One could raise the question if perhaps web interfaces—something like windows into different Thracian coin collections—might not be a more practical solution for other institutions. It must not be forgotten, however, that all coin data must still be linked to its standardized description. Such questions are still being discussed.  Individual entry: This rather special feature of the web portal brings its own difficulties. Accessing collections which are not yet in data bases and involving these members of the numismatic community is essential to the project. Holding very high standards for coin data (both in terms of the numismatic research and digital standards) is however equally important. A very strict entry mask could alleviate difficulties but does not offer a final solution. It remains to be seen how much reworking the individually entered coins will require.  Data model:  The data model is complex and its conception took time. Yet the even greater challenge was translating numismatic research goals into the language of data modeling. In other words—as an initial step before creating the data model—the challenge of understanding the aims of the research, the relationships between numismatic concepts, and how the research process itself would enfold.   Conclusion The Ancient Coins of Thrace strives to create a web portal that offers easy access to Thracian coins, that provides a typification of design and legend, and that furthers numismatic digital standards. It hopes to be of use to museums, private collectors, students and researchers. Because many aspects of the project are relatively new for the field of numismatics (such as a regional based web portal, individual coin entry, and linked data with nomisma IDs), it is certain that these efforts could be improved with time and experience. At the conference we wish to share our progress and developments thus far, receive feedback, and exchange ideas with other projects.  ",
       "article_title":"The Ancient Coins of Thrace: A Numismatic Web Portal",
       "authors":[
          {
             "given":"Elise",
             "family":"Hanrahan",
             "affiliation":[
                {
                   "original_name":"BBAW, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Due to an exceptional event, a fragment of The Dream of the Rood poem is found on the Ruthwell Cross (Dumfriesshire, Scotland) in the form of an inscription in runic characters; another fragment of the same work, although much smaller, is visible on the Brussels Cross (St. Michael and St. Gudula Cathedral, Brussels, Belgium); the two monumental crosses of Ruthwell and Bewcastle (Cumbria, England) share the presence of runic inscriptions and the same type of carved decorations; the two poems Elene and The Dream of the Rood (full version) are part of a florilegium of religious works, the Vercelli Book (MS CXVII, Biblioteca Capitolare di Vercelli, Italy), in virtue of the central role that the Cross plays in both: the first poem belongs to the texts inspired by the legend of the inventio crucis by St. Elena, mother of emperor Constantine, while in the second one the Cross itself appears in dream to the author and tells its own story. These witnesses of the Anglo-Saxon Middle Ages are closely related from a thematic point of view as well as for their contents1: one could say that this specific thematic cluster was handled by means of a sophisticated multimedia approach by Anglo-Saxon authors, especially visible in the case of the Ruthwell and Bewcastle crosses. The Visionary Cross project2 aims at creating a mixed media edition of these artifacts putting together not only the critical edition of the poem, which is going to be showed together with the digitized images of the Vercelli Book, but also the three-dimensional data related to the Crosses. 2. Methodological issues I: integration of heterogeneous data Within the scope of the project I am currently working with the ISTI-CNR researchers (Pisa) to refine and improve the current version of a combined 3D model / textual edition browsing software3, and to prepare a digital edition of the runic version of the Dream of the Rood poem. During this work we faced two critical issues:     the integration of heterogeneous data on the web platform that we are building to visualize the edition: namely, the 3D model of the Ruthwell Cross, which is in a standard format (PLY - Polygon File Format4) but needs specialized software libraries to be displayed in a Web browser window; the digitized images of the Vercelli Book manuscript; and finally, the edition texts (diplomatic and critical editions of the poem fragment);    the adoption of an encoding standard that would allow to preserve the different layers of the text (on the graphic level: rune characters vs. transliterated characters vs. modern rendering of text; on the edition level: diplomatic, interpreted and critical editions) at the same time connecting seamlessly with the 3D and 2D elements included in the web platform.    The two issues mentioned above are currently being solved by means of a software framework designed to be flexible enough to be adapted to different types of media and to cope with the specific characteristics of each object; and by use of the TEI XML schemas5, in a ad hoc customization using only the TEI P5 modules needed for our purposes, to accomplish an encoding of the runic text which is fully interoperable with both the web-based visualization system and the tools commonly used in the digital philology field. The rationale being that, while 3D models have recently started to appear and gain in popularity, they still are self-contained objects with little to no concession to the (possibly very relevant) textual content of the original artifact: our goal is to integrate the separate components in such a way that all the subtle interrelations and connections between the different objects, in short their original multimedia nature, are exposed and made explorable in a flexible browsing environment. For instance, the runes are inscribed on the two narrower sides of the Ruthwell Cross and it is likely that each side, including the larger ones with figural panels, were to be \"interpreted\" in turn according to the movement of the sun6; furthermore, the Bewcastle Cross shows similar iconographic content, but with subtle differences that can be ascribed to the influence of larger cultural traditions (Celtic, Roman) on the work of local artisans.   Fig. 1: Experimental edition of the runic text of the Ruthwell Cross  3. Methodological issues II: defining the user and what s/he can do with the edition While exploring the future shape of the aforementioned environment, though, we came up with new research questions begging for an answer:   who is going to be the “typical” user of our edition? This is apparently an easy question, but when we considered possible use cases we came up with many more than first anticipated, so that the perspective audience almost looks as heterogeneous as the different media of the edition; taking into account these use cases has led to new functionality being added to the browsing environment, and to the decision that it should be as modular and flexible as possible for future expansion;     what is our user going to do with our edition? Another apparently innocuous question, especially since we considered from the start, besides simple browsing of the content, the possibility of user annotation of edition objects; what is different, as we could try for ourselves, is that giving more powers to the user leads to interesting (and potentially risky) new scenarios.   A traditional edition lets the user verify the soundness of the editor’s constitutio textus by means of the critical apparatus: the latter is a very successful compromise dictated by the limits of the printed edition format, but a digital edition can go beyond such limits and make it possible not only to verify the current version of the edited text but also to experiment with alternatives7, creating a “personal edition” on the basis of the available material and the tools provided by the browsing platform, and sharing it according to the “social edition” concept8, although this will be limited to the \"collaborative annotation\" feature described by Siemens rather than to the \"user derived content\" one (in other words, we are not thinking of a full \"collaborative edition\"). Note that in theory this is possible not only with regard to textual content, but also for the 3D reconstruction we offer: the Ruthwell Cross is suffering by problems of incompleteness (the horizontal arm is a spurious piece dating to the XIX century), wrong reconstruction (the top piece was placed back-to-front), legibility of the inscriptions (part of the runes have been effaced by the prolonged exposure to the weather and other damages): making the 3D model dynamic and open to alternative positioning of selected parts, and offering digital restoration tools to insert “digital runes” where the original ones are no longer visible, it will be possible to build and check different theories about the Cross and the text it bears.   Fig. 2: The Ruthwell Cross 3D model as a teaching tool, clicking on the presentation arrows the model rotates andmoves to the location described in the slide  We intend to offer suitable tools to verify our own (as editors) hypotheses, but also to create and manage newhypotheses and to share them with other users, be they related to the textual layers of the edition or to thematerial aspects of the artifact(s) available in the browsing environment. To do this we will have to go beyond asimple catalog of possible use cases, but also prepare the environment for different user “roles”, assigning to eachrole an appropriate set of capabilities (and responsibilities). While the original plan was that of a singleenvironment for all types of possible users, in fact, our initial work with text and 3D models convinced us that it isimpossible to conflate together extremely heterogeneous features. Hence the need to set up a very small numberof different environments in which part of the functionality is shared and always present, while other features arespecifically targeted to a particular type of user. Conclusion This paper will report on the methodological issues described above, explaining which solutions have already beenfound and at least in part implemented in the browsing environment, which issues are still open and how theproject researchers intend to deal with them.   ",
       "article_title":"Mixed data, mixed audience: building a flexible platform for the Visionary Cross project",
       "authors":[
          {
             "given":"Roberto",
             "family":"Rosselli Del Turco",
             "affiliation":[
                {
                   "original_name":"Università di Torino",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In Earhart’s recent essay “Can Information Be Unfettered? Race and the New Digital Humanities Canon,” 1 Earhart critiques the digital “canon that skews toward traditional texts and excludes crucial work by women, people of colors, and the GLBTQ community” (316), advocating an activist model of grassroots recovery projects to expand current digital offerings.  In response to such concerns, Earhart and Taylor are currently testing a digital recovery project entitled White Violence and Black Resistance in Texas. This year’s conference theme is Digital Cultural Empowerment, a theme directly related to our testbed project which emphasizes the expansion of cultural capital and digital literary skills through our model pedagogical project.  We view our project as connected to interventions into current structures of production through the digitization and dissemination of materials about white violence and black resistance found buried in difficult to access rare book rooms, crumbling newspapers, analog and/or transcribed oral histories, and unknown journals. Working with primary materials of the reconstruction period, the project seeks to illuminate a time of great cultural conflict within Texas. Newspapers, Freedmen's Bureau records, marriage records, census records, legal records and oral histories illuminate the response to and resistance of emancipated African-Americans from 1867 through the turn of the century. Other projects, such as Black Gotham, have made such archives open to the public, but no project to date has been modeled on the type of student research learning model we are utilizing. Our ongoing project presents an activist model grounded in the classroom where undergraduate students are participants in canon expansion while learning valuable research and digital literary skills, a model that we believe other digital humanists interested in canon expansion and digital pedagogies might replicate.  Our project is developed to answer two challenges in current digital humanities practice.  First, we view our project as a way to leverage expertise and resources across historical areas of divide. Criticism of current institutional digital humanities practice has targeted the divide between institutional haves and have nots, often portrayed in tensions between well funded research institutions versus small teaching institutions. Our testbed of Texas A&M University and Prairie View A&M University2 provides an important site of intervention.  Founded in 1876, the two universities were divided by race, during segregation, and finance. Though the state constitution in Texas states that both are “universities of the first class” they have not seen funding and resources that make this true. Rather, the campuses have continued to be marked by the separations of race and resources which creates a space where Texas A&M is constructed as a predominantly white research university and Prairie View a historically black teaching university. We reject the differentiations and view projects like our current work as a way of disrupting such binaries, using carefully constructed technological projects to spread digital cultural empowerment through both universities and student bodies. Here we agree with the FemTechNet whitepaper, which states that “We seek to activate a learning process that recognizes and extends across global and cultural contexts. While the use of the world-wide-web and internet infrastructures enables communication among people at great geographic distances, it also strains the capacity for respect and the appreciation of the nuances of diverse backgrounds which increases the intensity of the work that must be done by teachers and organizers of the learning process.”3  In our paper we will discuss how the structure of the project is shaped by such concerns. Second, our project focuses on the recovery of cultural objects that have not been included in digital collections.  Recent work, particularly self defined postcolonial digital humanities projects, has pointed to the lack of attention that digital humanities pays to historical and literary production by marginalized peoples.  Our efforts are focused on the newspapers, photographs, historical accounts and other archival artifacts that discuss the racial violence, tensions and other aggressions (micro and macro) in our localized Texas environment.  By digitizing a related collection of materials we will diversify the digital canon related to conceptions of Texas and the universities. Through our digitization project students will be theoretically and methodologically immersed in practices of research and archival decision making critical to a broader more diverse digital canon.   This project marks an expansion of previously developed techniques. Our work grows out of individual recovery projects that have been tested by both Taylor and Earhart. Taylor has worked with her undergraduate students to collect the oral histories of women who have had a thirty year or longer relationship to Prairie View A&M University as faculty, staff and/or alum/student in the development of the Prairie View Women Oral History Project. Earhart has worked with her undergraduate and graduate students to recover the Alex Haley Malcolm X papers (Scholarly Editing 2014) and selected Black Radicals Papers which include materials from the Black Panther movement, Angela Davis’ prison stay, and various racial reform protests in the 1960s.   We have selected a well-known technology tool as a technological interface for collaboration.  Not only is Omeka an excellent entry technology because of its low cost, simple interface, and large developer and user community, but we argue that Omeka is useful as teaching tool due to the emphasis on Dublin Core metadata which forces students to engage with the ways that a controlled vocabulary has been culturally shaped and where such vocabulary is at odds with specific cultural norms engaged in the black resistance documents. Each class is working with collections held at their respective libraries to find the artifacts for the project then collaborate to curate a cohesive digital space which tells the story of White Violence and Black Resistance in Texas. Using Omeka as our bridge, Earhart and Taylor have modeled student research across the two universities, emphasizing individual archival collection and collaborative moments of interaction between the classes.  This paper will discuss the challenges with working across disparate universities where tradition and necessity have created the space for PVAMU to thrive, its one hundred and thirty plus year history itself a statement of resistance. This project expands the canon to understand the intersectional ways in which both universities actively and passively engage in moments of violence and resistance.  Each class has focused on a particular historical event that occurred in the local area.  Earhart’s class is investigating what we believe to be the largest “race riot” in Texas.  The reconstruction era conflict occurred in Millican, Texas, a town located 15 miles from the Texas A&M University campus. During the first KKK rally in Millican, in 1868, armed freedmen fired on the rally, driving the Klan out of town. After the rally, George Brooks, a local preacher, began a black militia. Several confrontations occurred including a march on Bryan by a large group of armed blacks, which ended in an armed conflict and lynching of Brooks and from 5 to 100 African-American men, women and children. Reports of the conflict were recorded in newspapers around the world.  In addition, the Freedmen's Bureau has records from those sent to investigate the incident. This sketchy information promises to prove fertile for investigation of what is a pivotal event in Texas and African-American history. Taylor will be investigating the ways in which this and other events serve as part of the cultural narrative historical legacy of students, faculty and staff of Prairie View’s earliest members. Students are working with university and community archives, historical newspapers, local church archives, interviews, local Blues lyrics and local literary narratives.  During the class, the students locate materials, digitize, transcribe, conduct historical research on the document and related people and places, and enter the materials and metadata into Omeka.  We will share materials developed for our classrooms that guide students through Omeka.  Of particular interest will be the materials that we have created to explain and facilitate the use of Dublin Core metadata.  In addition, we will highlight ways that we have focused interactivity between the separate institutions and continuing challenges of such work. Our pedagogical approach not only helps students to understand archive to digitization projects, but emphasizes critical engagement in technological decisions. We will highlight what our project has taught us about developing a more comprehensive, inclusive and effective digital pedagogy and discuss future steps in our collaborative project. Once a substantial set of materials are collected, future classes will construct timelines and maps of the occurrences. By treating the materials as data, using a controlled set of metadata and search techniques, we will be recovering important information that might interact with broader data sets of materials nationally and internationally, ensuring that these important events are not erased from history. Alan Liu has challenged digital humanists “To be an equal partner–-rather than, again, just a servant-–at the table,” by  finding “ways to show that thinking critically about metadata, for instance, scales into thinking critically about the power, finance, and other governance protocols of the world.”4 Our project provides a replicable model that we believe will empower other teachers and students to engage with digital humanities.  ",
       "article_title":"Digital Activism:  Canon Expansion and Textual Recovery in the Undergraduate Classroom",
       "authors":[
          {
             "given":"Amy",
             "family":"Earhart",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Toniesha",
             "family":"Taylor",
             "affiliation":[
                {
                   "original_name":"Prairie View A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "cultural studies",
          "digital humanities - pedagogy and curriculum"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This paper will present the findings from a case study undertaken with the support of the National Library of Wales. It utilises web log analysis to discover more aboutt he use and users of Welsh Newpspaers Online (WNO),[1]  a digitised newspaper archive which currently contains around 420,000 digitised newspapers from, and relating to, Wales. Web log analysis has previously been undertaken to analyse online user behaviour, with utility for websites 12, e-journals 345, and digital resources 67. To date, however, it has not been used to analyse the use of digitised newspaper archives. This paper presents the findings from a detailed analysis of content logs from a period of three months, starting from the launch of WNO in March 2014, and provides an important empirical study into how users are interacting with digitised newspapers. In doing so, it helps to illuminate some existing debates about the impact of digitisation upon reading and scholarship.  These debates have focused on how digitisation fo newspapers has changed the way users engage with these materials. Mussell, for instance, has noted that article-level representation foregrounds the partial textual transcript, even though the physical article is actually one textual component operating among many others. This means that users of a digitised newspaper may lose the original context of the material by approaching it in a different way online 8. Brake additionally notes that \"digital representations of nineteenth-century copy denaturalizes it and transforms the reader... into a user  who sees the content inextricably embedded in the matrix of the newspaper pages\" 9. These issues have been expressed in concerns that this will have a sever impact on reading behaviour 101112. This paper demonstrates that, while user engagement remains extremely high, it is evident that the nature of this engagement has been unavoidably transformed by digitisation.  2. Methodology In order to undertake this analysis, the National Library of Wales IT team supplied a complete set of content logs, collected for a period of three months from the launch of the collection in March 2013. These logs specifically recorded interactions with content discover and viewing mechanisms on the WNO website. As a result, they are not a complete log of the user's journey. Instead, the logs track a number of important basic behaviours: searchers undertaken by users on the website (search queries); occasions where users have browsed, filtered, or otherwise interacted with search results (search results queries); and instances where users have viewed newspaper pages (content queries). The weblogs record each interaction with the website as a single line of plain text, recorded automatically on the website servers. The following example demonstrates the format of a single content query: 2013-06-02T12:26:50+01:00 51a5c97c3c8d3 llgc-id:3036868 llgc-id:3039814 llgc-id:3037695 Aberystwyth Observer 21 September 1872 [2] ART40 The elements are, in order: date and time of interaction; unique user ID; server IDS for website content; title of newspaper viewed; date of newspaper edition; page number viewed; article number on the page viewed. Additionally, search queries contain a field with the user’s search terms, and search results queries include a field which shows the nature of the user’s interaction. In total, there were over 300,000 weblogs, which therefore provide a rich source of information about user interactions with the content of WNO. This data allows several metrics to be analysed: most viewed newspaper titles; most viewed years; most viewed page numbers; average number of pageviews per visit; and percentage of pageviews involving search, search results or content queries. To achieve this, the investigator heavily post-processed the data in Excel.         All non-relevant fields were stripped from the spreadsheet, then a column was added which automatically populated pageview numbers for each unique user ID, from 1 for the first page viewed by a user, to X for the last. Search, browser and content queries were also changed to numerical values of 1,2, and 3 respectively. This allowed the data to be processed into appropriate graphical representations. In particular, it allowed large-scale analysis of the complete dataset, in order to uncover overall patterns and trends in graphical form. 3. Findings  The content logs have provided a wealth of fascinating data on the behaviour and interests of users of Welsh Newspapers Online. We will discuss some of the most important findings in more depth below.             The following chart shows the findings from the large-scale analysis of the content logs, demonstrating the percentage of queries by type at each pageview. From this chart we can see which broad categories of user behaviour were most common for any given pageview:   Fig. 1:   The chart demonstrates that content queries remain an important part of user activity, regardless of how long visitors spend on the website. In fact, once visitors view over 100 pages, they seem to view increasingly large amounts of content. By contrast, the longer a user spends on the resource, the less likely they are to be engaged in search activities. Instead, we see that search result queries replace search queries in importance.  This tells us that the importance of search as a discovery mechanism becomes less important with time spent on a particular visit: the majority of users begin by searching the collection, and then slowly but steadily move away from searching to view content or to browse search results. However, the data suggests that digital reproductions of newspapers do effect how users engage with them, in one particular way: while they browse the website, extensively, we can see that they do not browse through newspaper content. The following chart shows that users primarily tend to view the title page but, in general, the further into a newspaper the page is,  the fewer times it will be read. This data should be contextualised: the average number of pages per newspaper is around 6.3. However, the drop in views begins from the second page, which strongly suggests that users are unlikely to browse through pages:   Fig. 2:   In this respect, the findings seem to confirm the fears that digitised texts interfere with the deep, ordered reading behaviour attributed to printed materials. While there are valid concerns about the way in which inline reading may distance users from the original context of newspaper material, we must also recognise that users certainly are deeply engaged with digitised newspaper archives: it is the type of engagement that has changed. This highlights two important conclusions: the side-lining of browsing makes serendipitous discoveries less likely, and means that search functionality, OCR quality, and website design are as important as the availability of content in ensuring that users are able to discover content; and that existing remediations of newspapers are some way from recreating the reading behaviour that has been attributed to the physical text 1314. 4. Conclusion This case study demonstrates that content log analysis can greatly enrich studies of the impact of digitised collections. They provide empirical evidence of user behaviour, which allows a granular, nuanced understanding of how researchers interact with digitised content online. We can therefore learn a great deal more about this understudied area. The findings also confirm, though, a commonly recorded problem with content logs; they provide a strong statistical base for analysis, but they cannot provide an insight into why users behave how they do. As such, an analysis which relies solely on content logs is necessarily incomplete. This paper will finish by proposing that web log analysis can still play an important role in analysing and understanding usage of digital resources, but that it should not be used in isolation. ---------------------------------------------- [1] http://papuraunewyddcymru.llgc.org.uk/en/home?  ",
       "article_title":"Exploring Usage of Digital Newspaper Archives through Web Log Analysis: A Case Study of Welsh Newspapers Online",
       "authors":[
          {
             "given":"Paul",
             "family":"Gooding",
             "affiliation":[
                {
                   "original_name":"University College London",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  In a letter that Willa Cather wrote in response to a reader's critique in 1924, she claims:“I had a perfectly good reason for writing ‘Antonia’ in the first person,  masculine—and I did not for one minute try to ‘talk like a man’. Such a  thing as humbugging any one never occurred to me. It does not matter who tells a story. It is merely a point of view, a position which the  writer takes in regard to his material...”.1 Looking into an author’s unpublished writings for additional insight is an  analytic strategy with a long history in literature studies. With  selections from Willa Cather’s extensive collection of correspondences  now available for study, scholars are presented with the opportunity to  compare Cather’s private letters with her published novels. A  statistical examination of Cather’s letters has, until recently, been  impossible; until the 2013 publication of The Selected Letters of Willa Cather, Cather’s letters were not available to the public.2 The publication of the letters offers a glimpse into the private life  of Cather, a life that she kept carefully guarded. The collection  contains approximately 550 letters, spanning her life from age 14 until  just days before her death. While Cather often spent years working on  her published writings, the fact that many of her correspondences were  hurriedly dashed off indicates that an examination of the letters could  reveal a style of writing that is less polished, worked, and  intentional. The potential applications of  this type of statistical analytic in literature studies as a whole are  wide reaching. Many authors have left behind extensive collections of  personal writing that could hold untapped insight about how they  communicated in different settings, and how their writing evolved. By  using similar tools to those we utilized in this exploratory research of Cather, scholars can continue to quantify the suppositions that  traditional methods of literary analysis have yielded. 2. Methodology Our research was divided into two main tasks. First, we  established a “personal voice signal” from Cather’s correspondence. To  derive a quantitative signal, we narrowed the available letters down to  164 letters using the hclust() function in R (For additional  information, see http://www.r-project.org/.). Setting the number of  clusters to 18, we chose the cluster that contained the highest  percentage of letters addressed to distinctly personal correspondents.  The personal signal was created by using a mean frequency threshold to  determine a set of frequently used words in the corpus of letters. In  the tradition of Burrows and others working in authorship attribution,  we elected to measure style based on the occurrence of high frequency  word features.34 This allowed us to both avoid arbitrarily selecting words, and to avoid using any context-sensitive words in our signal.5 It is likely that the 15 frequency features, or function words, that we selected are also less dependent on an author’s conscious decision to  vary his or her vocabulary.6 Thus, function words might offer a better indication of the  subconscious, intrinsic choices that inform a writer’s voice. It was our initial expectation that Cather’s personal voice signal would differ  from the narrative voices present within her novels. Our initial  research question revolved around the relationship between private and  public authorship: how (if at all) did Cather’s fiction act as a  mouthpiece for her personal voice? Once we  generated a signal from Cather’s letters, we used a clustering function  in R to compare the signal with 15 of her novels. In order to compare  Cather’s novels with the personal voice signal derived from her  correspondences, we used R to calculate two statistical measures of  similarity: correlation coefficient and Euclidean distance. These two  units of measure compare the similarity between Cather’s personal voice  signal and her use of function words in the corpus of novels. Examining  the Euclidean distance (with the dist() function) provides a way to  calculate the numerical distance between the frequency of the function  words within the personal correspondences and the frequency of the same  set of function words within each of the five novels. The correlation  coefficient measures the linear dependence between the function words in the correspondences and the function words in each novel. We divided  each of the novels into 1,000 word chunks in order to track the extent  to which portions of the novels were more similar to Cather’s personal  voice than others. In order to calculate a unique Euclidean distance for each novel, as opposed to its parts, we averaged the distances for all  the 1,000 word sections of each novel to determine a mean similarity  between each novel and Cather’s personal voice signal. 3. Observations While some of our findings on the similarity  between Cather’s personal voice signal and her use of function words  within her novels reaffirmed our initial hypotheses, some have defied  initial expectations. We found that Cather’s use of function words  within her novels was significantly different from the personal voice  signal calculated from the correspondences. However, the frequency of  function words did not vary significantly among 14 of the 15 novels we  examined, regardless of the gender of the narrator. This would indicate  that, while there is a difference between Cather’s “personal voice” and  14 of her published novels, certain aspects of the voice she adopts in  these novels remains constant regardless of the identity of the  narrator. We did identify a single outlier among Cather’s novels. The  frequency of function words within Cather’s novel My Mortal Enemy more closely resembles the use of frequency words in Cather’s correspondences than any of the other novels we examined.7 My Mortal Enemy was published in 1926, approximately halfway through Cather’s  publishing career. Previous scholarly commentary on Cather has also  indicated thatMy Mortal Enemy is a singular book in her  fiction corpus. The novel focuses on two periods in the life of Myra  Henshawe and her husband, Oswald. The narrator, Nellie Birdseye, who  many scholars argue is modeled after Cather, acts primarily as a frame  for sharing the Henshawe’s story. It is widely considered to be the  embodiment of Cather’s own novelistic ideal, the novel demeuble.8 For  Cather, the novel demeuble was an evocative form of realism, stripped of unnecessary detail and embellishment. The unique style of narration  present in My Mortal Enemy raises interesting questions about similarities between this novel and Cather’s correspondences. If My Mortal Enemy embodies Cather’s own idealized notion of stripped bare, to the point  realism, than is it possible that Cather also adopted a similar style in her personal letters?  Interesting questions are also raised by the way in which My Mortal Enemydraws from Cather’s life. The novel’s autobiographical nature has largely  been shrouded in mystery since Cather carefully attempted to conceal the events and people that influenced her novels. Charles Johanningsmeier  has made one of the most thorough attempts at unraveling the  connections, tracing the novel’s influences to Cather’s history with the McClure family.9 In this sense, our work appears to support Cather  studies by backing up scholarly claims about autobiographical influence  with quantitative data. We believe that examining the relative  similarity or difference between the ‘voice’ of Cather’s letters and  fiction is relevant because of the claims she made. Based on comments  made in some letters, it appears that she perceived differences in her  writing that surpass the conventions of genre. If we assume that in  writing about deeply personal issues, an author is likely to lapse into a more personal style, our findings seem to support the claim that My Mortal Enemy was highly influenced by Cather’s personal life. This raises an  interesting question: does the use of autobiographical details in  Cather’s fiction indicate an unconscious use of a more personal style of writing?   4. Future Work In the future, we intend to deepen and expand  the traits that define the personal voice signal we are deriving from  Cather’s correspondences. Sentence length, structure, and the choice of  infrequently appearing vocabulary are all elements of a writer’s unique  voice. Our research team will be investigating the ways in which these  characteristics impact the relationship between Cather’s private letters and her published works.  It is  also our hope that the research we have begun will lead towards further  studies on the relationship between My Mortal Enemy and Cather’s  letters. In addition, since scholars recognize the main character from  My Mortal Enemy, Nellie, as modeled after Cather, future research might  begin to look specifically at the ways in which Cather’s personal voice  is associated with specific characters in her novels. This question  might again return to issues of gender; if it becomes clear that certain characters within Cather’s work speak in a way similar to Cather’s  personal voice, the next question should revolve around what these  characters have in common.    ",
       "article_title":"Exploring the Intersection of Personal and Public Authorial Voice in the Works of Willa Cather",
       "authors":[
          {
             "given":"Laura",
             "family":"Dimmit",
             "affiliation":[
                {
                   "original_name":"Nebraska Literary Lab, University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Gabrielle ",
             "family":"Kirilloff",
             "affiliation":[
                {
                   "original_name":"Nebraska Literary Lab, University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Chandler ",
             "family":"Warren",
             "affiliation":[
                {
                   "original_name":"Nebraska Literary Lab, University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Wehrwein",
             "affiliation":[
                {
                   "original_name":"Nebraska Literary Lab, University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "literary studies",
          "English studies",
          "stylistics and stylometry",
          "text analysis",
          "text mining"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The book as a research tool is not obsolete, but it is being augmented by surrounding clouds of data. Web research lacks access to analog and archival sources, while those who access physical materials often do so in isolation from other information resources. Despite the relevance of the physical library for humanities research, its once uncontested position has been challenged by the vast, easily accessible resources available on the Internet. Kemman et al. (2013) found that even among humanities scholars in the Netherlands and Belgium, general search systems are predominant (e.g., Google and JSTOR)1 . Within these systems, searching with simple keywords is the primary search strategy employed by users, while more advanced search strategies are uncommon. The physical library, then, no longer exists in a vacuum; rather, the browsing experience is constantly being augmented by scholars’ perceptions and expectations of the virtual information that supplements the information contained in the physical copies of books. Library users accustomed to working in digital environments are eager to have additional information such as keywords, tables of contents, links to other literature, and thematic suggestions at their fingertips. Yet, humanities scholars rarely critically examine the intricate relation between the physical and digital protocols of information storage, access, and organization. To examine the relation between the physical library and online resources, we conducted a series of user studies in which we employed a head-mounted compact action video camera to record the experiences of participants as they navigate the physical space of the library 2. Two important findings emerge from this work. First, participants report that physical stacks are better suited for facilitating the discovery of new resources than the catalog. As one participant reported, “I feel that if I’m looking up a topic and I go to the shelf for videogame studies, I’ll find books that I hadn’t seen in the catalog, but in the general field; even if I lose track of a single book, I’ll find another in the general area; this happens to me constantly.” Second, our user studies have shown that going to the physical library to search and browse for books is still an important and rewarding component of students’ research.  The careful organization of information in libraries and archives, together with the tangible, physical presence of analogue documents, has given humanist scholars a level of comfort in their primary research space. These scholars allude time and again to the chance encounter with information as a welcome digression within the library stacks, so much so that this phenomenon has become a normal, even anticipated aspect of their research process 345. While serendipity is an elusive concept, a number of models have recently been proposed that decompose the phenomenon into specific facets. Not all models of serendipity emphasize the same facets; however, the majority of models tend to stress the relevance of “noticing” or discovering novel information as a central component to serendipity. This component is also central to the work of humanists as they navigate stacks, archives, and even digital environments. One of the first models of serendipity, proposed by Erdelez (2004), underscored the shift in attention that occurs when a person is solving a problem or engaged in a foreground activity and then suddenly encounters unrelated information 6. In this model of information encountering, noticing novel information is central to any kind of engagement with information. Similarly, in their model of serendipity in everyday chance encounters, Rubin et al. (2011) describe how the act of noticing is central to any discovery of new resources 7. Likewise, Makri and Blandford’s (2012) study demonstrates that new mental connections or insights result from encountering novel, unexpected information 8. This finding has been sustained by our own initial user studies, in which participants consistently reported finding additional, unexpected information when searching for a specific book in the stacks. Humanist scholars in general note the importance of serendipity, with Hoeflich (2007) comparing them to “ancient mariners” who “set out on voyages of discovery hopeful of finding new lands of milk and honey” (p. 813). For Hoeflich, Walpole’s two-part definition of serendipity as a union of accident and ability was missing a third part: opportunity. Hoeflich notes the importance of the tactile aspects of archival documents, arguing that significant qualities of historical artifacts can be lost when reproduced in a different medium. The loss of serendipity is often perceived as resulting from the move to digital environments, and away from deep engagement and interactions with physical books, manuscripts, and archival records, as Martin and Quan-Haase (2013) have shown 9. They found that serendipity was central to the work of historians, because the encounter with a single key resource on library shelves or in archives could significantly affect the outcome of their research. The historians interviewed by Martin and Quan-Haase noted that they tend to limit their use of digital environments to quick searches, browsing subject headings, and fact-checking until they can recreate experiences of chance encounters with material that is similar to that experienced in library stacks or archival collections. Bess Sadler, Manager, Software Engineering Team at Stanford Library, spoke about what she believes is missing from the process of library discovery in a blog post adapted from her keynote for ACCESS 2012. Sadler (2012) writes about what keeps scholars motivated to work, how they consider library browsing a pleasurable part of their research process, and that she believes there is \"something missing\" from the virtual library collection. She states, “I think we’re still falling short of a full replacement for physical browsing. I think we’re still falling short of providing the kind of emotional, physical, and spatial sensory experience that shelf browsing, at its best, can provide” 10. Sadler’s call for change is pertinent to any scholar who has benefitted from research in a library setting. She came to this conclusion after conducting a study with graduate students in the humanities and social sciences, in which she noticed their love of browsing physical stacks, and their inability to conduct their research in the same way in a virtual environment. Even since Sadler’s talk, there have been several attempts by digital humanists to remediate the chance encounter or serendipitous experience (serendip-o-matic.com, mechanicalcurator.tumblr.com) We will describe our own attempts to reconcile the physical and digital library spaces through the creation of a dedicated mobile app, STAK (Serendipitous Tool for Augmenting Knowledge). Built upon Greenspan’s web-based geolocative StoryTrek authorware 1112, this web app will be grounded in both the theoretical models of serendipity research and in user tests conducted with digital humanities scholars. STAK will use a combination of RFID and wifi triangulation to detect a researcher’s location in the library stacks with fine-grained precision. After determining the user’s proximity to specific subject headings, it will automatically retrieve full-text excerpts on the same subject from networked databases, along with reviews, relevant titles by nearby authors, and different works drawn from both open-source and proprietary databases, sorting these hits on-the-fly by type and relevance. The app will link to live datasets that refresh dynamically, putting vast repositories of research networks into the user’s hand, wherever she may be. STAK will bring the shelves to life by providing ready access to the growing clouds of virtual data that surround physical texts, effectively annotating the library’s physical holdings of books, journals, manuscripts, prints, blueprints, microfiche, films and videos. We will describe the results of our intermediate tests, in which we ask users to perform library search tasks using a simulation of the proposed STAK browser app while wearing a head-mounted action camera and speaking aloud (though softly) about their user experiences. Our test app (Figure 1) is designed to simulate the function of STAK without relying on actual geolocation by retrieving online information from a constrained dataset relevant to the user's pre-determined location in the stacks. Each test is followed up with post-test interviews in which we ask users to elaborate on their experiences of STAK's perceived functionality, accuracy, and user interface design.  We will detail how these tests are allowing us to develop a user model and search algorithms based on the following three questions:  Why is the library the perfect setting for serendipitous discovery? What makes this setting so conducive to the chance encounter? How do the serendipitous experiences of library users reflect the findings of previous models of this phenomenon? How does the physical, controlled environment affect the possibility of chance encounters of information?  The attempt by digital humanists to replicate serendipity cannot rely on online sources alone. We need to bridge existing notions of humanities research and its inherited practices with current information resources and digital tools. When fully implemented, STAK will allow users to combine their physical and digital navigational tactics in a single search environment. Books, no longer the end point of the research process, become the fiducials that allow access to a wider cloud of information.   Fig. 1: Figure 1 - Simulated STAK Browser App   ",
       "article_title":"STAK – Serendipitous Tool for Augmenting Knowledge: Bridging Gaps between Digital and Physical Resources",
       "authors":[
          {
             "given":"Kim",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          },
          {
             "given":"Brian ",
             "family":"Greenspan",
             "affiliation":[
                {
                   "original_name":"Carleton University",
                   "normalized_name":"Carleton University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02qtvee93",
                      "GRID":"grid.34428.39"
                   }
                }
             ]
          },
          {
             "given":"Anabel ",
             "family":"Quan-Haase",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Research habits of humanities scholars"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The following proposal presents the results of a longitudinal study of reading in both digital and physical environments carried out as part of the INKE project, by researchers at the UCL Centre for Digital Humanities from 2009-2013. Our aim was to understand how different digital devices and physical, printed publications are used, and integrated into reader behaviour. It took place at a time of rapid change in the technology of reading in digital environments: the Kindle, iPad, and other tablet computers were launched, and quickly became popular, during the period of the study. We might hypothesise, therefore, that such a period would usher in the transformation of the reading experience, and wholesale move from paper to digital devices and ebooks which had been predicted in the literature.12 Our study of reading behaviour provides evidence against which to test such claims.  1.1. Research context Reading has been studied by cognitive scientists (see for example the journal Scientific Studies of Reading); those interested in literary reading 3 ;  and in how children learn to read (for example the very widely cited study by Wagner et al. 1997) but the way in which we integrate reading into our lives and behaviour is relatively little understood. Two different broad types of reading may be observed: reading to gather information, and immersive reading. The first, described by Vandendorpe as ergative reading, is used to find information in printed texts or digital documents. This is, he argues, well suited to digital environments, which make information seeking faster and more efficient. 4 Immersive reading is a complex interaction with a text, which we may read for pleasure, or which literary scholars may study and read closely such as novels or poetry. This is more difficult to study, entailing understanding of complex language, extended narrative, rhetorical devices etc. 5 Yet, contrary to Vandendorpe’s predictions, it remains as common as ergative reading: millions of people engage in such an activity every day, and many of them use digital devices, such as tablets. Yet, arguably, such technologies do little more than mimic the affordances of the printed book, and some users still find it hard to read long documents in any form other than paper. Thus it is still unclear whether digital environments offer significant advantages to readers of long, complex narratives, as compared to print.  Our study was therefore aimed to understand how users carry out both kinds of reading; what kind of device they use for which tasks; what they enjoy and what frustrates them about such devices; and what features and affordances they would like to see in new digital environments. The results of our study provide an enhanced understanding of reading behaviour, at a time of remarkable technological change, and aims to evaluate the effects of such rapid developments on readers and their views about reading.  2. Methodology This study was intended to capture change over a lengthy period of time and is, as far as we are aware, unprecedented, in terms of a study of adult readers, although there are numerous longitudinal studies of reader development in children: recent examples include studies by Nation and Hulme,6 and  Oakhill and Cain. 7 Its longitudinal nature is important: while other studies have been carried out into the practice of digital reading (for example, Gerlach and Buxmann's study) 8, they provide  snapshot of a user population. We argue that only by repeating the same task over several years is it possible to determine how, and whether, reader behaviour changes in response to different technologies. It was a cross-sectional study: the research activity remained the same over the course of the project, but we did not track one user group over the study period. 9  We chose to study a group of Masters students at the UCL Department of Information Studies, because they were adult, needed to read widely as part of their course, and were, typically, from a background in the humanities, thus we hoped they would also be likely to read for pleasure. We also integrated the study into the teaching and learning of the Electronic Publishing module, as a way of introducing the students to the practice of action research, and enquiry-based learning early in their course. Thus the study benefited not only our research, but also the student learning experience. Each group also included a significant number of non-UK students, many of them non-native speakers of English: this therefore allowed us to determine whether nationality or native language had any effect on reading behaviour.  Each participant was asked to keep a diary of their reading for a week. We asked them to note the time period in which they read, where they were, what technology they were using (making it clear that printed books, newspapers and magazines are reading technologies), and to note any problems they encountered or other reflections that they might have. Diary studies provide information about behaviour patterns, and attendant problems when using different technologies, and we have previously used them to study of field archaeologists’ use of digital technologies 10. We then held a group discussion session, in which the students reported back on their experiences, and responded, firstly in small groups, then in plenary discussion, to a series of questions about reading in different physical and digital environments. This allowed us to capture their views, and suggest a series of requirements and affordances for future digital reading devices.  The reading diaries were then collected and content analysis techniques used to identify patterns in the data. As the study progressed, themes were compared with data from previous years to identify change over time 3. Findings We found that our users had a very complex, almost instinctive, understanding of the affordances of different reading devices and environments. They had a wide repertoire of contexts for reading; physical setting such as the bus to work, the office, the sofa or, of course, in bed; or devices, such as phones, cereal packets, free newspapers, computer monitors, printed books, and, with growing frequency as the study progressed, tablets and e-readers. They typically read for relatively short periods, and moved frequently from one device or context to another, choosing print or digital media depending on their task; physical setting; whether they were on their own, or with colleagues; and the technologies available; rather than having dogmatic preferences for one type of device or environment over another.  Despite the rapid development of tablet and e-reader technologies, we were surprised to find that their views about design and requirements changed relatively little. While appreciating the potential of tablets for information seeking, and for storage and fast retrieval of a large number of books or documents, users still felt an emotional attachment to the book as an object. There was also no significant difference between the behaviours and views expressed by students of different nationalities. Users were well aware of the affective aspects of books, such as the feeling of paper under the hands, and its flexibility and warmth, as opposed to the cold, uninviting feeling of plastic or metal, and valued attractive book design, and the distinctive smell of newsprint or rare books. Despite the numerous experiments with annotation, navigation and bookmarking technologies in digital environments, users also expressed a strong preference for annotating physical copies, and valued the ability to flip through pages of a printed book as a way of orientating themselves.   As the study progressed more of our users seemed comfortable with reading long documents, including entire novels, in digital form, but it appears that those who do so are heavy users of tablets or e-readers, perhaps as a result of being early adopters. Surprisingly large numbers of respondents still prefer to read in print, and many of these are relatively light users of digital devices. Many of these users reported that it was difficult for them to feel the sense of flow- that is being absorbed in the narrative, and unaware of the device on which it is being read- when using digital devices as opposed to reading in print. It is difficult, at this stage, to tell whether the ability to read at length on a tablet is something acquired through practice, and enthusiasm for this medium, or whether users who are unenthusiastic about digital reading do not persist, and thus never acquire the habit of use and resulting enjoyment of flow.  4. Conclusion The particular significance of our study results not only from individual findings, some of which others have also noted, 11, but in the fact that, despite our initial hypothesis, we found a remarkably stable set of behaviours and user requirements irrespective of rapid technological change. Users understand the advantages of digital reading devices, but still value the tactile, visual and affective aspects of the printed book. It appears that the printed book is not as easily modelled in, or replaced by, digital environments, as had once been thought: doing so remains a significant challenge for future scholarship.   ",
       "article_title":"The dog that didn’t bark: A longitudinal study of reading behaviour in physical and digital environments",
       "authors":[
          {
             "given":"Claire",
             "family":"Warwick",
             "affiliation":[
                {
                   "original_name":"UCL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Simon",
             "family":"Mahony",
             "affiliation":[
                {
                   "original_name":"UCL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Samantha",
             "family":"Rayner",
             "affiliation":[
                {
                   "original_name":"UCL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"The INKE",
             "family":"Team",
             "affiliation":[
                {
                   "original_name":"INKE",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   I. Introduction   Fig.1: Urban infill created from the procedural 'domus' rule.  This paper presents a suite of procedural rules for creating 3D models of Roman and Hellenistic architecture and urban environments. The term ‘rules’ in procedural modeling refers to the computer code that generates a 3D model. Unlike traditional 3D modeling software such as SketchUp or 3ds Max, which use polygons to simulate form, procedural modeling entails the use of computer programming languages in textual semantic description of a building that then generates a polygonal model. This represents not only a technical, but also an epistemological difference, as the choice of modeling method can influence not merely the cost or aesthetic outcome of a project, but also how information is selected, processed, and indeed what is considered to be information versus noise. Procedural modeling requires that each stage of the transmutation of data in the modeling process is rigorously thought out and documented, allowing 3D models to move beyond visualization to become robust research tools.  Procedural modeling has the potential to address a number of issues related to 3D archaeological reconstructions which are of concern to digital humanists. An important advantage of procedural methodology is that it allows for the rapid prototyping and interactive updating of 3D content. The use of attributes and parameters enables scholars to visualize change over time and gauge the impact of various factors on the built environment. Furthermore, these attributes and parameters can be tracked and harnessed as valuable geospatial data through the use of GIS software and of interactive visual displays. Of particular interest for archaeologists and architectural historians is the ability to test hypothetical reconstructions of ancient architecture in a fully realized urban context. Crucially for humanists, the procedural rules link each iteration of a model to its source material, allowing the degree of certainty present in each model to be accurately defined through the documentation of each step in the process of interpreting a given data set. Procedural modeling thus enhances the scholarly value of architectural reconstructions by providing a platform for the comparison and refutation of 3D visualizations.   II. Background and Software  Reconstruction of historical architecture and cities was an early application of procedural modeling. Significant test cases were built around ancient Rome and Pompeii.1 However, the main commercially available procedural software behind these projects, ESRI CityEngine, is currently being marketed mostly as a low-cost rapid-prototyping pipeline to urban planners and production designers rather than to scholars for its ability to create data-rich, detailed architectural models.2 Procedural engines are based on proprietary high-level graphics programming languages which are extremely time- and resource-consuming to produce for most academics, particularly in the humanities, and therefore few open-source alternatives exist.3 A precedent for my current project is Pascal Mueller’s 2010 PhD dissertation which used classical temples as a case study for demonstrating the potential of CGA shape grammar, the procedural language that eventually became the core of ESRI CityEngine. 4I chose to use ESRI CityEngine for my work, because I find it to be the best commercial procedural modeling product for architecture and cities, and because its recent integration with ESRI GIS software such as ArcMap provides significant advantages for managing and visualizing archaeological data (Fig.2).   Figure 2: Screenshot from ESRI CityEngine showing text editor for rules (left) model (center) and model attributes (right).     III. Methodology and Research Application   My work aims at the creation of a full suite of procedural rules for the main typologies of classical architecture.  A master rule for classical temples exemplifies this project. The temple rule was designed to produce a schematic model of any kind of classical temple with a minimal number of parameters. By selecting from a few options, a user can instantly generate, for example, a tetrastyle Tuscan temple on a high podium; a peripteral Doric temple with pronaos and posticum; or an ionic pseudo-dipteral Hellenistic temple with variable intercolumniation. The rules were based on sources such as actual archaeological remains in Italy and Asia Minor, as well as Vitruvian templates.The rules are designed to by fully modular, that is, the rule for a specific typology such as a temple, arch, or stoa collates several  sub-rules, which can be re-used and combined as necessary. These construct the components of a building, such as colonnades, entablatures, pediments, and roofs, or refer to a specific order, such as Tuscan, Doric, Ionic, or Corinthian.  Urban models are then generated from geodatabases imported from GIS software ArcMap. These geodatabases contain the footprint of the building, along with specific attributes necessary to create the model (such as column diameter, order, and building type), and the bibliographic citations that reference the source material from which the attribute data was derived. All of this textual material may be queried in the final visualization of the model. The original impetus for the creation of the suite of rules was the generation of a series of 3D models of early Republican Rome, eventually to be visualized interactively with Unity game engine web viewer5. The procedural rules facilitated the hybridization of actual Roman data, comparanda from other sites, and hypothetical interpolation that was necessary to complete the picture of the Forum Romanum in this time period. Eventually, the project and the rules expanded to cover the site of Magnesia on the Meander in Turkey, necessitating the addition of new typologies. To date, the suite of procedural rules includes a core set of typologies responding to the topography of Rome and Magnesia on the Meander. These include temples, altars, basilicas, houses, shops, streets, triumphal arches, arcades, colonnaded streets, stoas, theaters, and stadiums (Figs.1,3,4). It is anticipated that these rules will become valuable tools for visualizing the urban fabric at numerous other locations where the archaeological data must be supplemented with well-researched templates that provide customizable parameters.   Figure 3: Procedural basilica, shown in a configuration with a row of shops in front    IV. Conclusions  Procedural modeling presents a powerful new methodology that has yet been underexploited by the Digital Humanities. Contrary to traditional 3D modeling methods, procedural modeling forces the investigator to approach visual and 3D content through a rigorously syntactic and process-oriented framework. This framework preserves the hierarchy of decisions that result in a given visual interpretation of archaeological evidence. Models thus produced are extremely information-rich and the ways in which they can be used to aid research are just beginning to be explored.   Figure 4: The Ionic temple of Artemis at Magnesia on the Meander, procedurally generated from the master temple rule.    ",
       "article_title":"An Integrated Approach to the Procedural Modeling of Ancient Cities and Buildings",
       "authors":[
          {
             "given":"Marie ",
             "family":"Saldana",
             "affiliation":[
                {
                   "original_name":"UCLA",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  Arch-V (short for Archive Vision) is a newly developed C++ application that provides image based search capabilities for digital archives of print materials.  In 2013 the English Broadside Ballad Archive (EBBA) at the Early Modern Center, University of California, Santa Barbara was awarded an NEH Start-Up Grant to begin work on the Ballad Impression Archive (BIA), a component of EBBA specifically devoted to cataloguing the over 9,000 (and growing) individual woodcut impressions in EBBA and making them fully searchable as an image collection. A key component of this award was the creation of software to provide automated, image based searching of the collection.  The proposed short paper will introduce and provide an overview of the implementation procedures for Arch-V. 1.1. Overview   Arch-V is a platform for delivering automated, image-based indexing and searching of digital archives. While the state of the art in computerized image classification and recognition is quite advanced, the application of these technologies to the specific area of digital archives of printed material presents a unique set of challenges. As noted by Relja Arandjelovic and Andrew Zisserman, the problem of automated recognition of objects has been largely solved, “provided they have a light coating of texture” 1. This is because the state of the art in computer vision relies upon the refraction of light across the surface texture of  an object as it is captured in a digital image (or frame of video) in order to extract recognizable feature points as indexable markers of the object in the image. But in digital images of print artifacts, surface texture serves as a distraction from and not an indicator of the objects depicted in the print. This is because the texture belongs to the delivery medium, the carrier, and not to the objects being represented. As a result, the efficacy of current technologies is less than satisfying when applied to the area digital archives of printed materials. More importantly, this is not a problem that computer science researches are likely to solve for the humanities, as the primary interest, funding, and work effort in computer science is in the area of processing networked picture and video feeds such as surveillance footage, YouTube videos, and Facebook photos.  We were able to design and test a solution to this problem as part of the funding provided by the Start-Up award. This solution involves a process of normalizing color and black and white archival images to a common format prior to feature point extraction, utilizing a modified feature point extraction methodology, and combining the feature point extraction with a process of border contour extraction and comparison. This combination of practices allows us to produce a collection of feature points for each image that define the boundaries of the objects represented in them rather than variations in surface texture. Our solution has already been implement in the EBBA cataloguing interface, and it will be implemented on the EBBA website in early 2014.    We continue to investigate and implement improvements (along lines identified during the start-up phase) to the image-based searching technology that specifically address its application for digital archives of print materials, to refactor the codebase as a distributable software package that can be easily implemented by other digital archives without advanced technical knowledge or experience, and to produce companion documentation to insure both success and ease of implementation by other archives.  In its current form, the complete c++ codebase is publicly available via Git at https://bitbucket.org/cstahmer/archv/.   1.2. Methodology Arch-V utilizes of novel combination of SURF feature point extraction of raw images, and feature point extraction of extracted contours from the base image set into order to create a Visual Dictionary of extant features.  Each image in the archive is then processed using the same extraction algorithms and a Visual Word File representing a normalized histogram of the features found in each image is then created for each image.  The Visual Word Files are then indexed using Lucene, which serves as the query engine for image comparison. 1.3 Scope of the Presentation The proposed Short Paper will introduce the theoretical problems associated with performing visual searches of archives of print materials, give a short demonstration of the Arch-V software in action searching the over 9,000 images in the EBBA archive, and provide information on how users can Implement Arch-V in their own archives.  ",
       "article_title":"Arch-V:  A Platform for Image-Based Search and Retrieval of Digital Archives",
       "authors":[
          {
             "given":"Carl",
             "family":"Stahmer",
             "affiliation":[
                {
                   "original_name":"UC Santa Barbara",
                   "normalized_name":"University of California, Santa Barbara",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02t274463",
                      "GRID":"grid.133342.4"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Computer Vision"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper analyzes how digitized newspaper databases can be used in historical research on identity formation in public discourse. It discusses a new semantic text mining tool, Texcavator, which is currently being developed in the Dutch research program Translantis: Digital Humanities Approaches to Reference Cultures.1The paper presents a case study which combines the Texcavator tool with the publicly available Delpher2 and with traditional historical methods in order to analyze identity formation of health risk groups in Dutch public discourse in the twentieth century. In particular, it focuses on the construction of the identity of people with excess body weight. Although the case is built around the Texcavator and Delpher mining tools and the newspaper database of the Dutch national library, the paper aims to investigate techniques to combine close and distant reading that can be transferred to other tools and repositories as well. Newspapers are valuable sources in historical research. Until recently, however, investigating them was cumbersome and time-intensive. The repositories of digitized newspapers now available in many countries solve many practical problems and offer wonderful opportunities, but they also introduce methodological problems of their own. (Bingham 2010; Nicholson 2013) Bob Nicholson has recently shown how digitalization enables us to approach newspapers bottom-up instead of top-down, but he stresses the difficulty of creating useful keyword searches for doing this. (Nicholson 2013, 66–67) Adrian Bingham has also pointed this out, and has furthermore highlighted the danger that keyword searches (as well as other text mining techniques) pluck individual articles out of their original context, ignoring their position on the page, surrounding articles, and illustrations. (Bingham 2010, 230) Furthermore, Johanna Drucker has indicated that digital humanities scholars often aim to reduce complexity and remove ambiguity, while these are two values humanities research has to cherish, not avoid. (Drucker 2009, 5–7; Collini 2012, 65–84)  This paper takes such warnings into account and shows how these problems are being addressed by researchers working with the digitized newspaper database of the Dutch national library, thereby offering more concrete versions of the rather general solutions (e.g., ‘we should not forget the article’s context’) that are often suggested. At present, this database contains over 10 million pages from more than 200 newspapers and periodicals published between 1618 and 1995.3It can be approached in two ways: through Texcavator (in development, not yet publicly available) and through the national library’s Delpher tool (publicly available). The paper discusses a specific use case in which both tools are combined and used alongside traditional historical methods: researching identity formation in public discourse. It focuses on the identities of (health) risk groups, groups of people that are classified as ‘at risk’ with help of (health) risk factor classifications like the body mass index (BMI). For example, nowadays, people with a BMI above 25 are classified as ‘at risk’ because of their high body weight. This classification and the construction of this group is not a necessary outcome of biomedical research on the human body; instead it is historically contingent, strongly rooted in culture and practice. (Hacking 2007a, 2007b) The construction of these risk groups and the formation of their identity takes place for a significant part in public discourse. Digitized newspapers are valuable sources to study this identity formation: they provide a good entry into public discourse and typically span long time periods, enabling researchers to analyze the fluctuations in the identity of these groups (e.g., fluctuations between whether or not they are seen as (and see themselves as) ‘ill’). The paper presents the first results of the investigation of the identity construction of the risk group ‘overweight people’ between 1890 and 1990. It focuses in particular on newspaper advertisements in the first part of this period — a choice based on distant reading of the corpus with help of Texcavator. The paper discusses how Texcavator and Delpher have been used, focusing in particular on the interaction between close and distant reading necessary to do this type of research.  It shows how the direct connection between Texcavator and Delpher makes sure the researcher is constantly only one or two mouse clicks away from viewing the single articles in their original context — on the page, including illustrations, within the full issue of the periodical, as if going through newspapers on microfilm (or, depending on the size of the computer screen, leaving through them on broadsheet). Furthermore, it shows how Texcavator’s built-in visualization tools (time lines with number of articles diagrams, word clouds, named entity recognition) can be used to go back and forth between distant and close reading in order to build sophisticated queries that can easily be refined and modified within the tool. In this way, the paper shows the challenges but also the new heuristic possibilities of doing historical research in digital repositories of newspapers.  ",
       "article_title":"Using digitized newspaper archives to investigate identity formation in long-term public discourse",
       "authors":[
          {
             "given":"Hieke",
             "family":"Huistra",
             "affiliation":[
                {
                   "original_name":"Utrecht University",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          },
          {
             "given":"Toine ",
             "family":"Pieters",
             "affiliation":[
                {
                   "original_name":"Utrecht University",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction While sound studies is experiencing a resurgence in literary criticism, academic arguments involving sound are nearly impossible to make in traditional print media.  An article could include an excerpt of a score, but only scholars who can read music would be able to understand it.  Likewise, articles or books could include audio files externally, as did Nicholas Temperley’s special edition of Victorian Studies, which included a cassette tape with the songs discussed in the articles. 1  However, these solutions do not address the central problem:  readers will have difficulty finding the exact musical phrases mentioned in articles, and those with less musical expertise will be left out of the conversation entirely.  Newer options for incorporating music in academic articles include SoundCite (soundcite.knightlab.com), a tool that lets users embed sound clips in websites, Scalar (scalar.usc.edu/scalar), a publishing framework that lets users annotate media, and the strategy of assigning a QR code to each audio excerpt and inserting these into a print article, as Jennifer Wood suggests. 2  None of these options integrates the audio with the score:  SoundCite will only let users hear the audio, Scalar only supports textual annotations of media files, and QR codes require readers to have smart phones, which vastly limits the audience for the article.  To address these problems, I have built two tools:  \"Songs of the Victorians\" (www.songsofthevictorians.com), an archive and analysis of musical settings of Victorian poems with an interactive framework that highlights each measure of a score in time with its music, and \"Augmented Notes\" (www.augmentednotes.com), a public humanities tool that allows users who do not know how to program to build their own sites like \"Songs of the Victorians.\"   2. Overview of \"Songs of the Victorians\" \"Songs of the Victorians\" melds the archive and the scholarly article.  It examines both high- and low-brow Victorian settings of contemporaneous poetry by integrating scores, audio files, and scholarly analytical commentary in an interactive environment to help users understand both the literary and musical elements of the argument.  As an archive, it provides audio files of each song and archival-quality scans of first-edition printings of each score.  For every song, the user can listen to the audio while each measure of the score is highlighted in time with the music, as the archive page for William Balfe’s \"Come into the Garden Maud\" demonstrates (www.songsofthevictorians.com/balfe/archive.html).  The project also functions as a collection of  scholarly articles in which each song includes an analysis of the song’s interpretation of the text.  When the commentary discusses a particular measure, the users can click on an icon of a speaker, which will play the relevant excerpt of the audio file and highlight the score so they can hear for themselves the effect the commentary describes, as in the analysis page for Caroline Norton's \"Juanita\" (www.songsofthevictorians.com/norton/analysis.html).   Fig. 1: Musical excerpt from the analysis page of Caroline Norton’s “Juanita”  \"Songs of the Victorians\" includes Caroline Norton’s \"Juanita,\" Sir Arthur Sullivan’s setting of \"The Lost Chord,\" and two settings of Tennyson's Maud:  a parlor song by Michael William Balfe and an art song by Sir Arthur Somervell.  The site furthers scholarship for bibliographers, musicologists, Victorianists, and cultural studies scholars alike.  More generally, this new framework, which enables critics to describe musical arguments to non-musicians, facilitates this interdisciplinary approach of bringing music and literature together.  It also preserves the musical and cultural afterlives of well-known poems, as many of these scores have either disintegrated and been lost to time or are only available in select libraries.  \"Songs of the Victorians\" empowers users regardless of musical training:  those who cannot read music can overcome their feelings of intimidation at a musical score and can better understand the ideas described in the analysis, whereas those who can read music will still benefit, since few people can hear in their mind the music on the page.   3. Overview of \"Augmented Notes\" After the success of \"Songs of the Victorians,\" I used its framework to produce \"Augmented Notes\" (http://www.augmentednotes.com), a generalized, public humanities tool to allow anyone to develop similar websites.  \"Augmented Notes\" eliminates the need for users to understand programming by creating archive pages, like those from \"Songs of the Victorians,\" which users can tweak and redesign.  It is simple to use, as the site only requires audio files and images of the score to produce an archive page.  After the audio and image files are uploaded, users are taken to a page where they click and drag to draw boxes around each measure (they can also edit the sizes and order of these boxes), indicating what portion of the score should be highlighted when that measure plays.     Fig. 2: Box-drawing page of “Augmented Notes”  Users can also optionally upload an MEI file--the TEI-based scholarly standard  for music--for the score if they already have measure positions recorded in MEI. Users then set the times at which the highlighting box changes position through a \"time editing\" page.   Fig. 3: Time editing page of “Augmented Notes”  The site brings together the measure and time information, saving them in a JSON file, which enables each measure of the song to be highlighted in time with the music.  Users then click \"Download Zip\" to download a zip file with the HTML, CSS, and JavaScript files necessary for a complete archive page, which they can then restyle themselves.   \"Augmented Notes\" also has a sandbox (www.augmentednotes.com/example) through which users who would like to experiment with the technology but do not themselves have the requisite files can try it out.  \"Augmented Notes\" is already being used by scholars, both for archival purposes (such as the \"Performing Romantic Lyrics\" project from the University of South Carolina) and for pedagogical purposes such as generating interactive scores for use in music classrooms.  Since this tool produces websites with integrated audio and scores, it empowers users to preserve cultural archives, whether their materials include classical music, unpublished manuscripts, popular music, or folk music and traditional tunes from around the globe.   4. Implications This presentation will discuss the projects in greater detail, complete with live demonstrations and an explanation of the underlying technology to show their digital as well as scholarly innovations.  I will explain the rationale for my choice of poems, settings, sound files, and editions for \"Songs of the Victorians,\" as well as my plans for future collaboration and expansion for both projects.  The presentation will illustrate the sorts of arguments that this framework can enable: for example, my examination of Sir Arthur Sullivan’s setting of Adelaide Procter’s \"A Lost Chord\" challenges the received interpretation that the poem merely describes a domestic, uncomplicated religious moment of transcendence.  Likewise, Caroline Norton’s \"Juanita\" has been considered a conventional song that preserves the traditional rules of courtship and parlor propriety, but my analysis of the music helps us see that it critiques the Victorian institution of marriage as imprisoning.  I will conclude by exploring the ways in which \"Songs of the Victorians\" is itself a Victorian endeavor, as it uses new technology to collect, analyze, and bring together Victorian music and poetry, thereby giving voice to the silent page.   5. Funding This project was made possible by fellowships from NINES and the Scholars’ Lab.  ",
       "article_title":"Integrating Score and Sound:  “Augmented Notes” and the Advent of Interdisciplinary Publishing Frameworks",
       "authors":[
          {
             "given":"Joanna",
             "family":"Swafford",
             "affiliation":[
                {
                   "original_name":"University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "tool building",
          "English",
          "archive"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Goal of the project The European Union (EU) is a “union of law”: it is created by law; it enacts laws which confer rights upon EU citizens and impose duties; it acts in accordance with its law and under the legal review of the Court of Justice. The EU has, by its own, neither peoples nor territories. Its existence is entirely enclosed in its ability to bring closer its members states as well as the Europeans through the law. Thus, the knowledge (and the understanding) of the EU law is an essential part of an ongoing democratic process. In practice, pure and perfect knowledge of the law has always faced many difficulties. This is particularly true for the EU constitutional law. It is an economic and technical law faced with significant several changes in recent years and published in all official languages of the Union (24). The goal of the project is to create a tool for assisting the researchers in European Integration Studies in the analysis of the EU treaties. The system should allow the user to navigate through the treaties along with different axes of inquiry: access to a particular unit inside a treaty (part, section, chapter, article, etc.); multilingual alignment; modifications operated upon or by a treaty and the history of its different (consolidated) versions; status (entered into forced, repealed); possibility to add and retrieve user’s comments. The tool aims also to provide the EU citizens with the consolidated and the original versions of EU treaties enriched with additional materials (i.e. contextual resources, legal & economic doctrine, case law, …). The documents under study are part of the CVCE’s Lisbon research corpus, including founding treaties of the European Union and the treaties modifying them.  Although Web-based services allowing the navigation (EUR-Lex 1, LegiFrance 2, DOCLEG 3,the versioning (Progilex 4, MetaLex 5) and/or annotation (AT4AM for All 6) of legal documents already exists, there is no integrated solution addressing all the questions entailed by our research. So far, our experiments have been dealing with the identification of the documents structure and their relations, and the construction of small prototypes using XML-TEI as an encoding format. As the project is still in an exploratory phase, the paper will focus on the theoretical bases of the project, first experimental results and further development.   Overview of the process of creating/modifying European Integration Treaties The EU – formerly the European Communities – was established by the Rome Treaty concluded in March 1957. Since then, this treaty was modified more than 20 times, either in application of the general revision procedure or by simplified revision procedures. Every revision is enacted in the form of a legal act – be it a new treaty such as the Maastricht treaty (1992) or a secondary law like a Council decision. The act which introduces changes exists by itself, in addition to the act which is modified. However, nowhere the consolidated versions of the treaties and all their modifications are provided, even by Eur-Lex – the web service maintained by the Publications Office of the EU. This makes any analysis of EU legal texts highly tricky as any user looking for an updated version of a specific text has to compare its original version with all its subsequent revisions. Due to their complexity and their multi-linguistic nature, EU legal texts require also corrections which are legally binding. The rule relating to the allocation of seats in the European Parliament among the EU Member States since 1951 is illustrative of the complexity of the EU law. The modifications are laid down either in primary law or in secondary law; they insert, repeal, include either a whole Article or a part of it. The dates of adoption, of publication, of entry into force, of effective implementation vary from one modification to another. Some modifications were published but never entered into force applied, some other were changed before they become effective.   Fig. 1: Complexity of the EU treaties revision process: the example of the provisions on the allocation of seats in the European Parliament from 1951 to 2013.    Documents structure. Relationships Structure of the documents The text of a treaty can include the following elements:  preamble; main body of the treaty; annexes; protocols; declarations; final acts; corrections.  The corresponding TEI document may contain, apart from the metadata encoded in the TEI Header7, a text element 8with the following constituents: front(title, preamble); body (main body of the treaty); back (protocols, declarations, final act, corrections). Except from the title and the main body of the treaty, the other elements are optional. Imbrication between some of these components is possible. For instance, protocols or declarations can appear independently or be included in a final act. The main body of a treaty is structured in Fig. 2. Other components, like protocols or annexes, may have a similar configuration.    Fig. 2: Structure of the main body of a treaty  Smaller items can be further identified but the lowest unit considered in the study is the alinea (it corresponds to a paragraph in the non-legal writing) 9.   Relations The relations to be modeled operate either between treaties (Fig. 3) or at the fragment level, for example, an article from a treaty is modifying another article from a different treaty (Fig. 4). The relations between treaties (see also 10) are represented below and correspondingly in Fig.3.  amended_by / amends (oblique arrows pair); previous /next versions (other than linguistic) (horizontal arrows); linguistic_version (oblique arrow).    Fig. 3: Inter-treaties relations  The figure shows, along with the Modification and Time/Status axes, how the Treaty establishing European Economic Community (EEC, 1957) is amended by the Single European Act (SEA, 1986) and by the Treaty on the European Union (TEU, 1992), two subsequent consolidated versions being produced accordingly (EEC, 1986; EEC, 1992) (numeric codes are inspired by 11). The Language axis adds another dimension to the representation of the different linguistic versions of the treaties. Since the relations actually produce a multi-dimensional space, we can imagine the representation as functioning by parallel plans, rather than in a single three dimensional reference system. Moreover, the Time/Status axes are used to define three different timelines, for the creation/signature, entered into force and end of validity dates and places. The relations at the fragment level can be of the following types:  modified_by / modifies; cited_by / cites; previous / next.    Fig. 4: Relations at the fragment level  Fig. 4 illustrates the previous/next relations (horizontal arrows) between fragments (dispositions) from a version to the other and how a disposition from TEU (1992) repeals another from EEC (1986) (vertical arrows).    Experiments The experiments conducted so far dealt with encoding the structure of the treaties main body (Fig. 2) and the multilingual alignment. The production of XML-TEI documents involved a transformation chain represented in Fig. 5.   Fig. 5: XML-TEI transformation chain  First, the styled Microsoft Word documents, with styles corresponding to the structural components down to the level of Article (Fig. 2), were converted into XML-TEI (P5) using the OxGarage converter 12. The components were encoded using div elements. An XSL 13 file was created in order to enrich the encoding produced by the first conversion with attributes (@typeaccepting as values the components names, @xml:id, @n) for every div element. The transformation performed via Saxon 14also included procedures for the delimitation and identification of paragraphs and alineas (not marked by Microsoft Word styles). The resulted XML-TEI files were stored in an eXist-db 15database and HTML, CSS and XQuery 16 scenarios were added for visualization and queries. Fig. 6 shows the result of a search at the level of alinea and its multilingual alignment.   Fig. 6: Search by alinea. Multilingual alignment    Further work The complexity of the relations modeling involved in our study mainly resides, on one hand, in their temporal dimension (the modifications, the generation of new versions, their validity, and implicitly their relations, query and retrieval should operate in terms of time points and time frames) and, on the other hand, in the linguistic diversity of their expression (multilingual nature but also different ways of expressing, inside the same language, how a given fragment from a treaty modifies another). Our current inquiry is therefore related to the expression of the: types of amendment (e.g. repeal, insertion, substitution) 17; nature of the reference (document/fragment, internal/external, simple/multiple, contextual/non-contextual) 18; active/inactive time intervals 19. Other elements under study and experimentation are the relations encoding (TEI linking specifications 20; xLink, xPointer 21, 22), as well as the potential use of TEI extensions for legal texts 23. Aspects related to the balance between manual versus automatic processing, the corresponding workflow, and the maintenance strategies allowing the incorporation of new data or the integration of user’s intervention are also to be considered.  The presentation will focus on the theoretical bases of the project and the experimental results.   ",
       "article_title":"Building a multi-dimensional space for the analysis of  European Integration Treaties. An XML-TEI scenario",
       "authors":[
          {
             "given":"Florentina",
             "family":"Armaselu",
             "affiliation":[
                {
                   "original_name":"CVCE",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Allemand",
             "affiliation":[
                {
                   "original_name":"CVCE",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Language acts as the lynchpin of cohesion maintaining electronic conversations across social networking sites. Analysis of that cohesion facilitates the detection of linguistic patterns that initiate and compound hate speech in online environments. This research reports on analysis of hate speech in videos and asynchronous conversation using Systemic Functional Linguistics (SFL) within social networking site. Focussing on the architecture of language and the influence of social context, SFL facilitates the analysis of language in its temporal and contextual use.1 In applying SFL to the chosen corpus of texts the research team are building reference dictionaries of offensive words and reference catalogues for the clausal structures in which these words are used. The team is exploring the detection of and analysis of hate speech across conversation, that is across texts within social networking sites (SNS). In accumulating data that allows the expansion of dictionaries and clausal catalogues, the team is enabling the building of an automated alert system that scans texts as they develop independently and in their engagement with other texts across time. Overall, this paper outlines the application of SFL to texts accrued from SNS that exhibit aspects of hate speech associated with dehumanisation, details the analysis of visualisations of hate speech within developing texts and demonstrates the building of an automated alert system using SFL to detect hate speech across texts. 1.1 Overview of Context Raphael Almagor-Cohen describes hate speech as speech that intends to “injure, dehumanise, harass, intimidate, debase, degrade and victimize the targeted groups and to ferment insensitivity against them.”2 Speech acts have the capacity to carry out and compound the dehumanisation of people rendering them powerless within the confines of \"fields of recognition\" demarcated by the limits of language.3 Mining data retrieved from Youtube posts that dehumanise subjects provides the opportunity to analyse the overall ecology of texts as they develop online. The detection and analysis of latent dehumanisation is made possible by the application of SFL to this data thus empowering us to delineate the \"fields of recognition\"  and the reinforcement of that field through subtle and explicit modes of language use.  1.2. Methodology The corpus of texts used in this research are all drawn from Youtube and consist of the recording of a repeatable event in which  the videos' subjects, drug users, are subjected to dehumanising language and to an ongoing process of dehumanisation. The corpus consists of 20 videos and the associated metadata posted by unknown users of Youtube. Each video is a recording of a drug user in a public space in Ireland’s capital city, Dublin. A second set of videos of similar content recorded in Glasgow, Scotland is being used by the team for comparative purposes. The team have temporarily captured the videos and compiled transcripts of the audio and of the comments posted below. Both of these transcripts and the video are treated as objects that facilitate a users’ engagement with other users and with the different elements of the composite text. That is users may engage with the videos, with the asynchronous conversation that has grown in relation to the video or with media objects posted in relation to or response to these videos. These media objects consist of mashups, memes, links to other videos and to other websites. Linguistic patterns within each type of engagement act as the creators of cohesion within the text’s overall development. The team have focused on lexical cohesion as a way of analysing how items relate to each other and build the texture of the text.4 Lexical cohesion creates the threads through which language choice manipulates the \"finite nature of language as a semiotic system.5 We argue that is lexical cohesion that binds nodes of conversation with other media objects and which facilitate the development of the relationship between the composite elements of the text.  By compiling dictionaries of words associated with the dehumanising aspects of hate speech the team is building a framework enabling the initial identification  of  dehumanisation that provides opportunities for the analysis of the \"textual processes of social life\".6 Already marginalised, the recorded drug user is drawn into the connected city as a figure of disruption who is marginalised even further by the language choices evident in the analysed transcripts. The pounding vocabulary of the audio comment adds to their marginalisation as  across each video they are described in dehumanising language. This language becomes part of the rearticulation and recirculation of hate speech. SFL as a system considers language as part of a process of instanstiation.7 That is it builds and develops texts brick by brick and interaction by interaction. In this respect, the text is considered a \"complete linguistic interaction\" that builds continuously.8 Here  we use SFL to create markers for models that will be part of the automated system that detects how hate speech builds across composite texts, between elements of the texts and across the relationships established between particular posters. The dictionary of offensive words that dehumanise the subjects of our corpus are drawn from the encoded transcripts of both the audio and the textual comments.  Markers defining lexical cohesion facilitate the exploration of the users language as it engages with the overall field of the text, with the ideational expression of the text and the text's tenor. That is markers of lexical cohesion that bind immediate nodes of conversation with each other facilitate the immediate compounding of hate speech. It also enables challenges to posts promoting dehumanising hate speech. These challenges are acheived through the fractures in language  induced through interruptions in lexical cohesion.  Lexical cohesion draws on the temporality of the environment in which language  is used to bind conversation in immediate response structures and across more developed response structures. Interruptions to lexical cohesion can also be use to introduce new ideational expressions that counteract the binds that previously placed limits on the text's field. Further to this, the application of sentiment analysis (SA) along with SFL methods enables the visualisation of patterns of hate speech as a text develops and as patterns gather accumulative power. Thus visualisations of hate speech in a state of emergence empower moderators to detect less explicit hate speech that may otherwise go undetected. On going analysis of ‘emergent visualisations’ provides further opportunities to examine the participant’s use of language choice in conjunction with grammatical structures to bring cohesion to a text or to counteract the cohesion created through another user’s language choices and invocation of grammatical structures.  In examining the emergent visualisations the research team draws on concepts of lexical cohesion and the clausal structures of sentences to detect linguistic patterns. 2. Demo and Results By using the concepts surrounding lexical cohesion in conjunction with the dictionary of dehumanising words the team have identified, we are able to use our custom built programme to capture and analyse conversations that have developed as part of the \"complete linguistic interaction\" surrounding each media object in our corpus. An example of a captured conversation is shown below in Figure 1. The media object \"Dublin Junkies\" is shown at the centre of the visualisation. Each node of conversation representing a cluster of chat is represented in yellow. Our custom built programme allows us to identify whether these nodes are anaphoric or exophoric conversations. That is we can identify whether the nodes are related directly to the media object or not. Anaphoric conversations, those directly related to the video \"Dublin Junkies\", are demarcated by the colour blue in the second image, Figure 2. Exophoric nodes, those that do not relate to the video directly, are demarcated by the colour blue. In this second image the lines joining these nodes represent the result of our SA tool. Lines joining nodes that are red denote negative sentiment and those in green denote positive sentiment. The thicker the line the more negative or positive the sentiment.    Fig. 1: Visualising the ecology of online conversation    Fig. 2: Demarcating elements and relationships within developing texts  3. Conclusions Analysis of 'emerging visualisations' points to the strong negative sentiment between exophoric nodes of conversation shown in Figure 3. The yellow lines in this figure delineate the reorganisation of the text according to lexical cohesion. This reorganisation demonstrates the capacity of both SFL and our programme to make linkages across texts as they undergo a process of instantiation.   Fig. 3: Lexical Cohesion and Sentiment Analysis  Theorist Judith Butler argues that to be called a name is “to be initiated into a temporal life of language that exceeds the prior purposes that animate that call”.9  Dehumanising language calls subjects into a disempowered temporality rendered increasingly damaging by the capacity for rearticulation and reproduction facilitated by online environments. The use of SFL to analyse the construction of temporal fields that may be sealed by language and grammatical choices  compounding dehumanisation empowers moderators to detect hate speech in an online through longitudinal analysis. The updating of visualisations also allows moderators to identify  the reach of particular posters across texts enabling a vertical and an horizontal analysis of the development of hate speech online.   ",
       "article_title":"Fractures and Cohesion: Using Systemic Functional Linguistics to Detect and Analyse Hate Speech in an Online Environment",
       "authors":[
          {
             "given":"Deirdre",
             "family":"Quinn",
             "affiliation":[
                {
                   "original_name":"An Foras Feasa, NUIM",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Keith ",
             "family":"Maycock",
             "affiliation":[
                {
                   "original_name":"School of Computing, National College of Ireland",
                   "normalized_name":"National College of Ireland",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02qzs9336",
                      "GRID":"grid.462662.2"
                   }
                }
             ]
          },
          {
             "given":"John ",
             "family":"Keating",
             "affiliation":[
                {
                   "original_name":"An Foras Feasa, NUIM",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Following the lead offered in the \"Text Encoding Meets Text Analysis\" panel session at DH2012 in Hamburg,1 and also in the 2013 debate between Matthew Jockers and Julia Flanders on \"A Matter of Scale\",2 the research value of basic modeling for text analysis seems comparatively clear, even if it is as yet unrealized in practice. However, the payoff from the more complex forms of modeling that are evident in typical TEI-encoded thematic research collections has been less clearly demonstrated. The usefulness of such modeling is evident insofar as it supports the publication of these collections: information about document structures is used to produce display formatting and navigation, and information about content features such as named entities and genre is commonly used in searching. But the larger claims made for digital resources—that they \"allow new questions to be asked\"3—require us to distinguish between these kinds of functional value and what we might call \"real research value.\" The real research value of the modeling is reflected in the ability of the data to support complex inferences, at scale, that materially contribute to humanities research arguments. Huitfeldt et al. in \"Meaning and Interpretation of Markup\"4 offers a foundational demonstration of how markup \"licenses inferences\" in formal terms. But projects developing TEI-encoded archival collections have not yet articulated in detail how that markup might lead researchers from comparatively simple inferences (\"this paragraph contains a reference to such-and-such a person\") to more complex ones (\"if the name of a person classified in our personography as a war hero appears inside an advertisement within a magazine published after the date of the war in question, the advertiser may be using that person's identity to promote the goods being advertised\"; \"people whose names appear in both advertisements and poetic dedications command greater social capital than those whose names appear in either genre alone\"). And the conceptual leap from complex statements of this kind to larger conclusions is greater still. This paper seeks to explore how the modeling of textual data for humanities research connects to the high-level research questions humanities scholars address in their scholarly writing. It offers a detailed description of the modeling (transcription, text encoding, metadata) and the high-level research goals for two closely related digital collections: the Early Caribbean Digital Archive and the Women Writers Project. It then traces critically the inferential steps by which we seek to get from the data being captured to the theoretical concepts (\"culture\", \"geography\", \"influence\", etc.) that animate the research. The goal of the paper is to provide a much more exacting and thorough understanding of the complexity of data modeling required to support the argumentative nuance and conceptual subtlety of real-world, high-quality humanities research. In particular, our focus is on the possibilities and challenges of knowledge production afforded by modeling and encoding archives of materials that concern marginalized persons and non-canonical texts and histories. The two projects under discussion here are engaged in bringing to visibility texts and narratives that had previously been submerged beneath (or concealed within) more culturally promiment discursive forms. The Women Writers Project is currently engaged in a grant-funded collaborative research project funded by the National Endowment for the Humanities titled \"Cultures of Reception\" (http://www.wwp.brown.edu/research/projects/reception/) which gathers and digitizes periodical reviews of late 18th- and early 19th-century women's writing, to support the study of patterns of reception in an emerging transatlantic literary culture. The Early Caribbean Digital Archive (http://www.northeastern.edu/nulab/the-early-caribbean-digital-archive/) is digitizing a variety of Caribbean textual sources from the same period, with special emphasis on the study of the emerging culture of commodity circulation and its relation to the transatlantic slave trade and submerged narratives of race and gender. Both projects involve detailed TEI encoding of textual sources animated by research goals such as these:  trace and map the relations between texts as a function of time, human agency, and geography bring into visibility relations between locations of print activity across the Caribbean archipelago show relations among individuals, such as printers, consumers, merchants, runaway slaves, missionaries, plantation owners, abolitionists, military figures, and colonial political figures map relations between legislation, commodity prices, geography map the geographic circulation of literary tropes trace changes in the culture of reviewing over time with respect to the emergence of a transatlantic literary culture bring to visibility the evaluative frames of reference within which women's writing is read trace the cultural frame of reference for reviewers in England and in North America  In both cases, the projects must make conceptual and inferential bridges between the specific assertions constituted in the markup (observations about genre, named entities, time and location, textual structure, references to circulating cultural objects such as commodities and texts) and concepts operating a much more abstract level: \"culture\", \"geography\", \"influence\", \"relations\", \"frames of reference.\" Humanities scholars are comfortable using terms like these in their writing, as the currency of methodologies from cultural studies to cultural geography indicates, but what forms of evidence and inferential reasoning do they entail at the level of textual markup? We can unpack here, in a preliminary way, the kinds of reasoning through which these bridges might be built, and the final paper will explore these in more detail. First, there is a set of direct modeling activities (transcription and markup) through which the texts are constituted as research evidence. The activities of transcription produce from the source document assertions about the existence of strings of characters, and markup allows the transcriber and editor to identify areas where this evidence is ambiguous or missing. Through markup the editors can also identify specific strings as references to certain types of named entities (persons, places, books, publishing houses, ships, shipping companies, legislative bodies, etc.) and can associate these references with their target to provide unambiguous entity identification via linked data authority records. At a structural level, the markup can also be used to identify the genre and format of texts and parts of texts, and to associate metadata (author, date and place of production, etc.) with these. Following these activities, we must venture to make inferences based on our modeling. For example, the markup allows us to give greater precision to inferences common in text analysis: instead of judging collocation based on raw word proximity, we can identify word pairs or groups as being within the same textual feature (paragraph, poem, letter, advertisement, heading). In specific cases, we may be able to infer something more from such collocation, such as a connection between two authors mentioned in the same paragraph of a review. Genre and format information may enable us to sharpen these inferences further: mentioning an author in a review means something different from mentioning an author in a dedication; a commodity carries a different cultural freight when listed in an advertisement, a bill of lading, a receipt, a letter, a legislative document. The name of an enslaved person means something different when it appears in a bill of sale, a runaway slave notice, a poem. Taking metadata into account, we can also localize documents in space and time, which gives us the possibility of (cautiously) identifying trends, causation, cultural significance. In building these bridges, we need to be attentive to the gaps or weak inferential points as we move from the modeling to the research. For example, what does it mean to infer relationships between entities from their proximity within documents? Where are these inferences strongly grounded (for instance, co-authorship reflected in metadata records) and where are they weak (for instance, two names appearing in the same paragraph of a historical account)? Or, in another vein, is the model of geography that emerges from textual attestation (i.e. the inventory of place names and location references) adequate for the kinds of geographical analysis we want to do? Or again, what does \"circulation\" (whether of physical objects or of ideas) look like as attested in data of this kind and how do we discover it? Finally, what is the relation between the encoding categories we have identified here and our knowledge production with respect to marginalized texts, persons, and narratives? How will our modelling decisions erase or repeat historical occlusions in the archive, not only by determining what aspects of the text are marked but also by imposing existing frames of knowledge on the archive?  ",
       "article_title":"From Markup to Analysis: Culture Claims and Code in the Digital Archive",
       "authors":[
          {
             "given":"Julia",
             "family":"Flanders",
             "affiliation":[
                {
                   "original_name":"Northeastern University",
                   "normalized_name":"Universidad del Noreste",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/02ahky613",
                      "GRID":"grid.441462.1"
                   }
                }
             ]
          },
          {
             "given":"Elizabeth Maddock",
             "family":"Dillon",
             "affiliation":[
                {
                   "original_name":"Northeastern University",
                   "normalized_name":"Universidad del Noreste",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/02ahky613",
                      "GRID":"grid.441462.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction: A digital tombstone archive 1.1. Overview: Scope and motivation Since 2007 we work on the construction and exploitation of a digital tombstone archive, “ThakBong”, an archive which mainly contains tombstones of Taiwan, but includes also tombstones of China and tombstones of Chinese migrants in Asia, Europe and the USA. So far, on 850 visits to 500 graveyards, about 170.000 photos of 42.000 tombs have been taken. Repeated visits to graveyards allows us to document regionally different temporal patterns of ancestor veneration and the life-cycle of a tombs, including the burial, a second burial, a tomb renovation and the removal of the tomb. In fact, graveyards in Taiwan continue to disappear from the geographic and cultural landscape through development projects, natural catastrophes and the transformation of graveyards to bone-ash-towers. About half of all graveyards have already been lost or will be removed according to governmental projects in the years to come. As graveyards continue to be taboo for Taiwanese researchers, the records of graveyards we produce are among the only available.   Fig. 1: A graveyard in Southern Taiwan  Using geo-referenced photos as primary data, tombstones, tombs and graveyards are digitally reconstructed, linking back and forth between images and data. Images and data are constantly updated and made available to the scientific community via the research data archive DANS . They can be used in a wide range of research approaches, including corpus linguistics, social history, anthropology and human geography. The wealth of the data, and the interleaving levels of language, culture, geography and history however require the elaboration of more integrative and cross-disciplinary approaches. Beyond the topics in relation to Taiwan, the data permit empirical tests for theories of colonization, globalization, cultural contagion , and social struggle through cultural practices . The approach that we take to our data is that of a Digital Anthropology that combines sociological theories with the transformation of cultural practices . 1.2. Scope of this Presentationy In this presentation we will summarize the main aspects of our digitization work, starting with the sampling and finishing with some theoretical insights we gained through our work. The purpose of this presentation is two-fold. First, we want to provide those who intend to launch a similar documentation and research project of  their local graveyards with a short project description, which they might follow and, if needed, modify. Second, we want researcher who consider to use the \"ThakBong\" archive for their future studies to be able to evaluate the scope, coarseness and reliability of the data. 2. Methodology 2.1. Sampling The purpose of the sampling is to produce an adequate representation of the reality in all dimensions we can think of, to avoid many of the unmotivated generalization we find in more traditional research. We are especially eager to capture the voices of poor and uneducated people, as they are usually overheard in linguistic and historic accounts. We therefor sample, without distinction, old and new tombs, king-size national monuments and most elementary tombs. In addition, we try to cover all ethnic groups, all religious orientations and all administrative divisions. For practical reasons, however, we cannot achieve a truly balanced sampling, because urban regions and catastrophe-prone areas have already lost most of their historic graveyards.  2.2. Fieldwork The central tools for the fieldwork are digital GPS-cameras, costing about 250€, which store in the EXIF-header the position, altitude and orientation, along with more common metadata. For geographic analyses and the mapping of tombs, graveyards and cultural practices these data are indispensable.  To optimize the automatic use of these data, photos are taken in a regulated way, using two circles with two defined centers as a reference for the photos: All photos from outside are all taken into the direction of the tombstone, allowing the orientation of the tomb and the orientation of the shots to be calculated from the direction of one central camera shot. Photos in the area of the mourners, in front of the tombstone, are taken from the center of this space, allowing to calculate the location of the components of the tomb around the mourner from the orientation of the tomb. Also, through this model, photos are linked in a systematic way, making it possible to browse and virtually explore the tombs.   Fig. 2: The model regulating how photos are taken, so that automatic processing and browsing of the tomb become possible.  2.3. Processing of Primary Data For the processing and multi-user annotation we use the postgreSQL database the postGIS GIS extension, storing transcriptions in XML. The database tables represent digital objects as bundles of features. A feature is a unique combination of an attribute, e.g. 'direction', a value, e.g. '180' and if necessary a unit, e.g. 'degree'. These objects are called \"graveyard\", \"tomb\", \"tombstone\", \"transcription\" and \"person\", representing the corresponding real-world objects. The object \"person\", for example, contains the features 'surname', 'given name', 'religious name', 'date of birth', 'date of death', 'date of burial', 'ethnicity', 'gender' and 'role', e.g. 'mourner' or 'deceased'. An average tomb has about 50 defined features, but numbers might be much higher for family tombs. In a first processing step, these objects are created manually through a web-interface that segments the stream of photos. Images that show an object are linked to that object. Images showing inscriptions, offerings, symbols and figurative representations are specifically tagged, so that images can be further filtered, to facilitate the annotation process. More than 10.000 lines of program code in plpqsql implement PostgreSQL triggers, which help to reduce the manual annotation labor. They can be divided into five groups:  Rules that extract and processes data from the image. From these the position, altitude and slope are calculated.  Rules that fill in logically implied values. E.g. if no image of a tombs is tagged for 'offering', the tomb is marked as offerings='no'.  Rules for a model-based annotation. Models are used for non-visible features, such as the 'ethnicity' of the deceased, if not known otherwise. These statistical models maximize the number of correct annotations over the whole data set, allowing at the same time manual corrections of individual tombs to be respected and calculated into the model. Using external statistical data, for example, on the relation of surnames, administrative regions and ethnicity  , the most likely ethnicity for the deceased can be calculated, given the region and the surname on the tombstone.  Where no external data resources are available, e.g. for the prediction of the 'gender' from the 'given name', we use bootstrapping: Using statistical data that have been produced in the manual annotation of unambiguous cases, ambiguous cases are automatically annotated when the memory-based model makes clear predictions for a given name. For all features, their epistemic status are retained: Model-derived data are updated when manually set data change.   2.4. Annotation and Transcription After experiments with OCR on tombstone images brought no results, tombstones are transcribed manually, phrase by phrase. Until now 24.000 tombstones have been completely, about 10.000 partially transcribed. Transcribed phrases are classified semi-automatically into 'semantic roles', such as 'place', 'person', 'date of birth' etc. Then, example-based taggers extract relevant data, such as dates, names and family relations and fill in the relevant features. Where automatic processes do not yield a clear classification, the system shifts to an interactive mode. All other features, e.g. the color and form of the tombstone are annotated manually until we will have completed the extraction of these features from other photos, using an example-based approach and a similarity metrics of photos. For the entire project, 6 man-years have been invested in 7 years, showing that with the right balance of automatic processing and manual annotation huge data can be created.   2.5. Analysis and Theory Formation The most striking fact about the data is the enormous variation one finds through time and space. This variation contrasts sharply with the literature on funerary traditions in Taiwan, and, second, with statements that informants produce when explaining their traditions. The data thus not only question the foundations of established research on funerary rites, but also on research that uses informants as a source of information. In fact, the relations between publications, informants’ opinions and a national ideology become palpable to such an extent that a scientific approach has to look into the involvement of ideologies in the transformation of cultural practices. The DH-approach is quite suitable for this endeavor: Digitizing scientific and political publications that stand in relation to funerary rites, we could contrast publications with the reality they pretend to describe and reveal their influences on social practices. More particularly, we could show how a governmental publication influenced the way that Taiwanese refer to their ancestral home through tombstone inscriptions .    Fig. 3: An example for the variation through time and space. The onset of the place-name type 'tanghao' (blue) between 1900 and 2000. Adjacent regions may show similar or very different patterns in the development. B refers to the correlation to the Baijaixing, a century-old book which specifies which surname matches which tanghao. High correlations identify the place-name as a literary reference.   ",
       "article_title":"Creating a Digital Tombstone Archive:  From Fieldwork to Theory Formation",
       "authors":[
          {
             "given":"Oliver",
             "family":"Streiter",
             "affiliation":[
                {
                   "original_name":"National University of Kaohsiung, Taiwan",
                   "normalized_name":"National University of Kaohsiung",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/013zjb662",
                      "GRID":"grid.412111.6"
                   }
                }
             ]
          },
          {
             "given":"Yoann",
             "family":"Goudin",
             "affiliation":[
                {
                   "original_name":"Institut National des Langues et Civilisations Orientales (INALCO), Paris, France",
                   "normalized_name":"Institut National des Langues et Civilisations Orientales",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/023zg8w32",
                      "GRID":"grid.434827.e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "creating a digital resource",
          "Taiwan studies",
          "digitization of cultural heritage"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Motivation Historical knowledge is fundamentally uncertain. A given account of an historical event is typically based on a series of sources and on sequences of interpretation and reasoning based on these sources. Generally, the product of this historical research takes the form of a synthesis, like a narrative or a map, but does not give a precise account of the intellectual process that led to this result.  Our project consists of developing a methodology, based on semantic web technologies, to encode historical knowledge, while documenting, in detail, the intellectual sequences linking the historical sources with a given encoding, also know as paradata1. More generally, the aim of this methodology is to build systems capable of representing multiple historical realities, as they are used to document the underlying processes in the construction of possible knowledge spaces. Overview of the Approach Semantic web technologies, with formal languages like RDF and OWL, offer relevant solutions for deploying sustainable, large-scale and collaborative historical databases (see for instance 2). Compared to traditional relational databases, these technologies offer more flexibility and scalability, avoiding the painful problems of large schema migration. They are grounded in logic and thus permit us to easily conduct semantic inferences.  Some very stable semantic based ontologies like CIDOC-CRM, now an ISO standard, have been used successfully in the cultural heritage domain for about 20 years 3.  However, the languages used in the semantic web technologies have a major limitation that prevents their usage for encoding metahistorical information. Expressed knowledge is typically formalised with RDF triplets which are not objects in the same order as the knowledge content (RDF resources identified with URIs) to which they link. For example, it is difficult to document the source, the author or the uncertainty of given RDF statement.  One way to compensate for this flaw, while respecting the W3C norms, consists of transforming each RDF triplet (subject predicate object) into three triplets (statement rdf:subject subject), (statement rdf:predicate predicate), (statement rdf:object object). Using this approach, it becomes possible to add new triplets with a given statement as subject, documenting additional paradata about this statement. The resulting knowledge base can include metahistorical information, i.e. information about historical information creation processes. This metainformation can document the choice of sources, transcription phases, coding strategies, interpretation methods and whether these steps are realised by humans or machines. Thus, each historical database designed following this methodology integrates two levels of knowledge. The first level provides the documentation about the origin, the nature and the formalisation used to encode historical data, while the second level codes for the historical data itself.  The Knowledge Construction Vocabulary (KCV) We are working on a specific RDF vocabulary, called Knowledge Construction Vocabulary (KCV), which will enable us to implement the two level organisation using the standards of the semantic web. KCV RDF statements represent knowledge construction steps, while effective historical knowledge is only expressed through reified triplets. An important concept in this vocabulary is the notion of knowledge spaces. A knowledge space designates a closed set of coherent knowledge, typically based on a defined set of sources and methods. Examples of knowledge spaces include documentary spaces (e.g. a defined corpus of sources) and fictional spaces (e.g a coherent world typically described in a book).   Figure 1 shows an example of the kind of graphs that can be built using the KCV vocabulary.  In this example, two knowledge spaces have been defined: one documentary space (DHLABDocuments) and one so-called fictional space (HistoireVenise_S1). Each of these two spaces is defined as a unique resource with an associated URI.  A statement (Statement1) stands for a reified triplet defining that (HistoireVenise) is a kind of Book and is linked to the documentary space. The KCV vocabulary allows us to document who entered the information (fournier) and the creation time of the statement (May 06th). To formalise the fact that the book, HistoireVenise, is used as a knowledge source, a specific resources HistoireVenise_KS is created and linked with the HistoireVenise, the book, and the general document space DHLABDocuments.    Fig. 1: A \"toy\" example of the use of the KCV vocabulary to code historical and metahistorical information  In the fictional space HistoireVenise_S1, a statement (Statement2) codes for a reified triplet indicating that the reconstruction of the Rialto bridge occurred during the period of 1588-1591. Information about the author, the creation date and the reliability of Statement2 are documented using various KCV triplets. The link between the document space and the fictional space is encoded by a link between the knowledge source HistoireVenise_KS and a statement, Statement2_origin, linked to Statement2 of type interpretedtextknowledge.  We can make three remarks:  This is obviously a \"toy\" example (real graphs encoding historical data are typically much bigger), but it illustrates how historical and metahistorical information can be coded with a linked data approach. This allows us to envision queries mixing both historical and metahistorical requests, for instance reconstructing an historical context based only on certain kinds of sources or excluding information that was provided to the database by some authors. The kind of intellectual processes documented by KCV can easily include algorithmic steps like digitisation, optical character recognition pipelines on documents,  text mining, semantic disambiguation, etc. The version and the author of the algorithms used can easily be included using KCV statements. This kind of documentation permits us to exclude historical information linked with some processing using early versions of algorithms that may have \"polluted\" the data. This is an important prerequisite for building sustainable databases in the long term.  Documenting metahistorical information using KCV may look like a tedious process; however, in most cases, this information can be inserted automatically using a higher-level interface. A database interface in which the user is logged permits to easily produce historical data based on the KCV vocabulary, taking the form of reified RDF triplets, while documenting the author, the data and the methods used.   Ontologies Matching The KCV approach for encoding historical databases is also interesting from the perspective of ontologies alignment: a notoriously difficult issue 4. Each research group tended to code historical data using their own local ontologies, adapted to their research approach. The metahistorical documentation provided by the KCV vocabulary enables us to envision strategies for mapping such ontologies to a pivot ontology. Figure 2 shows this general process in which several knowledge spaces are linked. Each group locally describes the source documents used (1), transcribes their content (2) and eventually codes/interprets this content (3). Throughout this process, two groups produced two independent custom ontologies (A and B). The alignment process proceed in two additional steps. First, both local ontologies are mapped onto a general content ontology (4) (for instance CIDOC-CRM, but not necessarily) and then, once expressed in this common conceptual model, the information contained in the graph is aligned and the content is merged (5).    Fig. 2: The general process of ontologies matching  Figure 3 gives a more detailed account of the final step. First knowledge sources are mapped, then types are mapped and eventually predicates are mapped. In some cases, only a partial level of correspondence can be reached. These steps can be done manually or automatically and are, of course, subject to errors. It is therefore crucial to document the authors of these matching steps, whether they are humans or algorithms. This is why the authors are, linked all the other steps, described in the KCV vocabulary.    Fig. 3: Detail of the ontologies matching process  Conclusion The approach briefly presented in this paper enables us to encode historical and metahistorical data in a unified framework. The method we describe is fully compliant with the current technologies and standards of the semantic web (RDF, SPARQL, etc.). It does impose a unified historical terminology but can also be used in conjunction with existing standards. For instance CIDOC-CRM can be used to describe historical knowledge extracted from archival documents (e.g events, people, places) using RDF triplets and KCV can be used to code information about the CIDOC-CRM triplets themselves, such as documenting who entered a particular triplet. The originality of our proposal comes from the introduction of the this second level (metahistorical) on top of the existing RDF ontologies.  This does not necessarily impose an additional burden on the person encoding the historical data. Using a dedicated web interface, the metahistorical information can be added automatically as the data is progressively entered.  Coding metahistorical information by making explicit the many underlying modelling processes allows us to prepare for possible ontology evolution and enables easier ontology matching. More importantly, our approach does not impose the search for a global truth (a unique and common version of historical events) but pushes towards the explication of the intellectual and technical processes involved in historical research, thus giving the possibility of fully documented historical reconstructions.   ",
       "article_title":"Encoding Metaknowledge for Historical Databases",
       "authors":[
          {
             "given":"Marc-Antoine",
             "family":"Nuessli",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Reifed RDF"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This paper reports on interdisciplinary work carried out as part of Trading Consequences 1, a two-year Digging into Data project 2.  The focus of the project is to mine large quantities of historical documents, extract information on commodity trading in the nineteenth century British World and visualise the mined output in dynamic and interesting ways, thereby bringing archives alive in ways that authors of original documents would have never imagined. The Trading Consequences interface is aimed at historians studying commodities and their environmental consequences. Their studies have tended to focus on a manageable number of commodities (e.g. William Cronon's research on beef, lumber and wheat 3).  The Trading Consequences Project aims at identifying global trends in commodity trading for many different natural resources, raw materials or lightly processed goods by correlating information extracted for one commodity with that of others or showing all commodities relevant to particular locations and dates. In this paper, we first present an overview of this collaborative project that involved environmental historians, text mining, database experts and visualization researchers. We then report on lessons learned from a workshop where we collected feedback from historians and geographers after they interacted with the interface prototype in a series of exercises. This feedback informed the further adaptation of the underlying technologies for historical research.   2. Trading Consequences The Trading Consequences system encompasses three main technical components: a text mining system, a database and a web-based user interface with dynamic visualisations (Fig. 1). The data analysed using this system is comprised up of several nineteenth century British and Canadian text collections 4. These sources amount to over 11 million pages and over 7 billion analysed word tokens.   Fig. 1: System architecture.  2.1 Text Mining The text mining (TM) tools are developed by the Language Technology Group at the University of Edinburgh. We adapted an existing pipeline built on LT-XML2 and LT-TTT2 to process historic text 5. The TM component is made up of a series of linguistic processing steps which build up the linguistic properties of the language in a given text.  A pre-processing stage includes tokenisation, sentence-splitting, part-of-speech tagging and lemmatisation to determine words and sentences, identify their syntax and compute canonical forms of word tokens. All of this information aids down-stream TM processes.  The next steps are named entity recognition and grounding. This means that mentions of locations, commodities and dates are automatically identified in the text and grounded to unique identifiers in existing knowledge databases.  We ground location mentions to GeoNames identifiers and their corresponding latitude/longitude values 6. We use an adapted version of the Edinburgh Geoparser for this geo-referencing process 7.  Commodity mentions are grounded to DBpedia 8 concepts in a semi-automatically constructed commodity lexicon developed in this project 9. Finally, date mentions are grounded to year, month and date attributes.  The last TM step identifies relations between commodity, date and location mentions to identify the relevance of commodities in space and time. The extracted and enriched TM output is stored in a relational PostGreSQL database set up and hosted by EDINA 10 for subsequent querying and visualisation. 2.2 Interactive Visualisations The strength of information visualisation is to make abstract concepts and relations within data visible and explorable 11. In the context of Trading Consequences we aim at providing visualisations of the mined data to:   - enable open-ended explorations of the document corpus beyond target search 12, i.e., supporting visual querying along spatial, temporal, and conceptual dimensions, and  - highlighting trends within a range of document data, for instance, relations between different commodity types.  While the first approach facilitates the discovery of related documents in ways that common text-based search interfaces cannot, the second approach can lead to new insights or research questions based on collection sizes that exceed possibilities of traditional research methods in the humanities. Our visualizations are web-based to make them easily accessible worldwide (see [1]). The implementation is based on JavaScript (D3.js and jQuery) and PHP. In this paper we briefly describe the Trading Consequences visualisation tool and how it was experienced by environmental historians as part of a workshop. Inspired by 13, our visualisation consists of three interlinked representations (Fig. 2): a map showing the geographic context in which commodities were mentioned, a vertical tag cloud showing the 50 most frequently mentioned commodities, and a bar chart representing the temporal distribution of documents within the collection. A ranked document list provides direct access to the relevant articles.   Fig. 2: Interlinked visualisations provide an overview of the document collection.  Interaction with one visualisation acts as a filtering mechanism and adjusts the data shown in the other visualisations. For instance, zooming into the map adjusts the tag cloud to only include commodities mentioned in relation to visible locations; the bar chart only shows documents that include these commodity/location mentions (Fig. 3). Particular time frames can be selected to further filter the document corpus; the other visualisations are updated accordingly (Fig. 4).   Fig. 3: Specifying the location adjusts the other visualisations.    Fig. 4: Specifying a time frame adjusts commodities and locations shown in the tag cloud and map.  Lastly, historians can specify commodities of interest, either by textual query or by selecting commodities from the tag cloud. All visualisations adjust, with the tag cloud showing commodities related to the selected ones. An additional line chart presents the frequency of mentions of selected commodities across time (Fig. 5).   Fig. 5: Specifying particular commodities of interest further adjusts the visualisation.   3. Feedback from Historians  To gain expert feedback on our approach of combining text mining with visualisations to facilitate research in environmental history, we conducted a half-day workshop where we introduced our visualisation prototype to historians. The workshop was held at the Canadian History & Environment Summer School 2013 with over 20 environmental historians participating 14. At the workshop, we asked historians to explore the visualisation tool in small groups (Fig. 6). To promote engagement with the different visualisations and to fuel discussions, the explorations were guided by a number of open-ended tasks, such as querying for commodities of interest or focusing on a geographic area.   Some historians immediately started to focus on the Vancouver Island area where the workshop took place. Others experimented with commodities and locations related to their own research. In general, these first exploration periods were about verifying familiar facts to assess the capabilities of the visualisation and the trustworthiness of the underlying data. The historians quickly understood the general purpose and high-level functionality of the visualisations and were able to start their explorations immediately. There was some confusion, however, about lower level details. For instance, the meaning of the size and number of clusters in the map was unclear (e.g. do they represent number of documents, or number of commodity mentions?). Observing changes in the visualisations while adjusting parameters helped, but our observations highlight that clear labelling and tooltips are crucial for visualisations in the context of digital humanities, not only because these are a novel addition to traditional research methodologies, but also because they can be easily misinterpreted. The meaning of visual representations needs to be clear in order to make visualisations a valid research tool.   Fig. 6: User workshop at CHESS 2013.  Workshop participants found the meta-level overviews of the visualisations valuable as these can aggregate information about the document corpus beyond human capacity. In the short time of the workshop, historians made (sometimes surprising) discoveries that sparked their interest to conduct further research. While it is unclear if these discoveries withstand more detailed investigation (there is still some noise in the data), this shows that visualisation has the potential to support exploration and insight in the context of history research. A large part of the discussions focussed on what kind of insights can be gathered from the visualisations. Some historians pointed out that the visualisations represent the rhetoric around commodity trading in the 19th century: they show where and when a dialogue about particular commodities took place, rather than providing information about the occurrence of commodities in certain locations. This raises the question of how we can clarify what kind of data the visualisations are based on to avoid misinterpretation. 4. Conclusion In general, we received positive feedback about our approach of combining text mining and visualisation to help research processes in environmental history. Historians saw the largest potential in the amounts of data that can be considered for research but also in the open-ended character of the explorations that the visualisations support in contrast to common database search interfaces. Other types of visualisations were suggested to help analyse and discover relations and patterns in the data, something that we are currently developing.  Our future research will explore how our approach integrates into research processes in environmental history and how it can produce profound outcomes. This will involve controlled experiments including directed and open-ended tasks.  We will also conduct long-term studies to evaluate the discoveries and limitations that historians encounter when using our tools. The wide-ranging feedback from the workshop was crucial in helping the computer science team members understand priorities and research methodologies of environmental historians. Expert feedback is an important component of interdisciplinary research in digital humanities.  ",
       "article_title":"Trading Consequences: A Case Study of Combining Text Mining & Visualisation to Facilitate Document Exploration",
       "authors":[
          {
             "given":"Uta",
             "family":"Hinrichs",
             "affiliation":[
                {
                   "original_name":"SACHI, University of St. Andrews",
                   "normalized_name":"University of St Andrews",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02wn5qz54",
                      "GRID":"grid.11914.3c"
                   }
                }
             ]
          },
          {
             "given":"Beatrice",
             "family":"Alex",
             "affiliation":[
                {
                   "original_name":"ILCC, School of Informatics, University of Edinburgh",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          },
          {
             "given":"Jim",
             "family":"Clifford",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Saskatchewan",
                   "normalized_name":"University of Saskatchewan",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/010x8gc63",
                      "GRID":"grid.25152.31"
                   }
                }
             ]
          },
          {
             "given":"Aaron",
             "family":"Quigley",
             "affiliation":[
                {
                   "original_name":"SACHI, University of St. Andrews ",
                   "normalized_name":"University of St Andrews",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02wn5qz54",
                      "GRID":"grid.11914.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "information visualisation",
          "text mining",
          "digital history"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This contribution presents an XML schema for annotating a high level narratological category:  speech, thought and writing representation (ST&WR). It focusses on two aspects: Firstly, the original schema is presented as an example for the challenge to encode a narrative feature in a structured and flexible way and secondly, ways of adapting this schema to TEI are considered, in order to make it usable for other, TEI-based projects. The phenomenon ST&WR ST&WR refers to the way the voice of a character is embedded in the narrator's text and is a feature that is present in most works of fiction. It has been widely studied in narratology, as it contributes to the construction of a fictional character, the narrator-character relationship and fictional world-building in general. Though ST&WR is partly defined by formal features like punctuation, verb mode, and sentence structure, narrative function is what is of interest in literary studies (cf. 1 for an overview). The challenge is to develop an annotation schema which is sufficiently structured to allow consistent annotation (especially with multiple annotators) and still captures nuances that are relevant for literary scholars. The schema presented here – called ST&WR schema (ST&WR-S) – ties into literature studies as it uses categories agreed upon by most scholars and is similar to categorial systems proposed by narratologists Genette and Leech/Short (cf. 2, 3). The main influence was a project of Semino and Short, who annotated a corpus of English fictional, newspaper and (auto)biographical texts for ST&WR with an SGML-conformant schema (cf. 4).  ST&WR schema ST&WR-S was developed for manual annotation of a corpus of 13 German narrative texts written between 1786 and 1917 (about 57 000 tokens). This corpus was then used as a reference for the development and evaluation of automatic methods for ST&WR recognition (cf. 5). The purpose of ST&WR-S was twofold: It allows for a very fine-grained classification of ST&WR instances which is helpful in order to study the phenomenon and to do statistical studies on manually annotated data, like in Semino/Short's project. On the other hand it was designed to be modular and easily simplified to accommodate for the rougher classifications of automatic recognizers. Experiences during corpus annotation strongly influenced the design of the annotation schema. ST&WR-S has three levels of specificity: Main categories, attributes and in some cases different values for further specifications of certain attributes. These are modelled as XML tags with attributes and values.  The manual annotation was done in the GATE framework for natural language processing (cf. 6, http://gate.ac.uk). ST&WR-S is specified in XML schema files used by the plugin Schema_Annotation_Editor. Primarily, it is designed for inline XML, but GATE internally manages annotations as nodes and can convert them to a standoff XML format.  The main categories can be described with two axes: One axis represents the medium – speech, thought or written text (e.g. a quote from a character's letter). The second axis represents the four most common techniques of ST&WR: direct representation (\"He said 'I am hungry.'\"), free indirect representation (\"Well, where would he get something to eat now?\"), indirect representation (\"He said that he was hungry.\"), and reported representation, which can be a mere mentioning of a speech, thought or writing act (\"They talked about lunch.\"). This results in twelve main categories which are modelled as XML tags (direct_speech, direct_thought, etc.).   However, such a set of categories is necessarily rigid. When annotating a narrative phenomenon in a real corpus you will find many instances which are not clear-cut realisations of a predefined category. To deal with this fact, rather than just adding a confidence marker to the annotation, attributes are used to classify the type of deviation, so that the cases may be further studied and contrasted. As all attributes are optional and can be added to any main category, ST&WR-S allows for different levels of detail very easily. It is also possible to filter your annotation results afterwards by ignoring instances that carry a certain attribute. Structurally, there is one numerical attribute (level), three attributes which are binary and just indicating whether the feature is present or not (narr, prag, metaph), two with optional further specification (border, non-fact) and one with mandatory further specification (ambig). All lists of attribute values are closed sets. Table 1 gives an overview.      Attribute name  Description Values     level  level of embedment  numeric (default: 1)    ambig  ambiguity of the main category  Name of an alternative main category    non-fact  non-factual (eg. negated or hypothetical ST&WR) (\"He did not admit that he loved her.\")   neg, hyp, fut, ques, imp, plan, unspec (default: unspec)    border  borderline case of ST&WR(\"He knew that he had lost.\")  percept, feel, state, unspec(default: unspec)    narr  Ambiguity between ST&WR and non-verbal action(\"She greeted her friends.\")  binary (dummy value: yes)    prag  ST&WR, but with non-representional intent (e.g. politeness(\"I suggest you leave now.\")  binary (dummy value: yes)     metaph   metaphorical use(\"His conscience told him to go.\")  binary (dummy value: yes)    Functionally,level stands alone in the group as it does not mark a non-prototypical instance but is rather a 'monitor attribute'. It captures the level of embedment of a ST&WR instance, e.g. an instance of indirect thought that appears as part of an instance of direct speech would be tagged as level=”2”. This marker can then be used to study the behaviour of such embedded instances and compare their behaviour to non-embedded ones.    All other attributes deal with instances that deviate from the prototypical idea of ST&WR in relation to the definition of the main categories.   Ambig and narr both mark ambiguity. While ambig indicates that there is uncertainty as to which main category should be applied, narr signals that it is uncertain whether the instance is a case of ST&WR at all.   Border deals with uncertainty in regard to what is considered speech, thought and writing respectively. Especially thought representation is extremely tricky, as you have to decide what constitutes a thought. For example, the sentence “He knew he had lost.” would be marked as <indirect_thought border=”state”>, as “to know” expresses a state of knowledge rather than a clear-cut thought. Border can also be applied to speech representation, e.g. if it is unclear whether there is a true verbalization like in the sentence “He screamed bloody murder.” Non-fact deals with instances where the ST&WR is non-factual and thus not a real 'representation' in the story world. Similarly, prag marks instances where ST&WR forms are used for non-representational purposes, especially politeness, and metaph represents metaphorical use of ST&WR.  In addition to that, the ST&WR-S contains two special categories modelled as XML tags. One is frame, which marks the framing clause of a direct representation which is not part of the representation itself but still interesting in the context of ST&WR. The other is called embedded. It can be used to mark embedded narratives which appear in direct representation (usually direct speech), e.g. if a character tells a story. Marking such cases with embedded essentially shifts the whole annotation level into a new narrative frame and gives it a different status than direct_speech. The use of embedded is optional and the tag can be easily transformed to direct_speech if this effect is not desired.   Adaptation to TEI Guidelines  ST&WR-S is a valid XML schema but not compliant to TEI Guidelines. For sustainability it would be desirable to adapt it, as this would allow its usage in TEI-conformant documents without compromising their validity.    However, such an adaptation is not straightforward. The logical starting point is <said>, a tag from the quotation context which is defined for passages thought or spoken by real people or fictional characters (cf. 7). Though <said> is clearly intended to capture instances of ST&WR, its scope is narrower than the instances covered by ST&WR-S. In its core form, it only carries the attributes aloudand direct, both specified by truth values. Aloud is designed to distinguish between silent thought and passages spoken aloud (speech), but does not accomodate writing representation. Direct does not allow for any distinction between the ST&WR categories free indirect, indirect and reported. Of course, the rich attribute system of ST&WR-S does not have a predefined equivalent in TEI, either.  Several possibilities are considered how to adapt ST&WR-S while conserving its power as well as its modularity as much as possible. Ideas include use of standoff markup, possibly via the <span> tag, modelling of the complex categorizations via feature structures, referenced by the @ana attribute, or extentions of exisiting TEI-tags (most likely <said>).   ",
       "article_title":"An XML annotation schema for speech, thought and writing representation",
       "authors":[
          {
             "given":"Annelen",
             "family":"Brunner",
             "affiliation":[
                {
                   "original_name":"Institut für deutsche Sprache",
                   "normalized_name":"Institute for the German Language",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00hvwkt50",
                      "GRID":"grid.443960.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "german studies",
          "literary studies",
          "xml",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Over the last two years, Massive Open Online Classes (MOOCs) have been unexpectedly successful in convincing large number of students to pursue online courses in a variety of domains. Contrary to the \"learn anytime anywhere\" moto, this new generation of courses are based on regular assignments that must be completed and corrected on a fixed schedule. Successful courses attracted about 50 000 students in the first week but typically stabilised around 10 000 in the following weeks, as most courses demand significant involvement. With 10 000 students, grading is obviously an issue, and the first successful courses tended to be technical, typically in computer science, where various options for automatic grading system could be envisioned. However, this posed a challenge for humanities courses. The solution that has been investigated for dealing with this issue is peer-grading: having students grade the work of one another. The intuition that this would work was based on some older results showing high correlation between professor grading, peer-grading and self-grading (Wagner et al. 20111, Topping 19982). The generality of this correlation can reasonably be questioned. There is a high chance that peer-grading works for certain domains, or for certain assignment, but not for others. Ideally this should be tested experimentally before launching any large-scale courses. EPFL is one of the first European schools to experiment with MOOCs in various domains. Since the launch of these first courses, preparing an introductory MOOC on Digital Humanities was one of our top priorities. However, we felt it was important to first validate the kind of peer-grading strategy we were planning to implement on a smaller set of students, to determine if it would actually work for the assignments we envisioned. This motivated the present study which was conducted during the first semester of our masters level introductory course on Digital Humanities at EPFL.  Method 56 students were asked to produce a blogpost in which they had to identify trends in Digital Humanities based on the online proceedings of the DH2013 conference in Nebraska3.  Students had to choose three abstracts from the conference, summarise and compare them, then use the Wordpress blog dh101.ch to publish their post. Students were informed that their post would be graded by the professor but also by other students. Following the usual Swiss grading system, the grade range was from 0 to 6. The students were informed that only the grade given by the professor would count for their semester results but that 10% of their semester results depended on whether they took the peer-reviewing seriously.  The grading criteria was presented in detail to the class at the same time. Students had to check whether the blog post followed the guidelines of the assignment (discussing three articles, identifying a trend) (4 points); whether the English was correct and clearly understandable (+0.5); whether the keywords and post layout were adapted to its content (+0.5); whether the post was not just a summary of the three articles but really compared them, and, more subjectively, whether the post's content was well discussed (+0.5) and the identified trend interesting (+0.5). The students were also asked to verify that the blog post did not contained plagiarised content.  Each student had to anonymously grade five randomly chosen blog posts. In order to simplify the task and to reduce the risk of manipulation errors, we developed a simple dedicated Wordpress app4 to organise this process. Students used their Wordpress account to log in and by doing so accessed a page listing the five posts that were assigned to them, as well as one checkbox per criteria to be checked. This assignment process was done beforehand in an automated way: each paper was assigned once to the professor and randomly to five students, but under the constraint that no student could get their own paper and no more than five papers in all. Although we are aware that more sophisticated systems exist for assigning work in peer-reviewing processes (e.g. The Caesar system developed at MIT 5), we assumed that this random assignment process was relevant in this context, given the uniform nature of the content to be graded. The professor graded all the blog posts without any information on the results of the peer-grading process.   Fig. 1: Wordpress app for peer-grading Results and Discussion 52 blogs posts were produced and published on the public website dh101.ch. 47 students completed the peer review grading, and all of them graded all of their five papers. Three students accessed the website without grading any papers, and two didn’t even attempt to log in.The process drew a lot of interest and questions from the students. The criteria of the grading grid were questioned by many students, in particular the one linked with the more subjective evaluation of the posts (several students thought this was unfair). Other studies have shown that students’ writing and understanding in core courses can be improved through peer ranking (Rhett A. et al 20056). Although it is difficult to measure, it seems that the precise explication of the grading criteria imposed by the peer-review system had a positive effect on the quality on the posts when compared to the last year's course, when this system was not included. We measured a strong correlation between the average of the peer-graded marks and the mark of the professor ( r(50) = 0.39, p < .01).  Figure 2 shows the level of matching between the professor's grade and the peer graded marks after normalisation to the closest half point. In 38% of the case, the peer-graded mark is the same as the professor's, in another 38% of the cases the mark shows a 0.5 difference. The remaining 24% of cases show a larger difference. These latter cases mostly correspond to situations where either the professor or one of the students concluded that the post did not respect the instructions for the exercise and therefore gave a sanctioning mark (0 in case of plagiarism, 3 in the case of uncompleted post). Fig. 2: Level of matching between professor and student grades   This relationship can also be shown by breaking down the ratios of validated criteria: Figure 3 shows the average grades given by students (sorted ascending), the squared mean error intervals for each of them and the professor’s gradings (x marks), confirming that 76% of the papers received an average grading within 0.5 to the professor’s.  Fig. 3: Average grades, per paper   Figure 4 shows a comparison between the grading criteria used by the students and the professor’s grading. The distribution is visually similar (we cannot perform a more detailed statistical analysis because of the sample size). Fig. 4: Criterias Distribution  Figure 5 presents the correlation between the student's grade and their own grading. Although it is not obvious at first sight, there is a marginal significant negative correlation between these two variable datasets ( r(50) = –0.26, p = .07). This could suggest that students who wrote good papers are more critical of their peers.  Fig. 5: Correlation between received and given grades  Figure 6 presents the grades in the order published (the first posts are the ones submitted the earliest by the students). The professor graded the posts following this order. As expected, no correlation exists between the order and the grades in the students' gradings, as they were assigned randomly. However, there seems to be a tendency towards lower grades in the professor's grading sequence. This could be explained if a correlation existed between the quality of the post and their publication time (the best students would publish the first). However, this correlation was not found in the student’s grading. This could suggest a potential temporal bias observed through the fact that the last evaluations were tendentially lower that the first ones (the professor becoming more critical). Fig. 6: Evolution of grades with time Conclusion This article presents a preliminary study for a Digital Humanities MOOC conducted on a class of 56 students. In the present context, we can conclude that peer-grading is highly correlated to the professor’s grading. In about ¾ of the cases, the grades obtained by this method are within a range of 0.5 points of the professor’s grading. Qualitative observations tend to show that the quality of the posts increased when compared to the previous year, likely because students had to reflect on the grading criteria and were cautious of producing good work when this work would be evaluated by their peers. In addition, our study may suggest some possible temporal biases in the way the professor grades a long sequence of work, reinforcing the idea that peer-grading may not only be an interesting positive alternative to traditional grading but also that it may, in some case, be less biased. Nevertheless, these preliminary results, dependent of the particular context of this study, should be extrapolated with care and would not eliminate the need to conduct regular quality evaluation in the context of a MOOC on Digital Humanities. Indeed, these results do not guarantee that the same peer grading method could scale to a 10 000 student MOOCs without problem. As the number of students increases, and the cultural backgrounds and linguistic competencies diversify, part of the behavioural homogeneity that we observed in this prestudy may no longer be valid. For this reason, this research should definitely be completed with a posteriori study testing the efficiency of peer-grading with a similar method in a randomly chosen set of learners of the entire MOOC. Acknowledgments:The authors would like to thanks Andrea Mazzei for fruitful discussions about the analysis of the results of this preliminary study.  ",
       "article_title":"A Preparatory Analysis of Peer-Grading for a Digital Humanities MOOC",
       "authors":[
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Cyril",
             "family":"Bornet",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "digital humanities",
          "teaching and pedagogy",
          "crowdsourcing",
          "pedagogy and curriculum"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Over the past few years, the literature on race in digital humanities has steadily grown. From various articles in Debates in the Digital Humanities to the development of Postcolonial Digital Humanities and MLA E-Roundtable, “Assessing Race in Digital Humanities”, many have explored the theoretical and activist potential of addressing race in DH. Simultaneously, venues and projects like THATCamp and The Praxis Program have progressively pushed DH beyond the bounds of research one institutes. However, much has yet to be said about the complexities of involving students with digital humanities at under-resourced institutions. Teaching humanities classes at Shaw University, the first historically black college in the South, has given me an excellent opportunity to do just that.  This paper discusses how I dealt with a lack of technology and student confidence to create a modern archive of Shaw University student life, called #myshawu, by using smartphones and an open-source Ruby On Rails engine for harvesting Instagram photos. Though the assignment taught me the potential of DH as an empowering tool for my students to tell their stories in a public venue, it ultimately convinced me that the greatest hurdle to getting people of race involved in the field is not just exposure to technology, but teaching them the skills and critical thinking necessary for true digital empowerment. The idea for the assignment began when I read the MacArthur Foundation report, “The Future of Learning Institutions in a Digital Age.”1 In the report, Cathy Davidson and David Goldberg disclose a sobering reality: “Despite government pronouncements to the contrary, ‘digital divide’ is not just an old concept but a current reality” (Davidson and Goldberg 20). The vast public acceptance of a so-called Internet Generation or Generation Y ignores the very real tech fluency differences that often exist along class and race lines. Siva Vaidhyanathan points out just how detrimental this assumption is for underprivileged students in his essay, “Generational Myth”: “to assume an entire generation is ‘born digital’ willfully ignores the vast range of skills, knowledge, and experience of many segments of society. It ignores the needs and perspectives of those young people who are not socially or financially privileged. It presumes a level playing field and equal access to time, knowledge, skills, and technologies.”2 My experience teaching at one of the country’s first HBCUs confirms the cautionary words of Davidson, Goldberg, and Vaidhyanathan. Additionally, when I came across Adeline Koh’s “Race and Digital Humanities: An Introduction” HASTAC presentation, Alan Liu’s “Where is the Cultural Criticism in Digital Humanities”, and other articles addressing the issue of DH and race, I began to wonder what digital humanities could do for my students. Could a DH project be a way to bridge this digital divide? Could a DH project be done with such limited resources? Not only do many of my students lack technological fluency, many of them don't own computers, there are few computers labs on Shaw's campus, and the university has no technology unit that students or myself can turn to for help. My struggle with these limitations coincided with an interesting project being done at a neighboring institution. NCSU Libraries was using mobile devices to have students generate an archive of its new library in a project called my #huntlibrary. A few conversations and a partnership later, #myshawu was born. #myshawu is a repository of student-generated images collected from Shaw’s first-year, almost exclusively first-generation students during a one-semester basic writing course into which 90% of Shaw’s incoming freshman are placed. We bridged some digital access hurdles by using largely free tools and open source software. Students were already utilizing Instagram, an app accessible to most of them through their smartphones, a familiarity which lessened the learning curve and gave students the confidence to dive into the assignment. To gather all these images, I spun up an app which uses Lentil, the Rails engine that drives My #huntlibrary, on a free hosting service, Heroku. All the students had to do was use their Instagram accounts, take photos of their lives around campus, and tag them “#myshawu”. Through these simple steps, students were able to represent themselves and share their images with other students, the university community, their communities of origin, and the public. As a companion to the photo collection, students published long-form, photo-rich narratives on a class collaborative Wordpress blog of the same title, #myshawu. These essays focused on their photos and their understanding of themselves in relation to the university and, hence, in relation to their academic identity.    Fig. 1: A page of student-generated images from the #myshawu website.  When students submitted their Wordpress essays, they also completed a survey to indicate the level of difficulty and the level enjoyment they experienced with the assignment. Out of 91 students who completed the assignment, 14 had difficulty getting the required hardware, 10 had trouble setting up required software accounts, and 15 students had problems using this software. These numbers show the relative ease with which students were able to access the necessary technology, an ease dependent on using personal and open-source tech resources rather than non-existent institutional tech resources. Yet, many of the students struggled to complete the assignment as successfully as I’d hoped. Only about half the images that students used in their final essays on Wordpress ended up on #myshawu, and a good many of these images were copied from the Internet. However, 85% of the students who completed the assignment said it was not difficult to take pictures. There could be various reasons for this discrepancy between confidence and successful execution, such as poor resiliency or effort, and/or inconsistent classroom attendance. Yet despite some shortcomings, the unintended, often unquantifiable successes continue to emerge. As Shaw approaches its sesquicentennial and HBCU across the South consider closing their doors, the students involved with #myshawu are going to capture oral histories, chronicling in photos, video, and audio the stories of alumni so important to Shaw's history. The project has engaged many students beyond the classroom as they visit local history museums and inquire about the possibilities of exhibiting our work. They reimagine the project with me and ahead of me, asking for opportunities to use their pictures to raise funds to for my technology resources and for more campus events.  Another facet of this project shines a light on students’ perspective on social media and adds another layer of complexity to the argument that media studies and DH scholars have been posing about race and technology. Logan Hill, in “Beyond Access”, put it best: “Universal access isn’t just about being able to surf the Web, it’s about the ability to participate and compete in a technology-driven industry and society” (29).3 To have a wide range of students from all race and class backgrounds succeed in this media-saturated society, we can’t just give them new technologies to create content only their friends will consume. We need to show them that the technologies they use every day can be harnessed to empower their academic and professional lives. #myshawu has also taught me the importance of digital humanities for helping my students develop a more critical perspective on technology. The full scope of what these kinds of projects teaches us about DH, HBCU students, cultural representation and empowerment is still unfolding, but it’s clear through collecting and analyzing data, student-generated reflective writing, and evaluations, that digital tools like these cause necessary shifts in students’ understanding of their own agency, particularly the role that writing, technology, and image creation can have on their power as scholars and professionals. Unfortunately, largely due to financial constraints and the many needs pulling at institutions like Shaw, HBCUs are some of the most unlikely to support faculty technology training and least likely to have the resources to support digital projects. Yet DH tools belong in the hands of those who have the most at stake as they become invaluable tools for engagement and student success. That said, certain assitance is needed from the digital humanities community for these projects and these students to reach their full potential. Partnerships with other universities, academic technologists, and more accessible and flexible technologies are just some of the support that can be extended by the greater DH community to HBCUs like Shaw, thereby creating cross-institutional collaborations that highlight the strengths of all institutions. Then #myshawu can become just one effort in a larger movement towards developing a DH pedagogy that can be inclusively practiced without relying on the financial privilege so often synonymous with DH.   ",
       "article_title":"DH on the Fringes: Using Smartphones, Instagram, and Ruby on Rails to Archive the DH Experience at an HBCU",
       "authors":[
          {
             "given":"Desiree",
             "family":"Dighton",
             "affiliation":[
                {
                   "original_name":"Shaw University",
                   "normalized_name":"Shaw University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/049x01c36",
                      "GRID":"grid.263467.2"
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Norberg",
             "affiliation":[
                {
                   "original_name":"North Carolina State University Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "cultural studies",
          "sustainability and preservation",
          "repositories",
          "digital humanities - pedagogy and curriculum",
          "archives"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Language changes. On this everyone would agree. But how can we track this ever-changing phenomenon? If we focus on modern languages, the task is easier since we have native speakers whom we can ask, “How is this usage different than this other one?”  But in the case of historical languages, and especially those spoken and written millennia ago, this task becomes much more difficult. How is it possible for us to create, or at least simulate, in ourselves the language proficiency of a society that has been dead for hundreds or thousands of years? And if we cannot rely on native proficiency, how can we track systematic language change and, thus, come to a better understanding of the language and texts of any particular period. The pioneering work most closely associated with John Sinclair gives us our best answer: Trust the Text!1  We have millions of words of, e.g., Greek, ranging over a time-span of 3000 years from Homer to the present day.2 What we need are methods that can help us to harness this huge amount of information. David Bamman and Gregory Crane have already begun working in the field of historical word-sense variation in Latin.3  Relying on translation equivalents, they were able to successfully track word sense variation in Latin in a 389-million word corpus. By their own admission, however, this method has the drawback of requiring “large amounts of parallel text data”4 in translation. In contrast, the method proposed here, comparison of co-occurrence patterns in two or more corpora, which has been applied in many other fields (see below), only requires simple, plain-text input in a single language. The first theoretical foray into computational analysis of co-occurrence patterns came in 1955 with Warren Weaver’s article “Translation.”5 Starting from the recognition that the sense of any word is ambiguous if examined in isolation, he asserts, “But if one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.”6  The necessary corpora and computational power to realize Weaver’s theory, however, only came much later.  Computational analysis of co-occurrence patterns on large-scale corpora began with the COBUILD project, which set out to build “the very first dictionaries to be based completely on corpus data” and, in doing so, systematically tracked collocations, defined as “the high-frequency words native English speakers naturally use with the target word.”7  Since then, co-occurrence analysis has been used in several fields in which word-sense disambiguation is necessary, such as speech recognition,8 machine translation,9 and topic modeling,10 and it is the basis for the field of distributional semantics.11 In this paper, I will present my application of co-occurrence analysis to the problem of historical word-sense variation, what I call in my title “semantic drift.”  I have chosen to carry out these experiments using as my two corpora the Greek Old Testament (the Septuagint) and the Greek New Testament for several reasons: the texts are easily available in digital form, have been deeply researched and, thus, deeply annotated, exist in multiple translations that can be used to benchmark methods and to test results, are of great interest to millions of people around the world, and, finally, because they are the most influential texts in the history of western civilization, the research can be easily extended to other corpora and, with more difficulty, even into other contemporary languages such as Latin, Hebrew, Aramaic, and Coptic, to name just a few.  The presentation will have two primary foci: the method and exemplary results.    The method consists of the following steps.  First, I tokenized the texts and calculated co-occurrence counts for every word in an 8-word window.12  Using these co-occurrence tables, I calculated the statistical significance of each collocate word to each node word using the log-likelihood measure as described by Manning and Schütze.13 Log-likelihood was chosen primarily because it deals very well with sparse data and can be easily interpreted without recourse to, e.g., chi-squared tables.14  The former is important because most data in language is quite sparse and I was reluctant to eliminate a large amount of my data simply because the chosen method could not deal well with it.15  Ease of interpretation was important because, instead of using the measure as a means of hypothesis testing, in which I would expect to get a yes or no answer, I used it as hypothesis weighting, i.e., to measure how much more likely one thing is than another.  My purpose is not to decide if two words certainly form a set collocation but, instead, to measure the strength of collocation, ranging from strong repulsion to strong attraction, and compare this range with the ranges of other node words to find relationships.  Having calculated the statistical significance of these relationships, I used the cosine similarity16 measure to determine the strength of relationship, first, of every word in the Old Testament with its counterpart in the New Testament (e.g., θεός (God) in the Old Testament to θεός in the New Testament) and, second, of every word in the Old Testament with every other word in the Old Testament and the same for the New Testament.  These results allow me to discover which words’ senses have changed the most (comparison of Old Testament to New Testament) and how they have changed (comparison of the words most similar to, e.g., θεός in the Old Testament with those most similar to θεός in the New Testament).    Fig. 1: Results based on the differences in cosine similarity measure between θεός (God) and the list words.  Those on the left are nearer to θεός in the OT, on the right to θεός in the NT.  After relying purely on computational methods to this point, the final results of my research come through qualitative analysis of the comparisons described in the previous paragraph.  The two tables above show the 20 words most closely associated with θεός (God) in the Old Testament and the New Testament based on the differences between the cosine similarity scores in each testament between θεός (God) and the words in the list.  The colors have been added by me to highlight what I see to be related words in each list.  What we see on the left is that God in the Old Testament is more closely related to words concerning ruling (in yellow: “Solomon”, “command”, “anointed”, “Benjamin”), violence (red: “destroy utterly”, “to lay hold of”, “to drive away”, “to strike”), agriculture (brown: “field”, “cattle”), and the Exodus (green: “captivity”, “foreign”).  While in the New Testament, God is more closely related to (evil) rulers (yellow: “Satan”, “Pharisees”, “centurion”, “Pilate”), servants of God (dark purple: “Peter”, “Christ”, “apostle”, “Paul”, “disciple”), and words that relate the servants to God (light purple: “to believe”, “faith”, “gospel”, “grace”, “love”). So, by classifying the words most closely related with θεός (God) in each of the testaments, we are able to determine not only that the portrayal of God had changed from the Old Testament to the New Testament, but also to see how it changed (move from a ruler who leads and makes war to a patron who offers to and receives favors from clients) and to guess at the probable historical cause (change from an independent monarchy to a Roman province).  In the second part of this paper, salient examples, such as that described above for God, will be used to demonstrate the effectiveness of this method. The final section of the paper will be a look forward at how this method could be extended to other corpora and even other languages, allowing us to tell the stories of language development with more precision and so, ultimately to understand historical texts better.  ",
       "article_title":"Tracking Semantic Drift in Ancient Languages: The Bible as Exemplar and Test Case",
       "authors":[
          {
             "given":"Matthew",
             "family":"Munson",
             "affiliation":[
                {
                   "original_name":"Georg-August-Universität Göttingen",
                   "normalized_name":"University of Göttingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01y9bpm73",
                      "GRID":"grid.7450.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Historical Word Sense Variation",
          "Ancient Greek",
          "Distributional Semantics",
          "Natural Language Processing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction Sudan is one of the most diverse and culturally rich countries in the world. It is ethnically diverse: the Sudanese are divided among 19 major ethnic groups and about 597 subgroups and speak more than 100 languages and dialects. It is also culturally diverse: tradition, ceremony, language, poetry, art, drama, music and dance, are all vital cultural practices, and Sudan is one of the richest countries in Africa in archaeological remains. Sudan's cultural riches rival those of Egypt, Greece and Rome, but war, famine, displacement and the ravages of time, climate and lack of funds means that the cultural heritage of the country is under severe threat. The preservation and recovery of cultural heritage through digitisation is well-understood by the Sudanese, and many outstanding projects exist throughout the world for Sudan to draw upon. The world knows much about other ancient civilisations, but not much about Sudan. Digitisation will help show the riches of Sudan to the world--and to itself. Many citizens are ignorant of the greatness of the history of their country, and schoolchildren and their elders can benefit greatly from access online to their rich heritage.  The digitisation of selected material of cultural heritage is a national initiative led by the Sudanese Association for the Archiving of Knowledge (SUDAAK), a Sudan-based NGO, to guarantee the long-term preservation, integration, authenticity and accessibility of important cultural content in respective concerned national institutions. The project addresses some of the main issues related to digitisation networks and services in the cultural domain. It specifically aims at safeguarding and reinforcing Sudanese cultural heritage through new technologies. In its initial stage the project will aim at identifying and facilitating the urgent needs for the implementation of appropriate applications of digital technology in cultural content storage and sustainability. SUDAAK is a cultural non-governmental organisation (NGO) concerned with archiving Sudanese life in history, politics, folklore and culture. While the term archiving is mostly associated with records, the role envisioned for SUDAAK is organising the discovery and display, the celebration and preservation of the traditional and modern knowledge together with the achievements attributable to imagination and leadership of those who were pioneers in laying the foundations of the Sudan and its political, social, economical and cultural strengths. Their major programme now is the Archiving of the 20th Century Sudan Intellectual Heritage, but all other periods and all types of artefacts are within SUDAAK's scope.   1.1. Overview Digital Sudan The overall goals of Digital Sudan are:  Storage of selected recorded cultural material on Sudan within a well-designed selection policy; Electronic treatment of old and decaying books and pictures; The facilitation of accession to Sudan folklore related material reserved in prominent research institutions;  Facilitation of access to National Library content needed for Government processes and decision-making; Improvement and enhancement of digitisation facilities and services in Sudan; The creation of an online national library serves as a model for integrating multi-format and multi-lingual resources from museums, archives, libraries, and bibliographic and Web resources and develops retrieval capabilities;  Development of a collaborative infrastructure that can support an increasing number of contributing partners nationally; and  User provision of integrated digital materials that seamlessly link all types of resources.   The key stakeholders are currently:  National Record Office / Ministry of Council of Ministers University of Khartoum/ Ministry of Higher Education & Scientific Research Sudan Radio Corporation/ Ministry of Culture and Information Sudan National Television/ Ministry of Culture and Information National Corporation for Archaeology & Museum/ Ministry of Tourism Photography Unit/ Ministry of Culture and Information Film Production Unit/ Ministry of Culture and Information National Library of Sudan/ Ministry of Culture and Information National Research Centre - Information& Documentation/Ministry of Science and Communication Sudan Folklife Documentation Centre / Ministry of Culture and Information Africa City of Technology/ /Ministry of Science and Communication Sudanese Association for Archiving Knowledge: / Non for Profit Civil Society Organisation  SUDAAK is also working with institutions outside Sudan with expertise in digitisation and digital library development. Currently these include Durham University, with whom SUDAAK have a Memorandum of Understanding, and the Department of Digital Humanities at King's College London, where there is a great deal of expertise in all aspects of this area. In April 2013, the stakeholders listed above were formally constituted as the National Cultural Heritage Digitisation Team (NCHDT).  SUDAAK is also planning to work with other institutions world-wide in the development of the plans for the Digital Library.  The content available for digitisation is rich and diverse: In the National Archives alone, there are 76 million photographic negatives recording all aspects of life in the Sudan over the past 100 years. The university library has priceless manuscripts from the beginning of Islam; there are 9 museums throughout the country with artefacts from more than 4000 years of history; film, radio tapes and video record all the major events in the country, as well as the music, dances and traditional practices. Traditional foods and medicine are of great importance too, and there are samples, photographs and documents concerning these in the archives. Sudan has a good education system overall, with a high level of participation in urban areas. Literacy rates are relatively high, though both participation and literacy rates are lower outside urban areas. The universities are excellent, and there is a modern Open University, established in 2003, that has links to the UK's Open University and the University of Cambridge. The Open University uses all forms of modern technology to communication with students: video conferencing, Skype, Facebook, websites, as well as radio, television and telephones. In planning for Digital Sudan, the country has both advantages and challenges. In terms of advantages, the country has an excellent tele-communications infrastructure. It is modern, well-designed, robust and capacious. Sudatel, the main tele-communications company and the National Information Center can provide some of the storage, connectivity, and band-width that should be needed for Digital Sudan, and as the resource grows, the capacity can be increased. In the Ministry of Information and the cultural institutions there is already some technical knowledge, and more importantly, there is huge enthusiasm for the project and a willingness to make things happen. The National Library, National Archives, and the National Museums have good catalogues in place: these are the backbone of any digital library. There are a number of digitisation projects already being undertaken in the cultural institutions and the universities: for example, the University of Khartoum holds the Electronic Sudan Library which provides rare Sudanese materials of historic and cultural significance, with full text that can be searched in Arabic, English and other languages. Sudan Radio has already digitized 27,000 hours of historic radio tapes; the National Museum has digital images of artefacts attached to a catalogue records. But there is much to do, and many challenges and risks. Digital preservation, for example, needs serious consideration. Here, though, Sudan can benefit from excellent work being done in this area by major institutions throughout the world: the US Library of Congress; Europeana; the British Library; the National Library of Wales and many other institutions.  The condition of the analogue materials is also a serious consideration. An intense programme of physical conservation is needed alongside any digitisation activities, and storage of the valuable original artefacts in better conditions than at present is an urgent need.   Next steps SUDAAK and the NCHDT, together with their international partners, are in discussion with the Ministry of Information and with other funders to identify possible sources of funding for the activity. They are also taking some steps towards training staff in digitisation skills, and digital library development. A major new development is the signing of an agreement wit the University of Bergen, Norway, to digitise the archive radio, TV and film materials of the Sudan Radio and Television Corporation.    Conclusions For a country to embark upon a program as ambitious as this is a huge challenge, and will be costly. Even more costly would be the risk of doing nothing. Sudan is emerging from strife and division into the modern world, and is moulding its new identity by building on the strengths of its cultural memory. Digital Sudan has a huge role to play in this.   ",
       "article_title":"Digital Cultural Heritage and the Healing of a Nation: Digital Sudan",
       "authors":[
          {
             "given":"Marilyn",
             "family":"Deegan",
             "affiliation":[
                {
                   "original_name":"King's College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The most difficult point in the digital analysis of classical Chinese texts is that they don't have any spaces or punctuations between words or between sentences. They consist of continuous strings of Chinese characters from the start to the end of texts. Contrary to the analysis of modern Chinese texts, which have several punctuation marks and can be fragmented into phrases with these punctuation marks, the analysis of classical Chinese texts has to begin with finding out the ends of sentences. Classical Chinese is an isolative language, which doesn't have any inflection or agglutination. Furthermore, we don't have any generally accepted word-class system for classical Chinese. We first ought to develop machine-supported word-class system for classical Chinese. However, in classical Chinese, many morphemes may be observed as nouns and verbs, etc. In this paper we propose a method to analyze classical Chinese texts. In our method, we use our original morphological analyzer based on MeCab 1. We propose a new four-level word-class system for classical Chinese on the MeCab-based analyzer. We design the top level of the word-class system to represent the predicate-object structure of classical Chinese. The second level is the ordinary word-class of classical Chinese. The third and fourth levels are word-subclasses to describe detailed behavior of the words in classical Chinese texts. The development of our four-level word-class system for classical Chinese was not straightforward. At the early stage, we developed a prototype dictionary from IPA Japanese Dictionary 2 and defined a prototype word-class system for classical Chinese. We also developed a prototype corpus along the prototype word-class system. And then, at the later stage, we examined the prototype corpus and redefined our four-level word-class system to be more suitable and systematic for classical Chinese. Especially, we excluded “adjective” from the second level of our new word-class system, since, in classical Chinese, there exists no essential distinction between “verb” and “adjective” 3. We refactored the prototype dictionary into our new dictionary, and the prototype corpus into our new corpus.   Fig. 1: Our Four-Level Word-Class System for Classical Chinese In our new word-class system (Fig.1), the top level, which we call “word-superclass,” is defined to represent the predicate-object structure of classical Chinese: “n” represents objectives, “v” represents predicates, and “p” represents others. The second level is the ordinary word-class of classical Chinese: noun, pronoun, numeral, verb, preposition, adverb, auxiliary verb, particle, and interjection. We first constructed the word-class from a famous classical-Chinese dictionary Zenyaku Kanjikai 4, and we reconstructed the word-class, especially excluding adjective. In our system, noun, pronoun, and numeral compose “n” word-superclass; verb, preposition, adverb, and auxiliary verb compose “v” word-superclass; particle and interjection compose “p” word-superclass. The third and fourth levels are word-subclasses to describe detailed behavior of the words in classical Chinese texts. We first tried to construct these word-subclasses from Word List by Semantic Principles5. However, its levels were stratified too deep and its category was highly depended on Japanese. Therefore we constucted rather shallow word-subclasses, suitable for a morphological analysis of classical Chinese texts, from scratch (Fig.1). We have often revised the third and fourth levels of our word-class system. Whenever we revise our word-class system, we should modify our dictionary and corpus. For the development of a large corpus, the collaboration of linguistic experts, scholars of classical Chinese, input operators, and data managers is required. We use a distributed version control system, Git, to support the collaboration for the development of our corpus. Git is a powerful but complicated system, so we restrict our use of Git to avoid conflicts between versions of our corpus. And we have developed our own “skin” to hide the complicatedness of Git. Our own “skin” mainly consists of Git-based corpus manager, our Mecab-corpus editor (mentioned below), a system updater of our dictionary and corpus, and a system updater of the framework. In order to make corpus for classical Chinese on MeCab, we have constructed a MeCab-corpus editor based on XEmacs CHISE 6. We use the MeCab-corpus editor to compile our digital corpus and our digital dictionary based on our four-level word-class system for classical Chinese (Fig.2). In our MeCab-corpus editor we first input typical sentences from classical Chinese texts. Second we push the right-most button “classical Chinese” of the editor, then we obtain a morpheme sequence temporarily segmented by MeCab. Third we edit the sequence to categorize its words, looking up authoritative textbook refereneces of the sequences. And last we include the morpheme sequence in our corpus for classical Chinese. Our corpus for classical Chinese on MeCab now includes about 20,000 sentences, written in our four-level word-class system. Our dictionary for classical Chinese on MeCab includes about 5,000 words, which we categoraized into our four-level word-class system. We keep increasing our corpus, and we also keep selecting new words from our corpus to add them into our dictionary. In conclusion, we made a morphological analyzer for classical Chinese. The analyzer required a dictionary and a corpus based on a word-class system. We developed our four-level word-class system, suitable for analysis of classical Chinese, originally made from some other dictionaries, and then we reconstructed the word-class system. We also developed the Git-based framework including our Mecab-corpus editor, which allowed us to edit the corpus and dictionary effectively.  Fig. 2: Screenshot of an Authoritative Textbook and Our MeCab-Corpus Editor  ",
       "article_title":"A Morphological Analysis of Classical Chinese Texts",
       "authors":[
          {
             "given":"Koichi",
             "family":"Yasuoka",
             "affiliation":[
                {
                   "original_name":"Institute for Research in Humanities, Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Naoki",
             "family":"Yamazaki",
             "affiliation":[
                {
                   "original_name":"Faculty of Foreign Language Studies, Kansai University, Japan",
                   "normalized_name":"Kansai University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/03xg1f311",
                      "GRID":"grid.412013.5"
                   }
                }
             ]
          },
          {
             "given":"Christian",
             "family":"Wittern",
             "affiliation":[
                {
                   "original_name":"Institute for Research in Humanities, Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          },
          {
             "given":"Yoshihiro",
             "family":"Nikaido",
             "affiliation":[
                {
                   "original_name":"Faculty of Letters, Kansai University, Japan",
                   "normalized_name":"Kansai University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/03xg1f311",
                      "GRID":"grid.412013.5"
                   }
                }
             ]
          },
          {
             "given":"Tomohiko",
             "family":"Morioka",
             "affiliation":[
                {
                   "original_name":"Institute for Research in Humanities, Kyoto University, Japan",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "corpora and corpus activities",
          "asian studies",
          "linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  High-resolution page images are providing digital humanities researchers with unprecedented visual access to historically significant works located around the world. As libraries and archives continue digitizing their historical document collections, they are increasing the quality and resolution of their document imaging systems and producing images that, while unprecedented in their clarity and detail, are inconvenient to navigate and manipulate in traditional browser-based environments. Users find themselves waiting for large PDF files to download, or clicking endlessly through thumbnail after thumbnail to find a page image that contains materials of interest to them. These methods of document navigation and viewing have been in place since the infancy of the web browser, and are needlessly awkward given advances in creating asynchronous web applications. The most common interface paradigm for browsing images online is the ‘image gallery’. To illustrate this type of interface we will use the example of the Early English Books Online (EEBO) interface as one with which some readers may be familiar. In the EEBO image viewing interface, users navigate a document as if it were a series of independent images, or image gallery, viewing small thumbnails that, while efficient for downloading to a browser, make it impossible to see the actual content of the page (Figure 1). To examine any single page, the user must click on an image, bringing up a second view of the page optimized for viewing in the browser, but may not be usable for close examination if the text on the page is too small. Should a user wish to examine any part of a page in particular detail, there may be an option to download a larger, high-resolution image, but the user must wait for this large image to download to their browser, which may take several minutes depending on the speed of the network connection and the size of the image. If the user waits for the full quality image to download but wishes to continue browsing the document on the next page they must traverse back to the smaller thumbnails and start the process again.   Fig. 1: Viewing page thumbnails in the EEBO collection.  An alternative to the image gallery mode of interaction is the use of a browser-based book reader component. Several of these systems are available for managing user interactions with page images. Perhaps the most well-known purpose-built web-based document viewer is BookReader by the Internet ArchiveI1. Developed as part of the Open Library project, this software presents the user with a book metaphor, inviting them to 'turn' the pages of a book. While this provides a useful alternative to the image gallery mode of viewing, the IA BookReader requires that each page is represented by a complete image file, so zooming in and viewing a page in detail requires the user to wait while the entire image is downloaded—which can be slow and cumbersome depending on the size and resolution of each image. This is also true for PDF-based document image display, where a user is forced to accept a trade-off between viewing low-resolution versions of page images, or waiting for extremely large PDF files to download before they can view any of the pages.  To optimize viewing large, high-resolution documents in a web browser we developed the Diva document image viewer. Diva features several methods for managing user interactions with document page images. Users interact with the full document by scrolling the document, as they might with a PDF file. However, in Diva all page images are composed of smaller tiles. These tiles are of a fixed size (256x256 pixels), and all pages (and their tiles) that are outside of the user's viewport are not downloaded to the browser. This creates an ‘instant-on’ effect to viewing a document, since the user does not have to wait for the entire document to download, but just a small portion. As a user scrolls, new tiles are downloaded on-demand. To view higher or lower resolution page images, users can 'zoom' between resolutions. Zooming in and out on an image will download just the portion of the page that fits on the users’ screen (Figures 2 and 3).   Fig. 2: A zoomed-out view of a manuscript page.    Fig. 3: Zoomed-in detail of the lower-left corner of the page shown in Figure 2.  With the ubiquity of mobile devices it is important to ensure Diva functions on low-memory systems, like the iPad or iPhone. Diva.js uses several methods unique to document image viewers for optimizing memory usage and display. While a document may be several hundred pages long, Diva keeps just three pages in memory at any given point in time, dynamically adding and removing page elements from the browser as the user scrolls. This creates a fast and efficient browsing system for both mobile and desktop devices.   Furthermore we have built a number of image manipulation tools into Diva.js that allow users to engage with a document. Many documents, especially older manuscripts, feature faded inks or text that is written perpendicular to the captured page orientation (e.g., marginalia). Using Diva, users can manipulate brightness, contrast, and page rotation in their browser via an unique set of HTML5-based image manipulation tools, allowing them to enhance faded inks or rotate a page to read margin notes or tables (Figure 4). Other viewers that offer this functionality manipulate the image on the server and then send it back to the client. This is a high-latency operation. With browser-based image manipulation users can see the results of their changes immediately.   Fig. 4:  Manuscript page (in Arabic) rotated 90° to view perpendicular text on flyleaf. The controls on the left allow the user to manipulate brightness, contrast, rotation, zoom, and individual RGB colour channels.    We have used Diva.js as the presentation layer of a document image search system. When a user searches for a given word or phrase, the results are presented in situ on page images, highlighting the exact location on each page where their result occurs.  All components of Diva are available as free and open-source software, available on GitHub 2. Diva may be integrated into existing digital library systems. On the server-side, Diva requires the IIP Image Server 3, a giga-pixel image server that serves the page image tiles, and a standard web server, such as Apache or NginX. Document images can be encoded as either multi-resolution JPEG2000, or pyramid TIFF files. The JavaScript components of Diva.js will work in any modern web browser. These components manage the asynchronous communication process between the user's browser, the web server, and the IIP Image Server. We have also built in a comprehensive API and plugin system that provides ‘hooks’ into the page loading and image manipulation systems. In our presentation we will provide a demonstration of Diva.js and an overview of its background and development history. We will discuss several case studies where we have employed Diva.js for viewing and searching large historical document image collections, where it is used to display page images captured at over 1,200 PPI. Zooming in on images at these resolutions allow users to view individual brush strokes, paper detail and condition, to view details of manuscript illuminations, and several other important document factors that are lost in lower-resolution image displays. Finally we will demonstrate several new features for highlighting and annotating places of interest on page images and integrating page images with the output of optical character recognition software.  ",
       "article_title":"Accessing, navigating, and engaging with high-resolution document image collections using Diva.js",
       "authors":[
          {
             "given":"Andrew",
             "family":"Hankinson",
             "affiliation":[
                {
                   "original_name":"McGill University",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Laurent",
             "family":"Pugin",
             "affiliation":[
                {
                   "original_name":"Swiss RISM/ Fribourg University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ichiro",
             "family":"Fujinaga",
             "affiliation":[
                {
                   "original_name":"McGill University",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction In the field of digital humanities, scholars are increasingly producing digital editions of texts and manuscripts. The representation of knowledge included in literary texts is a complex issue, requiring rich vocabularies, also called ontologies, for representing the many different aspects that are investigated by scholars. In literature, there are many ontologies that focus on different aspects of textual information but one single ontology representing all these aspects does not exist. The “Towards a Digital Dante Encyclopedia” project is a three years Italian National Research Project, started in 2012, that aims at building a prototypical digital library endowed with services supporting scholars in creating, evolving and consulting a digital encyclopedia of Dante Alighieri and of his works. The digital library is based on a semantic representation of Dante’s works and of the knowledge embedded in them in RDF language 1, a language recommended by the Web Consortium for the representation of knowledge. In RDF, every piece of knowledge is represented as a triple (subject predicate object), and a set of triples form an RDF graph, generally called semantic network, in order to highlight the formal linguistic nature of the representation. The services being developed address several tasks carried out by the scholars building the encyclopedia, starting with the visualization of references to primary sources (i.e., other authors’ works which Dante referred to his own works), their types and their distribution both in time and in the works of Dante. The overall goal is to shed light into the cultural context in which Dante wrote his works and into the development of Dante’s reference library over time. This part of the project is divided in several phases. The first phase regards the creation of an ontology for the knowledge embedded in scholarly commentaries to Convivio 2, the philosophical treatise which we choose as initial case study. In the second phase, the ontological model is generalized to represent the knowledge embedded in the scholarly commentaries to other Dante’s works. In the third phase, Dante’s works along with their attached commentaries are inserted into the digital library, as part of the semantic network being built. In the fourth phase, the primary sources referenced by Dante in his works, as reported by the commentaries, are inserted into the digital library, following the same semantic approach. In the last phase, services are developed, as web applications that allow scholars to browse the semantic network of Dante’s work, of primary sources, or of references linking the former to the latter. The references will be visualized in an intuitive way through tables and charts, highlighting their distribution in Dante’s work and over time. We present the structure of the semantic network, as it currently stands and indicate how it will be further developed. Furthermore, we highlight the benefits brought by the visualization service of primary sources to scholars. 2. Ontology for the representation of convivio In order to detect the primary sources used by Dante to write his Convivio, we relied on the most recent and updated commentary to the text, that of Gianfranco Fioravanti (Mondadori, in the press), and created an ontology for representing the relevant knowledge carried by this commentary. In particular, our ontology represents:  the passage of Dante’s text (e.g., “Sì come dice lo Filosofo nel principio della Prima Filosofia”) to which a quotation from a source refers; the correspondent book, chapter and paragraph of Dante’s text the author of the work referenced in the commentary (e.g., Aristotle); the title of the work referenced in the commentary (e.g., Metaphysics); the thematic area of the work referenced in the commentary (e.g., Aristotelianism).  In order to create an ontology for the semantic representation of the above information, we investigated several existing ontologies (e.g. CIDOC-CRM 3, FRBR 4 , FaBiO 5, SKOS 6), and we chose the classes and properties that we considered the most appropriate to represent the above information. Furthermore, we added our own classes and properties for the representation of the categories of knowledge that were not addressed by the existing ontologies. Then, we transformed the initial commentary into an RDF graph structured according to the ontology 7. On the basis of our ontology, we are approaching the remaining phases of the “Towards a Digital Dante Encyclopedia” project. To such aim, we are using the ontology developed so far in order to represent other works of Dante (e.g. De Vulgari Eloquentia, Monarchia) as well as the knowledge carried by commentaries to them. At the same time, we are collecting the primary sources of Dante’s work in a digital format, for insertion into the semantic network underlying the digital library. Our diachronic analysis, in fact, aims at representing the evolution of Dante’s knowledge about primary sources. 3. The model population In order to enrich our RDF graph, as we have done for Convivio, we are collecting information for other Dante’s works. In particular we are focusing on (i) the text of the work along with the attached commentaries; (ii) the primary sources referenced in the notes. We are currently storing the RDF triples generated according to our ontology both for the notes and the primary sources. We are relying on the Virtuoso [8] technology for storing and accessing large RDF graphs. It is important to note that the works of Dante, as well as most of their primary sources, exist in some digital format. However, to the best of our knowledge, there is no semantic representation that integrates this information into a unique body of knowledge, expressed through a formal ontology. We do not expect the knowledge base that we build to give a coherent view of Dante’s works. The knowledge in it may, and in general will be incoherent and incomplete, and our ontology is flexible enough to allow both.  The creation of the semantic network is a very time-consuming and knowledge-intensive process. It requires researching the most appropriate ontologies for representing all aspects, and in several cases it requires developing a new ontology to fill the gaps of existing ones. Once the ontology is created, the works of Dante, the primary sources, and the knowledge embedded in them will have to be expressed in this ontology, and this is also a technically demanding task. But the benefits are enormous. The digital representation of the knowledge can support scholars in several conceptually simple but time-consuming tasks, allowing them to focus on the more intellectual aspects of their work. The semantic network will be usable for a wide variety of purposes, which go well beyond the specific services built by our project. It will constitute a backbone that can be enriched with other knowledge about Dante and the historical events, people, artistic movements, etc. that have come across Dante and as such contribute to form the context in which Dante’s life and art took place. In this sense, creating the semantic network is the most important achievement of our project. Our project will build only one part of this network, but will also lay the bases for the extensions and enrichments that will complete what we have started. 4. Why this unified archive will be important tu study the culture of Dante The importance of the archive and tools described above in order to study how the culture of Dante developed in time is obvious. The fact of gathering the current information on the primary sources used by Dante in his works, and the fact of having this information available in digital format, will improve and make more efficient the research of primary sources by the scholars. Having all the information dispersed on paper books, in fact, makes impossible a systematic overview of the culture of Dante and a well-ordered perception of how it was gradually set up in time. On the contrary, the automatic visualization of data about primary resources, according to different parameters (in chronological order, or by type of source, or by author, by work, etc.), will allow to explore the dynamics of the multi-faceted culture of Dante in relation to the diverse and often conflicting stages of his biography and to study the evolution in time of Dante’s cultural background.  ",
       "article_title":"Towards a Semantic Network of Dante’s Works and Their Contextual Knowledge",
       "authors":[
          {
             "given":"Mirko",
             "family":"Tavoni",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Filologia, Letteratura e Linguistica, Università di Pisa",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          },
          {
             "given":"Paola",
             "family":"Andriani",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Filologia, Letteratura e Linguistica, Università di Pisa",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          },
          {
             "given":"Valentina",
             "family":"Bartalesi",
             "affiliation":[
                {
                   "original_name":"ISTI-CNR",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elvira",
             "family":"Locuratolo",
             "affiliation":[
                {
                   "original_name":"ISTI-CNR",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Carlo",
             "family":"Meghini",
             "affiliation":[
                {
                   "original_name":"ISTI-CNR",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Loredana",
             "family":"Versienti",
             "affiliation":[
                {
                   "original_name":"ISTI-CNR",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "computer applications in literary",
          "software studies",
          "the creation of humanities digital resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Venetian maritime empire is the subject of numerous works and monographs (e.g. Ercole 20061, Lane 19732, Luzzatto 19413). This paper focuses on the period between the end of the 13th century and the fall of Constantinople in 1453. During that period the Venetian state set up seven regular shipping lanes, linking the Republic of Venice with the oriental and the occidental Mediterranean basins,  the Black Sea, England and Flanders. Special warships—called galleys—were readapted to perform commercial duties during peacetime on these shipping lanes. Every year, the Venetian Republic organized an auction system—the Incanto—to assign the commercial space on these ships. Subsequently the Senate was in charge of determining the mandatory stopovers, duration of the call, date of departure and date of return to Venice. All of this precise information was recorded in the Venetian official administrative documents.  Several authors have tried to reconstruct the Incanto system from the highly detailed information contained in these administrative documents. In 1961, Tenenti and Vivanti produced a series of chronological maps showing the evolution of the lanes year by year. Unfortunately, their model of the archives is not available for further investigation. More recently, Doris Stöckly extracted from the Venetian state archives—and other sources—a detailed list of all the information related to the ships on a year by year basis. She published her analysis in a monography (Stöckly 19954). The compiled tables appear as appendices to her Ph.D thesis; and are only available in printed form (see figure 1). For this work, we take these printed tables, digitize, automatically transcribe and structure them. We perform new analyses of the structure and evolution of the Incanto system. Our ambition is to go beyond the textual narrative or even cartographic representation to perform a network analysis which potentially offers a new perspective on this maritime system. Method Step 1 : From Printed Tables to Structured Data The first step of our project was the transformation of the appendices into structured data ready for analysis. We scanned these documents and processed each page using a specifically designed pre-processing pipeline, aimed at improving the quality and highlighting the structure of the scanned images. The pre-processing step included several computer vision-based procedures, serving two main purposes: the adjustment of moderate rotations introduced by the scanning process and the removal of noisy components that may disturb the recognition process. To explicit the structure of the table, we elaborated a method based on horizontal and vertical projection profile that automatically fit rows and columns of the document table. This grid was then used in conjunction with Optical Character Recognition Software (ABBYY Fine reader). We extracted 1480 lines of data. Each line matches a galley and includes the following information: name of the line, year, number of ships, stopovers, and optionally duration of stay.   Fig. 1: Excerpt of the extracted data from Doris Stöckly Ph. D thesis appendix.    Step 2 : From Structured Data to Networks We transformed the resulting table into a network. First, we applied a set of rules in order to clean the data. Then, we removed the stops marked as “facultative”. The stops mentioned without any temporal detail were considered as equal to one day—the shortest unit of time. Names of places and geolocations were standardised using a spatial database of Ancient Ports and Harbours based on Harvard’s DARMC5 and the Pleiades data6. We grouped the stopovers under two generic labels for Crete and for Cyprus.   Fig. 2: The 170 years of the Incanto system visualised as a network.  We decomposed—using an R script—the structured table into individual segments made of paired consecutive stopovers. By connecting these directed segments, we created a global directed network encoding 170 years of navigation (see figure 2). The vertices of this network represent all the ports and places mentioned for this period. The size of the nodes is proportional to the sum of in- and out-degree measures of the node. The arcs represent maritime traffic. Two attributes are associated to each arc: one for the year of the trip and another one reporting the number of ships in each convoy.  From the global network, we produced separated subnetworks corresponding to each year of navigation. These subnetworks inherit their attributes from the main network: the number of ships and days. In figure 3, we illustrate evolution and dynamics of the Venetian maritime routes for the three years before and the three years after the Chioggia war (1351-1354) between Venice and Genoa.    Fig. 3: Network visualization of six years of maritime routes before and after Chioggia war (1377-1381)  Network Analysis: Crete vs. Cyprus We focused our investigation on two particular islands located in the oriental basin of the Mediterranean Sea: Crete and Cyprus. After its acquisition by the Venetian empire and for 460 years, Crete was a fundamental naval base in terms of localisation, logistics and safety (Dudan 2006, Major 1989). Cyprus had a similar strategic position; it was an intermediary stop and became part of the Venetian empire in 1489.  Based on the network extracted from the Incanto dataset, we computed a measure of commercial betweenness of the islands of Crete and Cyprus. In figure 4, we show its time evolution in the period comprised between 1283 and 1453. We highlight three patterns emerging from the computation of this measure and interpret them using three events in the maritime history of Crete and Cyprus. The first time histogram contains a blue box encapsulating that measure on Crete between 1344 and 1377. During that period, the maritime traffic density increased because of the reopening of the Alexandria lane, as Crete was the last stopover for all the convoys heading to Egypt. It is interesting to compare this change with the increase of commercial betweenness, as highlighted in the figure 4. In the second time histogram, two red boxes highlight two historical events related to Cyprus maritime traffic. The first one reflects the betweenness of Cyprus as an important stopover on the way to Armenia (1283 - 1338) (Balard 1987). During this period the measure of betweenness naturally skyrockets, as the island had acquired a strategic position as a maritime hub. On the contrary, the second box shows very low measures of betweenness; corresponding to moderate maritime traffic. This was due to the fact that the Senate of Venice reorganised the commercial exchanges by opening a new lane towards Beirut. During this period (1375 - 1444), Cyprus lost its strategic position for maritime activity directed towards Syria and Egypt. One can notice that the re-opening of Alexandria as destination for Venetian navigation (1344) had the opposite impact on the maritime traffic passing through Cyprus and Crete.   Fig. 4: Betweenness of Crete and Cyprus with respect to the maritime traffic (1283 - 1453)  Conclusions and Future Work It sounds like a commonplace to describe the Mediterranean Sea, geographically and historically, as an area of intense exchanges and communications; however the fact is that any visualisations up to this point, when they exist, never went beyond the narration and failed to give a concrete idea of the pace imposed by Venetian navigation over a period of 170 years.  With this work, we go beyond that common way of visualising maritime historical data. First, we have designed processing procedures to automatically digitise data present only on paper documents. Second, based on this digitised data, we modelled the Venetian maritime connections over 170 years as a network. Third, we magnified the network over Cyprus and Crete and extracted a measure of betweenness for these two islands. From a qualitative analysis point of view, we showed the consequences of three historical events with respect to the Incanto system. We are confident that we can apply this methodology to better explain historical events and quantify their influence on the global maritime network.  ",
       "article_title":"A Network Analysis Approach of the Venetian Incanto System",
       "authors":[
          {
             "given":"Yannick",
             "family":"Rochat",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Melanie",
             "family":"Fournier",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Andrea",
             "family":"Mazzei",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Frédéric",
             "family":"Kaplan",
             "affiliation":[
                {
                   "original_name":"EPFL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Content analysis",
          "digitisation",
          "networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  A number of recent initiatives within the DH community promote the design, development, and implementation of digital tools aimed at speeding up, clarifying, or otherwise improving the research practices of humanities scholars. This year, the One Week | One Tool (OWOT) summer institute, funded by the National Endowment for the Humanities, resulted in the creation of Serendip-o-matic, a serendipity engine for digital research. This tool relies on users to feed it a selection of text or citations in order to create a list of keywords, which it then uses to find related information. The documents returned are taken from the Digital Public Library of America (DPLA), Europeana, and Flickr1. The participants of the 2013 OWOT initiative are not alone in their quest to design a digital tool geared toward enhancing the chance encounter with information, resources, ideas, research materials, and even people. Tim Sherratt, the manager of Trove at the National Library of Australia, often includes an element of chance in the tools he designs for use in the humanities. For instance, in his tool Trove News Bot, Sheratt (2013) allows users to interact with a Twitter stream by sending tweets with directions (such as #luckydip), which will return random results from the National Archives of Australia’s digital collection2. Similar tools have been developed that introduce serendipity into the collections of the DPLA and the British Library. One motivation for the development of digital tools aimed at enhancing serendipity in digital environments comes out of the need to redesign and recreate the complexity of the research environment found in library stacks and archival collections. It is often argued that this complexity may be lost in digital environments, which are highly predictable and primarily based on keyword search. To what extent serendipity is reduced in digital search is debatable. Nonetheless, this perception of loss directly affects how scholars, and in particular humanities scholars, adopt and use digital tools. A study of historians’ research practices suggests that these scholars are skeptical of conducting their research exclusively in digital environments because they lack the ability to encounter key resources (primary and secondary materials) that could have a major impact on their research findings 3. In this study, the authors also found that historians were willing to experiment with digital tools, if these could recreate opportunities for encountering information. Hence, scholars perceive the discovery of resources, browsing, and chance encountering as central elements of their research practice that can, and need to, be supported online. Outside of academia, a number of tools have emerged that try to introduce serendipity into the online experience. What is less clear from the literature is how to best support this process, as a wide range of approaches have been suggested ranging from interactions in social media 4, exploration in non-search related digital environments, and information search in digital environments 5. The approach most commonly taken is to introduce serendipity into the online information search experience; this is often done by introducing some element of randomness and thereby reducing the predictability of search results. An example of this approach is BananaSlug, which returns random results to a search query. Other approaches include reversing or modifying the ranking in which search results are presented online 6. This would draw attention to a different set of items because users commonly tend to investigate only the first and perhaps second pages of search results. All of these approaches aim at broadening “the search space, promoting encounters with items that might not, under existing algorithms, come to the attention of the user”. While the majority of digital tools aimed at promoting serendipity have emerged outside of the humanities, a series of tools have recently been developed with humanists in mind. These tools have garnered considerable attention in the field, but it remains unclear what element of serendipity they support. Part of the problem is the fact that the concept of serendipity is elusive 7 and difficult to pinpoint. Reducing it to the introduction of randomness, however, does not seem to be the best way to move forward, even though it is the one most commonly utilized. A second problem, and perhaps more concerning, is that scholars need to first understand that serendipity is not a one-dimensional concept but, rather, includes a number of related facets, which need to inform tool design and implementation. The present paper critically examines four DH tools that encourage serendipitous results and attempts to place these within current models of serendipity:  Serendip-o-matic (http://serendip-o-matic.com/) Trove News Bot (https://github.com/wragge/trovenewsbot) Mechanical Curator (http://mechanicalcurator.tumblr.com/) DP.LA Bot (https://github.com/wragge/trovenewsbot).  As a basis for this examination, we have established the main facets of serendipity obtained from the extensive literature in Library and Information Science (LIS). Through this comparative study, we aim to accomplish two goals. First, there is a gap in understanding exactly what aspects of serendipity digital tools support. By merging the literature in LIS with tool design in DH, we hope to create greater clarity as to what aspects have been supported. Second, the results of the study will determine what future developments are needed to better support the work of humanists in digital environments.  Interviews with 20 history scholars inform the first phase of this study. These scholars indicated a desire for serendipitous encounters with material to remain a part of their research process after the integration of digital texts to their work. After discovering that historians were seeking new methods of information acquisition online, further interviews were conducted with DH scholars to see what methods they were using to browse information. The results of these two sets of qualitative data will be discussed and used to demonstrate a need for a serendipity tool within the DH community.  The second phase of this research is an in-depth exploration of the four information-discovery tools listed above. These tools will be examined in terms of Erdelez’s (2004) model of information encountering outlined below 8. After analyzing each tool carefully, follow-up interviews will be conducted with the creators of each tool to discuss their intentions for and reflections upon, the use of the tool by humanist scholars. A wide range of models of serendipity have been developed relying on very different data sets and assumptions. Erdelez (2004) developed one of the first models and emphasized the experience of information encountering (IE), which she defined as a type of opportunistic acquisition of information. Erdelez’s (2004) utilized an experimental setting, where participants were asked to look for information related to a foreground problem and the researcher observed how they would react to information related to a background problem. As part of her model, Erdelez (2000) identified five elements:  noticing: the perception of encountered information;  stopping: the interruption of the initial information seeking activity;  examining: the assessment of usefulness of the encountered information;  capturing: the extraction and saving of the encountered information for future use;  and returning: the reconnection with the initial information seeking task.  In Erdelez’s (2004) model, a person is primarily focusing on the information needs related to a foreground problem. However, cues related to another problem, a background problem, may catch the person’s attention. If the person notices the cues and stops to examine the newly encountered information, then there is an opportunity for discovering unexpected resources. It is this process of noticing, examining, and capturing that digital tools try to emulated or support.  Each of the four tools reflects one or more aspects of the serendipitous process as outlined by Erdelez, (see Table 1).     Noticing Stopping Examining Capturing Returning    Serendip-o-matic     ✓  ✓        Trove News Bot   ✓  ✓  ✓  ✓      Mechanical Curator   ✓  ✓  ✓  some       DP.LA Bot   ✓  ✓  some       The tools listed above, with the exclusion of Serendip-o-matic, select materials randomly and then present these to followers on Twitter. Randomness, as we know, does not necessarily mean that serendipity will occur. These tools all provide links to places that users can go to receive extraneous materials in the hopes that something of interest will come their way.  Interestingly, the capturing element of these tools seems to be largely disregarded. Considering the DH community is acutely aware of the need to instantly capture digital documents and the associated metadata with citation tools (Zotero), none of the examined tools includes this element in their framework. This leads the authors to conclude that future design could focus on this element of capturing information, and could introduce a method that allows for the saving of documents so users can retrace their footsteps after returning to the initial task or foreground problem. Our critical analysis of various DH tools and how they support serendipity provides opportunity to further enhance these tools as well as a means to design additional tools that can impact the research practices of humanities scholars.    ",
       "article_title":"Designing the next big thing: Randomness versus serendipity in DH tools",
       "authors":[
          {
             "given":"Kim",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          },
          {
             "given":"Anabel",
             "family":"Quan-Haase",
             "affiliation":[
                {
                   "original_name":"University of Western Ontario",
                   "normalized_name":"Western University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02grkyz14",
                      "GRID":"grid.39381.30"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "serendipity",
          "information discovery"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The large-scale digitization of public domain texts carried out in recent years by Google and university libraries offers broader scope for literary-historical research into the development and cultural function of specific literary forms and genres. Traditional scholarship on select canonical texts can now be combined with the computational analysis of large document sets to provide insight into what makes those texts distinctive or representative of larger historical patterns. This paper discusses work in progress from Understanding Victorian Poetic Style, a project that examines how text analysis methods can be adapted to the study of poetics at the large scale. Stylometric analysis has largely been focused on identifying the distinctive linguistic patterns used by particular authors; I’m interested in extending these methods to examine poetic genre and form as shared historical, cultural practices.  To do this requires attending to the mutiple ways that poems create meaning through deliberate structures, such as enjambment, repetition, and rhyme. Recent scholarship in nineteenth-century poetry has returned to the cultural study of form and to the study of historical poetics, which examines the history of theories about poetry’s linguistic textures.1 23 The nineteenth century produced a tremendous variety of metrical, rhymed, and stanzaic verse as well as free verse, which does not follow a set meter or rhyme pattern.  The digitization of ninteenth-century texts now affords the possibility of contributing to our historical understanding of poetic form with large scale analyses of poetic practice. This paper presents my current research into using computational text analysis for understanding the historical practice of enjambment, a key feature of the poetic line. This research contributes both to the project of sociological poetics, the understanding of literary form in its broadest historical function within human culture, and to the development of a computational poetics.4               The poetic line is a distinguishing feature that separates poetry from prose. Like prose, poems contain sentences which can be analyzed syntactically and semantically. But in verse those sentences are arranged in lines. The poetic line is defined rhythmically in metrical verse; is marked through sound in rhymed verse; and is visually reinforced by white space in free verse and indeed in printed poems of all kinds.                                          Lines of poetry are defined to a large degree by their endings: some lines are firmly “end-stopped” by closing punctuation, like a period, and some “enjambed” lines deliberately continue into the line which follows, often by breaking in the middle of a syntactic clause.  Rather than seeing these as simple oppositions, John Hollander looks at the common notation for marking poetic line endings when quoted in the midst of prose (i.e., Wordsworth’s “I wandered lonely as a cloud / That floats on high o'er vales and hills,”) and proposes that we understand enjambment as:          \" . . . a kind of spectrum, along which we would arrange all the possible ways of terminating lines, considered not as boundaries or termini, but as the kinds of cutting into syntax which the slant-dash notation illustrates\" (99).5  Enjambment can thus be understood as one measure of the relation between the poetic line and the syntactic sentence. As T.V.F. Brogan suggests, “The sense in prose flows continuously, while in verse it is segmented so as to increase information density and perceived structure.”6  Poetic style can only be partially understood through “bag of words” or even n-gram analyses, since those words are deliberately arranged not only in sentences but in lines. Developing quantitative measures for this segmentation contributes to an historical poetics that defines form and genre as cultural phenomena.   This paper describes my current approach to computationally analyzing enjambment in a corpus of poetry published in England between 1840-1900. It compares the utility of three different measures of the relation between the poetic line and the syntactic sentence:   (1) a simple line:sentence ratio;  (2) a spectrum definition marking degrees of enjambment based on different kinds of punctuation;   (3) a spectrum definition marking degrees of enjambment based both on different kinds of punctuation and part-of-speech tagging.    These measures are considered as features of poetic style that can be used alongside other features in classification experiments or as markers of historical change in poetic practice. This computational analysis can contribute to our understanding of enjambment as a feature of an individual poets’ style; as a feature of particular poetic forms, themes, and genres; and as a feature of poetic discourse in particular historical periods.  As James Scully suggests:   \"There is no unpositioned, dehistoricized technique – no way to comprehend line breaks in and of themselves. It’s not simply that these are socially specific practices, nor that there are different kinds of line breaks . . . but that line breaks do not work the same way in ballad quatrains as in blank verse, nor in prescribed verse as in free verse. . . Free verse too has a lineage, a historically reproduced repertory of conventions through which it works and to which it responds\" (108).7 The computational analysis of enjambment as it occurs in both prescribed and free verse in the nineteenth century can help us better discover and understand that repertory of formal conventions. By moving the study of poetics to the large historical scale, computational analysis can begin to generate a more detailed historical account of the historical and cultural functions of poetic form.  ",
       "article_title":"Enjambment and the Poetic Line: Towards a Computational Poetics",
       "authors":[
          {
             "given":"Natalie",
             "family":"Houston",
             "affiliation":[
                {
                   "original_name":"U of Houston",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" 1. Introduction How can digital humanities contribute to empowering processes through arts and music1? In this long paper, we aim to analyse Tunisian representations of Europe through images, music and video clips. This in turn forms part of a broader research project funded by the Swiss National Science Foundation that investigates undocumented mobility in the context of recent developments in Tunisia. Even after the events of the “Arab Spring” and its demands for dignity and liberty, the desire of young men (harragas) to “burn their papers” (harga), to leave their country of first citizenship and to reach Europe still persists. However, such a desire to escape one’s overall circumstances cannot be reduced to merely economic motives. Undocumented mobility is by no means a one-dimensional, single-layered process governed by “push-and-pull factors”, but reflects the transnational social imaginaire and its various cultural resources as well. 1.1. OverviewNew media such as social networks, blogs and YouTube, as well as their mobile symbolizations, sounds and images, contribute to the dissemination of mobilization, dissent and disagreement, creating a transnational2 socio-cultural space and public spheres in which current (and past) situations are negotiated and contested3. Based on ethnographical fieldwork in Tunisia and Switzerland, as well as on digital anthropology of social networks4 and blogs5, this research project deals with the increasingly important role of these spaces and thus contributes to a fresh and innovative approach that relates undocumented mobility, (political) mobilization, transnational practices and the (gendered) social imaginaire.  1.2. Methodology What do Tunisian migrants expect of the country they emigrate to? Do they hope to find a job easily, improving their living conditions as soon as they arrive in Europe? In this paper, we aim to reveal migrants’ expectations of moving from Tunisia to Europe. We will take into account the representations of migrants before they leave their country of origin as well as those of migrants already living in Europe. We also seek to understand if these expectations are always the same or if they differ from one person to another. Can we identify certain mental representations that are widespread? By listing semiotic as well as audio-visual representations, we aim to understand what the most prevalent themes and images of migrations are.Do mental representations evolve? When meeting people who recently emigrated to Europe, we will ask them if the way they now see Switzerland is the same as before. Are they satisfied with their new living conditions? Would they rather leave this country and give another country a try – Canada, for example? On the one hand, we will analyse the way people talk and write about these subjects on blogs, Facebook and YouTube. On the other hand, we will interview both Tunisians who have only been living in Switzerland for a few months and Tunisians who have been living here for much longer.  2. Getting Started Our paper will take into account documented as well as undocumented migrations. In many videos posted on YouTube and Facebook, boat people explain why they emigrated. Sometimes the people we listen to are still waiting to leave, and sometimes they have already arrived in Lampedusa and are hoping to continue farther into mainland Europe. Finally, there are videos in which people who have arrived at their final destination tell us what they now think of undocumented mobility. Do they think they made the right choice? In this paper, we will analyse the vocabulary as well as the references they use in their discourse.At the beginning of our research, we planned to analyse audio-visual material available 1) via YouTube and 2) via blogs. Fieldwork conducted in February 2014 has shown that it is more crucial to focus on Facebook, since it seems to have a much wider audience than blogs on irregular migration. The material analysed will be delimited by five key words in Arabic (different spellings will be taken into consideration) and French (with all possible combinations searched: k1; k1+k2; etc.). Firstly, an expert in media and information technology will help us perform computer-assisted content analysis. The second step is to add manual content and discourse analysis (of semiotics etc.) under the leading research question \"How can digital humanities contribute to empowering processes through (popular) arts and music6 (namely Mizwoud7 and rap music8)?\" We plan to conduct a general analysis of power relations later on, but within the limited space of the present paper, we will focus on processes of empowerment and general forms of representations of Europe.Five kinds of video clips will be researched: a) music clips, b) music clips without video but with photos, c) news reporting, d) videos taken by harragas during their trips and e) other types of video. Our research schedule consists of four steps: 1) content analysis of the clips and comments posted by the author(s), 2) analysis of images, representations and symbols used in the videos, 3) analysis of the sound and the music, and 4) analysis of hypertext links and YouTube suggestions leading to other videos. This material will be juxtaposed to ethnography \"in the real world“: participant observation in the harraga milieu, semi-guided interviews and informal talks and life histories in Tunisia and Switzerland.The desire for a better life abroad has been emphasized after the \"Arab Spring\" by the development of a transnational youth culture disseminated via Facebook, YouTube, and Twitter9. Representations of success in Europe also circulate thanks to transmigrants who bring back images and symbols of a higher social status to their home villages. These images and symbols are videotaped, transformed and used in music clips uploaded onto the Internet. New music styles have also emerged as part of a global vernacular language. An example is Mizwoud, which has partly become a language of the political resistance movement in the Maghreb countries. It also belongs to the \"migrating population, and with them travelled outside Algeria into France, then Europe\" (Naïr 2007: 65). Hence, cultural resources10 have given rise to the development of a common artistic language of exclusion, dreams, representation of a better life and resistance11. Videos, soundscapes and images bridge the gap between the street, the sea and the virtual, empowering highways and gates through which multiple belonging processes12 take place.  ",
       "article_title":"Digital Humanities Empowering through Arts and Music. Tunisian Representations of Europe through music and video clips",
       "authors":[
          {
             "given":"Monika",
             "family":"Salzbrunn",
             "affiliation":[
                {
                   "original_name":"ISSRC-UNIL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Simon",
             "family":"Mastrangelo",
             "affiliation":[
                {
                   "original_name":"ISSRC-UNIL",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This paper focuses on the history of the English lexicon, and on displaying a new approach to this history through the database of the Historical Thesaurus of English1 (hereafter abbreviated to HT). It does so by reference to the semantic space of English, following Lehrer’s statement that ‘the words of a language can be classified into sets which […] divide up the semantic space or the semantic domain in certain ways’. This space is described in this paper as the total accumulation of the various individual semantic fields which make up the language, as represented in the HT database. The paper therefore computationally analyses the size of the English lexicon in these semantic clusters over time, including the metaphorical links which weave between these fields, and so aims to demonstrate the use of the HT in digital humanities by giving a digital analysis of the history of English in ways which were previously not possible. As part of two wider projects,23 the present paper focuses on describing the general empirical outlines and development of the English semantic space, accompanied by a case study of three contrasting semantic fields and their metaphorical relationships. These are outlined below, following a description of the methodology and theoretical basis of the paper.   2. The Historical Thesaurus and Lexical History The data used in this paper is drawn from the database of the HT, which arranges into hierarchical semantic categories all the recorded words expressed in English from Anglo-Saxon times to the present day, with 793,742 entries within 225,131 categories, each category representing a distinct concept. These concepts are arranged hierarchically and semantically, so that each concept is placed near or within other, similar concepts. In so doing, the HT unlocks the linguistic and historical data which is currently inaccessible in any usefully-structured way inside historical dictionaries such as the Oxford English Dictionary (OED)4. As Charlotte Brewer says, with reference to a review in the Times of the OED: \"...even the intensively habitual user [of the OED] could not hope to construct, from an overwhelming multiplicity of individual items, the complete picture, ‘the various forms of [...] civilization arranged in chronological strata’...\"5 Alphabetical arrangement, absent any alternative structure, makes this construction incredibly difficult, if not impossible. But the HT, which structures itself based on meaning and not the alphabet, does give researchers access to this ‘complete picture’. This was one of the intentions of the HT from the beginning: Professor Michael Samuels, founder of the project in 1964, saw it as a way of revealing the information about social and cultural change inside and throughout the lexicon which was not easily available for researchers to access.6 The HT is therefore a massive digital resource for the study of this phenomenon.   3. Semantic Space Key for the first part of this paper is that the HT, when analysed in database form, gives an indication of rates of lexicalisation in the history of English. This relates to the phenomenon of synonymy, a situation in a language where a number of words are created (or lexicalized) for a single concept (for more on the following discussion, see, amongst others, Lyons 19957, Verhagen 20078, Hughes 19899, and Taylor 200310). While synonymy is a common occurrence in English, as in many other languages, the linguistic insight that synonymy is a form of recategorization, where speakers create a new synonymous term because they wish to reflect a shift in their understanding of, or attitudes towards, a particular concept, allows the use of data on lexicalization rates as an indicator of particular speaker attention to a given concept. Therefore, a situation where there are multiple words for a given concept reflects the evolution of speakers’ reactions, attitudes, perceptions and awarenesses of that concept, as human language is too efficient a system to permit there to exist large sets of undifferentiated terms which mean precisely the same thing. The present paper therefore uses this measure as a rough proxy for importance of a concept (just as frequency is used as a similar measure of importance in corpus linguistics, with all the associated issues that varying corpus construction techniques brings with it). Therefore, for the first time, the database of the HT can give us an empirically based view of English by viewing the changes in the internal structure of the language from an entirely semantic viewpoint. By separating the story of English into the multiple stories of interacting and interrelated semantic fields, this approach can describe the history of English as one generally characterized by overall growth accompanied by occasional trauma which results in sudden expansions or contractions of the English lexicon. The rate of change of each semantic field is therefore a statistic which demonstrates the incidence of such instances of trauma, growing or declining in response to external and internal factors either particular to a semantic field or general to the language as a whole. Such general factors in English include the well-known sudden growth in the mid-1400s which occurs at the start of the English Renaissance, and the Elizabethan and Jacobean spurt which begins a little after 1550, which can be seen in Figure 1:   Fig. 1: The growth of the English language across time, as recorded in the HT.  In addition to presenting an overview of the growth of the semantic space of English between the years 1100 and 2000, the paper will also give short case studies of three aggregate semantic fields (figure 2) and their metaphorical relationships (see section 4 below):   02.01.15 Attention and Judgement, a very large and highly variable category, with an increase of 1000 words in the 1575-1600 period, but a fall of 261 in 1875-1900. 03.10.13 Trade and Commerce, a category which is relatively small but has one of the highest rates of growth spurts, punctuated with long plateaus.  03.05.05 Moral Evil, a category which peaks in 1650 and is one of the rare examples of frequent decline across the history of English, with a loss of 246 words between 1650 and 1900.    Fig. 2: The growth of three semantic fields. Square: 02.01.15 Attention and Judgement; Circle: 03.05.05 Moral Evil; Triangle: 03.10.13 Trade and Commerce.  Each of these reflect both global trends in the history of English (such as those above, in addition to relative plateaus in the 1700s) while also containing their own internal factors, such as shifts in religious emphasis and in broader economic and industrial patterns. Not all of these factors are expected; there is no mention in the literature of the rise and fall of lexicalization in the semantic field of Moral Evil, nor in many of the other unusual patterns in the rate-of-change data described in this paper. The new data described here gives rise, in the tradition of digital humanities, to the necessity for further explanations from a range of humanities disciplines, such as linguistics, history and literary studies (see Alexander and Struan 201311 for an interdisciplinary study in a further semantic field).   4. Metaphoricity Beyond these rates of change, each semantic field above has metaphorical links to other areas of the language, which the HT can reveal to us. Far from being a solely literary technique, much of all language is figurative – recent research has shown somewhere between 8% and 18% of English discourse is metaphorical, with an average of every seventh word being a metaphor.12 This is problematic, as while advances are being made in the semantics of digital texts, alongside emerging concepts of a semantically-aware Web, we are at a very early stage in comprehensively and systematically understanding English metaphor, and therefore at an early stage of being able to accurately deal digitally with the meanings encoded in those texts. By mapping the HT's semantic categories onto one another in order to analyse the degree of lexical overlap in different conceptual fields, we can provide results which will comprehensively demonstrate the widespread, systematic and far-reaching impact of metaphor on English. This is the aim of the Mapping Metaphor project at Glasgow,13 which provides some of our data in this paper, demonstrating empirically the systematic lexical connections between our case study fields (such as that between attention and vision, or evil and darkness).   5. Conclusion Overall, as well as giving an overview of the history of the English semantic space and its metaphorical interrelationships, the paper also argues for a semantically-informed history of English which operates from a top-down approach, picking out broad patterns and the connections between various semantic categories in order to highlight for analysis those noteworthy elements in a large sea of data. As ever, such large-scale analyses are only possible through a combination of database techniques, statistical analysis, visual displays of complex datasets, and humanities scholarship.  ",
       "article_title":"\"Civilization arranged in chronological strata\": A digital approach to the English semantic space",
       "authors":[
          {
             "given":"Marc",
             "family":"Alexander",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Wendy",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The paper introduces the disciplinary developments in Estonian humanities that are intertwined with use of digital resources and methods. Estonia forms an interesting case. On the one hand, the post-Socialist country is well-known as an example of technological innovation, it was the first to introduce paperless governmental processes, the electronic ID-card is compulsory document and its electronic functions are widely used, to name a few examples.1 However, at the same time, the developments in the field of digital humanities have not taken place at the same pace with the ones in USA or Western Europe. The aim of this paper is to compare and contrast the use of digital technologies in Estonian humanities to the developments of the field of digital humanities. 2. Use of the digital technologies in Estonia In 2012, 78% of the population of Estonia aged 16-74 years used the internet. There are various electronic services, like e-Tax Board where 93% of income tax declarations have been made in 2012. From 2005, it is possible to participate in elections electronically. In October 2013, 21,2% votes in the local elections were given electronically. And what is more, 62% of the inhabitants took part of the 2011 Population and Housing Census electronically.2 However, the use of digital solutions is not a spotless success story: according to PIAAC (Programme for the International Assessment of Adult Competencies) study in 2011-2012, 30% of 16–65 year old test group refused to take a part of the test online and 13% did not succeed in the simplest computer-related tasks. The results of PIAAC study reveal a digital divide that is related to the linguistic divide: there are no significant differences in gender, education or the social background, but Estonian-speaking group had in general better skills in the use of technological tools than the group of the speakers of other languages.3  Apparently, Estonian society is not able to provide all its IT-advantages in the languages other than Estonian enough to integrate the non-Estonians to the Estonian IT-world, thus leaving them isolated to a certain extent. Despite being the wired country in terms of the e-Government, not all fields of life have been equally keeping up the developments in the IT, the humanities being one of these areas. The National Strategy for the Development of Information Society until 2020 addresses the problem of inequality and of moderate specific IT skills in the fields other than ICT.4 3. Field of digital humanities in Estonia  Schnapp and Presner 5 have distinguished between two waves of digital humanities: quantitative and qualitative one. According to the divide, the first wave was about digitisation projects and creating infrastructure, the other wave consists of interpretation and research methodologies for digitised and born-digital materials. Taking this approach as a framework, Estonia could be described as being in the first wave of digital humanities. In archives and libraries, there have been large-scale digitization projects from early 2000s onwards, the digital archival systems and repositories have been created and made publicly available for many different collections.   For example, in Estonian Literary Museum, a file repository and archival information system Kivike6 has been created to manage the collections to store the digital materials and metadata, and to make the collections as much as possible publicly available via Internet. New archival data is being added to the online repository and described there, among other materials the born-digital sources (photographs, audio and video recordings, as well as “digital manuscripts”) that have been collected systematically since 1994.   This illustrates the tendency of including some of the notions of second wave of digital humanities, because the emphasis on preserving of born-digital materials has also been apparent in several institutions. It is partially connected to the practical needs of document management in a country where governmental practices have moved online. The digital preservation department in Estonian National Archives was founded in 1999 with the goal of “permanent preservation of digital data despite the changes in society and technology”. Web pages are also digitally archived: Estonian National Library is in charge of creating and maintaining a web archive of the web resources that are important for the Estonian culture. For born-digital materials, these approaches still focus on creating an infrastructure rather than using the data. Creating and maintaining the collections have been in the centre of digital humanities projects in Estonia, using them for educational or research purposes is scarcer. One of the most clearly developed disciplines that uses digital data sets and methodology is linguistics. There has been a constantly developing co-operation of the linguists and computer-scientists since the 1950s in the field of language technology, there has been formed a specific curriculum of computational linguistics at the University of Tartu in the 19977, large corpora and dictionaries are in constant progress and in 2006 the special national financing program has been founded in order to promote the development of language technology and digital language resources.8   Except of the linguistic studies, the use of digital methods in the humanities research has been somewhat sporadic and depends of the interests and skills of individual researchers. In the curriculums of humanities there are no comprehensive courses on digital research methodology (the social sciences with its traditional data analysis are in much more better situation here). In some of the humanities fields digital methods are being taught in specialised courses. For instance, students of archival science need to follow the course on digital preservation. To sum up the situation in Estonia, the digital infrastructures for the major humanities collections in Estonia have been created by now or are in progress. There are several original web solutions for presenting and/or collecting the humanities data, a couple of successful examples of crowdsourcing have taken place. The creation of the software for the audience in humanities has been more than modest. Because of the linguistic restrictions, and the small size of researchers' community the specific software (e.g. for the analyse of the Estonian folk song melodies) would be tailor-made for the needs of (the group of) individual researchers and is not developed for the public (or multilingual) use. Use of the digital methods in the research has been constantly increasing, but only occasionally, the lack of systematic technological knowledge base seems to be crucial. All this does not apply for linguistics. However, the research mostly takes place within the disciplinary boundaries and the collaboration between the digital humanists of different disciplines has been modest, without signs for the need of the umbrella discipline.  Only in October 2013, first seminar of digital humanities in Estonia took place providing an overview of the projects in progress within the field.9 Different fields of humanities were represented: linguistics, archeologists, literary studies, folkloristics, arts. In the presentations, digital humanities in general got little attention, most of the presenters focusing on digitizing, maintaining and/or presenting the collections (either digitized or born-digital) rather than interpreting the data. In addition to this a couple of research projects were presented, as well as an technologically innovative film project, two software projects for documenting the artefacts, and a few initiatives of using open data in web projects. It is clear, however, that the seminar did not involve all the digital humanities projects in course. For example there was no paper from Estonian National Archives, although it is serving as the competence center in the field of the digital preservation in Estonia, and there are several IT projects ongoing.  After the seminar an informal network was created with a mailing list and homepage. According to a small survey taken in early 2014, the humanities scholars who work with digital technologies feel the need for collegiality in the field, i.e for wider digital competence among their colleagues both for developing new tools and using the already existing ones.  4. Discussion Digital humanities became a discipline because the humanities scholars as users of digital resources needed to understand the digital mechanisms and get some academic recognition for using them. One of the crucial questions of the field is how to bring the technological advantages and knowledge within the reach of researchers so they could develop the tools and environments they need. To code or not to code, that is the question, when being probably the only person in the world who would need a language-specific tool, or if the audience of a web environment would consist of mere ten people. The size of the linguistic community restricts the reasonable amount of work hours spent for the IT solutions or web-pages. On the other hand, the available English-language software is often either not known or not easily applicable for several reasons. This is how the linguistic divide becomes the digital divide even in the country with well-established technological infrastructures. The awareness and the use of various IT solutions in Estonian society with its linguistic divide between Estonians and non-Estonians follows the model of the situation in the digital humanities leaving the peripheral group to the isolation to some extent due to the language barrier. 10  ",
       "article_title":"Digital humanities in Estonia: digital divide or linguistic isolation?",
       "authors":[
          {
             "given":"Mari",
             "family":"Sarv",
             "affiliation":[
                {
                   "original_name":"Estonian Literary Museum",
                   "normalized_name":"Estonian Literary Museum",
                   "country":"Estonia",
                   "identifiers":{
                      "ror":"https://ror.org/02yewpr08",
                      "GRID":"grid.454918.5"
                   }
                }
             ]
          },
          {
             "given":"Kaisa",
             "family":"Kulasalu",
             "affiliation":[
                {
                   "original_name":"Estonian Literary Museum",
                   "normalized_name":"Estonian Literary Museum",
                   "country":"Estonia",
                   "identifiers":{
                      "ror":"https://ror.org/02yewpr08",
                      "GRID":"grid.454918.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The use of social media tools within digital archaeology helps create an engaging setting for archaeological content, which integrates archaeology into a broader social context of use by connecting scholars, archaeological heritage professionals, and the wider public. Social media offers various opportunities: researchers may find it useful for discovering, using and sharing information, organizations may use it in promoting institutional agendas and communicating with wider audiences, lay people may see it as a platform for participation, and more. More broadly, social media is transforming ways in which we perceive information, with its use becoming an increasingly important skill for researchers. This paper attempts to address issues of social media usage in digital archaeology through the case study of studying Lithuanian archaeology practices on Facebook. The study of social media use in archaeology is still new as a topic of research, because social media haven't been around long enough to develop clear patterns of use. However, particular research questions, as well as answers, are emergining (e.g., Morris, 2011; Whitcher Kansa & Deblauwe, 2011; Pett, 2012; Richardson, 2012; Sanchez, 2013); nevertheless, discussions of social media in archaeology are still more often discussed in conferences and seminars, and also on blogs, forums, etc. Authors typically acknowledge the importance of social media, and point to successful examples providing evidence that it stimulates communication between researchers, helps information sharing and reaching wider audiences, as well as fosters community engagement and social participation. However, the diversity of existing practices opens new research questions, transcend disciplinary boundaries and challenges established authority structures. The research project presented here is a case study of archeaological communication on Facebook (currently the most popular site for digital social networking in Lithuania), based on analysis of empirical data from thirty Lithuanian Facebook groups and pages related to archaeology. The study depends on a mixed methods approach, combining digital ethnography, content analysis and social network analysis aspects. Initial analysis revealed that overall activity relies on engaged communities rather than on research institutions, or custodian archaeological organizations, considered to be directly responsible for the creation and curation of digital archaeological content. The scope of the research covers, therefore, a wider landscape of observable social media practices, by actors including not only research organizations or professional networking groups, but also semi-formal or informal groups. Its objective is to map and understand existing trends, and to provide further insights about new phenomena that emerge from these kinds of interactions. The paper investigates Facebook profiles of individual users (archaeologists, amateurs) and organizations, specific activities they engage in such as posting, commenting, liking, sharing, etc., and the content that is shared within the network. It seeks to address questions arising from this case study, as well as develop insights for broader research issues, such as: • Who is using social media in archaeology, and for what reason and purpose? What are the qualitative traits, and in depth profiles, of of the most effective users? What is the nature of the shift towards public archaeology and community engagement practices? What is the role of individual archaeologists in social media? Could Facebook contribute to research proper, or be used for academic purposes? • Do social media shape and change the content itself? How do people use and make sense of these resources? What are the most common kinds of archaeological objects that people share, like and comment on Facebook., and why? How is content influenced by the complex relation between archaeological heritage and society? What is the balance between expert knowledge and amateur perspectives? • More generally, are we fully aware of the opportunities and challenges brought by social media? What additional value does communication among individuals and institutional structures create? How does this kind of synergy improve knowledge transfer? Does it empower organizations, or people? How is communication carried out? What is the structure of interactions between users? In what way is Facebook-based activity shaped to satisfy the needs of its users?  This paper will, firstly, provide an overview of Facebook use in archaeology by focusing on three core dimensions: users, content and communication. It will then present a detailed composition of Facebook users in Lithuanian archaeology, in an attempt to understand the position of archaeological institutions and archaeologists in social media, as well as reasons for the lack of participation as the case study suggests. Furthermore, it will describe the main types and subject-matter of current digital archaeological content, and discuss how user responses and interactions could influence the way in which we conceptualise and interpret the past. Finally, the paper will present and compare different cases of archaeological Facebook use, and will examine in what manner archaeological heritage operates in digital social media, how it serves institutional and individual needs, and what criteria could enable successful communication.   ",
       "article_title":"Archaeology in social media: users, content and communication on Facebook",
       "authors":[
          {
             "given":"Ingrida",
             "family":"Vosyliute",
             "affiliation":[
                {
                   "original_name":"Vilnius University Faculty of Communication",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "Archaeology"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Overview  This project employs machine learning and other text-mining based clustering techniques to study the relationship between taxonomic systems for categorizing prose fiction in the eighteenth century. Our goal is to challenge narratives of the so-called “rise” of the novel (Watt, 19571; McKeon, 19872; Richetti 19993) that trace a continuity between the prose fiction that emerged in the eighteenth-century and the novel as a fully realized critical object in the late nineteenth century. Our project instead uses statistical modeling methods to theorize an alternate history for the novel: not, as proposed in Franco Moretti’s Graphs, Maps, Trees4, as a history of genres, but instead as a history of genre labels--that is, a history formed by the interaction between paratextual, self-identified literary labels that circulated in the eighteenth-century marketplace and the texts that they defined. Our project, which is currently in its final stages at Stanford University’s Literary Lab, bridges the historical and computational to enact a form of literary history that puts pressure on traditional theories focused on charting the rise of the novel. The project is deeply invested in using digital and statistical methods to excavate the nuances in the ways in which eighteenth-century genres evolved. Instead of analyzing texts via contemporary genre categorizations, we recover the relationship between the self-applied taxonomy of eighteenth-century prose fiction and the lexical and semantic features of the texts themselves to recuperate a historically-determined literary field that has been largely overlooked (or noted and elided) by modern criticism. Drawing from a corpus of 2,385 digitized texts from the Eighteenth Century Collections Online (ECCO) database, we ask how new computational methods can aid us in uncovering what kinds of work self-identified genre labels in titles (specifically “story,” “history,” “tale,” “letters,” “romance,” “life,” “adventure,” and “novel”) accomplished within the literary marketplace of the long eighteenth-century, and how that kind of work is distinct from critically- or retroactively-designated genres (“gothic,” “jacobin,” “epistolary,” “historical,” “it-narrative,” “oriental tale.”)  Aims Our project seeks to answer a series of critical questions about the taxonomic systems of eighteenth-century fiction using computer generated models. Do generic labels formally differentiate separate kinds of writing in a useful way (such that a “novel” is lexically, semantically or formally different from a “romance”) or do they merely function as signals within the marketplace itself, so that there is no textual or formal differences between a “tale” or a “story” beyond mere marketplace convention? How does the relationship between different genre labels change over time, and how can it assist us in understanding the evolution of titles as a representative labeling system? This approach takes advantage of recent advances in probabilistic modeling to recover the meaning of labels that have been homogeneously condensed under the rubric of novel; it simplifies the complexity of the word “novel,” with all its attendant genres and subgenres, to make the field that “novel” inhabits more complex. Developing from these research questions, our method-driven aims are: (1) to detect and assess large-scale trends in the development of and relation between genre labels across time (2) to isolate formal differences in the corresponding full texts identified under these categorical labels, and (3) to compare these differences to a corpus of texts categorized according to modern genre designations.  Methods and Narrative   In the early stages of our project, we began with an exploratory approach to our data, using statistical, unsupervised learning techniques to identify word patterns and clusters within the texts, prior to classifying the texts’ content onto the label categories. Our initial approach sought to determine if some underlying structure existed among these texts that we could then map onto a taxonomic system, such as genre, or in our case, title labels. Using a series of clustering techniques built off of different feature sets (most frequent words, most distinctive words, etc), we found that the assumptions that provided the foundation for our project were vindicated: not only did the titles under each label cluster together based on the Most Frequent Words in each text, but there were intriguing differences between clusters. For instance, though a coherent cluster of “history” is present from the first quarter of the century, definitively clustered labels “novel” and “romance” don’t emerge until later in the century. These differences opened our research to a new set of concerns, such as the relative stability of each category, or the ways in which the labels individually and collectively change across time. Indeed, a major finding of this phase was the dramatic influence time itself had over every other variable we considered. Such a finding strengthened the presence of a literary-historical component of our research and prompted us to pay closer attention to the divisions of time we were employing. In our current work, we have employed supervised learning techniques and machine classification to specifically interrogate the relationship between text and label: which labels have a high level of cohesion, and thus predictability, and which labels are less cohesive, and were perhaps more tolerant of formal deviation and experimentation? To answer this question, we employed a discriminant function analysis to determine if texts could be reliably algorithmically classified into their labels, treated in our analysis as a taxonomic system. These results were validated through a leave-one-out cross validation to determine the strength of our predictive model. . The results confirmed the findings of the PCA: our model performed better than chance, categorizing each of our labels, on average, 75% correctly. Another dimension of our study was to investigate differences on a label-by-label basis: what does it mean, textually, that a “life” is harder to algorithmically classify than a “romance”? Did difficulties in categorization have anything to do with the textual heterogeneity within the labels themselves? These questions prompted us toward another set of analyses designed to evaluate the self-similarity of each label. We divided each text in our corpus into ten equal parts, measuring the distance from each part to the other in a matrix that, when averaged, resulted in a distance score for each. The findings from this activity helped to both clarify and complicate the story that was emerging regarding labels over the course of the century. Our most recent work has taken us in a more broadly comparative direction: to examine the textual and larger structural differences between our current corpus of texts, classified according to generic labels, and another corpus we have compiled from modern, genre-focused bibliographies. Using the same techniques of machine learning and self-similarity, we have started to interrogate the logics by which generic labels organize and describe the literary field of the eighteenth century in ways we have yet to discover. Results  Our initial results proved the value and worth of studying this type of paratextual literary categorization. The temporalization of data allowed by our method lets us observe, first, the simultaneous dominance and differentiation of “novel” and “tale” at the end of the century, leading us to speculate, as Anthony Jarrells (2012)5 has, on twin “rises” of genre in this century as opposed to one rise; that is, the eighteenth-century appears to be as much a story of the rise of “novel” as it is of the rise of “tale.” The result of our machine learning classifications and self-similarity form a compelling argument for the ways in which, throughout the eighteenth-century, the field of prose fiction underwent a transformation through the processes of both differentiation and consolidation. From these results, we argue that the novel can be viewed as merely one branch, rather than the single formal end to which all eighteenth-century prose fiction trends. Other important results, observed from our PCA charts, indicate a general movement across all generically-labelled texts from the usage of words that indicate exteriority (such as “law,” “city,” “army,” “money,” “order,” and “public”) to words that indicate interiority (such as “lover,” “marry,” “imagine,” “woman,” “write” and “read”). In the final stages of the project, we are working to apply similar computational tools to genres conventionalized by modern criticism -- such as the Gothic novel or Oriental tale -- to examine how such categorizations relate to self-applied genre labels.  Preliminary results indicate that, after sampling our texts and controlling for time period and author, generic labels classify with a success rate comparable to texts designated as such by critics. The question then becomes not whether genre labels organize more successfully than conventional genres, but whether and how they organize differently--if generic labels can tell us something about the trajectory and organization of formal literary history that genre cannot. Our presentation will include a more detailed discussion of our methods and results and of the implications of the latter for traditional teleological narratives of eighteenth-century prose fiction  ",
       "article_title":"The Cryptic Novel:  A Computational Taxonomy of the Eighteenth-Century Literary Field",
       "authors":[
          {
             "given":"Mark",
             "family":"Algee-Hewitt",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Laura",
             "family":"Eidem",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Ryan",
             "family":"Heuser",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Anita",
             "family":"Law",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Tanya",
             "family":"Llewellyn",
             "affiliation":[
                {
                   "original_name":"Stanford University",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "data modeling",
          "English studies",
          "content analysis",
          "data mining"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction  This paper will argue that removing high-frequency, low-semantic weight words from topic models of poetry corpora improves the coherence of Latent Dirichlet Allocation (LDA) topics and addresses reasonable concerns by some literary scholars that removing such language undercuts the methodology’s value as a mode of literary inquiry.  Exposing technical and theoretical decisions made while topic modeling 4,500 English language poems, this paper demonstrates how words such as “look” and “saw” remain influential and semantically present in document to topic distributions. Finally, it suggests that literary scholars will need a different hermeneutic approach to topic models of poetic corpora that better accounts for ambiguity and figures of speech.  Background Ekphrasis—poems to, for, and about the visual arts—offers a wealth of opportunities to ask familiar humanities questions about canon-formation, literary tradition, and genre definition, and at the same time affords avenues for the advancement or refinement of methods and tools in the field of digital humanities.  The story of the ekphrastic tradition, and women’s relationship to that tradition, is in many ways the story of data collection and curation.  In his influential essay on the genre, W. J. T. Mitchell radically shifts critical studies of poetic engagements with images away from metaphorical comparisons by arguing that ekphrasis activates historical and ideological oppositions between the linguistic and spatial arts as a staging of anxieties about “otherness.”1  Mitchell goes on to explain that the “treatment of the ekphrastic image as female other is commonplace in the genre” (164).    To date, Mitchell’s theorization of ekphrasis as social contest remains a powerful influence on our critical approaches to how the genre operates because it pushed beyond previous studies that simply compared the two arts formally.  Mitchell’s “cannon,” however, consists of four poems, all by male poets: Wallace Stevens’s “Anecdote of a Jar;” William Carols Williams’s “Portrait of a Lady,” John Keats’s “Ode on a Grecian Urn;” and Percy Bysshe Shelley’s “On the Medusa of Leonardo Da Vinci in the Florentine Gallery.”  Though recent scholars--such as Elizabeth Loizeaux2, Jane Hedley3, and Barbara Fischer4—have challenged the limitations of Mitchell’s model, two challenges have stymied its revision: identifying genre conventions as succinct and intuitive and surveying a much larger collection of ekphrastic examples.   Literary scholars respond to questions of genre by creating models.  For example, when Mitchell describes the “suturing of gender stereotypes” onto the “interworkings of ekphrasis,” he does so by creating a model he calls the ekphrastic triangle.  Mitchell’s triangle stages a relational exchange between a poet/speaker,  a feminized art object and the reader, in which the speaker instructs the reader to “look” and “see,” cautions him against silence and stillness, and confides a desire to ravish the feminized image. This presentation demonstrates that ekphrasis's re-deployment of multiple discourses expands our model of ekphrasis as a network  by situating individual poems among multiple, ongoing, and constantly changing discourses within the topic model.  Opportunity Computational tools, such as topic modeling, have the potential to help literary scholars redefine the limits of reading distance, not because they read better than humans but because computers compute better than humans.  Soon after he first coined the phrase “distant reading,” Franco Moretti claimed that distance is a “condition of knowledge”—one limit of many.5 6 Leveraging the strengths of technology to broaden the reach, scale, and scope of our exposure to ekphrastic poetry improves our capacity to view the ekphrastic tradition on a much larger scale. Although the history of ekphrasis—as the hostile, gendered contest between speaking male subjects and silent female objects—seems particularly inhospitable territory for women poets, many acclaimed poems by women in the 20th century participate in the genre, including Elizabeth Bishop's \"Poem,\" Anne Sexton's \"Starry Night,\" Jorie Graham's \"San Supolcro\" and Elizabeth Alexander's \"The Venus Hottentot.\" Topic modeling’s generative, probabilistic, and non-semantic methods offer a promising opportunity to revise our critical understanding of ekphrasis.  Removing words such as \"see,\" \"look,\" and \"say\" from ekphrastic poems offers opportunities to refocus our critical lens on other language patterns that have been overlooked by human pattern recognition due to the high frequency of \"look\" and \"see\" throughout the ekphrastic canon. Methods This paper asks: if we can discern salient questions about the ekphrastic tradition that computer reading is designed to address, how might we respond to Adrienne Rich’s call for “re-vision,” whereby learning to see what we already know differently is an act of survival?  Since we know that computers are not the same kind of readers that humans are, it is important to be aware of the “conditions” that shape the ways computers and humans read.  Questions that require attention to quantity, scope, and scale are particularly suited to computation, and computation helps adjust the aperture on the lens that a scholar can have of a corpus of texts.  The challenge, however, is to continue to refine our understanding of what questions might be most fruitfully asked with an awareness of computational and human conditions.   In the study of ekphrasis, for example, interpretive stress is placed on high-frequency words that, in prose, hold relatively little semantic weight--particularly words such as \"look\" and \"see.\"  Distinct for its highly concentrated language, poetry places an increased degree of significance on even the poet's \"smallest\" word choices.  Preprocessing texts for topic modeling in Mallet strips documents of upper and lower case letters, removes line breaks and enjambments, deletes high-frequency words, including articles, prepositions, pronouns, conjunctions, and common verbs--like \"is,\" \"are,\" and \"were\"--and turns documents into strings of sequential words that no longer bear the same syntactical relationships they once did.  Given this, how can a methodology that requires radical decomposition of a poem's linguistic meaning offer valuable insights into exploring texts? For example, heavy with subtext, the first line of Robert Browning's \"My Last Dutchess\" would be removed in its entirety from the text of the poem through preprocessing using the default stop list available in Mallet.7  If the computer doesn't value such words, could the model it produces still be useful to someone interested in ekphrasis?  To determine whether or not LDA could still produce models that would be useful to the study and revision of the ekphrastic tradition, four experiments were performed on a dataset of 4,500 English language poems using four different preprocessing techniques: 1.) removing no words before creating the model 2.) removing only 50% of the Mallet stopwords 3.) removing all the Mallet stopwords except a small group that relate to ekphrastic conventions (eg. look, see, saw, seen) and 4.) removing all the words on the suggested Mallet stoplist. After preprocessing, each dataset was treated identically, producing 40 topic models. This paper addresses the decision process for assembling each list and points to where the lists can be found online.  Discoveries Contrary to expectation, the model with the greatest topic key word distribution coherence was the model with the most stopwords removed.  Although ekphrastic poems beseech their readers to \"look\" and to \"see\" more clearly, the ekphrastic poems themselves surface more coherently in models where the words \"see,\" \"look,\" and \"still\" are removed.  Like ghosts in the model, similar topics focused on looking and describing form even when specific words referring to the activity are no longer present, and the model's prediction of topical distribution more accurately reflects human estimation of the numbers of \"ekphrastic\" versus \"non-ekphrastic\" poems were included in the dataset to begin with.  In fact, topics in the lightly edited stoplist test are reflected in the model where all stopwords are removed, but the list of most likely words associated with each topic is less coherent in the former than the latter's keyword list.   Using specific examples of topics created with models using various stopword lists, this paper tells the story of ekphrasis as it is told through stopword filters, as topical coherence rises, the questions one might ask about the corpus changes. Concentrating on terms such as \"still,\" \"look,\" and \"see,\" this paper will demonstrate how LDA identifies topics where issues of vision, description, and color become refined as the words that directly refer to the act of observation are removed.  Visualizations of relationships between poetic topics and topic word and document distributions will also reveal the ways in which latent patterns of words that have been removed from the corpus can still be evident in topic formation. Copies of the model keyword distribution lists and document to topic distributions are available by request. Conclusion  This paper uncovers the complementary theoretical and methodological decisions required in order to approach questions of tradition and canon formation with topic modeling corpora of poetry.  An act of critical \"deformance,\" topic modeling uncovers differences between Keats' \"still unravish'd bride of quietness\" and Carol Snow's \"Positions of the Body\" even without many of the words that scholars have argued were critical identifying features of the genre.  Such discoveries prompt us to return to close readings of ekphrastic poems with new questions about the conventions of a genre evident in Western letters since Homer's first description of Achilles' shield in the Illiad.  ",
       "article_title":"The Story of Stopwords: Topic Modeling an Ekphrastic Tradition",
       "authors":[
          {
             "given":"Lisa",
             "family":"Rhody",
             "affiliation":[
                {
                   "original_name":"Roy Rosenzweig Center for History and New Media",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "knowledge representation",
          "genre-specific studies: poetry",
          "literary studies",
          "cultural studies",
          "data mining / text mining",
          "project design",
          "art history",
          "english studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The open-access, multimedia, peer-reviewed journal Southern Spaceswill soon begin its tenth year.1 Published by the Emory University Libraries, Southern Spaces is an online-only journal of critical regional studies that takes as its subject the real and imagined spaces and places of the US South and their global connections. From its years of encouraging digital cultural empowerment as a strategy to transform scholarly publishing, Southern Spaces offers a case study for inquiring into the effectiveness of a project intent upon increasing participation in the creation, dissemination, and curation of humanities scholarship. Given the entrenched commercial clout of conventional scholarly publishing and the slow-to-change institutional structures for tenure and promotion that still resist digital humanities scholarship, how can we assess the significance of this project of digital cultural empowerment? And how do the years of experience in publishing Southern Spaces contribute to an understanding of broader movements of cultural critique and social change?  Writing about the future of digital scholarly publishing in Educause Review Onlinein 2013, Edward Ayers points to several examples of “acceleration into a full, digital-only environment.” \"Scholars, libraries, and professional organizations in my own field of American history are sustaining innovations in online journals such as Southern Spaces and the Journal of Southern Religion and in digital meeting places such as Common-place and History News Network (HNN). These projects bridge traditional practice and digital possibilities in strategic ways. . . . Blogs and online conversations advance and deepen scholarly conversations, with their impact measured immediately in the number of downloads, views, forwards, comments, and tweets.\"  Ultimately, however, Ayers argues that outside of a limited number of examples, “the articles and books that scholars produce today bear little mark of the digital age in which they are created. Thus the foundation of academic life—the scholarship on which everything else is built—remains surprisingly unaltered.”2 Years after the emergence of open-access scholarly publishing platforms, the persistence of institutional inertias, rigid business models, and professional habits of judgment delay and thwart wider deployment of the rich media environment and continually expanding platforms of digital dissemination.3 Established academic print journals have little reason to change their modes of publication and few have done so.4 As a result, young scholars, especially in humanities disciplines, find themselves startled by atavistic attitudes. Consider the following excerpt from a letter I recently received from an untenured assistant professor about his journal publishing options:  \"I have to share some unfortunate news with you about my contribution to Southern Spaces. I’ve recently been able to have a series of conversations with the new director of my program, who is also my advocate and advisor with regard to tenure and promotion—my review will begin next academic year. I discussed Southern Spaces with [my director] and while we both think that the online format and the multimedia potential of the format are important to the future of scholarship, her advice to me was that other members of my P/T committee have far more traditional views and would be reluctant to recognize Southern Spaces as a publication on par with a print journal. Needless to say, I don’t agree with that assessment, but I think I have to accept it.\"  Expressed apologetically, “needless to say” acknowledges the tempting, strong, and rising currents of digital publishing practice that are transforming scholarship by making its production more participatory and “free” (in the sense of free speech),5 and its reception widely available. Necessary to say, peer-reviewed Southern Spaces essays are used by scholars in a variety of successful professional efforts including tenure and promotion, applications for post-doc fellowships, and landing new kinds of jobs with digital libraries and regional centers.  As part of its intentionality, the collaborative process of producing an open access journal can create a training center where students acquire a range of skills—working with open source software, copyediting, map making, assisting writers and videographers in developing essays, creating technological tools, implementing layout and design—that are valuable in the new digital publishing environment. In a sense, this is a kind of “building scholarship,” in which “the interventions that occur as a result of building are as interesting as those that are typically established through writing.”6 If conceptualized broadly, open-access journal production should also involve a network of independent scholars; researchers associated with nonprofit, grassroots, and nongovernmental organizations; alt-academic writers; as well as the expected cast of college and university-based professors. Supporting this broad understanding of scholarship, Southern Spaces (and its social media tools of promotion, e.g., Facebook, Twitter, RSS) has served as an available publishing platform for community-based groups and regional writers engaged in contemporary research related to immigration, environmental destruction, and the crisis of public education. Alerting a networked, topical public about critical writing of interest can produce tens of thousands of readers, as evidenced by a Southern Spaces essay on the crisis of government in North Carolina.7 With persistent advocacy, the presence of a journal project can lead to institutional commitments for long-term archival preservation of the born-digital materials generated in the work of the journal and into mutually supportive intergroup alliances through organizations such as the recently created Library Publishing Coalition. It is necessary to say, and to articulate in detail, the various practices of open access publishing that, taken together, comprise a strategy that extends beyond the academy into wider social networks. Drawing upon a decade of publishing practice, my proposed paper will identify and elaborate how a project such as the multimedia journal Southern Spaces works to advance cultural empowerment through digital design, creation, dissemination, and curation.  ",
       "article_title":"“Needless To Say”: Articulating Digital Publishing Practices as Strategies of Cultural Empowerment",
       "authors":[
          {
             "given":"Allen E.",
             "family":"Tullos",
             "affiliation":[
                {
                   "original_name":"Emory Center for Digital Scholarship",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "licensing",
          "video",
          "multimedia; copyright",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In this paper, we will present a case study of how an ongoing, multi-faculty, interdisciplinary DH project focused on the Susquehanna Valley in Pennsylvania has created, and continues to explore, ways in which students can excel both inside the classroom and outside.  These DH projects involve undergraduates working with faculty on an unfolding expansive research project that affords otherwise unachievable opportunities for undergraduate student engagement, the development of new skills, and meaningful ongoing interaction between the institution and community that have, in turn, furthered the scope and scale of the project.  While it is recognized that the most compelling pedagogical experiences bridge the divide between semesters, and even years, of study at the undergraduate level, there has been little examination to date of how digital pedagogy affords particularly effective forms of promoting long term student engagement, challenging students and instructor to consider and reconsider course matter from new and provocative vantage points. In early digital humanities programs, considerations of ensuring that “the acculturation and professionalization that takes place in the learning community is relevant to the students” has been largely situated in graduate programs, leaving undergraduates in learning environments where digital engagement focuses on tool training rather than one in which they learn digital “habits of mind” that involve participation in nuanced humanistic discourse with their professors. In small liberal arts colleges, where close faculty/student interaction is at the core of high impact practices, opportunities to advance prolonged faculty-student collaboration can produce exceptional results. The Andrew W. Mellon Foundation has identified the importance of these opportunities for curricular digital engagement at liberal arts education through strategic multi-year digital initiative grants at an increasing number of liberal arts institutions. A particularly valuable area for ongoing pedagogical engagement is in developing place-based projects that also enlist local communities in the digital, conceptual mapping of historical and cultural resources.  And yet, such rich and nuanced considerations of local place, culture, and environment call for extended student engagement over time and even across years of undergraduate study.  Thus, the traditional classroom model for the execution of such DH place based projects is inadequate. Extending the classroom outside (both spatially and temporally) allows for the development of rich deep local knowledge in both digital learning tools and content. Indeed, extending the faculty-student collaboration to include students from outside traditional humanities departments also reifies the value of interdisciplinary research at an early level and reflects the professional digital humanities research model employed by larger scale projects. Also, undergraduates engaged in digital humanities work can perform a sort of outreach, demonstrating that “the humanities [belong] to everyone, not just trained professionals” and thereby help “bridge the widening gap between academic humanities and broader American culture.” At Bucknell University, beginning in 2011, faculty in Comparative Humanities, English, Geography, and Environmental Studies have developed and taught a slate of courses relating to issues vital to the interpretation and conservation of the environmentally impaired Susquehanna River. These courses form a de facto core curriculum designed around the region and consider questions of the environmental effects on regional resources, the eradication of the traces of Native American history and culture as a result of European immigration and settlement, and economic under-investment in post-industrial rural towns. To this point, DH engagement has focused on the collection and analysis of GIS-related materials, work that has been instrumental in the garnering of Federal recognition of the cultural importance of the Susquehanna River through its designation as a National Historic Trail under the umbrella of the National Parks Service.  Students and faculty continue to work with non-profit agencies and the NPS in the development of digital layers of scientific and economic data that will expand the reach of this originally DH project.  A new phase of this project that will begins this summer with the digitization, transcription, and critical analysis of the collected correspondence, journals, records, and incidental papers of James Merrill Linn (1833-1897), held in the Archives of Bucknell University.  This phase will involve Bucknell faculty, staff, and students in a research project that will develop within and beyond the classroom.  The Linn Papers project is ambitious and represents a new model for collaborative digital humanities research and teaching at Bucknell. Because of Linn’s relationship with “place” (Bucknell, Lewisburg, and the sites of battles and campaigns in the Middle Atlantic and Southern states during the Civil War) this project offers an important opportunity to expand and reconsider Bucknell’s commitment to considerations of “spatial thinking.” The Linn Papers archive includes documents in several media forms: manuscripts, drawings and sketches, printed records, archival newspaper clippings, and hand-drawn maps. Because of the multiple forms of inter-reliant media, the collection encourages analysis of people and places across document types. This form of analysis is best-suited to digital forms of curation and publication.  The Linn Papers project will a) make available in digital form a wealth of information about a historically under-resourced area of the Susquehanna Valley; b) leach students the principles and techniques of DH, and in particular, TEI-compliant XML; c) enable students to be active and engaged participants in the reframing of humanistic pedagogy and relevance in an age that sees almost daily public media questioning of the value of the humanities. This phase of the project will begin with a pilot undertaken in summer 2014 with the digitization of a selection of the papers.  The Linn project will  become a central facet of newly designed HUMN 100 courses offered in 2014-15 through the Comparative Humanities program, that are open to first and second year students only. Student engagement with the materials will also serve as a test case to determine best practices for incorporating TEI, GIS and network analysis skill development in a variety of courses, effectively creating a DH training stream at the university.  ",
       "article_title":"Digital learning in an undergraduate context: promoting long term student-faculty (and community) collaboration in the Susquehanna Valley, PA",
       "authors":[
          {
             "given":"Diane",
             "family":"Jakacki",
             "affiliation":[
                {
                   "original_name":"Bucknell University",
                   "normalized_name":"Bucknell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00fc1qt65",
                      "GRID":"grid.253363.2"
                   }
                }
             ]
          },
          {
             "given":"Katherine",
             "family":"Faull",
             "affiliation":[
                {
                   "original_name":"Bucknell University",
                   "normalized_name":"Bucknell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00fc1qt65",
                      "GRID":"grid.253363.2"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "geospatial analysis",
          "interfaces and technology",
          "sustainability and preservation",
          "repositories",
          "digital humanities - pedagogy and curriculum",
          "interdisciplinary collaboration",
          "encoding - theory and practice",
          "archives"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Abstract In the many conversations touched off by Alan Liu’s question, “Where is cultural criticism in the digital humanities?”, there has been little attention paid to the first term of that question: where? Treating this term abstractly has robbed many of the resulting discussions of the relationship between the digital humanities and (what Liu terms) “the mainstream humanities” of their usefulness. We argue that focusing precisely on the where of the digital humanities, that is on sets of material practices employed at a specific site during the production of digital humanities work opens stale questions to new analytical approaches and, much better, to new avenues of intervention.  Liu's Description of Digital Humanities as Caricature Most responses to Liu’s question have pursued answers in terms of disciplinary identity and its accompanying political and organizational boundaries (intellectual history honed to an instrumentalist point) 1. In the version of his essay on this question published in the Debates in the Digital Humanities volume, Liu himself focuses on the production and consumption of “the digital humanities” in terms of what he calls \"the great postindustrial, neoliberal, corporate, and global flows of information-cum-capital\"2. This commitment to examining the digital humanities from a world system view does much to explain, if not excuse, the caricature of digital humanities work that follows: “It is as if, when the order comes down from the funding agencies, university administrations, and other bodies … digital humanists just concentrate on pushing the “execute” button….” From previous work such as The Laws of Cool 3 through “Where is Cultural Criticism in the Digital Humanities?” and forward into essays such as “The Meaning of the Digital Humanities” 4, Liu’s critical work has been intermixed with advocacy for a vision of the humanities that emphasizes resistance to an academic culture that is “postindustrial, neoliberal, corporate”, etc. Thus it would seem a willful misreading to understand Liu’s portrayal of digital humanities work in the Debates essay as something other than a continuation of his advocacy for a particular regime within the world system he constructs. Yet, many readers seem to have taken this figuration of digital humanities as descriptive rather than rhetorical. Work Sites as Opportunities for Analysis Indeed, to save the question of “Where is cultural criticism in the digital humanities?” from tendentiousness requires richer, more detailed accounts of the digital humanities as sets of material work practices 5. As Clifford Geertz complained of structuralist accounts in anthropology, “Whatever, or wherever, symbol systems ‘in their own terms’ may be, we gain empirical access to them by inspecting events, not by arranging abstracted entities into unified patterns” 6. As a contribution to the debate over the relationship of the digital humanities to the humanities, we offer an account of attending to, and then trying to re-make, specific material practices at a site of digital humanities work. The specific case we will discuss involves challenges that have arisen from a model of organizing work around “fellowships”, a structure inherited and adapted from the “mainstream humanities”. We will narrate the creation and formulation of a model for organizing work practices created in response to perceived structural flaws in the fellowship model. This “anti-fellowship” model, which we have called “The Digital Humanities Incubator” (henceforth, the Incubator) prizes long-term enrichment by the variety of participants engaged in DH work versus short-term gain by project-driven “fellows.” Our discussion of the Incubator then is a critical reflection on the the program and our thought process in revising it from one iteration to the next. The Incubator functions as a work of critical design 7; it is one of our tools for thinking through how to make digital humanities work. This example is drawn from the site we are best able to describe in sufficient detail, our own place of work, the Maryland Institute for Technology in the Humanities (MITH), an active digital humanities center in the U.S. Of course \"digital humanities\" is a purposefully capacious term used to cover many different types of work by many different practitioners 8. Furthermore, we acknowledge that there are relatively few centers and therefore relatively few digital humanists who work in them relative to the scale of the global digital humanities field. Yet, as Donna Haraway argues in her essay on situated knowledges, the goal should not be “a doctrine of objectivity that promises transcendence, a story that loses track of its mediations just where someone might be held responsible for something, and unlimited instrumental power\" 9. We certainly do not mean to advance any singular account of digital humanities work practices nor do we make claim to “unlimited instrumental power” for our descriptions. Yet, it is our hope by focusing on the practice of DH labor related to fellowships that we can encourage practitioners to explore the material realities of their own practice. Organizing Work: A Brief History of the Fellowship Model Popularized by the Institute for Advanced Technology in the Humanities (IATH) at the University of Virginia and adapted at MITH and other digital humanities sites (not all officially centers), digital humanities fellowships have served to connect faculty---in established and ostensibly fixed and stable organizational roles--- to newer digital humanities activities. At the same time, these fix these novel, alternative and contingent digital activities to existing structures under the rubric of innovation or enhancement to scholarly activities. Fellowship experiences were most often configured as short-term (1-2 years), focused on the creation of a specific product, tool, project, and reliant divisions of labor between participants with unequal agency and power 10. Faculty could be recruited as participants, supporters, and advocates and the digital humanities initiative could demonstrate its effect on the local academic community—as service, if nothing else. For a new endeavor in the fundamentally conservative academy, the adoption of a pre-made structure of organizing labor was politically astute and laid much of the groundwork for the wide success of the field today. Fellows became advocates for the digital humanities and examples of its proliferation into humanities disciplines. Yet it is crucial to remember that this model—leave from the assumed structures and schedules of faculty life, provision of technology, and access to staff with computational or other forms of alternative expertise—is an artifact of a particular historical moment. By historicizing “the fellowship” we mean to disrupt a tendency toward a teleological or developmental view of this type of work organization as a necessary step in the introduction of the digital humanities to a new site. We do not claim that these fellowship arrangements have not produced some excellent digital humanities scholarship. However, based on our experiences working in and managing these activities, we began to feel that the arrangement suffered from flaws not in just its execution in any particular case but structurally overall. What Studying Work Can Do The distinction between individual project successes and structural flaws is important to emphasize because it contains the most crucial lessons to be learned for the digital humanities and humanities enterprise. By shifting focus from particular work products or outputs (where Liu and many of his respondents focus their attention) to the structuring of material conditions at the site of work production, which make certain kinds of digital humanities possible, we engage issues raised by Richard Sennett's notion of \"the workshop\" as \"a productive space in which people deal face-to-face with issues of authority\"11. Sennett observes that “the successful workshop will establish legitimate authority in the flesh [in the form of recognized mastery of skill], not in rights or duties set down on paper.” In too many cases, a faculty fellow is not the head of a happy, successful workshop in these terms even if the end product is lauded. Using ethnographic approaches to the study of work, especially as filtered through fields such as science and technology studies (an avenue suggested by Liu himself), perspectives from critical and participatory design 12 13, feminist theory , and labor studies allows us to unpack the specific material practices of labor in the contemporary digital humanities project-as-work-site/workshop. As careful articulations of the material conditions and practices of a particular site of work intersect with questions of power and authority, the management of digital humanities work can provide productive interventions in questions such as the disciplinary identity of the humanities and, implicitly, their future.  ",
       "article_title":"Making Digital Humanities Work",
       "authors":[
          {
             "given":"Trevor",
             "family":"Munoz",
             "affiliation":[
                {
                   "original_name":"University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jennifer",
             "family":"Guiliano",
             "affiliation":[
                {
                   "original_name":"University of Maryland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "digital humanities - institutional support",
          "history of humanities computing/Digital Humanities",
          "digital humanities - nature and significance"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Walt Whitman’s Leaves of Grass is one of the most famous and often-studied works of American literature. In the century since Van Wyck Brooks declared Whitman the originator of “the sense of something organic in American life”—the first to combine high art and rude experience—Whitman’s masterwork has been thoroughly digested into a series of critical truisms that gives even new readers of the poems a sense of familiarity. Whether we have his poems committed to memory or have never actually read one of them, we “all know” that Whitman eschewed traditionally poetic diction, that his is a poetry of inclusiveness, that the first edition of his text in 1855 is more daring, lively, and experimental than later editions, etc.  Such axioms are comforting in the face of what is on many levels a difficult text (actually, a set of texts) to assimilate. Because Whitman applied the title “Leaves of Grass” to more than ten distinctly different volumes over the course of three and a half decades—not only adding poems but also retitling, cancelling, drastically revising, combining, and re-grouping existing ones—the goal of accurately tracing the book’s evolution has consistently frustrated scholars. Recognizing that “for the reader to understand how Leaves of Grass grew from edition to edition, some sense had to be made of these often bewildering textual permutations,” a group of late-twentieth century scholars labored for over a decade to produce a variorum edition, a tremendous accomplishment that has, unfortunately, done little to alleviate the bewilderment of permutations.  The hope that digital technologies might offer a way, at last, to lucidly represent the various stages in the evolution of Leaves of Grass was one of the early motivations for the creators of the Whitman Archive in the late 1990s. We have often revisited the question of how to convey visually the information represented in the arcane coding of the 3-volume print variorum and inherent in the separate digitized editions. Nearly two decades later, however, we haven’t made much progress.  Though they cannot provide the kind of detailed, objective understanding that might be conveyed by the schematic, interactive interfaces that we’ve sometimes (very hazily) imagined—ones that somehow collate whole texts, poems, lines, and phrases—we have begun to experiment with distant reading strategies that provide a different sort of view. So while collation tools do not cope well with the scope of transformation involved in Whitman’s reworking the first edition’s 10,000-word prose preface into the 4,000-word poem “By Blue Ontario’s Shore,” text analysis tools such as Voyant offer a number of potentially enlightening prospects on the two works and their relationship.  Likewise, such tools can begin to offer ways to assay and quantify some of the critical commonplaces that have grown up around Leaves of Grass: Is Whitman’s diction, in fact, innovative and what makes it so? How do Whitman’s early poems compare to his later poems? What basis might be found for claims that Whitman is the great poet of America, women, the body, male homoeroticism, or democracy? At the University of Nebraska–Lincoln's Center for Digital Research in the Humanities we have been experimenting with a new way of visualizing phenomena in TEI corpora and have created Indigo, an experimental XSLT-based tool that queries TEI files and generates animated videos of the results. Using XPath and XQuery techniques, this tool makes it possible to ask specific or general questions of a corpus. The data are then output as scalable vector graphic (SVG) files that are converted to raster images and rendered in high definition H.264 video at 30 frames per second. At its core, Indigo is a program for performing scripted stop-motion animation, arranged in one or more scenes. What each scene contains is up to the user: it might include letters, numbers, shapes, colors, gradients, patterns, lines, paths, or imported raster images, each moving or not moving. The only requirement is that a scene must be modeled in XSLT, with SVG structures as the initial output. For the user wishing to visualize aspects of TEI text corpora, the news is good, for that format shares membership with XSLT and SVG in the XML ecosystem. Indigo provides a method for presenting, in fresh and unexpected ways, quantitative data relevant to scholarly questions in a way that is open-ended, making the user a co-creator with Whitman in the “meaning” of his texts.   Our experiment involves such activities as creating quantitive analyses of some of the linguistic characteristics of Whitman's poetic corpus, comparing them to those of some of his popular contemporaries, and then \"presenting\" the results as a video sequence. Such a procedure is admittedly outside the mainstream of critical methodology in the humanities, but it is entirely in keeping with Whitman’s own theories of the proper relationships among authors, readers, and texts. “The process of reading,” he said, “is not a half-sleep, but, in highest sense, an exercise, a gymnast’s struggle; . . . the reader is to do something for himself, . . . must himself or herself construct indeed the poem, argument, history, metaphysical essay—the text furnishing the hints, the clue, the start or frame-work.”1 As Tanya Clement has recently observed, \"sometimes the view facilitated by digital tools generates the same data human beings . . . could generate by hand, but more quickly,\" and sometimes \"these vantage points are remarkably different . . . and provide us with a new perspective on texts.\"2 And as Dana Solomon has written, \"due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve … and overall 'cool,' the practice of visualizing textual data has been widely adopted by the digital humanities.\"3 In representing the literary work as an absorbing performance, one that comprises both \"data\" and \"art,\" the method we are presenting is calculated to provoke responses in both informational and aesthetic registers. It is, in the terms of Jerome McGann and Lisa Samuels, an act of “interpretive deformance,” whereby “we are brought to a critical position in which we can imagine things about the text that we didn’t and perhaps couldn’t otherwise know.”4  ",
       "article_title":"Leaves of Grass: Data Animation and XML Technologies",
       "authors":[
          {
             "given":"Brett",
             "family":"Barney",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Pytlik Zillig",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Today we have more information at our fingertips than at any other time in human history. The problem is no longer finding information, the problem is being overwhelmed with the amount of information. This is no different in the realm of the digital humanities.  Information on people, projects, resources, methods, and tools exists in quantity everywhere we look, and yet we still have difficulty finding what we need. This paper will describe a transatlantic effort on the part of DiRT in the United States and DARIAH in Europe to construct a taxonomy of scholarly methods, that can be used not only to organize single collections of DH information and resources but also to allow these collections to interface with each other, creating a web of linked data that can be effectively searched for information across distributed collections. DiRT and DARIAH are not trying to impose a restrictive, monolithic scheme on DH; rather, our goal is to construct a lightweight, basic taxonomy of higher order goals and first-order methods that can be easily expanded in all directions by linking lower order techniques to multiple goals and/or methods to create machine-readable paths among the various resources. In building this taxonomy, we heavily rely on input and feedback from the digital humanities community. Still, at least for the intended use cases, we believe a stable taxonomy has advantages over more open, folksonomy-based solutions. The taxonomy as it exists now is based upon three primary sources: 1) the arts-humanities.net taxonomy of tools of DH projects, tools, centers, and other resources, especially as it has been expanded by digital.humanities@oxford in the UK and DRAPIer in Ireland; 2) the DiRT collection of digital research tools, re-launched under Project Bamboo in the US but now continuing on after the end of that project; and 3) the DARIAH ‘Doing Digital Humanities’ Zotero bibliography of literature on all facets of DH. These resources were studied and distilled into their essential parts, producing a simplified taxonomy of two levels: 8 top-level goals that are broadly based on the steps of the scholarly research process and a number of general methods under these goals that are typically used by scholars to achieve these research goals. The updating of the taxonomy and the definition of the types of relationships to be described in the resulting ontology will be carried out by a joint working group in the DARIAH-EU and the NeDiMAH projects in Europe, which will conduct large scale desk and field research into scholarly practice to determine how best to describe the relationships between and among the goals, methods, and techniques of scholarly practice.  The future expansion of this organizational system will not be as a hierarchical taxonomy but, instead, as a linked ontology as lower-level techniques are attached to one or more methods, linking all the existing entities in the ontology together. The projects and collections that use this schema will play an important role here: as resources are added to these collections and linked to the taxonomy, the resulting ontology will grow in complexity.  This complexity will be more help than hindrance precisely because it will be a machine-actionable complexity.  Computers will traverse this web of relationships for us, only bringing back results that are closely related to our needs. This may seem excessively optimistic, but this paper will support these claims by describing three very different types of resources that have used and expanded the taxonomy not only to improve the findability within their own collections but, more importantly, to link to each other in a machine-actionable way. These resources are the DiRT directory of digital tools, the DARIAH ‘Doing Digital Humanities’ bibliography, and the DARIAH-DE service-oriented project portal. A brief description of each of these collections and how they will profit from this taxonomy/ontology follows.  2. DiRT DiRT (Digital Research Tools, http://dirt.projectbamboo.org) is a longstanding US-based directory for scholars interested in digital tools, which provides basic information about software that can facilitate the research process at different stages. The classification of tools by category has always been fundamental to DiRT: in its earlier incarnation as a wiki, wiki pages each corresponded to a category of tools, and the tools were presented in a list on the page. In 2011, DiRT was rebuilt using the Drupal content management system, which allowed information about each tool to be stored in a structured manner that enables faceted search and browsing. While users can now create complex queries on DiRT (e.g. using operating system and price to narrow their results), tool categories remain the primary way of navigating the site. With support from the Andrew W. Mellon Foundation, DiRT is currently undergoing a new phase of development, with the goal of making information about digital tools available outside the DiRT directory itself. Since its inception, DiRT has used its own ad-hoc list of categories. All tools must belong to at least one category, though these categories can be supplemented with user-generated tags. The shortcomings of DiRT’s categories list can be illustrated through the example of OCR tools-- some are classified as “transcription”, others as “conversion”, and while neither is ideal, both are a reasonable approximation given the other options. Replacing DiRT’s former categories with the taxonomy will improve the consistency and quality of the data, and also provide a shared facet that can connect DiRT’s tool data with information provided by other projects, once DiRT’s contents are made available using RDF. 3. 'Doing Digital Humanities' bibliography Another resource directly connected to the taxonomy is DARIAH-DE's ‘Doing Digital Humanities’ bibliography. The bibliography can be accessed on Zotero (www.zotero.org/groups/113737) or on the DARIAH-DE portal (https://de.dariah.eu/bibliography). Like DiRT, the bibliography is one of the seed activities for the taxonomy at the same time as being one of the already defined use cases, representing the application domain of making medium-sized collections of bibliographic references discoverable. This Zotero-based bibliography offers suggestions for introductory readings as well as more in-depth coverage of research literature in various areas of digital research, teaching and infrastructure planning in and for the humanities. The bibliography is carefully curated collaboratively, is freely accessible, currently has around 800 entries, and is being updated continuously. Right now, the bibliography is already divided into thematic collections based on the \"goals\" defined in the taxonomy. Each collection, hence, covers one prototypical aspect or goal of the research process in the humanities as it is practiced with digital tools, methods and data. In addition, all entries in the bibliography are discoverable through keywords covering, on the one hand, typical research methods and activities in the humanities, and on the other hand, a wide range of objects of research. The current closed list of keyword represents an early draft version of the taxonomy described here.  Once a first stable version of the taxonomy is available, the bibliography's keyword implementation will be updated. Sharing a keyword system with other projects will make it easier for users to find related resources. And the public documentation of the taxonomy, including concise scope notes for all methods and techniques, will make the bibliography's keyword-based search more transparent and increase its usability. 4. DARIAH-DE portal A third use case aims to examine the taxonomy as a functional structure for DARIAH-DE’s service-oriented website, the DARIAH-DE-Portal. Launched in a first version in May 2013, it will receive a makeover in the early stage of the upcoming German DARIAH II project scheduled for March 2014 that is based on the taxonomy. The website is designed to offer a wide range of services concerning Digital Humanities in Germany and addresses both researchers who already work digitally and those seeking information or advice. The services provided are as heterogeneous as the DH landscape. They cover informational aspects on specific research projects, information on DH Centers, Bachelor/Master Programmes and tools as well as their documentation, tutorials and teaching materials. Services offered by DARIAH-DE (like the embedded bibliography mentioned above, the DARIAH-DE Working Papers, or hosting services and a developer’s portal) are complemented by external resources like blogs and a DH-calendar (a cooperation with calenda.org being currently on its way). The variety of this content leads to multi-purpose requirements that enable a flexible access to information relevant to individual users. This use case meets that challenge by implementing the taxonomy in RDF, thus interlinking content and making it multi-purpose. In that way, the taxonomy will function as a ‘meta-service’ that meets the interests of an active and interlinked community, that visualizes Digital Humanities and promotes its results.  5. Conclusion The purpose of this talk is not to convince the audience that we in DiRT and DARIAH have all the right answers.  Instead, it is to continue a conversation about the importance of ontologies for managing the over-abundance of DH information, present our own work on this problem and our approach to gathering and incorporating community feedback, in hopes of spurring further work in this area.  ",
       "article_title":"Scholarly primitives revisited: towards a practical taxonomy of digital humanities research activities and objects",
       "authors":[
          {
             "given":"Luise",
             "family":"Borek",
             "affiliation":[
                {
                   "original_name":"Technical University of Darmstadt, Germany",
                   "normalized_name":"TU Darmstadt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05n911h24",
                      "GRID":"grid.6546.1"
                   }
                }
             ]
          },
          {
             "given":"Quinn",
             "family":"Dombrowski",
             "affiliation":[
                {
                   "original_name":"UC Berkeley",
                   "normalized_name":"University of California, Berkeley",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01an7q238",
                      "GRID":"grid.47840.3f"
                   }
                }
             ]
          },
          {
             "given":"Matthew",
             "family":"Munson",
             "affiliation":[
                {
                   "original_name":"University of Göttingen, Germany",
                   "normalized_name":"University of Göttingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01y9bpm73",
                      "GRID":"grid.7450.6"
                   }
                }
             ]
          },
          {
             "given":"Jody",
             "family":"Perkins",
             "affiliation":[
                {
                   "original_name":"Miami University, United States of America",
                   "normalized_name":"Miami University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05nbqxr67",
                      "GRID":"grid.259956.4"
                   }
                }
             ]
          },
          {
             "given":"Christof",
             "family":"Schöch",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The content of well-being and the means for increasing it, at both individual and societal levels, are fundamentally utopian concerns. Consequently, positive psychology, the science of human flourishing, is an essentially utopian project: it intervenes in what it considers an unsatisfactory present and attempts to create (and educate for) something better. Its explicitly activist and pragmatic agendas recently have been bolstered by the explosion of research into what I term “digital happiness.” In his 2011 book Flourish: A Visionary New Understanding of Happiness and Well-Being, leading positive psychologist Martin Seligman briefly outlines “positive computing” as using technological means and methods (such as data mining, social networking, and personalized apps) to “go beyond the slow progress in positive education to disseminate flourishing massively” (94).1 Digital happiness proponents have taken up this call fervently, and positive psychologists, data analysts, coders, and policy makers regularly invoke the utopian possibilities of both technology and happiness as they collaborate for salvation on a grand scale: the H(app)athon Project hacks happiness in order to “save the world”2; video gamers and virtual reality developers (in Jane McGonigal’s words) create their products to “change the world”3; computer scientists offer their “Hedonometer” to measure, and therefore “improve or understand” well-being more completely;4 and the newly launched social network “Happier” will help “you feel freaking awesome.”5 Digital happiness initiatives are a fascinating site of inquiry because they combine the close reading and personal tracking aspects of the the quantified self movement with the large scale data mining, aggregating, and visualization efforts associated with the digital humanities, “distant reading” (Moretti),6 and “network sense” (Mueller).7 Fueled by algorithms for happiness and subjecting qualitative phenomena to quantitative analysis (in ways reminiscent of Jeremy Bentham’s “felicific calculus” and the “mathematically infallible happiness” in Yevgeny Zamyatin’s classic dystopian novel, We8), these initiatives track and triangulate individual internal emotional states, networked virtual data and connections, and real social relations and policies. In this paper, I first provide an overview of these varied attempts to assess and maximize  well-being using big data, sentiment analysis, crowdsourcing, social networking,  the quantified self, and biometrics, positioning their rhetoric and ideology within the larger discourses of self-help, positive psychology and utopian  studies. I then critically interrogate these projects’ utopian aspirations,  analyzing their aims and methods for instantiating different ways of being and  living, of creating both the happy individual and the good society in  the image of (and from the) raw data of individuals’ emotions.   I ask what digital happiness methods, tools, applications, and findings teach us about what we should desire (and not desire), what we should value (and not value), what type of people we should be (and not be), and what type of actions we should take (and not take). Throughout the paper, I am in dialogue with Sara Ahmed’s notion of happiness as performative and normative in The Promise of Happiness(2010), and I highlight “not only what makes happiness good but how happiness participates in making things good” (13) and how “happiness shapes what coheres as a world” (2).9  In doing so, I not only critique digital happiness initiatives on an ideological level but also on technical and methodological levels. In particular, I interrogate happiness/well-being apps' use of both active and passive data to fuel their algorithms; the methods of quantification, semantic analysis, and natural language processing in studies using social media to assess/analyze/improve happiness, well-being, and life satisfaction; how people interact with the technology that is tracking their happiness, and how these users often skew their responses in public, networked settings in order to present versions of their best selves to others. Ahmed has argued that happiness's methods of self-reporting “both presumes the transparency of self-feeling (that we can say and know how we feel), as well as the unmotivated and uncomplicated nature of self-reporting. If happiness is already understood to be what you want to have, then to be asked how happy you are is not to be asked a neutral question” (5). In addition to this problem, already embedded within the positive psychology methodology, digital happiness assumes the transparency and translatability of language/texts and affect/emotions, and undertheorizes how the dynamics of a digital networked space change the way we communicate and connect with others. Digital happiness proponents advocate their work as contributing to the creation of a more utopian future, and argue it is democratic because it is tracking raw data from the people themselves. But the questions of what raw data is assessed and who determines the metrics raise crucial questions about the type of vision these digital happiness experts put forth.  This paper also contextualizes my work on “digital happiness” within the larger discourses of both the self-help genre and positive psychology, and demonstrates how digital happiness showcases the competing tensions of individual improvement and social justice, apolitical progress and politically engaged action, and descriptive reporting and prescriptive advice in both. In doing so, I highlight positive psychology’s “discursive and political labor” (Yen 76).10  Particularly troubling is positive psychology’s conceptualization of its own politics and pedagogy, which teach us to be certain types of people in pursuit of the good life without consideration that its notion of the “good” is not morally universal but inextricably bound to the discipline’s ideological assumptions, cultural contexts (in particular, American individualism), and a particular interpretation of what is “positive,” valuable, and desirable.  I argue that, while digital happiness research’s use of big data and crowdsourcing partially tempers the rampant individualism that dominates positive psychology’s vision of the good life, its notions of the quantified self glorify the desirability of self-monitoring, normalcy, and discipline in the Focualdian sense, which is a hallmark of much of the self-help genre (and positive psychology more generally). Self-help “render[s] social relations of power invisible and non-negotiable” and “counsels subjects to sculpt a meaningful life without addressing or questioning the horizon of social relations and the contexts of social power” (Rimke 65). Instead of serving as outlets for potential change, “[p]ractices of self-help are thus connected to the management and government of populations” (Rimke 72).11 Similarly, self-help catches its users “a cycle of seeking individual solutions to problems that are social, economic, and political in origin” (McGee 177).12 Therefore, its aggregated view of subjective well-being still sidesteps the important work of defining the ideological content/function of happiness and addressing its role in maintaining structural inequality.  This paper argues that positive psychology and, by extension, many digital happiness projects that are built on its values/methods, is inherently conservative, in the sense that it does not actively encourage radical possibility and transformation. While it is useful to identify and nurture the strengths that we already have, to reflect on experiences and create a positive meaning/communicate a positive message for them, and to derive satisfaction and pleasure from our activities/connections, these techniques run the risk of functioning remedially and driving us to become more complacent with “what is.” This limiting of possibility is one of the most troubling aspects of positive psychology’s work. As Levitas (2013) reminds us, the refusal to limit possibility is an essential part of the utopian project: “Utopia also entails refusal, the refusal to accept that what is given is enough. It embodies the refusal to accept that living beyond the present is delusional, the refusal to take a face value current judgements of the good or claims that there is no alternative” (17).13 I enact this utopian “refusal,” by arguing that positive psychology and digital happiness together create and endorse descriptions of and prescriptions for happiness and well-being that are quickly forming a unified front, a standardized, monolithic discourse that limits possibility.  These fields matter, immensely, particularly because research on subjective well-being is being institutionalized prior to (or in some cases, in spite of) conversations about its assumptions, values, goals, and consequences. Therefore, this paper opens up a crucial conversation about what gets excluded in current discussions of well-being and digital happiness projects' assessment and promotion of it.       ",
       "article_title":"Unhappy? There's an App for That: Digital Happiness, Data Mining, and Networks of Well-Being",
       "authors":[
          {
             "given":"Jill",
             "family":"Belli",
             "affiliation":[
                {
                   "original_name":"New York City College of Technology, CUNY",
                   "normalized_name":"New York City College of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/021a7pw18",
                      "GRID":"grid.260911.d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "opinion mining",
          "sentiment analysis",
          "self-help",
          "biometrics",
          "happiness",
          "positive psychology",
          "well-being",
          "social media",
          "big data"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The copyright expiry on James Joyce’s Ulysses in 2012 created a unique opportunity to read the seminal modernist text through the refraction of technologies made available by the Digital Humanities and techniques from Computer Science. Ulysses is avowedly and manifestly a work both constructed by and read through explicit references to geography and spatial relations. For instance, Frank Budgen attributes the following statement to Joyce, “‘I want,’ said Joyce, as we were walking down the Universitätstrasse, ‘to give a picture of Dublin so complete that if the city one day suddenly disappeared from the earth it could be reconstructed out of my book.’”.1 However, it has been suggested that uncertainty and disorientation play as great a part as explicit references to place and these qualities are evoked through specific narrative strategies.2 From a Digital Humanities perspective, being able to note such contested or defamiliarising areas presents a challenge.  Significant work has been done in the scholarly literature to manually compile and list named entities such as geographic and place name references in Ulysses. However a significant occasion exists to exploit techniques such as XML mark-up and Natural Language Processing (NLP) to explicitly render geographic and spatial references in Ulysses, make the references available for machine processing and accessible to users for reading the novel. This paper investigates the automatic extraction of toponyms from the Wandering Rocks episode of Ulysses, proposes a model for encoding the episode and accounting for different types of place (including uncertain locations) and, combining these elements, explores XSLTs and visualisations that support a spatial reading of the text. The model proposed by the paper supports not only the notion of the significance of place but also qualities of spatial uncertainty and disorientation noted in the critical literature. The approach taken in the paper leverages exisiting models and technologies.   2. Methodology This approach seeks to instantiate geographical evidence in the narrative that is almost exclusively transmitted to the reader through unstructured text in print presentations of the novel. This has been done through a combination of Natural Language Processing tools, geocoding the resulting data and merging the data into a TEI encoded version of the text and presenting the output in a web application. From a Digital Humanities and Computer Science perspective, a number of readily available tools, technologies and methodologies exist to link place names in unstructured text to geographical data. Such tools allow the novel approaches suggested by Moretti and undertaken by Clement.34For example, Named Entity Recognition (NER), a subset of Natural Language Processing, represents a viable methodology to extract toponyms from unstructured text. Geoparsing place names to match geographical coordinates is assisted through such openly available digital gazetteers such as GeoNames.5  Projects such as the University of Edinburgh’s  Unlock provide non-technical interfaces that allow for automated NER and geoparsing. 6    This paper addresses the role that names play in Ulysses, specifically the Wandering Rocks episode and what this role reveals about the novel as a whole. It confronts whether there is a topographical quality in Ulysses and if so how that quality is defined. Accordingly, it considers three hypotheses: that geoparsing of Ulysses enables distant reading that will in turn enable new interpretations of the text; that geoparsing Ulysses creates a virtual gazetteer of the text; that the development of a model for encoding encompasses areas, such as uncertainty as a quality of place, outside of the scope of geoparsing.    Geoparsing and geocoding have been utilised as a primary methodology for the project. A number of technologies such as the Natural Language Toolkit and software produced by Stanford’s Natural Language Processing Lab were available and assisted in determining whether such an approach was feasible78. Again, it was anticipated that an iterative approach would be followed where initial automated extraction of toponyms via NLP would inform the encoding of episodes. This encoding, in turn, would provide the basis for a coterminal presentation of text and geographical elements through the web application and as the basis for interrogating the text along a geographical orientation.     The data model for the Literary Atlas of Europe was utilised as a framework to assign types to toponyms in the resulting TEI.9 The framework provided by the LAE allowed for further analysis on the role of place in the text, particularly the representation of uncertainty. Accordingly, while the LAE model provides a preliminary scaffolding, the mode proposed in the paper combines semi-automated extration of toponyms combined with a document-based encoding.   3. Lessons Learned The work described by the paper resulted in a number of outcomes. Firstly, the development of the model and subsequent encoding of toponyms in the text rendered a comprehensive and programmatically presentable list of geographic references. Such a list constitutes a sort of virtual gazetteer for the novel. Secondly, this approach works towards identifying any inconsistencies in Joyce’s use of geographic references (with regard to the “traps” identified by Hart), indicates the role of geographical uncertainty in the episode and potentially suggests productive interpretive approaches.10 Thirdly, such an approach contributes towards the notion of a literary cartography and echoes the work undertaken by the Literary Atlas of Europe. The data produced by such an approach would be available for use in contexts outside the academic realm including use in literary tourism.   4. Conclusions This paper tentatively indicates that automated processing of text may support a procedural, iteratively based approach to geoparsing Ulysses that combines the application of software with manually identified terms. The project strongly suggests that the NER-CRF software was most effective in identifying explicit toponyms that were marked in the encoding as either routes or projected spaces. What the results of the project suggest also is that the application of typography might be partially automated; types of place may be determined through automated  processes. The application of the Literary Atlas of Europe’s five categories of spatial representation as a place type within the encoding of the episode clearly supports the contentions of Gunn, Hart and others that place plays a dominant role in the Wandering Rocks episode.11 The relative dominance of places of type “route” in the episode is not surprising and supports the notion of place as being significant to the novel.12 What this approach indicates though is that such critical insights may be verified using the algorithmic or distant reading frameworks. In this case, place is important to the episode because the majority of toponyms indicate explicit routes in particular, verifiable places. However, one is also left with the insight that uncertainty, as marked by the absence of geographical identifiers, is the highest for certain types of places in the text. While place's significance to the novel is undoubtedly a likely outcome of a traditional, close-reading approach, the model proposed in the paper enforces a certain rigour in its approach to the text. Therefore, while the episode may be in some way explicitly “about” place, roughly a third of the places are of uncertain locations. This would markedly imply that, in the critical literature, Bulson’s emphasis on the notion of disorientation and Hart’s attention to the various “traps” of place can be traced back to measurable “quantities”, within the confines of a constricted model, in the text. Additionally, one outcome of this approach is the difficulty in visually representing spatial uncertainty. This element is accomodated in the web component in terms of character routes rather than explicit location. While toponym extraction and a geographically-contextualised approach towards the text enabled visual representations of the types of place in Wandering Rocks, the evocation of uncertainty, as facilitated by the LAE data model, made representation of such data challenging in a web-based, visual environment. The work described in this paper is generalizable within the larger Digital Humanities context as it demonstrates the practical application of NER, uses document encoding to explore meaningful geographic relationships in the text and leverages these relationships to interrogate spatial uncertainty.  ",
       "article_title":"Mapping and Unmapping Joyce: Geoparsing Wandering Rocks",
       "authors":[
          {
             "given":"Caleb",
             "family":"Derven",
             "affiliation":[
                {
                   "original_name":"University of Limerick",
                   "normalized_name":"University of Limerick",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/00a0n9e72",
                      "GRID":"grid.10049.3c"
                   }
                }
             ]
          },
          {
             "given":"Aja",
             "family":"Teehan",
             "affiliation":[
                {
                   "original_name":"An Foras Feasa, National University of Ireland, Maynooth",
                   "normalized_name":"National University of Ireland",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/00shsf120",
                      "GRID":"grid.9344.a"
                   }
                }
             ]
          },
          {
             "given":"John",
             "family":"Keating",
             "affiliation":[
                {
                   "original_name":"Dept. of Computer Science, NUI Maynooth, Aja Teehan, Cognitive Corp",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "encoding - theory and practice",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction  In her article describing a night performance with a dancers troupe 1, Rambo Ronai proposes a \"layered account\" combining different perspectives, that of a dancer, wrestler, ethnographer and writer reflecting upon Derrida's concepts of \"mimesis\" and \"under erasure\", and the metaphor of drawing/writing as a way to express the \"layering process\" of live experience. Starting also from ethnographical observation and Ryle's notion of \"thick description\", Geertz 2 considers the interpretation process as founded upon \"piled-up structures of inference and implication\" and the detection in the observed object of a \"stratified hierarchy of meaningful structures\", like the \"twitches, winks, fake-winks, parodies, rehearsals\", etc. in the twitching/winking scenario or the Jewish, Berber and French \"frames of interpretation\" in Cohen's story. Our proposal relies on the hypothesis that a \"layered\" representation of an electronic text can bring into light some aspects related to the production and circulation of meaning in the reading/interpretation and writing process. The study refers to models and methods like textual zoom, text network analysis and text summarisation and proposes a combined approach for structuring the text on \"layers of meaning\".   Textual zoom, z-text   The model of z-text 3 was inspired by Neal Stephenson’s 4 fictional construct, a primer whose content expands itself in its interaction with the reader. A z-textual layout supposes a hierarchical structure of z-lexias (after Barthes’ lexia5, a unit of reading and analysis), i.e. potentially extensible units, disposed on levels of detail, along with the Z-axis. The processes of reading and writing z-lexias are called z-readingand z-writing. A parentz-lexia consists in a fragment which has engendered descendants, i.e. has been expanded on the subsequent levels of the representation, like zl1 and zl3 in Fig. 1. Each zoom-in operation performed by the reader replaces a z-lexia visible on the roll (the display device on the topmost plane) with its next-level children (if any), while a zoom-out substitutes all the displayed children with their previous-level parent (if any). The zooming mechanism entails a back and forth movement through the layers of text and the dynamic projection of z-lexias on the displaying device.   Fig. 1: Z-text model  The \"tri-dimensional space\" of a z-text can be turned multidimensional if the same z-lexia is expanded and then explored by different types of magnifying glass, i.e. following different points of view or perspectives (sometimes contradictory, like in Pavić's  Khazar controversy viewed through the lens of the red, green or yellow books). Figure 2 presents a z-textual layout of Barthes's S/Z. The representation starts with a fragment from Balzac's Sarrasine on the first level. New details are added gradually on each level:   the “units of reading”(lexias) and the attached interpretation codes, HER, SYM, SEM, etc. (level 2);  the description of the interpretation method and its codes, as a way to understand the plurality of text defined as a \"galaxy of signifiers\"  (level3);  more insight into the \"step by step\" analysis of text \"working back along the threads of meanings\", the \"weaving voices\" made apparent by the five codes, and the evaluation process echoing the writing practice and allowing us to distinguish the \"readerly\" and the \"writerly\" (level 4);   emphasis on the idea of enclosing the text in a fixed structure versus providing it with a \"structuration\", on considering the text as a process rather than a product, and on the reversibility of the writerly text, proved in the example by actually turning Balzac's Sarrasine into Barthes's S/Z (level 5).     Fig. 2: Layers of meanings in Barthes's S/Z  The process can continue with the reader's interpretation on Barthes's interpretation of Balzac, the zoom-in, zoom-out mechanism allowing to move back and forth from the initial text to an interpretation (or interpretation of interpretation, ...) of it, through different layers of meaning involving variable degrees of details.  Text network analysis The z-textual layout of S/Z was based on the assumption that Barthes's analysis contains in itself a certain stratification on levels of signification that can allow the gradual transformation of one text into the other. The levels texts were made up by fragments, not necessarily contiguous as in their original form, but following a certain hierarchical logic (e.g., level 1 - Sarrasine; level 2 - SEM, HER, ...; level 3 - codes explained, etc.).     Further analysis consisted in the use of TexTexture7, a visualisation tool based on the concepts of text network analysis and  betweenness centrality 8. The five files, corresponding to z-text levels (Fig.3), were processed via the TexTexture online service, each file representing a gradual enrichment of the Sarrasine text with Barthes's analysis as described above.    Fig. 3: TexTexture. S/Z z-textual layout  Figure 3 shows the most influential concepts in the texts, i.e. the words with the highest value of betweenness centrality (measuring how often a node appears between any other nodes in the network). The most influential contexts are also displayed as determined by nodes with high degree of connectivity (number of edges).  While high degree nodes can be influential within a given context, high values of betweenness centrality characterizes words supposed to function as shifting points between different clusters of meaning.   A left to right scan of the five columns denotes a certain dynamics in the transition from the Balzacian text to its Barthesian interpretation. Thus, from a first level description of the party salon, the emphasis shifts to a more analytic perspective articulated around the interpretation codes (sem, act, sym) and terms like antithesis and fantastic, resonant with the already highlighted ghost, dance, moon on the previous level. More details on the interpretation procedure added on the third, fourth and fifth levels bring about meaning circulation through nodes like code, signifier, unit, lexia (level1 to 3)  through text,voice, structuring, signifier (level1 to 4) and finally to code, text, signifier, structure, evaluation (level 1 to 5), elements that seem to approximate the different layers of meaning embedded in the z-textual layout.  Text summarisation Text summarisation, extractive 9 or abstractive 10 techniques, represents another point of interest for our inquiry. For instance, studies like 11, 12, 13, 14 making use of graph-based models in order to encode the structure of a text and to compute the most salient sentences/fragments to be included in the summary, or tools for variable summarisation, like 15, 16allowing to generate summaries covering a given percentage from the initial text.  Towards a layered interpretation of meaning  Our proposal consists in a theoretical approach combining textual zoom, text (network) analysis and gradual summarisation for the detection of key elements and the representation of layers of meanings in a text. The combined construct, at this point defined only at a conceptual level, may enclose potential functionalities such as:   highlight the most influential concepts/contexts in the text and their ranks computed according to particular relevance criteria (e.g. betweenness centrality or other);  propose candidate sentences to be included on different levels of summarisation, possibly based on a certain rank order (for instance, starting with lower or higher rank constituents on the lower levels);   assist the user in building further summarisation levels by gradually adding remaining constituents, until the whole text is covered; integrate the levels of summarization into a z-text layout that can be eventually explored by zoom-in and zoom-out.  The structure can be considered (in a kind of \"deformative\" interpretation 17 or close/distant reading scenario) in order to explore the layers of meanings \"hidden\" in a text. Our hypothesis is that words and sentences may appear in disparate places throughout the text, but from an interpretative or writing perspective they may belong to the same conceptual or symbolic level. Grouping these fragments on layers of meaning may bring new light on the process of text production and understanding.  Similar with the S/Z experiment, we may imagine, for instance, a \"step by step\" passage from the analyzed text to deeper analytic levels (like in Auerbach's 18 reflections on the representation of reality in Western literature, starting from a close reading of Odysseus's Scar, or in Greenblatt's 19 new-historicist analysis of Midsummer leading to a reconstruction of the historical-cultural context having inspired it). Other examples can deal with the variable degree of proximity/distance of the reader to a textual object (as in Shakespeare's Venus and Adonis or in a hypothetical \"behaviorist\" narrative progressively enriched with characters' psychology), the gradual movement from simple to complex, intuitive to abstract in pedagogical or philosophical scenarios (e.g. Wittgenstein's 20Tractatus), as well as the \"layered\" representation of a growing text - from a few initial paragraphs to a full-fledged story, resulted from a writing process. A layered interpretation of meaning may be aligned, besides Rambo Ronai's and Geertz's theses, with Iser's 21 assumption on the process of \"anticipation and retrospection\" implied by the act of reading , and Schor's 22 absorbed (or absent) detail and its capacity to “persist and inform in absence” . Every layer of meaning carries a potential both for retrospection and anticipation, acting, in an absence/presence scenario, as a bridge between the already-said and what is about to be articulated.   The presentation will include both theoretical aspects and a demo on the proposed topic.   ",
       "article_title":"The Layered Text. From Textual Zoom, Text Network Analysis and Text Summarisation to a Layered Interpretation of Meaning",
       "authors":[
          {
             "given":"Florentina",
             "family":"Armaselu",
             "affiliation":[
                {
                   "original_name":"Zoomimagine",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Where exactly is the place of digital humanities to be in undergraduate education? If, indeed, 21st century universities must begin to prepare students for professional work in which digital familiarity, skills, and facility are increasingly central, where is the site of responsibility for that training? It could be disciplinary or interdisciplinary, located within the curricula and pedagogies of existing departments or relocated to the information sciences, library, or core liberal education curriculum. Joining a chorus of scholars considering the place of undergraduate pedagogy and digital humanities, including those on DH2013's \"The Future of Undergraduate Digital Humanities\" panel, this presentation will detail the highly collaborative creation and facilitation of \"How To Do History,\" a course offered by Donna Gabaccia. In its old form, the course was one of the mainstays of upper-level offerings by the University of Minnesota's History department, serving as a way to prepare students' Senior Thesis and, implicitly, prepare these students for graduate school in History; it has at least temporarily became a venue for students explorations of, and contributions to, digital history in a disciplinary context unofficially known as \"How To Do (Digital) History.\" The core collaboration is between the presenters, respectively, a well-established Professor of History (Gabaccia) and the Digital Humanities Specialist for the University of Minnesota Libraries (Schell). In the summer of 2013, we met to discuss not just the content of the class (readings, assignments, etc), but also what kinds of support the Library could make available to students to actually make projects (rather than, as in the past, preparing to write individual research papers). By circumscribing the options available to them (limiting them to a few of the main digital humanities tools and methods, i.e. mapping, Omeka, digital storytelling), it made both the technologies, and the projects that could be created with those technologies, much more accessible. In addition to creating public projects, the students engaged critically with reframed historiographical questions (i.e. the writing and rewriting of history through Wikipedia) and digital literacy (critically examining Twitter and other online platforms of communication and how they relate to scholarly discourse), reading, among others, Cohen (2006)1, Rosenzweig (2011)2, Kelly (2013)3, and Nawrotszi and Dougherty (2013)Nawrotszi, K. and Dougherty, J. (2013). Writing History in the Digital Age. Ann Arbor: University of Michigan Press. 4.  While collaboration was essential to the undergraduates’ creation of digital projects, graduate students also became partners in the course. Instead of Gabaccia trying to supervise the seven groups of undergraduate students, she allowed graduate students to enroll in their own section of the course. In addition to doing further readings with her regarding the intellectual lineage and stakes of digital history and digital humanities (e.g  Gold 2012)5, the graduate students served as project managers for the undergraduate groups, getting valuable experience in facilitating collaborative projects and various digital humanities tools and methods, neither of which are normally part of their History graduate work. A further collaborative aspect to this version of \"How To Do History\" was extensive collaboration with the University of Minnesota Libraries. While many subject librarians will come once during the semester to introduce research methods, subject-specific database, and other source materials, Schell attended nearly every class, doing multiple presentations about specific ideas and technologies, ranging from introductions to significant digital history projects over the last 15 years to demonstrations of specific tools. In addition, he met multiple times individually with both graduate students (to help construct a blogging assignment for the undergraduate students) as well as the collaborative project groups, helping to refine the scope their projects to make them manageable as a single semester of work.  Finally, one project group worked specifically with the Upper Midwest Jewish Archives, part of the Libraries' Archives and Special Collections, creating an Omeka exhibition around previously unprocessed and undigitized materials. The group looked at the lives of two Jewish men in the early 20th century, through two World Wars, work in the printing industry, and global travel, all set against the backdrop of vicious anti-Semitism in Minneapolis, characterized at that time as the most anti-Semitic city in the United States. A second group created a web feature for the University’s James Ford Bell Library about changing cartographical representations of Scandinavia by mapmakers in the premodern world. The lessons from “How To Do History” do not end with the completion of these collaborative, public digital history projects. Reflecting on the course after its completion, we realized that, due to the project- and group-based environment, many students considered “middle of the road” in their skills and interests developed more research and technological skills than in previous iterations of the class taught by Gabaccia. While most students are not opting for a digital Senior Thesis, instructors have relayed to us anecdotally that they see a greater preparedness and skill in terms of research in those who took the digital version of “How to Do History” than other versions of the course. Furthermore, as we noted above, the graduate students who supervised and facilitated the undergraduate projects gained valuable experience that opened new directions for their own graduate work in terms of research and instruction. Reactions at the departmental level have been mixed.  The History Department proudly featured the student work on its webpage but also continued its ongoing internal debate about the future of HIST 3959--is the course necessary? Could methodologies other than digital history be featured?  Senior members of the department also continued to express skepticism about their ability to evaluate digital work; the department has responded by offering a future department-wide workshop on that issue. The knowledge gained from organizing and teaching the course can inform the development of similar courses in different departments (e.g., How To Do Digital Sociology, Anthropology, Ethnomusicology, to name a few). Schell’s position in the Libraries facilitates that possibility. Digital literacy is a critical element to any undergraduate education, regardless of discipline, and especially if one seeks to receive graduate training in that field. Integrating these digital humanities lessons within each discipline helps students engage more deeply in the development of critical inquiry and with the specific transformations underway in these fields. Furthermore, it creates opportunities for transdisciplinary education, as the digital tools and methodologies students used to create these projects are not just limited to digital history projects. Whether this be part of an emerging “digital humanities” cohort or a broader idea of “digital studies,” it allows for collaborative relationships where extra-disciplinary institutions, such as Libraries, are essential partners.   ",
       "article_title":"\"How To Do (Digital) History\" and Undergraduate Digital Humanities",
       "authors":[
          {
             "given":"Justin",
             "family":"Schell",
             "affiliation":[
                {
                   "original_name":"University of Minnesota",
                   "normalized_name":"University of Minnesota",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/017zqws13",
                      "GRID":"grid.17635.36"
                   }
                }
             ]
          },
          {
             "given":"Donna",
             "family":"Gabaccia",
             "affiliation":[
                {
                   "original_name":"University of Minnesota",
                   "normalized_name":"University of Minnesota",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/017zqws13",
                      "GRID":"grid.17635.36"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "pedagogy",
          "digital history",
          "tools"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   How do readers use social media to express the value and the pleasure that the experience of reading holds for them? And, given the rapidity with which corpora gathered from social media are growing, what kinds of methods are most useful for analysing this kind of (big) data so as to cast light on the phenomenology of reading experiences? This paper seeks to answer these questions by presenting the findings of a project on developing methods for analysing and evaluating literary engagement in digital contexts, funded by the Arts and Humanities Research Council under the auspices of the Cultural Value Project.1 It will report on what can be learnt from the large amount of user-generated data available on microblogging services and social network sites about the value that reading brings to the lives of individuals and communities, and will offer an evaluation of the various analytical tools and methods available to scholars working on reading and reception studies who wish to include born-digital data in their research. Work in reception studies is increasingly focusing on the ways that an understanding of the significance of individual reading experiences can be enriched by attending to occasions when readers join with others to express opinions about a text, and work together to construct its meaning. Scholars have argued that it is in fact in these acts of public negotiation of meaning – for example book group discussions – that readers can be observed doing the private cognitive work of textual engagement, as their interpretations change in the act of articulating their response in a social context.2 The fact that the rich textual data available on social media is often generated by readers in conversation with friends or acquaintances, in contexts quite different to interviews with researchers or questionnaires which might prompt a higher level of self-editing, makes it even more compelling to work with.3 The obvious advantage of working with this sort of born-digital material is that it lends itself to analysis using the growing number of tools and methods being developed within digital humanities, which have the power to integrate textual and geospatial information, and to identify lexical trends in time-stamped data. Such computational methods not only offer scholars the opportunity to analyse much larger bodies of text than is ordinarily possible for individual researchers to examine through close reading, but also to draw on, and discover patterns in, temporal and geospatial metadata. Data for this project was gathered from two different social media platforms, the microblogging platform Twitter and the book collection website LibraryThing.4 For the Twitter data, searches were performed for literary prizes (for example Man Booker Prize and Nobel), author names (for example [Eleanor] Catton and [Alice] Munro), and hashtags commonly used to signal reading-related tweets (for example #goodreads and #mustread). For the LibraryThing data, the results of the Twitter searches were used to suggest particular books to investigate, so as to enable a comparison of the way readers discussed books on the two platforms. The numerical review scores and the text of user reviews of these books were stored in a database, along with metadata about the user. While some interesting work on literary value has already been done by scraping data from Amazon,5 LibraryThing was selected for this project as it is a platform where readers gather primarily to share information voluntarily about books in ways not (directly) linked to commercial activity. Moreover, it is also possible to link some of this information to users’ reported geographic location, something which cannot be done with Amazon data. Various digital methods were then applied to the resulting datasets: thematic analysis using methods from corpus linguistics, analysis of trends in word usage over time using a burst detection algorithm, and geospatial analysis. 1) Thematic analysis Analytical techniques from corpus linguistics were employed to identify patterns of unusually prominent words, phrases and grammatical constructions. The textual data gathered were tagged with the CLAWS part-of-speech tagger,6 and the concordance program AntConc7 was then used to identify the most frequent words, determine their statistical significance as compared to a reference corpus, find the terms that most commonly collocated with them, and carry out other analytical procedures. Sub-corpora were separated out by hashtag and geographical location, and analysed individually. 2) Temporal analysis As all the Twitter data and a significant proportion of the LibraryThing data is time-stamped, it presented an opportunity to analyse trends over time, something that can be done with burst detection analysis in order to gauge how influential particular words or hashtags have been over time.8 The Sci2 tool9 was used to perform burst detection, and to visualise the results as temporal bar graphs. Terms that “burst” into prominence were then fed back into the corpus linguistic analysis, for example in order to examine the collocation patterns around them, and to attend to the context in which they initially appeared.  3) Geospatial analysis The software package ArcGIS was used to create a GIS database including layers derived from the Twitter and LibraryThing data, to see where particular geographical patternings in the search terms and hashtags occurred. (While not all tweets or contributions to LibraryThing have georeferences attached to them, a large enough number do to make this form of analysis worthwhile.) These data were then layered against census data (such as level of educational attainment or socioeconomic status) aggregated at the output area level, in order to enable semantic patterns in the articulation of reading-related tweets and posts to be considered alongside the demographic features of the places where they were articulated.  The paper will set out the advantages offered by thematic, temporal and geospatial analyses, and suggest the components of cultural value which are best addressed by each, while also considering how these different forms of analysis may be productively combined.  ",
       "article_title":"The social pleasure of the text: Applying digital humanities methods to reception studies",
       "authors":[
          {
             "given":"Anouk",
             "family":"Lang",
             "affiliation":[
                {
                   "original_name":"University of Strathclyde",
                   "normalized_name":"University of Strathclyde",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00n3w3b69",
                      "GRID":"grid.11984.35"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "self-reporting on social media",
          "the cultural value of reading",
          "Twitter"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In a paper we presented at the 2012 TEI Members' Meeting[1] and published in the Journal of the TEI[2], we introduced the idea of the \"transactionography\" as a way to model transactions found in historical financial records (HFRs). In this short paper we briefly review transactionography as a model and discuss a few problems that have arisen as we have been testing it.   Transactionography We have developed a data structure for recording transactions found in historical financial records and linking that data structure with the segments of running prose or apparently tabular information that attest it or refer to it. While this is certainly a labor-intensive approach, we consider it nonetheless one worth serious consideration. Our model is similar to the TEI P5 models for contextualization, which offer TEI-compliant models for standoff markup of extra-textual information in files such as prosopographies and gazetteers. Thus in the same way that a prosopography maps how a name of a person that appears in running prose refers to said person, a transactionography maps how the financial records that attest a transaction refer to said transaction. That is, there exists a real-world thing or action that is being referred to by the words (or other marks) in a document we are encoding. The transactionography is a separate file that brings together information that is attested in multiple archival documents to describe a series of transactions.  We noted in our previous paper:  Our model of a transaction is perhaps somewhat more inclusive than some dictionary definitions. We think of a transaction as a coherent set of one or more transfers of something of perceived value from one entity to another. A transfer has three main components, which may be summarized as \"what\", \"from whom\", and \"to whom\". Furthermore, the \"what\" is likely to be divisible into a certain amount (that is, a quantity and a unit) of a given commodity. It is worth noting that transfers take place at a certain point in time, even if we don’t know when that time was. Or, at least, transfers are completed at a certain point in time. A transfer may occupy a signiﬁcant duration of time from start to finish. For example, a transaction may be conducted by ground postal service.        Some common transaction categories can be identified.  A standard exchange of money for goods is a purchase, consisting of two transfers: e.g., I transfer $2 to the convenience store, and a  tiny little bottle of apple juice is transferred from the store to me. A similar trade that does not involve currency is  a barter, for example, trading a large red paper clip for a fish-shaped pen. A gift is a single uni-directional voluntary transfer; a single uni-directional involuntary transfer is called theft or embezzlement. A set of transfers among more than two entities may be referred to as a multilateral trade.   To model the \"what\", the TEI <measure> (or <measureGrp>) element seems tailor-made for the purpose. To model the \"when\", the attributes from the TEI att.datable class seem to be quite reasonable: they can handle specific dates, times, date ranges, and particular dates that took place sometime within a range.  For representing \"from whom\" and \"to whom\" we have created new attributes, fra= and til= (Norwegian for \"from\" and \"to\") as the attributes from= and to= already occur in TEI, and creditor= and debtor= have different meanings in different contexts.   Problems Data capture interface Complex transactions In our model, a _transaction_ is a series of one or more _transfer_s from one entity to another. (An entity here is a person, an organization, or an account.) This works well for simple purchases, barters, and trades (each of which comprises 2 transfers), as well as gifts and theft (each of which comprises 1 transfer). It also works well for simple multilateral trades. E.g., if Mr. Baxter gives 86.50 USD to Mr. Sheldrake, who gives flowers to Ms. Kubelik, who gives basketball tickets to Mr. Baxter, the transaction comprises three transfers.  Our model does not yet include a way to handle more complex transactions that involve multiple entities paying and receiving different amounts of cash. Such a transaction might look like this:   A pays $2 B pays $4 C gets $3 D gets $3  Our model requires the addition of a fifth entity to hold contributions before distributions can be made. A fictional or temporary account entity could easily be used for this purpose. Colloquially, we might refer to this account as a \"pot\" or \"kitty,\" borrowing terminology from card playing games.A different model, one perhaps in which transfers were listed by person or account rather than by event or date, might solve this problem. E.g., the following models the example above.   (Note that this example is quite simplified, in that whatever C & D give A & B in return for the $6 is not included.) However, this sort of model seems overall more cumbersome in the simpler cases.While we note this as an outlier case that our model handles only clumsily, we question how often it would occur in practice, and thus how much of a burden on the encoder of HFRs it would present.   Services We noted in our poster presented at TEI 2013[3] that services are a problem, but adding the attribute @service to the TEI <measure> element might solve it. We quote from said poster: Many historical financial records, however, include or are even primarily about the exchange of money for services (e.g., laundering, room and board, or domestic service). Since these services were more usually performed by women and often recorded by women, study of these types of HFRs is of particular interest to practitioners of women’s history. In our “transactionography” we have heretofore used the TEI <measure> element, with its @quantity, @unit, and @commodity attributes, to represent that which is transferred from one person or account to another in a transaction. But in the laundry list case, the work performed by the laundress is not a “commodity” but a “service”, the service for which the boarder paid the boardinghouse keeper in this transaction. However, using the <measure> element with existing attributes leads to markup that fails to distinguish the purchase of a garment from paying for the service of laundering it. One possible solution is to add a new attribute, @service. Thus for instance, a line from a laundry list might be marked up as follows:  This solution seems to have broad application. E.g.:  We will not be surprised, however, if there are cases it does not handle well. Another approach might be to think of \"shirt\" as the unit and \"launder\" as the commodity. This would have the advantage of not requiring the addition of an extra attribute to the <measure> element. And since we have marked the unit as \"count\" in both the examples of framing and shoe shining, these examples might suggest that this latter approach is more elegant than our more verbose use of @service. We wonder, though, whether the programming example might point to a place where the option of greater verbosity constitutes an advantage.Nor does the alternative approach offer a solution to at least two other complexities that occur when we try to formalize references to women’s work in HFRs. We might call these multiplicity and indirection. We quote again from our TEI 2013 poster:   By “multiplicity” we mean that the goods or services being transferred are actually a combination of distinct separate goods or services being transferred as a unit. A common example of this is a suit of men’s clothes, which at one point in the twentieth century might have meant either a jacket and trousers or a jacket, a vest, and trousers. The problem of “indirection” occurs when the goods or services being transferred are referred to indirectly or metaphorically. ... These two problems can, and often do, occur simultaneously. For example, the reference to “steak” on a restaurant receipt does not refer to a single chunk of uncooked meat as it would on the receipt from a butcher shop. Rather, it refers to the meat, and perhaps some sauce, in addition to the services of cooking the dish and delivering it to the table.  We do not know, at least not yet, which of these approaches is superior. However, we feel they are probably close enough to each other in expressive power and usability that the important thing, as Tommie Usdin points out[4], is for the HFR encoding community to pick one.   Conclusion We have considered here some problems that have arisen as we have tested the transactionography model. We are aware of numerous projects in which scholars are working with problems related to markup of HFRs, and we have created a website with space for both discussion of these problems and display of proposed solutions. We invite our colleagues to contribute to Encoding Historical Financial Records <encodinghfrs.org> as we continue to explore TEI-conformant models for markup of these abundant archival records.   Acknowledgement The authors would like to gratefully acknowledge C. Michael Sperberg-McQueen for not only finding problems with transactionogrophies, but also allowing us to bounce ideas around.  ",
       "article_title":"Problems in Modeling Transactions",
       "authors":[
          {
             "given":"Kathryn",
             "family":"Tomasek",
             "affiliation":[
                {
                   "original_name":"Wheaton College, Norton, Massachusetts, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Syd",
             "family":"Bauman",
             "affiliation":[
                {
                   "original_name":"Northeastern University, United States of America",
                   "normalized_name":"Universidad del Noreste",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/02ahky613",
                      "GRID":"grid.441462.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "standards and interoperability",
          "sustainability and preservation",
          "repositories",
          "archives"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Contrasting digital studies programs in the United States and Middle East highlight the impact of digitization, open access and digital collaboration on not only users’ cultures in these and other areas, but also the traditional scholarly culture. Early Islamic and Christian manuscripts offer invaluable opportunities to understand and analyze the transfer of mathematics, science and history through ancient texts.  Digitization, online scholarship and open sharing offer opportunities for broader access to manuscripts that are at increased risk in some libraries in the Middle East, with free access for citizens and scholars.  This open access empowers digital study by scholars and the public in the region and around the globe, while challenging traditional proprietary manuscript research and institutional traditions of protected access and exclusive study. These digital studies also empower communities of the Middle East – Christian and Islamic – to access their cultural history and artifacts, while highlighting the vulnerability of collections and the need to protect and preserve the digital data. In 2009, teams of scholars and technical personnel began three major manuscript digital scholarship initiatives:  The Walters Art Museum began digitization and online cataloging of Islamic and Western texts for free access, with the support of three successive National Endowment for the Humanities (NEH) grants.1 An integrated team of scientists, engineers, scholars and technical experts used privately funded spectral imaging to reveal the medical undertext of the Syriac Galen Palimpsest for free access in support of ongoing studies.2 St. Catherine’s Monastery in the Sinai Desert allowed the first large-scale spectral imaging of almost 200 palimpsests from the Monastery library by the Early Manuscripts Electronic Library (EMEL) for scholarly study, under Monastery access controls with support from Arcadia.3  These programs have had to address institutional, cultural and social challenges associated not just with the open study and access to large amounts of data, but amidst cultural upheaval and revolutions.  The intellectual property requirements of the owners and institutions also reflect different cultures – from that of a private owner and institution eager to share their collections with the world, to an ancient monastery contemplating how to transition their 1400-year tradition of protecting manuscripts held for centuries within their walls. Hosting Christian, Islamic and secular texts online highlights the challenges faced in addressing the cultural requirements of different religious and nonreligious traditions – not to mention the challenges of capturing, transferring and accessing data in a hostile region amidst kidnappings and multiple revolutions.4 This also poses significant challenges with ongoing global transitions within scholarship and institutions:   Institutions grappling with shifts from restricted pay-for-access models – based on physical protection of manuscripts – to free-access models with system maintenance costs. New generations of scholars capitalizing on digital access with cataloging/data integration tools, while others with limited technical expertise need mediated access with technical support. Scholars and institutions struggling with loss of control of their perceived patrimony and/or raison d'être.  The Walters Art Museum in Baltimore, supported by the NEH, is continuing its program to digitize their collection of medieval manuscripts.5 This started with digitization of their Islamic Manuscripts and continues by language and manuscript type with their Western Manuscripts.6 All data is hosted at thedigitalwalters.org, with new data uploaded regularly.7 The data includes “complete sets of high-resolution archival images of manuscripts from the collection of the Walters Art Museum, along with machine-readable TEI P5 descriptions and technical metadata, released for free under a Creative Commons Attribution-ShareAlike 3.0 Unported license8 for anyone who wants to use them.”  A workflow with virtual cataloging of the manuscripts from the posted images ensures this information is captured in the metadata.  Scholars provide cataloging information for each codex and leaf, which is then entered under their name into the digital record of the manuscript.  Uses of the data range from the Stanford University Mirador viewer study tool and online Searchworks catalog9 to the World Digital Library10, as well as Flickr11 – where images are used by global members to support a variety of interests. The manuscripts have been downloaded by institutions around the globe, including in the Middle East. The under text of the Syriac Palimpsest proved to be Galen’s medical treatise On the Mixtures and Powers of Simple Drugs in a linguistic transition from Greek to Arabic. The bound manuscript is an eleventh-century liturgical text that is also very important for the study of the hymns in its upper text.12 The private owner made all the image data and metadata available for free access at digitalgalen.net13, with license for use under Creative Commons Attribution 3.0 Unported Access Rights.14 This has enabled transcriptions and studies of the Galen text15, as well as an integrated study of herbals in ancient medicine with analyses of Galen’s work in the context of other texts.  It served as a catalyst for the “Floriental studies” by a group of technically savvy scholars who are analyzing works ranging from early cuneiform writings to Galen.16 With this open access, Dr. Grigory Kessel of Philipps-Universität Marburg was able to compare images of the Galen Palimpsest with other Syriac texts and find an additional four missing leaves. Two of these have now been spectrally imaged and are now freely available online1718, while the search continues for more. With the open access and collaboration that has developed around humanities data, including from the privately owned Galen Palimpsest and public Walters Art Museum manuscript collection, Archbishop Damianos and the monks of St. Catherine’s Monastery are assessing the potential and challenges of digital technology. They currently retain full control of the research data collected by EMEL with UCLA support, and a subset of the data is planned for public web release under the auspices of the Monastery.19 The Archbishop noted in a project workshop in October 2013: “Digitization offers opportunities for continued preservation of the data on various diverse geographical servers around the globe in the event of a catastrophe.”20 This is highlighted by the political situation in Egypt. The Archbishop also cited the importance of combining digitization and scientific/scholarly study, but with income for the monastery from digital images. To meet the Monastery’s wishes, access for initial scholarly study of the data is limited to a team of 20 eminent scholars in the 10 ancient languages found in the palimpsest undertexts, with information shared in an open-source cataloging tool. This changes the study methods of a generation of scholars who have traditionally conducted long-term, in-depth studies of texts with sole access. Currently this requires balancing the requirements for limited scholarly access with those for preservation and accessibility.  Each of these programs is addressing the impact that freely accessible data sharing is having on scholarly study. The integration of technologies and work processes to enhance digital scholarship and collections requires the following capabilities:   Virtual collaboration with dynamic online cataloging to host and update scholarly findings and shared research with user-friendly tools. Global teams of scholars digitally capture their latest findings and research in standardized catalogs of the Walters’ Art Museum and St. Catherine’s Monastery manuscripts. Broadly accepted standards for integration of data and metadata to ensure digitization of and online access to dispersed collection objects. This supports the digital reunification of diaspora manuscripts, such as the Galen Palimpsest originally from St. Catherine’s Monastery. Licensed Access to allow appropriate control over global sharing and access, while offering confidence to institutions and scholars that their intellectual property will be protected. This allows the monks of St. Catherine’s to share data from manuscripts they have protected for centuries21, yet flexibility for institutions to ease into broader access – as demonstrated by the Walters’ shift to less restrictive Creative Commons licenses.  Integrated teams of scholars and technical experts ensure scholarly needs are addressed in development of technology and data management. Embedding technically savvy scholars with each of these project teams has ensured analytical and academic research needs are addressed in all project phases, from data and metadata collection through hosting and access to the data.22 Open data pose challenges for an older generation of scholars who have traditionally held onto data until they complete their studies, while empowering a newer generation of scholars capable of collaborating with this data, including in conjunction with other data and tools.23 Including more technically facile scholars and students on project teams also provides support for the generation of scholars who lack needed technical skills to access and study digital data with appropriate technical tools.24 More important than the cloistered challenges posed by traditional academia are the challenges of preserving the cultural information and patrimony of communities at risk in the Middle East. These three projects highlight the new opportunities digitization and digital scholarship offer for research and analysis around the globe. This is especially true in the Middle East, where they contribute to understanding of long-standing Christian and Islamic communities and traditions. These traditions and cultures are at risk from secular, religious, economic and social conflict, as are the manuscripts that have supported their development. With data hosted in the United States, the Middle East or elsewhere, free data access and proliferation help ensure preservation of the cultural patrimony, while also supporting communication within and across cultures.   Fig. 1: Galen Syriac Palimpsest Leaf 166r-171v Pseudocolor Image   ",
       "article_title":"When Kidnapping is but One Risk: Digital Studies Challenge Scholarly and Regional Cultures",
       "authors":[
          {
             "given":"Michael",
             "family":"Toth",
             "affiliation":[
                {
                   "original_name":"RB Toth Associates",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"R. Douglas",
             "family":"Emery",
             "affiliation":[
                {
                   "original_name":"Schoenberg Institute for Manuscript Studies, University of Pennsylvania",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "medieval studies",
          "licensing",
          "cultural studies",
          "sustainability and preservation",
          "digital humanities - nature and significance",
          "repositories",
          "copyright",
          "digital humanities - institutional support",
          "interdisciplinary collaboration",
          "archives"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Considerable scholarship in stylometry has focused on authorship attribution. Such work is based on the assumption that rates of high frequency \"function\" words (in contrast to \"content\" words) are reliable clues to authorship and are largely independent of factors like theme or genre1. More recently, focus seems to have moved beyond the most frequent words to involve all vocabulary appearing in a corpus (2, 3, 4). As many of these words vary strongly by context, factors like theme, genre, literary period or literary form have received greater attention. This paper makes two contributions. First, we test the hypothesis that authorial style depends on genre and find that this is indeed the case, even when only considering the most frequent words. Second, in light of this result, we argue that adding additional features such as genre to a familiar model of authorship attribution offers a useful and novel way to investigate how authors' writing varies depending on context. We demonstrate how stylistic analysis making use of more articulate probabilistic models might move beyond established but limited models such as principal component analysis and distance-based clustering and achieve a better fit between model and hypothesis.   2. Data In French literary studies, there is longstanding interest in analyzing the formal and stylistic constraints associated with classical theater (5, 6). Playwrights from this period, such as Pierre Corneille and Jean Racine, figure prominently in early quantitative work in French literary studies, predating the use of digital computers (7, 8). Whereas this pioneering research focused on single texts or a single author's works, today's availability of a wide range of digital texts, of flexible tools, and of vastly increased computing power permits more complex methods of analysis. We have chosen to work on a corpus of 108 plays in three genres written by eight authors. The plays were produced over a period of roughly five decades (1630-1678) and the authors were selected because they wrote several plays in more than one genre. Table 1 illustrates the distribution of the plays across authors and genres.    comedy tragi-comedy tragedy    Corneille, Pierre 9 1  20    Corneille, Thomas 8 0  15    Du Ryer 1 7  6    Molière 7 1  0    Quinault 1 1  3    Racine 1  0  9    Rotrou 1 4  3    Scarron 8 2  0     Totals   36    16   56    All texts are taken from the \"théâtre classique\" collection (9) and have been preprocessed to include only character speeches.10 In order to better explore the variability of writing found among the authors and genres in the corpus, each play has been split into approximately 1,000 word sections. After processing, the corpus used for analysis contains 1,605 sections. Only the most frequent 100 function words in the corpus are retained.11   3. Hypothesis and Method Our hypothesis is that authorial style varies depending on genre. In order to test this hypothesis, we compare three models that predict the author of a section based on word frequencies and the genre of the section. The first model predicts the author based on word frequencies alone, ignoring information on genre. The second model adds to the first rudimentary information about how likely authors are to appear in each genre. The third model differs from the second in that it predicts the author of a section based on word frequencies for each genre separately. If authorial style varies by genre, then the third model should perform significantly better than the first model. All three models are multinomial logistic regressions.12 Multinomial logistic regression has been used for authorship attribution before13, but our approach expands on this by examining the use of a non-traditional covariate such as genre. Our aim is to encourage the building of interpretable models in order to understand how variables such as genre influence authorial style. In statistical terms, the first model includes a global intercept parameter and word frequencies as predictors. The second model adds a genre-specific intercept parameter. The third model differs from the second model by allowing the regression coefficients associated with word frequencies to be different depending on the genre. These models may be expressed symbolically as shown in Fig. 1 (where the text section is indexed by i and softmax_k(a) is the extension of the inverse logistic function to multiple categories).   Fig. 1: Three models of logistic regression  A point estimate for the parameters is obtained by maximizing the likelihood function using numerical methods. Models are fitted using randomly selected sections corresponding to four-fifths of the corpus. Models are then compared by measuring their out-of-sample predictions: an error rate for each model is calculated on the remaining fifth of the play sections, asking the model to predict the sections' authors based on word frequencies and, where applicable, genre information. This procedure is repeated fifty times, each time randomly partitioning the corpus.14   4. Results The error rates associated with each model are shown in Fig. 2.   Fig. 2: Held-out author classification error rate (100 features)  In 49 out of the 50 trials, model 3 had the lowest error rate. In this corpus and for these authors, there is therefore little doubt that authorial style varies by genre. Table 2 shows the average error rates by model and genre.      Model 1   Model 2   Model 3    Comedy 0.24 0.23 0.19   Tragi-comedy 0.25 0.22 0.10   Tragedy 0.17 0.15  0.13      5. Discussion The variation of authorial style by genre underlying these results is best illustrated by looking at the frequencies of selected words that depend on both author and genre. For example, a few words are used with consistency across genres by one author but in another author vary considerably depending on genre. Table 3 indicates relative frequencies for three such cases.     Pierre Corneille: comedy Pierre Corneille: tragedy Thomas Corneille: comedy Thomas Corneille: tragedy   \"est\" 22.0 20.9 31.6 24.7   \"par\" 6.4 6.4 6.3 9.4   \"au\" 5.1 5.9 6.1 6.3   The auxiliary \"est\" and the preposition \"par\" are both used consistently across genres by Pierre Corneille but with a widely varying frequency between comedy and tragedy by Thomas Corneille, while the opposite behavior is true of \"au\". The preposition \"par\" is associated very frequently, in Thomas Corneilles plays, with causality (reason or effect) linked to emotions or moral principles (par bonté, par la gloire, par le respect). While the auxiliary \"est\" (third person singular present tense of \"être\") has an even more elusive semantic charge, it is mostly associated, in Thomas Corneille's plays, with statements of fact. Both phenomena seem to indicate a greater reliance, by Thomas Corneille, on causal relations and factuality in the tragedies than in the comedies, whereas the same contrasting treatment cannot be observed in Pierre Corneille. The existence of such variation points to two notable facts. First, and contrary to common understanding, some very frequent function words other than personal pronouns do vary with genre within the work of a given author. Second, whether this is the case does not depend on the word in itself, but may differ from author to author. Therefore, such words are not exclusively or inherently markers of genre. Even when using only the very most frequent function words and even when excluding personal pronouns, then, authorship attribution cannot rule out that some influence from genre also comes into play.  On a different level, an explanation for the better performance of model 3 over model 1 brings in contextual information from literary history. Tragedies are usually described as being more closely bound to conventions of the \"doctrine classique\" than comedies or tragi-comedies (15, 16). Therefore, the range of vocabulary and the pattern of usage would be expected to be more predictable in tragedy than in other genres. Were this indeed the case, a model might achieve a lower variance in its predictions by considering tragedy separately. This hypothesis is difficult to evaluate as it is difficult to \"hold constant\" authorship; authors tend not to write in equal amounts in different genres.  A critical explanation of model 3's superior predictive performance would point out that the task of predicting an author on the basis of word frequencies might change dramatically depending on the authors being compared. It might therefore be suggested that the better performance obtained by model 3 reflects this fact more than it reflects within-author variation across genre. In response to this criticism, it should be observed that model 3 performs better even when the same authors are being compared; Pierre Corneille and Thomas Corneille dominate numerically the samples from comedies and tragedies. Furthermore, the words shown in table 3 demonstrate that there is variation within an author's style across genres. Model 3 is designed to use this variation to attribute authorship.   6. Conclusion We offer the following conclusions from this experiment. First, authorial style does appear to vary with genre even when considering only the 100 most frequent words. This suggests that factors such as genre should be systematically taken into account for authorship attribution. Second, logistic regression is a useful method in this context and should be part of the stylometric toolbox as it permits a range of information to be modeled jointly with authorship. Logistic regression could also be used to test for further relevant factors beyond genre, such as form (e.g. verse and prose) or theme (e.g. historical plays vs. religious plays).   ",
       "article_title":"Progress Through Regression. Modeling Style across Genre in French Classical Theater",
       "authors":[
          {
             "given":"Christof",
             "family":"Schöch",
             "affiliation":[
                {
                   "original_name":"University of Würzburg, Germany",
                   "normalized_name":"University of Würzburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00fbnyb24",
                      "GRID":"grid.8379.5"
                   }
                }
             ]
          },
          {
             "given":"Allen",
             "family":"Riddell",
             "affiliation":[
                {
                   "original_name":"Dartmouth College, USA",
                   "normalized_name":"Dartmouth College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/049s0rh22",
                      "GRID":"grid.254880.3"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "literary studies",
          "french studies",
          "data mining/text analysis",
          "authorship attribution",
          "stylometry"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  We may see, in a TEI transcription of an old book, the lines:  <pb n=\"[iii]\"/> <p>Quaestiones, quae ad mathematicae fundamenta pertinent, etsi hisce temporibus a multis tractatae, satisfacienti solutione et adhuc carent.  What do they mean? How do we know what they mean? Can we model their meaning formally? Formalizing the meaning of arbitrary natural-language utterances remains intractable today. But markup languages, being formally defined artificial languages, appear more approachable. So we may be able to explain what the <pb> and <p> elements mean, even if the sense of the Latin eludes formalization. Some propose to explicate the meaning of markup by specifying, for each construct in a markup vocabulary, a sentence schema in a natural language, with blanks to be filled in with data from the document 1; others make a similar proposal but allow sentence schemata in formal languages like first-order predicate logic as well 2. This appears straightforward, although far from trivial, for metadata 3and perhaps even for born-digital texts, but how shall the meaning of <p> be formalized in a markup language which defines it as containing a transcription of a text block in a manuscript? What does it mean for a document to be a transcription of another document? Can we formalize that? Earlier work has explored the nature of the similarity between transcripts and their exemplars. Perhaps it consists simply in their containing the same sequence of characters? This can be formalized but proves disappointing, partly because it omits text structures like division into paragraphs and partly because it offers no way of describing disagreements among transcribers about how to read the exemplar, or which character distinctions (e.g. i/j, u/v, s/ſ) to retain and which to level. It is also wrong: few transcripts have exactly the same character sequence as their exemplar 4. Later work extends the analysis from characters to higher-level textual structures and models transcriber agreement and disagreement explicitly 5 6. (Paul Caton has built on this idea to propose pure transcriptional markup as an approach to the problems of transcription semantics 7.) By treating higher-level constructs as tokens of higher-level types and using the sentence schemata mentioned earlier, we believe one can formalize the meaning of tokens (characters, words, and XML elements like <p> and <pb> alike) in transcriptions. Clearly, however, the meaning of tokens in a transcription depends on the transcription conventions adopted, which vary. There is hardly any universal transcription practice: for every generalization we find exceptions 8 9 1011 . Is everything in the exemplar transcribed? Not when deletions and irrelevant material are excluded. Does everything in the transcription reproduce some word or character in the exemplar? Not when line breaks are marked explicitly with vertical bars, or notes are added. Many scholarly editions account for variations like these in an explicit statement of transcription practice. Such statements typically describe ways in which practice varies from the usual practice, but rarely the ways in which it exemplifies normal practice. In any community of scholarship, some common practice is typically felt to be so obvious that it needs no mention or explanation. One job of formalization is to make explicit practices and assumptions otherwise passed over in silence. We propose a notion we shall call transcriptional implicature, denoting a set of rules which apply by default but which may be overridden in particular cases, analogous to the rules of conversational implicature proposed by H. P. Grice as a way of explicating the logic of everyday conversation12. The operational definition of transcriptional implicature for a given community is “the set of rules no one in the community bothers to mention explicitly”. Different communities of transcription practice have different sets of tacit assumptions and thus different rules of transcriptional implicature. Is there a common core of transcriptional practice shared by all communities? Maybe; it's an empirical question. A serious answer would require detailed study of a wide variety of communities of practice. We postulate, however, that the transcriptional implicature of any community of practice can be described with reference to some default set of rules for transcriptional implicature. The transcriptional practice of any given project is commonly documented by listing its deviations from the transcriptional implicature of the relevant community. If that transcriptional implicature can (as postulated) be described as a set of deviations from the default transcriptional implicature, then it follows that any project's transcription practice can be described with reference to the default transcriptional implicature, by merging the two lists of differences. We propose to identify this hypothetical default transcriptional implicature with the rules outlined below. In the formalizations, “T” denotes a transcript, “E” its exemplar. Adopting the extended use of the type/token distinction mentioned above, the default transcriptional implicature can be summed up in a single rule:   1. A transcript and its exemplar have the same type. Formally: type(T) = type(E)   In interesting cases, E will have a complex type consisting of some structure of smaller types (which in turn consist of smaller ones still), instantiated by a complex token which similarly consists of smaller tokens. It is a consequence of (1) that:   2. There is a one-to-one correspondence between the tokens of a transcript and the tokens of its exemplar, such that every pair of corresponding tokens have the same type.   Formally, this is a second-order statement, but we can approximate it using the following first-order sentence, which assumes a function tokens which maps from a document to the set of tokens contained in that document, and the relations RET (mapping from tokens(E) to tokens(T)) and RTE (the other way round). (∀ t1 : tokens(E))    (∃1 t2 : tokens(T))      (t2 = RET(t1)) ∧ (∀ t1 : tokens(T))    (∃1 t2 :tokens(E))      (t2 = RTE(t1)) ∧ (∀ t1 : tokens(E), t2 : tokens(T))    (t2 = RET(t1) ⇔ t1 = RTE(t2)    ∧ type(t1) = type(RET(t1)))   It is easier to relate variations in transcription practices to the default transcriptional implicature if we paraphrase (2) as a conjunction of simpler rules (3) - (6):   3. For every token in the exemplar there is exactly one corresponding token in the transcript. (∀ t1 : tokens(E)) (∃1 t2 : tokens(T)) (t2 = RTE(t1)) Applied to the example with which this document begins, this means: each token in the exemplar maps to a token in the transcript. We can infer that the sentence quoted does not contain the word non, because otherwise non would appear in the transcript.   4. For every token in the transcript there is exactly one corresponding token in the exemplar. (∀ t1 : tokens(T)) (∃1 t2 : tokens(E)) (t2 = RET(t1)) Applied to the example: each token in the transcript maps to some token in the exemplar. We can infer that the exemplar contains some token corresponding to the word Quaestiones in the transcript.   5. The relations identified in rules (3) and (4) are inverses: that is, for every pair of tokens t1 in the exemplar and t2 in the transcript, if t2 corresponds to t1 as described in rule (3), then t1 corresponds to t2 as described in rule (4). (∀ t1 : tokens(E), t2 : tokens(T)) (t2 = RET(t1) ⇔ t1 = RTE(t2))   6. In every pair of corresponding tokens, the two tokens are tokens of the same type. (∀ t1 : tokens(E)) (type(t1) = type(RET(t1))) Applied to the example: we can infer that the token in the exemplar which corresponds to the word Quaestiones in the transcript is itself a token of the same word type.   We observe that many, perhaps all, variations in transcription practice can be classified by which rule they override.   Silent expansion of abbreviations and normalization of spelling exclude some individual characters in E and T from the scope of rules (3) and (4); those rules typically still apply to words and higher-level tokens.   Expansion of abbreviations in brackets can preserve the character-level mapping, but introduces characters in T which are exceptions to rule (4), since they lack corresponding characters in E. The use of vertical bars (|) in T to record line breaks in E is also an exception to rule (4).   Omission of selected material (deleted words, additions from a second hand, ...) modifies rule (3) by identifying tokens in E which are not represented in T.   Both transcriptions which distinguish archaic allographs (i/j, u/v, s/ſ) and those which level non-graphemic distinctions obey rule (6), but they read the document with different type systems.   The principle of charity (“in cases of doubt assume E is correct”) can also be interpreted as a further elaboration of rule (6).   The full paper will explore these and further ways in which the practice of transcription can deviate from the default rules of transcriptional implicature as we proposed to define them, and show how the variations in practice can be described formally.  ",
       "article_title":"Transcriptional implicature:  a contribution to markup semantics",
       "authors":[
          {
             "given":"Michael",
             "family":"Sperberg-McQueen",
             "affiliation":[
                {
                   "original_name":"Black Mesa Technologies LLC, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Yves",
             "family":"Marcoux",
             "affiliation":[
                {
                   "original_name":"Université de Montréal",
                   "normalized_name":"University of Montreal",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0161xgx34",
                      "GRID":"grid.14848.31"
                   }
                }
             ]
          },
          {
             "given":"Claus",
             "family":"Huitfeldt",
             "affiliation":[
                {
                   "original_name":"Universitetet i Bergen",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Digital storage systems are like tins. They might have some content but the only evidence are labels, captions or any kind of lettering. In addition maybe the weight of the tin could be taken to judge about its inside but still to get full assurance the tin has to be opened to be able to identify the content correctly. In order to open it an appropriate tool is necessary and another one to take something out of it, something like a fork or a spoon.  A digital storage like a hard drive behaves very similar to that but in case of magnetic recording the data is not only invisible from in and outside the storage, not even can it be detected physically by a human being. Digital data has no appearance that can be touched, we have no sense for it.  In addition to that any bit stream stored on a data carrier needs to be migrated after a certain time to ensure accessibility and consistency because following three factors endanger the digital archiving process:  The storage media decays over time and it can fail by aging. Hardware gets incompatible so that accessing the data becomes impossible. File formats – technical metadata – change and develop over time. Therefore programs to interpret the file content might not be available in future. Missing contextual metadata make any digital bit stream more or less useless.   Migration Each of these possibilities is a major drawback for cultural heritage preservation and each one renders data into digital waste, data that is either lost completely or without meaningful sense. Continuous migration and copy storage content in periodic intervals are today’s best practice to transform binary information into the future. Theoretically migration works very well because digital data can be copied without loss, which is one of the major advantages of any digital code like binary information or our even alphabet. The ability to copy data lossless allows not only the arbitrary replacement of the data carrier, it also allows to increase redundancy by storing multiple copies, as e.g. proposed by LOCKSS1. The down side of migration is the financial effort associated with it. Independent of the specific costs per migration, archiving is getting expensive sooner or later because of the short lifetime of the technology. In addition its dependence on numerous cascaded technologies makes it a fragile process that can cause dramatic data loss if only one of the incorporated components fails.   Migration can be omitted if the storage media fulfills the following requirements:  It must contain human readable metadata in order to describe the archived object and it's context. Information on how to recover the original file (the decoding manual) must be part of the metadata. This knowledge is the key to interpret the archived byte stream. The file format must be well documented (open format) and it must be widely used to ensure its accessibility over time.    Digital data is stored hardware independent as far as possible. Thus it is not affected by the change of technology.  If a medium claims to be suitable for long-term preservation of digital data it has to fulfill more requirements. Lunt et al.2 identified 7 characteristics, which are particularly interesting to archivists regarding preservation of digital data. The first says, there shouldn’t be active maintenance or migration required to preserve actual data. They continue with: 2) no special storage conditions are necessary to preserve the storage media; 3) a minimum lifetime of at least 100 years, preferably more should be supported; 4) no energy is required to maintain the data; 5) the media is easily transported; 6) the data format is widely adopted; 7) the medium has a large storage capacity. Bits-on-Film Approach  Facing those facts the Digital Humanities Lab of the University of Basel has developed a workflow for migration-less preservation of digital data on optical media called “Monolith”. It combines the advantages of photographic material and standard digital imaging technology to create a long-term migration-less archiving system. This is achieved by the hybrid characteristics of the optical carrier. Any arbitrary binary bit stream is put right besides human readable technical, structural, and contextual metadata. Original files of any appropriate format are stored on film as visual 2D-barcodes. Technically spoken every bit of the original bit stream (the file to be stored) is converted into a spot representation on film. A full bit stream then results in a two-dimensional image, an “image of bits“. In other words the logical data-bits are transformed and represented by dye or silver of photographic film. This process can be regarded as a materialization of binary data, which becomes visual and physical. Monolith has no limitation regarding the format of the file to be archived. However, the documentation of the file format must be part of the metadata and therefor it should be an open standard like the widely used PDF-A or image formats like JPEG2000 (3). Metadata can be stored binary or as human readable text information, e.g. encoded and written in letters on film as any of the well-known standards like Dublin Core, METS or others.   This approach has various advantages: First, and most important, the bit stream on film can be read/captured by any digital camera, there is no special hardware necessary to transform the physical representation of the bits back into logical states within the computer system. This can be compared to the process of seeing. As human beings see letters – in fact digital data – the camera sees signs, spots on film – binary data; Monolith has a visual interface. The decoding of the binary bit stream is well defined because the explanation of the code is an inseparable part of the technical metadata set written on film. Like no other storage media Monolith can not only contain barcodes and text but images – e.g. thumbnails – as well. Besides its technological features the storage film has another advantage. It can be stored the same way as regular archival film. There are no special storage conditions necessary nor does the film need any specific care. Therefore, Monolith can be regarded more as an “engraved stone“ than as a data storage for computer systems. It is a “Digital Rosetta Film“.  But is the application of optical film for archival purposes reasonable these days? Many companies stopped production high fidelity film material and very likely the quality – not stability – of film will drop in the future. For the representation of photographic images this is of course a major draw back since image quality is directly related to film quality. In case of Monolith this is irrelevant. The only function the film has to fulfill is to separate dots spatially, requirements that are achieved by most photographic materials. The quality of digital originals will not be disturbed by film quality because they are stored as binary data and therefore decay of the material has little impact. In addition any well-known error correction method can be applied. The concept of a binary representation of data is a simple but a very efficient solution and it is the reason why every computer storage system is adapting this concept4. Even if film won't be available in future, for any existing Monolith this means no impact. Not for the sustainability nor for the future ability for data recovery. All those features show that materialized bits are not only a nice concept to mimic historic documents but also an efficient way to transport digital cultural heritage into the future5. In the presentation we will show how Monolith works and what its advantages are.    MonolithTM on 35mm color material   Fig. 1: Monolith™ includes all necessary information for future information recovery. Especially contextual metadata and the decoding manual to understand the structure of the bit-pattern.  Conclusion  Monolith is a solution that has made its way from university to a commercial company. It shows that there is a possibility for an alternative solution for classical digital archiving, that doesn't need to be migrated. The advantages are not only of technical but also of economical nature. Even if costs for plain storage media decrease with time, total costs of ownership for archived digital data increased in the last years continuously and migration is the primary costs driver of archiving. Therefore Monolith can not only be an answer for technological but also for economical challenges on the way of digital information to the future.  ",
       "article_title":"Monolith: Materialised Bits, the Digital Rosetta Film",
       "authors":[
          {
             "given":"Peter",
             "family":"Fornaro",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Wassmer",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Lukas ",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Rudolf",
             "family":"Gschwind",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "sustainability and preservation",
          "repositories",
          "archives",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction From colonial times to World War Two, most of Australia’s many newspapers incorporated serial fiction, including local and overseas titles. The Australian fiction in these periodicals has largely been identified (Austlit), and important research in this area is ongoing (Bode 2012; Gelder 2011). However, very little is known about the overseas works, including the titles, authors and themes, and their circulation and reception in Australia. An important reason for this lack of knowledge is the size of the archive. With hundreds of newspapers – many containing multiple instalments of novels per edition – a systematic manual search for fiction is unfeasible. The search possibilities for this archive have dramatically expanded with the creation of the National Library of Australia’s (NLA) Trove database. From 2007 to 2012, the NLA digitised over four million pages of Australian newspapers, from every state and territory, published from 1803 to the 1950s. Combined with digital humanities methods for data mining and analysis, this ongoing digitisation project makes identifying serial fiction in Australian newspapers possible for the first time in a systematic and reliable way. The project reported in this paper describes a computer-enabled approach to exploring the presence, circulation and reception of fiction in Australian newspapers that enables research, and advances arguments, relevant to bibliographical, book historical and literary studies as well as digital humanities.   2. Bibliography The project showcases how digital humanities methods can significantly enhance bibliographical records and knowledge. Searching Trove using terms associated with serial fiction – including ‘chapter’, ‘story’ and ‘fiction’ – enables identification of potentially relevant records. The bibliographic information and full text results of these searches are extracted as CSV and text files using a Python harvesting tool developed by Sherratt (2013). These files are supplemented through additional research (for instance, the authors’ nationalities and gender), and transfer to a database that will be freely accessible to researchers and the public. This approach is providing extremely effective in identifying serial fiction. The first search – of ‘chapter’ – yielded approximately 200,000 individual instances of fiction in Australian newspapers. Many other searches remain to be done; however, even this initial result demonstrates this method’s capacity to enhance bibliographical records. This search process will undoubtedly reveal previously unrecorded instances of publication, particularly of non-Australian fiction. Some of these instances will almost certainly be of titles that have not been indexed previously, including by well-known authors. More broadly, this project demonstrates the potential of digital humanities methods to maximise the utility, and thus enhance the value and consequence, of digital collections.   3. Book History The collected bibliographic data enables quantitative analysis of the transnational movement of fiction. This approach builds on earlier studies, most prominently, Moretti’s ‘distant reading’ (2005) and, more recently, Jockers’s ‘macroanalysis’ (2013). In terms of the archive searched and the cultural phenomena analysed, it is also indebted to Nicholson’s identification and analysis of American jokes in digitised nineteenth-century British newspapers (2012). Importantly, however, unlike these other works, the body of data underpinning this project’s arguments and findings will be publicly available, so other scholars can explore, check, extend and potentially challenge the findings; and so this data can be reused in future research. Findings of initial data analysis, for 1830 to 1880, already indicate trends that challenge existing perceptions of Australian literary culture. Where metropolitan newspapers are routinely identified as the main Australian serial fiction publishers (e.g. Webby 2000), this study highlights the strong involvement of regional newspapers. This finding challenges the existing centre/periphery understanding of colonial literary culture. Also contesting this model is the revelation that – while overseas fiction has been estimated to vastly outnumber local titles (Morrison 1998) – in this period, more local than overseas fiction was published. One interesting outcome of this strong local publication is a reversal of the much-discussed female-dominance of nineteenth-century serial fiction authorship. Although most American and British serial fiction was by women (Casey 1996; Coultrap-McQuin 1990; Thompson 1999), men wrote the majority of titles in Australian periodicals in this period. While local titles outnumbered overseas fiction, this initial search has identified a significant amount of non-Australian titles, including a higher-than-anticipated number of American stories, as well as fiction from a wide range of countries besides Britain, including China, Russia, France and Germany. As well as highlighting the status of Australian periodicals as ‘contact zones’ (Pratt 1990) for literature, this range of national literatures further challenges a centre/periphery understanding of colonial literary culture. This project’s combination of digital humanities and book history suggests important directions for the former as well as the latter field of study. Book history is increasingly recognised as playing an important role in the development of digital humanities. Alan Liu describes book history as a Levi-Straussian ‘trickster figure’ for digital humanities, uniting the field’s commitment to older humanities disciplines, and the value of the old itself, with more recent interest in emergent media and design (2013, 410). Elsewhere he points to the way book historians ‘increasingly compare, and not just contrast, earlier writing/reading practices to their digital successors’ (2012, 16), and the potential of this approach to enhance understanding the digital age and the digital humanities. The project employs this comparative framework to consider reading practices. While one might assume nineteenth-century newspapers differ entirely from the Internet, in fact both are networked interfaces uniting various content, including that previously published elsewhere, for readers who have significant autonomy in deciding what to read and what connections to draw. Notwithstanding these significant parallels, it is equally important that the use of digitised archives, and digital humanities search and retrieval methods, not occlude historical context. In particular, this project works to maintain a view of nineteenth-century newspapers as coherent and interconnected cultural artefacts rather than containers of discrete content (a perception potentially encouraged by search results in the form of individual articles).   4. Literary Studies The full-text records extracted from Trove provide the basis for computer-assisted textual analysis, particularly topic modelling. This aspect of the project will follow, and in so doing, test and extend Jockers’s analysis of influence in relation to Irish, English and American literature (2013). Topic modelling will be used to investigate whether, and if so, to what extent, local stories in Australian newspapers employed similar themes, language, or generic strategies to the other-national literatures alongside which they were published. The same method will be used to consider relationships between other-national literary forms. Like Moretti’s and Jockers’s analyses, this project will contribute to shifting literary studies beyond a nation-based framework. However, where these earlier studies consider general bibliographic corpora, in exploring texts published alongside one another, this project provides an important opportunity to consider influence in relation to a specific material context: that is, fiction received and experienced by particular readers at particular times.   5. Digital Humanities McCarty's notion of modelling is a key concept in this project's formulation and development. In McCarty’s words, a model is ‘an abstraction or simple representation of a more complex real phenomena’ (2008), and modelling enables exploration of and experimentation with phenomena that would otherwise be intractable or inaccessible (2005: 27). This project will complicate and extend this methodological framework by highlighting the multiple number and layers of models and modelling processes involved in exploring serial fiction in Australian periodicals. These layers include the digitised newspaper pages (themselves created from other models, predominantly microfiche), the Trove database more broadly, the database in which the search and harvesting results are represented, as well as the subsequent quantitative analyses of bibliometric and textual data. Where McCarty has always insisted upon the status of models as fictions, this foregrounding of multiple and layered models emphasises the radical contingency of this foundational concept for digital humanities, as well as the theoretical nature of its outcomes. Foregrounding the contingent and theoretical nature of modelling has two key implications for this project, and for digital humanities research broadly. First, it provides the groundwork for working with an historical record that necessarily contains multiple gaps: Trove has not digitised all Australian newspapers; some records have been lost, others are still to emerge; the quality of OCR for the texts differs radically; and the search process will not discover all serial fiction in Trove. Second, it enables a recognition that even the historical record we have – including what might be considered its obvious facts – needs to be treated as contingent and theoretical. For instance, bibliographic details added to the database – such as the name and gender of authors – are obviously facts, but may not have been present to historical readers (stories were published anonymously or under pseudonyms) and thus cannot be taken as absolute points of reference for understanding the historical circulation and reception of fiction. In moving away from understanding quantitative analysis of archival records as proof of historical phenomena, the underlying framework seeks to forge a conversation between bibliographers, archivists, book historians, literary critics and digital humanists that is data-rich, but oriented towards theoretical possibilities and constructs rather than proof and measures.  ",
       "article_title":"Mining a 'Trove': Modelling a Transnational Literary Culture",
       "authors":[
          {
             "given":"Katherine",
             "family":"Bode",
             "affiliation":[
                {
                   "original_name":"Australian National University",
                   "normalized_name":"Australian National University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/019wvm592",
                      "GRID":"grid.1001.0"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "bibliography",
          "databases"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Our paper aims at presenting TheofPhilo-Thesaurus of Philosophy, the prototype of a digital multilingual thesaurus in the field of Philosophy. With ‘thesaurus’ we mean a concept-based collection of terms1 that we are building by means of philosophical texts and dictionaries. The purpose of TheofPhilo is to test potentials and limits opened by the interaction between digital tools and devices (such as digital archives and libraries) and the long tradition of historical and lexicological studies of the Istituto per il Lessico Intellettuale Europeo e Storia delle Idee (www.iliesi.cnr.it), which developed the thesaurus. The work on the prototype implied the cooperation of experts in History of philosophy, Linguistics and Computer science, and has been conceived in the frame of the semantic web. We are currently testing the collection of terms and building up an ontology finalized to semantic enrichment and to information retrieval of the digital resources uploaded in the portal Daphnet. Digital Archives of PHilosophical Texts on the NET (www.daphnet.org). The plurality of languages coexisting in the portal lead us to design a multilingual collection of terms, in order to enable scholars, students, teachers and other interested users to search within the large quantity of texts in Daphnet and – this being an important added value – to make queries by using one owns mother tongue. TheofPhilo is not a dictionary of philosophy and it does not offer definitions of philosophical terms: we want it to be a useful map to enrich texts and to retrieve information. TheofPhilo itself could be an interesting object of research as, once completed, it will allow a linguistic analysis of the philosophical terminology structured according to the criteria we are adopting (see § 3). For this reason we intend to carry out a parallel work on this tool, developing a linguistic and an historical-philosophical study on the collection of terms we have selected.   2. The content The Daphnet portal, implemented within the project AGORA. Scholarly Open Access Research in European Philosophy (www.project-agora.org)23, consists of two Open Access platforms, Ancient and Modern Philosophy. The first one contains (a) the transcription of the collection of Presocratic thinkers originally edited by H. Diels and W. Kranz, with the Italian translation edited by G. Giannantoni; (b) the transcription of the Socratis et Socraticorum Reliquiae by G. Giannantoni; (c) the volume Vita e opinioni dei filosofi (the editorial collection is by R. D. Hicks, H. S. Long and M. Marcovich, the Italian translation is by M. Gigante); (d) the Opera Omniaof Sextus Empiricus (ed. by H. Mutschmann). The second platform,Modern Philosophy, gives access to a number of Latin, Italian, French and German texts which are considered representative of the philosophical thinking of the 16th, 17th and 18th centuries. It includes works by Alexander Gottlieb Baumgarten, Giordano Bruno, René Descartes, Immanuel Kant, Gottfried Wilhelm Leibniz, John Locke, Baruch Spinoza and Giambattista Vico. Daphnet also presents an OJS platform dedicated to secondary sources, the Daphnet Digital Library, containing a wide selection of articles published in the journal Elenchos. Rivista di studi sul pensiero antico (Bibliopolis, Napoli), in Lexicon Philosophicum. Quaderni di Terminologia filosofica e storia delle idee, and in the volumes dedicated to the proceedings of the international conferences organized by CNR-ILIESI. In addition to these critical essays, the portal presents the monograph by Emidio Spinelli, Questioni Scettiche: letture introduttive al pirronismo antico (Lithos, 2005) and the brand new online journal Lexicon Philosophicum: International Journal for the History of Texts and Ideas(www.lexicon.cnr.it). The set of lexical items extracted from texts and multilingual philosophical dictionaries (see § 3) includes Nouns, Adjectives, Verbs and Adverbs, both monorhematic and multiword expressions. Fig. 1 shows a query’s result, namely for the French philosophical subject ‘abduction’. The software presents the interlinguistic equivalents available in the five languages implemented so far (Latin, Greek, Italian, French, and English). Fig.2 shows another example, related to the multiword expression ‘harmonie préetablie’, that is presented in TheofPhilo both as autonomous lexeme included in the alphabetical list and in the box related to its belonging terms (Fig. 3). As result, from the expression ‘harmonie préetablie’, it is possible to reach the box related to the belonging lexeme ‘harmonie’ and viceversa.   Fig. 1: Philosophical Subject ‘abduction’    Fig. 2: Philosophical Subject 'harmonie préetablie’    Fig. 3: Philosophical Subject 'harmonie’  The complexity of terminological relations -caused by the complex and often problematic interchanges between cultures and languages during the history of philosophical thought- emerges not always so plainly as in the cases just presented. Fig. 4 shows the results of the query made for ‘acte’, which presents a large variety of equivalent terms. We can manage and control the large number of terms by using MySQL technology.   Fig. 4: Philosophical Subject ‘acte’     3. Tools and procedures The ontology456 built to represent the content, is structured according to the following four categories: Persons (philosophers, scholars); Relevant Concepts (philosophical subjects, relevant events); Relevant Subjects (geographical entities, philosophical themes, philosophical schools, quotations, titles); Sources (secondary and primary sources).  TheofPhilo’s specific purpose is to populate the sub-category of the philosophical subjects by two typologies of relations: interlinguistic equivalence and intralinguistic semantic relations.  From the procedural point of view, the work was carried out according to the following phases: digitization in Excel format of the multilingual entries systems lemmatized in N. Abbagnano’s Dizionario di Filosofia (Torino 1998) and in A. Lalande’s Vocabulaire technique et critique de la philosophie (Paris 1983); merging of the Greek and French philosophical subjects selected for the semantic enrichment experiments; acquisition of relevant French and Greek terminology using S. Maso’s Lingua Philosophica Graeca (Milano-Udine, 2010), in which relevant Greek philosophical terms are presented along with their Latin, Italian, English, French and German equivalents; acquisition of Latin, Italian and English equivalents. In the last phase, in order to enrich the interlinguistic equivalences, we used the following lexicographical sources: A. Bailly, Dictionnaire Grec- Français, Édition revue par L. Séchan et P. Chantraine, Paris 1950 (16th ed.); J. M. Baldwin, Dictionary of Philosophy and Psychology, Gloucester, Mass. 1960 (2nd ed.); B. Cassin, Vocabulaire Européen des Philosophie, Tours 2004; Enciclopedia filosofica, Roma 1979 (2nd ed.); L. Rocci, Vocabolario Greco-Italianο, Perugia 1993 (37th ed.); Liddell-Scott, Greek- English Lexicon, Rev. by H. S. Jones, Oxford 1968 (9th ed.); T. Sanesi, Vocabolario Italiano-Greco, Pistoia-Siena 1916 (12th ed.).  Currently the philosophical subjects are being implemented in a relational MySQL database created and managed by Dr. Ada Russo. This technology guarantees a more efficient data management, helps to control interlinguistic relation (equivalence), and supports the acquisition of Latin, Italian and English Subjects. At present the number of terms consists of 4549 (1006 Greeks, 948 French, 909 Italian, 895 English, 791 Latin), but it will increase, we in fact intend to implement also Spanish and German philosophical terminology.  In order to build the ontology, we have been using Pundit (http://thepund.it), a semantic web annotator created by Net7 (www.netseven.it) and employed in the semantic enrichment and semantic interlinking activities in the frame of the AGORA project. Conceived in the increasingly wider context of the semantic web technologies for encoding, managing and enriching digital object, Pundit allows to produce semantic annotations, whose semantics is machine-processable. Each annotation consists of a RDF triple, which is a statement made up of a subject (S), a predicate (P) and an object (O)8; according to this technology you could create, for example, a triple that states: “Sextus Empiricus (S) is the author of (P) Pyrrhoneae Hypotyposes (O)”.  Among the variety of annotations, the most useful to our purposes are those implying triples which connect:   A textual fragment to a philosophical subject, such as: “The selected text fragment (S) DealsWith (P) the Greek philosophical subject to alethes (O)” (Fig. 5) or “The selected text fragment (S) Defines (P) the Greek philosophical subject ataraxia (O)” (Fig. 6). According to its project’s goals, CNR-ILIESI team preconfigured the following properties (the inverse properties in brackets):   Defines (IsDefinedBy); IndirectlyDefines (IsIndirectlyDefinedBy); IsAnExtensionalInstanceOf (IsExtensionallyInstantiatedBy); IsAnIntensionalInstanceOf(IsIntensionallyInstantiatedBy); DealsWith (IsDealtWithBy).   A philosophical subject to another philosophical subject.While the interlinguistic equivalents are already available in TheofPhilo (see §2 and related figures), the intralinguistic relations will be implemented during the next phase of our work. RDF triples will be created considering philosophical subjects both as Subject and as Object of each triple and connected by the following properties (the inverse properties in brackets):   IsSynonymOf (HasSynonym); IsHomonymOf (HasHomonym); IsHyperonymOf (HasHyperonym); IsHyponymOf (HasHyponym); IsCo-HyponymOf (HasCo-Hyponym); IsAntonymOf (HasAntonym).    Fig. 5: Annotation Text to Subject with the RDF property DealsWith    Fig. 6: Annotation Text to Subject with the RDF property Defines    4. Conclusions TheofPhilo is a tool still in its pilot phase and work at its implementation is in progress. However, once completed the semantic annotation activity, it will enable users to access texts dealing with the subject the query was made for, according to the specific language chosen by the user. TheofPhilo will also allow scholars to deepen their research, making queries on the texts, according to both interlinguistic and intralinguistic relations. Furthermore, TheofPhilo will be considered as a large corpus of philosophical -interweave- terms (around 5000 at present) and  it will be analysed from both linguistic and historical-philosophical approach.   ",
       "article_title":"TheofPhilo. A prototype for a Thesaurus of Philosophy",
       "authors":[
          {
             "given":"Antonio",
             "family":"Lamarra",
             "affiliation":[
                {
                   "original_name":"CNR- Istituto per il Lessico Intellettuale Europeo e Storia delle Idee",
                   "normalized_name":"Istituto per il Lessico Intellettuale Europeo e la Storia delle Idee",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/011n2hw53",
                      "GRID":"grid.503056.5"
                   }
                }
             ]
          },
          {
             "given":"Michela",
             "family":"Tardella",
             "affiliation":[
                {
                   "original_name":"CNR- Istituto per il Lessico Intellettuale Europeo e Storia delle Idee",
                   "normalized_name":"Istituto per il Lessico Intellettuale Europeo e la Storia delle Idee",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/011n2hw53",
                      "GRID":"grid.503056.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Philosophy",
          "Ontologies",
          "Lexicography",
          "Semantic web",
          "Multilingual/multicultural approaches"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction How to represent texts in computer systems has always been an important topic in Digital Humanities. Tree based formalisms such as SGML 1 and XML (URL: http://www.w3.org/XML/ (checked 2013-10-26)) are useful for many purposes, but problems related to their hierarchical structures are inherent. Various solutions has been presented over the years, from questioning the existence of overlap in textual material 2 through various workaround for overlapping structures 3 (chapter 20) to the abolition of nesting formalisms, as in the example of MECS 4. In this paper I will focus on three different design principles for text representation systems, namely, linear, hierarchical and graph based. These labels represent concepts similar to Cayless' data types text as stream, text as tree, and text as graph 5. Examples of text modelling tool types focusing on each of these can be found in table 1. In the following I will analyse the relationship between the left and the right sides of the table based on experiences from the development and use of a text modelling tool called GeoModelText (URL: http://sourceforge.net/projects/geomodel/ (checked 2013-10-26)). Description of the development process can be found at the resource page for my PhD project (URL: http://www.oeide.no/dg/dp/ (checked 2013-11-01)). A use case is described in 6.   Type Text representation system  Example of modelling tool type    1  Linear Plain text   2  Hierarchical  XML encoding    3  Graph based  RDF encoding    Table 1 caption: Computer based text representation systems and modelling tools. 2. Previous work In the last 10–15 years, most practical work in text encoding have lived with the nesting structure of SGML and XML. Alternative formalisms, such as MECS, has mostly yielded to the dominant tree based structure. There has, however, been an undercurrent of experiments and theoretical research into other types of solutions. One important example originally intiated in 2002 is LMNL (URL: http://www.lmnl-markup.com/ (checked 2013-10-26)), with its range based annotation 7. A recent attempt to examine text markup under the microscope is pure transcriptional markup8. Both these examples show on-going attempts to get to the core of markup practice and were important in the development of this paper.  In my own research into semiotic differences between texts and maps I developed a system for computer assisted conceptual text modelling called GeoModelText where all three types from table 1 have the same status. GeoModelText is currently tailor suited to my work. One aim of this paper is to investigate into its usability for other types of research. Systems for visualising graph networks based on and connected to texts are easily available. What is lacking are systems for manipulating the graph structure as part of interactive text modelling, that is, including the graph structure in the internal editing system. To the degree graph based editing is included in XML editors it tends to be in an indirect sense, e.g., through manipulation of attribute values for ID–IDREF links. 3. GeoModelText GeoModelText is implemented as a Java application. The text to be analysed is imported from a TEI P5 document. The import results in a DOM structure representing the XML tree of the TEI source. Within the DOM structure the text itself is found in PCDATA segments. During analysis, these segments are used to reconstruct parts of the text as a linear structure; in this view the XML hierarchy is hidden from the user. An important function of a tree structure is inheritance, where aspects of a parent is inhereted by its children. This is used in the system to migrate responsibility to statements: the person responsible for a paragraph is also responsible for each sentence in the paragraph. One example is relationships between places claimed in the text. The claims are connected to the person responsible for the encapsulating paragraph. The places are referred to by names. In order to build up networks of related places, co-references between occurrences of names must be taken into consideration. For an introduction to co-reference see 9, and 10 for the use of co-reference networks. Thus, in order to establish this graph structure of related places, we need to read the linear text (type 1 in table 1), use inheretence (type 2), and then build up a network of related places and place names (type 3). This network connects strings which can be found at any level of the document structure. The co-reference networks change the document structure as a whole into a graph, as indicated in figure 1.   Fig. 1: The XML tree turned into a graph.  In the graph data structure, each node has a chain of links back to the place in the tree, and thus in the text, on which it is based. In this perspective, type 3 from table 1 is the dominant, but with links to the other types. In the distinction in 11 (p. 75) between hypertextual editions and editions in database format, GeoModelText is primarily a database system, but with aspects of a hypertext system. 4. The hierachy of structures  Everything we have seen in the example above is well known from XML tools, they are commonly used to establish such data structures. However, in the typical XML tool, the status of the three types are presented differently. To take one example: when opening an XML file in the Oxygen XML editor (URL: http://www.oxygenxml.com/ (checked 2013-10-27)) the tree structure is presented at the left of the Oxygen window as a number of expandable folders. This indicates that this structure is the base structure of the document. In the centre window we see the text as a sequence of tokens, with the XML elements shown in different ways or fully hidden at user discretion. Graph structures, e.g., a co-reference network, is only shown indirectly as attribute values. Thus, type 1 and 2 from table 1 are highlighted at the expense of type 3. In GeoModelText, on the other hand, separate windows give access to data in any of the three types. I do not claim that the graphical user interface of GeoModelText is better than the one in Oxygen; it is not. Rather, I want to make the point that the freedom of the object oriented programmer is not yet offered to the user of markup tools. What a user actually do with markup tools can be expressed in three different layers:  Create a tree model of the text Formalise that model in TEI/XML Reificaite the model into the editing tool  With these layers reified into the tool it is more difficult to get rid of, or even see, the straightjacket of the tree structure. While XML is useful for many purposes and is used both as input format and as storage format for GeoModelText, there is still a tendency to under-expose the non-hierarchical structure of the text. This tendency is there even if a number of technices are developed to overcome the problems created by the hierarchical nature of XML. A radical solution to these problem would be to leave XML, or, more generally, avoid hierarchical markup systems based on context free grammers all together. This was the solution of MECS. However,  this solution generated its own set of problems. In the development of LMNL, the choice was rather to use XML and the XML tools for what it is good for, and only leave the hierachical structure when one needs something else. This is also used as a design principle in GeoModelText. I use XML, both in the form of linearised files and DOM structures, whenever it is useful. But by operating in a programming environment (which is also where Piez finds himself in his work with the LMNL toolchain Luminescent (URL: https://github.com/wendellpiez/Luminescent/ (checked 2013-11-01)), although the langauge is different), I can leave the XML/DOM structure whenever I need to, e.g., to establish tools for capturing and visualising co-references.  5. The way forward Sometimes, in the frustration of the hierarchical straightjacket, it is tempting to leave XML as a whole behind. Keeping XML as a part of the information system, adding non-hierarchical modules as needed, is a better way forward for text encoding in general and for TEI specifically. XML is only a straightjacket in certain situations. Luminescent and GeoModelText are two examples of tools pointing towards a future where we can live with the limitations of XML because we are free to leave it whenever we need to. Craig outlines a complementary strategy for improvements of TEI 12. One central question remains, however. Is it possible to avoid the straightjacket as a user of a premade system, or is it something fundamental in my role as a programmer who gives me the freedom to create input mechanisms as well as visualisations based on all three types from table 1? Can we make tools which give users this freedom? When we create a tree model of a text we are still aware of the choices made, often out of convenience. When using an XML editing tool these choices has been implemented in a structure with a real material existence. The awareness of the straightjacket may be lost in the process from abstract model to editing tool.   ",
       "article_title":"Sequence, Tree and Graph at the Tip of Your Java Classes",
       "authors":[
          {
             "given":"Øyvind",
             "family":"Eide",
             "affiliation":[
                {
                   "original_name":"Universität Passau",
                   "normalized_name":"University of Passau",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05ydjnb78",
                      "GRID":"grid.11046.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1.1. Overview Automatically predicting where the beats fall in a line of English poetry is difficult. This is because the stress placed on a word will be dependent upon the meaning of the statement of which it is a part. Without approximating the meaning of a line, then, it is not possible to arrive at an accurate prediction as to where the beats will fall when the poem is read or performed. This is no trivial task. Yet without determining the beats, it is impossible to identify the metre of the line or its sound-patterning, such as alliteration and assonance, which are instrumental to the rhythm.  Because of this difficulty, the history of English poetic rhythm remains almost entirely uncharted. Twenty years ago, Preminger and Brogan's New Princeton Encyclopaedia of Poetry and Poetics lamented that 'there is at present no comprehensive and reliable history of the development of metrical practice in the West, not even competent accounts for any one language'.1 The intervening decades have not resolved this problem for English. While there have been notable attempts to construct a reliable history, none have been comprehensive. Working without the benefit of computer automation, literary historians have focused exclusively on a small number of major poetic figures.   Having done so, the current histories exhibit erroneous assertions of innovations in rhythm and rhyme, give credit to the wrong poets, and neglect formal experiments occurring on the margins. Further, most approaches tend to rely on the impressions of the critic, rather than empirical data. Consequently, the majority of statements about poetic rhythm have been made in a position of ignorance about the formal characteristics of the corpus of English poetry.  The problem becomes acute from the start of the nineteenth century as the volume of printed material explodes. Nevertheless, Martin J. Duffell's New History of English Metre (2008) considers only 32 major poets for the period, analysing 200-300 lines of poetry as representative samples of their practice.2 His analysis of poetic form therefore encompasses fewer than 9,000 lines. This is grossly inadequate, given that W. B. Yeats's Collected Poems alone surpasses this total by 2,000 lines. The number of poems considered by Duffell is a tiny proportion of the 151,299 poems in English considered important enough to be included in the Literature Online (LION) database for this period.  1.2. Methodology The problem of coverage can be overcome by harnessing the power of 21st Century achievements in speech synthesis and text-to-speech software.  My three-year project at King's College London, funded by the British  Academy, is concerned with doing just this. It uses the MARY text-to-speech  software, developed by the  DFKI (the German Research Centre for Artificial Intelligence) and the  Institute of Phonetics at Saarland University.  The intended outcome of the  project is a more reliable and comprehensive account of developments and trends in poetic rhythm, metrical forms, sound-patterning,  rhyme  schemes, and stanza types, in verse in English for the period of  1800--1970.  The MARY-tts software has  recently been used to visualize sound patterns in literary texts by  Tanya Clement et al, re-presenting aural data as high-lighted text.3 My  approach differs by extracting key data from an intermediate stage of  natural language processing that enable the software to identify some of the beats in a line of poetry. My scripts then extrapolate where the  other beats are likely to fall, if the line be read as poetry  rather than normal speech.  From these data, my tools can distinguish  between binary and ternary metres, determine the number of beats per  line in the metrical template, and identify trochaic and iambic verse.  It can spot refrains, and repeated structures, and sound-patterning such as internal rhyme and alliteration. The approach has been tested on  verse selected from a wide variety of sources, including Wordsworth,  Browning, Keats, Tennyson, Hopkins, Eliot, and the entire corpus of  Yeats's poems. There have been earlier attempts to automatically  determine formal qualities of poems using digital tools. Marc  Plamondon's tool developed for the Representative Poetry Online database at Toronto remains an impressive example.4 However, none of these approaches have employed text-to-speech software to predict phonemic properties of the text automatically. The manual  entry of syllables and phonemes for each word inevitably limits the  scope and accuracy of such tools. Other impressive attempts to uncover  the grammatical rules of particular poets' prosody have been similarly  limited by manual input of language processing.5 1.3. Conclusion My preliminary results suggest that the project  will succeed in its aim of massively enlarging and enriching our  understanding of literary history. Further, in assigning credit to the  real progenitors of formal trends, and in providing the data with which  to analyse formal developments empirically, rather than  impressionistically, this project has the potential to rewrite the  history of poetic form, and of poetic influence, altogether.   ",
       "article_title":"Mining poetic rhythm: using text-to-speech software to rewrite English literary history",
       "authors":[
          {
             "given":"Michael",
             "family":"Cade-Stewart",
             "affiliation":[
                {
                   "original_name":"Kings College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Text-mining"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction 1.1. Overview Previous research has demonstrated the utility of constructing undirected, weighted networks from the co-occurrence of people in images.1  Researchers have repurposed the technique to analyze the evolution of iconography in medieval artwork.2  Using this technique, when two saints appear together in an image, the nodes representing these saints are linked.  Moreover, each time these saints appear together the weight on the link connecting them increases.  This technique captures the evolution of these co-occurrences in revealing ways (Fig. 1).3  For example, lines of high weight capture important motifs in the artwork such as the links between Christ and Mary based on the common Madonna and Child image.  In a corpus of early images of Saint Francis, the evolution of this network captured the development of a stable core of saints consistent with the historical and artistic record.    Fig. 1: Undirected weighted network of saints created in Cytoscape   Unfortunately, these networks do not capture the hierarchies in such iconography.  For example, the presence of Saint Anthony of Padua, a Franciscan saint, depends on the presence of Francis.  Historically, Francis was the head of the Franciscan order and therefore Anthony depended on the institution bearing Francis' name.  Artistically, Anthony does not appear without Francis in imagery at this time.4  In order to capture the development of this imagery, including the hierarchical aspects of the organization of these saints, the technique must capture the direction of links representing these dependencies.   This paper demonstrates that association rule mining allows for the creation of directed, weighted networks of saints.  The social prestige of the saints can then be inferred from the structural prestige observed in the network. 1.2. Methodology The corpus includes 236 images of Saint Francis of Italian production from 1230 to circa 1320 and serves as an excellent case study for this technique.5 The catalog provides information about dating, provenance, authenticity, style and documentation.  The iconography of Francis provides a dramatic example of a transition from regionally-venerated to internationally-venerated saint.  Given this complex transition, the resulting network captures interesting shifts in the thematic content of the iconography.  The proposed technique along with previously developed network models provide powerful ways to explore iconographic trends.   Constructing the directed network involves standard techniques in data mining 6 and network analysis 7.  Cook's corpus was converted into a matrix for rule mining in RapidMiner Studio.8     The denormalized data captures the presence or absence of a saint in each painting.  The resulting matrix includes 236 rows representing the paintings and 102 columns representing saints.  After preparing the data, support and confidence metrics for each pair of saints was calculated. The confidence metrics model the strength of the relationship between saints in each direction, providing a basis for inferring rank in the relationships (Fig. 2).  For example, the thick pink link from Anthony to Francis represents a confidence of 1.0, meaning that every time Anthony appears in an image Francis certainly appears as well.  The thin blue link from Francis to Anthony, on the other hand, has a confidence of 0.122, signifying that Francis appears without Anthony in many paintings. The difference between these confidence metrics determines the weight and direction of the link between the two saints.  In this example, we would replace the two links pictured with a single link from Anthony to Francis bearing a weight of 0.878.  With this directed network, we can calculate the structural prestige of the saints in Pajek. 910   Fig. 2: Directed weighted links between Anthony of Padua (left) and Francis  For the purposes of demonstration, the calculations have been performed on a small set of data.  Table 1 shows the weight calculations based on the confidence measures derived from three images.  When these links are combined (Fig. 3), they produce a directed, weighted network well-suited to determining prestige.  Directed networks provide several straight-forward techniques for calculating structural prestige.11  The input degree is the number of links pointing to a node.  In the sample network, Francis has an input degree of 3 while Gregory IX has an input degree of 0. Although input degree is often illuminating, this measure of prestige only addresses direct connections.  The input proximity prestige, on the other hand, uses both direct and indirect links in its calculations of popularity.  Table 2 summarizes the calculations required to compute the input proximity prestige in a directed network.  Each of these metrics highlights Francis as the most prestigious saint in this simple network.    Antecedent-->Consequent  Confidence(A-->B)  Confidence(B-->A)  Link Weight    Seraph-->Francis  1.0  0.3333  0.6667    Gregory IX-->Francis  1.0  0.3333  0.6667    Narni-->Francis  1.0  0.3333  0.6667    Seraph<---->Narni  1.0  1.0  1.0     Fig. 3: Directed weighted network of saints     Saint  Influence Domain  Proportional Distance  Average Distance  Proximity Prestige    Francis  3  1.00  1.00  1.00    Seraph  1  0.33  1.00  0.33    Bartholomew of Narni  1  0.33  1.00  0.33    Gregory IX  0  0.00  Undefined  0.00   2. Results With the additional interpretive power of directed networks, researchers can better understand changes in the popularity of saints.  Somewhat surprisingly, the painters and patrons of the earliest surviving images of Francis (1230-1249) did not seek to juxtapose Francis with Christ, Mary or other well-known saints.  Instead, the early promoters of Franciscan iconography chose to portray Francis by himself or with other prominent figures in Francis' hagiography such as the Seraph.  As the cult of Francis grew, however, the prestige of Christ and Mary jumped from nil in the 1240s to 0.97 and 0.79 respectively in the 1320s.  Moreover, by examining the correlation of prestige measures of different saints, researchers can identify trends in the artistic tastes of the patrons driving the demand for these images.  For example, the prestige of the Seraph is negatively correlated with the prestige of Clare (-0.82), Benedict (-0.5), and Dominic (-0.6) meaning that when the Seraph is popular in this imagery the monastic and mendicant leaders are not.  This negative correlation echoes Cook's observations regarding the presentation of Francis in non-Franciscan houses, particularly that Francis is often presented without stigmata in this context.12 Apparently, non-Franciscan houses and even the Clares did not wish to emphasize the unique aspects of Francis' hagiography in their commissions.  The prestige figures also register the effects of specific events such as the canonization of saints.  For example, Louis of Toulouse was canonized on April 7th, 1317; his prestige in the decade 1310 to 1319 rose from nil to 0.33.  During this same decade, the prestige of Anthony of Padua, another male Franciscan saint, plummeted to 0.0 from 0.45.  As a popular new saint, Louis displaced Anthony for about a decade as the preferred male Franciscan to balance compositions with Francis.  Finally, the prestige metrics indicate a growing popularity of female saints after 1300.  Excluding Christ, 4 of the 5 most popular saints in these images are female: Mary (0.79), Clare (0.49), Mary Magdalen (0.45) and Catherine of Alexandria (0.42).  Given the results of this study, we aim to expand the research to include a wider range of images.  In particular, the inclusion of works of Dominican provenance will be helpful for comparing prestige metrics in both mendicant traditions.  Beyond this, we believe that applying the technique more broadly may shed light on some of the larger trends and issues in medieval art history.  For example, researchers have noted that new saints and new imagery related to saints appeared in medieval art in response to the Black Death.13  With this technique, we can gauge the relative prestige of saints before and after the Black Death to determine if the perception of these saints changed in the eyes of painters and their patrons.               ",
       "article_title":"Mining the Cloud of Witness: Inferring the Prestige of Saints from Medieval Paintings",
       "authors":[
          {
             "given":"Thomas",
             "family":"Lombardi",
             "affiliation":[
                {
                   "original_name":"Washington & Jefferson College",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "Association Rules",
          "Data Mining",
          "Medieval Art",
          "Network",
          "Prestige"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Problem of Time and Space: The Difficulties in Visualising Spatiotemporal Change in Historical Data Proposal for Digital Humanities 2014 Conference 1 November 2013  Tomás Ó Murchú, MPhil Digital Humanities, Trinity College Dublin, Ireland Professor Séamus Lawless, Trinity College Dublin, Ireland  Visualisations take advantage of the fact that the human eye has the ability to identify patterns and structures in images that computers are yet to match. Visualisations do this by exploiting features of the human cognitive processing system1. While much of our communication is done through words, we are connected to our environment primarily through vision. This has resulted in our visual perception having evolved to actively seek meaningful patterns in what we see2. Using a digital visualisation system in combination with flexible human cognitive capabilities, such as pattern finding, is far more powerful than an unaided human cognitive process3. Visualising historical data in relation to the information’s geographic and temporal attributes can help uncover hidden links and relationships within the data. However traditional spatiotemporal methods for visualising change are often insufficient for providing a spatial and temporal framework within which historical data can be explored. Historians (especially since they normally do not possess the required skillset themselves) have had to live with, or at best modify, existing tools from other disciplines. Because of this they have been channelled down spatiotemporal visualisation routes that are frequently a poor fit for their research.  This paper takes murder information that has been extracted from the 1641 Depositions (testimonies documenting the experiences of witnesses of the 1641 Irish rebellion)4 as a case study in creating a spatiotemporal visualisation using historical data. An existing online example that uses the data in an interface with Google Maps is taken as a starting point (downsurvey.tcd.ie/1641-depositions.php).    Fig. 1: Murder information from the 1641 Depositions  Using the same data from the 1641 Depositions, a spatiotemporal visualisation is created to illustrate the difficulties in using historical information for this process.   Fig. 2: Spatiotemporal Visualisation for the 1641 Depositions  This paper will investigate the problems associated with traditional spatiotemporal visualisations of historical data. It will examine our own comprehension of time and space and how understanding their vagueness, ambiguities and uncertainties are important when it comes to visualising and modelling changes in their components.  Most historical data has a spatiotemporal element to it. This could involve the movement of people over a temporal period within a specific area or the changing area of a political entity over time.  As these changes are normally recorded in written texts, historical spatiotemporal data can often exhibit a high degree of uncertainty. Texts are descriptive and by their very nature are vague and open to various interpretations. This qualitative nature of historical documents means that creating a spatiotemporal visualisation involves overcoming obstacles where descriptions of spatial areas or temporal periods are often vague or uncertain. The problem of ambiguity and uncertainty in data is an issue that visualisations do not deal with particularly well5.  When analysing spatial and temporal data historians need to be conscious of the uncertainties present within the data. Some of these ambiguities may not be immediately apparent and will require detailed analysis to identify. In historical data there are several reasons why uncertainty occurs. Historians often use data that was intended for a different end use than the analysis that they are trying to accomplish. Trying to convert the data into something usable for a spatiotemporal visualisation inevitably leads to a degree of data compromise. The entities, times and spatial areas mentioned in texts were often meant purely as descriptions related to a specific event so are frequently only vaguely defined. Sometimes the historical sources are transcriptions or translations of lost documents and uncertainties may have occurred due to the transcription or translation process. Similarly, as sources may be transcriptions of oral depositions, cultural, educational and linguistic differences between the transcriber and deponent may cause misinterpretations. How spatiotemporal data is modelled for spatiotemporal visualisations is an important factor when dealing with historical information. Data models are the conceptual core of an information system. Models should be designed to deal with uncertainties and to create meaningful visualisations of changes over time. Modelling the data includes defining data object types, relationships, operations and rules to maintain database integrity6. The data model needs to support the processes that the system will be required to carry out. When modelling spatiotemporal data, a further consideration is the fact that territorial structures and units change over time. The issue of situating (and thus visualising) the data within the correct spatial area at the correct moment in time needs to be effectively managed by the data model. For historians trying to visualise spatiotemporal information, a data model needs to be able cope with uncertainty and changes over time. Additionally trying to create links between vague concepts in a visualisation is fraught with uncertainty. The data structure supporting the visualisation can be too rigid to show anything other than a few select relationships. One method to try and reduce and manage uncertainty in modelling historical data is to use what is known as fuzzy logic and fuzzy set theory. The aim of fuzzy logic is to provide a means to cope with ambiguous entities without losing a record of the ambiguity7. This is achieved by using assessment rules that the researcher pre-defines and makes transparent so that they can be easily understood and evaluated by other researchers. Fuzzy logic and fuzzy set theory can be used for linguistic variables thus making it suitable for the modelling of textual sources. If used properly it can handle vague and uncertain linguistic labels such as ‘slightly’, ‘close to’ or ‘very’ (as in ‘is very old’). These linguistic variables are present everywhere in written and spoken communication but computers have difficulty in recognizing their correct application. Modelling qualitative spatiotemporal information provides many challenges. The computational nature of traditional visualisation systems mean that information needs to be categorised into set groups. Historians often resist such demands as they feel that some vital characteristics or attribute inherent in the language describing it will be lost when they do not fit precisely a particular category8. The context in which an entity is described can be as important as the attributes of the entity itself. A death in a source text may be described as a murder by one witness, an accident by another or an act of self-defence by another. Trying to categorise it or by assigning it neutral label such as ‘unnatural death’ robs the entity of all meaning in a historical context. Extracting data from historical textual sources by computational means is likely to miss out on much key contextual data. Domain experts understand that there is meaning present in descriptions that can defy conventional numerical and computational approaches. Overcoming uncertainties and vagueness in the Deposition texts proved challenging for existing spatiotemporal visualisation tools. In a text such as the 1641 Depositions, the multitude of ways dates are represented causes huge difficulties in linking particular events to the date they occurred. Phrases such as ‘at the beginning of lent of that year’ and ‘two months hence’ abound.  Representing all these different time periods on the same visualisation is very difficult. There may also be overlaps between the periods of time with one text identifying a particular event on a particular day while another text in the canon identifying the exact same event but it occurring somewhere in the period of a month. Similarly, identifying places in the Depositions is also problematic with many places referred to in uncertain terms. General terms such as ‘near’, ‘close to’ or ‘in the region of’ are used extensively in the texts. Anglicizations of Gaelic place names in the Depositions are inconsistent and often do not correlate to modern spellings. Finally, future work on how Linked Data and the Semantic Web may have the ability to help historians to overcome spatiotemporal visualisation limitations is considered. The paper concludes that new tools and data models are required to effectively visualise spatiotemporal historical information.  ",
       "article_title":"The Problem of Time and Space: The Difficulties in Visualising Spatiotemporal Change in Historical Data",
       "authors":[
          {
             "given":"Tomás",
             "family":"Ó Murchú",
             "affiliation":[
                {
                   "original_name":"Trinity College Dublin",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          },
          {
             "given":"Séamus",
             "family":"Lawless",
             "affiliation":[
                {
                   "original_name":"Trinity College Dublin",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction As widespread interest in the Open Data movement has grown in recent years  some museums from all over the world have started to share and provide their digital content with Internet users. These new practices not only involve users’ interest and traffic increase on museums websites but also magnify the institutional transparency which provokes a different conception of the museum authority. Then, we can find different ways of making available to explore and download high resolution photographs or exhibition catalogues as well as more complex procedures as open APIs development in order to allow users to create their own application based on museums data.  This paper seeks to establish the main issues related to museum discourses within the Open Data activities framework. Two case studies were chosen which construct their institutional and public digital identity on data openness. The Smithsonian Cooper-Hewitt Collection and the Rijksmuseum are well recognized by the professional museum community concerned with digital practices. Both museums had obtained important awards at the Museums and the Web 2013 conference as well as  being cited on several blog posts by some of the most relevant museum bloggers. As L. Manovich argues: ‘The use of software re-configures most basic social and cultural practices and makes us rethink the concepts and theories we developed to describe them’ (2013, 33). This assertion implies a re-configuration of the museum in epistemological terms provoked by digital practices. Taking into account the Foucaultian notion of discourse, it is necessary to analyse both the museum digital content and the institution and professional framework that arrange and define the discourse itself. The progressive Open Data implementation by institutions - not only by museums- could be interpreted as a sign of democratization even if this openness does not represent a new role for  museum institutions. However, as G. Lovink (2012, 49) states ‘visibility and transparency are no longer signs of democratic openness but rather of administrative availability.’ Then we could understand Open Data mechanisms in museums as just a public service which maintains the museum authority status according to digital age knowledge dissemination dynamics. Thus the question is whether open data is becoming merely a user service or something else?  In terms of open content access, users play an essential role receiving and reusing museum contents. Through open APIs they can elaborate computer mobile or tablet based applications. Eventually this option is oriented to amateurs, owners of specific informatics knowledge. The rise of amateurism and participatory culture argues for the author status dissolution. This is reflected in the reinterpretation of museum institutional authority. The museum is likely to maintain its status while granting more privileges to users. 2. Case Studies In order to study further this question, two case studies were chosen whose relevant use of Open Data is creating new museum models on the Internet, although the data typology used by them is diverse as well as their public strategy. On one side, the New York Smithsonian Cooper-Hewitt Collection1 has presented a website in beta mode that reflects the physical state of the museum which is being refurbished at the same time. On the other hand, Amsterdam’s new Rijksmuseum website2 has been launched at the same time as the reopening of the museum.  One of the most significant aspects is that both museums have published their database APIs on Github3 allowing users to access the museum collections data or metadata and develop API based applications. The Rijksmuseum digital identity is defined by the appeal of high resolution images of the artworks - which are currently being offered in its own website app Rijksstudio- while the Cooper-Hewitt Collection strength lies in the provisional and documentary nature of its data - which is composed by the objects raw data and metadata- eventually improved by the museum staff as well as wikipedians. The museum become ‘human and fallible’, just like the public.  Moreover, the Smithsonian Cooper-Hewitt Collection website is making an exemplary use of open data including other features as biographical pages enhanced Wikipedia integration, public and open geographic identifiers or information concordances with other institutions.  The applications created by developers in both cases are a reflection of the museums digital strategy and conception. Clearly the Cooper-Hewitt collection apps -or visualizations- tend to be more experimental in contrast to the Rijksmuseum ones which also are similar to other museum applications -developed by the museums themselves- and are based on a definitive database. Likewise, Rijksmuseum apps play with the high quality of the artworks pictures rather than textual data on the  Cooper-Hewitt Collection apps, where developers draw  attention to data visualization or automated Tumblrs.   Some of the Cooper-Hewitt collection applications best features are its digital work in progress  such as Curatorial Poetry Tumbrl4 , a stream of decontextualized descriptive texts pulled from museum collection meta-data, elaborated by the museum staff, or the visualization by Ruben Abad about the collection colour history5, have been documented on the Cooper-Hewitt Collection Blog. The process of developing apps acquires value by itself and this is reflected on the blog that documents the museum staff working activities. On the other hand, some of the Rijksmuseum apps, such as Faces of the Rijksmuseum6, which uses facial recognition, or Riiksify7, that mix a music playlist with the collection, were developed during the TNW Kings of Code Hack Battle 2012 held in Amsterdam, confirming once again a definition of the amateur user profile who is interested on APIs usage. Referring to P. Gorgels from the Rijksmuseum, they identify several target groups: the culture snacker, art enthusiasts and professionals, then to which group does the amateur/hacker user belong? Until the launch of the Live API on Github, the Rijsmuseum had shared on the museum website some of the apps developed with its OAI API8 although it is worth noting that the Cooper-Hewitt Collection is going further, publishing on its blog interviews or text written by the developer itself about the elaboration of API based apps. Definitely this action could be interpreted as a sign of openness recognizing the value of users’ contributions. The use of Github, a collaborative revised hosting service for software development projects, reveals the museums interest on hackers/developers behaviour. Also the Github adoption by Rijsmuseum could be a symbol of openness and adaptation and make us to think whether it is the museum or the public who decide to introduce standards. 3. Conclusion  It is clearly evident that these Open Data museum practices can represent  different levels and a wide conception of openness. Although turning back to the introductory points, is the museum institution really breaking their walls thanks to this new trend or they are just building a new ones? In fact, these two cases reveal the existence of new boundaries, the terms and conditions webpage of the Rijsmuseum API, for example, shows how the museum control the use of its data. Therefore, should  users ever obtain the same status as the museum in terms of authority? By extension, are the applications developed by users being valued differently regarding the museums ones? Clearly museums do, in the digital age they still are institutions that decide what is worthy and deserves their recognition. Maybe two case studies hardly can explain the substantial changes that Open Data is bringing about or whether the general public is understanding them as a sign of openness or even if museums are using it as a marketing strategy. However those substantial changes and their new discourses about openness are modelling the museum idea in the digital age.  ",
       "article_title":"Open content production in museums. A discourse and critical analysis of the museum in the digital age",
       "authors":[
          {
             "given":"María Isabel",
             "family":"Hidalgo Urbaneja",
             "affiliation":[
                {
                   "original_name":"Universidad de Málaga",
                   "normalized_name":"University of Malaga",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/036b2ww28",
                      "GRID":"grid.10215.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Museums",
          "GLAM: Galleries",
          "Archives",
          "media studies",
          "Libraries",
          "open data"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In printed texts usually a lot of words are separated by a hyphen at line breaks. Such a hyphenation is made if the last word is too long for the current line particularly with regards to a justified text alignment. Whereas in many cases an additional hyphen (soft hyphen) will be appended to the first part of a word, some words already contain a hyphen (hard hyphen) that can be used for the line break. During different steps of automated text processing hyphenation can be hindering if the correct spelling of a word, whether with or without hyphen, is unknown. Just think of applications in which the text shall be annotated automatically or shall receive a different typesetting. In such cases it is desirable to use a self-acting or at least a semiautomatic approach in contrast to make manual decisions for every word's correct spelling, which can be notably time-consuming for long texts. There is only a sparse amount of comments in the literature how to handle the problem described above, especially in French. Some publications propose to make the decisions manually 12. The documentation of the Oxford Concordance Program 3, which is a software from the 1980s, states that it \"has a facility to request that hyphenated words at the ends of lines should be reconstituted\" but without giving details of the realization of this feature. One trivial procedure, removing all end-of-line hyphens, is used in a paper about tokenization 4. This paper also mentions the use of a dictionary as possibility to reduce the error rate, which is an essential part of our approach discussed later. Simply joining the separated parts of a word by leaving the hyphen out may solve the problem for most instances in many languages, e.g. English or German. In French however a more complex approach is necessary because the hyphen is frequently used in positions other than the end of a line. Particularly this includes the building of compounds with prefixes, nouns and pronouns as well as numbers that made their way into the written language. Whereas hyphenation in French usually follows well-defined rules nowadays, these rules changed through time and had not been applied consistently. Thus a reliable rule-based approach for disambiguating end-of-line hyphenated words is unlikely. To solve the challenge we have developed a dictionary-based technique for reversing the hyphenation for a given French text. The approach consists of three steps. First, an internal attribution is computed which determines the number of occurrences for every word of the text under consideration. Thereby only occurrences not separated by a line break are considered. Thus the text itself will become a reference for the correct spelling of a given separated word by comparing the number of occurrences of both possible spellings. The second step is a query in an external dictionary. Again both spellings of the word, whether with or without hyphen, are searched in the given dictionary. The third step merges the information of the two previous steps in order to provide a guess for the correct spelling. Both of the previous steps may have led to either no indication at all, or to an indication for exactly one spelling or to an indication rendering both spellings probable. Thereby 16 cases are possible. Our approach assumes that the spelling with hyphen is correct if the internal attribution returns only the entry for the spelling with hyphen even if the external dictionary says differently. This keeps the consistent spelling of the author or the age of the text. The hyphen is also chosen if the dictionary only provides this spelling and simultaneously the internal attribution either has no entry or has entries for both notations. In all other but two cases the spelling without hyphen is assumed correct. The two exceptions are the cases where the internal attribution contains both spellings and the external dictionary simultaneously provides either no or both entries. In these cases the highest number of occurrences in the internal attribution is decisive. If they are equal the spelling without hyphen is chosen. As previously mentioned the heuristic “always use the spelling without hyphen” is the obvious way to handle hyphenation in most languages. We tested our approach against this simple heuristic with respect to the number of faulty decisions. For comparison we used a book by Guillaume Raynal in four different editions which were published in 1770, 1774, 1780 and 1820 5 and a dictionary of the ABU : la Bibliothèque Universelle 6 with more than 250,000 entries of common words for the second step. The slightest relative difference between both techniques occurred in the edition of 1820 which contains 1,339 individual hyphenations of 52,372 words and 7,198 lines. Our approach resulted in 30 wrong guesses (2.240%) instead of 45 (3.368%) made by the heuristic which is a decline by the factor of 1.5. The biggest difference appeared in the edition of 1780 with 1,063 individual hyphenations, 44,078 words and 6,290 lines. While the simple heuristic resulted in 45 faulty decisions (3.814%), our approach nearly cut the number of errors in half to 24 (2.034%). Concerning the editions of 1770 and 1774 the outcome was 6 errors (0.819%) instead of 10 (1.364%), and 14 (1.317%) instead of 23 (2.164%), which is about the same level. The effectivity of our approach becomes apparent if the four editions are considered as one text. Our approach benefits from many words with multiple occurrences in the concatenated text consisting of 155,160 words and 21,551 lines. Only 39 (1.107%) of 3,522 individual hyphenations are reversed incorrectly. In contrast the simple heuristic makes 98 faulty decisions (2.783%). In summary our approach dominates the simple heuristic regarding the number of wrong spellings without being free of errors itself. This is important if a researcher depends on an automated disambiguation of the end-of-line hyphenated words due to the size of the text or missing expertise for deciding the correct spelling. Furthermore our approach can be helpful to considerably reduce the manual effort scholars have for checking correctness. For the tested text all but one¹ error of our approach occurred for words without any information in the internal attribution and the external dictionary. Thus a researcher can focus on these not reliable cases instead of checking every word separated by a line break. This will reduce the effort to 298 instead of 733 individual hyphenations in the edition of 1770 (40.655%), 222 instead of 1,063 in 1774 (20.884%), 197 instead of 1,180 in 1780 (16,695%), 62 instead of 1,339 in 1820 (4.630%) and 270 instead of 3,522 in the concatenated text (7.666%). While nearly all errors of our approach occurred for words without information in both the internal attribution and the external dictionary, skipping the second step would result only in slightly increased error rates. In contrast using the external dictionary without an internal attribution would lead to apparently more errors. Both issues may be due to the historic word forms and proper names found in the text. However a large amount of entries in the internal attribution which requires that the text under consideration is relatively large seems to be the important factor for lowering the number of errors and reducing the semiautomatic effort respectively. Keeping this in mind, the approach can easily be extended by filling the internal attribution with larger corpora so that reversing the hyphenation of a relatively small text will benefit from the advantages described above. The same can be done in step two by using multiple external dictionaries.   Acknowledgments This research was funded by the German Federal Ministry of Education and Research (BMBF) [grant number 01UG1247] as part of the project “Semi-automatische Differenzanalyse von komplexen Textvarianten” under the direction of Prof. Dr. Thomas Bremer, Prof. Dr. Paul Molitor, Dr. Jörg Ritter and Prof. Dr. Hans-Joachim Solms. Also we would like to acknowledge and thank our project collaborator Susanne Schütz.   Notes ¹ This exception is a contradictory case for the word  “par-tout” in the edition of 1780 which was guessed falsely with hyphen as it was found 17 times with this spelling in the text but only without hyphen in the external dictionary. The situation is a different one if the four variants of the text are considered in all as the spelling without hyphen was used more often in the other editions than 1780.  ",
       "article_title":"On automatically disambiguating end-of-line hyphenated words in French texts",
       "authors":[
          {
             "given":"Marcus",
             "family":"Pöckelmann",
             "affiliation":[
                {
                   "original_name":"Institute of Computer Science, Martin-Luther-University Halle-Wittenberg, Germany",
                   "normalized_name":"Martin Luther University Halle-Wittenberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05gqaka33",
                      "GRID":"grid.9018.0"
                   }
                }
             ]
          },
          {
             "given":"Julia",
             "family":"Ritter",
             "affiliation":[
                {
                   "original_name":"Institute of Romance Studies, Martin-Luther-University Halle-Wittenberg, Germany",
                   "normalized_name":"Martin Luther University Halle-Wittenberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05gqaka33",
                      "GRID":"grid.9018.0"
                   }
                }
             ]
          },
          {
             "given":"André",
             "family":"Gießler",
             "affiliation":[
                {
                   "original_name":"Institute of Computer Science, Martin-Luther-University Halle-Wittenberg, Germany",
                   "normalized_name":"Martin Luther University Halle-Wittenberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05gqaka33",
                      "GRID":"grid.9018.0"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Information retrieval"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Motivation After Schmidt’s article 1 on modelling and representing various versions of text with so called Variant Graphs was published in 2009, web-based tools were developed that utilize and adopt the presented model to facilitate the work with digital editions of text in the browser. CollateX 2 is one of these tools. It computes a static, horizontally aligned, directed acyclic graph with vertices showing the various text fragments and edges labeled with edition identifiers connecting subsequent text fragments. The tool Stemmaweb 3 extends the CollateX graph to allow for user-driven annotation and modification of the graph structure (e.g., merging and splitting of vertices). Despite the attached interaction capabilities, it seems that there is little value put on designing the graphs. The purpose of this paper is to raise awareness for improving the readability of Text Variant Graphs. We propose a list of design rules for styling the graph and its vertices and edges to facilitate a rapid comprehension of the underlying alignment structure by the user. 2. Design Rules for Text Variant Graphs When defining rules for the layout of Text Variant Graphs, we refer to the visualization of CollateX, as its layout is also used in Stemmaweb, and it can be seen as an improvement in comparison to the generated graph of the tool NMerge 4 – provided by Schmidt –, where edges carry all types of information.  When we take a look at a resultant CollateX graph (see Figure 1), it is hard to find out how often a text fragment occurs over all given editions. Thus, it is hard to compare, e.g. the numbers of synonyms. The only chance is to count the edition identifiers at the labels of the incoming edges of the desired vertices. But we can easily put this information on the vertex layouts, which leads us to Rule 1: Vary vertex label sizes! As Wattenberg proposes for the ”Word Tree” 5, we suggest weighted vertices also for Text Variant Graphs. The usage of font size as a metaphor to reflect the number of occurrences of individual text fragments helps to immediately differentiate between frequent and infrequent branches of the graph.   Fig. 1: Fourth Bible verse in 7 different editions: Text Variant Graph computed with CollateX    Fig. 2: Fourth Bible verse in 7 different editions: Our visualization  It seems to be obvious to draw the edges of a directed acyclic graph in the shape of arrows. But for what reason, when we know that we read a text with a dedicated writing direction? Then, most probably, the user is supposed to read the graph in the same direction, and it is counterintuitive to move the eyes backwards when reading (in Figure 1, we find an edge from ”saw” (right-top) to ”the light” (bottom-left) – we call this a backward edge). In graph theory, the common style for a directed acyclic graph is a so called layered graph drawing 6 with all edges pointing in the same direction. Thus, we define Rule 2: Abolish backward edges! When doing so, we can reduce the cognitive load of the visualization by drawing undirected edges instead of arrows. The labeling of edges with edition identifiers as it is done in CollateX leads to two problems. Firstly, the additional text labels interfere with the vertices' texts. Thus, the reader has to visually separate vertex labels (text fragments) from edge labels (identifiers). Secondly, if lots of editions pass an edge or long edition identifiers are used, the corresponding edge labels become very large. As a consequence, adjacent vertices drift apart and the reader quickly loses the context of a text fragment. Therefore, follows our Rule 3: Do not label edges! As an alternative, we suggest drawing an edge for each edition in a different color. However, as the human ability to distinguish colors is limited, it only works well or a small number (<10) of editions. But, with varying stroke styles for edges (e.g., line, dashed line, dotted line) we are again able to increase this number. In any case, a legend is required to map the given styles (or in the CollateX case, the identifiers) to its corresponding edition name.  When analyzing and comparing text editions to each other, the user is often interested in those editions, that deviate from the ”general case”. Within the Stemmaweb tool 7, edges, that are passed by most editions, are labeled with \"majority”, thus, the labels are bundled. When following Rule 3, we receive multiple lines instead of multiple labels, hence, Rule 4: Bundle major edges! We highlight both resultant edge types – bundled and unbundled edges – in a different way. Unbundled edges receive saturated colors in visually attracting hues, whereas bundled edges are colored in a plain gray, but drawn with slightly thicker strokes. Thus, the deviations from the general case can be detected easily. When following Rules 3 and 4, we are able to reduce the number of edges to be drawn – and therefore, the cognitive load of our approach – to a passable minimum.  Last but not least, the main problem we identified when reading the horizontal aligned Text Variant Graphs was the required horizontal scrolling. Especially, when the source texts are long, the user quickly loses the context and it is hard to keep track of how individual editions disseminate in the graph. Moreover, a lot of space is wasted since the height of the graph is rather small compared to the height of the screen. The outcome of a survey within the TAdER Project 8 to avoid horizontal scrolling when reading texts in the browser underpins our hypothesis that the user is accustomed to scroll vertically. Thus, here comes our final Rule 5: Insert line breaks! It may sound tricky to cut the graph into pieces, and thereby, keeping it easily readable. But, why shouldn’t we adopt the behavior of a text flowing in a book (with line breaks) for Text Variant Graphs? When following Rule 3, we receive different colored edges (or edge bundles) at the end of each line, so that all paths are visually seizable at the beginning of the next line. This approach supports the user in following individual editions even for large graphs, and the user also receives more context on the screen for a specific position in the graph. Figures 1 and 2 juxtapose the resultant Text Variant Graphs for the fourth Bible verse with CollateX and our visualization that implements the listed design rules above. 3. Following the Rules: 7 English Translations of the Bible We are working with seven English translations of the Bible in our project 9, which turned out to be a very good use-case for the presented visualization approach, not only because the Bible is a very influential and well-known text. Another reason is, that these translations are all derivatives of the same Hebrew and Greek original, often trying hard to preserve the exact wording, and refer to an existing and well structured text, divided into canonical books and verses.  Figure 3 shows a screenshot of the first five Bible verses. After tokenization, normalization and alignment procedures, we layout the resultant directed acyclic graph by following the rules proposed in the previous section. For the seven editions, we chose the following colors of the 12-color palette for categorial usage suggested by Ware 10 to facilitate maximal visual differentiation by the user: red, blue, green, yellow, orange, brown, and purple. To support answering various research questions, the user is able to modify the visualization. Firstly, when the user hovers a vertex all individual edges of the corresponding editions are drawn in the dedicated colors. This mean of interaction helps to highlight the paths containing the dedicated token and to clarify those editions forming majority edges. Secondly, unimportant editions can be removed from the graph. Thirdly, the user is able to select one of the editions as a main branch, so that the corresponding vertices are drawn on the same horizontal level – variations to the other editions can be considered easily. During the development phase, the humanists of our project steadily evaluated our design, so that the result remains intuitive even for the inexperienced, maybe sceptical user. We strongly recommend such an iterative process when developing visualizations for humanistic applications as it turned out to be very successful. In comparison to a plain graph layout, the presented design for the Text Variant Graph and the project page reminds the user, that it is a book to be read, not just some string of letters – which was a major concern of the humanists. Our presented approach is still applicable for examples where whole blocks of text have different orders among the various editions, but matching text blocks may strongly drift apart. In the future, we direct our attention on developing algorithms that visually align such structures more properly.   Fig. 3: Screenshot of the Bible use case. In Genesis 1:4, all paths containing the token ”divided” are highlighted.   ",
       "article_title":"5 Design Rules for Visualizing Text Variant Graphs",
       "authors":[
          {
             "given":"Stefan",
             "family":"Jänicke",
             "affiliation":[
                {
                   "original_name":"Image and Signal Processing Group, Institute for Computer Science, Leipzig University, Germany",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          },
          {
             "given":"Annette",
             "family":"Geßner",
             "affiliation":[
                {
                   "original_name":"Göttingen Centre for Digital Humanities, University of Göttingen, Germany",
                   "normalized_name":"University of Göttingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01y9bpm73",
                      "GRID":"grid.7450.6"
                   }
                }
             ]
          },
          {
             "given":"Marco",
             "family":"Büchler",
             "affiliation":[
                {
                   "original_name":"Göttingen Centre for Digital Humanities, University of Göttingen, Germany",
                   "normalized_name":"University of Göttingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01y9bpm73",
                      "GRID":"grid.7450.6"
                   }
                }
             ]
          },
          {
             "given":"Gerik",
             "family":"Scheuermann",
             "affiliation":[
                {
                   "original_name":"Image and Signal Processing Group, Institute for Computer Science, Leipzig University, Germany",
                   "normalized_name":"Leipzig University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03s7gtk40",
                      "GRID":"grid.9647.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "natural language processing",
          "visualisation",
          "relationships",
          "user studies / user needs",
          "interdisciplinary collaboration",
          "text analysis",
          "networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  I. Introduction Following the attacks of September 11, 2001, interviews were conducted with first responders to the World Trade Towers. Each of those 503 interviews describes one witness account of the event – skyscrapers collapsing, choking dust, chaotic communications, fatal desperation. Although focused on individual narratives, each interview corresponds to a larger sequence of events: two massive violations of the right to life that took place in New York City. Given this larger frame, a documentation source dense relative to the event's spatiotemporality, and the evocative, idiosyncratic nature of testimony, we developed a method to computationally elicit and visualize the narratives running across the corpus. This paper describes the work's narratological theory, the visualization developed to facilitate the identification and exploration of transversal narratives, and our analysis of the World Trade Center (WTC) Task Force Interviews. II. Fabula and witness testimony As testimony, for 1, is centered on the body, and the body is contingent with the witness, our analyses focused on the fabular, raw events of the narrative as a foundation for crossdocument coreference. Narrative testimonies like these are, to 2, speech acts in the public sphere that serve to solidify a collective memory. Witnessing, consequently, is historiography dependent upon comprehensible dramatic unity as put forward in 3. Narratology indicates that the four primary elements of fabula are events, actors, time, and location 4. This corresponds to the events model proposed by 5 for human rights violations reporting. Accordingly, we extracted information from the WTC corpus corresponding to these elements, enalbling the decontextualization of information from existing narration, the identification of conflicting, factually questionable accounts, and the visualization of transversal narratives. III. Applied narratology The semi-automated information extraction pipeline described in  necessitated a method for spatio-temporal interpolation based on keyframing. Events appearing in many narratives, such as the collision of Flight 175 into Tower 2, become key moments for emploting fabula. Given a sufficient number of these events, and of geocodable locations, absolute referents for intermediary material can be interpolated with an accuracy sufficient for correlation. Lookups to gazetteers provided absolute keyframe data.  For extracted Storygrams (computed fabula) of person-place-time without absolute reference, a dramaturgical sequence numbering system was used to emplot all events in the order in which they occurred. These sequence numbers were used to provide interpolated time for events between keytimes. Quotations were used for the named and unnamed entities. Gazetteers were developed to provide absolute values for key times and locations; the global timing list contained 11 events, of which 8 were used. The location list comprised 2,151 names indicating 1,399 unique locations. Many locations lacked a geocodable referent. In those cases, we interpolated the data to suggest a likely location. Recognition of Storygram elements enabled cross-document coreference of implicit entities. Correlation of personal and global data, when visualized, revealed narratives running transversely throughout the corpus. IV. Narrative visualization Many tools can visualize temporal aspects of events, notably timeviz.net 7, and Google charts. Spatial visualization is dominated by mapping tools like Google Maps and Earth, Open Street Maps, ArcGIS, and MapBox. Tools like Google Earth visually animate changes over time. Animations are cognitively demanding, requiring viewers to track changes frame-by-frame. Irregular intervals make this task more problematic. Multiple interactive 2D synchronous views for time and location are an alternative to animations; operations like zooming, panning, and filtering in one view automatically updates the remaining views. Color and shape are other ways to represent event information. Jern et al. use color coding to link temporal data with spatial data 8. Color meaning is culturally dependent, so it is not reliably intuitive. In addition, color palettes have to be constrained to allow for visual disambiguation, thereby forcing the system to artificially bin events to a number of discrete color values. Event type diversity should dictate design, not the number of recognizable color options. Shapes present their own challenges 9. Events can also be shown using a 3D space like Kachina Cube 10.The cube base on the X-Y plane contains a map and the Z-axis represents time. Though 'details-on-demand' techniques can be applied to the visualization, this approach suffers from generic 3D problems like glyph cluttering and scalability. Also, it is often hard in 3D UIs to compare data points in two dimensions (X-Y and Z). V. Storygraph and narrative To visualize the narratives and address the issues above, we developed a 2D integrated spatiotemporal visualization called Storygraph 11. Storygraph, an extension of parallel coordinates 12, has two parallel vertical axes and an orthogonal horizontal axis. Our novel application adapts this information-rich visualization technique for the presentation of explicit and implicit narrative. The vertical axes represent latitude and longitude and the orthogonal axis represents time. A map location, such as a city or street corner, is represented as a line segment linking the parallel axes. Events occurring at a location are represented by a point on the location line as shown in Figure 1. This technique shows, in 2D, the scope of a corpus, the relative frequency of documentation at all locations in the corpus, and patterns like co-occurrence in time, co-occurrence at location, or co-occurrence in time and location – one of the unique properties of Storygraph. Storygraph also facilitates Storylines: linear connections emphasizing the movement of people through the spatiotemporal context of the corpus. Storylines are polyline segments chronologically connecting entities at location. In our implementation, we use dotted lines for storylines to mark the uncertain space between observations. VI. Visualizations of transversal narratives of 9/11 We applied our visualization to the WTC corpus comprising 17,000 question and answer pairs aimed to elicit first-person narratives of the event. To feed the data into the Storygraph, named and unnamed entities were extracted. This semi-automated process involved much manual verification due to the ambiguity in the natural language. Gray lines in Storygraph in Figure 1 indicate locations. Each red point shows one Storygram. Black vertical bars indicate global events: the collapse of Tower 1 and Tower 2, the period when people were seen leaping from the towers. The horizontal funnel layout of the points, with the mouth to the left axis, indicates that documentation shows people converging from a wider geographic area to the narrow area around Ground Zero. In essence, Figure 1 shows first responders converging on the scene of two terrorist attacks.   Fig. 1: Storygraph showing all the events extracted from the narratives of the firefighters.  Figure 2 uses the same data to show Storylines of four emergency personnel as they move throughout the spatio-temporal corpus domain. Four features in Figure 2 are of particular importance. First, the geographic domain is highly constrained and covers an area from Staten Island to Central Park. Second, with just 10-20 extracted fabula, a sense of the path of these individuals through the event emerges. The chaotic jumble of points in the period from 8:39 AM to 9:30 AM corresponds to the event's most chaotic moments. Third, the lines of Firefighters Loutsky and Smith stabilize at two locations as they move from emergent crisis to emergency care. And finally, there is the blue storyline of Chief Ganci, which ends at 10 : 12 at 40.71, −74.01, approximately 16 minutes prior to the collapse of WTC Tower 1. Chief Ganci, the highest ranking uniformed fire officer in FDNY, died in that collapse. What Storygraph enables is the identification and organization of fragments from others' statements to reveal the story of what happened to Ganci that day.   Fig. 2: Storylines of three firefighters and one EMT.  VII. Next steps: violations taxonomy and the South Africa truth and reconciliation corpus  Extensions of this work will begin with analysis of South Africas Truth and Reconciliation Proceedings. Over a two-year period, the TRC collected 7,000 amnesty applications and 22,000 witness statements describing 34 years of abuses occurring nationwide. This material will expand our techniques to larger geographic contexts, more diverse time frames, elements of a second-language, and a much wider range of rights violations. As each mass violation event has a particular violations ecology 13, one challenge for violations researchers is to identify the emblematic subset of violations. Currently, we are working on methods for the automatic correlation of a violation description to a taxonomy of violations 14. This method will help classify the narrative events in the context of particular violations within each narrative and further our work in entity resolution. It will also expand our methods into what Salway and Herman refer to as top-down, hypothesis-driven, and bottom-up, data-driven, methods 15. Additional visualization elements currently under development include UI tweaks to automatically generate a colorspace, and analytic affordances allowing for the drilling down from Storygrams to the original source document fragments.  ",
       "article_title":"Visualizing Computational, Transversal Narratives from the World Trade Towers",
       "authors":[
          {
             "given":"Ben",
             "family":"Miller",
             "affiliation":[
                {
                   "original_name":"Departments of English and Communication, Georgia State University",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          },
          {
             "given":"Ayush",
             "family":"Shrestha",
             "affiliation":[
                {
                   "original_name":"Department of Computer Science, Georgia State University",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          },
          {
             "given":"Jennifer",
             "family":"Olive",
             "affiliation":[
                {
                   "original_name":"Department of English, Georgia State University",
                   "normalized_name":"Georgia State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qt6ba18",
                      "GRID":"grid.256304.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The Kinomatics Project is a multidisciplinary study of the industrial geometry of culture focussing in particular (but not exclusively) on the cinema. The project results from both the recent digitisation of the cinema industries as well as contemporary research practices in the discipline1. The researchers have collated a unique dataset of global cinema showtimes which alone, and in combination with additional datasets, challenge many of the unspoken assumptions and ordinary practices of conventional film studies research. The Kinomatics Project proceeds from the emergent understanding that the cinema is not an isolated set of practices. Cinema comprises institutional, social, and commercial networks that are interdependent which in turn influence and shape our approach to cinema research. This view of cinema is relatively recent. To date, the study of cinema has been predominantly concerned with issues of film content (the text) and with little regard for the events that occur around the actual consumption of film2. By shifting the focus from film content to cinema as a cultural practice we open the way for new questions and approaches to research that effectively draws together a number of discipline areas. This also distinguishes our work from others with a more formally or textually focussed approach to the computational turn in Cinema Studies such as Lev Manovich’s3pioneering studies. In particular the advent of big data has meant that a wider range of digital data types, formats, and sources can be used in innovative ways by all disciplines including the humanities and social sciences. The availability of big cultural data enables the unprecedented mapping of the industrial geometry of motion pictures at an international scale. This paper uses three case studies to demonstrate how the digitisation of cinema can be understood as a set of located and network practices.   A Big Cultural Dataset to Track Film Flow and Diffusion Across the Globe Over a 12 month period, we have tracked the global flow of film screenings by gathering specific cinema location information for over 47,000 films throughout 48 countries internationally. For each of these 48 countries we have data for every film screening event (down to date and time for each screen) for all venues (a global total of 30,000) resulting in a database of over 120 million records. Data was obtained from a third party source and is directly downloaded to the project database.  Their data comes directly from cinema venues mostly through automated electronic means and also email and phone calls. Until now, databases dealing with cinema consumption and exhibition have been limited to case studies that are either national scope or defined by special interests. Examples include, historical database initiatives such as the substantial Dutch database “Cinema in Context”4 and the “Cinema and Audiences in Australia Project” (CAARP) database5 as well as the GIS based work of Robert C. Allen6 at the University of North Carolina and the “Australian Cinemas Map” database in Australia7. This project extends the scope of such databases by taking it to an international scale, to create the first global study of the film industry.   Method and Analytical Approach In this paper we will demonstrate how we can further our understanding of cinema as a set of network practices both economically and geographically through the collection of digital datasets and utilising new technologies for analysis. This will be addressed in three linked projects, all of which focus on the global reach of cinematic data and practices. Whilst there are some differences in method across the projects, each of the inter-related projects feature the use of visualisation to explore, analyse, and communicate the information and findings. Visualisations are used throughout the process as it is an effective way of dealing with big data, making the proliferation of data readily accessible. Each of the related projects are briefly summarised as follows:   1) Tracking the global movement of films The success of the film The Hobbit: An Unexpected Journey has been taken as a case study to track the movement of film at an international scale. The Hobbit was chosen due to the challenges it poses from the large amount of viewings, the geographical reach of its screenings, and also the complex temporal and spatial itineraries involved in ‘staggered’ film releasing strategies. The use of GIS and temporally sensitive visualisations have enabled us to track the spatial and temporal relationships of The Hobbit, highlighting the complexities of international cinema enterprises and the subtleties of contemporary releasing strategies.   2) Interoperating data – linking remittances and the movement of film at the cinema  Using India as a case study we explore the relationship between remittance flows and the movement of film around the globe. India provides an ideal case in point to study this relationship given that a relatively large proportion of Indians work abroad sending remittances back home and that India has its own unique highly successful global film industry emanating from Bollywood.  By merging data on the flows of remittances from countries returning funds to India with data on the screening of Indian film we have used visualisation and economic modelling techniques to identify the pattern of movement of Indian film around the world, with particular focus on how Indian film flows into the various remittance-sending countries. This analysis is based on bi-lateral remittance flow data sourced from the World Bank. In order to investigate the importance of remittances as a factor helping to explain the flow of Indian film around the world we scale remittances relative to the size of the sending countries overall population. As a result, we are able to test whether a greater presence of Indian nationals within a given country supports a higher level of cultural diffusion.   3) Spatial and temporal persistence in distribution patterns during a period of industrial transition The distribution industry has been explored through modelling the spatial and temporal attributes associated with the diffusion of films. Although the digitisation of the film itself has opened up the possibilities of new distribution markets and strategies, we have found that there is still a strong relationship to pre-digital distribution territories. Through a number of visualisation techniques including Network Analysis and Circular Statistics, our analysis has also found that there is great variation in distribution patterns dependent on variables such as genre, production company, and country of origin.   Conclusion Rather than measuring the comparative cultural value of film texts as favoured by traditional cinema studies, the Kinomatics Project traces the flows and pace of industrial change in the cinema and measures the intensity of its dynamics. This paper describes the intersection between a revised qualitative cinema historiography (focused around an industrially informed and consumption attentive view of the cinema) and the use of innovative information systems inspired by new research approaches found in big data analytics such as data mining and digital visualisations.  ",
       "article_title":"Kinomatics: big cultural data and the study of cinema",
       "authors":[
          {
             "given":"Deb",
             "family":"Verhoeven",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Bronwyn",
             "family":"Coate",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          },
          {
             "given":"Colin",
             "family":"Arrowsmith",
             "affiliation":[
                {
                   "original_name":"RMIT University",
                   "normalized_name":"RMIT University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/04ttjf776",
                      "GRID":"grid.1017.7"
                   }
                }
             ]
          },
          {
             "given":"Alwyn",
             "family":"Davidson",
             "affiliation":[
                {
                   "original_name":"Deakin University",
                   "normalized_name":"Deakin University",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02czsnj07",
                      "GRID":"grid.1021.2"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  In this contribution we present a pedagogical approach with the intention to introduce digital humanities to undergraduate students. Our approach may be regarded from three angles: first, as the construction of tailored toolkits of digital methods for students; second, as a contribution to the analysis of material properties of cultural productions, and; third, as a mid-term strategy to orient students toward design-based learning techniques. The way in which our pedagogical practice connects the three perspectives is as follows. We consider the realm of cultural productions populated by music albums, films, comic books, TV series, video games, digital art, architecture, industrial design, etc. Now, with the emergence of various kinds of tools and scripts for analyzing media data (text, images, audio, etc.), we select and assemble several of them in a tailored toolkit for studying cultural productions. Then we use the toolkit as a teaching methodology in the classroom. In the mid/long-term, our intention is to move students from the use of tools (as it happens in undergraduate courses) to the creation and design of tools, services and processes (as it happens in postgraduate courses). In the present work, we discuss some experiences with undergraduate students in information and communication sciences at the University of Paris 13. 2. Analyzing cultural productions Before introducing digital tools and methods in the classroom, we discuss about cultural productions: what they are and why/how to study them . Within the context of the undergraduate curricula, our course explores the possibilities of digital and connected technologies. Our course tries to complement other types of communicational analysis such as discourse analysis, semiotic analysis, and quantitative methods. From this perspective, our methods put special attention on the analysis of material properties of cultural productions.  In that respect, the analysis of cultural productions deals with tasks such as gathering, documenting, representing, and exploring valuable data about forms, materials, contexts, techniques, themes, and producers of these productions. Cultural objects, or cultural productions, represent the tangible or perceivable result of cultural labor. In our course, we tackle the analysis of material properties of cultural productions from three dimensions: texts, images, and networks. For practical goals, we first ask students to select a production of their choice: a CD album, a film, a comic book, a series of comic covers, a video game, a music video clip, etc. Then two main types of data are collected. On the one hand, media-based data (texts, images, videos, audios, etc.) and, on the other hand, data about data (metadata) such as years, places, actors, roles, etc. Our toolkit of digital methods is tailored to suit the analysis of each dimension. Once data has been collected, the next step is to perform information processing techniques in order to generate ‘analytical maps’, which are the formal outcome of the analysis of material properties of cultural productions. These maps are helpful in the processes of identification of relationships, observation, comparison, evaluation, formulation of hypothesis, verification of intuitions, elaboration of conclusions, and other social sciences methods. By learning to manipulate tools and studying material features of cultural productions, students generate their analytical maps and use them as a support to reflect on second-order questions: Why the production was made in such a way? Who created it, how, and by which means? Which actors contributed to it and which roles they played? In which manners those actors influenced the final product? How does the production reflect on societal, scientific, temporal, artistic, and geographic aspects of its time?  3. Our approach   3.1. First step: gather data  As we mentioned above, we first ask students to select a cultural production. The selection is free and subjective, it is an individual decision in order to create a comfortable ambient for research. Students are naturally attracted to an artist or film or CD and this might stimulate to dig deeper in the gathering of data. From another perspective, the choice also reflects ideological presumptions, intuitions and trends in a generation. For media data, the sources vary according to the choice of the cultural production. In the case of a CD album, for example, texts can be found in the lyrics of songs; for a film it could be the script or even a SRT subtitle file. For comics, it could be the dialog balloons and other paratexts. For images the case is not very different. Images are considered as any graphical information that pertains to the cultural production. CD albums have covers, booklets, etc. Films have frames, posters, etc. Comics have covers, pages, frames, etc.  For data about data (metadata), students use extensively search engines, Wikipedia, specialized online databases (AllMusic , IMDB , etc.) and Google services (Ngram Viewer , Zeitgeist , etc.) to gather data associated with the production: persons and roles (producers, directors, artists, designers, engineers, etc.); years, places, company, label, duration, technical details, etc. In any case, students take their own decisions about what kind and how large the corpus of analysis should be. This is the reason why we let students to select freely the cultural production, if they like it they can go deeper and construct bigger corpora.  3.2. Second step: analytical map of digital texts  The first type of analytical maps we generate have text as media data input. We mainly rely on four techniques: 1) generating a word cloud; 2) generating a list of word frequencies; 3) generating a word trend graph and identifying the word in context; 4) generating an exploratory visualization of text: a phrase network or an experimental representation of text. These techniques are coupled with technological tools. We use easy-to-use web-based software. Word clouds are generated via Wordle . A list of word frequencies, a graph of trends, and words in context can be obtained with voyeurtools.org. Finally, exploratory representations of text can be achieved with ManyEyes  or other Voyeur tools .  3.3. Third step: analytical map of digital images  The second type of analytical maps we generate have images as media data input. We consider five techniques: 1) extracting the color scheme and listing color values; 2) evidencing shapes; 3) distributing colors according to the RGB color model; 4) generating orthogonal views of video sequences. As it happens with text, image techniques correspond to specific tools. For technique no. 1 we use the add-on tool Rainbow 1.5.1 available for Firefox . For technique no. 2 we use the online editor Pixlr , specially the filter ‘detect contours’ combined with an adjustment of brightness and contrast. For technique no. 3 we use the Firefox add-on Color Inspector 3D . For technique no. 4 we use the tool slitscanner.js  (only available for HTML5 videos).  3.4. Fourth step: analytical map of digital metadata  The third type of analytical maps we generate have metadata as input. Networks are about rendering evident the relationships between data (for instance, persons involved at some point or playing a particular role in the production) of the cultural object. We work on two techniques: 1) cleaning and preparing data in a spreadsheet; 2) generating and navigating network diagrams. The first technique is accomplished with Google Spreadsheets, and the second one with ManyEyes.  3.5. Fifth step: analysis of analytical maps to elaborate conclusions  The last part of our approach involves all the analytical maps together. The main goal is to use social sciences methodologies (observation, comparison, etc.) to elaborate conclusions about the second-order questions: Why the production was made in such a way? How does the production reflect on societal, scientific, temporal, artistic, and geographic aspects of its time? This last part is most of the time conducted by students themselves or in teams. They often recreate some of the latter steps or they start searching for more resources. In the end, they are free to design a display support for the analytical maps and the conclusions. I, as teacher, do not make suggestions at this stage because students now use more naturally the web as a service.  4. Conclusions and perspectives  We have used our toolkit as teaching strategy for two years and we have documentation on more than 100 student projects . We have collected informal data about student experiences: technical issues, methodology and even cultural trends (for example, most analyzed groups and films). Among our ideas for evaluation and improvement, we foresee: to design higher level courses based on the learning outcomes of this course; to make available a reference manual of DH techniques for students; and, to collaborate closer with other colleagues to complement other types of analysis. Our toolkit of digital methods is inspired by techniques that come from the domain of text analysis, visual semiotics, and network analysis. Within a digital context, we believe they foster a more scientific web culture as the web is regarded as a platform and service for research. In that manner, the role of the teacher is more to assist students in every step of the analysis and to help  identify valuable insights that could only be appreciated through digital methods.  ",
       "article_title":"Introducing digital humanities through the analysis of cultural productions",
       "authors":[
          {
             "given":"Everardo",
             "family":"Reyes-Garcia",
             "affiliation":[
                {
                   "original_name":"University of Paris 13",
                   "normalized_name":"Paris 13 University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/0199hds37",
                      "GRID":"grid.11318.3a"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "pedagogy",
          "cultural analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Perceptions of falling enrollments and demands for closer alignment with the labour market have placed the humanities under pressure in North American higher education.[1] These issues have been particularly pressing at mid-sized institutions such as Wilfrid Laurier University in Ontario, Canada.[2] Faculty at Laurier responded to this challenge by developing a digital humanities program with its first course offerings expected in 2014-2015. This paper discusses the initial design of that program and its relationship to three major questions in digital humanities pedagogy. First, should digital humanities programs be structured around a common core of learning objectives, or instead differentiate?[3] Second, what is the relationship between digital humanities curricula and demand in the workforce – should digital humanities programs be designed to pursue indifferent academic knowledge or attempt to engage more actively with vocational preparation? Finally, should the teaching of digital humanities focus on specific skill development, or instead cultivate “methodologies” or critical perspectives on technology and its application?[4] In addressing these questions, the paper adds the experience of one institution to the ongoing conversation among emerging programs.[5] Although it has been asked whether only research intensive universities might have the expertise to field digital humanities programs, at Laurier we were mindful of the possibilities opened by a primarily pedagogical approach.[6] To begin with, while the secondary literature’s discussion of digital humanities research overshadows teaching, pedagogical arguments can be crucial to attract administrative support.[7] This is especially the case in faculties that are sensitive to undergraduate enrollments and their volatility. Digital humanities programs can present an attractive option for students while demonstrating the continuing relevance of the humanities to technological change.[8]  The Laurier curriculum is a program option that majors or minors can include in their course of study. Structured around a cluster of required courses, the curriculum allows departments throughout the Faculty of Arts to add courses as they are developed. Students use their elective courses to design a specific pathway, including learning to program.[9] The required courses expose students to historical analysis and computer science. The purpose of the curriculum, broadly conceived, is to improve students’ “digital literacy” or their ability to find and analyze digital information, and to use digital tools in active and creative ways.[10] Recent research has noted that, “There is considerable evidence to support the view that many students do not explore information in any deep or reflective manner.”[11] Other authors have preferred to emphasize student “multiliteracy,” arguing that literacy in the digital age is a broad concept, and reflects fluency with and access to a broad range of representative forms, such as visual or audio media.[12] A narrow concentration on traditional textual literacy, it is argued, misses the scope of literacy in a connected, technologically saturated world.[13] Though some commentators also worry that young people are being transformed into passive recipients of digital media, others argue that technology opens their creative potential in blogs and other formats.[14] All agree, however, with the necessity of transforming students from “consumers” of digital content into “creators.”[15] The literature has also argued that digital humanities can develop students’ critical skills as they engage with complex digital information on the web and elsewhere.[16]  Mindful of the growing significance of digital literacy, the Laurier program prompts students to realize the challenge of using and gathering “deep data,” rather than relying on data returned from basic Google searches. Throughout the curriculum students use methods from history to interpret textual information, and weigh and contextualize evidence. This approach connects a qualitative layer to the quantitative and analytic skills learned from computer science.[17] For example, the program’s foundation course introduces students to the possibilities of big data. Using existing queries and code they investigate familiar data sources, such as Twitter and Google. They then use knowledge from the humanities to contextualize and shape that data. Students are asked to consider the limitations of digital information and how to make data meaningful: what socially significant questions might they ask of it? During the final project students communicate their findings using digital media. The course attempts to demonstrate to students that the familiar digital universe they inhabit can reveal surprising discoveries with the right tools.  At Laurier three factors shaped the development of the program: concerns over costs, an increasing emphasis on differentiation within the Ontario university system, and the challenge of engaging faculty who had a pre-existing knowledge of the subject. These pressures demanded the program leverage existing institutional strengths.[18] For example, without funds to support new hires, the program was by necessity an interdisciplinary effort among the faculty already working in the digital humanities. Consequently, their knowledge directly affected what was initially possible within the program.[19] As we developed the program we realized that it was possible for these perceived drawbacks, such as lack of faculty expertise concentrated in a research cluster, to become strengths. In response, our program became not only interdisciplinary, but a scaffold for faculty to build their expertise and advance their knowledge through teaching. These factors shaped the curriculum so as to differentiate it within the Ontario system at a time when such diversity is becoming a compelling trend in higher education. As universities attempt to communicate their distinctiveness to applicants, digital humanities programs can benefit by their alignment with the institution’s academic identities.[20] In the case of Laurier, this tilted the curriculum towards business, one of the university’s strongest areas. Our experience suggests explicit differentiation is not only the preferable strategy, but also perhaps a necessity given the resource constraints and the dynamics of higher education in North America.  The development of the Laurier program was also related to specific data about the job-market. Whether humanities programs should explicitly adopt a vocational orientation has been a subject of pyretic debate.[21] A curriculum that trains students primarily to investigate academic problems in the humanities might be especially suitable for research universities. At Laurier, however, we shaped the curriculum in consultation with the Career Centre to advance our students in post-graduate employment more directly. Like it or not, many students and applicants are preoccupied with the job prospects associated with their major.[22] Among employers, we have learned, there is concern that graduates in arts will be intimidated or flummoxed by even basic tasks using digital tools. These misgivings might be unfounded, but the Laurier program explicitly cultivates digital literacy to equip students for knowledge employment in the future.  For example, the program builds on Laurier’s strength in business administration to provide an entry point into the burgeoning field of analytics. This focus is especially important since the university is located in a region with large technology and insurance sectors.[23] The curriculum exposes students to big data problems beginning in the foundation course, while prompting them to think about the social meaning and application of this information by drawing on knowledge from the humanities. A stream may be added to educate undergraduates specifically in big data and analytics. An emphasis on employable experience is also reflected in the experiential and co-op learning integrated into the curriculum.[24] Students, to give one example, can receive course credit for work at the Laurier Centre for Military Strategic and Disarmament Studies. They will undertake projects such as digitizing and making the Centre’s archival holdings publically searchable. These kinds of work opportunities, which combine training in the humanities with digital work, are not only pedagogically desirable, but meet our students’ demand for co-op experience in the humanities. Among the hardest questions to answer is whether digital humanities courses should teach a defined set of skills over more broadly conceived methodologies. In balance is the preference among employers that new hires should already have a minimal level of job-related training, yet programs focused on imparting specific skills risk narrowness.[25] Instead we have embraced the concept of digital literacy: students should have broad facility with digital work, and be confident and able to self-learn or advance their training in specific areas.[26] For example, the exposure during the foundation course to code does not teach them coding, but rather demonstrates how code works and its limitations. Students choosing to specialize can take advanced programming electives. However, all students should leave the program with at least enough understanding to customize off-the-shelf tools.  Can the digital humanities draw attention to the vitality of the humanities? Part of doing so may be to demonstrate that the humanities have much to offer the digital economy. Moving forward at Laurier, we intend to conduct a more thorough investigation into the relationship between the digital humanities and the contemporary workplace. By proceeding with this research we accept that the humanities can be more explicitly oriented to post-graduate employment and the challenges of “knowledge work.” Though we live in a time of doubt about the humanities, such strategies may instead reveal that this is a moment of renewed vigor.  ",
       "article_title":"Advocating for a Digital Humanities Curriculum: Design and Implementation",
       "authors":[
          {
             "given":"David",
             "family":"Smith",
             "affiliation":[
                {
                   "original_name":"Wilfrid Laurier",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In 1936, the notable English bibliographer A. W. Pollard admitted in his Preface to Frank Isaac’s English Printers’ Types of the Sixteenth Century that “[he] had a very poor eye for distinguishing types and a very poor head for remembering them.”1 Pollard is hardly alone among experts in the history of printing in this shortcoming.  Even among scholars with decades of experience in scrutinizing features of the printed book, the ability to distinguish and identify typefaces is a notorious challenge.  The literature about early type designs and designers (known as punchcutters) is partial and contradictory; the variations in typefaces are subtle and, at times, inconclusive; and the ability to make differentiations has been considered less a matter of regimented principle than of elusive skill.  As Harry Carter suggested, “it is evident that in considering the face of a fount of type we are in a world of art, . . . not a mechanical proceeding or anything susceptible of scientific treatment.\"2 However, it is precisely the consideration of “founts of type” that is currently engaging a majority of the Early Modern OCR Project (eMOP) team. eMOP, a 2-year Mellon Foundation-funded grant project underway at the Initiative for Digital Humanities, Media, and Culture (IDHMC) at Texas A&M University, aims to OCR the documents that comprise the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) collections. As a project that involves collecting and aggregating huge amounts of data, OCR’ing 45 million page images on a high-performance computing cluster, and the development of several software tools and services, eMOP is technology-laden. But at its heart eMOP is a Humanities project, conceived  by Humanists, driven by the needs of Humanities scholars, and supported throughout by book history and an understanding of the development of print type in the 15th-18th Centuries. For eMOP’s book historian, Dr. Todd Samuelson, one of the difficulties in conceptualizing the scope of eMOP has centered in a potential conflict between DH methodology (as encompassed by “big data”) and the traditional means of approaching type identification: as Carter noted, it is an art steeped in years of hard-won practice rather than a science with predictable and reproducible models. While DH is focused on humanities questions and methodologies, it does employ scientific principles as well, especially when dealing with a very large set of documents, and conflict can arise by trying to synthesize a skill set based on minutiae with an extremely large data set. By contrast, even when big data projects incorporate crowdsourcing and the oversight of human experts, they require the ability to find readily transferrable commonalities, rather than to establish proficiency in a small number of experts. In the course of the eMOP project, we have found that the development or adoption of specific software tools has helped to ameliorate this conflict and incorporate type history scholarship into the training of OCR engines. One of the ideas driving eMOP work is that, by training OCR engines to recognize specific early modern fonts, we can increase the accuracy of those engines when used to OCR documents printed in those fonts. To accomplish this, the eMOP team has spent most of the last year investigating font history, creating a database of early modern printers and the fonts they used, and developing and testing tools and techniques to train Tesseract (an open-source OCR engine) to recognize these fonts. The ability to distinguish between different, but sometimes closely related, fonts, and to train Tesseract to recognize these distinctions has been a central focus. For example, the general classification of different families of typefaces has been attempted by book historians, including Adrian Weiss, who categorized unknown English typefaces of a certain period as either “S-face” or “Y-face”.3 So, though the source of the typeface may not be ascertainable, certain characteristics can be defined which allow scholars (and potentially OCR engines) to identify and group the typefaces more accurately.  As has already been noted, identifying examples of S- and Y-face characters and distinguishing between them, especially when both can be present in one document, is a difficult enough task for an expert. Trying to find all instances of the lower-case letter ‘w’ in a document, as an example, and then deciding which exemplars match some specified “ideal” is difficult and time consuming. Fortunately, eMOP has software tools that can drastically simplify this task, and even allow non-experts to do some of the work. Those tools were originally developed to create training for Tesseract to recognize early modern typefaces, but can also be applied to support research into the typefaces themselves.  To create specific font training for the Tesseract OCR engine, a team of undergraduate student workers, lead by IDHMC graduate student Katayoun Torabi, first process the available page images using Aletheia Desktop. (Aletheia was developed by the Pattern Recognition and Image Analysis (PRImA) Research Laboratory at the University of Salford. Apostolos Antonacopoulos, IMPACT Work Package leader for PRImA, University of Salford, has made Aletheia and other tools available at http://www.primaresearch.org/tools.php.) Aletheia Desktop includes several semi-automated tools that identify and define layout regions, lines, words, and individual characters (glyphs) within documents. Aletheia reads the text in the page image (using Tesseract) and assigns a Unicode value for each letter, number, and punctuation mark.    Fig. 1: Aletheia Desktop with identified glyphs and some of their associated Unicode values.  As output, Aletheia creates an XML file that contains a set of XY coordinates, along with the associated Unicode value, for each identified glyph. The data contained in this XML file is then ingested or imported into a tool created by IDHMC graduate student Bryan Tarpley called Franken+. Franken+ uses a MySQL database to associate each glyph image with its corresponding Unicode character. The user can then select any glyph from a drop down menu to see every instance of that character in a window (Fig. 2). With every instance of a particular glyph (for example all the ‘a’s) from a document available in one window, the user can quickly identify mislabeled glyphs and choose the best exemplar (or exemplars) for each glyph in that font set (Fig. 2). Once the user has isolated the best instance(s) of each character, Franken+ uses a standard text document to produce a set of synthetic TIFF images and XML files, producing a “Franken-text” with only these ideal characters. This Franken-text matches the characteristics of Tesseract’s expected training file and so can be used to train Tesseract to recognize the typeface being processed.   Fig. 2: Some images of the Frank+ user interface.  eMOP’s book history team immediately realized that the capabilities of Aletheia and Franken+ would tremendously benefit their research into the S-face vs Y-face font question. The ability of Franken+ to display all instances of a given letter from a set of page images in one window dramatically simplifies the task of identifying all examples of any letter in a set of pages. And, being able to examine all these examples alongside each other makes comparing similarities or differences much easier and faster (Fig. 3). After a quick installation of Franken+ and less than an hour of training, the book history team was able to commence work on their research question in earnest. Since Franken+ was introduced at the 2013 Doc Eng Conference,4 the eMOP team has been contacted by several international scholars interested in learning more about Franken+ for use in their research on typefaces.    Fig. 3: An image from Franken+ of a set of exemplars of the “a” glyph from one document.  The study of early modern fonts is a road less traveled in the landscape of Humanities research. Based as it is on minutiae and requiring incredible attention to detail, this work traditionally has been left to a handful of individual scholars. However, the development of Franken+ for eMOP, when used in conjunction with Aletheia, promises to open up this field of study to scholars who may have been interested in it, but found the challenges too daunting. This paper will describe aspects of the eMOP work being done in the field of early modern type research, and will introduce Franken+ as a valuable new tool in this research. The creation of tools like Franken+ have the potential to increase attention and alter research methodologies for this field.   ",
       "article_title":"Book History and Software Tools: Examining Typefaces for OCR Training in eMOP",
       "authors":[
          {
             "given":"Matthew",
             "family":"Christy",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Todd",
             "family":"Samuelson",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Katayoun",
             "family":"Torabi",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Bryan",
             "family":"Tarpley",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Grumbach",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Early Modern OCR Project (eMOP), currently underway at the Initiative for Digital Humanities, Media, and Culture (IDHMC) at Texas A&M University, is a Mellon Foundation-funded endeavor tasked with improving, or creating, OCR (optical character recognition) for the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) collections. The basic premise of eMOP is to 1) use book history to identify the fonts represented in the collections and the printers that used them; 2) train open source OCR engines on those fonts; and 3) OCR documents using an engine trained on the font specific to each documents. In addition, as a Mellon Fountation-funded project eMOP is tasked with using open-source solutions and producing open-source tools, workflows, and processes that are reproducible and which can be implemented by other scholars in their own digitization projects. One of eMOP’s end products will be an open-source workflow of our entire process using Taverna. As eMOP enters its second year, intensive work on developing and testing training for the Tesseract OCR engine has demonstrated a failing in the three-fold basic premise. Many of the page images which we are trying to OCR are of such poor quality that no amount of training will produce OCR results that meet the standards we have set for the grant outcome.1 These images are already binarized, low-quality, low-resolution, digitized images of microfilm, converted from photographs—4 decades and 3 media generations removed from the originals.Typical problems include noisiness, bleedthrough, skewing, and warping, but there are many more. There already exist many algorithms that can fix most of the problems extant in our collection of page images.23 Applied during a pre-processing stage, these algorithms have the potential to improve page image quality to the point that they can yield excellent OCR results. But with approximately 45 million pages in eMOP’s data set, determining which pages need which kind of pre-processing proved problematic at best.    Fig. 1: A sample of part of a page image from the eMOP collection showing skew, noise, bleedthrough, over-inking, and an image.  To this end, the eMOP management team, along with our collaborators, Loretta Auvil and Boris Capitanu at SEASR (Software Environment for the Advancement of Scholarly Research, at the University of Illinois, Urbana-Champaign), and Dr. Ricardo Gutierrez-Osuna and graduate student Anshul Gupta of Texas A&M University, decided to focus our proposed post-processing triage workflow on the problems that exist in our page image inputs. Originally stated, our triage process would examine OCR results and decide whether the documents would be routed to different tools being built for eMOP to perform automatic word correction, crowd-sourced line segmentation correction, by-hand font identification, or automated re-OCRing with different font training. However, the presence of so many low quality page images in our input required a more robust system for handling the output. What we needed was a triage process that would allow us to programmatically diagnose our input documents based on the output of our OCR system. The open-source Tesseract OCR engine is capable of producing both plain text files and files in an XML-like format called hOCR. hOCR files contain wrappers around each found word, line, paragraph, and region, and these wrappers contain bounding box coordinates for each entity (Fig. 2). A close examination of the text and hOCR results for nearly 600 poor quality page images revealed certain patterns, or ‘cues’, which could be used, singly or in combination, to uniquely predict individual problems that exist in the original page images.   Fig. 2: Bounding boxes for lines (red) and words (blue) drawn on a page images based on hOCR output.  For example, documents printed in a blackletter or gothic font, but OCR’d with Tesseract trained for a roman font produce a text file with a character frequency distribution different from that expected of English language documents. Basically, if Tesseract is trained with a roman font, characters printed in a blackletter font look predominantly like m, n, u, and l. Similarly, documents containing a lot of noise (e.g. numerous spots and blotches on the page) typically produce “words” found in areas of the page outside of the main text area, have word bounding boxes of widely varying heights, and have line bounding boxes that overlap. Page images that exhibit heavy skewing (the text lines are tilted at an angle from the horizontal) also pose problems for Tesseract, as it will often begin reading one line and then at some point jump to the line above or below (depending on the direction of the skew) to finish reading the \"line.\" In these cases the hOCR again contains overlapping line bounding boxes, but also has word bounding boxes that don’t have contiguous coordinates, i.e. it is finding words out of the reading order as they appear on the page to a human reader. These are just a few examples that demonstrate the problems we’ve encountered and the cues we’ve discovered to identify them, which we will identify in this paper. Cues like these and others have provided us with the mechanism we were looking for to identify page image problems based on OCR output. In order to take full advantage of this information however, we are also developing a full post-processing workflow. Beginning with OCR results, the output of this workflow will be either 95%, or better, corrected text or a per-page indicator describing what kind of pre-processing should be performed before each page is re-OCR’d. We are also working with our collaborators on developing a mechanism to assess the quality of our OCR output. We have combined different analysis techniques developed by collaborators at SEASR and Texas A&M University, to examine text data (examining character unigram frequency distributions and word lengths), page data, (determining the main text area of the page and looking for outliers), and hOCR bounding boxes (calculating box heights and widths). Applying these mechanisms to the results of each page will yield a score that constitutes a prediction of how the document would compare to a ground-truth transcription. Test results show a strong correlation between these predicted scores and actual scores produced on documents that do have ground-truth available. Page results receiving a high enough score can then be sent for further text analysis, including dictionary look-ups, to correct as much of the OCR output as possible. Those pages that receive scores below the threshold undergo an iterative process of looking for different cues in order to identify the likely reason the OCR process failed for each page.    Fig. 3: Proposed eMOP post-processing workflow.   Much work has already been done with regard to OCR post-processing, but it has concentrated on questions of identifying and correcting bad OCR.45 In this paper we will report on the development of an OCR post-processing workflow that can evaluate and identify a broad range of defects common to page images of early modern printed documents. The result of this workflow can then be funneled into a pre-processing and re-OCR’ing process later. We plan, by grant-end, to release an open-source workflow and code that can be used by other groups or individuals engaging in large-scale OCR projects. Given the inherent problems that these documents pose for OCR engines, we view this kind of analysis as a vital step forward in the comprehensive understanding and digitization of large collections of early modern printed documents.  ",
       "article_title":"Diagnosing Page Image Problems with Post-OCR Triage for eMOP",
       "authors":[
          {
             "given":"Matthew",
             "family":"Christy",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Loretta",
             "family":"Auvil",
             "affiliation":[
                {
                   "original_name":"University of Illinois",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Ricardo",
             "family":"Gutierrez-Osuna",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Boris",
             "family":"Capitanu",
             "affiliation":[
                {
                   "original_name":"University of Illinois",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Anshul",
             "family":"Gupta",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Grumbach",
             "affiliation":[
                {
                   "original_name":"Texas A&M University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "ocr"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper focuses on the methodology applied to the development of components in the domain of collaborative philology in the Memorata Poetis Project. This initiative, led by the University of Venice, coordinates eight units sharing the same cyber-infrastructure and is co-funded by the Italian Ministry of Instruction, University and Research (PRIN 2010/11). The project aims to study the multilingual intertextuality between epigraphic texts and literary epigrams, the transmission of themes, motives, etc. between different communicative situations (epigraphic versus literary) and different civilisations (Greek, Latin and Italian). As a control group, we analyse a corpus of epigraphic and literary texts in Arabic which do not belong to the same tradition as the others. The study of intertextuality affects both the reconstruction of the text (constitutio textus), by providing variants from the indirect tradition, and its interpretation (interpretatio), by widening the contexts in which the text has been reused. Methodology By following a top-down approach the article will discuss the following three aspects of the general design of the components developed by the Institute for Computational Linguistics of the National Research Council (ILC-CNR) in Pisa, which will be integrated into the shared infrastructure managed by the Venetian working unit of the project.  Firstly, we will introduce the ongoing modelling of the philological domain from a formal point of view. Secondly, we will discuss engineering methods for the analysis of the required components. Finally, we will describe the application of the aforementioned methodology to the specific part of the project developed in Pisa.  Computational philology has so far focused on the formalisation of only some aspects of the philological domain, such as stemmatics, derived from the Lachmannian methodology 1, but it is necessary to take into account the formalisation of other aspects essential to understanding the history of the tradition 2 as well as the relation that a text has with its text bearing object (TBO), reusing non-textual annotation tools 3. Thus, any proposed formal models should reflect a representative range of philological methods and practices.  Whereas stochastic theories and processes borrowed from computational linguistics have been successfully employed in computational philology, formal models based on selected logical axioms specific to the philological domain, have not been sufficiently developed 4.  In this aspect of our work, our attention is addressed to an overall class of problems rather than just a single project. The ultimate goal is to model how various kinds of philological data serve as evidence for the construction of dynamic critical editions and critical commentaries. As another outcome, these logical models might result in the development of an extensive domain ontology and subdomain ontologies.  An example should illustrate the benefits that such a process of formalisation could have in the development of software tools for projects in the philological domain such as Memorata Poetis. An analyst designing software for a project that must deal with textual variance due to the existence of several diverging manuscripts of the same work, can afford to focus on creating tools to handle different chunks of text starting at the same textual position, as per his design specifications, while neglecting to deal with the issue of multiple syntactic interpretations in ambiguous sentences. A different project requiring such an extension to the original software in order to record concurrent syntactic analyses suggested by different scholars in commentaries will have to incorporate a comprehensive process of refactoring, instead of a simple development that extends the functionalities of the software developed in the previous project.  Much work in computational philology in the last few decades has been driven by the idea that the design and development of a digital platform for text criticism can be carried out by simply transferring and customizing many of the tools that have been developed in the field of computational linguistics for studying modern languages 5; 6; 7. However we think that it is necessary to develop a different line of research in which the tradition of philological studies can advance into the digital era without relying on such a simplistic view of the relation of such work with computational linguistics. The development of software components for the philological domain at ILC-CNR is based on the agile paradigm of software development: we mix a top-down with a bottom-up approach, which requires a continual improvement in design and implementation. Ongoing Results The library of core components under development is structured into the following packages:  philological content management, TBO management, editing, management of layers of analysis, relations (linked data) management, indexing, search, view.   Fig. 1: Class Diagram of the Aligner Component  Philological entities can be either represented as linear or non-linear structures; in the latter case we have the choice of representing textual variants as graphs 8 or in other ways (e.g. as a swarm of variants). The choice is determined on the basis of the best trade-off between fast access, representation of variable granularity, etc. The strategy for the actual representation of texts with variants will be implemented in the extended classes of the abstract PhilologicalEntity class, which provides methods to set and get the textual variants. TBO components deal with information related to the epigraphic device, in our case a small subset of the epigraphs. These components manage the multidimensional models (e.g. 3D) and any other relevant information related to theepigraphic situation. Epigraphy, as a specific communication process of writtent text, gives complementary examples of the scientific and digital requirements for a global approach of the TBO. By focusing, among many other complex aspects, on writing and context, epigraphy concerns itself with entangled information from the process of communication that computational linguistic processes only partially take into account. This is necessary to the overall scientific interpretation and understanding of any text. Editing components manage the creation, reading, updating and deletion of the data stored in the system, preserving the integrity of the data, tracking multiple versions of the information, etc. The following types of objects are affected by editing: texts with variants, automated analyses described below (in order to manually review them), data entries for free annotations (such as commentaries) and structured annotations (such as the tagging of themes and motives and semantic analyses, according to the SIMPLE methodology 9). Components related to linguistic and stylistic automated analyses both implement cutting-edge algorithms for lemmatization and pos-tagging 10 as well as embedding tools developed in the Perseus project like Morpheus. Components for metrical analysis 11 and 12, individuation of named entities, etc. are pluggable extensions. Here it is interesting to note that adapting computational models developed for Western languages could result in the loss of information regarding innate characteristics of different and more remote languages as pointed out in recent projects such as Sharing Ancient Wisdoms (SAWS-KCL). For instance the word analyses made by Buckwalter's morphological engine are not marked according to Arabic grammar but according to their translation in English 13. For example, the word biHaq~i is analysed as a preposition and this is incorrect. The words commonly used to translate biHaq~i in English, e.g., “against”, are indeed prepositions, but in Arabic grammar, biHaq~i is composed of the concatenation of  three parts: (1) bi=PREP + (2) Haq~=NOUN+ (3) i=CASE_DEF_GEN. For these reasons, we have brought about improvements to the current morphological analyzers which allow detailed analyses respecting the grammar and granularity of Arabic 14.Linked data components will be developed in order to handle the overall relations between the entities involved in the system through an identification scheme (e.g. RDF). The linking is done at different levels of granularity and between different types of objects. For example, a philological entity can be linked to another philological entity and a character can be linked to the related box in its three-dimensional model.Indexing components will create and handle data structures necessary to efficiently access stored resources.   Search components, devoted to information retrieval, will combine the data indexed in the persistence unit and exploit a large number of query techniques for accessing databases (xquery, sql, sparql, etc).View components will take into account the data structures that represent content combined with multiple levels of analysis. The interaction between the user and the system through the graphical interface (user experience) must be suitable for philologists and their specific needs, avoiding limitations due to the adaptation of the user experience of different domains.   Fig. 2: Web Interface showing the text of an Arabic epigraph aligned with its Italian translation and related morphological analysis   Conclusion In conclusion, our approach tries to model the principal entities, their relations and their behaviour in the domain of philology  at a high level of abstraction and, consequently, we derive a framework that is not based on the requirements of a specific project, but that derives from the logical modelling of the domain. Eventually, the actual software components developed according to the framework will be used for a collaborative project that combines multiple levels of analyses and annotations, in order to enrich the traditional methods applied by philologist to study intertextuality. Applications developed with the CoPhi components are made available here: <http://cophilab.eu>.  ",
       "article_title":"A top-down approach to the design of components for the philological domain",
       "authors":[
          {
             "given":"Federico",
             "family":"Boschetti",
             "affiliation":[
                {
                   "original_name":"ILC-CNR of Pisa (ITALY)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Angelo Mario",
             "family":"Del Grosso",
             "affiliation":[
                {
                   "original_name":"ILC-CNR of Pisa (ITALY)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Anas Fahad",
             "family":"Khan",
             "affiliation":[
                {
                   "original_name":"ILC-CNR of Pisa (ITALY)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marion",
             "family":"Lamé",
             "affiliation":[
                {
                   "original_name":"ILC-CNR of Pisa (ITALY)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ouafae",
             "family":"Nahli",
             "affiliation":[
                {
                   "original_name":"ILC-CNR of Pisa (ITALY)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "classical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction 1.1. Overview The technology of semantic web seems to be a suitable environment to offer solutions for linking poetic repertoires that belong to different European traditions and literatures (González-Blanco & Sélaf 2013)1. The problem of linking poetic repertoires is not simple, as there are not only technical issues involved, but also conceptual and terminological problems: each repertoire belongs to its own poetical tradition and each tradition has developed its own analytical terminology for years in a different and independent way. The result of this uncoordinated evolution is a bunch of varied terminologies to explain analogous metrical phenomena through the different poetic systems whose correspondences have been hardly studied.   1.2. Methodology The aim of this paper is to present a model able to serve as a uniform solution for terminological issues in order to build a solid semantic structure as a basis to link the different poetic systems. This structure will be used to publish repertoires on the web in a structured format and using open standards in order to build an open-source and collaborative platform based on a poetic ontology which lets interoperability among the different European metrical repertoires with different applications, such as faceted searches based on SPARQL or different kinds of visualizations, very helpful for comparative analysis. The first step to organize and manage repertoires and database systems was the construction of conceptual schema to define their basic entities and relationships. The ER (Entity-Relationship) data model is the most commonly used for this purpose, together with the data model based on records for the logical implementation (Elmasri & Navathe 2011, 27-ss)2, which is also widely accepted. To implement this conceptual model, the project ReMetCa (Digital Repertoire on Medieval Spanish Poetry: www.uned.es/remetca) has tested different systems (commercial, free, open-source, and proprietary). The final decision, after experimenting with Oracle Express Edition (González-Blanco & Rodríguez 2013)3, has been MySQL combined with a XML tagging using the TEI-verse module. The relationship between ontological models and TEI is being taken into consideration very seriously in the last years, as it is shown by the activity of the SIG ontologies group wiki.tei-c.org/index.php/SIG:Ontologies and the specific papers published on this topic (Eide & Ore 2007)4. There are also projects that have applied these techniques to the study and analysis of medieval documents (Ciula, Spence & Vieira 2008)5.   2. Getting Started From the three levels described (conceptual, logical and physical), this paper will focus on the first layer: the semantic description with the design of the semantic ontology, whose elements will be extensible and reusable for its application to other poetic repertoires. The conceptual model, designed on the basis of ReMetCa, will be transferred to the semantic Web as Linked Open Data. The abstraction of this initial model is prepared to be amplified with the necessary fields and terms to define metrical phenomena which are not shown in the Spanish poetic system or in the other repertoires which have been taken into account to design this first version of the semantic prototype. In order to enlarge its horizons, structure, description and contents, datasets of the following corpora have also been taken into account:  The Cantigas de Santa Maria Database: csm.mml.ox.ac.uk Analecta Hymnica Digitalia: database on Medieval Latin poetry: webserver.erwin-rauner.de/crophius/Analecta_conspectus.htm Bibliografia Elettronica dei Trovatori: w3.uniroma1.it/bedt/BEdT_03_20 Le Nouveau Naetebus: database on French narrative Medieval poetry : www.nouveaunaetebus.elte.hu Répertoire de la Poésie Hongroise Ancienne (RPHA) : Repertoire on Medieval Hungarian poetry: rpha.elte.hu MedDB: Lírica Profana Galego-Portuguesa www.cirp.es/pls/bdo2/f?p=MEDDB2 Corpus rhythmorum musicum (IV-IX secolo): database on Latin Medieval poetry accompanied with music www.corimu.unisi.it Skaldic poetry of Scandinavian Middle Ages: https://www.abdn.ac.uk/skaldic/db.php English Broadside Ballad: ebba.english.ucsb.edu/ Dimev: Digital index of medieval English verse: www.cddc.vt.edu/host/imev/record.php?recID=6768  To implement the conceptual model, the project uses one of the most recognized standards for semantic Web description: the Ontology Web Language (OWL), developed by W3C as an extension of RDFS. OWL is used to define the different classes, their properties and the instances of classes. It integrates sets of predefined metadata using namespaces. The set joins not only traditional well-known initiatives, such as Dublin Core, MARC or TEI, but also local proposals such as those used by some of the digital poetic repertoires that serve as a basis for this project. The TEI-Verse module6plays also an important role, due to the use that several repertoires have made of it, such as Henrik Ibsens (http://www.ibsen.uio.no) or the project of Lyrik des Hohen Mittelalters, (whose web access is not public yet), or ReMetCa itself by the addition of a XMLType field to its relational database.   Fig. 1: ReMetCa database screen with XML-type field  The software used to build the collaborative ontology is Webprotege (Tudorache et al. 2011)7, initially combined with Poolparty to create and organize vocabularies. It has been installed at ReMetCa server and opened via web in order to let participation of researchers with similar projects in the field of metrical repertoires. This system presents a light and intuitive interface but solid enough to develop a complex ontology with OWL. An important advantage is that it offers multilingual edition, which is very important for the development of such an international proposal. Once the model had been set, a metadata system has been designed to link the conceptual and logical levels based on a global abstract classification (schema), in which the different particular embodiments of each poetical tradition will be progressively included. This proposal shows both the consistency of this general language purpose and the benefits that can be obtained from the application of this model to the different local projects using a collaborative and open work system, which is essential for this new paradigm. There are a few studies which deal obliquely with some of the above mentioned aspects (Bootz & Szoniecky 20088and Zöllner-Weber 2009)9, but there is not yet a conceptual model of ontology referred to metrics and poetry. The closest related works to this topic are probably the conceptual model of CIDOC (www.cidoc-crm.org), the vocabularies of the Getty Museum, as they are designed to express relations and artistic manifestations in the field of humanities (http://www.getty.edu/research/tools/vocabularies/), the controlled vocabularies of English Broadside Ballad Project http://ebba.english.ucsb.edu/ and the linked data relations offered by the Library of Congress (http://id.loc.gov/), which do not offer a deep information on metrics vocabulary. To sum up, this project of a poetic and metrical ontology intends be much more than a repository of datasets, thesauri or controlled vocabularies. It aims to create a semantic standardized structure to describe, analyze and develop logical operations through the different poetic digital repertories and their related resources. Its final objective is to interconnect, reuse and locate the data disseminated through poetic databases in order to get interoperability among projects, to perform complex searches and to make the different resources “talk” to each other following a unique but adaptive model.  ",
       "article_title":"Building a metrical ontology as a model to link digital poetic repertoires",
       "authors":[
          {
             "given":"Elena",
             "family":"González-Blanco",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación Distancia, España",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Levente",
             "family":"Seláf",
             "affiliation":[
                {
                   "original_name":"ELTE University, Budapest",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"María Gimena",
             "family":"Del Rio Riande",
             "affiliation":[
                {
                   "original_name":"Secrit-CONICET, Argentina",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Clara Isabel",
             "family":"Martínez Cantón",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación Distancia, España",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"María Dolores",
             "family":"Martos Pérez",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación Distancia, España",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "ontologies",
          "philology",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction (1)  Digital Humanities (hereafter DH) is surely a field in rapid evolution, where open questions (2) are numerous and self-reflexivity is not new. This paper aims to contribute to the discussion around general and somehow obvious questions -  debated globally, often daily, by scholars - by reformulating the issues at stake in the following terms: other scholarship in the humanities and techno-sciences (McCarty 2013b1) has passed/passes via the experimental and the formal; in what way is the experimental and the formal done in DH different and similar? To address this question the argument will circle around two interrelated concepts:    that of ‘text’, intended in its processuality (Meister 20072) and in a wide sense from linear to discontinuous narrative, from manuscripts to printed editions, encompassing hybrid modalities such as maps (Eide 2013a3) and ‘narrative drawings’ (Groensteen 20124); that of 'modelling', intended as DH-specific research and teaching activity (modelling rather than models; see McCarty 2005, Jannidis and Flanders 20136), but also connected to multifaceted conceptualisations of modelling (in particular Kralemann and Lattmann 20137) as used and seen by other disciplines and practices.  Firstly, ways in which new technologies and languages influence approaches to texts and the consequences for research will be discussed by recalling the results of some research projects (e.g. AGORA8, Marras and Lamarra 20139, Marras 2013b10) and by referring to the literature that reflected at large on the influence the creation and use of, for example, standard markup languages have on the relation between scholars and texts (e.g. Buzzetti 200211).  This has an important epistemological consequence that in the paper will connect directly the reflection on one of, or possibly the privileged object/s of humanities research - text – to the second focus of the argument: by modelling knowledge we somehow provide for an abstract way of looking at the world. Is DH research prone to privilege a symbolic analysis of texts in opposition to a pragmatic one? Despite being informed by modelling as rooted in computing and therefore in mathematical reasoning, modelling in DH is directly interconnected to the work on texts, but is not only a way to see patterns of similarity across texts (e.g. Eide 2013b12). Beyond what is branded as DH research, other imaginative practices of assembling 'toolkits' to find patterns in human production exist (e.g. Hockney 200613). Can we sustain the unicity of the lens of computing, of modelling through computing? While being a rather theoretical exercise, a comparative perspective on modelling would need to be experimental in its nature: is modelling in computing going to lift our way of seeing and therefore thinking to another level of analysis? While keeping these questions in mind, we note that significant things are happening. DH or humanities computing in its former vest (Schreibman et al. 200414) is being institutionalized and is an example of how cross-border fertilisation, namely interdisciplinarity, is possible (e.g. McCarty 2013a15, Marras 201216, Ciula 201317). We acknowledge that DH scholars and professionals are making use of a ‘blended’ style (intended as in McCarty 200918) reflected both in their language, by adopting expressions to account for a new scenario, and in their research and teaching approaches, often integrating computational methods and terminologies with modes of discourse associated to more established scholarship in the humanities (Marras 2013a19, Flanders 200920). DH opened the stage to doing and talking about research recurring to innovative and diverse knowledge as well as to an innovative and diverse way of organising and conceptualising it. Software has also been developed to explore and represent current networked knowledge configurations (e.g. Lima's 'Knowledge Atlas', Quagiotto’s ‘Knowledge Cartography’21). However, a (cultural) change to inform the sharing of practices and results is possibly still lacking behind (Ciula 201322). Specific modelling practices in DH could be lead to combine theoria cum praxis, anchoring innovative approaches to a solid theoretical framework.    2. Pragmatic Modelling  In this paper we claim that language as mediation in designing and contextualising models is crucial. We do so by focusing on the concept of pragmatic modeling (4) intended as research strategy, framed within the complex cognitive, social, and cultural functioning of DH practices affected by cross-linguistic and interdisciplinary dimensions. Pragmatic modelling is understood as being anchored to theory and language, while at the same time claiming some freedom from both (e.g. in digital textual editing one might adopt the OHCO model while at the same time questioning it deeply). It operates within the relational and dynamic aspects of modeling.(5) Middle out method and metaphoric reasoning/language Metaphors as meta-models (Kralemann and Lattmann 201323) and linguistic tools are called to explore the creative power of pragmatic modelling (Getachew 200624 and Mazzocchi-Fedeli 201325) and to move forward the theoretical reflection as framed so far. With this respect, metaphorical reasoning exemplifies a specific strategy to guide modelling in DH: pragmatic modelling can be understood as facilitating a middle-out approach based on metaphorical language. In general terms, this translates into a move away from the dichotomy at interplay between bottom-up (models emerging from particulars or 'artifacts of study') and top-down (models imposed on particulars) approaches to what we propose to call a middle-out method; a method that acts at the crossing point of data and models adapting itself to specific “textual contexts”. Three interrelated properties (adapted from Verschueren, 1995) can be associated with this method:  Variability - the range of choices in the use of language cannot be seen as static in any respect. Negotiability - such choices are not made mechanically or according to strict rules or fixed form-function relationships, but on the basis of highly flexible principles and strategies, thus also implying the indeterminacy and unexclusiveness of the choices made. Adaptability - such negotiable choices can be adapted based on specific needs and contexts according to the variable range of possibilities.   DH Practices We will narrow down our approach with selected case studies that show how in DH (in comparison to other contexts) choices are not made mechanically or according to fixed theories, but on the basis of flexible principles and strategies potentially open to creative reasoning. Indeed, with respect to modelling theorised in computer sciences, the challenge in DH is to shift the lens of computing up the scale, to embrace the experimental nature of modelling at the lower level of the scale (e.g. in computing coding) and see indeed how it can scale up (e.g. to do critical scholarship with/via it). The opportunities enabled by modelling (e.g. emergence of patterns of relation, behaviour, and shape) are rooted not only in a ‘demonstrative’ and ‘literal language’ but also in the metaphorical one (Marras 201327, McCarty 200628). Some modelling attempts in DH and cultural heritage formalisations more in general have embarked in dedicated efforts to problematise terminology (e.g. CIDOC-CRM29; Pundit30); some prominent DH scholarship is reflecting on the limits of adopting uncritically the language of computer sciences (e.g. Eide et al. 201331 on spatio-temporal concepts in humanities and arts; Simpson et al. 201332 on what a ‘person’ is in ontological models for the humanities; Renear 201333 on what ‘datasets’ are for libraries, publishing, data curation, and DH); some other DH scholarship has ventured in creative attempts at establishing neologisms (e.g. “factoid” as described in Short and Bradley 200534): is there a trade-off in projecting historical lexicons in new contexts of use?   3. Conclusions In conclusion, departing from open and interdisciplinary conceptualisations of objects of analysis – such as texts – and of certain explorative and epistemological strategies of analysis – such as modelling – the authors will show how the spectrum of research in DH is indeed expanding our boundaries of knowledge. However, modelling practices are more than often constrained by the language they are embedded in, either because terminology is not problematised enough or because language is not used imaginatively.   Notes (1) This paper is the result of an intense discussion carried out between the two authors in the last years on the nature of DH research practices and strategies. This discussion took place in different contexts, in particular during the work on DH infrastructures carried out at ESF (Moulin et al. 201135), but also as part of informal exchanges stemming from diverse teaching experiences and works within collaborative research projects in the broad area of DH. Recently a discussion focused on modelling within the forum of Humanist (see references36) triggered some further reflections partially formalised in this paper. (2) “Is there such thing as DH? Is DH unique in its practice and research strategy?” See Gold 201237 for a rich overview on this discussion. (4) We subscribe to a functional perspective on the study of language. By focusing on use, a pragmatic perspective is also integrative in that it aims at encompassing the full complexity of the cognitive, social, and cultural functioning of language (Verschueren 199538). (5) We mean both the interplay between the object of analysis and the model (usually referred as mapping; e.g. Kralemann and Lattmann 2013, 341739), as well as across different levels of the interpretative process (e.g. close and distant reading, symbolic/sintagmatic and semantic/paradigmatic levels of text analysis).  ",
       "article_title":"Circling around texts and language: towards ‘pragmatic modelling’ in Digital Humanities",
       "authors":[
          {
             "given":"Cristina",
             "family":"Marras",
             "affiliation":[
                {
                   "original_name":"Lessico Intellettuale Europeo e Storia delle Idee",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Arianna",
             "family":"Ciula",
             "affiliation":[
                {
                   "original_name":"University of Roehampton, London, UK",
                   "normalized_name":"University of Roehampton",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/043071f54",
                      "GRID":"grid.35349.38"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "digital humanities - nature and significance",
          "content analysis",
          "interdisciplinary collaboration",
          "text analysis",
          "history of Humanities Computing/Digital Humanities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  For quite a few years the long-term preservation of digital data and resources  has been an ongoing topic within the IT-industry and archiving community. While the OAIS reference model offers a very reasonable framework for long-term archival of digital data such as digitized images, sound or text documents, the archival of highly structured digital data such as databases still poses a lot of problems. Flattening databases to XML text files has been used successfully to archive the contents of relational databases (RDBMS) 1 2 3 . However this method reduces the accessibility, since the XML-files have usually to be read back into a RDBMS to be used. Todays best practice to keep the usability of structured data as high as possible is to migrate data repositories and it’s software environment (user interfaces , analytical tools etc.) to new technology to ensure its accessibility 4 . Yet, replacing obsolete hardware and software infrastructure is an ongoing labor-intensive process that requires continuous financial effort. In addition, given that online research data is usually constantly being modified to reflect new findings and thus is changing dynamically, referencing it (e.g. for citations) is not straight forward. Despite these difficulties, the use of digital research data including databases has become very common in humanities. At the same time, the term itself of “data” is not sufficient to describe the resources used and produced by the humanist researchers: collections of digital data, digitized manuscripts, collections of digitized photographs and metadata related to it, as well as new digital resources and objects produced in the digital cultural framework. As long as project funding is available, many of these digital sources are made available to the research community. However, after the funding ceases, most of these digital sources will remain accessible only as long as the hardware and software remains in working order. After some time – typically some years – most data will go offline because of lack of maintenance. Thus, most of the digital sources created within research projects will have a rather short lifetime and are no longer available for the research community after some time. However, these digital sources are a valuable base for possible new projects but it's sustainability is not ensured due to missing fundings. Given this disappointing situation, the Swiss Academy of Humanities and Social Sciences (SAHSS) - on the behalf of the State Secretariat for Education, Research and Innovation (SERI) - has launched a project to address this situation in the national context of Switzerland. The Digital Humanities Lab of the University of Basel (DHLab) in conjunction with the Universities of Lausanne (Ladhul) and Bern, in association with the Swiss National Archives, participated in a tender and have received the task to establish a solution. In a first two years period, a pilot for a \"National curation and service center for digital data in Humanities\" (DCSC) will be established. Using several test cases of different sizes and complexity from different disciplines, the methods and processes, legal aspects, infrastructure needs, and last but not least, the cost and expenses, have to be evaluated. The proposed DCSC is based on the following premises:  Preserving software in a useable and working condition is still a very difficult task, as illustrated by the recent meeting “Preserving.exe: Toward a National Strategy for Preserving Software” by the American Library of Congress 5 . Thus it would be to difficult and costly to maintain a multitude of different systems for a long time. Emulation of obsolete hard- and software as proposed by R. A Lorie 6 is also very difficult and has its share of problems (for example see 7). Therefore the different digital sources or databases have to be integrated into a minimal number of hard- and software infrastructures. Ideally, only one hard- and software system has to be maintained.   In a first phase the adoption of existing data sources of research projects at or beyond the end of funding will be dominant. In a second step the goal must be that researchers of ongoing or future research projects are escorted through the creation and use of digital sources in order to facilitate the accessibility of the data after the end of funding.   For the exchange with other platforms and infrastructures, the DCSC implements interfaces for import, export and querying of information (as far as not restricted by  legal constraints such as copyright issues and/or protection of personal rights). The adopted digital sources must be accessible through a powerful user interface for search/analysis,  a RESTful web-service or as SPARQL-endpoint in order to integrate the data into other research projects and/or databases.     The DCSC should encourage new research models in order to allow for optimial use of digital sources and to propose efficient training modules and support for all the new research projects funded by the Swiss National Science Fundation.    International contacts are a key-point for this center, in order prepare Swiss digital Humanities research to be interrelated to international research.   Given the nature of research in humanities, we expect data sources the DCSC has to deal with to be very heterogeneous and consisting mostly of qualitative data (which is possibly linked to digitized objects). We have chosen to use the virtual research environment SALSAH 89 as a technical platform for consolidation of different data sources. SALSAH is RDF/RDFS-based and thus well suited to emulate the basic functionality of RDBMS's, simple databases such as MS-Access, FileMaker etc. SALSAH is currently actively developed by the DHLab.   Fig. 1: The webinterface of SALSAH implements a desktop metaphor within the webbrowser window in order to work with  multiple sources simultaneously.    Fig. 2: SALSAH allows a dynamic visualization of the graph-like structure of the RDF-representation of the data.    Within a research project funded by the Swiss National Science Foundation SALSAH is currently being extended with several new important features (expected in 2015):  a \"time machine\" which will allow digital objects to be referenced by permalinks which include the time of referencing. Thus such a permalink will always show the digital object in the state it had at the time being referenced. These permalinks will add true \"citability\" to the SALSAH environment.   SALSAH, which is currently organized as a (technically) centralized system, will be transformed into distributed, self-organizing P2P system. At the same time, an archival system based on DISTARNET 10 will be added to SALSAH to secure the against data loss du to catastrophic events like hardware failure, flooding, fire etc.at any SALSAH location. DISTARENT also uses P2P technology to maintain redundant multiple backups of the data within the network.   Within the DCSC project, SALSAH will be extended to support \"open data\"-standards 11  for access and \"linked data\" 12 . However, open access may be restricted by legal reasons (copyright, privacy etc.). SALSAH includes a fine-grained identity and rights management.  SALSAH will be continuously enhanced according to the needs of the researchers using the platform. It is planned to move SALSAH to \"open source\" by the end of 2014. The main tasks of the DCSC will be threefold:   Maintaining the technological infrastructure and adapting it to the needs of the researchers and changing technology. This task will be located in Basel during the pilot phase, but since SALSAH will become an open source project, other institutions and individuals may contribute to the SALSAH base. However, in our experience open source projects need a powerful \"coordinating institution\" in order to be successful. The DHLab will be available to play this role.  Assistance to the researchers. In Humanities many researchers working with digital sources do not have the technical knowledge to fully exploit the advantages of the digital processes. The DCSC will support the researchers to use digital methods and tools in the best possible way for their research, and encouraging training and education. To create a report with recommendations on how to proceed and transfer the pilot project into a permanent institution.  It has to be noted that the projects funding goes way beyond a \"normal“ scientific project funding and shows the strong commitment of all involved parties (SAHSS, SERI, Swiss National Science Foundation) to define a persistent national data curation and service center for digital research data in the humanities. The team composited of the University of Basel, Bern and Lausanne and the National Archives demonstrates that as well. The project pilot phase is financed by the SAHSS and the SERI. Since Switzerland is a multilingual nation with a highly federalistic structure, we decided to create the DCSC as a \"virtual\" center where the technological infrastructure currently will be located and maintained in Basel, but all the other tasks will be performed by local \"branch offices\" which are very close to the researches; during the pilot step, Lausanne and Bern are testing this “branch office” or “satellite” model. As soon as SALSAH has the P2P functionality implemented, also the technical infrastructure may be distributed if necessary or desired. Given the DISTARNET archival system, the data is secured against loss without the necessity of the local branches to build an expensive and complicated backup infrastructure.  ",
       "article_title":"National Data Curation and Service Center for Digital Research Data in the Humanities",
       "authors":[
          {
             "given":"Lukas",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab / University of Basel",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Peter",
             "family":"Fornaro",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Lab / University of Basel",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"Clivaz",
             "affiliation":[
                {
                   "original_name":"LADHUL / University of Lausanne",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Research in the emergent field of Spatial Humanities, especially in GIS-based approaches, is primarily conducted on a spatial macro-level. Statistical information, such as distributions of archaeological finds or movement and communication patterns is generally mapped to two-dimensional representations of the areas under consideration.12 Humanities research related to spatial contexts is not limited to quantitative scope. Studies in archeology, art history and human geography, to name only a few disciplines, focus on places as small as public spaces and buildings as the stage for liturgical, political and economic practices.3 But important analytical approaches in these fields necessitate three-dimensional representations. Important functional properties of objects in space, such as visibility and accessibility, cannot always be determined by two-dimensional ground plots. To tackle this shortcoming, the research project Inscriptions in their Spatial Context (IBR) develops research methods and corresponding software tools for Spatial Humanities research on the 3D-geometry of smaller environments like building interiors. Geometric data drawn from geodetic scannings are semantically enriched and connected to scientific textual and visual data. Theoretical foundations as well as the software are being tested in an extensive case study on a medieval church. IBR is a joint research project of the Institute for Spatial Information and Surveying Technology at the FH Mainz - University of Applied Sciences (i3mainz) and the Academy of Sciences and Literature in Mainz. It is funded by the German Federal Ministry of Education and Research (BMBF) and brings together experts from the fields of geoinformatics, surveying engineering, digital humanities, art history and epigraphy.   2. Acquisition of geometries Panoramic photography has become a common technique for merging smaller photographs into one larger image. Not only established software solutions like Google Street View are able to visualize such panoramic images. Also popular scientific applications4 are able to convey the impression of standing at the captured location. Since images do not represent 3D-information, a measuring campaign is necessary to fulfill the real needs of spatial research. Terrestrial Laser Scanning (TLS) creates high quality point clouds in the range up to several hundred meters.5 Until now, tools for processing this kind of information have been mainly designed for the use in the field of engineering. In the Cultural Heritage domain, as in archeology, TLS is usually used for documentation and modeling only or for 2D-analysis, such as for the extraction of floor plans. Parts of the content of the point cloud data are often neither extracted nor used, but can now be reused with the IBR software. The “GenericViewer” is an easy to handle web-application which provides typical functions of a panorama viewer and allows the user to identify objects in point clouds and to annotate them semantically. Thereby it is possible to work on 3D-objects in an intuitive way, because the software connects panoramic images with point cloud data.6 The application communicates with two data storage units: A triple store contains all connections between expert data, spatial data and external resources. Another database handles point clouds, panorama images and spatial objects. It is possible to access the user created data via the GenericViewer itself and through a machine-readable interface. Only Javascript and a browser with webGL-support are necessary to use the web application.   3. Semantic enrichment The GenericViewer integrates a customized version of the website-annotator Pundit.78 Users formulate simple subject-predicate-object-statements over geometries and texts that are stored in the form of OAC-conformant RDF-triplets. Semantic entities can be drawn from generic ontologies like DBpedia and if more specialized vocabularies9 do not suffice, also from project-specific resources. Physical objects identified in the point cloud are represented by 3D-coordinates, i.e. numerical values. To make them become meaningful objects, some semantic description is necessary. Therefore, geometries can be identified as specimen of a certain type, e.g. a tomb slab, and semantically connected to other textual and geometric resources. The tomb slab, for example, can be tagged as an instance of the class “tomb_slab”, and its inscription can be connected to a corresponding critical edition by the relation “is_edition_of”. If a researcher wants to suggest a sculptor for this object, knowing that this attribution remains merely a hypothesis and that there might be conflicting opinions, that hypothesis can be expressed as such via suitable predicate. Thanks to the OAC format, annotations come to represent a scholarly discourse rather than just a set of semantic descriptions. The annotation repository can be programmatically accessed and queried via a SPARQL-endpoint, thereby enabling quantitative analyses (e.g. how many tomb-slabs are there in the church) and the creation of aggregate resources with new applications and information services on top of it. The goal is to create a 3D-GIS for quantitative as well as qualitative research with a focus on the interconnection of resources and geometric analysis, as an alternative to approaches centered on multimedia presentations.10   4. Case study The GenericViewer is currently being used experimentally in a case study on the late-gothic parish church Liebfrauenkirche in Oberwesel (Middle Rhine Valley, Germany), using 3D-data that has been gathered in a TLS-campaign. The study investigates inter alia the placement of tombs and memorial inscriptions. It looks at potentially relevant factors like the social division of the congregation room, procession routes, and other places of liturgical practices. Among the research questions are: What does the placement of a tomb say about the social status of the deceased, given social features of the surrounding tombs? How does an inscription text relate to liturgical texts that were read on procession stations in its proximity? Who could actually see an inscription from which position in the church, and do some texts appear to have been directed at certain groups within the congregation room? The evidence relevant to such questions as well as conclusions drawn from the data are connected to the 3D-representation as semantic annotations.  An important textual source in this study is the epigraphic database “German Inscriptions Online” (DIO), which provides critical editions for the inscriptions under consideration. The spatial configuration of the objects bearing inscriptions and liturgical \"key-positions\" like the apse and the chancel have been analysed with respect to visibility patterns. The analytical perspective of Space Syntax11, which has been been successfully employed in earlier works on historic liturgy and interiour church architecture12, seems to be a promising approach. Due to the complexity of the spatial configurations, our approach offers advantages over conventional text-image representations in terms of analysability and confirmability. The foundational inscription, a glass painting in the apse of the church13, is a case in point. It has been described as a self-assured, even provocative message from the citizenry who build the church to the clergy and especially the bishop. The founders chose the rather exceptional place of the apse windows,  because here it was most visible from the choir and the main altar,  where the clergy assembled.14 This presupposition was empirically tested in a visibility analysis.   5. Discussion At the moment, the GenericViewer is usable with TLS-Data only. However, scanning campaigns can be quite expensive, and the geometrical accuracy delivered by this technology is not needed in every case. There are other measurement techniques, e.g. “structure from motion”, that should be supported in future versions. Visibility is relative to human perception and it is dependent on a variety of factors, many of which can not currently be modelled in our algorithms. Certain aspects of visibility, e.g. the ability to recognize a certain gesture from a distance, still have to be approximated by human experiments.15 How useful the data produced with our software is for a broader scientific community will depend on the choice of semantic resource (users are always faced with a trade-off between specificity and generalizability) and the underlying text-document. Pundit, like many similar annotation tools16, annotates positions in the DOM-tree of HTML-documents. But that means annotating the presentational layer of a text rather than the text itself. Because of that, IBR decided to annotate tree positions in TEI-encoded XML-documents. But not many digital documents in the humanities domain are available in this format, and it is not clear yet how to treat PDF-files.  IBR offers the possibility to contribute to or extend the open source code and to connect third-party applications, analysis tools and ontologies. More informations are available on www.spatialhumanities.de.  ",
       "article_title":"Relating texts to 3D-information: A generic software environment for Spatial Humanities",
       "authors":[
          {
             "given":"Martin",
             "family":"Unold",
             "affiliation":[
                {
                   "original_name":"i3mainz - Fachhochschule Mainz",
                   "normalized_name":"University of Applied Sciences Würzburg-Schweinfurt",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01k5h5v15",
                      "GRID":"grid.449775.c"
                   }
                }
             ]
          },
          {
             "given":"Felix",
             "family":"Lange",
             "affiliation":[
                {
                   "original_name":"Akademie der Wissenschaften und der Literatur Mainz",
                   "normalized_name":"Academy of Sciences and Literature",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01kdxra28",
                      "GRID":"grid.461597.8"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The use of topological graphs, or networks, to represent and analyze the semantic contents of source materials, such as texts and images, has become a signature contribution by the digital humanities to the humanities in general. Specific techniques, such as topic modeling and network analysis, and general approaches, such as macroanalysis and distant reading, exemplify the popularity and effectiveness of methods based on the graph theoretical representation and statistical modeling of cultural materials. However, because of their mathematical complexity and their focus on very large corpora of texts, these methods are beyond the reach of many humanists interested in the interpretation of smaller sets of source materials for cultural meaning. They are also suspect since they introduce ontological commitments that both elide traditional notions of human agency and reframe culture as a set of abstract, metrical dimensions. In this talk, I introduce “common container correlation” (C3) as a relatively simple and transparent interpretive method for the graph theoretical analysis of source materials that may be practiced by both students and more advanced researchers to excavate and make sense of cultural models implicit in textual materials. C3 may be described as a variation of co-occurrence analysis designed to take advantage of the abundance of encoded cultural materials available to the digital humanist and to allow for the analysis of small sample sizes, such as individual texts. Formally, a common container correlation is just a link, or edge, that is asserted between any two items, regarded as nodes or vertices, that are contained within the same structural container. The set of all such links produces a graph of nodes and links based on their co-occurrence in a common container. In some cases these graphs will have meaning—that is, they will exhibit patterns that lend themselves to structuralist and other forms of interpretive analysis. These patterns may sometimes be correlated with psychological, sociological, or material causes that will be of interest to the humanist. For example, in a novel marked up with TEI-based schema, we may choose to define the paragraphs of the text as container elements and tagged references to proper names as contained elements. We then assert that all named agents in a given paragraph are related to each other (in the special sense of co-occurring). The set of all of these assertions for all paragraphs will produce a kind of social graph that may then be visualized and analyzed in structural terms. In such a case, it may emerge that two characters consistently appear on opposite sides in multiple instances of a force-directed representation of the graph. This may be evidence of a structural opposition that will have emerged from the statistical distribution of the selected elements. Other approaches may use other container elements, such as scenes, and combinations of contained elements, such as places and people. The C3 method is easy to implement using available tools. Container and contained elements in XML encoded materials may be extracted using simple XPath statements (by means of a variety of tools) and dumped into tables with columns for container IDs and contained IDs. Such tables may then be transformed using simple SQL queries into various graph data formats for visualization and analysis in tools such as Gephi, GraphViz, SHIVA, and D3. Depending on the intention of the user, the resultant graph may or may not reflect the frequency of edges and vertices in the source data. In this talk I will describe the C3 method using examples taken from three digital humanities projects with which I have been associated. First, I will describe the application of the method to rhetorical figures (containers) and characters (contained elements) using data from the Princeton Charrette Project. Second, I will describe how undergraduates in an introductory digital humanities course at the University of Virginia created a database relating characters and paragraphs in Austen’s Persuasion. Third, I will describe the use of the method in Stephen Railton’s Digital Yoknapatawpha Project, drawing on data correlating scene containers to people and places as contained elements in William Faulkner’s corpus of fiction. In each case, I will explore the interpretive implications of the algorithms used to visualize the data, taking particular care to describe the specific steps involved in going from markup to data representation to visualization to interpretation. In this way, I hope to connect the discourse on data-driven textual analysis to traditional interpretive methods, such as close reading and structural analysis, in order to produce a genuinely humanistic use of quantitative methods that does not alienate the researcher from the tools of interpretation.  ",
       "article_title":"Common Container Correlation: A Simple Method for the Extraction of Structural Models from Statistical Data",
       "authors":[
          {
             "given":"Rafael",
             "family":"Alvarado",
             "affiliation":[
                {
                   "original_name":"University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "data-driven interpretation",
          "Cultural network analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In this paper, I argue that the concept of potential criticism addresses many key challenges and issues involving cultural empowerment processes in the digital humanities. With the proliferation of digital humanities projects that engage with the study and analysis of large sets of digital texts, potential criticism proposes an analytical methodology that has broad applicability across different types of texts, media, and disciplines. At its core, potential criticism is an idea derived from the work of the Oulipo and recombinatory poetics. The Oulipo is an acronym for the Ouvroir de literature potentielle, which roughly translates into English as \"Workshop of potential literature.\" The writers and mathematicians of the Oulipo focused on creating new works through the use of constrained writing techniques. Well-known members of the Oulipo include Georges Perec, Italo Calvino, and Raymond Queneau.  But constrained and algorithmic techniques can not only be used to generate new literary works, such methods also can be used to analyze existing texts, as evidenced by the work of the Oulipo's Harry Mathews and his Mathews's Algorithm. Mathews advances the idea that pre-determined and mathematical constraints can be used as a way of both recombining and analyzing existing texts. As Mathews states as the beginning of his essay explaining his algorithm: \"Potential reading has the charm of making manifest the duplicity of texts, be they oulipian or not.\" (trans. Shannon Clute). This work has important consequence for cultural empowerment processes in the digital humanities. As we seek to analyze ever-larger bodies of digital texts through our digital humanities projects, we need new methodologies that can extract meaningful data sets for further analysis and discovery.  In our 2011 book, The Maltese Touch of Evil: Film Noir and Potential Criticism (Dartmouth College Press), Shannon Clute and I laid out how the methodology of potential criticism could be used to analyze moving image texts (specifically in our case, films noir). Our method has important implications for the digital humanities, which I will expand upon in this paper. First, we focused on a moving image archive to show how constrained analytical techniques can move beyond the analysis of alphabetic and/or literary texts. Potential criticism can be used on any digital text or medium. Second, echoing the work of Stephen Ramsay who deploys a similar methodology in his book Reading Machines: Toward an Algorithmic Criticism, the results of potential criticism are not an end in and of themselves. These new data sets, revealed through the application of mathematical or algorithmic means, produce new and generative starting points for further investigation and discovery. Furthermore, the resultant data sets are not merely a random remix of a body of texts, they are constrained data sets that reveal potentially new information about the overall corpora itself.  This paper argues for the broader applicability of the concept of potential criticism in the digital humanities. I will highlight two practical demonstrations of potential criticism as a working methodology. First, we used the concept of potential criticism in the Film Annotator's Workbench Project from Indiana University (IU) in conjunction with IU programmer Will Cowan. Cowan's work on this project was funded by the NEH. In this project, which began in 2010, we used pre-existing digital humanities tools (in this case, the Film Annotator's Workbench and Omeka) to demonstrate how the potential criticism methodology can analyze and annotate a large number of films noir. Second, we similarly used the concept of potential criticism as the basis for our new investigations around films noir, which was published in 2011.  The goal of this paper is to share our findings and disseminate our approach to potential and algorithmic criticism. I will contribute my first hand observations from our multi-year investigations into how potential or algorithmic criticism can be used to analyze any kind of digital text and media, especially our work around film and video analysis and annotation. Moreover, I will discuss how potential criticism as a working method can empower certain types of scholarly communities around shared corpora of texts. Both the Film Annotator's Workbench Project and our Maltese Touch of Evil Project operate on the open web through freely available tools to publicly disseminate constrained data sets for other scholars to explore and discover.  In keeping with the theme of this year's DH conference, potential criticism as a practice intentionally encourages scholars to work across disciplinary boundaries to share their insights and ideas in a workshop fashion. In many ways, potential criticism needs to be an open-ended practice that does not presuppose the primacy of any particular hermeneutic method, but rather encourages a variety of approaches in order to reveal the potential information that exists within large bodies of texts. In closing, potential criticism can operate as a method for rapidly testing out new hypotheses that will advance our cultural understanding of large corpora of texts and media. This is one of the benefits, but also one of the key challenges, of potential criticism. Each algorithmically derived data set is novel investigation into a body of texts. Therefore, potential criticism generates its own kind of scholarly archive that requires consideration and development. It is imperative among digital humanities scholars, researchers, and programmers to continue to explore and build new ways of making newly generated data sets readily available to other scholars through the open web.   ",
       "article_title":"Potential Criticism in the Digital Humanities",
       "authors":[
          {
             "given":"Richard",
             "family":"Edwards",
             "affiliation":[
                {
                   "original_name":"Ball State University",
                   "normalized_name":"Ball State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00k6tx165",
                      "GRID":"grid.252754.3"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "film and cinema studies",
          "literary studies",
          "corpora and corpus activities",
          "cultural studies",
          "digital humanities - nature and significance",
          "content analysis",
          "media studies",
          "text analysis",
          "text generation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  Computational modeling of social categories can be found in a wide range of digital media works. For example, within computer role-playing games (RPGs), racial categorization is often used to style the visual appearance of a player’s avatar or trigger different canned reactions when conversing with a non-player character (NPC). In social media, users might join groups based on shared taste or categorize each other into groups such as “colleagues” or “family members” using privacy settings. However, in most such systems, category membership is determined in a top-down fashion. Members are often slotted into single, homogeneous groups, with no possibility for hybrid identities, identities that exist at the margins of groups, or identities that change over time. Taken holistically, such approaches have many limitations. These deficiencies are particularly visible when trying to accurately model the nuance of social category membership in the real world.  Our Chimeria platform (hereafter Chimeria) addresses this deficiency. It create more nuanced social categorization models in two primary ways: (1) by modeling the underlying structure of many social categorization phenomena with our Chimeria engine; and (2) by enabling users to build their own creative applications about social categorization, using the engine as a backbone. Drawing on theories from sociolinguistics (Polyani, 1989), cognitive science (Lakoff, 1987), and sociology of classification (Bowker and Star, 1999), the underlying engine allows for the movement of individuals within, between, and across social categories. It also allows for members to be more central to a group than others, to assimilate or naturalize in relation to a hegemonic group, and to claim membership in multiple groups. In this paper, we discuss the components of Chimeria and two sample applications built with it.   2. The Chimeria Authoring Platform  Chimeria supports authoring narratives of group membership in any social identity domain through a data-driven approach. Chimeria is divided into three components (Figure 2).    Fig. 1: The Chimeria Platform   Chimeria Engine: A mathematical model of users’ degrees of membership across multiple categories. It provides the functionality to calculate, modify, and simulate changes to these memberships and serves as the logical processing component of the system. It models users’ category memberships as gradient values in relation to the membership values of more central members (Harrell, 2010; Bowker and Star, 1999; Lakoff, 1987). This enables more representational nuance than binary statuses of member/nonmember. Narratives processed by Chimeria are authored using a GUI or in the XML file format with a narrative structure as described in (Harrell et al., 2013).  Chimeria Application Interface: A visual interface for user interaction and for experiencing the narratives related to the category membership changes driven by the Chimeria Engine. It provides freedom and flexibility over the aesthetic and visual components of narratives. The interface can take on multiple forms (e.g., a text-only interface or a 3D virtual environment).1 The separation between the back-end (the Chimeria Engine) and the front-end (Chimeria Application Interface) provides the flexibility to go through the same narrative trajectory in relation to membership shifts but with varying visual appearance.  Chimeria Domain Epistomologies: An “epistemology” is an ontology that describes cultural knowledge and beliefs (Harrell, 2013). In Chimeria, they are the knowledge representations describing the categories being modeled. The data utilized by Chimeria    to present these categories to users include both author-contributed (e.g., artworks or narratives) and data-driven (e.g., an API call to YouTube to query for a video) assets.  3. Chimeria Application Domains To better illustrate the capabilities of the components within our system we describe two very different narratives created using Chimeria: 1) a fictional social networking application which models social categories in the domain of musical preferences (Harrell, 2013); and 2) a computer role-playing game (RPG) scenario which models a conversational narrative between the player and a non-playable character (NPC).  3.1 Chimeria: Musical Identity Social Network  In Chimeria: Musical Identity Social Network, the Chimeria Engine models category membership based upon musical preferences that are automatically constructed from a user’s set of music “likes” (binary indications of positive valuation) on a social network profile. These “likes” constitute a set of musical artists from which we extrapolate, using commercially available musical classification data, moods (e.g., cheerful, gloomy, etc.), themes (e.g., adventure, rebellion, etc.) and styles (e.g., film score). This provides the context for non-binary group membership and passing (the “ability of a person to be regarded as a member of social groups other than his or her own...generally with the purpose of gaining social acceptance,” Renfrow, 2004). Each user’s set of moods, themes and styles, then impacts the generated narrative in fundamental ways. We construct a conversational narrative on a social network structured by a model of conversation from sociolinguistics (Polanyi, 1989).  The Chimeria Application Interface consists of a procedurally generated photowall: a dynamic collage of photos representing the user’s musical taste preferences. A feed of recent updates, posts, and invitations appear in an adjacent vertical timeline (see Figure 2). The system reacts to the user by generating interaction events from computer-controlled users who make up the user’s social circle within the system.   Fig. 2: A screenshot of the Chimeria: Musical Identity Social Network application interface  Figure 3 presents a screenshot of Chimeria: Musical Identity Social Network. Using musical preferences from the user’s Facebook music likes or by manual entry, a hybrid real/fictitious narrative experience progresses over time. A series of dynamically generated posts by the user’s friends (non-player characters) comment on the user’s membership within one or more musical affinity groups (i.e. “You’re a raucous rock fan now?” or “Want to hear some airy jazz music?”). The user may “like,” “dislike,” or simply ignore these posts, resulting in group membership changes illustrated by alterations to a self-updating “photowall.” Some friends might question newly discovered interests, while others might pass judgment on prior affiliations. The resulting narrative may describe passing or assimilating as a member of a new group, reinforcing a prior group affiliation, or even being marginalized in every group. Some groups are deemed oppositional, privileged, or marginalized relative to others.    Fig. 3: The Chimeria Platform Applied to Musical Identities in a Social Network  3.2 Chimeria: Gatekeeper  Chimeria: Gatekeeper models a common RPG scenario – a player trying to gain access to the inside of a castle. Within this sample application, we demonstrate the power of the Chimeria Engine for enhancing this scenario by modeling more complex, adaptive, and nuanced conversations between PCs and NPCs, overcoming limitations identified in other videogames (Harrell et al., 2014). Figure 4 shows a preliminary visual design from Chimeria: Gatekeeper.   Fig. 4: Chimeria: Gatekeeper preliminary scenario visual design  Drawing on the work of Erving Goffman (Goffman, 1963), Chimeria: Gatekeeper attempts to model the effect of stigma on conversation. Within the scenario, the PC is initialized to the ‘discredited’ category and the NPC to the ‘accepted’ category. The accepted category is prototypically defined as the Brushwoods race – short, plain-spoken, and wearers of rough spun clothing. The discredited category is prototypically defined as the Sylvanns race – tall, well- spoken, and wearers of fine clothing.2 To gain access to the inside of the keep, the player has to convince the guard that she or he is among the accepted category, in effect “passing” as a member of the category that has been instantiated as “accepted” (Harrell et al., 2014). User actions and responses (e.g., slouching to adopt the posture of a prototypical Brushwood or displaying fine Sylvann clothing) incrementally shift the NPCs model of the PCs membership with respect to the categories, bringing the player closer to gaining access to the keep or to being rejected. Internal thoughts of the PC emphasize trade-offs between gaining utilitarian access to the keep and the loss of self-identity that can occur in trying to pass. The guard’s responses of approval or disapproval respond accordingly to chosen actions. A transcript of a run-through of Chimeria: Gatekeeper is shown in Figure 5. Using Goffman's notion of impression management, we handle alternatives to the common trajectory of intentionally passing by considering other player decisions such as voluntary disclosure of stigma and slipping (trying to pass as a member of an accepted category, but failing). The modeling of passing and social categorization membership in Chimeria: Gatekeeper seeks to capture the stakes and power relationships often at play in real world social interactions.    Fig. 5: Chimeria: Gatekeeper sample run-through  4. Conclusion  In this proposal, we have presented Chimeria, a platform for creating and analyzing narratives related to social group membership. By modeling character identities in a dynamic and nuanced fashion, we explore complex identity phenomena. By modeling social identity phenomena related to categorization, we use Chimeria to suggest how to better critically examine and express how identities are negotiated using digital media systems.  ",
       "article_title":"The Chimeria Platform: User Empowerment through Expressing Social Group Membership Phenomena",
       "authors":[
          {
             "given":"D. Fox",
             "family":"Harrell",
             "affiliation":[
                {
                   "original_name":"Comparative Media Studies Program, Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Dominic",
             "family":"Kao",
             "affiliation":[
                {
                   "original_name":"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Chong-U",
             "family":"Lim",
             "affiliation":[
                {
                   "original_name":"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Jason",
             "family":"Lipshin",
             "affiliation":[
                {
                   "original_name":"Comparative Media Studies Program, Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Ainsley",
             "family":"Sutherland",
             "affiliation":[
                {
                   "original_name":"Comparative Media Studies Program, Massachusetts Institute of Technology",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "multilingual / multicultural approaches",
          "cultural studies",
          "software design and development"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction The Victorian author Charles Dickens was among the first publishing entrepreneurs to run mass- produced weekly/monthly magazines on a successful commercial basis.  He employed many ‘salaried staff　writers’ (Nayder, 2002), who had to write under anonymity, including Elizabeth Gaskell, Adelaide Anne Proctor et al., in Household Words and All the Year Round, the journals ‘conducted by’ Dickens (Stone, 1968; Thomas, 1982; Allingham, 2011). On the other hand, Dickens collaborated with his younger contemporary Wilkie Collins on a number of stories, typically for the Christmas Numbers of his journals. While some of their collaborative pieces were written with the assistance of other staff writers, four works are known to have been co-authored by Dickens and Collins alone (Nayder, 2002): The Frozen Deep (1857), The Lazy Tour of Two Idle Apprentices (1857), The Perils of Certain English Prisoners’ (1857), and No Thoroughfare (1867). The four collaborations can be seen as betokening what appears to be a ﬁrm presence of Collins, a foothold he had gained, in the Dickens circle by the time he and Dickens launched into the joint works beginning in 1857. These collaborative writings vary in design and style from one another as well as in theme and setting. In some cases, one chapter can be read as radically different from another due in large to the varying proportion of contribution by each of the duo: some chapters were written either Dickens or Collins alone, while there are other chapters that were jointly written although the extent of collaboration has yet to be identiﬁed quantitatively. In order to provide new insight to the nature of collaborative authorship, the present study applies s series of stylometric techniques: (1) Craig’s extension of Burrows’s Zeta test for reliably extracting author markers from a large number of candidate words; (2) Cluster analysis based on Burrows’s Delta distance measure (Burrows, 2002) to compare the collaborations with the canonical works of Dickens and Collins; and (3) Rolling Delta (Eder, Kestement and Rybicki, 2013) in an effort to detect authorial takeovers or to estimate the extent of contribution by each of the two authors in their collaborative writings. 2.   Single authorship and mixed authorship Although the lack of byline makes it difficult to determine the authorship of Christmas numbers for Dickens’s periodicals, the account book in the office of Household Words helps identify many of his collaborators (Thomas, 1982). Table 1 shows bibliographic details for the four collaborative works between Dickens and Collins. The Frozen Deep was originally written as a drama in three acts. Collins drafted the manuscript and Dickens heavily revised it. The script of the drama remained unpublished until 1866, when Collins altered it single-handedly, getting rid of Dickens’s hand. After Dickens’s death, Collins adapted the play as a novella for use in his public reading tour in 1874 (Brannan, 1966). No Thoroughfare was also written first as a drama and then rewritten into a novella form. When the collaborative pieces are divided into smaller units like a chapter or act, eight units (including The Frozen Deep) are of single authorship, with the remaining six units being a case of mixed authorship.   Fig. 1:   3. Testing the authorship of collaborative chapters The following experiments draw on a Dickens corpus comprising 22 texts and a Collins corpus with the same number of texts as a basis of reference, with which we compare the style of the collaborative chapters (see Tables 3 and 4 in Appendix for details). The ﬁrst round of analysis is to run Craig’s version of Burrows’s Zeta test in order to extract Dickens markers as well as Collins markers. The vast majority of authorship attribution studies have relied on n most common words in the corpus/text in question (Burrows, 2002/2005; Eder, 2010; Eder & Rybicki, 2009; Hoover, 2003/2004; Rybicki, 2009; to name but a few). The recent works by Craig & Kinney (2009) and Hoover (2010; 2011; 2013), on the other hand, have demonstrated Zeta’s strong power of differentiation between two sets of text samples. Other keywords extraction techniques include the use of Log-likelihood ratio (Dunning, 1993) criticised by Tabata (2012) for being prone to burstiness (and for its tendency to produce too many false-positives); Mann-Whiney U test (Kilgariff, 2001); t-test (Hoover, 2010); bootstrap test (Lijffijt et al., 2012); Random forests (Tabata, 2012), and so forth. The particular strength of Craig’s version of Zeta analysis is its simplicity and effectiveness well documented in Hoover’s studies mentioned above.  A Zeta distinctiveness ratio for the word i is calculated in the following formula:   Fig. 2:   where N(x) = Total number of text-segments in the corpus x; DF(x)i = Document (or Segment) frequency for the word i in the corpus x; N(y) = Total number of text-segments in the corpus y; N(y) – DF(y)i  = Document (or Segment) frequency negative for the word i in the corpus y. The R package stylo (a suite of tools for stylometric analyses) (Eder, Rybicki, and Kestmont, 2013) includes the function oppose() with an option to calculate Zeta scores. In the present case, each text was sliced into 10,000-word segments with rare occurrence threshold set to 2 so as to exclude hapax legomena and ﬁltering threshold set to 0.5 to extract strongly discriminating words (thus only words(i) with Zeta(i) < 0.5 or Zeta(i) > 1.5 are picked up). The procedure detected 122 authorial markers: 61 Dickens markers and 61 Collins markers as listed in Table 2.   Fig. 3:   When the marker words were fed into a cluster analysis, the resulting dendrogram (Fig. 1) clearly differentiated the two authors in the distinct clusters.  The marker words are also sensitive enough to show sub-patterns: both in Dickens and Collins clusters, the texts written in their early career ﬂock together (Dickens’s texts in the 1830’s and Collins’s texts published in the 1850’s). Dickens’s early works branch close to the sketches/travelogues as opposed to fictions.   Fig. 4: Cluster analysis: Dickens versus Collins (Distance: Burrows’s Delta)  Fig. 4 is a cluster dendrogram with the collaborative pieces included in the analysis. The prominent feature of this result is that:  1. The single-handed chapters/acts ﬁt in well with the author’s cluster (Chapters 1 and 3 of The Perils of Certain English Prisoners perch in the topmost sub-cluster together with the Overture and Act 3 of No Thoroughfare, whereas The Frozen Deep and Act 2 of No Thoroughfare are found as the nearest neighbours to each other, with Chapter 2 of The Perils placed in the Collins cluster. 2. The co-authored chapters/acts form slightly distant sub-clusters in each of the two author’s main clusters  4.Letting the Delta roll through to find dynamic shifts in style Although the result of cluster analysis, being in consonant with the bibliographical details given in Table 1, helps conﬁrm the effectiveness of style markers found through a Zeta test, it inevitably tells us about the limitation of the procedure that captures only a static snapshot of stylistic similarity or difference between texts. Language is never monolithic throughout a text. Language in ﬁction varies wildly from narratives to dialogue, or vice versa. Language in ﬁction is indeed quite mobile.  We need, therefore, a technique to capture dynamic style change. If the collaborated chapters can be sliced into consecutive segments of n words with a partial over- lapping so that we can roll through to focus upon a certain stretch of text like a moving camera rather than take the entire text/chapter in one snapshot, it will be possible to detect subtle fluctuations of style between one stretch and another as well as to pinpoint where one author takes over from another, etc. Eder, Rybicki, and Kestemont’s Rolling Delta was developed exactly for this purpose. Figure 5 shows a result of Rolling Delta run with a window size set to 3,000 words, a step size of 300 words, using 100 most common words as variables. The whole text of The Perils of Certain English Prisoners is cut into consecutive 3,000 word-segments, with each segment compared with the centroids of Dickens and Collins, respectively.   Fig. 5: Rolling Delta: The Perils of Certain English Prisoners and the centroids of Dickens and Collins  What strikes us is that the two major intersections (marked with green vertical lines) roughly correspond to the chapter boundaries in the text, a result that illustrates how the technique is capable of pinpointing possible authorial takeovers.   Fig. 6: Rolling Delta: The collaborated chapters of the Lazy Tour (The thicker line indicates Dickens’s centroid)  Fig 6 displays Delta polygons of the four collaborated chapters of the Lazy Tour of Two Idle Apprentices. Of particular interest is that a remarkably similar pattern holds throughout the four diagrams: it is always Dickens who takes the lead at the outset of a chapter. He runs quarter or halfway (at most) into each chapter before passing over to Collins. The series of Rolling Delta plots seem to reﬂect an interesting nature of collaboration as well as the unequal partnership (Nayder, 2002) between Dickens and Collins: Dickens always takes initiative, sets a keynote for the whole chapter, which Collins takes over and continues the rest of chapters, a typical relationship between a master and his disciples.   ",
       "article_title":"Stylometry of Collaborations: Dickens, Collins and their collaborative writings",
       "authors":[
          {
             "given":"Tomoji",
             "family":"Tabata",
             "affiliation":[
                {
                   "original_name":"GSLC, University of Osaka",
                   "normalized_name":"Osaka University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/035t8zc32",
                      "GRID":"grid.136593.b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "stylistics and stylometry",
          "text analysis",
          "authorship attribution / authority"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Maps are one of the most universal forms of communication. Having no need for a written language or alphabet, their ability to convey meaning lies within the images that are drawn on the canvas, be it a cave wall, a piece of parchment, or a computer screen. This paper investigates how digital mapping tools present students and readers of literature with an unprecedented ability to map fictional spaces in their own liking and how these various representations can influence our understanding of and relationship with literary works. Typically thought of as a tool to assist the user in finding a certain location, when used to identify fictional spaces, maps possess the power to convey societal and cultural ideologies. In recent scholarship, there has been a growing interest in the interplay between maps and narrative. “Story maps,” “fictional cartography,” and “geospatial storytelling” are some of the terms utilized to describe the relationship between place and space (Caquard). Our paper takes the current discussion of the connection between narrative and space to another plane by exploring the exchange between fictional spaces and narrative as represented by maps. We will construct the the following spaces conceived of by Spanish and Latin American novelists through immersive, 3D mapping technology: Mocondo, the setting for many of Gabriel García Marquez’s writings loosely based off of his hometown in Columbia. Obaba, a town continually developed in the mind of Basque writer Bernardo Axtaga for his novels and short stories; the village of Ordial in the mythical region of Celama generated by Luis Mateo Díez as the setting for his novels in the Leonese countryside; Región - the territory invented by Juan Benet for his novelistic trilogy; and Clarín’s (Leopoldo Alas y Ureña) Vetusta - a reimagination of the Asturian capital, Oviedo - in what many scholars consider the most important Spanish novel of the 19th century, La Regenta. By focusing our work on a defined time period and culture it provides us data to not only analyze how fictional space is described and mapped, but also how it can be culturally determined. Often the inspiration for fictional maps comes from literature and their creation is not necessarily left solely to cartographers. Bottecelli’s depiction of Dante’s Inferno portraying the various stages of hell is one of the more iconic examples of a fictional map of popular literature, depicting the various stages of hell as described by Dante. The image, however, while rooted in the words of Dante, also serves to reinforce the modernity of Renaissance society through its geometry of order and symmetry, a trend in many works of the Renaissance. By depicting the chaotic world of hell as seemingly rational and ordered, it allowed Renaissance society to interpret Dante as a corollary to their contemporary world, or at least their ideal vision of it (Padron). The maps that accompany J.R. Tolkien’s Lord of the Rings novels are a much more recent example of this phenomenon in which maps of literature are utilized to bridge the gap between fantasy and real world ideals. Tolkien uses his maps to instill the idea of tranquility being found in the eastern world (the Shire) and evil situated in the western world (Mordor), a direct correlation to current thought in the wake of World War I (Croft). The relationship between the fictional world of literature and a “real space” provides a powerful tool that allows readers of these works to interpret the creator’s words in a way which attempts to link them to the real world and prevailing ideologies (Piatti).  This line between the space and place of the real world and that of the imaginary world becomes increasingly blurred in the last half century (Joliveau). The works of Henri Lefebvre and Ed Soja contribute to the dissolution of concrete boundaries between the materiality of the physical space and the abstraction of the mental space. They theorize that the influence of ideas, signs, and texts on spaces is just as influential as the materiality of a space (Lefebvre, Soja) This grants imaginary spaces another sphere of influence upon the real world and begs for further examination into how readers interpret literary spaces and what impact they may have on their understanding of real spaces. Fortunately, new digital tools in mapping and geospatial analysis allow for a more thorough and in-depth evaluation of these fictional spaces of literature. They provide the opportunity for the individual reader to craft the imaginary space in their own image instead of defaulting to the omniscient representation provided by the cartographer, artist, or scholar. Digital tools are becoming increasingly more intuitive and their interfaces and operability are more user friendly. This contributes to their use by an ever-growing audience and allows for more rapid production with less years of rigorous training. Innovations in digital tools also provide various ways in which spaces can be mapped. GIS provides a solution to the problematic of shifts in places over time. A particular location or space is not static and spaces evolve over time. These mutations could be caused by natural or man-made disasters--such as an earthquake or tsunami in the former and war in the latter--or simply by natural evolution over time. In literature, characters often move between space and place, at times in a non-linear fashion. When attempting to visualize changes to fictional landscapes over time, digital mapping technologies resolve many complexities by allowing the reader to create thick maps that look at time and space through layered pieces.  GIS-based tools allow users to map spaces in more conventional methods, while other applications that integrate technologies such as Google Earth provide users with a platform to map not only spaces, but also plot journeys that characters take through these spaces. Other technologies take this one step further and grant the ability to create more immersive 3D environments that attempt to generate a viewpoint similar to what the literary character actually experiences. Furthermore procedural modeling software such as ESRI CityEngine allow for rapid and adaptable modeling of large spaces with written rules, a skill that was previously reserved for the domain of those skilled in modeling software, which is labor intensive. Couple this with new software capable of creating large and expansive terrains, such as VUE, and a user has all of the necessary tools to create expansive imaginary spaces unique to their interpretation of a reading. Finally one may take this even further by incorporating these individually designed environments into amateur gaming software, such as Unity, and share them with other users from around the globe to take them on an immersive tour of the space. This new way of developing imaginary spaces provides critical insight into literary works and opens new avenues of study, which were previously unavailable. In addition to examining how GIS tools enhance the reader’s experience with literary texts, we will discuss the importance of combining cartography with literature in the classroom. The combination of digital mapping with literary studies allows us to maintain some of the fundamental pedagogical principles of the humanities, such as close reading and attention to detail. The interpretation of literary texts to create digital maps that represent imaginary or fictional locations, often rooted in real spaces, results in the translation of a text into a new medium. Preparing a digital map of a fictional space requires students to critically examine a narrative in order to recreate a visual representation (via a map) of the space described. Working with GIS platforms shows students how to interact with literary texts in a new way, while teaching them digital mapping techniques and new skills in computer-mediated learning. George Siemens’ learning theory entitled “connectivism” helps underline the importance of combining humanistic study with digital technologies. The connectivist’s theory incorporates the ways we ingest information and learn in the digital age, where as behaviorism, cognitivism, and constructivism (“three broad learning theories most often utilized in the creation of instructional environments”) ignore the technical advances that now greatly impact the way in which students learn. (Siemens).  ",
       "article_title":"A Sense of Place: Mapping Fictional Landscapes in Literary Narratives",
       "authors":[
          {
             "given":"John",
             "family":"Lynch",
             "affiliation":[
                {
                   "original_name":"University of California, Los Angeles, US",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          },
          {
             "given":"Wendy",
             "family":"Kurtz",
             "affiliation":[
                {
                   "original_name":"University of California, Los Angeles, US",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Rocchio",
             "affiliation":[
                {
                   "original_name":"University of California, Los Angeles, US",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "knowledge representation",
          "interfaces and technology",
          "geospatial analysis",
          "literary studies",
          "resource creation",
          "and discovery",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction “Humanities on the Z-axis” is an interdisciplinary research project that works across modernist studies, geospatial humanities, and desktop fabrication. Through a combination of techniques in three-dimensional (3D) fabrication, geospatial mapping, speculative computing, and pattern analysis, z-axis research expresses the geospatial narratives of modernist novels by geo-referencing them and then using that geo-data to transform base layers of maps from the modernist period. The output of the research includes warped, 3D maps of cities (e.g., Paris and Dublin) central to modernist literary production. These maps can be viewed as 3D models on a screen or as physical prototypes in hand, and they are currently being transformed using geo-data drawn from novels by Djuna Barnes, James Joyce, and Jean Rhys. Ultimately, they show how modernist authors wrote the city, and findings suggest they contradict existing research in modernist studies about how, exactly, cities are expressed in modernist novels.   Research Problems This project addresses two specific research problems that currently exist across geospatial humanities and modernist studies. First, in fields such as digital humanities, geospatial mapping techniques (Moretti 2005) and data visualizations (Bostock 2012) tend to produce isomorphic cartographies or flat representations of data (Drucker 2011), even when literature resists this type of representation. One consequence of these flat or isomorphic approaches is that they tend to ignore the importance of subjective experience to literary criticism (in particular) and the humanities (in general). Second, and related to the first point, mapping techniques in geospatial humanities research can too easily be applied across literary periods without regard to the historical, material, or formal differences between texts, especially when something like Google Maps or Google Earth is the core technology. As a result, geospatial methodologies do not always persuasively correspond with the literary period, aesthetics, and textual particulars under examination. In response to these problems, z-axis research tailors mapping practices to suit the needs of literary periods. For instance, modernist literature deliberately resists isomorphic representations of geographic space (Vidler 2000). It also frequently treats the city as a medium, which is represented through fiction. Consequently, we argue that modernism not only calls for speculative, non-isomorphic modes of geographical expression (i.e., deformed maps) but also techniques that engage geographic representation directly (i.e., by distorting a map's base layer instead of \"pinning\" data on top of it). Additionally, the z-axis methodology involves a \"text-first\" workflow wherein the specificities of the text practically dictate the mapping method and, by extension, the aesthetics of the transformed, 3D maps.   Research Questions Z-axis research currently asks the following research questions of geospatial humanities and modernist studies:  How and to what extent do geospatial approaches to modernist novels benefit from distinct methods of analysis? With what implications on existing geospatial methods in digital humanities? How do modernist authors write the modernist city? Through 3D maps, how can we compare multiple, literary versions of the same modernist city, using the same base map? How can traditions in speculative computing (Drucker and Nowviskie 2004; Drucker 2009) and deformance (Samuels and McGann 1999) be applied to geospatial analysis and the modernist novel (Nowviskie et al. 2013)? In modeling and fabrication practices, to what degree (if at all) do 3D-printed maps afford interpretations that screen-based maps do not? Where visual expression is concerned, how do we put screen and print into conversation, and to what effects on the trajectories of scholarly communication?    Literature Review Many geospatial projects in digital humanities are largely two-dimensional and rely significantly on isomorphism. In the case of modernist cities, projects such as Walking Ulysses (2012) and WatsonWalk (2012) pin events to flat base maps. While the Scholars’ Lab at the University of Virginia is working on “social and spatial maps of modernist correspondence,” projects of this sort are rare in the field. Building on these initiatives, z-axis research uses historical maps as a medium for expressing social and cultural currents in the modernist city. At the same time, many writers interested in modernity have documented the constructed character of modernist geographies. Henri Lefebvre (1974) unpacks the social production of urban space, arguing that discrete social and spatial practices are embedded in different cities. Elsewhere, Foucault’s heterotopias (1984) and Marc Augé’s (1992) non-spaces chart the social construction of space as it ruptures geographic locales, producing overlapping and contradictory spaces. Embedding the social and political nature of modern cities into their narrative, many modernist novels construct the city or treat it as a medium. In his work on cartographical rhetoric in Ulysses, Jon Hegglund argues that the very act of mapping within the text is seen as a way of knowing. Richard Zeikowitz (2005) and Deborah Parsons (2000) similarly analyze the construction of feminist cityscapes in Jean Rhys's works, while Amy Wells-Lynn (2005) reveals the way Djuna Barnes and other female modernists “construct new Parisian geographic and literary female spaces” (79). When combined, geospatial research in both digital humanities and modernist studies suggests that specific mapping approaches to literary modernism underscore the importance of socially constructed, geographic space. Here, work in speculative computing (Drucker and Nowviskie 2004; Drucker 2009) and deformation (Samuels and McGann 1999) provides precedent when blending computation with humanities inquiry.   Method and Workflow When studying modernist novels, the current workflow for z-axis research is bifurcated into two processes. The first process involves geo-referencing plain-text versions of modernist novels, and includes the following steps: 1) use VueScan software to produce high-resolution scans (600 dpi) of the text; 2) run the text through ABBYY FineReader to render it machine-readable; 3) where necessary, correct the text; 4) geolocate the narrative of the text in XML (if a character appears in a certain location while imagining or talking about another location, then the location is tagged as the place where the narrative occurs); 5) conduct a word-count to see how many words are nested in certain geographic locations; 6) record these numbers in a spreadsheet; and 7) divide the number of words per location by the total number of words in the text to produce a ratio. The second process mobilizes the geo-data and ratio from the first process to transform historical maps in 3D. It includes the following steps: 1) use a high-quality scanner to digitize an archival map; 2) use Photoshop to convert the scanned archival map into a displacement map; 3) apply the displacement map to a flat subdivided mesh using Autodesk’s Mudbox, then scale the plane along its z-axis to render details from the displacement map as changes in elevation (figures 1-3); 4) procedurally apply the bulge function for each area indicated by the data model, warping the three-dimensional map along its z-axis (figures 4, 5), with differences in warping determined by the geo-data ratio; 5) use MeshLab to determine if the Mudbox model is watertight; if it is not, then automatically correct errors if they are not glaring; and 6) print the model using a desktop 3D printer.                  Findings One of the key findings of this research is the articulation of a methodology and workflow for expressing the geospatial narratives of modernist novels through transformance and speculative computing. Additional, related findings suggest that, contrary to an abundance of modernist scholarship (e.g., Hegglund 2003), James Joyce's Ulysses does not provide an isomorphic representation of Dublin. Instead, the novel presents a biased version of the city, privileging specific geographic areas over others. What’s more, in the case of modernist novels about Paris, constructions of the city are highly contingent upon constructions of sexuality, especially when Barnes's Paris is compared with that of Rhys. That is, the sexual politics of literary Paris dramatically influence how and what parts of it are represented in texts from the 1920s and '30s. Finally, where comparisons between screen and print media are concerned,  findings suggest that the latter not only affords tactile engagements lacking in the former but also bypass many visual design problems, including tendencies to squeeze too much complex information into a single frame or window. More generally, our findings suggest that the humanities can be empowered through the material transformation of scholarly communication, including evocative objects and publications in 3D.   Acknowledgements This research has been conducted at the Electronic Textual Cultures Lab and the Maker Lab in the Humanities at the University of Victoria with support from the Modernist Versions Project (MVP) and Implementing New Knowledge Environments (INKE). The research is supported by the Social Sciences and Humanities Research Council (SSHRC).  ",
       "article_title":"Z-Axis Scholarship: Modeling How Modernists Write the City",
       "authors":[
          {
             "given":"Alex",
             "family":"Christie",
             "affiliation":null
          },
          {
             "given":"Stephen",
             "family":"Ross",
             "affiliation":[
                {
                   "original_name":"University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Jentery",
             "family":"Sayers",
             "affiliation":[
                {
                   "original_name":"University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"Katie",
             "family":"Tanigawa",
             "affiliation":[
                {
                   "original_name":"University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          },
          {
             "given":"",
             "family":"INKE-MVP Research Team",
             "affiliation":[
                {
                   "original_name":"University of Victoria",
                   "normalized_name":"University of Victoria",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04s5mat29",
                      "GRID":"grid.143640.4"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "desktop fabrication",
          "3D modeling",
          "modernism"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  John Dewey writes in Schools of To-Morrow: “Unless the mass of workers are to be blind cogs and pinions in the apparatus they employ, they must have some understanding of the physical and social facts behind and ahead of the material and appliances with which they are dealing.” This remark is not unlike the image Fritz Lang depicts at the outset of the 1927 film Metropolis: slaves to a machine becoming food for the machine. The danger in fetishizing machines is that we become subject to them. But turning away in the face of the digital will lead to much the same fate. Rather, we need to handle our technologies roughly -- to think critically about our tools, how we use them, and who has access to them.  Like Digital Humanities, Digital pedagogy has been variously defined. Brian Croxall and Adeline Koh offered a very inclusive, broad-stroke definition at their MLA Digital Pedagogy Unconference, saying that “digital pedagogy is the use of electronic elements to enhance or to change the experience of education.” And Katherine D. Harris offered up the components of her digital pedagogy -- which she borrows in part from the “mainstays of Digital Humanities” -- during a NITLE seminar on the subject: “collaboration, playfulness/tinkering, focus on process, building (very broadly defined).”  Digital pedagogy is an orientation toward pedagogy that is not necessarily predicated on the use of digital tools. This is why I like Harris’s focus on process and Croxall and Koh’s use of the seemingly vague, but in fact quite lovely, phrase “electronic elements.” The phrase dissects the notion of an educational technology, turning the discussion to a consideration of the smallest possible element that might influence teaching and learning: the electrical impulse. At this level, we’re not talking about how we might use Wordpress in a composition class, or how Smart Boards failed to revolutionize K-12 education, but about how the most basic architecture of our interactions with and through machines can inspire new (digital or analog) pedagogies. Thus, Kathi Inman Berens says paradoxically that “the new learning is ancient.”   Many have argued that the digital humanities is about building stuff and sharing stuff -- that the digital humanities reframes the work we do in the humanities as less consumptive and more curatorial, less solitary and more collaborative. I would argue, though, that the humanities have always been intensely interactive, an engaged dance between the text on a page and the ideas in our brains. The humanities have also always been intensely social, a vibrant ecosystem of shared, reworked, and retold stories. The margins of books as a vast network of playgrounds.  The digital brings different playgrounds and new kinds of interaction, and we must incessantly ask questions of it, disturbing the edge upon which we find ourselves so precariously perched.And what the digital asks of us is that every assumption we have be turned on its head. The digital humanities asks us to pervert our reading practices -- to read backwards, as well as forwards, to stubbornly not read, and to rethink how we approach learning in the digital age.  In fact, the course itself is one of our central texts, a collection of stories about reading and writing, that can be actively hacked and remixed. Sean Michael Morris writes, “A course today is an act of composition,” an active present participle and not a static container. This is more and more true of courses that live online, which demand that we carefully examine the digital as a frame, while recognizing that the digital does not supersede and can never unseat the work we do in the world. Kathi Inman Berens writes, “It doesn’t matter to me if my classroom is a little rectangle in a building or a little rectangle above my keyboard. Doors are rectangles; rectangles are portals. We walk through.” This is where learning happens, at the breaking point of its various containers.  This is true just as well of the literary texts we analyze (and ask students to analyze) with digital tools. In the syllabus for a recent undergraduate seminar in the digital humanities, I pose the following questions:   How is literature and our reading of it being changed by computers? What influence does the container for a text have on its content? To what degree does immersion in a text depend upon the physicality of its interface? How are evolving technologies (like the iPad) helping to enliven (or disengage us from) the materiality of literary texts?   Literature, film, and other media are changing, and the way we interact with them is also changing. As we imagine a digital approach to the humanities, we must look back even as we look forward, considering what media has become while we simultaneously examine the hows and whys of its becoming. We used to watch films only in a darkened theater without the distraction of other external physical stimuli. Increasingly, though, we watch film on hand-held digital devices, many with touch screens that allow more and more interaction with the content. Our apparatuses for media-consumption juxtapose digital media, literature, and film: Now, we watch Ridley Scott’s Alien in a window alongside Twitter and Facebook. Film no longer exists as a medium distinct from these other media.  The same is true of new modes of reading. Digital texts invite (or allow) us to do other things with our eyes, brains, and bodies as we experience them. As I write this, I have 9 windows open on my computer, each vying for my attention. Some of these windows have several frames in further competition. Advertisements. E-mail. Documents. Widgets. Social-networking tools. Chat interfaces. Each of these layers has an effect on how I engage the digital text. In spite of all these layers, I don’t think we experience a decreased attention; rather, the digital text demands a different sort of attention. Even as my direct engagement is challenged, my brain is offered more fuel for making connections and associative leaps. A proactive approach to online and digital pedagogy asks us to put these associative leaps to  work. So, Twitter and FaceBook may be a distraction, but that distraction can be harnessed for good pedagogy.  Social media can function as a site for democratic participation, a leveled playing field, a harbinger for another sort of attention. The keenest analysis in the digital humanities is born of distraction and revels in tangents. The holy grail of this work is not the thesis but the fissure. Breaking Stuff as an Act of Literary Criticism  The digital humanities is about breaking stuff. Especially at the undergraduate level, this is the work of the digital humanities that most needs doing. Mark Sample proposes “what is broken and twisted is also beautiful, and a bearer of knowledge. The Deformed Humanities is an origami crane -- a piece of paper contorted into an object of startling insight and beauty.” And, by the end of a class, if it’s successful, this is what becomes of the syllabus, the texts, the assignments, and us. Sample continues, “every fact is a fad and print is a prison. Instructors are insurgents and introductions are invasions.” In this way, my digital humanities courses work to violently dismantle fact and print, instructors and introductions, and I revel together (and part and parcel) with students in both discovery and uncertainty.  The digital humanities course I teach for undergraduates has as its first assignment the breaking of something as an act of literary criticism. Specifically, I ask students to take the words of a poem by Emily Dickinson, “There’s a certain slant of light,” and rearrange them into something else. They use any or all of the words that appear in the poem as many or as few times as they want. What they build takes any shape: text, image, video, a poem, a pile, sense-making or otherwise.  This paper expands upon a brief article I wrote about this assignment, analyzing several of the resulting student works and exploring the new pedagogies that the digital humanities demand and give rise to.   ",
       "article_title":"Digital Pedagogy is about Breaking Stuff",
       "authors":[
          {
             "given":"Jesse",
             "family":"Stommel",
             "affiliation":[
                {
                   "original_name":"University of Wisconsin-Madison, United States of America",
                   "normalized_name":"University of Wisconsin–Madison",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01y2jtd41",
                      "GRID":"grid.14003.36"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "Critical Analysis",
          "digital humanities - pedagogy and curriculum Keywords: Pedagogy",
          "teaching and pedagogy",
          "Breaking",
          "Making"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The English language has continually borrowed from foreign languages—close to 30% of modern English words are loanwords from French, and another 30% are borrowed from Latin. These words are often concentrated in semantic frames associated with their origin languages—legal vocabulary contains a preponderance of words of French origin, and the vocabulary of the natural sciences contains many words of Latin and Greek origin. The etymology of words in a text, therefore, may be suggestive of its context or its level of discourse. Should a writer choose the Latinate term “masticate” over the Anglo-Saxon term “chew,” for instance, one might assume a scientific context or a high level of discursive formality. By computing the proportion of origin languages for all the words of a given text, we may quantify stylistic properties that are potentially revealing about the text and its rhetorical modes.  The Macro-Etymological Analyzer is a computer program that I wrote for this purpose. Written in PHP on a LAMP stack, it is a web app accessible at http://jonreeve.com/etym, and is freely available for all to use, modify, and distribute under the GPLv3. It accepts as input a user-uploaded text file, and looks up each word in Gerard de Melo’s Etymological Wordnet database. These words are then counted by language of origin using two generations of language ancestry, and then categorized by language family. The results are displayed as a pie chart made with the Google Data Visualization API, along with a CSV log file which can be used for comparative analyses. Currently, the program accepts only English texts, but the database supports queries from any source language, and plans are in place to make the program fully multilingual.  Figure 1 shows the proportions of Latinate words—words descended from Latin or romance languages—for each of the 15 genres in the Brown Corpus. Learned texts and government documents show the highest proportions of Latinate words, whereas romance and adventure stories show the lowest. The same textual categories sorted by proportion of Hellenic words (words of ancient Greek origin) show changes in certain categories—religious language exhibits a higher rank, and that of mystery stories is ranked lower than in the Latinate scale. These data suggest that a high proportion of Hellenic words is correlated with religious language, among other genres, and that a high proportion of Latinate words is correlated with learned language. Once literary works are analyzed with this method, these hypothetical correlations become potentially useful as literary critical tools.    Fig. 1: Brown Corpus Genres  In one such analysis, the chapters of A Portrait of the Artist as a Young Man were run through the Macro-Etymological Analyzer. This novel, James Joyce’s Bildungsroman, is known for its style—one that mimics each progressive age of its protagonist Stephen Dedalus. Early chapters, when he is young, are written with infantile language; later chapters are written with more elevated language. The program’s results quantify this stylistic mode, to some degree—Chapters 1 and 2 show low proportions of Latinate words, whereas later chapters show higher proportions, as shown here in Figure 2. The fact that the proportion of Latinate words begins to plateau starting with Chapter 3 might be used to argue that Stephen has at this young age already reached a precocious maturity of vocabulary, which may reflect his study of Latin.   Fig. 2: A Portrait of the Artist as a Young Man  In another analysis, the extracted monologues of the seven narrators of Virginia Woolf’s novel The Waves were computed with this program. As shown in Figure 3, the two university-educated characters, Bernard and Neville, show the highest proportions of Latinate words, while the housewife Susan shows the lowest. In fact, the male characters rank higher for Latinate words than the female characters—this would be an interesting starting-point for a discussion of gender in The Waves, especially framed by Woolf’s much-discussed writings on gender politics.   Fig. 3: The Waves Narrators  The Macro-Etymological Analyzer was also used to chart variations between editions of a text. The seven revisions of Whitman’s Leaves of Grass made available by the Whitman Archive were analyzed with this program. The results show a gradual increase in Latinate words from the 1855 edition to that of 1891-2. This might be used to argue that Whitman inflated his style with each revision, introduced foreign loanwords as he gained a more international reputation, or used a greater breadth of words as his vocabulary increased.  These experiments were not without their surprises, of course. An early test of selected books of the King James Bible seemed promising, as it revealed the gospels Matthew, Mark, Luke, and John to have much higher proportions of Hellenic words than other books (see Figure 4). Unlike the books of the Old Testament, which were mostly written in Hebrew, these books were translated from the Greek—a fact which might seem to explain the presence of Hellenic words. Upon closer examination, however, the program was discovered to be counting the etymology of frequently-mentioned names like “Jesus” among words of Hellenic origin, and it was these names that accounted for most of the Hellenic words. Although the language of the source text did not prove to be the determinant here, this discovery may yet be valuable for other reasons—the synoptic gospels of Matthew, Mark, and Luke show similar portions of Hellenic words, whereas that of John is 100% greater. This would seem to support the hypothesis that the synoptic gospels were adapted from a common source text, whereas that of John had an independent source.   Fig. 4: KJV Bible  A number of other experiments were also conducted, and are described in this paper. Included among texts analyzed by the Macro-Etymological Analyzer were: selected Canterbury tales (in modern English translation), a series of early    and late Henry James novels, a collection of Victorian novels compared with a collection of modernist novels, and groups of French and German novels in English translation. Questions to be explored include:  Do translated works show a larger-than-normal proportion of words with etymological origins in the language of the source text?  Given a large enough data set, can linguistic trends (such as a general decrease in the use of Latinate words) be detected with this program? Can macro-historical events such as the Scientific Revolution be detected?  Do male and female writers of the 19th century differ in the origin-types of words they use?  Can the semantic frames in which these etymological groups of words are concentrated be explained historically, such as through the habits of the French-speaking English aristocracy in the era following the Norman Conquest?   Finally, this paper will discuss how this new tool might contribute to the suite of computational stylistics tools already available, and how macro-etymology might constitute a new metric that could be used towards stylistic fingerprinting or authorial detection.   ",
       "article_title":"Macro-Etymological Textual Analysis",
       "authors":[
          {
             "given":"Jonathan Pearce",
             "family":"Reeve",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America ",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "natural language processing",
          "literary studies",
          "linguistics",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Recently, the collaboration between the Language Technology community and the specialists in various areas of the Humanities has become more efficient and fruitful due to the common aim of exploring and preserving cultural heritage data. It is worth mentioning the efforts made during the digitisation campaigns in the last years and within a series of initiatives in the Digital Humanities, especially in making Old Manuscripts available in the form of Digital Libraries. Having in mind the number of contemporary languages and their historical variants, it is practically impossible to develop brand new language resources and tools for processing older texts. Therefore, the real challenge is to adapt existing language resources and tools, as well as to provide (where necessary) training material in the form of corpora or lexicons for a certain period of time in history.   Another issue regarding historical documents is their usage after they are stored in digital libraries. Historical documents are not only browsed but together with adequate tools they may serve as basis for reinterpretation of historical facts, discovery of new connections, causal relations between events etc. In order to be able to make such analysis, historical documents should be linked among themselves and should be linked with modern knowledge bases. Activities in the area of Linked Open Data (LOD) play a major role in this respect Most digital libraries are made available not only to researchers in a certain Humanities domain (e.g. classical philologists, historians, historical linguists), but also to common users. This fact has posited new requirements to the functionalities offered by the Digital Libraries, and thus imposed the usage of methods from Language Technology for content analysis and content presentation in a form understandable to the end user.  There are several challenges related to the above mentioned issues:  Lack of adequate training material for real-size applications: although the Digital Libraries usually cover a large number of documents, it is difficult to collect a statistically significant corpus for a period of time in which the language remained unchanged. Historical variants of languages lack firmly established syntactic or morphological structures thus the definition of a robust set of rules is very difficult.  Historical texts often constitute a mixture of multilingual paragraphs including Latin, Ancient Greek, Slavonic, etc. Historical texts contain a large number of anon-standardized abbreviations. The conception of the world is somewhat different from ours, which makes it more difficult to build the necessary knowledge bases.   Whilst these issues are generally accepted there is still less research done towards the content annotation of old texts. Language technology tools are mostly adapted in order to make corpus linguistics research (Piotrowski 2012), (Vertan et. al 2012) but not really used as means for text processing. One of the main barriers is the text readability in terms of language style and terms.  The aim of this paper, describing on-going work is to demonstrate that multilingual aspects in historical texts are a big challenge but can serve also for building a knowledge-based to be used in text presentation. 2. Selected materials The selected texts are works of Dimitire Cantemir, prince of Moldavia at the end of the XVII century, but also historian, philosopher, composer, musicologist, linguist and much other. As member of the Prussian Academy of Sciences he was asked to write a history of the Ottoman Empire as well as a history of Moldavia, both unknown territories for Western Europe at this time. These remarkable works remained until the middle of XIXth century the only generally accepted reference. Even if some historical aspects are interpreted in a subjective manner, the works of Cantemir represent a unique testimony of that time. He is not describing only dates and places of historical events but presents daily life, occupations, country organisation as he saw himself as prisoner in Istanbul and prince of Moldavia. The „History of growth and decay of the Ottoman Empire“ and the §Description of Moldavia“ were written initially in Latin. Later they were translated in German, and then the German Edition was translated in English, French, Romanian and Russian. All these Editions are no tone-to-one translation but in many cases they are influenced by the perception of the translator. We consider that the works of Cantemir are of particular importance fort he history of Easter-Europe, about which with exception of specialists, is less known. The current project aims at the presentation and explanation of these works, by means of language technology tools. For the moment we concentrate on the German, English and Romanian Editions of the „Description of Moldavia“, all available in the Library of our institution and intend to extend the work to the other language editions. 3. Exploiting Multilinguality Usually the multilingual problem of the old texts is resumed to the fact that Latin and sometimes ancient Greek passages are found. The works of Cantemir have the particularity that they introduce words in the language of the described country, namely Romanian and Ottoman Turkish. Therefore a processing step is needed in order to identify these words. State-of –the art methods in language identifications based on n-grams cannot be used as both old Romanian were written with other alphabets (church Slavonic) and the transliteration rules were not standardised. Our method uses the multilingual versions of his works help here in identifying the foreign words. Explanation paragraphs differ slowly but the terms are preserved. Therefore we use comparison at sentence level in order to identify common words, which we mark afterwards as Named Entities. Following example form the „Descriptio of Moldavia“ in German is illustrative:  „Der Watawul de Aprodschi de Tyrg ist Herr über die Gerichtsdiener, welche den Tribut und andere Abgaben der Bürger eintreiben, und andei Schazkammer liefern. „  Here we observe the following: the expression „Watawul de Aproschi de Tyrg“ is for a language technology tool dealing with German completely noisy. On the other hand the expression is an approximate transliteration of old Romanian written in Church Slavonic. Nowadays the correct transliteration would be „vatavul de aprozi de târg “. In comparison here the entry in the R Romanian version  „Vatavul de aprozi de târg; este asupra slujitorilor divanului, cari strâng birul si alte dări ale orăsenilor si le aduc la Visterie;  Thus the algorithm for the processing of the text involve the following steps:  1. Sentence splitting of German and English and Romanian texts  2. Sentence alignment by means of comparable corpora sentence extraction (Smith et. al. 2010) 3. Identification of common identical strings 4. Extraction of common passages and normalisation using resources in old Romanian /ottoman Turkish as well as heuristic rules 5. Marking of identified expressions as named entities in order to be blocked from further processing.  We use the output of this process in order to built a multilingual knowledge base, and match the terms on wikipedia entries. 4. Conclusions and further work Through this contribution we want to raise the attention on less studied aspects of multilinguality in historical texts and show how methods from multilingual language technology, here exploitation of comparable corpora, can be used for dealing with this issue. In the presentation we would describe in detail the algorithms used for the construction of the multilingual knowledge base and give relevant examples. As mentioned before, this is an on-going project. After creating the knowledge base we intend to built a presentation interface where entries in the knowledge base may-be available at the moment of text reading through a mouse-over function. We intend also to asses how much from the found entries match Wikipedia, and show that in fact the information is complementary. We intend also to extend the methods to other works.  ",
       "article_title":"Less explored multilingual issues in the automatic processing of historical texts – a case study",
       "authors":[
          {
             "given":"Cristina",
             "family":"Vertan",
             "affiliation":[
                {
                   "original_name":"University of Hamburg, Germany",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "historical studies",
          "content analysis",
          "multilingual / multicultural approaches",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Framework of thought and specific purposes  The processes of assigning value to cultural objects, as well as the establishment of the canons which derive from those processes, have constituted until today one of the intellectual, ideological and political foundations of the development of Art History discipline as an institutional discourse (Halbertsma, 2007). This explains why the critical dismantling of the concept of canon as a structure of power, criterion of authority and legitimizing argument represented a significant line of inquiry for the post-structuralism and, more recently, for the postcolonial theory (Parker and Pollock, 1981; Bloom, 1994; Perry and Cunningham, 1999; Gorak, 2001; Bart, 2005; among others). Especially, it has been emphasized the need to bring out a critical awareness of the multiplicity and heterogeneity that define the processes of assigning  value and meaning to objects on the basis of the variety of cultures, genders, races and territories.  It has been stated that, in our global world it is essential to understand the concepts of canon and value in terms of plurality and difference. It has also become necessary to explore the specific idiosyncrasies of those processes as a mean to make recognizable and significant that diversity.   Within this framework of thought, the so-called ‘digital turn’ offers to us another scenario of critical analysis to rethink these issues from the perspective of the new conditions of the digital society, which is modeled by the prevalence of the software, and it is characterized by the potentiality of interactivity, user-generated content, and – at least in theory - global access to and massive distribution of cultural images and objects.  This is the intellectual background of my proposal.  As a response to the theme of Digital Humanities 2014 (Digital Cultural Empowerment), I propose to explore how the digital turn, which has brought a new model of society, economy, culture, and a new epistemology (i.e. new ways of production, narration, distribution and consumption of knowledge), is leading to a redefinition of the processes of assigning values -and the values themselves- that have hitherto prevailed in the comprehension of cultural objects within the Art History discipline, resulting in new forms of canonization.  My approach is inspired to the current Digital Humanitie’s thought which proposes to rethink the circumstances and the consequences of this 'new' disciplinary field from the perspective of the cultural critique (Lothian and Phillips, 2013; Dacos, 2013; Galina, 2013; Fiormonte, 2012; Liu, 2012; McPherson, 2012; Higgin, 2010, among others). The field of Digital Humanities is becoming aware that there is a real risk of perpetuating in the digital world and in the practice of digital scholarship the same problems of marginality and subalternity that characterized our pre-digital world. In the field of Art History this trend is represented by the super-imposition of specific canons for the understanding and explanation of artistic phenomena. A critical approach to Digital Humanities requires a review of both established and new structures of power that are emerging.However, although the field of artistic culture is one of the most affected by these new processes, the critical discourse is still in its embryonic stage within the context of Digital Art History studies. In my opinion, there is an urgent need to conduct a thorough analysis from the perspective of critical theory. My scope would be to develop such a perspective, unveiling and questioning what kind of art-historical discourses and narratives, and what kind of digital artistic culture we are building on the web (Rodríguez Ortega, 2013). Now then, we must bear in mind that the building of the digital artistic culture, and the growth of the emerging Digital Art History itself are defined by a dialectical tension between the new processes of assigning value and the maintenance of those traditional structures that had characterized the development of Art History discipline during the Twentieth century [1] (Baca, Helmreich and Rodríguez Ortega, 2013; Kohle, 2013). Examining this tension is a complex task, since these practices and criteria are simultaneously interlaced and in confrontation.  Any inquiry must be based then on a dual question: a) we must scrutiny what is really changing in the digital medium in regard to the processes of assigning value to cultural objects, and to what extent these new processes are entailing a destabilization of the traditional criteria of Art History’s institutional discourses; in short, the aim is to explore to what extent the Art History discipline and its allied institutions (Museum, Art Criticism, Market, etc.) are being put in crisis as argument of authority; b) perhaps more importantly, we must be aware that, while these changes –sometimes very visible- are taking place, the logic that governs the processes of assigning value based on institutional policies and established power structures is maintained, as well as it is preserved the canons  that characterized the critical and conceptual definition of artistic objects and images during the twentieth century –essentially, Western, white and male.  2.  Defining hyper-canonization and de-canonization processes  For this presentation, I will focus on two of theses processes, which are related to the conceptualization of the social web as the new laboratory of cultural production. In this scenario, new actors, hitherto completely unrelated to the traditional ecosystem of Art History (Academy - University, Museums, Critique, Market), arise and perform, fostering a paradoxical redefinition –paradoxical due to its ambivalence- of the traditional concepts of canon and value.    Firstly, I would like to address the process that I propose to call ‘hyper-canonization’ since this type of process superimposes and at the same time encompass the traditional ones. Therefore, as indicated above, a regime based on institutionalism and authorial power structures remains. The challenge lies, then, in determining which are such arising power structures and who has the ability to control them.   In part we can associate this process to the rise of software oligopolies and social networks companies (Google, Facebook, Twitter, Apple, Microsoft, etc.) [2], which belong to the same Western and Anglophone economic-cultural context. They control the technological infrastructures, the algorithms for data processing and retrieving, the channels for content distribution and the social interactions platforms that are used by cultural institutions to interrelate with their audiences (see, for example, the massive presence of museums in social networks as inexcusable part of their communication policies and activities). This indisputable technological and economic supremacy can lead us to new forms of digital colonialism and new cultural monopolies. Some of them are obvious. From my point of view, one of the clearest cases is represented by the Google Art Project, whose declared objective is to become the global gate for accessing the entire collections of museums worldwide. Nevertheless, the philanthropic mission of providing a comprehensive and free access to the objects of world culture underlies the threat that the museum identity can get lost on the web. Each museum, as a differentiated institution, is defined by certain discursive strategies, intellectual positions and critical criteria. However, these signs of identity could dissolve if the collections would be seen preferably ‘through’ Google. Not surprisingly, it is frequent to find that museums’ websites use Google Art Project among their recommended and authorized information sources. Consequently, museums themselves are participating in this process of legitimating Google – the Google Cultural Institute - as a new institutional discourse.  Others are less obvious, but equally disturbing. For example, despite all digital archives and online catalogs developed by public and private institutions, the largest digital images archive and the most accessible is – let admit it - Google Images. Google Images establishes a hierarchy of the images retrieved based on computational procedures that run according to algorithms completely unrelated to the epistemological, aesthetical, historical and/or symbolic specificities of artistic artifacts. Thus, the software, whose conceptualization has nothing to do with these specific aspects, assumes the power of the decision making when ‘ordering’ the images of our cultural heritage.  ‘De-canonization’ is the name that I propose for the second process that I want to address in this presentation. This process emerges directly from the social and distributed users’ interactions with the cultural images and objects on the web. Under my perspective, what is in crisis here is the concept of ‘canon’ itself, because of its bottom-up orientation which dissolves the idea of canon understood as the institutionalization of specific values representing the ideas and interests of those that hold a sort of privileged position of authority (intellectual, economic, political, etc.) This process is linked to the unprecedented empowerment of social communities to interact with and give new meanings to cultural artifacts through their multiple, heterogeneous, and distributed digital activity.  It is thus set up a new scenario that unfolds outside the institutional frame, and whose processes of assigning value are governed by very different criteria [3]. Hence, social memory, subjectivity, emotionality, etc. become fundamental factors for the re-semanticization of cultural objects and for their relocation in new scales of value. This new context involves a disruption of the principle of authority in the Art History discipline and its allied institutions, which comes into confrontation with these actions in a double way: or ignoring them, or appropriating them.  In fact, the appropriation of the logics of participation and sharing that characterize the web 2.0 is the basis of the so-called 'social museum' (Simon, 2010). Nevertheless, these actions bring about another problematic issue on which we need to reflect critically.  Certainly, the valuable social knowledge found in the users’ interactions have already been recognized by projects that advocate for a hybrid knowledge (expert plus non-expert), which may result in a new process of assigning value and in a new canonization model. See for example, Your Painting (http:/www.bbc.co.uk/yourpaintings), a project based on the social tagging of British paintings (Baca, 2013), or the History Harvest (http://historyharvest.unl.edu), an open digital archive of historical artifacts collected by various communities through the United States, which are systematized and prepared for research and interpretation by a group of scholars.  While recognizing the positive aspects of these initiatives, some questions arise   To what extent the institutions are appropriating these logics of participation and sharing in order to subsuming them as part of their institutional discourses and canons? To what extent are we facing a phenomenon of ‘domestication’ and an attempt to attract the outsiders to the 'center', establishing a sort of 'controlled' framework for their activities, such as perturbing sometimes for the institutions? 3. Open questions: What are facing?   I will conclude with a set of open questions that underlie this approach and that should be discussed in depth in following studies: To what extent the Art History discipline is possible outside an institutional framework? Is that condition an argument to explain the need for operating an institutionalization of the digital environment, which is, by nature, open, distributed, and multiple? To what extent the discipline of Art History and its allied institutions are willing to share their position of authority, at least consciously? And to what extent they are aware that they are yielding this position to new structures of power?Recently, James Cuno (President and CEO of the Getty Trust) wondered from a postcolonial perspective: Who owns the past? (Cuno, 2013). Now, I think, it is the time to ask: Who owns the value and the canon in the digital realm? Who has the ability to assign value to cultural objects and images?  Who holds now the authority and power to establish the new canons and legitimizing discourses in the context of digital society?     Notes   [1] We should not forget that the dialectic tensions and contradictions have been defining factors in the development of the Art History discipline since its early beginnings (Donald Preziosi. Rethinking Art History. Yale University Press, 1989). Therefore, the challenge now is to examine which are the new factors that participate in this process.  [2] Regarding the new inclusion-exclusion regimes associated to the software oligopolies, see  Juan Martín Prada. Prácticas artísticas e Internet en la época de las redes sociales, Madrid: Akal, 2012.  [3] As examples, see the following projects: www.Bdebarna.net; or www.cabanyalarchivovivo.es/index.html. Both initiatives are based on the appropriation by social communities, belonging to a specific territory, of the cultural heritage related to such territory, using for that digital infrastructures and strategies. The objective is to give them –both cultural heritage and territory- new meaning and value, and rethinking them from the point of view of the social memory and collective interests.   ",
       "article_title":"Canon, value and artistic culture: critical inquiry about the new processes of assigning value in the digital realm",
       "authors":[
          {
             "given":"Nuria",
             "family":"Rodríguez-Ortega",
             "affiliation":[
                {
                   "original_name":"University of Málaga, ES",
                   "normalized_name":"University of Malaga",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/036b2ww28",
                      "GRID":"grid.10215.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction How to figure out the formation process of concepts has long been a significant yet elusive problem in Humanities studies. In order to better understand this problem, we have drawn on insights from History of Ideas (Lovejoy, 1936; Pocock, 1898; Skinner, 2002; Jin et al., 2008), Conceptual History (Koselleck, 2002), Key word studies (Williams, 1983) and computational linguistics (Wittgenstein, 1953; Austin, 1962; Deignan, 2005). More importantly, we have also employed data analytics (Liu et al., 2011), Zipf's law, and statistical methods (Jin et al., 2012). As a result, in terms of methodology, we have come up with a new approach of Chinese historical studies, which explores temporal analysis of keywords and their collocations and promises to outline the trajectory of conceptual formation more accurately. By means of this new approach, we have already investigated closely the formation processes of three concepts (“ism”, “Chinese”, “Chinese labor”) in modern China with good results (Chan et al., 2011; Jin et al., 2012). These studies demonstrate the potential power of DH methods for the study of ideas. Encouraged by the fruits of previous studies, we now try to look into the formation process of three important keywords in the construction of modern Chinese national identity, i.e., “guojia (nation-state)”, “sovereignty”, and “tongbau (siblings).” By utilizing DH methods, we aim to analyze the processes, structures and patterns of formation of concepts in critical phases, and to portray the dynamic schema of the interactions between ideas, events and actions. In the future, we will apply more computational linguistics methods in our study of concept formation process, in order to understand more clearly the birth and evolution of key ideas.   2. Body Paragraphs Given the fact that during the modernization process of China, the formation of concepts concerning national identity such as “nation-state”, “sovereignty”, and “ siblings” is a very important cultural issue; we need to have new approaches and perspectives to study it, especially in the face of the new age of big digitalized corpora. By handling the huge historical corpora with DH methods, we are able to better grasp the interactive processes and patterns between concept formation, important social events and actions of different groups in modern China. To investigate the concept formation of “nation-state” in modern China, we choose “Xinmin Congbao” and “Xin Qingnian” as two major sources for analysis. The former was published in the final years of late imperial China, while the later in the early Republican period. By identifying frequent keywords with the PAT-Tree method and computing statistics of these keywords and their collocations, we analyzed the differences between the two sources in terms of their interpretations of the “nation-state” concept. In addition, we explored the complicated relationship between the changing definitions of “nation-state” and important social events and actions. As a result, we have found out that, before 1911 while the formation of “nation-state” concept was inchoate, this concept was mainly embedded in the concepts of citizen and individual, and became popular mainly as a result of civic education. However, after 1911 when “Xin Qingnian” was published, the “nation-state” concept has become very widespread. In particular, along with the breakout of the World War I, under the “party-state” system in modern China, the “nation-state” concept became highly ideologized. Textually speaking, it frequently appeared with terms such as “class” and “capital” in various discourses, relating closely with class revolution and economic revolution as well. On the other hand, by mapping out the linguistic development of “sovereignty” in modern China, we have outlined three stages of the concept formation of this term. During the first stage (1864-1898), the Western definition of “sovereignly” was selectively interpreted in late imperial China. While “sovereignty” was introduced as a modern idea representing the right of a nation, the Chinese emperor was still conceived as the only master of the sovereignty. In other words, the modern (Western) sense of sovereignty was partially accepted for its instrumental function for dealing with international affairs. During the second stage (1899-1915), China was trying to exercise sovereignty as a modern state. China has then transformed from a dynastic empire into a nation-state and more competitions and negotiations between China and other nation-states took place. Thus, the modern idea of sovereignty became much more popular. During the third stage (1916-1924), the sovereignty concept underwent a drastic transformation. The “party-state” system became dominant and produced a new set of moral-political ideologies, which came to define the sovereignty of a nation-state under the party-state framework and claimed it should be under control by the party-state system. As well known, the construction of modern Chinese identity is closely related to the idea of “tongbau,” which literally means that every member of the nation is blood kin, united by blood bond. We have utilized DH approaches to explore how and why this “familial” term has not only transformed people’s loyalty from families into the nation but also “naturalized” the sense of solidarity and patriotic love. Our initial investigation of the origin of the modern meaning of “tongbau” has revealed that modern Japan might be the source. We have found that the highest frequency of using the term “tongbau” in its modern meaning appeared in three famous journals (Jiangsu, Xinmin congbao, Qingyibao) published after 1898 in Japan by the exile Chinese intellectuals and students there. This finding is very significant. First of all, the timing itself deserves to be explored further. As well known, the failure of the 1898 political reform in china stimulated many intellectuals to seek popular support for political reform instead. In other words, the rise of “tongbua” discourse in modern China was closely related to the political development in late Qing. Secondly, the fact that “tongbau” discourse originally appeared in Chinese journals published in Japan also provides a piece of solid and interesting evidence to further testify the complicated relationships between modern China and Japan, especially regarding the construction of modern national identity.   3. Conclusion The birth of a key concept is related to important events and actions, and the formation of a new concept will inevitably bring out changes of values. We have utilized new DH approaches, computational linguistics, analysis of the formations of concept terms, in order to achieve the following goals: (1) outlining the formation structure of concepts, (2) probing how China transformed from a traditional dynastic empire into a modern nation-state, (3) examining the contour of the changing identity from a “subject” into a “citizen” among the Chinese people, and (4) exploring how key Western concepts were translated and reinterpreted in China in the past 150 years. The target sources for our study are mainly important journals published during the late Qing and early Republican period: “Qingyibao” was published in Japan after 1898 by a pro-emperor group. It was later on succeeded by “Xinmin Congbao.” The editor-in-chief and mastermind for both journals was the important thinker Liang Qichao. Hence, these two journals vividly manifested the changing attitudes and ideas of the pro-emperor group. “Xin Qingnian” was an important journal during the early Republican period, indicating the intellectual trend of the time. With the help of DH methods, we were able to study them in a new manner and therefore reached our research goals aforementioned. It should be mentioned that, since modern Chinese newspapers served as the platforms for enlightenment and reform, they featured both the elite and the mass. Although it is impossible now for our studies to include all variables, we will in the future try our best to use “media,” “readers,” “editors,” “political stereotypes” or “newspapers position” as our major variables for further investigation. Moreover, in order to strengthen our digital skills, we will use statistical keyword extracting analysis and co-occurrence word cluster statistical method, as well as social network analysis, citation analysis and spatial-temporal analysis. After all, the main characteristics of our studies lie in using DH methods to do Chinese text analysis. We are among a small pioneer group attempting to experiment DH approaches in Chinese studies. We are convinced that the results of our studies are very meaningful and they will provide rich resources of reference for applying DH approaches in textual analysis in other languages.   Based on what we have done so far, we will further employ theories from cognitive sciences, allegory theory, statistics methods, thinking about the possibility of new breakthroughs of DH approaches. We are confident that our new methods will shed new light on current DH studies. The unique perspectives of DH studies, which differ from the traditional historical approaches, will offer new methods and open up new problem domains, making important paradigm shift in Humanities studies.  ",
       "article_title":"Ideas, Events and Actions: The Digital Humanity Study of the Concept Formation in Modern China",
       "authors":[
          {
             "given":"Cheng",
             "family":"Wen-huei",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Yang",
             "family":"Jui-sung",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Chiu",
             "family":"Wei-Yun",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Liu",
             "family":"Chao-lin",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Jin",
             "family":"Guan-tao",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Liu",
             "family":"Qing-feng ",
             "affiliation":[
                {
                   "original_name":"National Chengchi University (Taiwan), TW",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "asian studies",
          "near eastern studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Existing historical linguistic corpora vary a great deal with respect to formats, corpus architecture, annotation types and values and preparation steps. The LAUDATIO-Repository (www.laudatio-repository.org) provides an open access environment to facilitate the management of such heterogeneous research data with an extensive, uniform and structured documentation and faceted and free-text search without limitation with respect to formats or annotations. For this purpose, we have developed a meta-model which is expressive enough to represent a large variety of corpus formats. This meta-model, described as a TEI-ODD specification with automatically generated schemas (Burnard & Rahtz 2004), is also the basis for the technical implementation in the repository.  Building and analyzing historical corpora often incorporates diplomatic transcriptions, normalizations of these transcriptions and research specific annotation layers which will be illustrated with the help of two corpora; the German Manchester Corpus (GerManC) and the RIDGES Herbology Corpus. GerManC1 (Durrell, Ensslin & Bennett 2007) contains for instance two formats (TEI XML and CoNLL) which represent different kinds of annotations and analyses. The TEI XML format contains a diplomatic transcription and a register specific mark-up. By contrast, the CoNLL format contains token annotations for normalization, part- of-speech (POS) and lemmatization (e.g. the STTS tag set, Schiller et al. 1999) as well as morphology and dependency annotation for syntactic relations between the tokens (e.g. Foth 2006). Thus, this corpus uses two formats for encoding different kinds of annotations and analyses.2 On the other hand, the second version of the RIDGES Herbology Corpus3 contains all annotations in one format (EXMARaLDA, Schmidt & Wörner 2009) which is then converted into the relANNIS format used by the ANNIS corpus system (Zeldes et al. 2009) for search and visualization capacities. The corpus architecture of RIDGES is specific in the following way: Via multiple segmentations, annotations can refer to different basic textual data in the corpus (Krause et al. 2012). To normalize separate spellings of complex verbs in historical German such as zusammen gesetzet to zusammengesetzt (RIDGES, Curioser Botanicus oder sonderbares Kräuterbuch, 1675), the tokens need to be merged in the normalized annotation whereas tokens need to be separated when normalizing zuverstehen to zu verstehen (RIDGES, Alchemistische Praktik 1603). Every further annotation — for instance the POS annotation may either refer to the diplomatic segmentation layer or to the normalized segmentation layer.  Having identified what exactly needs to be described by a meta-model, we then define the actual use-cases associated with this meta-model. With respect to range, specificity and user scenarios, distinct requirements could only be designed for concrete applications. For this study, the LAUDATIO-Repository (Krause et al. 2013) is taken as an example. In this case, the meta-model will enable a retrieval of, a structured search on and a holistic and extensive documentation of the heterogeneous historical corpora and their preparation (for further details see Odebrecht & Krause 2013 and Odebrecht & Zipser 2013). It should be possible to search for a distinct annotation type or content in several different corpora within the repository. Along with the content requirements the repository needs a structured, machine readable metadata format which can be represented in a graphical interface for the display of information and in the repository system for the different ways to search through the data, e.g. faceted search and free-text search. The meta-model developed from these requirements results in a metadata TEI XML format for the LAUDATIO-Repository but is also designed for and may be applied to other use cases and applications.  The meta-model is designed as an analytic class diagram for which the Unified Modeling Language is used4. Such a diagram is useful to document the important issues or concepts in an abstract way. Therefore, the class concepts represent the concepts for the subject-specific application domain ‘historical corpora’.5 Four main classes are defined: ‘corpus’, ‘document’, ‘annotationKey’ and ‘annotationValue’ which refer not only to historical corpora but to textual corpora in general:    Fig. 1: Meta-model of a corpus. For the sake of concision, the attributes of the classes are left out.   As shown in figure 1, the meta-model6 defines a corpus as the sum of all documents regardless of their structure and size. A document is defined as the sum of all annotations regardless of their structure, format and content. ‘Annotation’ is defined by the sum of all annotation keys and values. For the meta-model, it does not matter whether they have flat, hierarchical or semantic relations. Every concept carries its own attributes. A ‘corpus’ is a conceptual collection of digitized and not only linguistically processed (here historical) text. It carries among other things the attributes title, creator, creation date, revision history etc. The class ‘document’ represents the actual historical text – source text - with its own attributes such as author, date and publication history. The classes ‘annotationKey’ and ‘annotationValue’ constitute a document because the sum of transcriptions, normalizations, including segmentations and further annotations build - technically speaking - a ‘document’. ‘AnnotationKey’ in turn carries attributes similar to ‘corpus’ and ‘document’ such as date, author and revision history. For example, the attribute ‘author’ can refer either to the creator of a historical text or to the annotator of a certain annotation layer and may also refer to the same entity or person. This is important for corpus documentation. When re-using corpora, for instance further annotations on an existing corpus are made by third parties, a clear reference can be made to the copyrights. Attributes such as ‘date’ also refer to every class of the meta-model, meaning that ‘corpus’ as well as ‘annotation’ may have a date of creation like ‘document’ which genuinely has a publication date.  For the technical realization7 we used a customization of TEI XML with an ODD specification. The meta-model was mapped to three TEI header structures, one for each    concept: a header for ‘corpus’, ‘annotationKey’ and ‘annotationValue’, a header for each ‘document’ in the corpus and a header for each preparation step of the corpus in general:    Fig. 2: Technical mapping of the meta-model and the TEI xml header structure  The attributes of each class are mapped into the corresponding TEI element sets. For example, the attributes date and author correspond to the TEI elements <date>, <author> and <editor> with a specifying attribute @role for “annotator” in the <fileDesc> element. The <publicationStmt> element contains the attributes revision and/or publication history for each class. The classes ‘annotationKey’ and ‘annotationValue’ are realized with the element set of <elementSpec>. With the help of the attribute @corresp, references between the list of annotation keys and values of the whole corpus to each document and to each preparation step, including information about formats and annotation relations such as segmentations of the annotations, are technically implemented. Each TEI header is customized with the help of ODD8. The TEI headers are the technical basis for the uniform display and search of every class and its attributes in the repository. For every corpus, e.g. RIDGES and GerManC, the values for <author> referring to either a distinct annotation layer or a distinct document can be uniformly searched via a faceted search or can be displayed in the corpus view.  The meta-model presented here provides a generic mechanism for the representation of multiply annotated corpora that probably goes beyond the scope of historical corpora alone. Our experience with dealing with a variety of available historical resources has shown how flexible and reliable the model can be in this domain, though work remains to be done in dealing with more relational annotation schemes describing disconnected sources such as annotation between documents in the same or in different corpora.   ",
       "article_title":"Modeling Linguistic Research Data for a Repository for Historical Corpora",
       "authors":[
          {
             "given":"Carolin",
             "family":"Odebrecht",
             "affiliation":[
                {
                   "original_name":"Humboldt-Universität zu Berlin",
                   "normalized_name":"Humboldt University of Berlin",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01hcx6992",
                      "GRID":"grid.7468.d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "metadata",
          "licensing",
          "corpora and corpus activities",
          "copyright",
          "interdisciplinary collaboration",
          "linguistics",
          "and Open Access",
          "data modeling and architecture including hypothesis-driven modeling",
          "history of Humanities Computing/Digital Humanities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction  For several years, we have taken initiatives in view of clarifying women’s realparticipation in the European literary field before the early 20th century – as opposed to the relative absence of women in literary histories of the 20th century. We have worked in this large field through a series of successive projects funded on a national as well as a European scale. At present, the HERA project Travelling TexTs 1790-1914. The Transnational Reception of Women’s Writing at the Fringes of Europe (2013-2016) and the CLARIN-NL project Connections Between Women and Writings Within European Borders (COBWWWEB, 2013-2014) provide the context of our presentation. Roughly speaking: the HERA project is about the content, while in the CLARIN project developers are preparing a new structure, allowing the database WomenWriters to connect to other – either structured or editing – projects in the field of women’s literature: for the sake of testing, in the first instance, to the Swedish Selma Lagerlöf Archive, the Norwegian Female Robinsonades and the Serbian Knjizenstvo project; others will follow.   The intended large-scale approach is meant as a complement to ongoing individual digitizing projects, such as the Huygens ING “Digitizing Isabelle de Charrière’s letters” using the eLaborate edition project. Admittedly, “literary women” are benefitting from the new drive to digitize, but we are not yet able to feel the full benefit of these digitized texts. As Jacqueline Wernimont (2013) states, “simply saving women’s work in digital form” is not enough. The women’s texts are often presented in isolation from their historical reception context – which makes it impossible to evaluate their historical importance.   The large-scale and the individual Referring to Labrosse 1985, we propose to take this reception context as a starting point for a large-scale approach on female authorship from previous, older periods in time. It represents the other end of the communication process in which these women engaged, and it helps us to select authors and texts that should be studied in more detail. When using these reception documents as an “entry” to the texts themselves, the emphasis obviously is on what struck the contemporary reader, in a positive and negative sense: we are thus in the middle of a dialogue. Putting this data – with the appropriate metadata – in our online database (discussed and tested during an earlier COST financed phase of the collaboration) allows us to roughly situate these authors and works before analyzing them.  Giving these women their own place in the virtual representation of the literary field, where communication, circulation and transmission can be made visible, provides context and promotes understanding on a larger scale.   Linking for understanding relevance It is, indeed, not the sheer presence of these women’s texts on the Internet that advances scholarship. It is the possibility of understanding their relevance – which has often been systematically denied, without reference to empirical data by way of arguments. Understanding relevance cannot be reached by any unprepared reading of the writings. Approaching texts from a “prepared” perspective obviously requires time, but frees these texts from prejudices that are inherent in the late 19th and 20th-century literary historiography of women’s authorship and that – although denounced by Virginia Woolf in A Room of one’s own (1929) – inevitably influences even our post-feminist students. On the practical level, the connection between the structured database content, on the one hand, and the online edited texts to be studied, on the other, can be made visible by using the same terminology for (1) distinguishing database categories, and (2) annotations in the digitized text in editing platforms such as eLaborate. Linking this data is particularly essential for research in women’s literary history because of the small amount of information available, and therefore the need to compare women authors, postulating that problems encountered by one author are experienced similarly by her colleagues.    Use cases   The objective of this presentation is to highlight the importance of ICT tools used in connection and in complement with each other, for research in domains (not just the one of women’s history) which have fallen behind. We will illustrate this using two examples, representing the two ends of literary communication:    A female sender, the Dutch-Swiss author writing in French Belle van Zuylen/Isabelle de Charrière (1740-1805), and her international (male + female) reception: she has her place in the WomenWriters database, and her letters are being digitized within the eLaborate project; A male receiver, the Dutch 19th-century literary critic Conrad Busken Huet, who commented upon important numbers of (Dutch and foreign) women authors: he is present in the WomenWriters database, and his critiques are presented (without the possibility of annotation) in the Digital Library of Dutch Literature DBNL.   Briefly describing these two cases we want to illustrate the way in which this collective and transnational research in women’s literary history not only relies on combining several types of ICT tools, connecting different kinds of data (empirical data, “capta”, (references to the) primary and secondary texts), but also requires the participation of different categories of collaborators: not only professional researchers and ICT specialists but also volunteers.  This “mixed” collaboration is needed at a time when computers are still unable to communicate through the use of different languages. As this project collates women authors from all over Europe different tags and categories (denominated in English) are needed to refer to texts and aspects of texts in different languages. This can currently only be done by judgment, by readers who understand which elements of the (narrative) texts are characteristic or a-typical.  Many of the volunteers obviously will be women: the potential readers of both our research output and of the works written by “our” early women authors. They represent, typically, a category of people who will be “empowered” by the access that is given to their foremothers and potential role models….     Administrative obstacles  This complicated interconnection of collaborators may prove to be difficult. However, scholarship in the academy can no longer be fulfilled exclusively by the work of the professoriate. Technological innovations are not just a matter of devices and tools. They concern social practices; they concern users and uses. We can no longer ignore that, next to professionals (researchers, information technologists, librarians, and even students), there are “end users”, who are in fact essential members of our interdisciplinary community; they justify our very activities. It is important – given the precarious position of women in present-day society and the small number of historical role models – to “spread the word” about these early writing activities and the women behind them. This massive, complex collaboration is not, at present, recognized by institutions and stakeholders; consequently, young and senior researchers, as much as students, are often not ready to invest themselves in this kind of collaborative project. In our presentation, we will also denounce the gaps between digital technologies, the willingness of potential volunteers and the entire educational and academic system, and we will propose some possible strategies.  ",
       "article_title":"Digitizing Women’s Literary History: The Possibility Of Collaborative Empowerment?",
       "authors":[
          {
             "given":"van Dijk",
             "family":"Suzan",
             "affiliation":[
                {
                   "original_name":"Huygens ING - Royal Dutch Academy of Arts and Sciences, Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ronald",
             "family":"Dekker",
             "affiliation":[
                {
                   "original_name":"Huygens ING - Royal Dutch Academy of Arts and Sciences, Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Henriette",
             "family":"Partzsch",
             "affiliation":[
                {
                   "original_name":"St. Andrews University, UK",
                   "normalized_name":"St. Andrews University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00nb5xj61",
                      "GRID":"grid.422527.4"
                   }
                }
             ]
          },
          {
             "given":"Montserrat",
             "family":"Prats Lopez",
             "affiliation":[
                {
                   "original_name":"Vrije Universiteit Amsterdam, Netherlands",
                   "normalized_name":"VU Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/008xxew50",
                      "GRID":"grid.12380.38"
                   }
                }
             ]
          },
          {
             "given":"Amelia",
             "family":"Sanz",
             "affiliation":[
                {
                   "original_name":"Complutense University Madrid, Spain ",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          },
          {
             "given":"Gertjan",
             "family":"Filarski",
             "affiliation":[
                {
                   "original_name":"Huygens ING - Royal Dutch Academy of Arts and Sciences, Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "literary studies",
          "gender studies",
          "content analysis",
          "multilingual / multicultural approaches",
          "digital humanities - institutional support",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  A recent article by Paul Gooding, Melissa Terras, and Claire Warwick argues that a gap in our knowledge about the impact upon scholars of “large-scale digitized collections” of textual data has spawned “myths” about mass-digitization—those surrounding the distant- vs. close-reading debate, as well as dystopian arguments about digitally disrupted attention spans (Gooding et. al.; Moretti; Trumpener; Guillory; Hayles). “Where understanding lags behind innovation,” Gooding, Terras, and Warwick argue persuasively, “the rhetoric of technological determinism can fill the void” (633). Central to the myth that digital media produce “crowds of quick and sloppy readers” (632) is ignorance: the last decade has witnessed the emergence of a spate of digital editing and annotating tools as well as the emergence of what might be called “network editing.”[2] As has been amply revealed by the Australian Newspaper Digitisation Program, as well as other projects involving the crowd in correcting textual transcriptions such as Transcribe Bentham, people are as much engaged by the task of modifying digital textual archives as they are in using them (Halley, Terras), and textual “modding”[3] by networks of users requires paying close attention to texts, as might networked monasteries of monks in the process of transcribing them. The need for distributed networks of people helping to solve problems endemic to creating large textual corpora, in other words, fosters close attention to text. This fact is demonstrated by the Early Modern OCR Project (eMOP)[4]: the attempt to produce searchable text for 45 million page-images of texts published between 1473 and 1800 would fail were it not for the project’s adaptation of Cobre, a tool originally designed for closely examining various 16th Century Iberoamerican imprints, and Cobre elicits careful scholarly attention from globally distributed experts and citizen scholars wishing to take part in improving the quality and kind of information available on the Internet. The eMOP project focuses on how to digitize the archive of early modern texts, despite problems entailed. The printing process in the hand-press period (1473-1800), while systematized to a certain extent, nonetheless produced texts with fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables). Adding to these factors, the quality of the digital images of these books is very poor: ProQuest’s Early English Books Online dataset (EEBO) and Gale’s Eighteenth-Century Collections Online (ECCO) contain page images that were digitized in the 1990s from microfilm created in the 1970s and 80s. Hand-press printing as well as skewed low-quality images with no gray-scale originals creates a problem for Optical Character Recognition (OCR) software. OCR engines are notoriously bad at translating into texts digital images of early modern texts even under the best of circumstances (Gooding, Terras, and Warwick). That is trying to translate the images of these pages into archiveable, mineable texts. The Early English Books Online dataset (EEBO) consists of a collection of approximately 15 million digital page images of texts published between 1473 and 1700, and these page images are practically impenetrable to OCR engines. Moreover, metadata for such early texts is notoriously unreliable: according to David Foxon, title pages don't only lie, they sometimes joke, naming the printer typically used by a rival author as a way of implicating that author in the text's composition, or naming a bookseller in an area of London such as the theater district, for satirical purposes. Not only the binding of books, but the re-use of previously printed materials in \"new\" books makes it very difficult to know what is actually proffered by any title--what editions of other works might be included, unacknowledged in the metadata. A consortium of libraries called the Text Creation Partnership (TCP) has decided to key in, type by hand, one instance of each title in the collection, but obviously, in this context, “the same title” rarely indicates what “buried treasures” lie beneath its mark (Jackson). Moreover, it is even the case that individual witnesses of the same edition vary because of stop-press additions and corrections, changes made during the run of a single printing of one edition. Gibbs muses that “even once we have more reliable OCR technology, it would be nice to have an infrastructure to allow the manuscripts to be viewed together and improved by user expertise” and expresses hope for a transcription editor with an unobtrusive, functional, and intuitive interface, that allows text to be easily (re)configured while displaying variations between versions. Cobre (COmparative Book REader), a suite of image viewers and tools developed to facilitate detailed interaction with the collection of 16th Century New World imprints in the Proyecto losPrimeros Libros de las Américas: Impresos mexicanos y peruanos del Siglo XVI en la bibliotecas del mundo meets those needs.[5] Cobre ingests content from an OAI/PMH enabled digital repository. To populate a Cobre instance with texts for eMOP triage, we first structure the page images and their associated OCR transcriptions in the DSpace Simple Archive Format for ingestion into a DSpace repository, from which they can be imported into Cobre. Intrinsic to Cobre’s functionality is a Detailed View that not only places page images in context, via a filmstrip metaphor, but provides multiple zoom levels and the ability to drag the page in the viewer pane (Liles et al). Cobre’s Comparison View likewise uses a filmstrip view of two or more books together (Liles et al). These filmstrips can be locked, keeping them aligned when any one filmstrip is moved and when a thumbnail in the filmstrip is clicked, a side-by-side view of all the pages appears (Liles et al) in a Quick Comparative View.   Fig. 1: Cobre's Detailed View with imported OCR and pane for text correction    Fig. 2: Cobre's Comparative View of multiples exemplars    Fig. 3: Cobre's Quick Comparative View of page images and OCR output side-by-side.  Though the Cobre tool was built for the purposes of transcription and thus is technically a crowd-sourced transcription tool like the Bentham wiki and the other tools listed by Melissa Terras,[6] it resembles Ben Brumefield’s FromThePage, also mentioned by Terras, in being half transcription tool and half social editor of the sort described by Ray Siemens et. al. Like Bentham, Siemens’s own Devonshire ms. uses Wikimedia so that editing can be discussed as well as implemented. Cobre too allows for transcription, annotation, and—a key attraction for experts—editing and adding information to page images and to the metadata. We performed user studies on the tool, bringing in book history experts James Raven and Robert D. Hume to test the tool, and we videotaped their eye movements while recording their comments. There are many things that they did not find intuitive which we fixed on our last development sprint. We will be performing another set of user studies when we teach approximately 50 people to use Cobre at a pre-conference workshop at the American Society for Eighteenth-Century Studies, to be held in Williamsburg, VA, 19 March 2014. Transcribe Bentham and the ANDP have been very successful at recruiting experts and citizen scholars (Causer, et. al., and Holley). While the ANDP required a very light form of attention, transcription, and Transcribe Bentham slightly more – users were asked to encode as well – we will be asking users of Cobre to transcribe and compare transcriptions to multiple pages of various editions. Can a network of “authorized” users who will be carefully comparing pages of editions be generated of sufficient strength? Can there be networked—and so massive—close-reading? We will report on whether and how it is possible to generate distributed forms of attention that are required for careful digitization en masse. [1] Kermode’s phrase encapsulates the scholarly output over the last two centuries that have produced close readings of the meanings and forms of canonical works of art and literature [2] Siemens et. al. describe “the social edition,” a procedure that was debated at a recent SSHRC-sponsored conference called “Social, Digital, Scholarly Editing,” hosted by Peter Robinson at the University of Saskatchewan (ocs.usask.ca/conf/index.php/sdse/sdse13). Gibbs calls for a “community transcription tool [that] will reduce significantly the barrier to entry and encourage mark-up of texts,” such as: the CWRC (www.dh2012.uni-hamburg.de/conference/programme/abstracts/cwrc-writer-an-in-browser-xml-editor), FromThePage (beta.fromthepage.com) and “Textual Communities” (www.textualcommunities.usask.ca) [3] Craig Chappel’s view that game-modding is in decline may portend a trend against participation, but that view is arguable. [4] emop.tamu.edu [5] primeroslibros.org, libros.library.tamu.edu [6] melissaterras.blogspot.com/2010/03/crowdsourcing-manuscript-material.html: Scratch, Remote Writer, and the tools hosted by the Australian National Digitisation Program (ANDP) and BYU Historica Journals.  ",
       "article_title":"Distributed “Forms of Attention”:  eMOP and the Cobre Tool",
       "authors":[
          {
             "given":"Anton Raymund",
             "family":"duPlessis",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Laura",
             "family":"Mandell",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Creel",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Alexey",
             "family":"Maslov",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "digital humanities - nature and significance",
          "renaissance studies",
          "encoding - theory and practice",
          "scholarly editing",
          "text analysis",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Automatic discovery and recommendation systems are often designed with one of two audience groups in mind: in academia, the target is the dedicated researcher who actively seeks out particular sources, whereas companies like Amazon or Netflix design recommendations for the casual, passive browser, with convenience as the top priority. Often, however, a user is both browser and researcher in separate tabs; while diving into research in a scholarly database, a user can simultaneously peruse news aggregators or Amazon. For-profit companies often recommend cultural products such as books and movies, but do so with a single goal—increasing the company’s profit. As digital humanists, we should rethink the structure of recommendation algorithms to make them more appropriate for audiences interested in deeper explorations of cultural heritage. At HyperStudio, we are investigating how digital tools can encourage discovery and serendipity in the humanities, with a focus on art objects and museum collections. For this short paper session, we  propose to share our research on the process of discovery, assessing algorithms used in research and recommendation tools on both scholarship and industry platforms. We will survey existing projects that allow scholars and casual users alike to discover new art. We will also discuss a tool that we are building, tentatively titled ArtX, that empowers users to discover cultural events, exhibitions, and art objects in the Boston area. Informed by our theoretical research into cultural recommendation systems, we are prototyping and testing this tool this spring and will be sharing our results at DH2014. Recommendation systems are typically divided into two approaches: collaborative filtering and content-based filtering.1 While many digital tools use these in combination, here we outline the approaches and their limitations separately. Content-based filtering approaches, such as traditional tagging systems, look at the properties of the content rather than the user. Whether human- or machine-powered, tagging involves inferring what an object is “about” and how one might search for it, and assigning keywords of names, topics, or entities. The act of classifying culture is by its nature restrictive; when an art object is called “surrealist” or “American,” it is placed in a particular discourse and others are implicitly excluded. Even outside-the-box descriptions such as “hazy” are just different boxes. Artsy’s “Art Genome Project” offers a more nuanced approach to tagging (with gradients from 0 to 100, rather than 0 to 1), but this runs into the same problem.2 When an authoritative institution such as a museum produces tags, the tagging system lacks dynamism. User-generated tagging, or folksonomies, add a dynamic element but require that users actively and continually contribute to building up the tags, a process that is difficult to maintain. Collaborative filtering attempts to sidestep these limitations, focusing instead on the user and their online behavior, similar users, and social networks. User history-based approaches like Amazon’s maximize efficiency at the sake of variety, assuming that a user has no desire to try something new. Social curation tools such as Curiator, ArtStack, Pinterest and Tumblr allow users to build their own collections and share with others, but they perpetuate what is already popular or the most reblogged. Collaborative filtering may work when shopping for a product, but risks creating a filter bubble for art. It shepherds audiences into identical routes of understanding, stifling productive conversation and undiscovered treasures in the process. At the heart of these approaches is the notion that more personalization leads to higher quality, and that existing networks and canons should be reinforced; these are meaningful signals, but they should not be the only ones. One alternate approach is to include a serendipitous chance in the discovery process. The role of serendipity in scholarly research has been a growing topic of investigation in recent years.3 Serendipity has historically played a significant role in science, mathematics, and the humanities. As resources are increasingly digitized, an oft-cited lament is the lack of serendipity, yearning for the days when a scholar would go to the library stacks looking for one book and happen upon another that sparks his or her thinking in new directions. While serendipity is chance-based and cannot be controlled, perhaps it can be engineered. A few existing digital humanities and cultural heritage projects experiment with engineering serendipity. Serendip-o-matic, launched in August 2013, aims to re-incorporate chance into the scholarly research process. On the website, users input a text; the tool identifies key words in the text and responds with primary source images from several online collections. The goal of Serendip-o-matic is to yield happy accidents for a wide range of users, whether students in search of inspiration for a paper topic or scholars looking for materials to enliven a current project.4 Another example is Magic Tate Ball, a mobile application designed by digital studio Thought Den to encourage a general audience to discover works of art in the Tate’s collection. Using GPS location, time of day, weather, and analysis of ambient noise, the application returns an artwork, explaining why this work was selected and providing content that allows the user to learn more.5 Magic Tate Ball enables users to engage with works they would not have sought out otherwise while infusing play in the discovery process. At HyperStudio, we hope to incorporate a similar sense of serendipity in ArtX. Serendipity has the dual advantage of skirting traditional boundaries and adding a playful element to the user experience, which serves both browser and researcher. As we aim to make meaningful and creative connections between the art objects that comprise our past and the events of the present, we believe we can incorporate both audience groups without sacrificing archival rigor. To do so, we will need a holistic, audience-centered approach to digital curation and recommendation. To achieve this goal, we plan to start small. Through specific partnerships with museums in Boston, we are building a closed and controlled system that can serve as a testing ground for new models of recommendation. Free from industry demands such as growth and scale, we can perfect our schemas and our assumptions before expanding to other institutions. We are also hopeful about creating a collaborative, open-source approach to art recommendation, particularly given the close secrecy with which proprietary recommendation algorithms are guarded. By encouraging open conversation around the ways we recommend art, we may find unique approaches and ways in which current recommendation systems are insufficient or misleading. We have many questions and challenges ahead. It will be important to understand our audience: How much control over the discovery process do users want, and how can we best balance the sliding scale between browser and researcher? We expect our primary audience to be Boston-area residents and university communities—a casual but informed audience that bridges aspects of both. We hope to instill a scholar’s depth of interest and rigor in the casual user and we hope scholars too can employ the tool as serendipitous inspiration for their own work. But how transparent can we be about the logic behind our recommendations? How can we scale such a strategy, connecting artworks to books, lectures, music, movements and ideas? Perhaps most importantly, while we have explained “why serendipity,” we must address the “how.” Serendipity involves more than simply selecting objects at random, but what signals are important? How can we prime a user for the mindset of serendipitous discovery, rather than rote research? Moreover, is it truly serendipitous if we are closely engineering the suggestion? We look forward to addressing these questions, but with care to not create our own faulty algorithms. One of the challenges in this process is to avoid reducing cultural objects to the level of products, and museum audiences to consumers. Looking past the current limitations of discovery will be vital for generating new connections and ideas.  ",
       "article_title":"Rethinking Recommendations: Digital Tools for Art Discovery",
       "authors":[
          {
             "given":"Liam",
             "family":"Andrew",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology, United States of America",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          },
          {
             "given":"Desi",
             "family":"Gonzalez",
             "affiliation":[
                {
                   "original_name":"Massachusetts Institute of Technology, United States of America",
                   "normalized_name":"Massachusetts Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/042nb2s44",
                      "GRID":"grid.116068.8"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "organization",
          "software design and development",
          "project design",
          "art history",
          "GLAM: galleries",
          "libraries",
          "archives",
          "information retrieval",
          "museums",
          "management"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In the age of Big Data and technology-assisted research, scholarship in the humanities is developing innovative new approaches and methodologies, be they quantifiable and visual (Moretti, 2005), algorithmic (Ramsay, 2011), built upon ‘distant’ and ‘machine’ readings (Hayles, 2012) and ‘cultural analytics’ (Manovich, 2009), or of the many possibilities and techniques offered in the field of Digital Humanities (Burdick et al, 2012). Film and television analysis, however, has been slow to adapt digital data-driven research. This presentation will demonstrate a software project entitled ClipNotes, a software application created by Dr. Stephen Mamber at UCLA along with a team of graduate students that are helping to develop it. Examples will be shown of both traditional textual analysis amplified by the software and my own research that quantifies product placement and brand integration in film and television.  Though cinema and media studies has rightfully continued on its course of splintering and diversifying into a multitude of interdisciplinary amalgamations and sub-disciplines, close textual analysis remains at the heart of what we do, particularly with regards to teaching. Technology has dramatically increased our ability to deconstruct a film text, from VHS to DVD to digital interfaces which allow even more minute control over image and sound. Creating clips and screengrabs is becoming easier and is an increasingly common feature of lectures and presentations, but the method of visual analysis itself hasn’t advanced very much, nor has the incorporation of many other research technologies. Notably, the use of programming and algorithmic analysis is quite limited in film studies research compared to other disciplines within the humanities which are utilizing digital tools to invigorate their methods and broaden their scope. ClipNotes is an iPad and Windows 8 app that facilitates quick segmentation, annotation and presentation of film clips, an example of the research possibilities that are provided in a collaborative, database--driven, XML--based software environment. The design of ClipNotes is deceptively simple: it allows users to mark up video files with metadata and present this analysis. Start/stop times, clip descriptions, and captions are assembled in easily-produced XML files, which stands for extensible markup language that is both human and machine readable. When the XML file is linked to the video file, precise, granular analysis is made possible and is easily presented and disseminated. A public repository for these XML files is available at clipnotes.org, which will allow for widespread sharing of textual analysis and should provide an invaluable teaching resource. Users are encouraged to upload their own XML files for inclusion and we have begun building an extensive database of freely available teaching and research materials. For obvious copyright reasons, the films are not included, but guides are available to demonstrate how DVDs can be easily and legally encoded into digital files under fair use exemptions. Then, the video is linked together with the XML file by the application. In practice, ClipNotes collapses the research and presentation process by bridging the two: your textual research and analysis is the development of your presentation.  To get the archive started, a series of films have been coded in XML and are available for teaching usage. Citizen Kane, of course, has been catalogued, providing quick access to its landmark visual style. All instances of deep focus, triangular framings, door and window framings, graphic matches, media representations, and the motifs of light and bulbs, bars and fences, and mirrors are quickly and easily accessible through ClipNotes. The ability to quickly but briefly demonstrate a series of scenes -- particularly ones involving sound or camera and character movement -- is a tremendous    resource in a lecture or presentation situation. One of the distinct strengths of film is the symphonic arrangement of audiovisual patterns, something that is lost in merely showing screen grabs or a few extended scenes. Beyond this kind of instructional usage, however, the practice of granular analysis can reveal new discoveries in even the most well-worn films. Hitchcock is likely the most thoroughly researched auteur, but with the ability to isolate and compare shots on a frame-by-frame basis, very subtle visual patterns can be identified. For instance, the ability to document the most minute patterns in Psycho reveals a micro-detailed facial dramaturgy that adds significant visual emphasis to Hitchcock’s already complex exploration of identity.  Douglas Sirk is another master of this kind of over--determined mise--en--scene, and a fitting subject for ClipNotes, as well as my own work that concerns the political economy of media as a textual phenomenon. Written on the Wind is a fine example, and for my analysis I catalogued over 100 examples of 4 distinct patterns: frame within frames and obscured frames, the mirror motif, vertical objects as phallic imagery, and what I hope is a new addition to the already -extensive Sirk Studies archive: the product shot. While Sirk’s satirical savvy is well-documented by scholars, the contradictory impulse -- the ways in which he helped promote and fetishize the very consumer culture he was satirizing -- is a less prominent element of his legacy. Barbara Klinger provides a useful corrective of his body of work in Melodrama and Meaning: History, Culture, and the Films of Douglas Sirk, in which she considers the various extra--textual discourses (academic, industrial, trade, popular press, star, gossip, camp) that have contributed to Sirk’s legacy, but she expressly decides not to engage in any textual analysis, as her interest is in the surrounding discourse. I believe textual analysis could be a useful addition to furthering her nuanced reading of Sirk, particularly the industrial and promotional elements of    his work that get overlooked in favour of his more critical characteristics.  Klinger shows how Written on the Wind was heavily promoted in women’s magazines as a part of lifestyle marketing, particular tie--ins with its fashion, make--up and home decor. Textual analysis of the film itself reveals a similar impetus for commercial promotion, catalogued here as the “product shot.” At least two dozen shots of consumer products are featured in the film, most prominently luxury cars, fashion, and jewellery. The first scene immediately establishes the patterns of framing, mirrors and vertical objects, but it also quickly pronounces the importance of products, by featuring Kyle’s shiny, yellow sportscar. Far from claiming that this kind of quasi-product placement deems Sirk some sort of sell-out or salesman, I think a more complete picture of all that Sirk accomplished in his films just adds to his stature. That he was able to satisfy the high standards of critics, scholars, studio executives, and promotional agents is a testament to the complexity of his filmmaking ability. It also provides some insight into the early history of product placement, brand integration, and transmedia. ClipNotes and XML--encoding provides the opportunity to generate and catalogue large data sets of analytic material for audiovisual texts, lending quantification and data -processing possibilities in the future. Sound and image are inherently more difficult to catalogue and quantify than the written word, which accounts for some of the delay in the use of digital humanities methods in cinema and media studies, but creative new digital tools should be able to bridge this gap. Similar to the Text Encoding Initiative, the data-mining prospects generated by ClipNotes are impressive. With the assistance of digital tools, we will be able to both dig deep into solitary texts, discovering and quantifying micro--relationships, while also mapping broad, macro--cultural dynamics as a result of this wide--ranging data. In a digital era marked by the vast proliferation of complex texts, software applications can be utilized to enact a more rigorously detailed analysis, to archive and disseminate provocative insights, and    to extend digital scholarship.  Thank you, and please visit clipnotes.org for more information.  ",
       "article_title":"ClipNotes: Digital Annotation and Data-Mining for Film & Television Analysis",
       "authors":[
          {
             "given":"Andrew",
             "family":"deWaard",
             "affiliation":[
                {
                   "original_name":"University of California, Los Angeles, United States of America",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "film and cinema studies",
          "metadata",
          "image processing",
          "software design and development",
          "bibliographic methods / textual studies",
          "content analysis",
          "xml",
          "encoding - theory and practice",
          "multimedia",
          "media studies",
          "text analysis",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. The need for theory Harold Somers and Fiona Tweedie pose the following question: If a vocabulary- based authorial attribution technique fails to attribute an original text and a pas- tiche to different authors, “is this because the pastiche is good, or because the technique is faulty” (Somers and Tweedie, 2003)? The question places computa- tional techniques and literary concerns into a direct relationship, inviting a formu- lation that might “leap from [word] frequencies to meanings” (Craig, 1999), and so seems an ideal opportunity to explore and interrogate our assumptions surround- ing literary interpretation, literary style, and genre. Moreover, a failure or refusal to attend to the question would seem to be a catastrophe for attribution studies: without a generalized set of criteria to critique how stylistic concerns influence an algorithm’s effectiveness at identifying an author’s statistical “fingerprints,” the general validity of authorial attribution techniques will remain contested despite persuasive examples of authorial attribution techniques like Hoover (2003) and Garcia and Martin (2007). While scholars such as Jockers (2013) are interested in exploring how attributes such as lexical variability, word frequencies, word choice, and other statistical measures can be used as indicators of authorship, style, genre, gender, and even nationality, there remains a paucity of theory to explain why these and other indicators happen to be more or less effective with respect to the finite sets of authors, books, and/or genres they are applied to. Somers and Tweedie approach their question pragmatically—that is, they subject Alice in Wonderland, Gilbert Adair’s pastiche entitled Alice through the Needle’s Eye, and several “con- trol” texts to a battery of authorship attribution techniques and report the results of their tests. They do not, however, provide a theoretical framework to understand why one technique would be more or less effective than another. Nor are their re- sults generalizable to other cases because there does not exist a larger theoretical framework to understand how Somers and Tweedie’s experiments may relate to a different set of originals and pastiches we might examine. The success or lack of success of the statistical techniques used to distinguish authorship has much to do with the idiosyncrasies of the individual texts and authors being considered and is frequently aided by our historical knowledge of existing texts for which author- ship is already known. But far too little effort has been devoted to developing a theoretical model that might provide us with a compendium of the possible ways our statistical methods might fail. 2. The interdependence of authorship and style  Somers and Tweedie’s question highlights the contextual dependencies of our terms and the basic differences in assumption between nontraditional authorship attribution and computational stylistics. Effectiveness, for example, appears sen- sible in the context of authorship attribution techniques, but less so in the context of stylistics. Nontraditional authorship attribution techniques exist in a system for which the value of the question hinges on its falsifiability. Is a text of unknown authorship written by author X or Y given a set of existing texts written by both authors? There can only be one correct historical answer—which is usually only one author—and this correct answer is mutually exclusive to any other answer. Such “facts” are independent of the method we use to discover them. Alterna- tively, the question of pastiche quality—of whether the imitation is well or poorly done—is a question for which stylistics should provide an answer; nontraditional authorship attribution may also influence judgments of quality. When performed under the banner of literary studies, computational stylistics is concerned with in- teresting answers that point us to new interpretive insights about a particular text we are studying. These facts are less stable in that they depend on the methods which allow for their discovery and are not necessarily mutually exclusive.  Yet the epistemological distinctions above are countermanded by cases in which the concerns of authorship interpenetrate the concerns of style in ways that are difficult to generalize. When Erasmus declares a certain letter to be incorrectly at- tributed to St. Jerome based on the belief that “Jerome has a special quality about him, a kind of mental savour and temperament, a quality which may be felt rather than explained,” and, earlier, when we see this “never-failing quality, his lively humour...which the learned admire in Cicero,” the stylistic concern of quality is being used to determine authorship (1992, 80; see also Love 2002:18-22). Yet are issues of authorship and style always interrelated? The answer, I contend, is yes; in limiting cases where we have appeared to isolate these concerns it is because we have already (intentionally or unintentionally) picked our texts in such a way that separation becomes possible. When we point to a question that does appear to belong exclusively to the do- main of stylistics or authorship attribution, is it not always the case that a careful a priori selection of the texts was conducted at an earlier stage of analysis in which authorship and style did impinge on each other? The decision, for example, to un- dertake a nontraditional authorship attribution test necessarily entails never losing sight of the relationship between authorship and style (i.e., genre) since the signal from the latter sometimes “overpowers” the signal of the former (as one sees in Hoover 2013). And when we do find a statistical result that countermands our (literary) expectations, is not a useful first step to examine the interdependence between authorship and style to account for the surprise?   3. Pastiche Quality and Authorship  Somers and Tweedie’s original question can be separated into two: the first relates to the fundamental validity of computational authorship attribution tech- niques and the second relates to a functional definition of what a pastiche is. Ex- plicating these two questions in detail is useful in developing a theoretical perspective to critique and explore Somers and Tweedie’s paper as well as for developing a better theoretical foundation for the acceptance or rejection of certain assumptions inherent to the contemporary practice of computational stylistics.   As Somers and Tweedie note, for authorship attribution techniques to be most effective one tracks linguistic habits “which may be the least susceptible to variation” (412)—that is, we look at features that an author does unconsciously since the features an author has no control over are those expected to be least affected by the idiosyncrasies of genre, historical moment, and so forth. Yet if we are seeking to examine the “quality” of a pastiche, as Somers and Tweedie ask, then we are seeking features an author is consciously employing in pursuit of imitating another author. The similarities that are relevant to the literary quality of a pastiche would necessarily be those features for which a human reader is able to readily identify and is likely to be those which a reader has had the most practice at identifying. To be sure, the category of pastiche has perhaps a more overt connection to both past literature written and contemporary culture—its existence is defined directly by what has already been written and depends upon the reader’s recognition of this. These relationships between tradition and culture are perhaps no more or less important to other literary forms, but the category of pastiche specifically asks the reader to reflect upon such relationships directly and overtly.  Reframing Somers and Tweedie’s question as two questions allows us to adopt a scheme from R. G. Collingwood’s “On the So-Called Idea of Causation” so as to parse the original ambiguity in Somers and Tweedie’s into several logically distinct classes. This parsing will allow us to see that attributing the success or failure of authorship attribution algorithms to only the two possibilities of algorithm effec- tiveness or pastiche quality effectively equates two incommensurable ontological systems as if they were logically consistent. To avoid this difficulty, we need only clarify the original question so that we are acting in a logically consistent manner. However this particular scheme comes with a high theoretical cost. To resolve the ambiguity inherent to Somers and Tweedle’s question, it may be necessary to re- sort to literary descriptive categories for which the identification by a finite series of computable steps may be theoretically forbidden.   ",
       "article_title":"Incommensurability? Authorship, Style, and the Need for Theory",
       "authors":[
          {
             "given":"Aaron",
             "family":"Plasek",
             "affiliation":[
                {
                   "original_name":"New York University, US",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "philosophy",
          "literary studies",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" DiagnosisThis paper is a diagnosis and a polemic. It takes as its occasion the startling recent popularity of topic modeling among practitioners of the digital humanities (Nelson, 2010; Weingart and Meeks, 2012; Jockers, 2013; Tangherlini and Leonard, 2013; Laudun and Goodwin, 2013). As diagnosis, I propose that the significance of topic modeling can be contextualized within the rising predominance of social, political, and cultural themes as the major interests of literary scholarship in the last forty years. This predominance can, I show, itself be concretely grasped using topic modeling, as in the three figures below. Fig. 1: Yearly proportions of recently-rising topics in a model of seven literary studies journals, labeled by most frequent words. Continued in figure 2.These figures visualize all the recently-rising topics in a 120-topic model of a corpus of literary-studies articles from seven generalist journals from the 1889–2013 period. Before looking at time series, I coded each topic as “social/political,” “formal,” “other themes,” or “non-thematic.” Of the 26 recently-rising topics shown in the figures, 14 are classifiable as “social/political”; of the remaining 94, only 4 are. (See “Methods” for details). I argue that the recent turn in the digital humanities to computational studies of literary an   d cultural texts in the aggregate, typified by the work of Franco Moretti and Matthew Jockers (Moretti, 2013; Jockers, 2013), is best understood as an incomplete methodological response to an already-existing dominant thematic trend in literary studies.  PolemicThis historical diagnosis then leads to my polemic: let digital humanities be sociological! Instead of insisting on the distinctiveness of a “humanistic” interpretive approach—as, for example, Alan Liu has recently done in a sharp critique of “tabula rasa” digital interpretation—humanists should recognize the problem of interpreting cultural text in the aggregate as one they share with social science (Liu, 2013). This recognition can, in turn, help to clarify the controversy over whether the digital humanities deliberately neglect the social and political concerns central to literary and cultural studies in the last four decades (McPherson, 2012). Recognizing the sociological in the digital humanities would help to see how quantitative methods could address the fundamental concerns that humanists share with social scientists. Fig. 2: Continued from figure 1; continues in figure 3.In this short paper, I focus on the case of topic modeling: though this technique emerges from machine learning (Blei et al., 2003) and has been discussed as a form of distant reading, I argue that topic-modeling analyses of literary material (including my own in this paper) should be categorized as content analyses in the social-scientific sense. Although connections   between content analysis and humanities computing are of long standing (see Weber, 1985), the relevance of this methodology for topic modeling has not been widely remarked in the digital humanities. According to a standard book on the technique, “Content analysis is a research technique for making replicable and valid inferences from texts...to the contexts of their use” (Krippendorff, 2013, p. 24). The triple demands for validity, context-sensitivity, and replicability represent the fundamental social-scientific methodological contribution to this work. In work on topic modeling, these methodological problems have been addressed especially by political scientists (Quinn et al., 2010; Grimmer, 2010; Grimmer and Stewart, 2013) and sociologists of culture (DiMaggio et al., 2013; for a recent work on validation with literary topic models, see Mimno and Jockers, 2013). Topic modeling should not be valued as a tool for discovery alone but as offering evidence of systematic cultural variation. Emphasizing discovery (e.g., Blei, 2012), has led some to insist that the final task for humanistic topic modelers should be to return to “close reading” individual texts (Rhody, 2012; Tangherlini and Leonard, 2013). Yet this return to reading risks neglecting both the promise and the challenge of the topic model, which can reveal the workings of larger-scale cultural and social contexts by systematically and replicably classifying linguistic patterns, including thematic and rhetorical patterns. Fig. 3: Continued from figure 2 These patterns are of interest not in themselves alone but for their cultural, historical, and social contexts.  In my own argument, the category of “recent decades,” which highlights the rise of “social” topics, is actually a proxy for historical causes, including the institutional change represented in the corpus by the inclusion of “theory” journals newly established in the 1970s, Critical Inquiry and New Literary History; it remains for future work to incorporate indicators of these historical forces into the analysis of topic models.Even this preliminary content analysis suggests that digital humanists who study texts in the aggregrate might reconsider the context in which their own work emerges. Current discussions of “distant reading,” “macroanalysis,” “surface reading,” and “quantitative formalism” converge with sociology in terms of method but not necessarily subject matter (Moretti, 2013; Jockers, 2013; Best and Marcus, 2009; Allison et al., 2010). At the same time, the aggregate of literary studies has been converging with sociology in terms of subject matter but not method, and even the most recent turns to the sociological in literary studies have largely shied away from the quantitative approaches that digital humanists have embraced (see English, 2010). My polemical goal is to advocate for a dual convergence—not only in the case of topic modeling but across the set of quantitative techniques for studying cultural texts that have become central to the digital humanities.Methods Latent Dirichlet Allocation has been applied to corpora of scholarly journals by others (Blei and Lafferty, 2009; Mimno, 2012; McFarland et al., 2013); this work applies it to scholarship in literary studies, with the institutional history of the literary humanities as an interpretive frame. The modeled documents consisted of all the items classed as “full-length articles” by JSTOR that exceed 1000 words in length in seven journals chosen for chronological range and broad disciplinary scope: Critical Inquiry (1974–2013), ELH (1934–2013), Modern Language Review (1905–2013), Modern Philology (1903–2013), New Literary History (1969–2012), PMLA (1889–2007), and theReview of English Studies (1925–2012). Wordcounts and document metadata were supplied by JSTOR Data for Research ( JSTOR).Obvious item misclassifications were corrected. I excluded an extensive set of stop words, including common words, abbreviations, and first names, and retained only the 10000 most frequent word types. MALLET’s Latent Dirichlet Allocation implementation was used, specifying 120 topics and hyperparameter optimization feature (McCallum, 2002). The choice of documents to model and the construction of the stoplist emerged from work by Ted Underwood and me; Underwood should not be held accountable for this paper (Goldstone and Underwood, 2012; Goldstone and Underwood, forthcoming). Additional analysis relied on the R mallet package (Mimno, 2013) and my own R programs.The procedure for classifying the topics was as follows. I conducted a trial run by hand-classifying a 64- topic model of PMLA articles alone, developing an ad hoc scheme. Then, before visualizing any topics over time in my full seven-journal model, I examined the list of the twenty most frequent words in each topic and applied the categorization scheme to each topic:  S: Social or political topics, including national, ethnic, sexual, or gender identities;  T: Other thematic material, including religion, moral philosophy, love, nature, etc.;  F: Formal topics, including form, language, style, and genre;  NT: Non-thematic topics, including other languages, proper names, organizational labels, topics classifying textual studies, and clearly methodological discourses.  I classed as “recently rising” topics any topic for which the total proportion of those topics in each of the four decades after 1970 was greater than the total proportions in each of the decades from the 1930s through the 1960s. This heuristic was again devised with respect to the smaller trial model, then applied to the larger model. In future work ahead of the Lausanne conference, I plan to systematically vary the “recency” cutoff in order to test the sensitivity of my claims to the choice of 1970 as a demarcation line.The breakdown of all topics was as follows:  code not recent recent F 13 3 NT 56 4 S 4 14 T 21 5By this effort to make interpretive assumptions explicit (and to highlight the involvement of the researcher in classifying topics), I seek to bring the humanistic analysis of topic models closer to the demands of sociological content analysis.   ",
       "article_title":"Let DH Be Sociological! [Short Paper]",
       "authors":[
          {
             "given":"Andrew",
             "family":"Goldstone",
             "affiliation":[
                {
                   "original_name":"Rutgers University, US",
                   "normalized_name":"Rutgers, The State University of New Jersey",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05vt9qd57",
                      "GRID":"grid.430387.b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "content analysis",
          "english studies",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This paper discusses the research project Translantis, which uses innovative technologies for cultural text mining to analyze large repositories of digitized public media, such as newspapers and journals.1 The Translantis research team uses and develops the text mining tool Texcavator, which is based on the scalable open source text analysis service xTAS (developed by the Intelligent Systems Lab Amsterdam). The text analysis service xTAS has been used successfully in computational humanities projects such as Political Mashup, WAHSP, BILAND, and DutchSemCor. Within the context of the Translantis project, xTAS, coupled to Elasticsearch, will be further developed. Future versions will include clustering concepts and sentiment mining of issues in public debates. Translantis researchers are using Texcavator to detect and track cultural references in large textual corpora.  Use case: mining transnational references in public discourse In order to test the potential of cultural text mining, Texcavator will be used to analyze the role of reference cultures in debates about social issues and collective identities. The central use case of this project is the emergence of the United States in public discourse in the Netherlands from the end of the nineteenth century to the end of the Cold War. This concept of reference culture is be used to discuss long-term asymmetrical processes of cultural exchange involving dimensions of power and hegemony. The concept recognizes the fact that some cultures assume a dominant role in the international circulation of knowledge and practices, offering or imposing a model that others imitate, adapt, or resist.  Reference cultures are mental constructs that do not necessarily represent a geopolitical reality with an internal hierarchy and recognizable borders. These culturally conditioned images of trans-national models are typically established and negotiated in public discourses over a long period of time. However, the specific historical dynamics of reference cultures have never been systematically analyzed and hence are not fully understood. To explore these dynamics, this project asks three interrelated questions.   How can e-tools be used to map trends and changes in relation to the economic power, cultural acceptance, and scientific and technological impact of the United States as reference culture? How does public discourse reflect and influence the emergence and impact of reference cultures? How were ideas, products and practices associated with the United States valued in Dutch public discourse between 1890 and 1990?   We propose that the key to understanding the emergence and dominance of reference cultures is to chart the public discourse in which these collective frames of reference are established. Text mining methodologies allow us to trace changes in “big data” repositories of public media, such as newspapers, journals, and other periodicals. Central to this project is the large digital data collection of the National Library of the Netherlands (KB), which contains 9 million newspaper pages and over 1.5 million journal pages2. This large collection of serialized historical texts, which have been OCR-ed and provided with meta-tags, allows us for the first time to study long-term developments and transformations in national discourses in a systematic, longitudinal, and quantifiable way, by using innovative text-mining tools.  Methodological innovations and challenges  The semantic text mining tool Texcavator has direct access to historical textual repositories and is able to handle queries on-the-fly, and to produce visualization such as timelines and word clouds based on integrated topic modeling and NER modules. This allow us to test the value of qualitative heuristic models and to pair them in a meaningful fashion with quantitative methodology. Some of the methodological challenges involve the calibration between close and distant reading, the normalization of search results from unevenly distributed historical media, and adjusting for lexicological changes that affect the accuracy of sentiment mining and concept mining.  First results indicate the ability to mine “hidden debates” in public media in a bottom-up (inductive) manner, based on the footprints that used terms leave behind. More importantly, the tool is innovative in that it pinpoints continuities and discontinuities in public discourse, for instance by showing variations in the context in which key terms are used, and changes in sentiment values of words over time. We argue that this marks a promising transition from text mining to “concept mining” and new forms of cultural text mining that go beyond already established mining features.  Conclusions We will demonstrate that semantic mining of big data open new vistas in historical research because they (a) provide a robust framework for producing new vistas on macro history; and (b) can be complemented with numerical data sets provided by other researchers, for example on economic and social trends. This, ultimately, is the transformative promise of digital humanities as a multi-dimensional window on political, economic, and social change.         ",
       "article_title":"Cultural text mining: using text mining to map the emergence of transnational reference cultures in public media repositories",
       "authors":[
          {
             "given":"Toine",
             "family":"Pieters",
             "affiliation":[
                {
                   "original_name":"Utrecht University, The Netherlands",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          },
          {
             "given":"Jaap",
             "family":"Verheul",
             "affiliation":[
                {
                   "original_name":"Utrecht University, The Netherlands",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "historical studies",
          "digital humanities - facilities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction During the last 15 to 20 years, a considerable amount of premodern Chinese texts have been made available electronically, both for free and unhindered use and commercially in dedicated and locked down applications. Examples for the first type include projects such as the Chinese Buddhist Electronic Text Association (www.cbeta.org) and Wikisource (zh.wikisource.org) and the Internet Archive (www.archive.org), while examples for the latter includes products like the Siku quansshu electronic edition (四庫全書 電子版) by Digital Heritage Publishing or Zhongguo jiben guji ku 中国 基本古籍庫 by Airusheng.  For scholars wanting to make use of these ressources for their research, there are a few obstacles, including:  different formats and ways to access the texts in many cases, texts do not confirm to philological standards researchers can not annotate the texts and share their notes  Now, the projects described here attempt to develop an infrastructure for enabling scholars to work with repositories of freely available texts using a rapid prototyping approach with an expanding group of scholars for testing and early adoption. Technically, the main idea is to develop this as a network of repositories of texts, where each node a network consists of a set of git repositories, that can represent multiple editions of texts.   Kanripo: A repository of premodern Chinese texts First experiments for one node of such a network have been started at Kyoto University's Institute for Research in Humanities and its associated Center for Informatics in East-Asian Studies, CIEAS. In this experiment, the distributed version control system (DVCS) git is used as a basic transportation layer. Every text in the repository is represented by one DVCS node; different editions of the text can be represented by different versions or \"branches\" within this node; digital facsimiles can be associated with such versions. Users can also \"fork\" public projects and create new branches with their own annotations and comments and share these with other researchers, either in closed groups or with the general public. The interaction with Kanripo occurs mainly through the web interface, but can also occur directly from the desktop tool Mandoku. However, as the experience so far has shown, it seems necessary to further develop the web interface, to enable it to become not just a hub for interaction between the users, but also a full-fledged client for editing texts in the repository or add comments and annotations. For this purpose, a system similar to the popular Github site (github.com) is envisioned, starting from a open source clone of github called gitlab. This is in a very early stage of development and any feedback from the audience will be much appreciated.   Mandoku: A tool for interacting with the repository Development has also started aiming at a convenient desktop based tool for interacting with the repositories; a preliminary development version of this tool called Mandoku will also be demonstrated during the presentation. Mandoku tries to meet researchers of premodern Chinese texts where they spent most of their time, that is reading, annotating and translating texts. This is why the current prototype is build on the powerful and extensible editor Emacs, while as a future implementation a interface for more casual users is also planned. To incite users to overcome the initial hurdle of adopting to a new and unusual editing program, a number of tools have been implemented, that enhance the usefulness of the system, among them a keyword in context (KWIC) index is generated on each text in every repository node, which can then be queried through Mandoku and the aggregated results will be displayed, a cumulative index for the query of dictionaries and specialized reference works; further text-analytic tools are planned.   Repository for digital publications Another problem this system tries to adress is a serious problem with the current mainstream form of digital publication: Currently a website serves usually as the main and in many cases as the sole venue of publication, thus usually hiding a complex textual resources between one browser-mediated interface (For a further discussion of this problem and a model for overcoming it see1, {6}, {7} and {8}). This topic has been discussed for some time and valid suggestions and a discussion of the requirements can be found in 2, 3, 4 and 5. Here, this proposal is taken up and expanded, namely by adding the requirement that the text will not only be made available to the scholarly community, but that it also be able to annotate it in a way that can be owned by the scholar adding the annotation and still be shared with interested colleagues. In the framework presented here, fulfilling these requirements on a technical level is constructed as follows:  A text available from the repository (which can be edited only by the editors) is \"forked\" into a private text repository on this or any other node in the network. The researcher can now edit and annotate the text to its hearts content, if so desired making the annotations available to others through pushing them to the forked repository. Occasionally, the researcher might come across errors in the text or has material he would like to offer for inclusion in the authorative published text. He now issues a \"merge request\", which alerts the editors of the published texts to the existence of this piece of information, which usually is already visible on the private repository. The editor will consider the merit of the correction and can then incorporate it into the published text. When doing so, the origin of this information and any communication concerning the reason and argument for this correction will retain their relationship to this piece of text and is available as part of the scholarly record for this text.  If used correctly, this mechanism could provide a solution to the above mentioned problems with online digital publications. First experiments with scholars connected to the Kanripo research project showed that the technical protocol is not easily transparent to the scholars who are supposed to use it and does require more fine-tuning and better tool support in order to become more widely acceptable.   Conclusion In an attempt to provide a stable, extensible platform for the curation of the textual heritage of China, a blueprint for text repositories that can form a network of related, but independent repositories for critically edited texts has been provided and a prototype of this implemented as the Kanseki Repository of texts (Kanripo), which can be accessed through one prototype client Mandoku. Further development will occur in collaborative form with scholars using this framework by attending to their emerging needs and thus hopefully developing into sustainable resources that can provide a solid base for all kinds of scholarly inquiry that relates to premodern Chinese texts.  ",
       "article_title":"Kanripo and Mandoku: Tools for git-based distributed repositories for premodern Chinese texts",
       "authors":[
          {
             "given":"Christian ",
             "family":"Wittern",
             "affiliation":[
                {
                   "original_name":"cwittern@gmail.com",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "Chinese Studies",
          "Research infrastructure",
          "Tool development"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  heureCLÉA (www.heureclea.de) is a BMBF-funded eHumanities project1 which combines the two conceptual perspectives on object annotation that set apart the humanities and the 'hard sciences': strictly rule-based explication of uncontroversial object features as exemplified in the measurement of values, such as length, height, density, etc., versus the hermeneutic response for which observation and emotive engagement from a subjective point of view must go hand in hand in order to facilitate interpretation. These are of course ideal types and the actual practice of annotation in the humanities is situated at their interface, which is heuristics—the methodologically controlled 'art of finding' that goes beyond pure measurement, but whose purpose it is to generate relevant questions rather than conclusive answers. Against this backdrop heureCLÉA aims to implement a digital heuristics module for the text annotation tool CATMA (www.catma.de) so that we may benefit from synergies between the computationally automated and the subjectively motivated, human generated annotation of texts.   1. The heureCLÉA Project The practical backbone of heureCLÉA are two software developments: HeidelTime, which was developed at Heidelberg University, is a rule-based system for the extraction and normalization of temporal expressions.2 It needs to be significantly modified to cope with the complexity of literary narratives.3 – The CATMA (Computer Aided Textual Markup & Analysis) markup tool was developed at Hamburg University. The current release of CATMA (version 4.0) is open source and provides a robust web based annotation environment for collaborative markup.4 CATMA not only supports intuitive text annotation in a flexible, XML/TEI-compliant format, but also integrates markup with analytical and visualization functions (cf. Fig.1). This enables users to switch ad hoc between text annotation and text analysis in either direction as well as recursively. CATMA thus supports what Burnard referred to as the 'continuous turning of the hermeneutic wheel'5—i.e. the back and forth between formal text analysis and the generation of interpretative hypotheses. In its most recent development phase (called CLÉA)6, CATMA then progressed from a stand-alone desktop application to a web-based solution. This adds yet another conceptual dimension, that of collaborative markup:7 in CATMA researchers can now share, reuse, amend and dispute each other's markup.    Fig. 1: Exploration and Annotation in CATMA  CATMA's overall design is based on the premise that a DH tool should emulate the methodological and social practice of traditional philology as closely as possible. This practice integrates three methodological primitives: analytical/declarative operations, hermeneutic operations, and discursive critique of explications and theories.8 This conceptual high-level design goal also determined our choice between the two competing paradigms of embedded (in-line) markup and external standoff markup, which we regard as methodological rather than technological opposites. Accordingly, embedded markup represent the idea of objective taxonomic universality and the potential immanence of 'truth' in an object. External standoff markup, on the other hand, is based on the acknowledgment of the contingent nature and historical transience of object interpretation. In a contemporary philological perspective, embedded markup therefore constitutes something of a methodological anachronism: for conceptually it resembles the pre-enlightenment model of canonical text exegesis which the modern humanities have long replaced by a critical, self-reflexive hermeneutic approach. Yet this critique is of course of a purely philosophical nature when dealing with pre-interpretive analytical and declarative tasks, such as POS tagging.  However, once higher-level semantics are at stake, these considerations force us to adopt a truly 'hermeneutic' approach to markup.9 Interpretation varies depending on interpreter, context and interpretive theory. Accordingly, even elementary markup produced in order to support higher-level interpretation must still remain transparent as one possible account among many, and users must be able to produce and store ambiguous and indeed even contradictory markup for the same text in a standoff manner. Since rich interpretations are best generated in a discursive practice, it is also necessary to enable the easy sharing and combining of markup generated by different interpreters. However, while these desiderata can all be considered emulative goals which informed the development of CATMA, their conceptual benefit for the digital humanities at large lies elsewhere. A truly non-deterministic and discursive approach to markup yields diverse annotation data—and that type of data can subsequently be analyzed in order to \"push back\" the boundary separating interpretation and declaration.  What this means is best illustrated by outlining the three components and phases of heureCLÉA (cf. Fig. 2):  1. Narratological Analysis of Temporal Phenomena: In heureCLÉA the identification of the temporal structure of narrative texts is approached concurrently through (a) manual collaborative annotation with CATMA, and  (b) automated temporal tagging with HeidelTime.  In (a) we draw upon (but do not restrict our taggers to) the narratological taxonomy of Genette10,  which is supplemented by a taxonomy suited to capture action and event segmentation. In (b) automated temporal annotations are generated via an UIMA pipeline11 that includes HeidelTime as a rule-based temporal tagger, the TreeTagger12 as a POS tagger, and Morphisto13 for a morphological analysis. 2. Machine Learning Approach towards an Automation of Complex Time Annotation: The next step is the learning of new rules for automated annotation from the manually generated markup. Different methods for the derivation of rules—especially those for typical co-occurrence of temporal expressions—are used. Once integrated into the components of the heureCLÉA UIMA pipeline these rules enable the system to handle more complex annotations. This process is dynamic as growing quantities of markup facilitate more complex modeling strategies based on e.g. distributional approaches (such as Latent Semantic Analysis). Finally, patterns representing typical temporal sequences may be extracted (Sequence Mining). 3. Integration of the Heuristic heureCLÉA Module into CATMA:Once a functional threshold has been passed (i.e.: reliability of automated detection of temporal references of low complexity; performance/robustness) the heureCLÉA UIMA pipeline will be integrated into CATMA as a service. It then provides a 'digital heuristics' for the partially automated, partially interactive generation of temporal markup, and will be tested and evaluated to verify the adequateness of the automatically generated markup (partially through stochastic methods.)   Fig. 2: heureCLÉA's interrelated areas of work            2. The Heuristics/Hermeneutics Divide As a Conceptual Boundary in the Digital Humanities What is the methodological relevance of this work toward a digital heuristics? The realization that text markup is essentially interpretive per se is anything but new.14 Indeed, the argument about what markup is seems somewhat artificial; it might have sufficed to ask literary scholars what markup is there for: in their view the raison d'être of any object annotation and classification is always interpretation. But is the boundary between a declarative and an interpretive method rigidly defined for the digital humanities?  Some experiences gained in the heureCLÉA project may offer an answer. To begin with, the hermeneutic nature of building time constructs from narratives proves to be only partially owed to the 'fuzziness' of natural language. As a complex symbolic system narrative is also characterized by the intricate coupling of a referential and an indexical semiotics. 'Time' illustrates this: on the surface it is referenced as chronological structure of a particular 'story world'.15 Yet on a deeper level it is also communicated as an implicit processing instruction encoded in the form of temporal deixis and ellipsis, and of linguistic markers for the contraction or expansion of time (viz. a summary vs. a scenic description). This information is termed 'indexical' because it refers back to the instance of utterance (the narrator or narrating character). The reader needs to process that information in parallel with the referential in order to reconstruct two intersecting chronologies—that of the 'story world', and that of the representational discourse which he is trying to reverse engineer. However, the reach of the indexical extends beyond the text-reader-system: as we read we also become subject to the indexical temporality of the 'how' of representation (discours) on an existential level.16 As Ricoeur argues, the reconstruction of temporality on the referential and discursive levels of narrative is indeed how we learn to experience temporality.17 Time is only one of many phenomena of narrative representation and understanding characterized by this triple-layered semiotics—and against this backdrop it becomes clear why hermeneutic text interpretation cannot be automated. The machine is (as yet) not an interpreting agent able to engage reflexively and speculatively with its object. It is confined to a fully explicated, operational definition of 'relevance' in terms of known tasks and objectives. While it can resolve an indexical reference in order to compute, say, chronological order and extension, the interpretation of the existential relevance of that double-encoded message as one which also addresses the interpreting mind will remain beyond its ability for as long as it is not equipped with a concept of 'mind'.  However, the foundational human interpretive operations taking place on the elementary declarative and inferential levels can in part be approximated statistically through recursive routines. This recursive approximation is the functional characteristic as well as the outer boundary of what we term a 'digital heuristic'. By this we mean a computational tool able to support the heuristic operations of analytical identification and categorization of phenomenological primitives that necessarily precede hermeneutic synthesis. These operations form the indispensable basis of any higher-order 'interpretation'. It is in this intersecting terrain of the hermeneutic and the heuristic that the digital humanities might help to \"push back the border\" and question the hegemony of interpretation.   ",
       "article_title":"Pushing Back the Boundary of Interpretation: Concept, Practice and Relevance of a Digital Heuristic",
       "authors":[
          {
             "given":"Jan Christoph",
             "family":"Meister",
             "affiliation":[
                {
                   "original_name":"University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          },
          {
             "given":"Janina",
             "family":"Jacke",
             "affiliation":[
                {
                   "original_name":"University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "interpretation",
          "narratology",
          "hermeneutic markup",
          "literature",
          "machine learning",
          "collaborative markup",
          "heuristics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In this research, I propose an alternative technique to the traditional method of constitution of the literary canon. Instead of basing the determination of the canon on different values, I scrutinize the Modern Language Association International Bibliography database in order to determine the most cited authors and literary works. Specifically, I study the literature of the United States of America. Thus, through the process of data mining, I obtain a sample of over 290,000 references that allows us to observe the chronological evolution and the linguistic distribution of the critical bibliography about USA literature. This quantitative technique yields a corpus of more than 100 titles and 100 writers that are cited more than 100 times in the database. Consequently, this bibliography is not the result of subjective selection criteria, but is based on the law of large numbers. Furthermore, this study shows that the quantitative analysis of bibliographic databases is an effective way to bring new light to the field of literary studies.  ",
       "article_title":"Literary Canon and Digital Bibliographies: The Case of the United States",
       "authors":[
          {
             "given":"Carolina ",
             "family":"Ferrer",
             "affiliation":[
                {
                   "original_name":"Université du Québec à Montréal",
                   "normalized_name":"University of Quebec at Montreal",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/002rjbv21",
                      "GRID":"grid.38678.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "knowledge representation",
          "literary studies",
          "corpora and corpus activities",
          "cultural studies",
          "sustainability and preservation",
          "repositories",
          "digital humanities - pedagogy and curriculum",
          "databases & dbms",
          "archives",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The desire to reveal the history all around us, to see into the past, is as old as civilization. With the emergence of augmented reality (AR) technology, historians and public humanities professionals are exploring new ways to research, teach and learn about the past. AR applications augment the physical world by embedding it with digital data, networking, communication abilities and enhanced properties (Mackay 1996). When harnessed for history and public humanities, AR represents a disruptive way of accessing knowledge, making discoveries, and communicating history in new and imaginative ways. In “History All Around Us: Towards Best Practices for Augmented Reality for Public History and Cultural Empowerment”, the authors reflect on the design, development and testing of two location-based AR applications, and propose best practices for using AR to enrich our understanding of history, and support the cultural empowerment of citizens. The paper is organized in three parts. In part one, the authors draw on the digital humanities to form conclusions about best practices for AR design and development with a focus on two iPhone applications (see www.ihistorytours.com). These apps introduce visitors to the history of the villages of Queenston and Niagara-on-the-Lake, Canada (the latter of which hosts more than 2 million visitors a year). The authors reflect on the effectiveness of the design of these apps, and the development team's decision to offer visitors to these villages two kinds of experiences. The first experience, called “Roam Mode”, follows the user’s directions. It functions like an on-demand tour guide, providing the user with information about the historical buildings and objects that surround her. Apps that employ strategies similar to “Roam Mode” are now ubiquitous. Much less common is the second experience, called “Quest Mode”. It uses gamification (Deterding et al, 2011) to draw the user into exploring the history of the villages to solve long-standing mysteries. Inspired by real-life events, “Quest Mode” features historical personages linked by sometimes real, and sometimes imaginary events. In the case of Queenston, for instance, the user is enlisted to help solve the mystery of who bombed the nearby monument to a local war hero. Along the route, the user must solve puzzles; for example, the user must spot the differences between the real Fort Niagara, which stands watch imposingly from the banks of the United States, to an image of the fort that has been “discovered” by local historians.  The authors outline several best practices for augmented reality design and development for public history, giving special attention to “Quest Mode” and the concept of gamification. They note that the roots of gamification theory lie in the work of game theorists who have separated virtual environment games from real-world environments. These scholars have spent a decade attempting to define what games are, and therefore what they are not (Juul, 2005; Zimmerman, 2004; Pearce, 2004), and have pointed out the many ways that space (virtual, or real world) is treated differently (McGregor, 2006). The authors disagree with a strict adherence to this virtual-/real-world distinction, and suggest that there is much to learn by considering virtual and real-world environments together, because best practices for the development of experiences in one can be applied to the    other, resulting in a better understanding of the attributes of gamified augmented reality applications, and contributing to the emergence of (and our understanding of) new forms of expression.  At the same time, the authors urge caution about drawing too heavily on established game theory, and making unwarranted connections between games in virtual space and (augmented) real space. Foundational authors in game studies, such as Roger Callois (1961) and Johan Huizinga (1964) have classified games as activities essentially separate from normal life. But gamified augmented reality applications, such as the two iPhone applications addressed in this paper, often take place during working hours on city streets, and involve “players” playing in the midst of everyday life. (de Souza e Silva, 2008). Similarly, scholars have classified games as primarily escapist – causing players to disengage from their “real world” communities – but the application of interactivity to political and social issues has shown the potential of gamified augmented reality environments for collective cultural empowerment.  In part two of the paper, the authors draw on social science theory and methodology to provide a preliminary report on the testing of these iPhone apps. The authors note that while development of these kinds of digital environments (including, but not limited to augmented reality applications) is now commonplace in the digital humanities, rigorous testing for user engagement and learning within these applications is less common. They note that there is little research that addresses the assessment and appraisal of learning and engagement in augmented reality applications on mobile platforms, and highlight the need for principled and replicable methodologies.  The authors report on their progress towards evaluating the two augmented reality iPhone applications in terms of fostering learning and engagement. Specifically, the authors report on their use, for the purposes of testing, of two theoretical frameworks: i. the Benchmarks of Historical Thinking as outlined by Peter Seixas (2004, 2011; Peck & Seixas, 2008) and; ii. the Control-Value Theory of Emotions as described by Reinhard Pekrun (2009). In addition, the authors outline how they intend to promote learning and engagement in these kinds of apps by embedding dynamic assessment mechanisms to adaptively modify the content that is provided to the users, and improve user experience.  In part three, the authors briefly speculate about the ways in which the imminent (2014) arrival of commercial augmented reality platforms such as Google Glass (http://www.google.com/glass/start/), and connected Google Glass heritage- and history- themed applications such as “Field Trip” (https://play.google.com/store/apps/details?id=com.nianticproject.scout&hl=en) will transform the ways in which digital historians and digital humanists develop and use ubiquitous computing and augmented reality for cultural empowerment.   ",
       "article_title":"History All Around Us: Towards Best Practices for Augmented Reality for Public History and Cultural Empowerment",
       "authors":[
          {
             "given":"Kevin Bradley",
             "family":"Kee",
             "affiliation":[
                {
                   "original_name":"Brock University, Canada",
                   "normalized_name":"Brock University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/056am2717",
                      "GRID":"grid.411793.9"
                   }
                }
             ]
          },
          {
             "given":"Timothy",
             "family":"Compeau",
             "affiliation":[
                {
                   "original_name":"Western University, Canada",
                   "normalized_name":"Western University",
                   "country":"Cambodia",
                   "identifiers":{
                      "ror":"https://ror.org/02agqkc58",
                      "GRID":"grid.443228.b"
                   }
                }
             ]
          },
          {
             "given":"Eric",
             "family":"Poitras",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "virtual and augmented reality",
          "historical studies",
          "games and meaningful play"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  One recent trend in computational stylistics and authorship attribution has been to “tune” the word list for better results. T-tests can identify words with statistically significant differences in frequency between two authors; they remain an excellent method for one-on-one problems (Burrows 1992; McKenna and Antonia 1996; Hoover 2010). Eder and Rybicki (2011) use elegant methods to identify “sweet spots” in the word frequency spectrum by testing a range of numbers of the most frequent words (MFW). They also progressively remove words from the top of the spectrum, eliminating the function words favored by so much previous work. Personal pronouns have been removed to minimize differences in point of view and the numbers of male and female characters (Hoover 2002). The word list has been “culled” by removing words that are frequent because of high frequencies in one text (Hoover 2003, 2004; Burrows 2005). Rybicki and Eder (2011), Jockers, Witten, and Criddle (2008), and Rybicki and Heydel (2013) have culled words absent from any or many texts. Burrows’s Zeta and Iota select words consistently used by one group and avoided by another, ignoring frequency. These tuning methods vary in effectiveness with the number, size, date, genre, and language of the texts, and with the number of authors involved.  I propose here a transparently motivated form of tuning with a strong a priori plausibility: selecting unevenly distributed words, using the coefficient of variation (CoV). This dispersion measure is defined as Stdev/Ave. Freq.*100, expressed as a percentage of the average frequency. Consider the following four words, with statistics from 21 novels and short fiction by Willa Cather and Edith Wharton:    Word Ave.  Stdev  CoV  # Texts  Rank    magnificent  0.0034  0.0064  192%  5  3374    longing  0.0034  0.0107  316%  5  2427    generally  0.0036  0.0059  163%  7  2994    father  0.0368  0.0600  163%  14  332    Magnificent and longing both appear in 5 texts with similar average frequencies but very different Stdev, CoV, and rank: longing varies much more in frequency than does magnificent. Generally and father show that the Stdev is too closely tied to the average frequency to measure dispersion here: low-frequency words tend to have small Stdev’s. For the 21 texts above, among the 17,000 word types, only 1 of the 100 largest Stdev’s comes from a word ranking 1,000 or higher. The frequencies and Stdev’s of father are about 10 times those of generally, so both have the same CoV, which is thus a fairer measure of variability than the Stdev.  Unfortunately, simply analyzing words with the highest CoV is unworkable. The Stdev tends to be lower for less frequent words, but rare words and those occurring in a small number of texts have very large CoV’s. For the 21 texts above, 9,000 of the 17,000 word types appear in only one text and share the largest CoV (447%), regardless of their frequency. Some character names in long texts rank as low as 63rd, but 7,200 are hapax legomena. Furthermore, only 300 of the words with the 12,000 largest CoV’s appear in more than two texts. The need to identify words used fairly frequently and in many texts but with widely varying frequencies suggests combining Rybicki-style culling with the CoV in a method I call CoV Tuning. Now, some experiments. First, consider the Cather/Wharton set above, containing 3 Wharton novellas and 7 stories and 11 Cather stories. Standard cluster analysis (Ward linkage, squared Euclidean distance, standardized variables) correctly groups all texts in analyses based on 990 and 900MFW and fail at 400MFW only for Wharton’s “The Hermit and the Wild Woman.” (The seemingly peculiar choice of 990MFW arises from a limitation in my statistics program, Minitab.) All other analyses based on the 100-800MFW show at least 3 errors, including that same story.  I tuned the word list by sorting the 1,500MFW on the CoV. The words with largest CoV’s, mainly character names, appear in only one text, so I re-sorted the words on the number of texts they appear in, retained only words appearing in 7 or more texts, then re-sorted on the CoV and retained the 1,000 most variable words (MVW). Cluster analysis using CoV Tuning is very effective: all 7 analyses based on the 400-990MVW correctly group all the texts. Retaining words appearing in at least 5, 6, or 8 texts is slightly less effective. T-testing the 1,500MFW and retaining only the 352 words with p < .05 gives perfect groupings using all 352 words and decreasing numbers of them, down to the two words with the highest T value, the classic authorship pair, till and until. Thus, for two-author problems, the t-test is clearly superior. Figures 1-2 show standard and CoV Tuned analyses.    Fig. 1: Standard, 800MFW    Fig. 2: CoV Tuned, 800MVW  Now consider some difficult multi-author problems for which t-test tuning is unavailable. First I tested a set of 43 late 19th and early 20th century novels by 15 American authors, 2-3 novels each. Errors for at least 3 authors occur in all analyses using the standard method. Using CoV Tuning retaining only words found in 34 or more texts, analyses of the 900 and 700MVW have just 1 error, and analyses of the 500, 600, 800, and 990MVW have 2. Minimums of 22 and 38 texts give weaker results. Figures 3-4 show standard and CoV Tuned analyses, respectively.    Fig. 3: Standard, 700MFW    Fig. 4: CoV Tuned, 700MVW  Turning to a different genre and shorter texts, I tested 25 pieces of literary criticism by 14 authors, 9 authors with 2 texts each, 1 with 3 texts, and 4 with 1 text (see Hoover 2001 for details). I made this problem more difficult by dividing the texts into 33 sections of about 4,000 words. The standard method works well here, correctly grouping all sections by 12 of the 14 authors in analyses of the 600, 700, 800, and 990MFW and 13 authors at 900MFW. I used CoV Tuning on this word list using whole texts, retained words found in 7 or more texts, and then tested the 4,000-word sections. The improvement over the standard method is less dramatic here: the 600MVW succeeded for just 10 authors, the 700 for 12 authors, and the 800, 900, and 990 for 13. Minimums of 5, 6, or 8 texts are less effective.  Finally, I assembled a set of 40 late 19th and early 20th century novels by 20 authors, 2 novels each, intentionally selecting authors and texts known to be difficult to attribute. Standard cluster analysis correctly groups the texts by only 12 of the 20 authors, from 990 to 600MFW. I used CoV tuning on the word list as above, retaining only words found in at least 28 texts. Here, CoV Tuning matches the results for the standard method for the 700 and 800MVW, but correctly groups 13 authors for the 600MVW and 14 for the 900 and 990MVW. A minimum of 33 texts is less effective. Figures 5 and 6 show standard and CoV Tuned analyses.    Fig. 5: Standard, 700MFW    Fig. 6: CoV Tuned, 700MVW  More testing will be required to determine whether CoV Tuning gives consistently superior results on most sets of texts, and an automatic method for selecting the optimum limit to set on the minimum number of texts in each analysis is needed. However, CoV Tuning already seems to be a potentially valuable method of combining the information about word frequency that has long been the focus of authorship attribution and computational stylistics with the information about consistency of use on which recent methods like Zeta, Iota, and Full Spectrum analysis (Hoover 2013) are based.   ",
       "article_title":"Tuning the Word Frequency List",
       "authors":[
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "literary studies",
          "authorship attribution / authority",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In Reading Machines: Toward an Algorithmic Criticism, Stephen Ramsay suggests that computational literary studies remain marginalized because they lack “bold statements, strong readings, and broad generalizations” (2011: 2). They are too cautious, too scientific, to interest literary critics, who value opening texts to new interpretations over solving problems (10-11). Ramsay suggests that a feminist discussion of The Waves challenges algorithmic criticism: “literary critical arguments of this sort do not stand in the same relationship to facts, claims, and evidence as the more empirical forms of inquiry. There is no experiment that can verify the idea that Woolf’s . . . ‘elision of corporeal materiality’ exceeds the dominant Western subject” (7). Although many critical claims are computationally intractable, Woolf’s “elision of corporeal materiality” surely has textual implications that might be tested computationally. Literary criticism’s problematic relationship to facts, claims, and evidence seems more like a bug than a feature, but here I want to re-examine and interrogate Ramsay’s algorithmic provocation.  Three male and three female characters in The Waves speak in alternating monologues, an experimental technique that has invited critical comment about what axes of difference or unity characterize the novel. “The ‘problem’ . . . with Woolf’s novel is that despite evidence of a unified style, one suspects that we can read and interpret it using a set of underlying distinctions. We can uncover those distinctions by reading carefully. We can also uncover them using a computer” (Ramsay 2011: 10-11). Ramsay treats the six monologues as a corpus of documents and investigates them with tf-idf, from the field of information retrieval: tf*(N/df). This, he suggests, should identify each monologue’s characteristic words more effectively than a traditional word-frequency list. Tf-idf is the term’s frequency (tf) multiplied by the total number of documents (N; here 6) divided by the number of documents containing the term (df; document frequency). Tf-idf reduces the importance of function words and increases the importance of speakers’ characteristic words because the frequencies of words used by only one speaker are multiplied by six (6/1), while the frequencies of words used by all six speakers are multiplied by one (6/6) (Ramsay 2011: 11). After identifying each speaker’s most characteristic words, he reveals that he has actually used the formula, 1 + tf *log(N/df), which includes a log function (reducing the effect of a word’s appearance in only one speaker), and adds 1 (preventing the measure from becoming negative). The purpose of the alterations “is not to bring the results into closer conformity with ‘reality,’ but merely to render the weighting numbers more sensible to the analyst” (Ramsay 2011: 15). Yet the variants are not “merely” at the whim of the analyst; they have testable consequences.  But let us travel a bit further with Ramsay. He presents the words with the highest tf-idf scores in Louis’s monologue (listed in Fig. 1, along with his tf-idf scores, my tf-idf scores, and  their frequencies), and suggests that “Few readers of The Waves would fail to see some emergence of pattern in this list” (12). For example, western seems to echo Louis’s concern about his Australian accent, and England (all top 25). But actually western, wilt, and thou appear only in Louis’s quotations from a sixteenth-century poem. Ramsay’s provocation ignores some interesting questions: should Louis’s quotations be considered his speech (and retained?), or the anonymous author’s (and omitted?).    Fig. 1: Louis’s Most Characteristic Words  Trying to recreate Ramsay’s analysis reveals further interesting points. The mismatched tf-idf scores (bold) reflect different word frequencies. His score for beast requires 6 occurrences, not 5, but the 6th is in the omniscient narration). His score for accent requires 13 occurrences, not 14, but Rhoda’s most characteristic words include them– and accent occurs once as accent–, which presumably reduces his count by 1. (What constitutes a word is a surprisingly complex question, but treating accent– as a word seems odd.) The rarity of the words shows how strongly tf-idf privileges words limited to one character (only accent appears in 2). Ramsay’s intervention raises interesting questions: What does it mean to choose this algorithm? How do the results affect our emerging reading of The Waves? (Ramsay 15). But how to answer these questions?  Even the identification of characteristic words is problematic. Tick and hoot both occur 4 times, only in Bernard, but all 8 occurrences are in 3 consecutive sentences. How “characteristic” is that? Low occurs just 5 times, “only” in Bernard, yet it also occurs in the omniscient narration. Analyzing only the 6 characters seems reasonable, but should Bernard’s characteristic words also occur in the narration? (Some consider Bernard to be modeled on Woolf herself [Ramsay 2011: 13].) Including the narration seems both intriguing and problematic, not least because it is not dialogue. Doing so removes low, canopy, bowled, and brushed from Bernard’s most characteristic words and beast, steel, and discord from Louis’s. What questions does this raise?  Most algorithms for computational approaches come from authorship attribution, where ostensibly correct answers exist. But Ramsay is right that the existence of “correct” answers to questions like “Do the men and women speak differently?” or “Do the characters have distinct and consistent voices?” is precisely at issue. Examining The Waves in the light of Ramsay’s provocation raises so many intriguing questions that they cannot all be addressed here. But we can approach the question of character individualization by using a radical deformation. I randomly sorted the lines of the six monologues, then selected the first 6067 words of each, the length of the shortest monologue (Susan’s). I identified the 50 most characteristic words using Ramsay’s tf-idf formula, and tested how well they group with the remainders of the longer monologues using cluster-analysis, starting with all 300 words (in descending tf-idf order), then reducing the number gradually. The best result, for the 20 most characteristic words, is shown in Fig. 2.    Fig. 2: Tf-idf and Character-Individualization  Bernard’s and Louis’s sections group together, while Neville’s and Rhoda’s fail (Jinny and Susan have too little text for 2 sections). A simple word frequency list, however, correctly groups all 4 in many analyses (see Fig. 3, based on the 300 mfw), providing a tentative answer to the question of distinct voices.    Fig. 3: The Most Frequent Words and Character-Individualization  Selecting the 50 most characteristic words for each monologue using Zeta (Craig and Kinney 2011), also produces many perfect results (see Fig. 4, based on the 200 most characteristic words). This very different method, which measures consistency of use rather than frequency, confirms the distinctness of the voices. Finally, testing the six characters in 2,000-word sections with 2-grams (based on the six full monologues) also yields many completely correct clusters (see Fig. 5 for an analysis based on the 900 mf2Grams).    Fig. 4: Zeta and Character-Individualization    Fig. 5: 2Grams and Character-Individualization  Ramsay suggests that treating the question of whether the six characters in The Waves share “the same stylistic voice” as a problem to solve is a “category error,” and that the proper question–one computers cannot answer–is “Can I interpret (or read) it this way?” (2011: 9-10). Critics still can read the novel as a single stylistic voice, and the six monologues undoubtedly share many characteristics, but, in spite of a host of very interesting remaining questions about the status of algorithms, arguments, and evidence, it seems reasonable to make the bold claim that there are six distinct character voices in The Waves. Ramsay’s provocative intervention is valuable for forcing us to re-examine our methods and focus on questions of interest to traditional literary scholars. But further analysis of his provocation and his algorithm suggests that more attention to the text, to the nature and function of the algorithms, and to method may prompt bold claims that rest on a sounder foundation. Further work will help us explore the boundary between computationally-tractable and computationally-intractable questions and the significance of that boundary.   ",
       "article_title":"Making Waves: Algorithmic Criticism Revisited",
       "authors":[
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "literary studies",
          "stylistics and stylometry",
          "text analysis",
          "english studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This presentation will investigate K-Rad character substitutions as they were utilized in serialized ASCII text publications during the 1980s and 1990s. These text files, influenced by a genealogy of informational manuals on the topics of hacking and phreaking, sought to take writing beyond transparent signification, imposing hackerly techniques on to the text itself, by extending, disrupting, and hypermediating codes of discourse. In conventional informational writing, disruption and hypermediation are considered to impede signification; in K-Rad texts, they become key signifying agents.  K-Rad writing attempts to push boundaries, to present the unpresentable, and to break all codes, whether legal, moral, linguistic, or typographic. Although the style finds precursors in the underground discourse of piracy and software cracking, K-Rad was not universally esteemed in these contexts. For example, as Rabid Rasta says in “The Real Pirate’s Guide” (1984):  Real Pirates Don’t Say “K-K00L”, “K- Awesome”, “X10Der”, “L8R0N”, Or Anything Of The Sort.  Real Pirates Know The Difference Between “F” And “Ph” (I.E.”Philes”, “Phuck”, “Fone”, Etc.).  In Rabid Rasta’s opinion, unconventional spelling primarily demonstrates a pirate to not be “REAL.” His attitude attempts to legalize the spelling of words, which. according to Roland Barthes, “keeps the scriptor from enjoying writing, that euphoric gesture which permits putting into the tracing of a word a little more than its mere intention to communicate” (“Freedom to Write” 45). Such substitutions, particularly in published ASCII text files, are rarely accidental; these substitutions extend beyond conventional meaning, into the realm of the specifically technologized word. Consider, for example, the following example from “tHe PHiRzT StEp!!” by (\\/)[](_>|#|__ (1994):  iFh i HAvEN'T g0T y0U t0 S+oPA rEA|>iN' YeT tHeN i FiNK 0NlY dA Pe0PlE >tHAt wErE ELiTE eNUFF t0 tA/<3 iT sTaYeeD. On one hand, it is hard for the human eye to read, because it corrupts the alphabet, adding unfamiliar characters to the familiar ones, repurposing characters, using them in new contexts and infusing them with new meanings so as to make them strange. On the other hand, in an era of command line instruction, where computers were incapable of recognizing these visually corrupted renderings, it was only the human eye that could parse them. These words, which we cannot simply reduce to their legalized spellings, demand that we decipher the text on a character-by-character basis.  If the writing seems to falter at the sentence level, this is only because the writing style focuses primarily on the grapheme and the word, constituting an extreme close-up that blurs the shape of the sentence, the paragraph, and the text as a whole. Although we often, in the tradition of Saussure, consider the word to be the smallest unit of meaning, K-Rad graphemes become narrative elements, turning each word into a story. In this context, graphemes take on value within the world of the word. Stanley Fish says, “A reader’s response to the fifth word in a line or sentence is to a large extent the product of his responses to words one, two, three, and four” (Is There a Text in This Class? 27). With K-Rad orthography, we become aware that this process also takes place at the level of the grapheme—a reader’s response to the fifth grapheme is determined largely by graphemes one, two, three, and four. Every word requires active contextualization, active decipherment, a flow that does not merely move forward from left to right across each line, but scans in multiple directions within each assemblage of characters.  To the uninitiated, the writing might present itself only as a kind of line noise. Even for a well-versed reader of ASCII, the reading process never becomes a purely linear one in which words travel left to right without impediment, but, as Viktor Shklovsky says, “Art is not a march set to music, but rather a walking dance to be experienced or, more accurately, a movement of the body, whose very essence it is to be experienced through the senses” (Theory of Prose 22). Although texts might be said to transmit information or communicate, we cannot say that this is all that they do, unless we feel comfortable to ignore all of the text’s non-informational signifying elements. As Jerome McGann says:  \"When we imagine texts as transmitters we are not wrong in our imagination, but we are narrow—and much narrower than we should be if we wish to understand how texts work. Indeed, we easily confuse investigations of textuality when we study texts as machines for carrying messages. In the reading of poetry—those paradigm texts—this kind of confusion typically arises in thematic studies, where the “meaning(s)” of the texts are pursued. In poems, however, “meaning” is mistakenly conceived if it is conceived as “message.” Rather, “meaning” in poetry is part of the poetical medium; it is a textual feature, like the work’s phonetic patterns, or like its various visual components. (The Textual Condition 14-15)\"  The ASCII text file itself, as a medium, signifies a faith in the new technology of telecommunication and advances ASCII as a preferred means of communication. These texts are not only about their linguistic messages, but also about acquiring, arranging, and moving text, activities that constitute messages in themselves. And beyond all of this, we might also consider K-Rad texts as concrete poetry, a kind of ambient writing, a kind of visual arrangement that can make the initiate reader feel that they are beholding computer code itself and gazing upon some hackerly art. In a 1995 issue of Maclean’s magazine, Joe Chidley described the actions of the hacker:  \"His fingers trip lightly over the keyboard. With the punch of a return key, a string of characters – writ in the arcane language of computers – scrolls onto the black- and-white display in front of him. “OK,” he says, “I’m in.” Suddenly, horizontal rows of letters and numbers scroll from left to right across the screen – meaningless to the uninitiated eye. But for the hacker, the mishmash of data contains seductive, perhaps lucrative secrets. (“Cracking the Net” 54)\"  Lucrative secrets might be found in the technology of word processing, or the American Standard Code for Information Interchange, or the keyboard, or the technology of writing, or the technology of language in general. K-Rad adds extra texture to the word, complicating meaning and denaturalizing the basic elements of how we communicate. This kind of writing is radical not merely because of what it says but how it says. At the 2014 Digital Humanities Conference, I will present a series of K-Rad ASCII text files, demonstrating a sequence of increasingly baroque substitutions and suggesting conceptual reading practices that we might use to engage with these early avant-garde digital texts and also with conventional literature. These stylized visions do not only affect our reading practices here, but our reading practices everywhere, even in when dealing with texts that are not self-consciously stylized. These highly-coded and hypermediated texts confront us with the fact that there is no such thing as neutral writing, no such thing as neutral discourse, and once we have seen the word in its codified and disruptive form, there is no way to return to our previous unconscious state.  This topic of this presentation is based on “HACK,” a chapter from my recent dissertation, SCROLL / NETWORK / HACK: A Poetics of ASCII Literature (1983-1989).   ",
       "article_title":"\\/\\/ÆΓÑing: A Conceptual Parsing of ASCII Character Substitutions",
       "authors":[
          {
             "given":"Joel",
             "family":"Katelnikoff ",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "historical studies",
          "literary studies",
          "english studies",
          "creative and performing arts",
          "linguistics",
          "text analysis",
          "bibliographic methods / textual studies",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction This paper describes and implements a computational procedure for semantically analysing analogy in large bodies of text using a semantic annotation system based on the database of the Historical Thesaurus of English.1 In so doing, it demonstrates the value of a comprehensive and fine-grained semantic annotation system for English within corpus linguistics. Using log-likelihood measures on its semantically-annotated corpus of abstract popular science, the paper therefore demonstrates the existence, the extent, and the location of significant metaphorical content in this corpus. In so doing, it applies a version of Franco Moretti’s ‘distant reading’ programme in the analysis of literary history to non-narrative texts, as well as continuing work on integrating meaning into the methodologies of corpus linguistics.2   1.1. Analogy and Popular Science Following the 1980 publication of George Lakoff and Mark Johnson’s Metaphors We Live By,3 it has been frequently stated that human beings, as embodied minds perceiving the mental, social and physical worlds around them, understand abstractions in terms of concrete entities. While this is a well-explicated concept in cognitive linguistics and psychology, few studies have yet aimed to establish both the extent and operation of this in a large corpus of discourse. The standard methodology in cognitive linguistics tends to rely on introspection and the intuitions of native speakers, at the expense of empirical data.4 This lack of rigour has resulted in results which, though \"intuitively appealing\", are criticized \"for lacking a clear set of methodological decision principles\".5 Following earlier work we have undertaken on the investigation of analogy and metaphor in English from empirical groundings,67 in this paper we discuss a methodology for identifying these textual phenomena automatically, and in so doing aim to open up cognitive linguistics to more digital humanities techniques, in addition to demonstrating the use of automated semantic annotation and disambiguation techniques at an unprecedented level of granularity.   1.2. The Corpus We take as our initial data two book-length popular science texts which focus on explaining abstract concepts to a non-specialist audience, and therefore provide the greatest potential for the analysis of non-literary analogy - metaphor theory tells us that these should therefore be rich in non-abstract analogies. The corpus is therefore made up of Brian Greene's 2004 The Fabric of the Cosmos and Marcus du Sautoy's 2003 The Music of the Primes, although we have subsequently tested the methodology on other popular science texts. Through the procedure we describe in 3.1 below to analyse metaphor and analogy in these texts, we identify a range of domains which are unusually frequent in these texts and which are not pertinent to their subject matter (that is, not in the areas of physics, mathematics or general science). We then demonstrate in the remainder of section 3 that these domains are those analogies used systematically and consistently across the texts to elucidate and explicate the abstract concepts the books are focused on discussing. In order to do this, we identify all the semantic domains mentioned in these texts at very high levels of precision, using an annotation system built around the unprecedented detail found in the database of the Historical Thesaurus.    2. Semantic Annotation Semantic tagging and annotation is, we argue, the best solution we have to address the problem of searching and aggregating large collections of textual data: at present, historians, literary scholars and other researchers must search texts and summarize their contents based on word forms. These forms are highly problematic, given that most of them in English refer to multiple senses – for example, the word form \"strike\" has 181 Historical Thesaurus meaning entries in English, effectively inhibiting any large-scale automated research into the language of industrial action; \"show\" has 99 meanings, prohibiting effective searches on, say, theatrical metaphors or those of emotional displays. In such cases, much time and effort is expended in manually disambiguating and filtering search results and word statistics.  To resolve this problem, we use in this paper an early version of the Glasgow-Lancaster Semantic Annotation System, which we are currently developing at both of those universities. GL-SAS is a tool for annotating large corpora with meaning codes from the Historical Thesaurus, enabling us to search and aggregate data using the 236,000 precise meaning codes in that dataset, rather than imprecise word forms. These Thesaurus category codes are over one thousand times more precise than USAS, the current leader in semantic annotation in English corpus linguistics.8 The system automatically disambiguates these word meanings using existing computational disambiguation techniques alongside new context-dependent methods enabled by the Historical Thesaurus' dating codes and its fine-grained hierarchical structure. With our data showing that 60% of word forms in English refer to more than one meaning, and with some word forms referring to close to two hundred meanings, effective disambiguation is essential to GL-SAS.    3. Results 3.1. Methodology The 600,000 word corpus we outline above were lemmatised and then processed through our annotation system, resulting in texts with each word being annotated with a Historical Thesaurus meaning code. We then aggregated those codes into a dataset which summarised the frequency of each meaning code in the text, and took that frequency list and compared it to a reference corpus made up of a 14m word corpus of random selections from Wikipedia, to provide a comparison against standard expository text. Our comparison was based on a log-likelihood significance measure,9 which identifies, to an acceptable degree, those semantic domains which are mentioned unusually frequently in our popular science texts by comparison to the reference corpus, and therefore indicates a text's \"key\" domains (where the log-likelihood values are greater than around 20)10 - those domains which reflect what a text is \"about\".11   3.2. The Fabric of the Cosmos Brian Greene’s 2004 The Fabric of the Cosmos discusses theoretical physics and its relation to the concepts of space and time. Its key semantic domains are given in Table 1:    HT Category Category Name  Log-Likelihood Value    01.05.07 Space 13655.8    01.05.07.01 Distance  6344.8   01.04.07.05.04.08  Photon  4912.5    01.05.06.07  Computation of time  3603.5     01.02.09.15    Spinning textiles    3193.5      03.11.03.01.08.02    Stringed instruments    2277.7      03.11.03.02.09.14    Pattern/design    1949.8      01.02.09.14.01.03    Woven fabric    1922.2     While the first four domains are within the Thesaurus categories which refer to the text's topic, and therefore expected, the next four (in bold) are not immediately relevant to the book's topic. Looking for these domains in the text itself, chunked into 591 smaller files of 320 words each, we get a distribution like this:   Fig. 1: Analogical textual clusters in The Fabric of the Cosmos, shown by frequency of key semantic domains  (Here, the Thesaurus codes have been replaced by words representing those categories, for ease of reading.) The peak three-quarters of the way through the text indicates an area rich in mentions of textiles, and looking at this point in the text we find passages such as: Since we speak of the ‘fabric’ of spacetime, the suggestion goes, maybe spacetime is stitched out of strings much as a shirt is stitched out of thread. That is, much as joining numerous threads together in an appropriate pattern produces a shirt’s fabric, maybe joining numerous strings together in an appropriate pattern produces what we commonly call spacetime’s fabric. Matter, like you and me, would then amount to additional agglomerations of vibrating strings.12 The areas we have identified through the log-likelihood analysis are therefore those areas rich in metaphors of fabric and strings (as other examples show) which are used by the author to discuss physics. We can therefore use this technique to pinpoint areas of significant use of metaphor or analogy in a text.   3.3. The Music of the Primes As a check of the methodology, the same technique shows that in this particular book, which discusses prime number theory, there are highly key domains of travel and landscape in use alongside mathematical terms. Going to sections particularly rich in these domains gives analogical content over a long stretch, introduced by the following extract: Gauss’s two-dimensional map of imaginary numbers charts the numbers that we shall feed into the zeta function. The north-south axis keeps track of how many steps we take in the imaginary direction, whilst the east west axis charts the real numbers. We can lay this map out flat on a table. What we want to do is to create a physical landscape situated in the space above this map. The shadow of the zeta function will then turn into a physical object whose peaks and valleys we can explore.13   4. Conclusion We therefore demonstrate in this paper the use of a very fine-grained semantic annotation system, and establish the utility of such detailed annotations by describing a digital technique for discovering not only the existence of systematic metaphorical content but also its location and where it clusters. We believe that this result is significant in its own right, particularly for scholars of metaphor or cognitive linguistics, but we will also show that this represents only one of the uses to which highly-granular semantically annotated data can be put.  ",
       "article_title":"Metaphor, Popular Science and Semantic Tagging: Distant Reading with the Historical Thesaurus of English",
       "authors":[
          {
             "given":"Marc",
             "family":"Alexander",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Jean",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Alistair",
             "family":"Baron",
             "affiliation":[
                {
                   "original_name":"Lancaster University",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Fraser",
             "family":"Dallachy",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Christian",
             "family":"Kay",
             "affiliation":[
                {
                   "original_name":"University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Scott",
             "family":"Piao",
             "affiliation":[
                {
                   "original_name":"Lancaster University",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Rayson",
             "affiliation":[
                {
                   "original_name":"Lancaster University",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "corpora and corpus activities",
          "linguistics",
          "semantic analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Moving Towards Community Digital Heritage Rural areas are characterised by a strong identity of people with place. These identities draw on a repertoire of cultural norms, knowledge, histories, customs and practices which, taken together, construct unique place identities. This cultural distinctiveness is dynamic given traditional cultural practices are reproduced and others introduced as cultural systems evolve and adapt. Forms of cultural expression, such as story-telling, music and song, poetry and literature, dance and drama together with material objects, artefacts, sites and cultural spaces, are resources for interacting with the past and for experiencing the present. In the collection and transmission of these collections there has been a growing sense that the traditional methods for doing this are failing, Nora [1]. In order to address this problem, digital solutions have been sought but this has been a problematic process due to a number of variances. These include the constant changing of file types, software and codes of best practice, as well problems to do with cost and the sheer amounts of ‘analogue’ data to convert. Leading the way in this process have been national institutions but with the production of such local cultural repertoires, which as Flynn [2] suggests ‘are the grassroots activities’ where ‘control and ownership of the project is essential’ there has been a failure to consider the needs of community heritage groups in these processes. As such groups do not want to be subsumed into national archives, which they do not control, is not sensitive to their needs and is juxtaposed ideologically to the production of their own ‘place history’. Following Creswell’s [3] claim that such archives represent ‘spaces  of marginalized memory’ CURIOS is therefore seeking a solution using open linked data in which a system can be developed that is attuned to the specificity of a local heritage but can also take advantage of already collected materials from elsewhere.   2. Case Study – Hebridean Connections In the past 40 years around 22 ‘Comainn Eachdraidh’[1] (CE), have been established in the Outer Hebrides[2]. CE are community run groups that began in the 1970’s with a very specific political and cultural purpose – to preserve the culture, history and language of the primarily Gaelic regions of Scotland. Such community heritage practices have been described as a ‘messy’ endeavour with a wide variety of different formal and informal practices [5]. The archives embrace different registers of social memory from tangible to intangible heritage, which have been collected and ordered in a variety of different ways. Different CE groups collect and order their archives in a variety of different ways: from the highly ‘professional’ to the more bespoke and sporadic. As the CE groups are voluntary community archives, they are rooted in local historical values, hence there is often little consistency between groups regarding cataloguing, archiving and content management.  Hebridean Connections (HC), which is a community managed, online historical resource was formed due to the driving force of a single member of a CE who saw the benefit of digitising and connecting the different historical catalogues [5]. The idea was proposed to the different CE and four groups were actively involved in a Heritage Lottery Fund (HLF) bid that funded the creation of the HC website[3]. The project website was launched in 2006, holding some 100,000 records relating to the genealogy, history, archaeology, and cultural traditions of the Outer Hebrides. Currently, the system allows users to search using keywords, selecting relevant images, or with a map-based interface. Additionally, the website encourages contributions from its users and, therefore, has the potential to foster reciprocal knowledge exchange across geographical boundaries.    2.1 Sustainability HC is one example of a community-built digital cultural heritage repository where their long-term future is unclear. Many issues with the current system have arisen since the initial grant, particularly surrounding funding and scalability. There is a real practical question about how a project of this kind can be maintained over time with the resources available to a small-dispersed community, especially as the initial system was developed by a private development company, using proprietary software. As the project developed, this situation raised the problem that any changes to the system required more financial investment in the software. For the small community heritage groups involved, this was not feasible, especially as the CE became aware of what was possible through digitisation and wanted to expand. The process of digitisation has created three primary issues for HC:  How to expand the project remit without additional funding for developers?  Scalability issues, how can more CE collections be integrated in a closed system Can empowering communities to control their own digital heritage improve long-term sustainability?    2.2 An Archive for the Future? Motivated by the limitations of the current HC system, the CURIOS project’s aim is to produce a sustainable system that allows a community of users to manage a digital archive of cultural heritage data, or ‘cultural repository’, releasing them from any specific proprietary software platform. To achieve this goal, CURIOS has made use of existing open source content management system (CMS) software and Semantic Web standards. The emergence of the Semantic Web [6] has led to several standard formats for representing and interchanging data [7, 8]. By making use of linked data, cultural repositories would have the potential for reuse and integration with further related data sources. In recent years content management systems have gained popularity on the web by allowing users to build and publish web pages without requiring in-depth knowledge of the underlying web technologies. The CURIOS project has extended the web CMS approach to allow users to manage repositories of linked data. This Linked Data CMS approach makes use of existing CMS software to retain the usability and scalability of existing tools that are familiar to users, whilst allowing the users to exploit the benefits of linked data.  The Linked Data CMS approach has been implemented as a module for the popular open source Drupal CMS[1]. Building the next generation of Hebridean Connections on open source software and web standards has distinct advantages for future development and use of the system. The Drupal-based system can be maintained by its community of users and can be extended additional functionality developed by the Drupal open source community, e.g., to support blogging or e-commerce features. This community led maintenance allows for further future development of the cultural repositories as the archives develop.   3. Conclusion Open linked data can help make local cultural repositories sustainable and collective. Linked data allows for collaboration, mutual authoring, distributed responsibilities through community projects and the utilisation of other community or national resources [4]. The CURIOS project is enabling local cultural heritage repositories to become a meaningful identity resource for an international community, who previously had no access to them. By falling outside of national institutional frameworks, local people are the 'gatekeepers' of their own heritage and are selecting what to commemorate based on their own customs of remembering. This kind of digital archive can have, therefore, potentially significant social impacts which need to be better understood. The vision of Hebridean Connections is to expand the collections to incorporate those held by other Comainn Eachdraidh. Additionally, by making use of linked data, there is now the possibility to integrate further sources of data into HC from other historical societies or even national organisations.    4. Acknowledgments We would like to thank Hebridean Connections and the Comainn Eachdraidh for their ongoing commitment to this research. This work is supported by the Rural Digital Economy Research Hub (EPSRC EP/G066051/1).  ",
       "article_title":"CURIOS: Connecting and Empowering Community Heritage through Linked Data",
       "authors":[
          {
             "given":"David",
             "family":"Beel",
             "affiliation":[
                {
                   "original_name":"University of Aberdeen, United Kingdom",
                   "normalized_name":"University of Aberdeen",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/016476m91",
                      "GRID":"grid.7107.1"
                   }
                }
             ]
          },
          {
             "given":"Gemma",
             "family":"Webster",
             "affiliation":[
                {
                   "original_name":"University of Aberdeen, United Kingdom",
                   "normalized_name":"University of Aberdeen",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/016476m91",
                      "GRID":"grid.7107.1"
                   }
                }
             ]
          },
          {
             "given":"Chris",
             "family":"Mellish",
             "affiliation":[
                {
                   "original_name":"University of Aberdeen, United Kingdom",
                   "normalized_name":"University of Aberdeen",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/016476m91",
                      "GRID":"grid.7107.1"
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"Wallace",
             "affiliation":[
                {
                   "original_name":"University of Aberdeen, United Kingdom",
                   "normalized_name":"University of Aberdeen",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/016476m91",
                      "GRID":"grid.7107.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "software design and development",
          "and discovery",
          "resource creation",
          "cultural studies",
          "folklore and oral history",
          "cultural infrastructure",
          "interdisciplinary collaboration",
          "information architecture",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Within the Digital Humanities, there is a long history of debate and discussion as to how texts are accurately represented in digital form. Arguments as to how texts are encoded in both a logical and semantic sense are a recurring feature of past DH conferences.   Yet the intense intellectual focus on the precise details of marking up small corpora or even individual texts has masked the fact that issues related to the representation of large corpora of digitised materials - books, manuscripts, newspapers, records etc. - have been too often ignored.  Libraries, archives, museums and other collection institutions have now been digitising corpora of material for many years, but with a very few exceptions, it is still quite rare for an entire run of primary sources to be digitised and made available online. This means that there are gaps within the digital record.  Yet it is unusual for online resources to actively demonstrate these gaps; resources may be advertised as a growing corpus, but when searching through or downloading a digital resources there is rarely any indication of what has not been digitised. This skews the sense of the nature of the collection the scholar is working with and erodes trust.  This problem is compounded by assumptions made by end users that when a search is made in a digital resource, they actually are searching over everything in the original archive.  In most cases, this is far from being the case.   This long paper looks at this problem in the context of the Europeana Newspapers project (www.europeana-newspapers.eu), a three year, four million euro project, which is creating full-text for 10m pages of digitised newspapers from 12 libraries across Europe, and also developing an interface to allow for cross searching of over 18m newspaper pages. The final interface, available from the European Library in 2014 (www.theeuropeanlibrary.org), will also provide keyword searching over the OCRd (Optical Character Recognition) text and allow users to compare different newspapers from around Europe published on the same day. While it is an ambitious project, it is only a drop in the ocean of the overall number of digitised newspapers in Europe (a conservative calculation within the project put the number of digitised newspaper pages in European libraries at 130m ). What appears on the final interface will only be a sample of what actually exists in European libraries.   Moreover, other issues - political, economic, legal and technical - mean that the quality and national distribution of newspapers in the project (and therefore represented in the final online interface) are unevenly balanced. For the resource to be trusted by the academic community, this lack of balance must be acknowledged In terms of the economic and legal issues, the project is integrating newspapers from 12 existing newspapers online libraries, each of which have different business models. These different business models affect the final project interface. The National Library of Turkey and the British Library newspapers operate behind a pay wall, for instance - therefore the final Europeana Newspapers site will not be able to directly show images from their collection. Other libraries are wary of sharing full-resolution images, with the legitimate fear that the users will no longer visit their own national website. In such cases, only fragments of their newspaper images will appear in the central site. Legal issues are also pertinent; some libraries are unsure of the copyright status of some of their historic newspapers and therefore do not want to commit to allowing another entity to publish them In addition, there are several technical issues impeding uniform access to the resources. Nearly every digital newspaper collection today contains full-text derived from automatic processing with OCR software. But while some newspaper repositories grant access to the full-text, often the full-text is hidden and only exposed as an index for searching, but not available to the end user for online display or (programmatic) download, or sometimes not even for indexing by Google.  In other cases, full-text is made available, but not for the entirety of the collection, either due to IP issues or because the content holder took a deliberate decision not to show the full-text to the user, often because of the amount of error rate in the OCRd text. Regularly there is no sufficient information provided about the OCR error rate of a particular digital resource, which makes it even harder to assess what amount of the content can realistically be retrieved through a full-text search. There are also different ways how digital facsimiles are made accessible. Many recent online newspaper portals use the JPEG2000 image file format. The benefit of this is the ability to zoom more or less seamlessly in and out of the digital facsimile. But since JPEG2000 has not been around for a very long time in the digitisation community, many collections that have been digitised in the past are only available in TIF format. This means that zooming can only be provided in a static way on these images, e.g. through different resolution JPEGs. As a result, it is often not possible for researchers to explore these legacy resources in much the same way as they do with recently digitised materials.  In other cases, digital facsimiles have been produced by capturing existing microfilm copies rather than the original source material, thus the digital versions expose artefacts that were not present in the original paper source, but only introduced in the microfilm. However, this type of provenance is most typically not available to end users who are left alone in their interpretation of the differences in resource presentation and functionality. Finally, the metadata standards used to describe the digital contents also vary. Not only are there different representations in use for encoding full-text such as plain text, ALTO or TEI. But also descriptive metadata is commonly encoded in different standards, and with different degrees of granularity. While standard bibliographic information such as the title or date of publication are commonly available, more specific information on, for example, a particular article or the names of persons or places occurring in it rarely are. Within the Europeana Newspapers project a subset of 2m pages out of the total 10m will be refined further down to the article level, thus enabling more sophisticated search and retrieval functionality than the remaining 8m pages. A central point of this paper is that these issues are not just issues for librarians; it is not about showcasing how a digital resource is. Rather it is the urgent need to demonstrate how such issues have a profound effect on the academic community’s engagement with online resources. If a researcher wants to conduct a comparative analysis of newspapers in Chronicling America (the US historic newspaper site), the National Library of France and the British Library, she will have to use three different interfaces with different levels of content and metadata quality. Moreover, she will also have to grasp the particularities of each of these collections with regard to their quality and completeness and what that entails for her research. This paper will conclude with some recommendations for how those building digital resources can make their content choices more transparent. Informed dialogue between the cultural heritage organisations and the research communities is required. It calls for creators to tear down the illusion of completeness and help persuade end users that many digital resources are fragmentary things, where the representation of absence is just as important as representation of existence.   For a brief summary of the issue see Julia Flanders, ”Collaboration and dissent: challenges of collaborative standards for digital humanities” in Collaborative Research in the Digital Humanities (eds. Marilyn Deegan, Willard McCarty), 2012. The TEI mailing list provides ample evidence of such discussion listserv.brown.edu/archives/cgi-bin/wa?A1=ind1309&L=TEI-L. For instance, Johanna Drucker in “Performative Materiality and Theoretical Approaches to Interface”Digital Humanities Quarterly (2013, Volume 7 Number 1) and also “Humanities Approaches to Graphical Display”Digital Humanities Quarterly (2011, Volume 7 Number 1)  addresses theoretical concerns relating to the interface but with less focus on its practical representation within online resources. The issue has received much more attention in the world of 3D visualisation, e.g. with the creation of the London Charter (www.londoncharter.org). See “History, Digitized (and abridged)” for a summary of the extent of digitisation in 2007. www.nytimes.com/2007/03/10/business/yourmoney/11archive.html?pagewanted=all&_r=1. One of the findings in, Reinventing research? Information practices in the humanities, Research Information Network, 2011, www.rin.ac.uk/our-work/using-and-accessing-information-resources/information-use-case-studies-humanities.  David Nicholas, Ian Rowlands (2007), Google Generation, Paul Huntington www.jisc.ac.uk/whatwedo/programmes/resourcediscovery/googlegen.aspx. Alastair Dunning, European Newspaper Survey Report, 2012, www.europeana-newspapers.eu/wp-content/uploads/2012/04/D4.1-Europeana-newspapers-survey-report.pdf. For a comparative study of search ranking of digital newspaper repositories see Digital collections: If you build them, will they visit?, Frederick Zarndt et. al., IFLA WLC2013, Newspaper and Genealogy Section, Singapore, www.ifla.org/files/assets/newspapers/Singapore_2013_papers/day_1-_01_xzarndt_frederick_et_al_digital_collections.pdf. Jan Hillgärtner (2013), Digitalisierte Zeitungen und OCR: Welche Forschungszugänge erlauben die digitalen Bestände?, 18/03/13, newsphist.hypotheses.org/23. For a study in the methodology and analysis of digitised newspapers vs. paper copies see  The Digital Turn. Exploring the methodological possibilities of digital newspaper archives, Bob Nicholson, in Media History Vol. 13, Issue 1 2013, Special issue: Journalism and History: Dialogues.  For an example of this issue within the Digging into Data projects see One Culture. Computationally Intensive Research in the Humanities and Social Sciences A Report on the Experiences of First Respondents to the Digging Into Data Challenge,Christa Williford and Charles Henry, (2012) www.clir.org/pubs/reports/pub151 and also the aforementioned Reinventing research? Information practices in the humanities.    ",
       "article_title":"Representation and Absence in Digital Resources: The Case of Europeana Newspapers",
       "authors":[
          {
             "given":"Alastair",
             "family":"Dunning ",
             "affiliation":[
                {
                   "original_name":"The European Library, Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Clemens",
             "family":"Neudecker",
             "affiliation":[
                {
                   "original_name":"The KB National Library of the Netherlands, Netherlands",
                   "normalized_name":"National Library of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02w4jbg70",
                      "GRID":"grid.425631.7"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "and discovery",
          "corpora and corpus activities",
          "resource creation",
          "interface and user experience design",
          "sustainability and preservation",
          "repositories",
          "GLAM: galleries",
          "libraries",
          "archives",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" The method of distant reading - term proposed by Franco Moretti (Moretti 2005 , Moretti 2013) , or – using another term – macroanalysis (Jockers 2013) has become a mainstream in digital humanities in last couple of years. The general idea of the approach is to gain new knowledge about literary and cultural processes with the help of digital and quantitative models applied to all sorts of language or literary resources. In our paper we follow the distant reading method focusing on the phenomenon of naïve poetry – poetical opuses, composed by non-professional poets and distributed on special web- sites. One of them – Russian stihi.ru (stihi means 'verses') – has nowadays become a giant collection of diletant literature with more than 5000 authors, about 21 mln of works and everyday update. It, thus, can be regarded as an extraordinary cultural linguistic resource made by crowd sourcing. At the same time Russian National Corpus resources possess a unique resource - Poetic corpus (Grishina et al. 2009; http://ruscorpora.ru/search-poetic.html). In contrast to stihi.ru, Russian Poetic Corpus has been collected and marked up by the team of experts in linguistic and literary studies and it presents Russian poetical classics from the 18th century till the early 1930th. Comparison of the two poetic resources of naïve and classical poetry gives us an excellent possibility to use quantitative analysis to get some promising insights. We can understand more about the nature of literary imitations and epigone writings, the foundations and circulations of literary canon, the mechanisms of prosaic/poetic language shifts and many other topics related to sociology of textual culture that could not be studied with traditional methods. Some steps taken in this direction are presented in this paper. We look first of all at frequency measures in both resources and analyze the revealing fluctuations of different word frequencies. We use the frequency list of Russian National Corpus (called below the general frequency list) as a controlling benchmark, that helps us to separate the words, which are most frequent in common Russian Lexicon, from those, which get high frequency exclusively in Russian verses, high or naïve. Then we make a qualitative analysis of the nouns that occur in the top 100 of each frequency list. We identified a range of semantic domains that can be expressed by these nouns and compared the domains of each poetical resource. As a result we defined three main strategies of naïve composition. PreparationFor our research we have taken a sample of 50 mln word usage from naïve poetic corpus, which makes a representative corpus of more than 54 thousand authors. As our main aim was to find out what poetic patterns from high poetry are apt to be borrowed and imitated, we decided to extract a subcorpus of most typical imitating poetry. We searched for the authors who would bear in mind high poetical examples and try to go with them in their composition. The sorting has been made automatically. First we extracted all the bigramms from the high poetical corpus, and then we took only those documents from the naïve corpus, which a) have at least 50% bigrams that coincide with the bigrams of the high poetry list, b) are longer than 20 bigramms. Our final sample consisted of almost 9 mln word usage. The high poetic corpus has about 8 mln word usage. We lemmatized all the words in both corpora. After lemmatization the naïve poetic corpus consisted of 84 thousand lemmas MethodologyWe conducted three analyses based on the comparison of the three frequency lists: the frequency list from naïve poetic sample, the frequency list from Russian Poetic Corpus and the general Russian frequency list based on Russian National Corpus. In the first experiment we compared the    change of ranks of very high frequent words in the naïve sample relatively to high poetic and general lists. Secondly we considered outliers of naïve poetry frequency list: those words that demonstrate dramatically different frequency behavior. The last experiment consisted in relating all the top 100 nouns of each frequency list to semantic domains, that they are most probably used for. Then the contents and the variety of each domain in each list has been analysed. ResultsThe interpretation of the resource comparison results can be summarized by defining three basic strategies in naïve poetry: imitation, self-actualization and naming. Each of the strategies will be illustrated below by data examples. 1. The top frequency list of naïve poetical resource shows interesting deviation both from high poetical corpus list and general frequency list (see table 1) Word (in Russian)Word (translation)Position rank in naïve listPosition rank in high poetical listPosition rank in general frequency listИand 1 1 1 Я I2  3 5 не not 3 4 3 в in 4 2 2 ты you 5 7 33 то this 6 5 23 что that 7 11 9 быть be 8 10 6 на on 9 6 4 как as 10 9 19 с with 11 8 8 мы we 12 1318  а but 13 17 10 мой my 14 15 60 но but 15 14 16 так so 16 27 30 за for|behind 17 22 24 любовь love (noun) 18 52 307 любить love (verb) 19 66 181As we can see from the table, the naive poetry demonstrates important lexical features, some of them are specific, and some of them are typically poetic, being shared with the list of high poetry frequency. We observe an interesting tendency at the very top, where personal pronouns I and you displace the most common propositions in and on from the second and the forth positions correspondingly. Both pronouns I and you can be considered as lexical traits of poetical discourse. But the naïve poetry shows higher ranks for both of them (2 vs 3 in high poetry, and 5 vs. 7 in high poetry correspondingly). We see increase of frequency of those words which are already indicative for high poetical frequency list. The tendency to intensify specific poetical lexical features can be called the imitative strategy. The rank shift of the words love (noun and verb) is even more straightforward manifestation of the same strategy. While in general frequency list those words are not even in top 100, they occupy 52 and 66 positions in high poetical list and so far being an etalon of poetical shift they become the most frequent words in naïve poetry 2. The table below shows 5 nouns which have the biggest difference of ranks between naïve poetry list noun frequency of the Russian Poetical corpus (see table 2)  word word (translation)Position rank in naïve list  Position rank in high poetical list Position rank in general frequency list фото photo 971 30827 3400 сигарета cigarette 93928957 2109  проблемproblem 610 12572 197 мама mum 330 2125 309  девчонкаbabe, gal (derivative from girl) 865 5030 2563 These words behavior is various: some of them are more frequent comparing the general frequency list (photo, cigarette), some of them stay on roughly the same rank (mum), some are more rare than in general, but still show immense difference with the high poetical list (problem). The gap between naïve and poetic frequency list signals that there are some semantic zones where naïve poetry seems to be independent from the classical poetic canon. This trend can be defined as a self-actualization strategy which is in some sense opposite to the imitative strategy.  3. We took top 100 nouns of every list and compared their lexical distribution . The nouns had been grouped into abstract semantic domains. Some words could be associated with several domains due to their polysemy. As a result we have identified 13 semantic domains, 12 of them are shared between naïve poetry, high poetry and common frequency lists and the 13th is not presented in the naïve poetry list. The domains we have defined are as follows: Mankind (everything that may characterize a person: soul, beauty, name, heart, strength etc.), Body, Emotions, Mind, Existence (God, world, truth, time, fate etc.), Speech, Person (father, son, friend, enemy etc.), Event (love, happiness, past, disaster etc.), Time, Nature, Geography (road, hill etc.), Home (window, door etc.). The 13th domain which is found only in the high poetical list and in the general frequency list is Social and it includes such words as people, labor, fame in the poetical list, and state, money, law etc. in the general list. Analysis of the overlaps and varieties of the naïve and high poetical lists showed differences in the elaboration of the domains in two corpora. The general frequency list helped to draw out the words that are commonly frequent and their presence in the list cannot be understood as a signal of the poetic concentration on the domain. The results are demonstrated on the graph below: As we can see from the graph, there are three zones of the naïve poetical sample that demonstrate high lexical variety of frequent nouns in comparison to the poetical corpus. These are Emotions, Event and Speech. Most of the words of those domains are not frequent in general lexicon. The lexical multiplicity can be explained by extensive strategy: the naïve poets do not use sophisticated verbal apparatus to express the conceptual space of the verse, but prefer straightforward lexical naming (pain, wish, encounter, grief, love, question, answer etc.) ",
       "article_title":"Distant reading of naïve poetry: corpora comparison as research methodology",
       "authors":[
          {
             "given":"Anastasia",
             "family":"Bonch-Osmolovskaya",
             "affiliation":[
                {
                   "original_name":"National Research Unversity Higher School of Economics Moscow, Russian Federation ",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          },
          {
             "given":"Boris",
             "family":"Orekhov",
             "affiliation":[
                {
                   "original_name":"National Research Unversity Higher School of Economics Moscow, Russian Federation ",
                   "normalized_name":"National Research University Higher School of Economics",
                   "country":"Russia",
                   "identifiers":{
                      "ror":"https://ror.org/055f7t516",
                      "GRID":"grid.410682.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "corpora and corpus activities",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:    Discuss why is it important to recover forgotten tools and the discourse around these instruments,   Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,   Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,   Discuss the command language developed by John Smith for interacting with ARRAS, and   Conclude with a more general call for digital humanities archaeology.    Zieliniski and Media Archaeology  Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen.  Busa and Tasman on Literary Data Processing  The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.  Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually   get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time.  Glickman and Stallman on Printed Interfaces  The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use.    Fig. 1: Example of PRORA output from the Manual  Smith and Interaction  One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading.  Conclusions  In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage.   ",
       "article_title":"Towards an Archaeology of Text Analysis Tools",
       "authors":[
          {
             "given":"Stéfan",
             "family":"Sinclair",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Geoffrey",
             "family":"Rockwell",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada ",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Introduction In distributed systems literature the orthogonal but interdependent characteris- tics of autonomy, distribution and heterogeneity are used to classify distributed systems [1,2]. From a holistic perspective on the arts and humanities, collections have evolved over decades or centuries from highly autonomous disciplines and institutions and are widely spread, which resulted in heterogeneous perspectives and data models [3]. Despite its negative notion as data integration problem, the term heterogeneity also symbolizes the diversity of research methodologies within the disciplines of the arts and humanities. Resolving heterogeneity hence implies an abstraction from the specifics that are valuable for focused disciplinary and interdisciplinary research projects. Our approach presents a novel concept for data federation in the arts and humanities, which focuses the needs of research projects as well as interdisci- plinary and broad use-cases. We especially address the reusability of explicated knowledge on correlations between schemata and digital collections and show where domain experts are required to bridge semantic gaps. 2 Background Approaches to data integration often follow the theoretical foundation expressed in [4] by employing the concept of a global view. While being highly distinctive in terms of their underlying concepts, established examples such as ISIDORE [5], OAIster [6] and Europeana [7] share the goal of facilitating access to a wide range of research data through integrated schemata or ontologies. Aside from broad services, an integration need that focuses on a specific topic and related research questions is addressed by the Steinheim Institute, which provides a search in the context of german-jewish history and judaism [8] Despite usability concerns in having to identify relevant services and accord- ingly collections, the reappearing need to overcome the same aspects of hetero- geneity in reaction to new use-cases is one of the problems we address. 2.1 Use cases  Our federation concept is primarily focused on the realization of the DARIAH- DE Generic Search (http://dev3.dariah.eu/search), which includes support for queries over large sets of unre- lated collections (broad search) and tightly correlated data (deep search)—with different information needs in mind:  – Broad search: Due to the quantity and distribution, the relevance of digital collections for particular research questions is not easily assessable. A broad view assists scholars in finding and evaluating possibly relevant data. Figure 1 shows an exemplary, collection-level aggregation of results based on term statistics in the prototype of our search, which will be continuously extended by other relevant visualization techniques (e.g. with respect to spacial and temporal aspects).  – Deep search: If the granularity of local data models can be used to formulate more specific queries targeting structure and content (e.g. in search facets), the deep search utilizes mappings specified in the DARIAH-DE Schema Reg- istry. Broad search continuously fades to deep search with an increasing count and richness of mappings and hence typically smaller sets of selected collections.    Fig. 1: Result aggregation in the generic search  Despite the focus on the generic search with its virtual integration at query-time, the proposed concept also addresses requirements of a materialized integration of data:  – Data migration and consolidation: Traditional applications of data integra- tion often do not require a dynamic adaption to selected collections, but de- termine a set of relevant data sources and an appropriate integration schema or ontology [4]. Examples include data migration induced by the introduction of new information systems (e.g. replacement of outdated archive informa- tion software) or the consolidation of selected data sources under a merged schema for the purpose of interdisciplinary analysis and visualization e.g. in the DARIAH-DE GeoBrowser [9].   2.2 Problem Definition  The common objective of data integration approaches is to resolve heterogeneity on various levels: Syntactical aspects such as the existence of different access and encoding methods can be solved by technical means, whereas structural and semantic heterogeneity depend on the application of background knowledge [1]. Despite continuing efforts in the fields of schema and ontology matching, the manual intervention of domain experts—especially for large or complex schemata and ontologies often found in the arts and humanities—has shown to be essential to generate high-quality results [10].  The correlation of the used schemata and ontologies is an inherently complex manual task in our context, which depends on the fragmented and distributed knowledge of individual disciplines, collections and scholars. Requiring a common understanding, research projects concentrate knowledge about schemata and semantics used in relevant collections and specify meanings and correlations. In order to integrate the described data and establish technical interoperability, an application of digital methods and tools is required.  3 Concept Abstracting from aspects of technical and syntactical heterogeneity concerned with accessing, preprocessing and integrating data in a generic fashion, we aim to enable researchers to focus on those aspects of integration, that depend on their knowledge and expertise: the description and correlation of schemata and ontologies. Despite the immediate benefit for individual integration tasks, the centralized formalization and explication of semantics results in the significant advantage of knowledge reusability. 3.1 Semantic cluster The logical architecture of our idea is represented by a directed, weighed graph, where the schemata and ontologies are described by vertices, and mappings be- tween them are symbolized by edges. Whereas correlations between structural elements symbolize a relation of the described concepts (e.g. persons, locations) and could be considered undirected, more specific rules that are required for data transformation can be composed of non-reversible functions (e.g. the concate- nation of fields). For that reason, parallel edges are required for the description of both mapping directions. Differences of schemata in terms of their complex- ity and expressiveness reduce the achievable level of accumulated completeness, which is represented by the value of cohesion Figure 2 indicates how the cohesion between schemata can be utilized to sug- gest semantic clusters: C1, C2 and C3 could be the result of research projects, which needed a high level of mapping completeness between relevant schemata. By interrelating clusters or generically used schemata (S10), the expressed se- mantics can be reused in other contexts. 3.2 Use-case orientation  Our example indicates the difference to the commonly found integration pattern of a global ontology or schema. Despite its theoretical foundation, simplicity and proven applicability for broad integration use-cases [5,6,7], we consider the approach to be impracticable for a holistic context of the arts and humanities because a global structure would either have to be an abstraction from collection or discipline specifics or unmanageably complex.  Narrowing this context to individual domains or research projects, standards could be elected as appropriate integrative structures. As exemplified in figure 2, the schemata S3, S5 and S8 form the integration baseline within our clusters due to their cohesion with other schemata. Considering our deep search and data migration and consolidation use-cases, these schemata can be utilized to generate a fine-grained view over selected collections accessible within the cluster. In order to support interdisciplinary use-cases, clusters can be combined (symbolized by the strong cohesion between S5 and S8) to resolve semantic gaps.  For broad use-cases we rely on the collaborative and continuous emergence of schemata or ontologies (compare S10) within our federation that are used to connect the clusters on the coarse levels sufficient for broad use-cases.    Fig. 2. Semantic clusters of schemata  3.2 Use-case orientation3.3 Scalability considerations The simplicity of traditional data integration emerges as new local schemata are added to the system and hence an appropriate mapping target needs to be identified. To ensure extensibility and scalability, our proposed federation concept depends on two strategies: Cluster globals: The concept of semantic clusters builds on the existence or ad- vancement of standards that are considered as appropriate common perspectives by research communities. Although clusters are not predetermined but expected to evolve, established standards such as the CIDOC Conceptual Reference Model (CIDOC CRM) or the Text Encoding Initiative (TEI) Guildelines could be iden- tified as initial cluster schemata, which can be mapped in a generic fashion [11]. As new schemata need to be added, the standard which promises to achieve the highest completeness is selected to be mapped. Model inheritance: Our proposal includes an approach to specify the actual usage of schemata more precisely than it is possible at the level of generic cross- walks. Figure 3 shows the exemplary derivation of the Dublin Core element dc:coverage to resolve an encapsulated substructure. Mappings are inherited to correlate the refined elements or to specify detailed data transformation rules. As derived schemata are related to their parent, generic mappings remain valid and can be utilized if specific rules are missing.   Fig. 3: Exemplary derived version of Dublin Core  4 Conclusion As we abstract from technical aspects of heterogeneity and reuse the valuable disciplinary knowledge explicated in terms of correlations, processing and trans- formation rules, the efforts required for integrating research data can be sig- nificantly reduced. Another important aspect that is currently being evaluatedconsists in appropriate techniques for the visualization of our federation concept and system. After all, domain experts need to be able to recognize clusters, im- portant schemata and ontologies as well as their correlations in order to identify semantic gaps and to collaboratively fill them.  ",
       "article_title":"A novel approach for a reusable federation of research data within the arts and humanities",
       "authors":[
          {
             "given":"Gradl",
             "family":"Tobias",
             "affiliation":[
                {
                   "original_name":"University of Bamberg, Germany ",
                   "normalized_name":"University of Bamberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01c1w6d29",
                      "GRID":"grid.7359.8"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":"Henrich",
             "affiliation":[
                {
                   "original_name":"University of Bamberg, Germany ",
                   "normalized_name":"University of Bamberg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/01c1w6d29",
                      "GRID":"grid.7359.8"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "knowledge representation",
          "metadata",
          "and discovery",
          "resource creation",
          "sustainability and preservation",
          "repositories",
          "databases & dbms",
          "archives",
          "information retrieval",
          "digitisation",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"     Work begun by Huitfeldt and Sperberg-McQueen (2008) and continued jointly with Marcoux (2009, 2010) has given us a powerful and intuitive model of the abstract object T that counts as a successful transcription of an exemplar E.[1] (For convenience I will hereafter refer to these authors collectively as 'HSM' and their work as 'the HSM model'.)[2] Work on the HSM model is ongoing and a comprehensive formal account of the activity of transcription remains some way off, but extrapolating from what HSM have done so far we can begin to determine what is proper to a model of transcription per se, what is complementary to it, and what intra- and inter-model dependencies exist between entities. We can project, as it were, from the existing HSM model a bigger picture; but we stay always within the terms of the HSM model because it is the terms themselves we use for our projection. We thereby also clarify the terms themselves as they are used in the context of the model. Here I focus on six terms fundamental to the 'bigger picture' and by elucidating these terms in relation to the model I sketch out the scope and composition of that picture: a necessary preliminary to any more detailed modelling to follow. The terms are: SURFACE, MARK, READING, TOKEN-SEQUENCE, EXEMPLAR, and DOCUMENT. Consider the following scenario. A cave explorer discovers a new chamber and finds inside it three rock faces. One has complex patterns of scratches on it, each of the other two has painted lines on it. In all three cases what the explorer sees looks like writing, but she recognizes none of it. She takes a photograph of the rock face with the scratches and of one face with painted lines, then her camera fails. She pulls out a sketch pad and makes a faithful drawing of the other rock face with paint on it. Later she takes the photographs and drawing to an archeologist, who recognizes that the painted lines are indeed writing, in a language he knows, but that the forms of the characters are older and in many cases different from those in the current orthography of that language. He then makes written copies of the characters he sees in the photograph and drawing, this time using the current character forms to make them easier for other scholars to read. Doubtful about the scratched lines he informs a naturalist friend who visits the cave and confirms that all the scratches have been made by animals sharpening their claws on the rock face. Now we retrace the conceptual movement of that narrative, introducing the fundamental terms in the appropriate places and using the context to clarify their relation to the HSM model and define them within that scope. Each rock face is a SURFACE. A SURFACE is a thing: it is perceptible and measurable, and in its normal manner of existence can be returned to. The normal manner of existence of an electronic display, for example, is for a machine that generates it to be switched on and working properly: and while this is the case, we can return to the display.[3] A SURFACE is necessary for transcription; a SURFACE itself depends on nothing within the scope of this discussion. Of all things I describe here, a SURFACE is the closest to being a primitive entity. Each scratch and painted line on the rock faces is a MARK. A MARK is a thing made upon a SURFACE by some agent. It is perceptible and measurable by contrast to the SURFACE, and therefore dependent upon the prior existence of the SURFACE. The most complex of all the model-related entities is a READING. I will say it is normative - though not necessary - that READING is motivated. We are a communicative species and we actively look for instances of communication, willing to give the benefit of the doubt in many cases. MARKS are necessary for orthography, therefore the presence of MARKS implies the possible presence of writing. Consequently the presence of MARKS also implies the possible presence of text (here and throughout intended in the sense described in Caton 2013a), and text is written communication. Hence, in the normative case, an understanding of the possible presence of text motivates READING by an agent. In terms specific to the HSM model, READING is the process by which an agent attempts to discover and establish at least one TYPE-SEQUENCE in MARKS on a SURFACE by recognising certain MARKS to be certain TOKENS. Because (in the normative case) READING is motivated, we must also grant that it may be entirely speculative. That is, it is acceptable that READING commences solely on the basis that MARKS are present on a SURFACE: there does not have to be certainty that at least one of those MARKS has token status. Recall that our cave explorer could not assign token status to any of the marks she saw. Aware of her ignorance, she took steps (taking photographs and making a drawing) that have an interesting status in the overall picture because they seem to perform transcription without READING and therefore to deny that READING is  necessary  to transcription. But this is illusory. The goal of READING is to establish a TOKEN-SEQUENCE/TYPE-SEQUENCE, and the act of READING attempts this, assigning token status to MARKS where possible. There is no criterion of success for the activity: simply performing it is enough.[4] There doesn't have to be a specific TOKEN-SEQUENCE/TYPE-SEQUENCE at the end of it.  Instead of 'success', we distinguish three result-states of READING: positive, negative, and zero. A positive result-state means the agent assigns, with a greater-than-zero degree of certainty, token status to at least one MARK. A negative result-state means that the agent, with a greater-than-zero degree of certainty, decides no MARK has token status. A zero result-state means that for every MARK present the agent has zero certainty that it is or is not a token. We shall return to this important point shortly.  An agent READS at least MARK by MARK (though more usually by groups of MARKS at a time), assigns token status where possible, and thereby constructs a TOKEN-SEQUENCE. A TOKEN-SEQUENCE cannot be empty: it must contain at least one TOKEN. Under any one READING R a TOKEN-SEQUENCE is neither 'right' nor 'wrong': it simply is the sequence under that READING, irrespective of the degree to which it corresponds to any text present on the SURFACE. The dependence relation here is strictly one way and is of transcription upon the TOKEN-SEQUENCE produced by the READING. If no READING (minimal or informed) takes place,   If there is a TOKEN-SEQUENCE, and if an agent desires to preserve its corresponding TYPE-SEQUENCE in another place by the activity of transcription, then that TOKEN-SEQUENCE assumes the role of EXEMPLAR with respect to the transcription, and in that respect we refer to it as the E-TOKEN-SEQUENCE. The manifestation in another place of the preserved TYPE-SEQUENCE as a TOKEN-SEQUENCE produces a TRANSCRIPTION (as result) and we refer to that sequence as the T-TOKEN-SEQUENCE.[5] Should an agent wish to transcribe this T-TOKEN-SEQUENCE, the sequence would then assume the role of EXEMPLAR with respect to this second transcription.  The essential insight of the HSM model is that transcription is that the T-TOKEN-SEQUENCE represents and preserves the E-TYPE-SEQUENCE. It should be clear then that with respect to the painted rock faces the cave explorer does perform transcription, despite her inability to consciously assign token status to any of the MARKS. By means of the photograph and the drawing, each E-TYPE-SEQUENCE of the painted MARKS is preserved and manifested in another place as a T-TOKEN-SEQUENCE, perceptible as MARKS on a SURFACE. Transcription is always possible when the READING result-state is either positive or zero. It does not have to happen deliberately, consciously - a transcription can be produced quite by accident. The archaeologist's transcription, unlike the explorer's, comes from positive result-state READING and he produces different T-TOKEN-SEQUENCES from the explorer, but they all represent the same E-TYPE-SEQUENCE. Transcription is possible from a zero result-state READING, but only possible. The cave explorer's photograph of the scratches, for example, is not a transcription because no TOKEN-SEQUENCE is present on the scratched rock SURFACE and thus there is no E-TYPE-SEQUENCE to preserve.[6] An act of transcription necessarily involves an EXEMPLAR, but does not necessarily involve a DOCUMENT. In relation to a model of transcription, and the model of READING which is necessary for it, we say that when there is at least one TOKEN-SEQUENCE / TYPE-SEQUENCE on a SURFACE, and the same READING that assigned token status to the MARKS also assigns TEXT status to the TOKEN-SEQUENCE / TYPE-SEQUENCE, then the SURFACE + TEXT combination acquires DOCUMENT status.[7] In a majority of cases an agent READS a SURFACE either certain it is a DOCUMENT or at least believing that highly likely. Hence DOCUMENT is a term frequently used in discussions of transcription, but transcription can take place without any DOCUMENT being present. Notes [1] For work responding to and building on the HSM model, see Caton 2009, 2013b. [2] I must assume the reader's familiarity with the basics of the HSM model, in particular with their use of Peirce's concepts of token and type. Huitfeldt and Sperberg-McQueen 2008 gives the initial exposition; Caton 2013 provides a summary. [3] I am avoiding the words  'material' and 'persistent' because (for the purposes of this discussion) those adjectives are not yet well enough defined with respect to the digital domain. Despite my expressive clumsiness I hope the reader understands that I am opposing the nature of SURFACE and MARK to the essentially transitory, of-the-moment nature of a phenomenon such as speech. [4] Obviously this differs from the normal usage, where we expect someone performing the activity of reading to recognize a specific token sequence and consider their reading incorrect if they don't. [5] Strictly speaking there is no T-TYPE-SEQUENCE prior to the existence of the T-TOKEN-SEQUENCE, only the E-TYPE-SEQUENCE. The T-TYPE-SEQUENCE is a product of the T-TOKEN-SEQUENCE. [6] Because a negative result-state involves conscious judgement, it is entirely possible for one agent to perform two different READINGS with different result-states: one negative (by looking at MARKS on a SURFACE and deciding that none has token status) and one zero (by also taking a photograph of the MARKS). [7] Because this ties DOCUMENT to a particular SURFACE it means every DOCUMENT is a unique object and not a 'repeatable symbolic expression' as discussed in Renear and Wickett 2009.  I consider this uncontroversial as a constraint for the purposes of modelling, but I believe it also reflects a core sense of the common usage. Certainly 'document' is often used in an abstract sense, as in 'Magna Carta is an important document', but the signification is strongly tied to the idea of a particular piece of paper (or stone tablet, parchment scroll, email, etc.).   ",
       "article_title":"Six terms fundamental to modelling transcription",
       "authors":[
          {
             "given":"Paul",
             "family":"Caton",
             "affiliation":[
                {
                   "original_name":"King's College London, United Kingdom ",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The evolution of literary form and style is an emerging area of academic research and offers a valuable case study in cultural evolution generally. Several notable papers have appeared recently. In particular, critic Franco Moretti has offered a number of provocative claims concerning the relationship between genre evolution and demographic changes in the 19th Century reading public:  1. Due to the growth of the reading public, the British novel underwent an abrupt change circa 1820: novels became far more heterogeneous and generically differentiated, aimed at specialized niches rather than readers in general. 2. The average lifespan of genres is ~25-30 years, the same as a human generation. This historical rhythm results from generational turnover in readers. 3. Literary genre evolution is characterized by alternating cycles of divergence and convergence—that is, periods of increasing generic diversity and differentiation followed by periods of decreasing diversity.  Statistician Cosma Shalizi argues in a response, “Graphs, Trees, Materialism, Fishing,” that while Moretti identifies provocative historical patterns, he fails to fully articulate the mechanisms underlying and driving literary genre evolution.  The objective of this paper is to take up Shalizi’s injunction by building a computational model of possible generative mechanisms driving genre evolution. We consider the following questions:  How do the static characteristics and dynamic behavior of the ‘reading public’ affect literary genre evolution? How is generic diversity affected by reader diversity? Is there a phase change as the reading public grows? Under what circumstances will the life cycle of literary genres parallel the life cycle of generations?  We investigate these questions by constructing an agent-based model of two populations: (1) cultural forms (e.g., books); and (2) cultural consumers (e.g., readers). The key attribute of agents in each population is a bit string of user-specified length. For cultural forms, this bit-string represents the morphological features of the work: for instance, in the case of literature, bits represent attributes such as authorial style, length, plot, and theme.[1] For cultural consumers, the bit-string represents an individual’s ideal preference. Each consumer has a tolerance for variation from this ideal represented as an acceptable hamming distance. Individual cultural consumers are in turn organized into larger preference landscapes, which vary in their levels of structure, entropy, and reader diversity (see diagram).    Once the preference landscape has been constructed at set-up, a genetic algorithm is run on the cultural forms in order to simulate evolution. The fitness of each book is measured by the number of readers it receives in that time period.[1] High fitness books are more likely to survive and reproduce, increasing their influence on the content of the next generation of literary works. Three reproductive mechanisms are used:  Survival: books carry over from generation T to T+1 with no change  Asexual: individual bit-strings from generation T are copied with a user-specified probability of mutation to create a new generation of books at T+1 Sexual: pairs of bit-strings from generation T are spliced in order to create a new generation of books at T+1  While the use of genetic and evolutionary paradigms to describe bibliographic change may at first seem suspect, each of these reproductive strategies has an intuitive interpretation in the context of literary production. Survival corresponds to the case in which market-successful books are simply reprinted. Asexual reproduction corresponds to the case in which successful books spawn similar works with slight variation: that is, authors copy and modify the template provided by recently successful works. Sexual reproduction corresponds to what we might call “genre-crossing”: authors take the features of two successful works and synthesize them in order to produce a new work. The current trend of “mash-up” literature provides a salient example. Best-sellers such as Abraham Lincoln: Vampire Hunter splice the features of already-successful genres (e.g., historical biography and gothic). Lest we dismiss such works as gimmicks, it is worth recognizing that many high-prestige genres emerged through hybridization. Modernist works such as James Joyce's Ulysses self-consciously combined the features of the realist novel with those of the classical epic.  Pastiche, bricolage, and the combination of high and low art were central to postmodern literature, epitomized by William Burrough's “cut-up” novels. Recombination is a widely-used mechanism in literary production.  The relative proportions of these reproductive strategies are parameterized variables, as is the mutation rate, which represents the probability that any feature of a work will be mutated during either reproduction process. The mutation rate also has an intuitive interpretation in the context of cultural production: it characterizes the average creative experimentalism of a particular cultural field, that is, how far authors are generally willing to depart from established models. We run simulated experiments in order to determine the impact of various scenarios on literary genre evolution, including (i) variation in reader preference landscapes features, (ii) demographic changes such as population growth and generational turnover, and (iii) feedback between reader preferences and dominant cultural forms. The results suggest a number of insights about plausible mechanisms driving the evolution of cultural forms generally and literary genre specifically.   First, generic diversity[1] cannot be explained solely in terms of the characteristics of the reading public: we also need to account for the characteristics of the creative process, in particular, the level of experimentation in the cultural market at a given historical moment, represented in this model by the mutation rate.    Second, contrary to Moretti’s claim, we show that growth in the reading public is not sufficient to guarantee an increase in either reader diversity or generic diversity. In fact, market growth may actually reduce diversity under certain conditions. To determine the effect that growth will have, we need to know whether the preference landscape was initially homogeneous vs. diverse and whether new readers have preferences that are similar to or different from the readers who already populate that market.     Third, the model predicts that dramatic changes in the preferences of cultural consumers—analogous to ecosystem disruption—lead to increases in creative experimentation (i.e., the cultural mutation rate). Lastly, we find that the preferences of conformist consumers have a highly disproportionate effect on the level of generic diversity relative to the rest of the consumer population, producing ‘phase change’ dynamics. Genres and cultural product categories tend to form around the preferences of conformist consumers because they have more reliable and predictable tastes.   Although the model above addresses a set of claims about literary genre, the implementation is intentionally general, relying on abstract feature and preference strings that can represent any cultural product that can be atomized into variable features. Our intention in future research is to calibrate the model against case studies from a variety of cultural markets (literature, film, plastic arts, etc.).  ",
       "article_title":"Simulating the Cultural Evolution of Literary Genres",
       "authors":[
          {
             "given":"Graham Alexander ",
             "family":"Sack",
             "affiliation":[
                {
                   "original_name":"Columbia University, United States of America ",
                   "normalized_name":"Columbia University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj8s172",
                      "GRID":"grid.21729.3f"
                   }
                }
             ]
          },
          {
             "given":"Daniel ",
             "family":"Wu",
             "affiliation":[
                {
                   "original_name":"Harvard University, United States of America ",
                   "normalized_name":"Harvard University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03vek6s52",
                      "GRID":"grid.38142.3c"
                   }
                }
             ]
          },
          {
             "given":"Benji ",
             "family":"Zusman",
             "affiliation":[
                {
                   "original_name":"University of Florida, United States of America ",
                   "normalized_name":"University of Florida",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02y3ad647",
                      "GRID":"grid.15276.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "drama",
          "poetry",
          "genre-specific studies: prose"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The proposed talk will discuss the application of forensic computer science tools and methods to born digital documents and parts of archives, focusing on the philological benefit for genetic scholarly editions and the critique génétique on the one hand as well as on issues of sane archiving and representation of the digital record in a scholarly edition on the other. In the course of the talk, the conceptual impact of this digital forensic perspective on the term 'document', on our concepts of the 'materiality of the historical record' and on 'textual genetics' will also be discussed.  Exemplary subject matter of the inquiry will be a selection of recovered materials from harddrives in the Thomas Kling archive which represents the range of retrievable transitory 'genetic' textual material, e.g. recoverable documents, temporary files, memory fragments and several disk operation artifacts on multiple generations of operating systems reaching back from Microsoft Windows® XP to 3.11. The harddrives have been forensically imaged for longterm preservation by the author of this paper and by a forensic laboratory recently, and it will be the first time these findings are being publicly discussed.  Thomas Kling (1957-2005) was one of the most influential contemporary poets of the last 30 years in the German-speaking countries. The historical, documentary quoting technique as well as his blending of poetry and performance inspired numerous other authors of his generation. Furthermore, his poetry is enormously aware of the effect that historical media development has on perception, on the way how storage media influence the concept of history and on poetic language itself (s.a. Trilcke 2012). As early as 1985, he wrote: „[...] and everyone knows: from now on, we cast out poetry on floppy disks only, sure thing.“ (Thomas Kling: Die verplemperten Sprachen; Wehr et al. 2012: 13) At the same time, his work reflects not only the medial blending of historical levels and documents as „sondage“, but also the threat of losing the „burning archive“: „It is the tongue-loss. Everything is archive, everything is about to become archive and end up in smoke.“ (Kling 2001: 111) Fortunately, this does not apply to his own archive, nor to the harddrives of his last three laptops, all of which are being kept in the collections on the Raketenstation Hombroich (Scharfschwert 2012). After a quick introduction to general archival aspects and methods of forensic work with bitstream-preserving images and the several levels of analysis (different kinds of file recovery, drive slack analysis, save operation artifacts, restore points etc., s.a. Ries 2010, Kirschenbaum et al. 2010, Reside 2011), the talk will discuss a couple of example findings from the Thomas Kling harddrive platters to show in which – sometimes surprising – places of these 'real life' case systems textual variants and fragments of poetic draft material actually reside. Possible candidates for this part are digital fragments of the dossier génétique to the poems third cartography and it's abdomen in constant movement (selection to be finalised). The example materials will show Thomas Kling as one of the German poets who relatively early embraced IBM-compatible personal computers with Microsoft Windows®, Word® as a writing tool and used it for most of the draft process, going back and forth between the digital document and corrected printouts after a conception phase on notebook and manuscript pages. Discussion will show to what extent the philological interpretation of these findings depend on the specifics of the operating system- and application context and that we sometimes have to deal with 'artificial' evidence. Furthermore, the range of variation in terms of completeness of the textual record will be mapped. A tentative genetic close reading of the fragments will also show how the reader's view on the writing process necessarily shifts, coming from a manuscript perspective, as one reads e.g. recovered digital born memory snaphot items.  Looking at the materials from a scholarly editing point of view, questions arise how these should be represented in a genetic edition, e.g. which metadata has to be included; in which meaningful way can the commentary cover the technical context of the mechanisms of historic software, and how is redundancy of the digital record to be dealt with? (s.a. Pierazzo 2011) How is the 'materiality' of the materials to be represented? (s.a. Ries 2010, Kirschenbaum 2006) In this sense of the meaning, this talk aims to help „empowering“ scholarly editors, philologists and scholars as readers of future genetic editions to deal with digital heritage collections and digital born documents and material as part of scholarly editions.  Delivering this talk in German would be an obvious choice, as the discussed archive materials by Thomas Kling are also in German and I am a native speaker. However, the talk can be held in English as well to reach out for the international audience. Regarding the advantages of both options, I would like to leave the choice of language for this talk to the conference board.   ",
       "article_title":"Harddrive Philology: Analysing the Writing Process on Thomas Kling's Archived Laptops",
       "authors":[
          {
             "given":"Thorsten",
             "family":"Ries",
             "affiliation":[
                {
                   "original_name":"Ghent University, Belgium ",
                   "normalized_name":"Ghent University",
                   "country":"Belgium",
                   "identifiers":{
                      "ror":"https://ror.org/00cv9y106",
                      "GRID":"grid.5342.0"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "philology",
          "literary studies",
          "sustainability and preservation",
          "repositories",
          "archives",
          "scholarly editing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  To date, most studies that foreground quantitative analyses of literature have focused exclusively on prose writing (the novel in particular) rather than poetry (Stanford Literary Lab 2011, Clement 2008). In part, this state of affairs is due to poetry’s highly figurative language and complex communicative intent, which poses acute problems for text mining and similar quantitative analyses that rely on lexical and semantic meaning and whose methodological origins lie in the “hard” sciences (Pasanek and Sculley 2008, Bei 2008). Poetry, however, offers a unique subject for quantitative analysis, independent of lexical and semantic meaning, that is largely absent from prose works: meter. The practice of scansion is an ancient study that precedes the advent of the English language. It has, nevertheless, always consisted mainly of counting, sorting, and indexing words and word components, endeavors to which quantitative analysis is especially attuned. Despite this apparent sympathy between metrics and quantitative analysis, however, the algorithmic detection of the complexities of meter remains outside of the current capabilities of the Digital Humanities. The irregularity of stress, syllabic schemes, and the rule-bending nature of poetic diction runs counter to the binary presence/absence process of most computational analysis.  In the second phase of the Stanford Literary Lab’s multi-year ongoing project to create a system for detecting the formal features of poetry, we have focused our attention on the question of meter. Using a new method that combines a series of rule-based analyses with an iterative probabilistic-based classification algorithm, we can now detect, with a high degree of accuracy, both the meter and line length of individual poems. We have trained our algorithm to recognize individual metrical feet, such as iambs, dactyls, anapests, trochees, and spondees, and to combine these identifications with a signal-processing approach to the entire poem to classify the overall metrical scheme of any given poem. We have trained and tested our algorithm on a corpus of over 300,000 English language poems from the late medieval period to the twentieth century. Moreover, we have also applied our algorithm to multi-lingual corpora: in our presentation we will demonstrate how our methods are effective on German and French, as well as English poetic forms.  This project builds upon the first phase of our project, presented to the Alliance of Digital Humanities Organizations conference in Lincoln, Nebraska, which successfully sought to recognize the syllabic scheme patterns in poetic lines. The overarching goal of our project as a whole is perhaps simple to state, but challenging to execute: we seek to create a program that can automatically identify poetic forms. In other words, we are in the process of designing a program that can read any number of poems and tell us their exact syllable count, meter, rhyme scheme, use of traditional forms (e.g., sonnet, ballad, sestina), etc.  Success in creating such a program would represent an important tool for scholars in the Digital Humanities, and would offer the ability to:  1) Create an inventory of all poetic forms, traditional and untraditional.  2) Trace the history of poetic forms, including:  - variation among poetic forms (e.g., Which forms were most popular in a given period? Were some periods more formally diverse than others? How does form diversity change over time?)  - variation within poetic forms (e.g., How do the forms themselves change over time? Are sonnets more metrically rigorous in one era than another?)  3) Better understand the relationship between form and meaning by relating analyses of scansion with those of lexical and semantic meaning.  4) Provide distant readings to help generate and/or support new close readings.  In constructing such a program, we break down the task of recognizing metrical schemes into the simpler, but by no means simple, components of recognizing: 1) Syllable Scheme; 2) Beat Scheme; 3) Rhyme Scheme; 4) Metrical Scheme; and then 5) matching any combination of these categories to a tradition name (e.g., sonnet, heroic couplets, rhyme royale, etc.) if one exists to describe it.  We began by designing a program that could accurately detect the number of syllables in a given line of poetry (item 1 above) because we believed it would be the most straightforward element to analyze. We additionally realized that if we limited our sample according to metrical foot, then syllable count would present a shortcut toward detecting a rough approximation of meter. For example, if we started by analyzing only iambic poems (as recognized by human readers), and our syllabifier counted 9-11 syllables in each line of a given poem, then we could have reasonable assurance that the poem was written in pentameter. A similar process could be applied to other metrical patterns. In training our syllabifier, we purposely limited our sample to poems composed from roughly the late sixteenth century to the late nineteenth century because metrical forms were most stable and recognizable in this period.  Building upon this earlier success, we have combined our ability to recognize syllabic scheme with a complex approach to meter that has, at present, an over 80% success rate in correctly classifying meter. Our presentation will discuss how our algorithm was built, the specific challenges that we faced (e.g., elision, extrametrical syllables, feminine endings, foreign words, and other features of meter that are commonly acceptable in the practice of poetics but that our program had difficulty overcoming), and we will present the results of our application of this technique to a large, multi-lingual corpus that shows the historical shape of various metrical forms important to European poetry from the sixteenth to the late nineteenth century. Our initial analysis, using only syllabic scheme, revealed a number of significant and unexpected observations, including the fact that the use of pentameter peaked around the middle of the seventeenth century and has been on the decline ever since, as well as the fact that the use of tetrameter has been reciprocally on the rise since the early eighteenth century and is today equally as popular as pentameter. In this second phase, we expand the results of this analysis to show the historical prominence of the sonnet and heroic couplet forms, the transnational inheritance of metrical form and the history of iambic pentameter in English poetry.  We believe that what we have achieved with this work so far will aid in future quantitative and digital work on poetry, a lacuna that represents a critical problem for the use of digital humanities in the study of literature, given the enormous significance of the poetic tradition within literary studies. We also believe, moreover, that our program has application well beyond the study of poetry and could help to analyze and detect metrical schemes in song, drama, and prose as well, generating topics of analysis that likely would remain undetected without quantitative analysis.  ",
       "article_title":"The Stanford Literary Lab Transhistorical Poetry Project Phase II: Metrical Form",
       "authors":[
          {
             "given":"Mark",
             "family":"Algee-Hewitt",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Ryan",
             "family":"Heuser",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Maria",
             "family":"Kraxenberger",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"J.D.",
             "family":"Porter",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Jonny",
             "family":"Sensenbaugh",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Justin",
             "family":"Tackett",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "literary studies",
          "corpora and corpus activities",
          "prosodic studies",
          "text analysis",
          "bibliographic methods / textual studies",
          "english studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Farm Security Administration – Office of War Information photographic dataset is a collection of over 170,000 monochrome and colour photographs, commissioned between 1935 and 1945 by the government of the United States of America. Offering a unique snapshot of the nation during the period, it serves as an important visual record for scholars and the public-at-large. The FSA-OWI photographic archive has been digitized by United States Library of Congress, and because the photographs were taken on behalf of the United States Government, access to and use of the collection is essentially free and open.  Under the direction of Professor Laura Wexler, (American Studies and Women, Gender & Sexuality Studies, Yale University), the Photogrammar project takes the conditions of this archive (pre-digitized and publicly accessible) as the starting point for a digital, scholarly and open source platform that builds upon, and significantly extends, the Library of Congress’ online collection. The subject of a successful Digital Humanities Start-Up Grant from the United States National Endowment for the Humanities, the project seeks to answer research questions that emerged from scholars at Yale University. Our paper will focus on three Digital Humanities techniques that we have used to analyze the corpus:  Geospatial: Computational derivation of latitude and longitude;  Text Mining: Vector-space analysis to expose thematic similarity;  Statistical Analysis: Contextual inference to “re-discover” missing metadata.  Each of these techniques is associated with significant information gain, relative to the previous state of scholarship on the corpus:  Geographic: The collection is often characterized as being about the dust bowl and rural poverty in the American south during the Great Depression. In fact, by mapping the photographs and analyzing photograph density at the county level by year, the popular characterization of the FSA-OWI does not hold. Rather, the scope of sight was much broader including a large focus on the United States Northeast and Midwest, as well as photographs beyond the continental United States such as the Virgin Islands and Europe. Text Mining: The 1940s ontology of the collection only allowed a photograph to be classified in one category at once. By looking at latent patterns in the free-form textual descriptions, we are able to surface photographs that participate in multiple and overlapping clusters. In this way, we can discover thematic similarity between the work of several different photographers, active at different times and in different places in the country (and around the world). This approach reflects a more general turn towards ‘latent’ patterns in unstructured data within the archive.  Statistical Analysis: The relatively large scale of the collection (available as both digitized negatives and physical prints) as well as constantly changing organizational systems through the years has unfortunately left a majority of the negatives with minimal documentation. Utilizing latent metadata attached to the photographs, we are able to take individual photographs and to put them back into strips of four and five. In turn, this allowed for us to insert new metadata into the photographs. For example, if a frame with an unknown photographer and location is between photos by John Vachon in Chicago, then we know the unknown frame is by this photographer in this location. We will discuss the statistical methods applied using R.  These three techniques open up new questions about this collection and historic period, and challenge previous scholarship. We believe the Photogrammar project can serve as one example of the general question of how to engage with large-scale digital archives of visual culture. This question is of particular importance for scholars who seek to bring Digital Humanities techniques to “Big Data” collections, whether those curated by libraries, museums, or scholars themselves. We anticipate both similarities — and important differences — with European archives of the same period, including the UK Mass Observation Archive (1937-1960s), and forthcoming collections hosted by Europeana Online.  In addition, we will discuss how this project offers a new, user-friendly way to access a visual archive of this size by sitting at the intersection of public and digital humanities. We will discuss the ways in which we intend to open up the collection to contributions from the public at large, with lessons learned from previous attempts to crowdsource metadata for this collection (Flickr Commons / New York Public Library 20084, Flickr Commons Library of Congress 20095). We will show a prototype of a publically-accessible Geographic Referencer, to allow end users to more accurately and appropriately locate photographs on both current and historic maps. And we will discuss some of the challenges in incorporating crowd-sourced metadata corrections into a historic archive, while preserving the integrity and historic character of a large visual collection.   ",
       "article_title":"Photogrammar: Organizing Visual Culture through Geography, Text Mining, and Statistical Analysis",
       "authors":[
          {
             "given":"Lauren",
             "family":"Tilton",
             "affiliation":[
                {
                   "original_name":"Yale University, United States of America",
                   "normalized_name":"Yale University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03v76x132",
                      "GRID":"grid.47100.32"
                   }
                }
             ]
          },
          {
             "given":"Peter",
             "family":"Leonard",
             "affiliation":[
                {
                   "original_name":"Yale University, United States of America",
                   "normalized_name":"Yale University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03v76x132",
                      "GRID":"grid.47100.32"
                   }
                }
             ]
          },
          {
             "given":"Taylor",
             "family":"Arnold",
             "affiliation":[
                {
                   "original_name":"Independent Scholar ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "metadata",
          "geospatial analysis",
          "interfaces and technology",
          "cultural studies",
          "sustainability and preservation",
          "spatio-temporal modeling",
          "repositories",
          "art history",
          "interdisciplinary collaboration",
          "archives",
          "crowdsourcing",
          "maps and mapping"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper reports on recent research that explores how geographical information systems (GIS) and related technologies can be used to understand texts, drawing on both literary and historical examples. In Graphs, Maps, Trees, F. Moretti identifies mapping as one tool that facilitates distant reading. Other researchers have subsequently demonstrated that GIS can be used to implement this.The research presented here illustrates that the potential for GIS and related technologies in the humanities goes beyond both mapping and distant reading. Specifically, we identify three general ways that geographical technologies can enrich our understanding of texts: first, distant reading using Geographical Text Analysis (GTA), a combination of techniques from GIS-based spatial analysis and from corpus linguistics; second, enhanced close reading based on using place names or maps as query tools; and third, geographical analyses of the texts using techniques such as network analysis and route reconstruction. The aim of these three approaches is to go beyond simply producing visualisations, and instead to allow us to improve our understanding of the text with an emphasis on its geographies.   Fig. 1: Cholera in the Registrar General’s reports showing (a) locations of cholera instances and (b) instances of terms associated with the water supply.    Fig. 2: Time series of cholera instances from the Registrar General’s reports. The 1868 spike in instances was not matched by a corresponding rise in deaths.  Distant reading through GTA effectively allows us to ask two basic questions:  what places is the corpus talking about? and  what places does the corpus relate to a particular theme? This involves more than simple mapping. First, place-names have to be identified using automated techniques. Once this has been done spatial analysis and corpus linguistics techniques allow the geographies within the text to be investigated either in an exploratory way that asks ‘where is the corpus talking about?’ and ‘what is it saying about these places?’, or in a more thematic way that asks ‘where the corpus is talking about in relation to my theme?’ and ‘what else is being said about these places?’. As all of the place-names are georeferenced, we are also able to integrate them with other sources that are also georeferenced. To illustrate the potential of this we use the Registrar General’s Reports, which document mortality and disease in England and Wales from 1851 to 1911. Using GTA to explore the Registrar General’s reporting of cholera showed a number of interesting things: first, that he was particularly interested in cholera in London (figure 1a); second, that the discourse on cholera in London was strongly associated with potential causes, particularly the water supply (figure 1b), whereas in other parts of the country he tended simply to acknowledge that cholera was occurring, increasing or declining; third, that the emphasis on London could not be justified either by the numbers of deaths from cholera in London or London’s death rate from the disease; and fourth, that whereas early spikes in instances of cholera in the Reports correspond with known cholera epidemics, the last large spike in 1868 (figure 2), was largely associated with the fear of an epidemic spreading to Britain. Given that there were relatively few cholera-related deaths reported in 1868, we have concluded that the improved understanding of the disease had led to improved measures to prevent it.  These types of distant reading techniques can also be applied to literary texts. Using a corpus of writing about the English Lake District we can show that whereas William Wordsworth was associated with a few central parts of the region in the Romantic period, Victorian readers associated him with sites throughout the Lakes. Using digital images from Flickr, furthermore, we can show that this trend has been reversed in the 20th century.  These top-down, automated techniques are valuable because they allow us to understand large corpora quickly, but they do so at the expense of losing much of the subtly and nuance that close reading can offer. It is frequently argued that one of the key advantages of digital texts is that they can be read in a non-linear manner. A weakness of this is that it is not always clear how to structure non-linear reading. Place offers one way in. The decline in mortality, particularly among infants (aged under one), started in the nineteenth century but is poorly understood. Much of the research that has been done focusses on the problems and solutions of large urban centres such as London. This is despite the fact that quantitative evidence shows that some rural areas started to decline far earlier than urban centres and at much faster rates. Despite this, there could be major variations between nearby rural areas with apparently similar quantitative characteristics. To explore this further, three neighbouring districts in rural Suffolk - Sudbury, Samford and Risbridge - were analysed. Sudbury and Samford both had relatively high infant mortality rates in the 1850s, the earliest decade for which data are available, but showed rapid improvements thereafter. Risbridge, by contrast, started with low rates, but only showed slight improvements through the rest of the century. In order to explain these variations we first had to identify all place-names within these districts. This was done using a GIS of the boundaries and a gazetteer. These were then used to query the British Library’s Nineteenth Century Newspaper corpus, which contains text from over two million newspaper pages. Additional search terms thought to be relevant to infant mortality decline were used to narrow the searches and this list was refined as the research progressed. Based on the articles found through these queries, we have concluded that the system of local government in Risbridge was far less effective than the systems in the other two districts. Despite many calls to improve drainage, housing and a range of other features that have well established links to infant deaths, little action was taken by Risbridge’s authorities. This can clearly be contrasted to the situation in the other two districts, where the local authorities took extensive action. Although this is not a definite causal link, it does provide strong evidence that local government played an important role in reducing rural mortality rates, something that has previously only been identified at the national level or for major urban centres. Again, similar techniques can be used in literature. We demonstrate this using map-based queries rather than place-names. A system was created that uses a Google Map to show every place mentioned in our corpus of Lake District writing as a point. Each point was linked to web-pages that include the full text. Clicking on a point on the map, presents the reader with a keyword-in-context list (or place-name-in-context) list of all of references to that place and hyperlinks can then be used to follow from these to the appropriate location in the full text. This allows the reader to query not only what is being said about a particular place, but also about nearby places.   Fig. 3: Network analysis of Norman Nicholson’s work. The diagram on the right shows the number of letters sent by Nicholson with thicker lines indicating more letters. The map on the right shows where recipients living in Britain lived.    Fig. 4: Cost surface analysis showing the combined estimated routes of Arthur Young (1770), Thomas Gray (1775) and Thomas Pennant (1771 and 1776). Reds and yellows indicate frequented routes.  Finally, geographical technologies can also be used to enhance texts in a number of ways. One way, shown in figure 3, is network analysis which can be used to explore, for example, networks of correspondence. We have used this to explore the correspondence networks of Lake District writers such as Norman Nicholson where a combination of diagrams to show who he was corresponding with and in what volumes, and maps to show where they lived was used. A different approach allows us to move beyond seeing places within texts as isolated points and instead to explore them as parts of journeys. This was done using a number of accounts of journeys through the Lake District. First, the texts were close-read to identify the order in which place-names mentioned were visited. These were mapped as points which were then used as the input into technique called cost-surface analysis which estimates the most likely route between points. This has been shown to be particularly effective in upland areas such as the Lake District (figure 4). This allows us to estimate and map the routes the writers are likely to have taken, and to explore the geographies of silence concerning the places which writers are likely to have visited but have not mentioned. In conclusion, the use of geographical technologies in understanding texts is potentially multi-faceted and goes far beyond producing maps. It is instead a useful tool for understanding and enhancing texts to produce the abstract summaries required for distant reading, to select parts of the text that require close reading, and to allow new forms of analyse to help understand the geographies within texts.  Acknowledgements The research leading to these results has received funding from the European Research Council (ERC) under the European Union’s Seventh Framework Programme (FP7/2007-2013) / ERC grant “Spatial Humanities: Texts, GIS, places” (agreement number 283850). We are also grateful to Sarah Hastings for her work on Suffolk while on an internship from Mt Holyoke College hosted in the Department of History, Lancaster University.   ",
       "article_title":"Digital approaches to understanding the geographies in literary and historical texts",
       "authors":[
          {
             "given":"Ian",
             "family":"Gregory",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Chris",
             "family":"Donaldson",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Patricia",
             "family":"Murrieta-Flores",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"C.J.",
             "family":"Rupp",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Alistair",
             "family":"Baron",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Andrew",
             "family":"Hardie",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Rayson",
             "affiliation":[
                {
                   "original_name":"Lancaster University, United Kingdom ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "corpora and corpus activities",
          "interfaces and technology",
          "geospatial analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction  Since place names form the underlying semantic content of almost all geographic documents, the ability to identify them in texts and images is essential in any attempt to work with, compare or interpret them. For early maps and geographic texts this ability is especially important, because while they rarely conform to standard geometries or schemas, they often provide the earliest attestations to towns, peoples, and other spatially localized phenomena. Tools, infrastructure and resources for collating, aligning, and exploiting toponyms in early maps and geographic documents would therefore have a broad and significant impact across a range of fields, including Archaeology, History, Classics, Genealogy and Modern Languages. In this paper we showcase early work on the detection of possible toponyms in digitized texts and scanned old maps. It builds upon the successful Pelagios initiative which has been connecting a variety of heterogeneous online resources related to classical antiquity. In contrast, Pelagios 3 will extend its scope to the European, Islamic and Chinese Middle Ages, but focus predominantly on geographic works. These in turn will form a core body of material around which we hope to see a more diverse body of references accumulate in time. Since our ultimate aim is to enable humanities scholars to annotate and discover places in documents for themselves, we discuss our use and adaption of existing open source tools within a framework that puts a premium on flexible, lightweight and easy to use resources. Moreover, that discussion will be based on two real-case scenarios, in order to demonstrate the strengths of our approach and flag up potential issues that require further attention.   Mapping places from texts: the Vicarello Goblets  Our first test case tackles the issue of extracting place names from a text. The Vicarello Goblets are a collection of four silver drinking vessels dated to around late third or early fourth century AD, engraved a land itinerary between Gades (modern Cadíz, Spain) and Rome. Each goblet indicates the road stations along the route (varying between 104 and 110 on each goblet), as well as the distances in miles between them. These unusual ‘texts’, the limited range of places that they represent, the easy identification of the majority of locations, along with the fact that there are images and transcriptions available online already, makes the Vicarello Goblets an optimal source for trialling the methodology that we will use on much larger corpora of texts, including travel guides, gazetteers, encyclopaedias and more.  To obtain annotations from the Vicarello Goblets, all the toponyms are matched against places in a URI-based gazetteer, that is, a directory of places which assigns a persistent Web address to each entity, allowing for disambiguation at a global level. The engravings on the Vicarello Goblets already represent ordered lists of toponyms. Therefore we can directly match the lists against the gazetteer based on name similarity, and then disambiguate by taking into account geographic proximity between different places in the list. For further documents which are of a less structured nature (i.e. which contain more free-form narrative text) we are experimenting with a combination of ‘geoparsing’ technologies, including the Edinburgh Geoparser and the Stanford NLP Toolkit.  Identification is only half the story, however. The data model that we have developed for Pelagios 3 allows for rich item metadata that cleanly differentiates between information about the item and information about the places that relate to it (and how). For instance, toponyms in a document may follow a certain sequence or layout. A simple mashup not only shows the toponyms from the four Vicarello Goblets on a map, but how they relate to one another as an itinerary. An information box at the bottom provides the information about the document itself (Fig. 1), while a small layer menu lets the user switch layers on and off for each individual goblet to allow immediate comparisons. Selecting a place displays a popup with a textual transcription from the Goblets, and metadata drawn from the gazetteer. What is noteworthy about this mashup, however, is not so much the map itself – for which comparable projects already exist – but rather that the map can be automatically generated from a simple Pelagios data file, containing item metadata and annotations in Open Annotation RDF format. Thus the pathway from data production to visualization is both efficient and highly scalable across large numbers of documents.  Extracting places from maps: Ptolemy’s Geographike Hyphegesis  Previous work on toponym recognition in scanned maps focuses on contemporary documents for the simple reason that old maps remain extremely difficult for machines to parse. Our proposal is to automate the identification of potential toponyms in terms of their location, extent and orientation on the map image, so that researchers can then associate the results with items in pre-existing gazetteer lists and ultimately with URI-based gazetteers. The example given here is of Ptolemy’s regional map of Ireland and Great Britain (Fig. 1), digitized by the British Library. Our first processing phase generates a black-and-white mask image, which isolates and separates “background” from “foreground”. The next phase locates and characterises features – in our case, connected objects – on the foreground image using an algorithm that detects contours. Since toponyms often consist of multiple features, the final phase aims to connect the detected features to groups that most likely represent a single toponym. Fig. 2 shows that for our test case the algorithm detected toponyms with a high success rate, correctly locating 38 of 41 places. Our initial work with additional (including visually more complex) maps has raised several error scenarios and prompted some initial responses:  Ornament irritation. Symbols and decorative elements that have structures in size and density (and colour) similar to toponyms frequently cause false positive detections. We expect that heuristics concerning the spatial density of matches and amount of overlap between them may be able to alleviate this problem, as these false detections exhibit distinctive clustering behaviour.  Line bleed. Toponyms that intersect or are located near lines (often borders, graticules or rhumbs), can distort the recognition result. We expect that proper tuning of image processing parameters in the first separation step (such as colour thresholds, or thresholds determining the behaviour of line removal algorithms) may be able to lower the number of such errors, but it is unlikely that they can be avoided altogether. Increasing the efficiency of human verification and correction is essential for addressing this challenge.  Toponym crosstalk. Especially in the presence of distracting elements such as lines, our algorithm can erroneously lead to toponym bounds that run across two neighbouring toponyms.. As in the case of errors caused by line bleed, it is unlikely that these can be avoided, but metrics based on the morphology of the toponym may help to detect and flag them to a human operator for verification. Split toponyms. Our current processing approach does not specifically deal with toponyms that are split across multiple lines. An example can be found in Fig. 2, where \"Alvion Insvla Britannica\" is split into two separate toponyms. Once again however, morphology and the spatial proximity of features will allow us to present human operators with potential candidates for merging into single features. Large area & curvilinear toponyms. Likewise, our heuristics are ill-suited to detect toponyms that cover large areas (e.g. regional toponyms), which are oriented significantly differently from other toponyms on the map, or which run along a curved baseline. Here, we may require a human to explicitly demarcate their bounds, although fortunately the size of such toponyms usually restricts their frequency in a given document.   While we expect that the amount of manual tuning and intervention will be further reduced by refining the processing workflow, toponym identification on old maps will never be a fully automated process. Therefore, we are also developing user interfaces and graphical tools that help both professional and public users carry out the manual work of aligning imagery to the gazetteers. On the one hand, we are experimenting with ways of re-presenting the user with re-oriented and visually enhanced image fragments so that they can be more easily interpreted. On the other we can use the spatial information in such images, and data from previous annotations, to propose likely candidates for ‘one-click’ annotations, and auto-completion of transcriptions.  Concluding Remarks   The data produced will provide us with opportunities to visualise both maps and texts in new ways. For instance, corpora of structurally similar documents, such as portolan charts, can be directly compared in terms of the places they refer to, the toponyms used, and their sequence along a coastline, in a similar manner to the itineraries described above. Alternatively, we can also blend out and replace toponyms with either modern or ancient alternatives where known, helping make these important documents easier to interpret for both scholars and public alike (Fig. 3). Most importantly we see this as the firsts steps in drawing new connections between the extraordinarily diverse range of early geospatial documents that have come down to us.   Fig. 1: Mashup showing route of the Vicarello Goblets (http://pelagios.github.io/demos/vicarello-alpha/)     Fig. 2: Part of Ptolemy’s regional map of Ireland and Great Britain. (Ca. 1480 © The British Library Board. Harley MS 7182 ff.. 60v-61.) Toponyms identified automatically and annotated with oriented bounding boxes.     Fig. 3: Original map (left) and map with original toponyms dynamically blended out and replaced with corresponding modern place names.   ",
       "article_title":"Pelagios 3: Towards the semi-automatic annotation of toponyms in early geospatial documents",
       "authors":[
          {
             "given":"Rainer",
             "family":"Simon",
             "affiliation":[
                {
                   "original_name":"AIT Austrian Institute of Technology",
                   "normalized_name":"Austrian Institute of Technology",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/04knbh022",
                      "GRID":"grid.4332.6"
                   }
                }
             ]
          },
          {
             "given":"Elton T. E.",
             "family":"Barker",
             "affiliation":[
                {
                   "original_name":"The Open University",
                   "normalized_name":"Universidade Aberta",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02rv3w387",
                      "GRID":"grid.26693.38"
                   }
                }
             ]
          },
          {
             "given":"Pau",
             "family":"de Soto",
             "affiliation":[
                {
                   "original_name":"University of Southampton, United Kingdom",
                   "normalized_name":"University of Southampton",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ryk1543",
                      "GRID":"grid.5491.9"
                   }
                }
             ]
          },
          {
             "given":"Leif",
             "family":"Isaksen",
             "affiliation":[
                {
                   "original_name":"University of Southampton, United Kingdom",
                   "normalized_name":"University of Southampton",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ryk1543",
                      "GRID":"grid.5491.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "classical studies",
          "natural language processing",
          "visualisation",
          "networks",
          "geospatial analysis",
          "interfaces and technology",
          "image processing",
          "concording and indexing",
          "content analysis",
          "semantic web",
          "semantic analysis",
          "text analysis",
          "maps and mapping"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper presents findings of an Andrew W. Mellon Foundation-funded project conducted at Penn State University in the period April 2012-June 2013. The project explored scholarly workflow of the Penn State faculty across the sciences, humanities, and social sciences, focusing on the integration of digital technologies at all stages of a research lifecycle—from collecting and analyzing data, over managing and storing research materials, to writing up and sharing research findings. The study also examined scholars’ attitudes towards the use digital technologies in their research practice, as well as the level of institutional support available to them in developing and implementing digital research skills. This paper harvests a comparative multidisciplinary perspective of our study in order to explore specificities of humanities scholars’ digital workflow, providing a ground to identify and develop a software and service architecture that supports those practices. Therefore, while focusing on current findings, the paper briefly highlights the future trajectory of our study, as well as planned next steps regarding technological initiatives aimed at addressing management of digital scholarly workflow in humanities scholarship.  The study was comprised of two research phases, each of which focused on a specific set of research questions and goals. The first phase included a web-based survey that consisted of twenty-five questions, which, in addition to demographic information, included queries about data searching, storing, citing, sharing, and archiving practices, as well as about scholars’ experiences in using digital research tools and resources. A total of 196 faculty (59% female / 41% male) completed the survey, most of them tenured faculty, with fixed-term (non-tenure track) faculty, and tenure-track faculty following. The Humanities tended to have older respondents (over 40 years of age), while the sciences and social sciences faculty skewed lower in age.  The second phase of the study included a set of face-to-face ethnographic interviews. A total of twenty-three scholars volunteered to participate in the interviews, and they were equally divided along the lines of disciplinary profiles, academic ranks, and gender: 13 were faculty in the humanities and social sciences (HSS) and 10 in the sciences; 11 were tenure-track and 12 tenured faculty; 13 were female and 10 men. The interviews were semi-structured and, on average, lasted an hour. The interviewees were audio-recorded and then transcribed by a professional transcriptionist. The interview transcripts were first coded into broader categories (nodes) by two independent coders. We then proceeded with focused coding, where the categories into which the data were originally coded had additionally been refined for relevant patterns, themes, and topics.  The results of our study show that digital technologies have different roles and levels of integration at various phases of scholarly workflow. For instance, digital tools are actively used for finding, storing, and archiving research materials. This finding is true across disciplines, although certain disciplinary differences can be traced. For instance, while the majority of respondents across disciplines (92%) actively store research materials important to them, humanities scholars reported the highest percentage of lost and inaccessible research files; predominantly (27%), inaccessible files resulted from failing to migrate research materials from obsolete to contemporary digital formats. Similarly, while searching for information electronically is a standard, daily practice of our respondents regardless of their disciplinary background and/or level of technical proficiency, humanities scholars commonly prioritize the Penn State library catalog as their search and access points, while scholars in the sciences prioritize Google Scholar. Our results also show, however, that across disciplines, the path towards finding information commonly starts with Google Search and Google Scholar, especially for scholars engaged in discovery search, which reaffirms results of other recent studies indicating the increasing prevalence of commercial over academic services for scholars’ information search (see: Nicholas et al., 2011; Kortekaas, 2012)  The results of our study further show that, in the phases of data collecting and analysis, the use of digital technologies significantly differs across disciplines. Our respondents in the science commonly noted that their work would be impossible without digital technologies, and scholars in the social sciences indicated digital tools and methods becoming ‘a new normal’ in their data gathering and analysis practice. Contrary to this, respondents in the humanities, with a few exceptions, implied the lack of digital technology use in those phases of their research process. Parallel with this, however, they indicated awareness of digital tools and methods that could facilitate their analytical practice, suggesting the lack of available training and time as key impediments to developing literacies needed for mastering those tools.  Disciplinary differences were evident in the activities of data sharing and communication, particularly in the use of social media. With regard to data sharing, two thirds (63%) of scholars in the sciences indicated that they actively share their research data, while a nearly identical percentage of the humanities scholars (69%) indicated opposite practice. Yet we found that in addition to disciplinary differences, differences in academic standing also influence data sharing practices of our respondents, with tenure- track faculty being more protective of their data than tenured scholars. We further observed widespread use of digital technologies in scholarly communication across disciplines, with a noticeable difference being frequent social media use among the humanities scholars, and nearly non-existent use among respondents in the sciences.  Annotating and reflecting emerged as research phases where the use of digital technologies is most idiosyncratic, that is, based on scholars’ personal preferences rather than the level of technical skills or availability of digital tools. With regard to citation, the use of citation management programs was somewhat higher in the sciences than in the HSS (55 % vs. 30 %), but the overall level of digital technology use in this research activity was lower than in other phases of the research workflow.  Conceptually, our results illustrate various ways in which integration of digital tools in one phase of the research processes influences other segments of the workflow. For example, scholars’ full reorientation on electronic search and access produces an abundance of collected materials, requiring adjustments in researchers’ storing, organizing, and archiving practices. As some of our respondents observed, integration of digital tools into their search activities resulted in a complete breakdown of their systems for organizing information, developed for print-based materials. Therefore, while implementation of digital tools into one phase of the workflow might be rewarding, it might also become a challenge in other phases of the workflow. This is particularly relevant in the perspective of tool development, implying that digital research tools should be designed to support a continuous research workflow instead of separate and disconnected activities.  Our findings also suggest that in a workflow of a digital scholar technical rather than traditional methodological expertise shapes interconnectedness among phases of the workflow. In our study, greater level of workflow interconnectedness was observed among scholars in the sciences, who tend to be more technologically savvy than scholars in the humanities and social sciences. This, as well as our previously mentioned study findings, indicates a significant scope of disciplinary differences with regard to the use of digital technologies in scholarly work. Broadly conceived, these disciplinary differences can be conceptualized as inherent and acquired. As an example of inherent disciplinary differences we could understand data privacy requirements, which widely differ across disciplines and, as our findings show, significantly determine the type and level of digital technology use. Acquired differences on the other hand can be observed in a set of habits and assumptions rooted in a particular community of practice. Technical architecture of digital research tools needs to support specific disciplinary needs in ways that address both inherent and acquired disciplinary differences. Data storage and management, for instance, has been identified as a dire problem across disciplines, but with distinctive disciplinary needs.  The next phase of our study (2014-2016) will be devoted to developing a digital research tool for humanities scholarship using Zotero as a test platform, in collaboration with George Mason University. Based on the results of the first phase of our study, we will focus on unifying several phases of the research workflow, and facilitating elements such as better integration of finding and archiving into the scholar’s online path. Discovery must be better finessed for the end user, and search and retrieval should be fully integrated into an interface that also allows annotation, organization, and archiving of research materials. Also, since the loss of information among the humanities scholars is significant, there is a need to build into the research workflow easy strategies for users to self-archive their work in storage services that are inherent to the individual or the institution. Optimizations to connect the institutional repository within Zotero, as well as expose references and metadata within uploaded PDFs will be explored.   ",
       "article_title":"Tracing Workflow of a Digital Scholar",
       "authors":[
          {
             "given":"Smiljana",
             "family":"Antonijevic",
             "affiliation":[
                {
                   "original_name":"Roskilde University, Denmark",
                   "normalized_name":"Roskilde University",
                   "country":"Denmark",
                   "identifiers":{
                      "ror":"https://ror.org/014axpa37",
                      "GRID":"grid.11702.35"
                   }
                }
             ]
          },
          {
             "given":"Ellysa",
             "family":"Stern Cahoy",
             "affiliation":[
                {
                   "original_name":"Penn State University, USA ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "and discovery",
          "software design and development",
          "resource creation",
          "interface and user experience design",
          "digital humanities - nature and significance",
          "user studies / user needs",
          "GLAM: galleries",
          "digital humanities - institutional support",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  New advances in online game engines have made it possible to easily view 3D virtual environments from any web browser, but the full potential of 3D humanities research has gone unrealized because of the difficulty in connecting important 3D findings to the work of traditional scholars grounded in texts. This presentation will discuss the current development and show demonstrations of the Scholarly 3D Toolkit, (S3DT) a plug-in for the Unity game engine designed to help better interface 3D historical reconstructions with other data. The work of a team lead by James Coltrain, S3DT will provide simple interfaces that allow creators to link their 3D scenes to sources and documents, and to dynamically import and view traditionally indexed digital humanities data from databases, spreadsheets, or GIS programs. The result will allow users to view multiple layers of data plotted within a single online 3D environment, showing markers for events, personal connections, documents, images, and annotations from multiple users, all in time and space. S3TD will allow for greater and more sophisticated interdisciplinary analysis, helping scholars studying three dimensional spaces to contextualize models of architecture, urban structures, and natural topography using texts and other spatial data. By comparing existing digital humanities findings with 3D scenes that show scale, light, and texture, the platform will allow for more complex and nuanced investigations of past spaces. Along with a discussion of the project’s progress and the theoretical questions at play, this presentation will show early demos of a test case for the platform. These will include a richly annotated high quality 3D reconstruction of Fort Stanwix, an 18th-century historic site and National Monument, with an existing database constructed by Nebraska undergraduates of over 400 letters, maps, and plans. S3DT will build upon the achievements of previous digital humanities projects by expanding the options scholars have for working in 3D spaces. Earlier platforms have allowed for the real-time display of annotated 3D models, but some could not stream live in a browser, and most allowed creators little in the way of customization.i Extremely important work has been done with diverse and creative applications of historical GIS, and S3DT will allow for those established types of analyses to be brought into the third dimension. ii More recently, some scholars have made use of online game engines like Unity to achieve some of the goals set forth in the S3DT project, including the use of advanced real time graphics in an online environment. iii However, these projects have not resulted in open, customizable platforms, and none allow for the importation of new 3D content. S3DT will build upon previous work in Unity by connecting 3D scenes from multiple creators to the layered viewing of all kinds of outside humanities data. The practical tools in S3DT also make many previously difficult modes of spatial analysis quicker and more accessible. S3DT scenes can show spaces changing over time with numerous iterations and nuance, and also display multiple interpretations of the same structure side by side as competing arguments. With 3D objects linking to multimedia sources, users can now better understand the interpretive leaps creators made, and which pieces of fragmentary evidence scholars privileged in creating coherent 3D spaces, information that also facilities efficient peer review. The ability to display different types of data can also promote public outreach in addition to academic collaboration, letting universities, museums, archives, historic sites, and even individual visitors contribute to the same online 3D spaces. The design of the S3DT plug-in for Unity will allow for open analysis of 3D scenes, while protecting scholars’ data for future use. The plug-in does not interfere with the traditional workflows for 3D content creation, and also stores all textual and multimedia data in standard MySQL databases. As a result, neither scholars’ 3D models nor their annotations or data will become stuck in the S3DT if creators find better future platforms for presentation. S3DT will consist of a two part plug-in for Unity. The first part, within the Unity Editor, will allow creators to add notes and metadata to imported 3D objects, and prepare them for publishing. The second, is a web template which will allow for the viewing and manipulation of published scenes, as well as the live importation and plotting of new data layers from outside sources. Below is a typical workflow for S3DT along with key features at each step. This presentation will conclude with early demonstrations of many features from both parts of the plug-in.   I. 3D Content Creation Creators begin by modeling and texturing a 3D scene in their typical workflow in a 3D suite such as 3D Studio Max, Maya, or Blender. When they have finished, they export their 3D models to industry standard formats (ex. .obj or .fbx) . They then download and install the Unity Editor and the accompanying S3DT plug-in. Finally they load their 3D scene into the Unity Editor. II. S3DT Plug-in for Unity Editor  With their models loaded into the Unity editor, creators will use the S3DT plug-in to prepare them for publication. This includes creating an object hierarchy, denoting nested neighborhoods, complexes, buildings, rooms, architectural features, and sub-features, each as defined by the creator. Once the objects are defined, creators can enter metadata for each scene object in any fields they like. In particular, creators will be able to enter time sensitive information, such as the dates for the object's creation, alteration, damage, and destruction. Creators will also be able to enter links to sources used in their interpretation of the reconstructed object, as well as notes about their specific decisions. Next creators will publish their scene. In the process each published scene is exported as two parts, a Unity 3D file formatted for web display, and a matching MySQL database containing all metadata and links to sources and annotations.   III. S3DT Plug-in for Browsers  The S3DT web browser plug-in consists of a Javascript library and web template for loading and displaying published S3DT scenes on the web. Most projects will use a customized version of the web template, but the Javascript library is available for projects that are integrated into existing sites or for which creators desire a higher level of customization. The S3DT browser plug-in has a simple user interface consists of the following:  The Main Window displays the published Unity scene in real-time 3D. The Timeline consists of a scalable time line with a slider to control time position and markers corresponding to time sensitive events plotted in the scene. The Layers List shows all the elements in the scene, organized by package. Each layer has a collapsible view that expands to show the entire object hierarchy as defined by scene creators in the S3DT Unity Editor plug-in. Any additional content or data loaded into the 3D scene will appear in the layers list as a new layer, including published S3DT packages, maps, images, collections of user annotations, collections of plotted events, GIS data, etc. Users will be able to toggle the visibility and opacity of any layer or any object within a layer hierarchy. The Tools Window features a set of utilities users can use to manipulate or analyze the scene. These will include:  Advanced Search - Allowing customizable complex searches bases on any metadata field or object attribute. Groups - Allows users to group objects from any layer together into a new layer. Edit Object Metadata - If enabled, allows users to add to or edit metadata for scene objects. Annotate -Allows users to leave comments live in the scene, either attached to scene objects or in freestanding 3D space. Camera Tools - Allows users to place custom cameras, define camera paths and animate them, and to take and save camera snapshots. Import Data - Allows users to import data with geographic information from outside sources including SQL, Excel, KML, and ARCGIS. Also allows users to choose and customize marker appearances based on imported data, or upload their own. Create Exhibits - A set of sub-tools will allow users to connect camera views, and animations over time and space with HTML text for guided tours and other exhibits. Created exhibits will load into the layers view. Map and Image Import- Allows users to import maps and images into the live scene, and to align maps to existing terrain or images to camera views. Imported maps and images can then load into the layers view.     ",
       "article_title":"The Scholarly 3D Toolkit: Annotation, Publication, and Analysis of 3D Scenes alongside Imported Humanities Data",
       "authors":[
          {
             "given":"James Joel ",
             "family":"DataColtrain",
             "affiliation":[
                {
                   "original_name":"University of Nebraska, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "classical studies",
          "visualisation",
          "geospatial analysis",
          "interfaces and technology",
          "spatio-temporal modeling",
          "art history",
          "archaeology",
          "interdisciplinary collaboration",
          "virtual and augmented reality",
          "historical studies",
          "maps and mapping"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Tesserae is a open-source, online tool for detecting allusions in Classical literature on an automated basis. Originally limited to Latin poetry, the corpus of texts available to Tesserae has recently expanded to include Greek poetry and drama. Word-level n-grams form the foundation of the existing detection algorithm: a standard search returns all instances wherein two words a phrase in a later text shares two words with a phrase in an earlier text. This method has been previously demonstrated to reliably capture intertextual parallels already noted by philologists and to identify significant, previously unrecorded intertexts. The ability to detect allusions across the language barrier would represent an evolutionary expansion in Tesserae’s functionality as well as a significant contribution to Classical philology. Roman poets openly acknowledged their indebtedness to Greek literature (Horace famously remarked, “Greece, being conquered, tamed its wild conqueror, and brought the Arts to rustic Latium\") and scholarly studies of Latin poetry have long commented on allusions to earlier Greek sources. To apply the existing system where Latin text alludes to Greek, Tesserae requires a translation dictionary linking Greek lemmata to associated Latin lemmata. This paper details two methods for building such a dictionary on an automated basis and compares their relative merits as measured by their ability to capture parallels between book one of Vergil's Aeneid and the Iliad of Homer, as noted by G.N. Knauer in his commentary. The first method represents an original application of Bayes' theorem to a word-by- word alignment of the Greek New Testament with Jerome's Latin Vulgate.     For a given Greek word Gi, the set of Greek Bible verses in which it appears is identified. The words contained in the Latin translation of these verses become the set of possible translation candidates L. For each Li, the set of possible Greek words G is gathered from the set of Greek verses corresponding to the Latin verses in which Li appears. P(Gi|Li) is represented by the number of words in set G which may share a lemma with Gi, divided by the total number of words in that set. The probability of Gi is represented by a similar calculation, where the set of all words within the Greek text is substituted for G. The value of P(Li) is analogous. The success of this relatively simple alignment algorithm as compared with more classical IBM Models or Hidden Markov Models may be explained by the grammatical similarity of these two inflected languages and importance placed by the translator in remaining precisely faithful to the syntax of the original text.  The second method employs English as a pivot language, in a method inspired by work done previously by Jeffrey Rydberg-Cox at Perseus on Latin-Greek synonymy. Using the XML-encoded digital editions of Lewis and Short’s Latin-English Lexicon and Liddell and Scott’s Greek-English Lexicon, two dictionaries widely considered authoritative for Classical languages and available through the Perseus Digital Library, each Latin or Greek headword is characterized by a feature set composed of the English words appearing in its definition. The Python-based Gensim topic modelling tools are then used to transform the English word counts to TF-IDF weights and calculate similarities between the dictionary entries. The similarity scores between entries are then interpreted as similarities in meaning between the respective headwords.   Each of the two methods described above produces pairwise similarities between all Greek and Latin words considered, with those pairings rated by a probability measure between 0 and 1. Because each Greek word may have more than one possible Latin translation, each method accepts the top two translation candidates as valid.  The text of Homer’s Iliad is then indexed according to a feature set made up of Latin translation candidates. Each Greek token is lemmatized, and the token is then indexed according to all possible Latin translation candidates. Because lemmatization is unsupervised, ambiguous forms may have multiple possible Greek lemmata. Each possible Greek lemma will have two translation candidates if the respective translation method is successful, or zero if no translations are found. The text of Vergil’s Aeneid is indexed simply according to the possible Latin lemmata of each token. A given token in Vergil matches a token in Homer where one or more possible lemmata for the Latin word match against the set of translation candidates for the Greek word. A pair of phrases, one in Greek and the other in Latin, which share two or more words that match in this way, is returned as a possible allusion. The two methods are evaluated by their ability to detect a subset of Aeneid-Iliad parallels collated from the commentary of G.N. Knauer. Each method retrieves a distinct, though partially overlapping, subset of the parallels noted by Knauer. Comparison of the respective performance of both methods suggests that, while each method can be shown to identify significant Latin-Greek allusions, the Bayesian alignment method provides better recall of the benchmark set than the 'pivot' method at the expense of precision. We ultimately aim to combine the output of both approaches into a single feature set.  ",
       "article_title":"Automating the Search for Cross-language Text Reuse",
       "authors":[
          {
             "given":"James",
             "family":"Gawley",
             "affiliation":[
                {
                   "original_name":"University at Buffalo, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Christopher",
             "family":"Forstall",
             "affiliation":[
                {
                   "original_name":"University at Buffalo, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Konnor",
             "family":"Clark",
             "affiliation":[
                {
                   "original_name":"University at Buffalo, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "text analysis",
          "philology",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In the act of interpreting and describing pictures, even in the fundamental process of cognition, there is a strong play of language in the visual field. Critics working in this field have described the same relationship between the word and the image through different phrases: what Foucault refers to as the “seeable” and the “sayable” (1982) is defined by Deleuze as the “display” and the “discourse” (1984) while W.J.T. Mitchell terms as the “showing” and the “telling” (1994). The study of word and image (painting and poetry, literature and the visual arts), their relationship, or the examination of culture using critical devices in each field has been a consistent theme in the literary fields since antiquity. Visuality requires verbal descriptives for interpretation, whether they are spelt out explicitly, or subconsciously attributed in the human mind.  Images, when committed to the digital space, pose new issues regarding cognition. How do we recognise the digital object outside the moment of experiencing it? The digital collection is an assimilation of filenames and to use the digital object, we must first be able to recognise it. An image file may be recognised through the filename extensions (JPEG, TIFF etc.), but to conclusively state that it is the digital image of a photograph cannot be done without first looking at the contents of the digital file. The digital object does not have a tangible form or lineament. Physical photographic artifacts reveal clues to determine different aspects of its source. The different material on which the photographic image is imprinted (glass or paper) can reveal clues towards the origins of the image. Since photography is as much a technological phenomenon as it is fruit of human endeavour, the physical object itself communicates moments of technological change. In the digital medium, however, the photo-ancestry is lost and a new inscription formed. How do we then, perceive the photographic artifact in its computerised form? The question of recognition is central to the argument of reading the digitised image.  How do we, as spectators of the photograph, read the image? The photographic image bears a likeness to its subject (icon) and is a physical extension of it (index). The photograph possesses an evidential force: it cannot be argued that what it captures, through the process of light falling on a photo-sensitive plate, was not there. The readings of a photograph is dependent on the layers of recognition that happen in the process of viewing the image. The recognition of the indexical contiguity is directly related to the spectator’s familiarity of the photographic subject. In the second instance, all images demand a recognition of purpose: this purpose can be the photographer’s own, or it could be one that the photograph creates for itself -- a new life for its subject. The photograph can only depict history in a bounded frame: it is unable to speak. Thus the recognition of purpose of the image provides readings into the contextual framework within which the image is placed. Slaughter Ghat, Cawnpore (BL, Photo 193/20) presents us with a topographical space -- a river bank where several small shrines border upon a lower mud shelf above the river, on the banks of which are tethered two country boats. The intention of the photograph is not to portray a simple country scene (though it might) but to draw our attention to the site of a brutal massacre where hundreds of British refugees were fired upon and killed by rebelling forces in India. The purpose, then, becomes an instrument in the reading of the photograph. Re-enforcing this view, John Berger (1972: 10) writes:  \"Every image embodies a way of seeing. Even a photograph. For photographs are not, as is often assumed, a mechanical record. Every time we look at a photograph, we are aware, however slightly, of the photographer selecting that sight from the infinity of other possible sights.\" The third and final instance is that of a recognition of source and this is of significant importance for the archival image. The little that we can articulate about the photographic image is derived from an understanding of the history of the object (the physical photograph). How we proceed to classify the image, place it within numerous other photographs is dependent on how we recognise the photographer, the period, the photographic plate and the photographic process. The history of the object is vital in our attempts to place it within the structures of a digital collection.  The naked digital artifact is wrapped in an envelope of tags and markers in an attempt to locate and describe the object. The categories described within the catalogue are translated in the digital medium as metadata. Metadata standards establish a common understanding of meaning or semantics of data. This aids proper use and interpretation of the data by its owners and users. My paper will explore the formation of metdata through the analysis (a combination of statistical and close-reading) of a body of annotated photographs. I propose to demonstrate means of extracting meaningful metadata which addresses the theoretical issues stated above, in order to place them within established, standardised formats. I will also demonstrate new methods of visualising large image collection through the use of this form of analysis. While my paper will focus on early photography from colonial India (1850-1900), the scalability of such a project will also be a point of consideration in this paper.   ",
       "article_title":"Readings of a photograph: Cognition and Access",
       "authors":[
          {
             "given":"Vinayak",
             "family":"Das Gupta",
             "affiliation":[
                {
                   "original_name":"Trinity College Dublin, Ireland ",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "metadata",
          "cultural studies",
          "sustainability and preservation",
          "repositories",
          "art history",
          "content analysis",
          "archives",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The goal of this paper is to present some reflections about the process of building things in Digital Humanities. It is based on our own experience in developing an analytic tool to study Internet Relay Chat (IRC) conversations within hacker and free and open source software communities.  Questions have been raised recently about the epistemology of building and of built artifacts within Digital Humanities. Following Lev Manovich's provocative statement that a “prototype is a theory”, Ramsay and Rockwell have argued that the activity of building a digital prototype should be “capable of providing affordances as rich and provocative as that of writing” (Ramsay and Rockwell 2012, 83). Galey and Ruecker (2010), for their part, propose that the prototype should be received as conveying an argument, as would a book or article, and be evaluated as such. These reflections are interesting in that they propose to go beyond the mere building of a tool to a thinking about things and the building process as valid scholarly contribution. We would like to pursue this line of reasoning but instead of arguing the epistemological validity of tool, we propose to consider the very building process as a methodological and ethnographically-oriented opportunity to reflect on the studied material and the design process. In a sense, we follow Phil Agre's approach, recently re-mobilized by Software Studies theorist Warren Sack (Forthcoming), in pursuing “a technical practice for which critical reflection upon the practice is part of the practice itself” (Agre 1997, xii). The tool we will present, IRCMine1, was developed in the first part of 2013, within a wider context concerned with data mining conversations and interactions in hackers communities (such as free and open source software and Anonymous). Indeed, although many tools were developed (or are still being developed) to study different aspects of free and open source software online communities – tools for the analysis of mailing list, bug trackers or repository commits  – the analysis of conversations from IRC logs remains neglected. It is still more important to look at this, since IRC is being used increasingly within free software communities, as an open, synchronous, group conversation protocol. Moreover, IRC is a tool of choice for many hacktivist groups such as Anonymous that coordinate their action in this space (Coleman, 2013).    We propose three axes of reflexive exploration about our experience in building the prototype:  1) Reflection about the studied material. The first axis of reflexive analysis concerns the material, and especially the format of the log files. Our design practices brought us to consider more closely the log files format and the form interactions held in IRC channels. For instance, what could be considered as a conversation in IRC files? Considering the close imbrications between metadata and messages (content), do we consider IRC files as a text? Also, the very choice of looking at IRC conversations – instead of mailing lists or commit logs – can also be reflected upon, since it was justified by the need to look at a less visible space of interactions. In a sense, choosing to give visibility to this space was also a choice about giving visibility to some kind of work over others (Star and Strauss 1999). 2) Ethical aspects of designing the interface. A second set of concerns is  related to ethical concerns, such as having a balance between ease of use and keeping the confidentiality of the studied data. Indeed, most of the time, IRC logs are not available publicly and can only be collected by the researcher, thus posing questions about confidentiality of the data. This presented some important conceptual and technical challenges since we decided to develop a web-based tool (JavaScript, HTML, etc.), thus relying on the web browser to execute the code. Although it could be easy for a technical person to install this code on a local machine and ensure the security, how do we design an interface so that users can trust that the data being analyzed will stay confidential? How do we balance usability and performance on one hand, and security on the other? This axis of reflexive thinking is similar to the proposal of a value sensitive design where attention to values and ethical concerns are integrated in the very process of design (Friedman, Kahn, and Borning 2002; Le Dantec, Poole, and Wyche 2009). 3) Reflection on our design (and coding) practice. This was interesting since one member of our team (Couture) did his thesis on source code, and coding. The project allowed him to experience the actual coding practices (after a long hiatus of coding), especially related to modularity and the circulation of code objects. Although at the start the programming was done in a very ad-hoc manner, it soon became important to modularize the source code and have some consensus on programming standards and the organization of the files. In a way, the organization of source code was articulated to reflect the organization of our collective work. Our coding practice also allowed us to investigate and better understand the different technological resources available to – and often used by – free software coders and other programmers. It would, for instance, allow us to better understand the dynamics around the GitHub Platform, something that has already received some attention by scholars in social science (Takhteyev and Hilts 2010).  This paper will summarize the problematic as well as our objectives in the development of this tool. However, we propose to concentrate most of our presentation on a reflexive analysis of our own design activity in the development of this tool.  ",
       "article_title":"Beyond the Tool : A Reflexive Analysis on Building Things in Digital Humanities",
       "authors":[
          {
             "given":"Stéphane",
             "family":"Couture",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Stéfan",
             "family":"Sinclair",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "internet / world wide web",
          "software design and development",
          "interface and user experience design",
          "digital humanities - nature and significance",
          "programming"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1 Introduction  The spatial variability found in dialects is an essential indexical property that is highly salient to listeners in everyday language situations: at social events, for example, one often hears conversations of the type “I have trouble localizing your dialect – where do you come from?”. Although listeners are typically unaware of the underlying linguistic mechanisms involved, they are actively engaging in perceptual dialectology (cf. Preston 1989, Clopper & Pisoni 2004) and they seem keenly aware of dialectal variation. It is interesting then that different language speaking groups seem to recognize dialects of their language with different degrees of accuracy. Leemann & Siebenhaar (2008) and Guntern (2011) show that naïve Swiss German listeners can accurately recognize a speaker’s dialect with a recognition rate of 86% and 74% respectively. However, Clopper & Pisoni (2005) report identification rates of only 30–50% for American and British English dialects; Kehrein, Lameli & Purschke (2011) report similar recognition rates for German dialects. Recent studies show that dialect recognition is possible via the mobile application Dialäkt Äpp (Leemann & Kolly, 2013; Kolly & Leemann, in review). This contribution describes work in progress: Voice Äpp, currently in development at the University of Zurich, is a follow-up project on Dialäkt Äpp. The main purpose of both smartphone apps is to identify users’ dialects on the basis of the dialectal variants of 16 words. Dialäkt Äpp users provide their pronunciation through tapping on the corresponding variant on the smartphone screen. However, the new Voice Äpp asks users to pronounce the word and uses automatic speech recognition (ASR) to identify users’ pronunciation variants. The ASR training for Voice Äppis partly based on acoustic data crowdsourced through Dialäkt Äpp.Voice Äppfurther aims at illustrating the individuality in users’ voices by providing a multidimensional profile of their voice. The launch of Voice Äppis planned in December 2014. Several research teams are interested in creating similar applications for other languages, using the frameworks put forth by Dialäkt Äpp and Voice Äpp: Mobile applications that recognize regional varieties of the entire German-speaking area, of American English, of British English, and of Italian, are currently under development.  2 Crowdsourcing data with Dialäkt Äpp  In 2013 we launched the iOS application Dialäkt Äpp, which capitalizes on the Swiss public interest in dialectology (Leemann & Kolly, 2013). We provided a functionality that, on the one hand, allows users to localize their own Swiss German dialect by indicating their pronunciation of 16 words (see Figure 1). Given the task to predict Swiss German dialects, a model was built by phoneticians who devised a set of maximally predictive words (i.e. maps from the Linguistic Atlas of German-speaking Switzerland: Sprachatlas der Deutschen Schweiz (SDS, 1962–2003)) that capture dialectal differences between localities. On the other hand, users can record their own dialect and listen to recordings of other users, thus discover the Swiss dialectal landscape. Figure 1 shows three screens of the application: the choice of dialectal variants for the word Donnerstag‘Thursday’, the identified localities as a list and on a map (Bern being the best hit in this example) and the distribution of users’ recordings covering German-speaking Switzerland.   Fig. 1: Screens of Dialäkt Äpp: (1) choice of dialectal variants with buttons; (2) result provided as a choice of five best hits and their corresponding positions on a map; (3) users’ recordings (one pin per locality)  Dialäkt Äpp was launched on March 22, 2013, and has been downloaded over 58’000 times (as of February 28, 2013). The data recorded by this application contains (a) (written) choices of pronunciation for 16 words by each user who localized his/her dialect and (b) audio data for the same 16 words by each user who chose to record his/her voice. For (a), the corpus contains data from over 42’000 subjects (58% males, 42% females). Most users are from the cantons (and capitals) of Zurich, Bern, Basel, Luzern, Aargau, and St. Gallen. 64% of the users’ pronunciation variants still correspond to the local variant recorded by the SDS (1962–2003) in the 1940’s and a large number of users report that the localization of their dialect by the application is very close to their dialectal origin. For (b), the corpus counts 38’477 recorded variants stemming from a total number of 2’633 iOS devices (which corresponds roughly to the number of speakers; 54% males, 46% females). The geographical distribution of users corresponds to that of the data presented in (a). The data elicited by Dialäkt Äpp has great potential for dialectological as well as forensic phonetic research. It can be used to create new dialect maps and compare them to the maps published in the SDS (1962–2003), thus to track sound change in progress. A number of maps have already been created (for the words Apfelüberrest‘apple core’, Bett ‘bed’, schneien‘to snow’, Tanne‘pine tree’, and tief‘low’). Preliminary analyses show that phonetic isoglosses, as illustrated in maps like Bett(quality of /e/) and Tanne (quanity of /n/) are congruent with data from the SDS (1962–2003) (Kolly & Leemann, in review). The data can also be used to compare dialects at the acoustic phonetic level: For example, preliminary results show differences in speaking rate between the Bern dialect and the Zurich dialect (Leemann, Kolly, & Dellwo, accepted). Furthermore, this corpus can be used to create population statistics for a variety of phonetic parameters, which is desirable for forensic phonetic voice comparison (cf. Nolan et al., 2009).  3 Development of Voice Äpp  Voice Äpp has two major aims: - To use ASR techniques to localize users’ dialects - To provide users with a multidimensional profile of their voice  3.1 ASR-based dialect localization  The novelty of this new project is to use ASR techniques instead of multiple choice buttons. Some difficulties can be expected as the ASR approach is not error-free, especially through a mobile application: recording conditions may vary a lot due to the distance from the microphone, noisy environments etc. However, the high-resolution microphones of smartphones, iPhones in particular, should facilitate the ASR task. Furthermore, identifying dialects, where small variation has to be taken into account, is not the initial purpose of ASR systems; the speech recognition domain aims at normalizing such variation and at being rather dialect- or speaker-independent. In addition to this, the number of possible pronunciation variants for each word is important. For example, the word Bett ‘bed’ only counts two variants in the SDS (/bet/ and /bεt/) whereas Augen ‘eyes’ has eleven dialectal pronunciation variants. Theß latter is highly discriminant – but the ASR task is more difficult. The algorithm will have to be modified since the voice recognition approach is not as reliable as the selection with buttons. In order to achieve this, an ASR system is trained with two corpora: (a) the Dialäkt Äpp corpus described in 2 and (b) the TEVOID corpus (Dellwo, Leemann, & Kolly, 2012). Corpus (a) contains about six hours of speech of over 2’600 speakers, covering a dense net of local dialects in German-speaking Switzerland. Each recording is an isolated word from a set of 16 words. Corpus (b) contains two hours and 45 minutes of speech of 16 Zurich German speakers. Each recording is either a spontaneous or a read sentence. While the second corpus has been segmented by hand, the first one needs data preparation and verification as it was collected without control of linguistic content nor acoustic environment. So far, encouraging results are obtained with limited training data. After ASR training with five variables from the Dialäkt Äpp corpus, dialect word recognition has reached accuracies of 92% (Bett‘bed’), 90% (Kind ‘child’, Apfelüberrest‘apple core’), 85% (Tanne‘fir tree’), 79% (fragen‘to ask’). These accuracies may increase with larger amounts of training data, which is currently being worked on.   3.2 Multidimensional voice profile and infotainment content  The second function of the Voice Äpp is a voice profile provided to the user. Based on a sentence recorded in their dialect, users learn about characteristics of their own voice in a playful way. A number of menus allow users to explore different aspects of speech, e.g. pitch, speech rate, articulation, auditory and visual perception. Pitch: The fundamental frequency (f0) of the users’ sentence is calculated and displayed in a histogram representing the distribution of the f0 of all the previous users. Speech rate: The speech rate of the users’ sentence is calculated and displayed in comparison to the previous users’ speech rate. Articulation: Users learn about sounds and their articulation. Upon clicking on an IPA symbol a sagittal cut is shown and the sound is played. In an interactive sagittal cut users move the position of the articulators and hear the corresponding vowel sound. Auditory perception: Users can listen to what their sentence would sound like to a person with a hearing impairment/a cochlear implant. Visual perception: Users are shown a video illustrating the McGurk effect (MacDonald & MacGurk, 1978) and the Cocktail Party Effect (Handel, 1989). Both effects illustrate that visual cues can be crucial for speech perception.  4 Conclusion  Voice Äpp should be as interactive as possible, allowing users to learn about the individual features of their dialect and their voice in a playful way. As shown by Dialäkt Äpp, a mobile application such as Voice Äpp is interesting for the user as well as for the researcher: by providing appealing content to the user, we gain large amounts of data. This crowdsourced data can be used to create population statistics, for example for analyses of speech prosodic features. In particular, Voice Äpp creates real time f0 and speaking time statistics, which represents a novelty for e.g. the field of forensic phonetics.  Acknowledgements  The project Swiss VoiceApp – Your voice. Your identity is funded by the Swiss National Science Foundation (SNSF); funding scheme: Agora; grant number: 145654.  ",
       "article_title":"Swiss Voice App: A smartphone application for crowdsourcing Swiss German dialect data",
       "authors":[
          {
             "given":"Marie-José",
             "family":"Kolly",
             "affiliation":[
                {
                   "original_name":"University of Zurich, Switzerland",
                   "normalized_name":"University of Zurich",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02crff812",
                      "GRID":"grid.7400.3"
                   }
                }
             ]
          },
          {
             "given":"Adrian",
             "family":"Leemann",
             "affiliation":[
                {
                   "original_name":"University of Zurich, Switzerland",
                   "normalized_name":"University of Zurich",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02crff812",
                      "GRID":"grid.7400.3"
                   }
                }
             ]
          },
          {
             "given":"Volker",
             "family":"Dellwo",
             "affiliation":[
                {
                   "original_name":"University of Zurich, Switzerland",
                   "normalized_name":"University of Zurich",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02crff812",
                      "GRID":"grid.7400.3"
                   }
                }
             ]
          },
          {
             "given":"Jean-Philippe",
             "family":"Goldman",
             "affiliation":[
                {
                   "original_name":" University of Geneva, Switzerland ",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          },
          {
             "given":"Ingrid",
             "family":"Hove",
             "affiliation":[
                {
                   "original_name":"University of Zurich, Switzerland",
                   "normalized_name":"University of Zurich",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02crff812",
                      "GRID":"grid.7400.3"
                   }
                }
             ]
          },
          {
             "given":"Ibrahim",
             "family":"Almajai",
             "affiliation":[
                {
                   "original_name":" University of Geneva, Switzerland ",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "interfaces and technology",
          "geospatial analysis",
          "teaching and pedagogy",
          "linguistics",
          "speech processing",
          "crowdsourcing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Perspective in novels has been an important subject of research in literary studies. Ishimaru (1985) defined perspectives as the viewpoint of narrators; she roughly classified perspectives in novels as the first-person perspective, where the central character narrates the story from his/her perspective, and the third-person perspective, where the omniscient narrator recounts the story from a neutral perspective. This is a basic classification of perspective in literature. These perspectives represent the spirit of the age, typically shown in the positivism in 19th century French novels (Ishimaru, 1985), and also affect a readers’ impression of the characters and involvement in the work, and thus perspective is an important subject in literary studies.  Computational stylistics has been one of the important subfields of Digital Humanities. Using computational methods with digitized text materials, we can obtain systematic findings that can complement traditional qualitative analyses. Although computational methods can be powerful tools for investigating issues in literary studies, perspective in novels has rarely been analyzed with such method. Against this background, we used computational stylistic methods, i.e., text classification and feature analyses by random forests machine learning methods, to tackle the perspective issue in literary studies. We selected Kotaro Isaka, who is a popular Japanese novelist, as the object of study; he explicitly switches perspective in his novels section by section, and this is an important reason for the popularity of his novels. Note that Haruki Murakami, another popular novelist, uses this perspective switching between two perspectives (Kudo et al., 2012). However, Isaka uses more varied perspective-switching patterns (Yamashita and Suzuki, 2013). First, we generated text files and applied morphological analysis. We then conducted random forests text classification and feature extraction experiments using text-feature matrices for two of Isaka’s novels. Then, we investigated (a) whether textual differences among perspectives can be detected or not, and (b) if detected, what types of textual characteristics contribute to the detection of perspective. By tackling these points, we will show the effectiveness of computational methods for analyzing the perspective issue in literary studies. 2. Data and methods We selected the following novels by Kotaro Isaka, “Odyubon no Inori” (Audubon’s Prayer; ADP, original 2000, pocket edition 2003) and “Gurasuhoppa” (Grasshopper; GHP, original 2004, pocket edition 2007) as objects. ADP is a work representative of the earlier period of the author’s bibliography, and GHP is representative of the author’s middle period. We used the pocket editions of these two novels because Isaka is known to revise manuscripts when his work is published in pocket editions. We constructed the texts using a OCR document scanner and manually corrected OCR errors. We also removed the rubi, i.e., kanas printed alongside kanjis. We applied morphological analysis using MeCab,[1] Japanese morphological analyzer. We divided the texts into sections and assigned perspective tags according to the perspective signs assigned by the author. Regarding ADP, we united all character perspectives except Ito, the central character, because the number of perspectives for each character is small. Without unification of perspective, it was difficult to perform meaningful classification and feature analysis experiments. Thus, we used two tags, Ito’s perspective and other characters’ perspectives. The numbers of sections was 56 for Ito and 22 for other characters. It should be noted that Ito’s section appeared after another of his section. Regarding GHP, we used three tags for the three main characters’ perspectives (Suzuki, Kujira, and Semi) according to the signs assigned by the author. The sequences of these three characters’ perspectives are essentially fixed, Suzuki first, Kujira second, and Semi third. In addition, the death of a character leads to the removal of that character’s perspective. The numbers of sections was 17 for Suzuki, 15 for Kujira, and 10 for Semi.  We calculated the frequencies of morphemes and basic textual statistics, and then we constructed the text-feature matrices using the relative frequencies of morphemes appearing in each text. We applied random forests machine learning methods proposed by Breiman (2001) with these matrices as data and perspectives as labels. We calculated the valuable importance provided by random forests and extracted important variables for classification, which are effective for differentiation among perspectives. We selected the random forests method because it has shown the best possible performance for authorship attribution in Japanese (Jin and Murakami, 2007) and is effective for extracting and analyzing the features that contribute to classification in related tasks such as computational sociolinguistics (Suzuki, 2009). 3. Results and discussion 3.1. Basic observation Table1. Basic data (ADP)      Number of tokens      Number of texts   sum   mean   s.d.   c.v.     Ito   56   118042   2107.89   1712.37   0.81     Others   22   23290   1058.64   1315.31   1.24     Table 1 shows the basic data for ADP, the number of texts, and the sum, mean, standard deviation (s.d.), and coefficient of variations (c.v.) of the number of tokens for each perspective. It can be seen that Ito has more that 70% of all sections, and others have larger variances of the c.v. It is assumed that the larger variances were caused by the unification of characters.  Table 2. Basic data (GHP)        Number of tokens       Number of texts   sum   mean   s.d.   c.v.     Suzuki   17   51229   3013.47   1930.56   0.65     Kujira   15   33453   2230.2   1251.70   0.56     Semi   10   27153   2715.3   946.63   0.35     Table 2 lists the basic data for GHP, the number of texts, and the sum, mean, s.d., and c.v. of the number of tokens for each perspective. The table shows that Suzuki has the largest section numbers and has the largest c.v. It is assumed that Suzuki’s perspective includes both small and long sections. 3.2. Classification by random forests  Table 3. Classification results (ADP)      Ito   others   error rates     Ito   55   1   0.02      Others   17   5   0.77      Table 3 shows the classification results obtained by random forests for ADP. Each column represents the original tags, and each row represents the results. It can be seen that 55 of 56 Ito texts were classified as Ito’s. It is assumed that Ito’s perspectives have special characteristics. In comparison, only 5 of 22 texts by others were classified as others and 17 of 22 texts by others were classified as Ito. It is assumed that these results were partly caused by the limits of our experiments; the number of Ito texts was much larger than others, and the text from several characters was merged.   Table 4. Classification results (GHP)      Suzuki   Kujira   Semi   error rates     Suzuki   17   0   0    0     Kujira   0   15   0    0     Semi   0   5   5   0.45     Table 4 shows the classification results obtained by random forests for GHP. Each column represents the original tags, and each row shows the results given by random forests. It can be seen that all Suzuki texts were 17 were classified as Suzuki’s, and all Kujira texts were classified as Kujira’s. Only 5 of 10 texts were classified as Semi, and 5 other texts were classified as Kujira. It is assumed that there were special characteristics for Suzuki and Kujira’s perspective; however, in comparison, Semi’s perspectives were rather characterless and closer in nature to Kujira’s texts. It is worth noting that both Semi and Kujira are assassins, and Suzuki is an employee; therefore, it is assumed that the fact that Semi and Kujira are similar characteristics indicates the author’s intent to differentiate these two characters and Suzuki. 3.3. Feature analysis Table 5. Top 20 important features (ADP)      feature   readings   translation   pos   variable importance     1   僕   Boku   I   noun (pronoun)   0.01911     2   だ   da   be   auxiliary verb   0.00661     3   日比野   Hibino   Hibino   noun (proper)   0.00404     4   ん   n   -   noun, auxiliary verb, particles   0.00293     5   。   -   -   symbol   0.00267     6   を   wo   -   particle   0.00262     7   静香   Shizuka   Shizuka   noun (proper)   0.00253     8   」   -   -   sign   0.00246     9   声   Koe   Voice   noun   0.00214     10   しれ   shire   -   verb   0.00177     11   よ   yo   -   particle   0.00172     12   伊藤   Ito   Ito   noun (proper)   0.00165     13   かも   kamo   May   particle   0.00142     14   歯   Ha   Dent   noun   0.00126     15   に   ni   -   particle   0.00113     16   いや   Iya   -   exclamations   0.00106     17   島   Shima   Island   noun   0.00095     18   返事   Henji   Reply   noun   0.00094     19   目   Me   Eye   noun   0.00093     20   ？   -   -   symbol   0.00092     Table 5 shows the top 20 variables that contributed to classification of ADP with English translations, indicates parts of speech, and shows the variable importance obtained by random forests. The variables include many proper nouns and content words such as “島” (Shima; Island) which simply represent contextual difference in the narrative. Table 5 also includes stylistic characteristics such as pronouns that represent the differences between the perspectives of Ito and others.  Table 6. Top 20 important features (GHP)       feature   reading   translation   pos   variable importance     1   鈴木   Suzuki   Suzuki   noun (proper)   0.00947     2   妻   Tsuma   wife   noun   0.00938     3   比   Hi   -   noun (proper)   0.00812     4   亡き   Naki   dead   adnominal   0.00781     5   鯨   Kujira   Kujira   noun (proper)   0.00764     6   亡霊   Borei   ghost   noun   0.00699     7   僕   Boku   I   noun (pronoun)   0.00664     8   子   Ko   Ko   noun (proper)   0.00560     9   槿   Asagao   Asagao   noun (proper)   0.00524     10   西   Nishi     noun (proper)   0.00477     11   岩   Iwa   -   noun (proper)   0.00475     12   与   Yo     noun (proper)   0.00452     13   彼女   Kanojo   she   noun (pronoun)   0.00393     14   ねえ   nee   -   noun   0.00367     15   おまえ   Omae   you   noun (pronoun)   0.00354     16   長男   Chonan   eldest son   noun   0.00322     17   君   Kimi   you   noun (pronoun)   0.00297     18   つう   Tsuu   -   auxiliary verb   0.00268     19   なかっ   nakatt   -   auxiliary verb   0.00254     20   だろ   daro   -   auxiliary verb   0.00224     Table 6 shows the top 20 variables that contributed to the classification of GHP with translations in English, indicates part of speech and presents the variable importance obtained by random forests. Table 6 includes many [part of] proper nouns, indicating that they are the most important characteristics for discriminating the perspectives of the three main characters. In addition, Table 6 includes “つう” (Tsuu) and “ねえ” (Nee), which are style markers specific to several characters (e.g., Kujira) This indicates that these special style markers are also important characteristics for discriminating the perspectives among the three main characters. 4. Conclusion This study analyzed the textual difference among perspectives in two contemporary Japanese novels. The results indicate that (a) respective perspectives have their specific textual characteristics, (b1) textual characteristics such as proper nouns that represent respective scenes are important for discriminating perspectives, and (b2) stylistic characteristics such as pronouns and nouns that represent styles of speech are also important. We conclude that computational stylistic methods can differentiate among perspectives in contemporary novels.  This study is a preliminary analysis of the study of perspectives using computational stylistic methods and is also part of an ongoing study of Kotaro Isaka’s work. In future, we would like to further investigate the effectiveness of computational methods for perspective issues and continue to analyze other work by Kotaro Isaka. Acknowledgements This study was supported by Grant-in-Aid for Scientific Research 23700288 for Young Scientists (B), from the Ministry of Education, Culture, Sports, Science and Technology, Japan. An earlier version of this study was presented at the 19th Annual Meeting of Japanese Natural Language Processing (NLP2008) at Nagoya University. This research includes revised and expanded content based on the gradation thesis presented by Natsumi Yamashita to the Faculty of Sociology, Toyo University.  ",
       "article_title":"Analysis of perspectives in contemporary Japanese novels using computational stylistic methods",
       "authors":[
          {
             "given":"Takafumi",
             "family":"Suzuki",
             "affiliation":[
                {
                   "original_name":"Toyo University, Japan ",
                   "normalized_name":"Toyo University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/059d6yn51",
                      "GRID":"grid.265125.7"
                   }
                }
             ]
          },
          {
             "given":"Natsumi",
             "family":"Yamashita",
             "affiliation":[
                {
                   "original_name":"Toyo University, Japan ",
                   "normalized_name":"Toyo University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/059d6yn51",
                      "GRID":"grid.265125.7"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Collective Biographies of Women, is an open-access project supported by the Institute for Advanced Technology in the Humanities, Scholars’ Lab, and the English Department at the University of Virginia, as well as an ACLS Digital Innovation Fellowship. In recent years it has grown from an online bibliography of all English- language books that collect three or more short biographies of women into a digital prosopography that interrelates women, printed books, and narratives in what we call documentary social networks (introduced at DH 2013). CBW stands out as a literary study of prosopographies in the print era, and primarily the transatlantic nineteenth century (see the bibliography, http://womensbios.lib.virginia.edu). Most research that employs the term prosopography allies itself with history or classical and medieval studies, and today, relies on databases and websites. We work with the concept as it is often defined, as collective biography, that is, printed prose collections of short biographies (see the selective bibliography for a context on prosopography, nonfiction narrative, and our method of mid-range reading).  The CBW database associates some 8700 persons, 13,000 chapters (biographies), and more than 1200 books of various types published in English 1830-1940 (see developing database at http://cbw.iath.virginia.edu/cbw_db). Our project, however, is neither a textual archive nor a biographical database but an experiment in interpretation using the tools of DH to recognize the conventions of a genre, biography, and the history of gender conventions in a certain social context. Specifically, we want to get at the conditions of nonfiction, which generate multiple versions and cut and paste with relatively little respect for authorship. Could narrative theory of nonfiction be developed through a technique of digital markup that allows us to compare multiple versions of one life and interrelated types of person and text? With Daniel Pitti, Suzanne Keen, postdoctoral Project Manager Rennie Mapp, and teams of graduate assistants, we have developed and deployed a stand-aside XML schema, Biographical Elements and Structure Schema (BESS), in sample archives of digitized collective biographies that include designated individuals (e.g. all collections in our bibliography that include Caroline Herschel, the astronomer).  Briefly, BESS is an XML schema with a controlled vocabulary for narrative elements that appear in a given text:    StageofLife: before, beginning, middle, culmination, end, after,  relative to the lifetimeofthebiography’ssubject   EventType e.g. illness, persona’s   AgentType e.g. mother, unnamed    Setting:   Location, e.g. city   Structure, e.g. school    Time: Dates, TimeofDay, Season     PersonaDescription e.g. physically daring   Discourse: e.g. retrospective, figureOrImage flower   Topos: e.g. influence, disgrace   Each editor in a trained team creates a separate XML file that in effect is an annotated outline, tagging types of elements identified in numbered paragraphs of a TEI file of the biographical narrative (from 3-100+ paragraphs).      BESS analysis enables us to compare versions of the same person’s life. When we have analyzed all versions in our corpus, we give unique ID numbers to the essential events in all versions (kernels) and the more or less common optional events (satellites, common or rare), and can compare the placement of these in the versions, much as folklorists have charted the variations on the main events of a tale. (Narrative theorists have developed analyses of events in these terms, but not for nonfiction.) BESS analysis reveals differences in narrative technique in books that take different perspectives on women’s roles and that select persons of different types. Thus, beyond the literal level of actions (events), we can measurably correlate, for example, the instances of direct address, use of ‘we’ alongside not only the topos (i.e. situation; underlying scenario) work as social service but also the topos temptation of status or goods. The conjunction of different elements in these biographies often challenges our own later assumptions about historical women and gender norms. As BESS work is completed, we expand a body of data that for the first time documents the distinctive characteristics of third-person narratives about real people.   Fig. 1:   Currently working with a web designer, we have a sustainable, accessible database that functions well for team workflow, with parallel display of text and BESS analyses. We plan to develop the visualizations of BESS beyond current designs of tables and graphs.    Fig. 2:   The CBW project’s BESS “reading” of many narratives is time-consuming and detailed, as in many literary digital projects. As in all DH, we encounter challenges when visualizing quantities of variable data, an issue that this paper merely touches upon. Our aim, instead, is to introduce and make a case for the mid-range approach of BESS, with some reference to other possible approaches.  Many methods of text “reading” en masse might be useful with the CBW books. Broadly, options range from word strings and topic clusters across a large corpus of digitized texts to systematic encoding of all textual features and variants in an onlineß edition of an archive (e.g. Online Froissart; the Rossetti Archive). On the vast end of the scale, we recognize the astounding range of a Google N-gram kind of data capture as well as the precision of some text-mining projects. On the closer focus of the scale, we think human curation is best suited for patterns of narration and ideology, and we begin with books and place them in a context of genre and publishing history. Thus we try to benefit from the precedents of literary editions, and yet CBW is not a project in textual editing. We have no wish to fine-tune exact digital surrogates of these books. The BESS schema and approach to many-versioned biographies within social and historical contexts is designed to moderate between distant and close reading—a comprehensive digital model of variations within a genre and fine textual details.  Many in DH have addressed the question of what to do with a million books. Franco Moretti has espoused distant reading, a term applicable to many kinds of directed and unsupervised queries in big archives. This is understood as opposed to \"close reading,\" the usual literary method (without computational mediation), one text, one person. The findings of singular textual analysis are less appropriate when describing patterns across a genre, especially of nonfiction where there are many versions of the “same” narrative and the author is less important than the protagonist, the representation of a real person. Thus, our method has some affinity for Sharon Marcus and Stephen Best’s concept of “surface reading,” as advocated in their manifesto to put a stop to the required \"critique\" or theoretical digging into a text for what it does not say for ideological purposes. Yet we retain a framing conceptual commitment to ideological critique, as we want to know about the changing gender ideology and historical contexts for women’s lives. More directly, we are pursuing the kind of \"social reading\" promoted by Alan Liu, as we hope to extend BESS as a tool available for other projects in digital interpretation of biographical narratives. BESS, interlinked with a database that reveals networks among historical persons and books about them, recruits computation to aggregate the interpretations of readers to parse a genre.  We call for a new metaphor or spatial model for hybrid methods of mid-range digital interpretation, whether using a stand-aside markup like BESS or other approaches. Our editing teams “hover,\" not at satellite level, but like balloon aerial digital cameras scanning a neighborhood, producing images that can zoom in and out. Such records do much less harm to the person, the individual record, than surveillance or drones. Our schema functions, alternatively, like GPR, \"ground-penetrating radar,\" used to detect buried structures two or three feet into the ground. Although such metaphors for our mid-range reading method with BESS have the inherent comedy of balloons and robot- like go-carts, we can seriously enhance what we know above or beneath the surface without destroying it—without murdering to dissect. I remind the BESS team that the texts are always still awaiting any method of interpretation, unmangled after we have subjected them to our skewed adaptation for prosopographical purposes. We’re not pretending to let machines discover without distortion. Nor are we clinging to the requirement that reading is an individual act—on the contrary. Like reading, biographies trope toward the collective and typological, the mid-range, even in monographic form. Inviting open-access play, we expect to be surprised by the details in the mosaic or wave- like picture from above or below.   ",
       "article_title":"An XML Schema to Interpret Networked Biographies: Reading Mid-Range",
       "authors":[
          {
             "given":"Alison",
             "family":"Booth",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Worthy",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":"University of Virginia, United States of America ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "networks",
          "literary studies",
          "gender studies",
          "relationships",
          "xml",
          "text analysis",
          "crowdsourcing",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The appreciation of literature is a subjective process. In reading and judging books, characteristics of individual readers interact with characteristics of books and their reputation. This paper looks at book ratings on a book discussion site and tries to assess the role of individual readers’ characteristics in these ratings. For that purpose, the paper inspects on the one hand the textual properties of the review texts that readers contribute to this site, and on the other hand the ratings that they assign. Given the well-established connection between word use frequencies and authorial style (e.g. Burrows, 2002; Burrows, 2003), the paper hypothesizes that these same style markers in texts by readers will correlate with these readers’ quality judgments about books. Patterns in word usage are known to reflect aspects of readers’ psychological make-up (Argamon et al., 2005; Noecker et al., 2013; Pennebaker et al., 2003), and these psychological properties, e.g. the Big Five personality dimensions, are related to aesthetic preferences in many fields (Golbeck and Norris, 2013; Gridley, 2013; Zweigenhaft, 2008), including books and literature (Cantador et al., 2013; Wiersema et al., 2012). Aesthetic appreciation has been shown to be a multi-faceted process (Myszkowski et al., 2014; Rentfrow et al., 2011). Here, I assume that literary appreciation is influenced by multiple aspects of the reader’s psychology, such as, among others, his/her cognitive, affective and social dispositions. Therefore, besides investigating the over-all most frequently used words, as stylometry often does, I will also look at the high frequency words within the categories of cognitive, affective, and social words, as defined by LIWC (Pennebaker et al., 2007). I expect that the relative frequencies of e.g. individual social words (rather than the category frequencies that LIWC-based research typically uses) will capture to some extent the nature of a person’s sociability and will to that extent also reflect how that sociability affects literary preferences.   Data The data for this paper come from Dutch book discussion site watleesjij.nu (whatareyoureading.now). The site is similar to e.g. Goodreads, LibraryThing or lovelybooks.de: users rate, label and review books, they can evaluate reviews by others, can strike up friendships with and send messages to other users. I downloaded the site’s content in June 2013. I investigate review texts and ratings contributed by the top 20 (in terms of total review length) contributors to the site (I removed two accounts that seemed to be used by multiple persons.) For each of these users, I create a file containing all of the review texts this user has contributed to the site. The average word count is 44036. I also collect the ratings (in terms of one to five stars) for all of the 624 books that were rated by at least two of the twenty users.   Method and results As a first step, I compute correlations between the word use frequencies in each of the word categories and the book ratings. The word frequencies are represented as a matrix of zscores, where users are rows and words are columns. For the computation of the zscores I use Eder and Rybicki’s stylo script (2011), then select only those words that form part of the relevant LIWC category. The ratings are given in a matrix with users as rows and books as columns. Non-rated books are represented by 0. To assess the correlation between these matrices I rely on the (bias corrected) distance correlation and the associated significance test as described by Székely and Rizzo (2013). Table 1 reports the results, including the number of words that gave the best results for each category (However, for all categories except Affect the correlations were significant at the .01-level even for the top 25 words.) The table also gives the percentage of words belonging to the category in the review files.   Table 1. Bias-corrected distance correlation between word usage and book appreciation for different word categories    Category   bias corrected distance correlation   p-value   Optimum number of most frequent words   percentage of words in category     All words   .49   <.0001   2900   100     Function words   .36   <.0001   250   50     Affect words   .21   .0025   225   3     Cognitive words   .35   <.0001   375   18     Social words   .47   <.0001   125   12    The table shows that frequencies in each of the word categories are quite significantly correlated with the word ratings. The relatively low effect from affective words may be due to the low percentage of affect words in the texts.   The most striking result is no doubt the performance of the category of social words. In order to further investigate this effect, users were clustered in two groups, based on their usage of social words (I employed the pam partitioning function in R.) I then looked at contrastive word use of these clusters and at the books liked by the cluster members. The oppose function from Eder and Rybicki (2011) was used to find words preferred by either cluster. The results are given in table 2. The first cluster shows an interest in people and especially family that the second cluster, with its mostly cognitive or procedural interest, seems to lack entirely.    Table 2. Words preferred (from all words) by the two clusters (translated from Dutch). For cluster one, only the top 20 preferred words are given    Cluster   Preferred words     1   daughter, parents, family [nuclear], mother, woman, together, father, children, past, young, child, debut, house, brother, women, tells, love, marriage, family [extended], care     2   so, perhaps, page, of course, pity, well, read, precisely, actually, just, immediately, think, for instance, part, viz., believe, even, sort of, interesting, by the way    In order to find out the sort of books preferred by the clusters, I summed the book ratings by cluster. I then selected and diagrammed a subset of books, consisting of the ten books best liked by either cluster, the ten books best liked ‘contrastively’ by either cluster (computed by subtracting the ratings for cluster 1 from those for cluster 2), and the ten books best liked by both. After removal of duplicates, thirty books remained. Figure 1 displays the books with their ratings by the two clusters. Point and title size reflect popularity on the site. Grayscale represents genre. Point positions were slightly changed to avoid overlap. Lines between title labels and points were suppressed in the interest of clarity.    Fig. 1: Books as rated by the two clusters.  The figure seems to show some systematic differences in the preferences of the two clusters. Cluster 1, that uses mostly family-oriented words, seems to prefer slightly more popular books (larger point size). Cluster 2, that uses procedural or cognitive words, has strong preferences for a number of staunchly ‘literary’ works, such as those by Grass, Binet and classical Dutch authors. As to a potential preference for suspense novels, this figure does not allow us to draw any firm conclusions.   Discussion The reason why different people prefer different books has often been sought in differing literary norms (e.g. Von Heydebrand and Winko, 2008). This explanation is not quite satisfactory, for two reasons: first because it does not explain why people develop different norms, and second because there are no a priori reasons why norms rather than, say pleasure or ‘thrills’ (Konecni, 2005) should determine one’s preference for one book over another. This paper takes another approach and the results presented here tentatively establish the existence of a correlation between book preferences and patterns of word usage in several psychologically meaningful categories. Especially the relation between the pattern of usage of social words and literary appreciation seems very strong, confirming the importance of extraversion for aesthetic judgment noted by Furnham and Chamorro-Premuzic (2004), but appreciation is also clearly related to usage of cognitive words and of function words. There are some obvious limitations to this experiment. The number of subjects is very small (dictated by the need to have a sufficient number of words). It would also have been better to use texts from another domain. However, an exploratory analysis of the effect of clustering based on social word usage seems to show that the verbally more ‘social’ group prefers the less literary or more popular novel. Given the small numbers, more than provisional results should perhaps not be expected. Next steps should include clustering on the basis of other word categories, an investigation into the independent effect of these categories, and case studies at the level of individual readers. It would also be very interesting to see to what extent the literary norms that readers formulate in the reviews can be shown to be related to the word usage patterns as discussed here.  ",
       "article_title":"Dimensions of literary appreciation. Word use and ratings on a book discussion site",
       "authors":[
          {
             "given":"Peter",
             "family":"Boot",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The ",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "stylistics and stylometry",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The “Architectural Practice in Post-War Queensland: Building and Interpreting an Oral History Archive” project is a collaboration between the University of Queensland, the State Library of Queensland (SLQ) and four of the longest-standing architectural firms in Queensland. The project’s aim is to build a comprehensive online multimedia digital archive that documents architectural practice in post-war Queensland (1945-1975) – a period that was highly significant in Queensland’s architectural history but that remains largely undocumented. The goal was to use innovative Semantic Web technologies to link tacit knowledge extracted from individual oral histories to tangible knowledge (drawings, books, photographs, manuscripts) that exists within personal archives, firm archives as well as State and institutional archives and libraries.   The approach involved firstly conducting and recording a series of oral history interviews and public forums with the key architects from this period. These events comprise both private interviews, one-on-one conversations between the project team and architect/s as well as a number of larger public forums held at the SLQ that focus on a specific theme (education, style, climate, regionalism, etc.) The oral history interviews and the public forums are filmed, captured as digital files (.wav and .avi) and transcribed. Both manual tagging and text processing tools are applied to the transcripts to semantically tag key entities (architects, firms, structures, places, dates) mentioned in the interviews and extract new knowledge in the form of RDF graphs. The resulting RDF graphs document relationships between architects, firms and buildings (with attribution to the source) and are able to be displayed, edited, saved and re-used via the LORE compound object authoring software (Figure 4). This paper describes our approach to establishing the online archive and evolving knowledge-base1 that together have been designed to be used for research, teaching and practice within the disciplines of history, architecture and design.   An overview of the system architecture is shown in Figure 1. The system uses the Omeka content management system to support the upload and description of content (oral history files, transcripts, photos, drawings, articles etc.) by the project collaborators. In addition the system provides the following components and functionality:   An OWL Architecture ontology was defined that specifies the core classes, class hierarchy and properties associated with each class (Figure 1); D2RQ is used to convert Omeka metadata to RDF and save it to a Sesame RDF triple store with a SPARQL query interface; User-authenticated annotation tools enable users to semantically tag transcripts by identifying people, places, buildings, firms, and events mentioned in the interviews;  The EYE N3 Semantic Reasoner (N3) is applied to the Sesame RDF triple store to reconcile common entities (via URLs) and to infer relationships between key entities (architects, firms, structures/buildings and articles/publications); A Search and Browse engine (based on Solr) enables users to search for specific entities or perform full-text searching across all transcripts and articles (and jump to the audio/video segments that contain the matching search term); Word clouds and word frequency histograms (generated from the oral history transcripts using D3) enable architectural historians to understand the main themes and influences on key architects from this period; Mapping and timeline interfaces enable users to interactively browse and retrieve information (interviews, photos, drawings) about buildings, people or events via maps and timelines; The LORE tool enables the visualization, editing, sharing and re-use of RDF graphs that document relationships between architects, firms, buildings, and related documents (Figure 3)  At the time of writing this abstract, the archive/database contains 64 interviews, of average length 83 mins. It also contains 64 transcripts, 725 photos, 612 articles, 305 line drawings and detailed information about 464 architects, 119 firms and 357 buildings/structures. The archive is growing continuously as more interviews and associated content are uploaded and annotated. The architectural historians involved in the project and their students, review the transcripts and using the integrated annotation tools identify and tag the names of people/architects, firms/organizations, buildings/structures and places. As new people, structures, firms and places are tagged/identified, they are added to the ontology. Authenticated users can also annotate relationships between people, between people and firms and between people and buildings, by drawing on a controlled vocabulary of relationship types. The reasoning engine then reasons across these relationships to infer new implicit relationships that can be recorded, searched and visualized through the LORE RDF graph visualization tool. Architects who studied and worked in Queensland during the post-war period are also invited to register, login and submit their own details including a chronology of practice and to provide feedback to the existing content. An additional blog monitored by the project team encourages the broader community (those outside the profession) to comment on aspects of post-war architecture (e.g., nominate their favourite building) and to upload related materials such as photographs or plans. Future work plans include undertaking a detailed user evaluation of the system with a set of test users that comprises architectural historians from academic, government and industry as well as users from the local architectural community - and refining and extending the system based on user feedback. Finally, our paper will also describe the challenges that this multi-disciplinary project faces including: how to attract and retain an active community of contributors; ensuring the archive’s sustainability, resolving issues of identity resolution and implementing quality control over the community-generated content.   Biography: Professor Jane Hunter is the Director of the eResearch Lab at the University of Qld – where she leads a team of post-docs, PhD students and software engineers working on innovative e-research services for a wide range of applications and communities. She has published over 100 peer-reviewed papers on semantic web, digital libraries and e-research and is currently the Deputy Chair of the Australasian Association for Digital Humanities and Chair of the Academy of Sciences Committee for Data in Science. She is a CI on the Mellon-funded Open Annotation Collaboration (OAC) project, the NeCTAR-funded HuNI and Aust-ESE projects and the ARC Linkage Project “Architectural Practice in Post-War Queensland: Building and Interpreting an Oral History Archive”.   Fig. 1: Technical Components underlying the Post-War Queensland Architecture Knowledge Base    Fig. 2: Overview of the Ontology underlying the Post-War Qld Architecture Knowledge-base    Fig. 3: Screen Shot of the Web Portal: Digital Archive of Queensland Architecture    Fig. 4: LORE Visualization and Editing Interface to a Relationship Graph about Karl Langer   ",
       "article_title":"Extracting Relationships from an Online Digital Archive about Post-War Queensland Architecture",
       "authors":[
          {
             "given":"Jane ",
             "family":"Hunter",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"John ",
             "family":"Macarthur",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Deborah",
             "family":" Van der Plaat",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Janina ",
             "family":"Gosseye",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Andrae ",
             "family":"Muys",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Craig ",
             "family":"Macnamara",
             "affiliation":[
                {
                   "original_name":"The University of Queensland, Australia",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Gavin",
             "family":"Bannerman",
             "affiliation":[
                {
                   "original_name":"The State Library of Queensland",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "ontologies",
          "sustainability and preservation",
          "relationships",
          "repositories",
          "art history",
          "content analysis",
          "GLAM: galleries",
          "libraries",
          "archives",
          "semantic analysis",
          "museums",
          "networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The paper will address some of the most common and frequent needs and obstacles regarding legal issues in current digital scholarship (e.g. ownership of digital copies, electronic provision of source material) and demonstrate some of the consequent misconceptions, restrictions and legal traps which result from the lack of legal certainty due to the heterogeneous international legal situation regarding IPR and ancillary copyright.   While the free availability of sources has been a long-lasting demand and desire in all fields of research, open access to the results of scientific research has become a de facto obligation in recent years. This is reflected in the requirements of many national and international funding bodies demanding the public and free availability of research results and publications.  Generally speaking, humanities research focusses on products of the human mind – hence, the research object is usually subject to intellectual property rights. Largely based at universities, cultural heritage institutions or other public research institutions, that research is usually non-commercial and based on a public mandate for education, with little to no funding available for the acquisition of licenses and the proper remuneration of IPR holders. Open (and free) access to sources – especially those available only in cultural heritage institutions like archives and libraries – gains further importance because national funding agencies (e.g. the Austrian FWF) generally do not allow for the inclusion of license fees in their grants. On the other hand, researchers themselves have a keen interest in defending their own intellectual property rights, in part due to economic concerns but also in terms of academic credit. This conflict of interest is evident even in the Universal Declaration on Human Rights, Art. 27, which sets the premise that “(everyone has) the right freely […] to share in scientific advancement and its benefits”, but goes on to say that “everyone has the right to protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author”.   Several European Council Directives (2001/29/EC, 2003/98/EC, 2004/48/EC) have made a strong case for public access and free use of educational and scientific resources. This political agenda has been visible in the 7th EU framework and is also evident in several UNESCO publications (e.g. “Recommendation concerning the Promotion and Use of Multilingualism and Universal Access to Cyberspace” and the “Charter on the Preservation of Digital Heritage”, both dating back to 2003). However, the national implementation of these ideals is lagging behind: The actual legal situation regarding the use of and access to digital resources in many member states of the European Union (and the UNESCO, respectively) poses a number of difficulties. While Common Law legal systems – most prominently the US and the UK with their allowances for Fair Use and Fair Dealing – focus more on society’s interest in the access to and use of publications for education and self-improvement, the Civil Law systems found in continental European countries stress the rights of authors. Therefore, the usage, distribution and especially the electronic provision of resources require distinct free licenses, i.e. privileges for the educational sector. For non-digital material, a tried and trusted system of such privileges has been in place for decades. However, many countries – Austria among them – have so far failed to implement the necessary legal changes to extend these licenses to digital sources.   This ambiguity between the treatment of non-digital and digital resources poses another problem: Most humanists (or scholars in general, regardless of their respective domains) are unfamiliar with the legal implications of their work. Often drawing assumptions based on long-standing experiences and practices with non-digital material, few are familiar with the details of current legislature on digital sources. Also, though notable and admirable exceptions exist, there is generally also little to no support from universities’ legal offices.   Where source material is owned by universities or cultural heritage institutions, or has moved into the public domain due to the expiration of applicable protection periods (usually 70 years for printed materials), humanities scholars have little need to address such concerns. But more recent sources – especially when dealing with the current interest in Big Data – pose a number of legal challenges. Also, orphaned works (works without a known and retraceable author), while at first glance not subjected to the usual IPR restrictions, are dealt with very differently in the various European countries, but usually involve collecting societies. Furthermore, cultural heritage institutions often insist that the ownership of physical resources automatically induces a right to their digital copies and demand fees based on that claim – which may, in fact, be unfounded, either because the digitization (which in itself is not subject to IPR but rather of ancillary copyright) was not done by the institution or other legal obligations (e.g. in national or regional archival laws) oblige them to freely provide material in the public domain.The increase of collaborative work across not only disciplinary but also national borders adds another dimension to the already puzzling situation: Which legal system applies to resources hosted in different countries and which legal framework must be used for electronic publications?   A number of these questions can be addressed by taking a look at international IPR treaties like the Berne Convention for the Protection of Literary and Artistic Works, the Trade Related Aspects of Intellectual Property Rights (TRIPS) or the World Intellectual Property Organization (WIPO) Copyright Treaty.  The paper will therefor demonstrate some of the most common legal obstacles that humanities scholars encounter in the course of digital research and teaching and try to provide an overview of the current legal situation, the differences and common denominators of Civil Law and Common Law systems regarding IPR (especially the electronic provision of material) and the international framework of Copyright treaties. Since an exhaustive juxtaposition of international legal differences is unfeasible due to scale, the Austrian example will be used to showcase some of the most obvious and momentous shortcomings of current IPR legislature, and will be compared with German law (which has addressed some of these issues while still keeping strong restrictions in place) and the Anglo-American concepts of Fair Use and Fair Dealing. In conclusion, the paper will try to define a possible best practice which draws on the analysis of the common denominators found in international treaties, UK and US law and the EC Digital Agenda for Europe as expressed in recent EU directives.  ",
       "article_title":"Intellectual Property Rights vs. Freedom of Research: Tripping stones in international IPR law",
       "authors":[
          {
             "given":"Walter ",
             "family":"Scholger",
             "affiliation":[
                {
                   "original_name":"Center for Information Modeling - Austrian Centre for Digital Humanities, University of Graz ",
                   "normalized_name":"University of Graz",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/01faaaf77",
                      "GRID":"grid.5110.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "copyright",
          "licensing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction From a historian’s perspective, I present an approach to navigate large amounts of Internet Archive information, drawing on a case study of 4.7% of the top-level .ca domains preserved in a scrape of the entire World Wide Web, the March 2011 Wide Web Scrape. Every day, users record their thoughts, feelings, locations, ratings, votes, reviews, jokes, and so forth; an assemblage of traces of the past that historians will be able to mold into narratives. Here, I explain one way to access them beyond the WaybackMachine using open-source tools such as WARC Tools, Apache Solr, and Carrot2 Workbench.  2. Literature Review and Project Rationale Information scholars and digital archivists are having a conversation around digital preservation and web archiving.1 However, there is a need to approach these issues from the perspective of a historian with an interest in using web archives. My focus is on use rather than preservation. The current way to access this material is through the WaybackMachine, run on the Internet Archive’s server itself at archive.org/web or as a local installation.2 The WaybackMachine is simple from a user perspective: one enters a URL and then the available dates of various snapshots are displayed across the top of the screen, and the web page is displayed if it is available.  I am concerned with the files that drive the WaybackMachine: the WebARChive (WARC) files, the international standard for preserving Web data.3 Archiving web sites is difficult: a single page is made up of hundreds of parts, with external calls for images or other code hosted elsewhere. A WARC file provides a container for it all. There are good reasons for a historian to be interested in these files rather than merely using the WaybackMachine. Chief among them is the latter’s lack of a full-text search function. The WaybackMachine takes a URL and generates the corresponding archived website. WARC files are the system’s building blocks. They contain data with can be explored another way. 3. The Canadian Internet Case Study I draw on the 80TB Wide Web Scrape, released in October 2012 to celebrate the Internet Archive’s accumulation of ten petabytes of data.4 The WARC files in this collection are the results of a crawl that began on 9 March 2011 and ended on 23 December 2011, totalling 2,713,676,341 websites over 29,032,069 hosts. It is important to note the limitations of this scrape. First, we do not have multiple ones: if we had more scrapes, we could compare them and thus establish temporal changes. My hope is that if we can make cogent cases for what we can do with this sort of data, more might be released. Second, the sampling practices of the Internet Archive profoundly impact this sample and are beyond my control. The exact percentage of preserved websites is unknown, but it is probably only a little more than half.5 Furthermore, they are not scraped and preserved on a temporally consistent basis. More work remains to be done to understand these processes, as they profoundly shape work done with it. Beginning by downloading all 111,690 index files (one line per URL), I found the WARC files that contained the largest number of websites within the .ca domain. I subsequently downloaded the top hundred WARCs containing a total of 397,221 Canadian URLs. Based on Internet Archive statistics, this dataset is 4.7% of the indexed .ca top-level domain. To extract information, I was primarily interested in drawing on the large body of text within this sample to analyze. Text analysis tools are most developed, and this is my active area of research interest. In order to convert these files into plain text, I relied upon the free and open-source WARC Tools collection. In short, this creates plain text files by running each website through Lynx, a text-based browser. I subsequently selected only those that had Canadian domain names, selected via regular expressions. Each of those full-text files was then transformed into an XML document with fields for the title (URL) and content (textual content of the website). This plain text conversion has also been extremely fruitful in exploring via other text analysis tools, such as Named Entity Recognition. 4. Findings The interplay between two open-source tools, the Apache Solr search engine and the Carrot2 clustering search engine, presents a fruitful way to explore these archives. Solr is a NoSQL search engine optimized for working with millions of documents. Once data is ingested into Solr, which provides basic search, I turn to Carrot2. It is useful because of the clustering function, which takes objects and groups them into sets sharing common characteristics. While Carrot2 offers several clustering algorithms, Lingo clustering is most fruitful. Its goal is to \"capture thematic threads in a search result, that is discover groups of related documents and describe the subject of these groups in a way meaningful to a human.\"6 I now want to provide an overview of what these processes were able to do in my explorations of the Wide Web Scrape. My choices of queries are necessarily limited, as this paper cannot do comprehensive justice. In my other work, I am a historian of youth cultures; how could this methodology help somebody with my research interests? Here, a query for ‘children’:   Fig. 1: The query ‘children’, demonstrating clusters within the collection  At left is an input panel, at right a list of clusters with the documents themselves. At lower left we have visualization options. This visualization is akin to an archival finding aid: we learn what these WARC files tell us about ‘Children,’ and whether this is worth further investigation. We see files relating to children’s health (Health Canada, Tylenol), research into children at various universities, health and wellness services, as well as related topics such as Youth Services, Family Services, and mental health.   Thus, we have both an ad-hoc finding aid equivalent as well as a way to move beyond distant and close reading levels. While future studies would need to expand the amount of websites studied, we can see in this tranche of 5% of the Web that we have a good amount of information pertaining to children’s health, universities, and beyond. For some researchers, this would be a boon – for others, an indication that they may need to look elsewhere. Some projects are purely exploratory, however, and with a confident sample we could begin to make overall statements concerning societal concerns towards children. Notably, this method helps us find relevant websites by putting them into easy-to-understand groups, allowing for both quantitative (number of sites) and qualitative (the sites themselves) findings. Clusters often contain more than one object, and the relationship between clusters sheds light on the structure of a document collection. Consider the following:   Fig. 2: The query ‘children’ visualized using the Aduna cluster visualization technique.  Labels represent clusters. If a document spans multiple clusters, it is represented by a dot connected to both labels, which represent clusters. For example, “Christian Education” appears in the middle left of the chart. There is one document to the left of it (partially covered by the label), a document that belongs only to it. Yet there is one to the right of it connected with “Early Learning,” representing a website that falls into both categories.   From this, we can learn quite a bit about the files that we can find in the Wide Web Scrape as well as suggest which might be most fruitful for exploration. In this chart, at the bottom we see  websites relating to children’s health, which connect to breastfeeding, which connect to timeframes, which actually then connect to employment (which often contains quite a bit of data and time information). We then also see that connect to early childhood workers, which in turn connects to early learning more generally. The structure of the web archive relating to children reveals itself.  A downside is that the individual files are plain text. However, we can use the online WaybackMachine. Using an Automator script in OS X, a service can be configured to prefix the WaybackMachine’s URL (web.archive.org/web) to any URL. Through three steps (service receives text, adds the prefix string, and displays the webpage), we retrieve the archived version. See Fig 3, 4, and 5 below for this process:   Fig. 3: An example of the plain text files that power the search engine    Fig. 4: WaybackMachine Plugin in Action    Fig. 5: From distant to close reading: the above website in the WaybackMachine.  5. Conclusions WARC files, and web archives more generally, should be understood as key components of a future historian’s professional training. Undertaking projects growing out of the 1990s or 2000s may require access to such archives. Finding aids are generally unavailable for this type of source, and would be impractical due to the sheer data quantity. This approach, integrating on-the-fly finding aid generation and access to both distant and close reading, should be considered for adoption by historians.   This project shows that by drawing on a large dataset, 4.7% of the top-level .ca domain, historians will be able to derive meaningful information and find connections between disparate bodies of information. As history enters the web era, new tools and resources will be necessary.  ",
       "article_title":"Clustering Search to Navigate A Case Study of the Canadian World Wide Web as a Historical Resource",
       "authors":[
          {
             "given":"Ian",
             "family":"Milligan",
             "affiliation":[
                {
                   "original_name":"University of Waterloo",
                   "normalized_name":"University of Waterloo",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01aff2v68",
                      "GRID":"grid.46078.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The aim of this paper is to highlight the need for digital atlases in historical research and to present the data model and the collaborative platform we have developed in order to produce a historical geographic information system (HGIS), the Geo-Larhra, which is suitable for producing a new digital historical atlas.  1. Background and purposes   At its beginning the project explored the possibilities of integrating geographical and historical data into the same digital research platform. Several reasons fuelled this project. The main issue that arose was how to obtain base maps to represent a specific historical time. Traditionally, printed historical maps and atlases provide spatialized maps usually connected with relevant historical dates. This chronological selection documents major historical events for a specific geographic area (e.g. the political borders of European Countries after the Congress of Vienna, 1815). More rarely, maps are provided by century (1600, 1700, etc.) and significant elements in the chronological development or specific changes in tight spaces do not appear. Moreover, there exists a limited range of digital historical maps, particularly with regard to freely accessible shapefiles (commonly used geospatial vector data format), and they often do not take into account the diachronic changes in political or administrative territories. The conclusion was clear, the advance of digital history needs a project for realizing a new digital historical atlas, enhanced by researchers working collaboratively in a coherent, easy-to-use environment.   Drawing on different successful experiences in historical atlases (e.g. A vision of Britain through time, Euratlas, Digitaler Atlas zur Geschichte Europas seit 1500, HGIS Germany, China Historical GIS), we have devised a method for modeling geohistorical data to process the evolution of territories. This work is an application of the SyMoGIH method, the MOdular SYstem for Historical Information Management (Beretta, Francesco / Vernus, Pierre (2012)). Against this background, the historical atlas will be available to historians participating in the SyMoGIH project but it will also be accessible to a larger public through the web site, www.geo-larhra.org, offering a basic mapping service online as well as downloadable resources. Geo-Larhra includes a gazetteer, a catalog of vector layers plus the digital historical atlas. In this paper we will describe the underlying data model and the principles and workflow of our collaborative approach.  2. Method and data model   To build such a collaborative historical atlas, we needed to develop a generic data model allowing the processing of any type of place and taking into account any kind of temporal evolution due to the toponymical, typological or spatial extent changes. To address the issue of multi-dimensional evolution, the team of the SyMoGIH project has developed a generic data model independent of any research problem [see the documentation on our website: www.symogih.org/?q=documentation]. In our model, we distinguish between the identification of places and their spatial representation.  The identification of places is carried out in the traditional way using a gazetteer: a place is identified by its name or names, a type and a geographical location in form of a point or a bounding box (cf. Hill, Linda L., 2006). Each place is identified by a uniform resource identifier (URI). The processing of spatial representation on the contrary is the most novel part of our method : we have introduced a distinction between the form of a place at a given time, that we call a “concrete time-specific form” or simply a “concrete form”, and the more or less accurate geometries (i.e. geo-referenced vector data) representing this form at different scales. The evolution of the place's form is first described and documented by historical information collected collaboratively by the historians participating in the project. The geometries are then produced by the GIS specialists according to the collected information. This modeling process and data production workflow is more flexible and suitable for historical research than the traditional method in GIS, which links data directly to geometries. By using SQL and spatial queries it is possible to output the shape of places and territories at a given date with a temporal scale which is currently accurate to the day. This method leads to a synthesis between combining the traditional practice of historical databases, the use of historical atlases and GIS methods. Geo-Larhra is intended to be a resource to address both of these needs : providing historically accurate base maps and allowing historians to make spatial analysis taking into account temporal evolution.  Our philosophy is based on a collaborative and open approach aimed at enriching and developing the historical atlas. The collaboration of historians and GIS specialists is carried out on several levels : they collect historical and geographical information from sources, maps and historical atlases ; they produce historical data in the collaborative database platform ; they produce geometries using the collected historical and geographical data. Digital maps are finally created and they can be successively added to, following the same workflow, if new or more precise information is collected. The paper will give some concrete examples of this collaborative approach.   Fig. 1: Screenshot of the historical atlas : www.geo-larhra.org   3. The platform   Technological choices for the project encountered strong constraints resulting from the collaborative aspect and the generic system and multiple uses for which it was intended. The software architecture has been constructed using the triptych PostgreSQL, Post-GIS and QGIS. The DBMS PostgreSQL provides several advantages. It is a free and open source, and useful to establish the precise management of users’ rights. The PostGIS extension is easily interfaced with other management database tools to query, analyze and visualize data (GIS software, statistical analysis, GIS web server). Geo-historical data are published with the TinyOWS map-server which provide WMS (Web Map Service) and WFS-T (Web Feature Service) in QGIS or OpenLayers.  4. Future prospects   To date, the data model presented in this paper seems to fit historians’ needs perfectly. However our team must now improve the ease of use of the platform accessed by scholars concerned by spatial analysis who would contribute to this project. We have already started to publish the gazetteer on the web and we provide some shapefiles, extracted from the historical atlas of the Italian peninsula territories, which was our first data set created according to this method. Our longer-term perspective is to expand the geographical area of the atlas with the help of international partners.  ",
       "article_title":"SyMoGIH project and Geo-Larhra: A method and a collaborative platform for a digital historical atlas",
       "authors":[
          {
             "given":"Claire-Charlotte",
             "family":"Butez",
             "affiliation":[
                {
                   "original_name":"LARHRA UMR 5190, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Francesco",
             "family":"Beretta",
             "affiliation":[
                {
                   "original_name":"LARHRA UMR 5190, France",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "databases & dbms",
          "spatio-temporal modeling",
          "maps and mapping"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The last few years have seen intense debates in the digital humanities community not only in its definition but also its configuration. Several scholars have pointed out that the community is predominantly made up of scholars from a handful of mainly Englishspeaking countries and that linguistic and geographic diversity is sorely lacking. Questions related to ethnicity, gender, race, language and class have been raised within the DH community, as it seeks to find a more global and inclusive organization model. One of the main issues of course, is attempting to integrate with groups of scholars that have not necessarily identified themselves yet as a community or do not even know that DH exists. Spanish and Portuguesespeaking countries, some with long traditions already in humanidades digitales/humanidades digitais, have hosted a number of DH events and activities in the last few years, including several conferences and seminars, and recent attempts to build formal networks and associations in these two languages have sought to address concerns regarding international representation and visibility. Therefore the first step in this networkbuilding exercise was to find those who identified themselves as “humanista digital” (Galina and Priani, 2011). As Isabel Galina (2013) pointed out in her keynote presentation at the DH2013 conference: “behind this problem of defining digital humanities (what we are and what we do) there is an additional now ineludible problem ‘who is we?’” The DíaHD/DiaHD (Día de las humanidades digitales/Dia das humanidades digitais) initiative aimed to answer this question. Coordinated by a group of digital humanities organizations and institutions in Spain (Humanidades Digitales Hispánicas), México (RedHD), Portugal (Faculdade de Ciencias Sociais e Humanas, Universidade Nova de Lisboa) and Brazil (Humanidades Digitais, Universidade de São Paulo), with the support of centerNet, the event sought to identify and bring together the work of Spanish and Portuguesespeaking digital humanists in Europe and Latin America. The proposal of DíaHD was based on the model of the international centerNetsponsored project Day in the Life of the Digital Humanities (DayofDH,digitalhumanities.org/centernet/initiatives/), in which digital humanists all over the world are invited to participate in “a social research project designed to document ‘just what do computing humanists really do?’\" (Rockwell et all, 2012). Even though the starting point of the project was effectively almost a direct translation of Day of DH, including the basic question: ‘¿Qué es lo que hacen realmente los humanistas digitales?’/ 'O que fazem os humanistas digitais?', we could not avoid the context in which the project was launched. Behind the question ‘What do digital humanists do?’ we also wanted to know ‘who is this we?’ and ‘where is this we?’. This dual aspect of identification and localization broadens the outlook of DiaHD as a social research project, which then becomes a “process of reflection of what we have created and how it fits in with the socalled global DH community” (Galina, 2014).   II. Execution of the Project On June 10th 2013 the Día de las humanidades digitales/Dia das humanidades digitais took place. The event was managed by two working groups: the international organization group, formed by representatives of the institutions involved, and the local organising group, formed by members of the technological staff of the Facultad de Filosofía y Letras de la UNAM, which managed the technical infrastructure. We were fortunate to count on the generous advice of the creators of the original Day in the Life of Digital Humanities, including Geoffroy Rockwell. Outreach and other activities related to being more inclusive are time consuming. It is important to note that multilingualism requires additional effort both in developing the tool (invitations and instructions for participating) as well as the more qualitative analysis of the data. This DiaHD selected only two languages but we are well aware that others could have been added. If DH is to be successful in expanding the extra effort required should be considered. However, as we believe DíaDH has shown, this relatively small effort pays off considerably in our ability to broaden our definition of ‘we’.   III. Answering the questions The original Day in the life of Digital Humanities was conceived as an individualistic approach to the activities of digital humanists: “The motif [of DayofDH] suggests the documentation of a subject's ‘real’ life, emphasizing the ordinary aspects of their environment over the extraordinary” (Rockwell et all, 2012). As our project was closely based on DayofDH, we expected that the participants might respond in the same way, as persons documenting the ordinary aspects of their lives. One of the most important results of our experiment was the manner in which the community decided to answer the question ¿Qué es lo que hacen realmente los humanistas digitales? / O que fazem os humanistas digitais? Whereas one part of the community, many with previous experience of participating in the Day in the life of Digital Humanities, followed the model of documenting everyday life, another group chose a different approach to answering the question: they decided to create collective blogs to document the work of their institutions or projects. One relevant example of this is the blog devoted to the humanidades digitais group in Brazil. What does this differing reception of our original invitation signify, and is it meaningful in defining the “we” in Spanish and Portuguesespeaking digital humanities? Does the “we” have a less individualistic nature in some regional and cultural contexts? For DiaHD 95 blogs were created, out of which 70 were actively used. Of this 57 of the blogs were written in Spanish, whereas 13 were written in Portuguese. As the invitation was based on language and not by region, the geographical location of the authors was not restricted to Spanish and Portuguesespeaking countries. We had authors from Argentina, Brazil, Canada, Great Britain, Mexico, Portugal, Spain, Sweden and United States. As far as participating countries was concerned, Spain was the most active with 29 blogs, followed by Mexico with 16, Portugal with 8, and Brazil and USA (both 5). This means that the project integrated researchers from different academic cultures and most likely with different notions of what DH is. We can see this contrast in the blogs created: 37 blogs were collective, and represent projects, magazines, labs, etc. (only 2 of them were located in a non Spanish and Portuguesespeaking countries) and 33 were personal blogs. These results seem to indicate that there are a number of scholars involved in DH projects in what are traditionally underrepresented regions and languages in the DH community. As a social media project, DíaDH points towards the fact that by developing tools in other languages other than English and directly targeting other communities by not relying only on traditional DH communication channels, it is possible to incorporate the experiences of digital humanists that do not usually participate.   V. Conclusions As a result of a preliminarily observation we might point that the absence of a consolidated, consensual and collective profile for Spanish and Portuguesespeaking digital humanities communities is relevant to understanding why many of the participants in DíaHD/DiaHD preferred an institutional voice to a personal one. This may imply, also, a segmentation in the Spanish and Portuguesespeaking DH community: some, with previous involvement in the international DH community, identifying themselves as researchers with a specific DH focus, while others prefer to identify DH with work in specific projects, and to maintain their professional identity as a distinct entity. As DH is a field whose definition has largely emerged from a North Atlantic tradition, and responds to a concrete subset of global academic culture, it is clear that there is much work to be done in negotiating the development of the field in other academic traditions, each with their own cultural and practical models. DíaHD/DiaHD not only focused debate on one area of nonAnglophone DH; it also led to a number of practical initiatives continuing the development of Spanish and Portuguesespeaking digital humanities, including the mapaHD project (mapahd.org) and the Portuguesespeaking association, AHDig (ahdig.org), an initiative that came about as a direct consequence of the participation of Portuguese and Brazilian researchers at DiaHD.  ",
       "article_title":"Who is we? The social media project: Día de las humanidades digitales/Dia das humanidades digitais",
       "authors":[
          {
             "given":"Ernesto",
             "family":"Priani",
             "affiliation":[
                {
                   "original_name":"niversidad Nacional Autónoma de México, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Spence",
             "affiliation":[
                {
                   "original_name":"King´s College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Isabel",
             "family":"Galina Russell",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional Autónoma de México, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          },
          {
             "given":"Elena",
             "family":"González-Blanco",
             "affiliation":[
                {
                   "original_name":"Universidad Nacional de Educación Distancia, España",
                   "normalized_name":"National University of Distance Education",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02msb5n36",
                      "GRID":"grid.10702.34"
                   }
                }
             ]
          },
          {
             "given":"Maria Clara",
             "family":"Paixão de Sousa",
             "affiliation":[
                {
                   "original_name":"Universidade de São Paulo",
                   "normalized_name":"Universidade de São Paulo",
                   "country":"Brazil",
                   "identifiers":{
                      "ror":"https://ror.org/036rp1748",
                      "GRID":"grid.11899.38"
                   }
                }
             ]
          },
          {
             "given":"Daniel",
             "family":"Alves",
             "affiliation":[
                {
                   "original_name":"Universidade Nova de Lisboa ",
                   "normalized_name":"Universidade Nova de Lisboa",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02xankh89",
                      "GRID":"grid.10772.33"
                   }
                }
             ]
          },
          {
             "given":"José Francisco",
             "family":"Barrón",
             "affiliation":[
                {
                   "original_name":"niversidad Nacional Autónoma de México, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          },
          {
             "given":"Marco Antonio",
             "family":"Godinez",
             "affiliation":[
                {
                   "original_name":"niversidad Nacional Autónoma de México, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          },
          {
             "given":"Ana María",
             "family":"Guzmán",
             "affiliation":[
                {
                   "original_name":"niversidad Nacional Autónoma de México, Mexico",
                   "normalized_name":"National Autonomous University of Mexico",
                   "country":"Mexico",
                   "identifiers":{
                      "ror":"https://ror.org/01tmp8f25",
                      "GRID":"grid.9486.3"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "multilingual / multicultural approaches",
          "digital humanities - institutional support",
          "digital humanities - nature and significance"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  As the term “Digital Humanities” has been gradually gaining attention around the world, with researchers in English-speaking countries gathering under this banner in increasing numbers. Among these, there are the earlier scholars who had previously known the field as Humanities Computing; and there are also scholars who have become involved more recently, directly under the rubric of Digital Humanities. Yet another new trend is that where scholars from non-English-speaking and non-Western countries have also been gradually getting involved in the international DH community. One of these recent entrants to the international DH community is the Japanese Association for Digital Humanities (JADH). The presence of this new organization constitutes one piece of evidence to show that the international community is gradually broadening the scope of its membership. This trend has been actively supported by the Multi-lingualism and Multi-culturalism Committee of the ADHO, as well as by individual scholars who believe forming a global community can only enrich DH and the humanities. In view of this fact, it seems that it will become worthwhile to release the CFP of the DH conference in many languages. Additionally, recently several non-English Western communities have been established. For example, Hispanic, Italian, and German DH were discussed during the DH2012 conference at Hamburg. Each language area has long and deep history to engage in the research and practice of DH. Moreover, Global Outlook::Digital Humanities (GO::DH) has started to cover a wider area, such as Latin America, China, Africa, and so on, especially focusing on communication and collaboration across and between High, Mid, and Low Income Economies. It is remarkable that the first pilot project of the GO::DH, “AroundDH in 80 Days” could immediately fill the list of DH projects around the world (Gil) with the help of international volunteers. In the context of the humanities, globalization is not always intrinsically good, but international communication would be significant for DH and the humanities. While there have been efforts focused actual local development, in some cases, such as of the Japan, most DH activities hadn’t been known in the global community and most of global DH activities hadn’t been known in Japan until several years ago. This is in spite of the fact that the number of identified Japanese DH-related researchers is over 200 and recently the domestic annual DH conferences have gathered 40-60 papers every year, with 800 papers being presented since 1989 from many universities, museums, libraries, and other institutions in a DH-related quarterly workshop (A. Charles Muller). As the case might be similar in other non-English and non-Western countries, it might be useful to report our recent attempts to bridge between the DH research being carried out by non-English-speaking scholars and those in the international community. First, the establishment of the JADH has proved itself to be one of the most effective solutions for closing this kind of gap. Since around 80 researchers participated in the first conference in 2011 in Osaka, 80-90 Japanese researchers have attended the annual conference and communicated with international researchers. Then, several germs of international collaboration have come into being there and Japanese researchers who paid attention to the results of research activities of the international DH community have gradually increased as a “methodological commons”—although most of the research is still focused on Japanese or Eastern materials. Secondly, an e-newsletter titled “Digital Humanities Monthly” (DHII and ARG) has been published by the International Institute for Digital Humanities since July 2011. It is has 390 subscribers and is also published on the Web. The e-newsletter written in Japanese consists of an invited essay, brief news of international activities of DH and Digital History, DH-event calendar, and reports of DH events held in Japan and foreign countries collaborating with some local and international voluntary DH researchers. The event reports are plotted on a time-space map of the Neatline.(fig.1) The total number of the access to the Web pages was over 5,000 this October. According to comments of the readers, it seems to have gained the attention of not only DH researchers, but also librarians, curators, archivists, publishers, and general public, enabling them to see the picture of the entire situation of the domestic and the international DH. Thirdly, we plan to make it easier to treat Japanese and Eastern materials compliant with kinds of international standards. So far, we are working to propose the encoding of Han characters that occur in our research materials in the Universal Character Set as a group of researchers (rather than as a national body, as has been the policy heretofore) so that researchers can not only treat the characters but also propose the inclusion of new characters more easily. Moreover, we are planning to form an appropriate guideline of text encoding of Japanese and Eastern materials in the framework of the Text Encoding Initiative P5 guidelines (Bauman) collaborating with related researchers around the world. As a preparation for this, we’ve held full-day TEI workshop by the participation of international TEI researchers over 10 times and taught the framework of the TEI to 50 researchers in total. While it has up to now been difficult to bridge the local and the global, we hope our attempts will be useful for an appropriate mode of globalization. We would like to discuss various possibilities with participants in the conference.   Fig. 1:    ",
       "article_title":"Bridging the Local and the Global in DH: A Case Study in Japan",
       "authors":[
          {
             "given":"Kiyonori ",
             "family":"Nagasaki",
             "affiliation":[
                {
                   "original_name":"International Institute for Digital Humanities",
                   "normalized_name":"International Institute for Digital Humanities",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/0454arg59",
                      "GRID":"grid.474291.d"
                   }
                }
             ]
          },
          {
             "given":" A. Charles",
             "family":"Muller",
             "affiliation":[
                {
                   "original_name":"Graduate school of Humanities and Sociology, University of Tokyo ",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          },
          {
             "given":"Toru ",
             "family":"Tomabechi",
             "affiliation":[
                {
                   "original_name":"International Institute for Digital Humanities",
                   "normalized_name":"International Institute for Digital Humanities",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/0454arg59",
                      "GRID":"grid.474291.d"
                   }
                }
             ]
          },
          {
             "given":"Masahiro ",
             "family":"Shimoda",
             "affiliation":[
                {
                   "original_name":"Graduate school of Humanities and Sociology, University of Tokyo ",
                   "normalized_name":"University of Tokyo",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/057zh3y96",
                      "GRID":"grid.26999.3d"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "multilingual / multicultural approaches",
          "digital humanities - nature and significance"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Digital humanities empower a creative transformation in both humanities and computing research by inspiring and fostering interdisciplinary interaction. Recently, digital visualization has been considered and established as a scholar methodology for digital humanities (Jessop, 2008). Projects, such as “Tooling Up for Digital Humanities” and “The Spatial History” (White, 2010) at Stanford University, have explored and experimented with various forms of graphic representation of data. Visualization is insightfully considered as part of a research process that may induce powerful arguments or raise new questions. It is also pointed out that visualization seems to give a sense of objective and scientific communication in the scholarly, yet sometime ambiguous, activities of digital humanities. One of the less addressed issues in digital humanities visualization concerns the exhibition facilities. Even though some display equipment and technologies have been developed for some times, their innovative integration with a large-scale auditorium space to create an exhibition facilities for digital humanities has actually been little reported. We developed an innovative exhibition facility for digital humanities visualization with a conceptual framework of place-making that exploits digital technological mediation of people and humanities. Similar to the museum experiences with innovative engagement (Falk & Dierking, 2000) (McCarthy & Ciolfi, 2008), the exhibition facility induces locative experience for sense-making and potentially plays a pivotal role in facilitating further advance of digital humanities. Our work provides a field tested contribution to the research community by engaging wider audience for digital humanities, facilitating its social impact, and filling the vacancy of building a physical platform for presenting and showcasing research results for better recognition. 2. Physical Interactive Space as Digital Humanities Exhibition Facilities  Following the notion of place-making in urban development and heritage studies (Malpas, 2008), a physical space forms an existential ground where people’s senses of digital humanities are shaped and defined. Therefore, an innovative exhibition facility can serve as a social and technical infrastructure of cross-disciplinary interaction and allow for new experiences with tangible and intangible forms of digital humanities. This opens up new ways of exploring and articulating digital humanities visualization with physical and social settings, and potentially widening appreciation and deepening recognition of digital humanities for general audience.  We developed the exhibition facility by transforming a large room used for library reference service and installing an array of display equipment for various forms of interactive visualization. With a floor space of 810 square meters, the room was re-conceptualized as a mixture of digital gallery and auditorium by novel interior design and technology embedment. Figure 1 shows the floor plan of the exhibition facility that comprises an inner conference room, a flanked outer corridor, and a lobby. The inner and outer space are separated by sliding doors in the front opening, auxiliary doors in the corners, and entrance doors from the lobby.   Fig. 1: Floor Plan of the Exhibition Facility for Digital Humanities  A number of ten display systems are either mounted or projected on walls in both parts of the facility, as listed below.    An arc wall in size of 12 meters by 2.5 meters (width and height) used as a touch wall display with projection blending of 5 projectors, rendering a surrounding effect of visualization.    Two 120-inch retractable projection screens, providing auxiliary displays.    Two 42-inch touch screens embedded in a wall book shelf, collaging digital and physical archival exhibition.    Two 12-inch monitors mounted on a photo collage wall, blending digital and physical image display.   A rear projection touch screen in size of 5 meters by 1.2 meters with projection blending of 3 projectors inside the partition wall, providing easy access and playful social interaction with digital images.   Two 60-inch 3D touch screens embedded in a partition wall, rendering 3D images of objects with 3D goggles.    A 55-inch touch screen embedded in a partition wall,    Two 42-inch transparent LCD boxes, exhibiting physical objects/materials inside the boxes while displaying digital information on the transparent screens.    A curvier arc wall in size of 8 meters by 2.5 meters used as a surrounded wall display with projection blending of 3 projectors, rendering immersive visualization.    A collage of wall-mounted four 46-inch screens in 4K2K resolution (ultra high definition), used as a digital signage board in the lobby.    Figure 2 through Figure 5 show actual images of the renovated results for an innovative exhibition facility.     Fig. 2: Evacuating a room previously used for library reference service    Fig. 3: Renovated as a conference room and auditorium, showing display systems #3 and #9 in Figure 1.    Fig. 4: Part of the corridor flanking the inner room, showing display systems #5 and #7 in Figure 1.    Fig. 5: Renovated lobby, showing display system #10 in Figure 1.  Figure 6 through Figure 9 show some of the exhibition highlights from a range of    Fig. 6: A workshop for digital humanities visualization in the conference room    Fig. 7: An international visitor appreciating an ancient book inside the transparent box, while getting information on the touch screen    Fig. 8: A group of students enthusiastically interacting with a large scale touch screen    Fig. 9: A group of international visitors enjoying a 3D digital simulation of the cultural heritage of lantern festival  3. Digital Presentation and Exhibition of Digital Humanities The developed facility provides intensive and large scale visualization in an atmosphere with aesthetics appeal (Guyer, 2004). Large sized interactive touch screens facilitates audience engagement and creates more persuasive communication. The integration of space and technology in the exhibition facility aims to create a sense of place with a prominent context of digital humanities in which a living and sustainable recognition with exhibited subjects can be induced. Exhibition audience is, therefore, contextualized with senses, feeling, and embodiment that underpins an interpretive process of meaning-making (Schorch, 2012) and leads to an internal understanding and empathy of digital humanities.  The exhibition facility has been completed and inaugurated in May 2013 and has offered a range of subjects on a regular basis. The place becomes a hot spot for campus activities and has been designated to receive dignified visitors. Audience generally expresses experiences of remarkable novelty and deep appreciation for digital humanities exhibition. The innovative facility transformation project has been regarded as an overwhelming success by both inside and outside of campus community and has strengthen the initiative of digital humanities as a university research agenda.    Fig. 10: Geographic distribution of analytic data being projected on the 12-meter arc wall.    Fig. 11: Cooperative research networks among faculty members with interactive query on touch screen  We illustrate two use cases of the facility. The first is for visualization support of analytic investigation. Figure 10 and Figure 11 show images of analytic data used in research meeting of digital humanities projects. It has been indicated that large scale visualization of exploratory data inspection process achieves effective communication and facilitates research progress. An interactive script of images also helps present research discovery to the general audience.    Fig. 12: The author digitally interacts with his own handwritten manuscripts, along with his pupil.    Fig. 13: Manuscripts of short articles are collaged on a wall with digital images of places in the articles.  The second use case is an honorary ceremony for a university chair professor, also a revered writer, along with an exhibition of his highly regarded books and original handwritten manuscripts over thirty years. Figure 12 through Figure 15 show a range of presentation forms to provide a rich context for the writer’s celebrated career. The chair professor was apparently moved by the immersive atmosphere that reflected his heartfelt memory of life.     Fig. 14: The chair professor gave a talk to many of his students and readers in the conference room    Fig. 15: Students listen to a poem recital in author’s recorded voice with an immersive textual and graphic background.  4. Conclusion and Future Work We conclude that an innovative exhibition facility is a vital infrastructure for digital humanities endeavor. It is argued that the existential grounding of digital humanities is better achieved with an engaging and interpretive process in an immersive atmosphere. This meaning-making system helps gather enthusiastic support for and arouses great interest in digital humanities. More importantly, the facility provides a playground for digital humanities activities and establishes a digital laboratory for digital humanities research.  Another implication is the exciting potential in pedagogical use. With project-based learning, interdisciplinary student teams are taught to digitally curate subjects of cultural heritage and to organize innovative exhibitions for their peers. Ongoing student projects include Chinese puppet show and a local heritage village with colorful house wall painting. We believe that the educational value of the exhibition facility will contribute to wider participation of younger generation in digital humanities.  We also remark that the exhibition facility along with the activities and presentation content that it facilitates are not applicable to a controlled comparison in a research lab. However, since its inauguration, the facility has hosted more than 100 school-level events with 2500 highly-impressed visitors in six months. We feel that this actual usage result provides stronger verification than lab experimental data. A preliminary sampled survey showed significant effects on enabling audience recognition and appreciation of digital humanities activities. Our future work includes a more formal user study.   ",
       "article_title":"Developing a Physical Interactive Space for Innovative Digital Humanities Exhibition",
       "authors":[
          {
             "given":"Jyi-Shane",
             "family":"Liu",
             "affiliation":[
                {
                   "original_name":"Natioanl Chengchi University, Taiwan, Republic of China ",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          },
          {
             "given":"Wen-Hung ",
             "family":"Liao",
             "affiliation":[
                {
                   "original_name":"Natioanl Chengchi University, Taiwan, Republic of China ",
                   "normalized_name":"National Chengchi University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/03rqk8h36",
                      "GRID":"grid.412042.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "digital humanities - facilities"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction  Up to now publishers made limited use of the WWW by using websites merely for promotion of their works or as extension of books to accommodate notes, appendices or illustrations that could not be included. This paper describes the results of an interdisciplinary collaboration in which researchers, digital archivists and private companies created web based dynamic visualizations to enrich publications.1 It presents the outcomes of a pilot project in which Brill Publishers, a game developer Wild Card,  historians of science of the Huygens Institute for the History of the Netherlands and the history of science museum Boerhaave and finally scientific programmers of Data Archiving and Network Services (DANS) experimented one day per week for a period of nine months with the development, preservation and reuse of animations of static illustrations of scientific processes and the working of mechanical instruments and tools. These animations were developed to enhance the understanding of complex, often abstract descriptions in seventeenth century publications of the sciences. In total six animations were produced that can be activated by clicking on QR codes next to illustrations in books on/facsimiles of seventeenth century works on astronomy, physics, biology, fortification, land surveying and mechanical engineering. The aim was not just to produce creative illustrations, but interactive scholarly multimedia animations that would contribute to a critical interpretation of the seventeenth century texts and comments hereof. Moreover, the experiment did not limit itself to creation of animated visualizations, but comprehended workflows for re-use and the development of business models leading to animated discussions about the ownership and responsibility of external links and issues of copy and author rights. The development of criteria for the choice of animations resulted in a typology of enhanced publications. For each of the 6 cases a scenario was developed to model the workflows to enable the publisher and the user to enrich the publications in question. Different types of target  audiences were individuated that resulted in discussions of how much freedom of manipulation the user would have and its impact on the storyboard that was developed for each case. After an overview of the implementation of the  various storyboards into animations,  the architecture of the archive work-flow and business models will be discussed to enable the re-use of the enriched publications in the future.  Toward a Typology of Enhanced Publications The name of the project that received funding was Dynamic Drawings in Enhanced Publications. DRIVER (2009), the Digital Repository Infrastructure Vision of European Research defines an enhanced publication as one that is enriched with three categories: 1) Research data (evidence of research), 2) Extra materials (to illustrate or clarify) and 3) Post-publication data (commentaries). Our project encompasses enrichments of all these three categories. However, we might have chosen for an alternative definition such as rich internet publication (but we wanted to stress the relationship between the analogue book and the digital enrichment) or scholarly media (but enrichments were not just intended for scholars, but in principal for cultural heritage and education as well). (KAIROS, 2011; Burgess and Hamming, 2011). In hindsight, we probably would have gone for enriched publications, at least if we had followed the distinction that Sondervan (2013) formulated between enhanced publications (hyperlinked content and data with added metadata) and enriched publications that offer more functionality (preferably inside the digital publication itself), with facilities to explore and to analyze data, providing better insight in the underlying research. Instead of contributing to this semantic discussion we created a more pragmatic typology to choose which cases to include for enrichments based on the following criteria: wide coverage of disciplines, degree of interactivity, complexity of dimensions (2D/3D), availability of sources in English and in portfolio Brill publishers and finally the potency for re-use:   Astrolabe: a two-dimensional model of the motions of the heavens and to measure time. The explanation of its working is complex and turned out to be a  promising case for exploring and reusing the possibilities of dynamic digital drawings in an interactive way for education, exhibitions and research. Refraction: Several decades before Newton announced his color theory, René Descartes proposed a physical model for light refraction and to explain the appearance of the rainbow. This case resulted in a discussion about how to visualize and contextualize an “incorrect theory.” Swammerdam’s microscopic drawings: Swammerdam’s aim was to highlight the analogous development, in corresponding stages of both the higher and lower animal species. Therefore a visualization with parallel sliders to animate these stages was chosen. Surveying/Triangulation: This casus zooms in on the Early Modern tradition of surveying treatises, which provide  practical instructions for measuring objects in the field and to represent them on drawings. For this casus interactivity in educational context was further explored. Fortification: Early modern science attempted to tackle real-life problems with applied mathematics. The challenge was here to visualize the mathematical optimization of regular polygonal fortified cities without forcing the user to switch to different levels.  Mill model Ramelli: Agostino Ramelli’s Le diverse et artificiose machine (1588) was a highlight in the Renaissance Theatrum Machinarum tradition, but its reconstruction raised historic-methodological issues. The  question was asked whether additional historical sources could be used for making the mill `work’ with digital tools.  Creating Storyboards The differences between the six chosen cases implied that for each animation a different storyboard had to be created. However, each story board has the following basic architectural features in common: original source –contextualization (in one or  more publication forms) – translation layer and the enrichment. (see Fig.1)   Fig. 1: Storyboard with basic architectural features of enriched publications. Casus Swammerdam   The Astrolabe case The astrolabe case is not only suitable to illustrate the added value of an interactive animation. In addition to a technical description to explain the working of a complex instruments for research, it is also popular in school and museum environments dealing with education of history of science. Therefore, it has a large re-use potential for non-expert groups as well, which from a business perspective allows spreading the investment over more publication types for different markets. Using the “Integrated Online Exhibit Model” (Marable, 2004) the astrolabe case was translated into a storyboard with three layers: “experience”, “exhibit” and “research” that all could serve as an entry point. The experience layer was used to answer the simple question: “what is an astrolabe?”. In the exhibit layer the user explores interactively functions. Following step by step instructions the user can find the time by measuring and processing solar altitude on the astrolabe (see Fig. 2). The third, “research level” is reserved for all the contextual source material for further in-depth analysis.   Fig. 2: Experience and Exhibit layer of Enriched Publication. Casus Astrolabe.   Archiving Interactive 3D Visualizations for Re-Use  An archiving workflow was developed with persistent identifiers to ensure the long-term availability of the enriched publication and the  reuse of the visualizations. Brill will be the primary publisher for the animated visualizations. DANS will archive the enriched publication, its source files and a durable screencast of the visualization.  Potential Business Models for Publishers Linked with the Brill content, these enrichments serve as a valuable supplement to current journal articles and books, not only in history of science but in all corners of the humanities & social science spectrum. Open Access publication of the enrichment under CC-BY license seems most suitable. As (governmental) research funding organizations increasingly pose the condition of open access disclosure for research-related data and conclusions, the visualizations should naturally also be freely accessible and ‘discoverable’ as much as possible; for scholars and non-experts alike. Books will become ‘richer’ in content and value to the reader. Even if the visualization is available in OA, such books can be priced higher – to recognize the additional value but also to cover costs of creating and facilitating the enrichment. With the outcome of this experiment Brill can ensure the dissemination and preservation of enriched publications. Moreover, the publisher foresees extra services to authors by providing a toolkit for new enrichments or to create those against cost prize, in case an author is unable to do so. 1 The authors are indebted to Leen Breure (DANS) who provided much information about new publication formats, to Huib Zuidervaart and Eric Jorink  (HuygensING) who gave critical feedback respectively on the Descartes and Swammerdam cases  and to Valentijn Gilissen and Heleen van de Schraaf (DANS) for their contributions to the development of archiving strategies for enriched publications for reuse. Financial support we received from the Royal Netherlands Academy of Arts and Sciences and Brill Publishers.  ",
       "article_title":"Dynamic Visualizations In Enriched Publications Of Seventeenth Century Science",
       "authors":[
          {
             "given":"Charles",
             "family":"Van den Heuvel",
             "affiliation":[
                {
                   "original_name":"Huygens ING (KNAW), The Hague Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Tiemen",
             "family":"Cocquyt",
             "affiliation":[
                {
                   "original_name":"Huygens ING, The Hague/Museum Boerhaave Leiden, Netherlands",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Maarten",
             "family":"Hoogerwerf",
             "affiliation":[
                {
                   "original_name":"DANS (KNAW), The Hague Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Dylan",
             "family":"Nagel",
             "affiliation":[
                {
                   "original_name":"Wild Card, The Hague Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Michiel",
             "family":"Thijssen",
             "affiliation":[
                {
                   "original_name":"Brill Publishers, Leiden The Netherlands",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "knowledge representation",
          "visualisation",
          "licensing",
          "sustainability and preservation",
          "repositories",
          "publishing and delivery systems",
          "copyright",
          "archives",
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Probing questions of literary interpretation with computer-aided text analysis Thirty-seven years after Milic (1966) rejected the untenable fear of many literary scholars that “the study of literature may become mechanical if it is processed by a computer” (4), Ramsay (2003) observes that while computers can successfully pursue “empirical validation of ‘impressionistic’ or ‘serendipitous’ critical readings” (173), such uses “have not penetrated the major journals of literary study,” and that, “in general, our methods are perceived as some sort of positivistic last stand” (168). Wanting computer-assisted text analysis to have a prominent place in the “wider community of humanist scholars” (2003: 173), Ramsay argues that all literary interpretation is more arbitrary than scholars acknowledge (2003: 170), and that interpretation “must present its alternative text as a legitimate counterpart—even a consequence—of the original” (Ramsay 2011: 56). But, à la McGann (2001), “[a] true critical representation does not accurately (so to speak) mirror its object; it consciously (so to speak) deforms its object” (173). Reading Machines echoes this: “The critic who endeavors to put forth a “reading” puts forth not the text, but a new text” (Ramsay 2011: 16). How we put forth “new” texts, McGann argues, deeply depends on our “idiosyncratic relation to the work” (116). Computers help us explore these idiosyncrasies by expanding the number of deformed texts we can produce; reading these “new” texts brings us “to a critical position in which we can imagine things about the text that we didn’t and perhaps couldn’t have otherwise known” (McGann 2001: 116). Ellis & Favat (1966) also employ rearrangement-as-interpretative procedure and provide a useful counterpoint to McGann and Ramsay by emphasizing that computers merely “enhance” what literary critics already do (1966: 637). Ellis & Favat use the “opportunity for the different examination and reordering of [Huckleberry Finn]” (637) to ask questions about Huck’s speech regarding the relation of “death” and “family” by generating concordances (630-33), but concordances have been used since at least the 13th century. What is new, Ellis & Favat contend, is that the critic can now “ask questions that previously he could have only wished to ask” (638) because the computational efficiency of evidence-gathering allows us to pursue questions previously pragmatically impossible (637). Yet Ellis & Favat suggest that certain kinds of reordering are not valid: “The fact that the computer has aided the scholar does not mean that this critical procedure has been violated” (637). Understanding what would constitute a violation positions us to better appreciate what is new in Ramsay’s algorithmic criticism. When does “the different examination and reordering of data” and the “grouping of [text]” cease to be valid “evidence” (637)? 2. Algorithmic criticism as literary chatbot Ramsay’s textual analysis, grounded in deformance, is a mechanism for destabilizing linguistic and cultural assumptions when reading texts. Although deformance has been a source of contention (Hoover 2005, 2007), many new DH scholars seem to underestimate its importance and implications. We argue that the practice of deformance arises, in part, from a broader conception of the humanities that emphasizes multiplying possible solutions and facilitating interesting and unpredictable discussions rather than finding particular solutions. Thus Ramsay insists that literary criticism should aim not to arrive at the meaning, but to ask “how do we ensure that [a text] keeps on meaning?” (2003: 170). He reaffirms this eight years later: “conclusions are evaluated not in terms of what propositions the data allows, but in terms of the nature and depth of the discussions that result” (2011: 9), echoing McGann’s claim that “the critical and interpretative question is not ‘what does a poem mean?’ but ‘how do we release or expose the poem’s possible meaning?’” (2001: 108). More recently, surprised by the difference in the words used by male and female speakers in The Waves, first discussed in Reading Machines (2011), Ramsay asks, Do we imagine that such further experiments would resolve long-standing questions about gender and language? Do we really want those questions resolved? That last question may seem slightly perverse, but I believe that, in the end, what is most distinct about humanistic inquiry is its resistance toward final answers. It is the goal of the seminar to answer questions,but mostly by proposing them more fruitfully. The humanities wants for itself a world that is more complex than we thought . . . We are in search of a conversation[.] (2012: 11-12) Ramsay’s question is “how successful the algorithms were in provoking thought and allowing insight” (Ramsay 2003: 173), and he is ultimately “more concerned with evaluating the robustness of the discussion that a particular procedure annunciates” (Ramsay 2011: 17). Unfortunately, emphasizing the “robustness of the discussion” that a deformed text promotes may de-emphasize the algorithms used to generate it. Yet if the algorithm that deforms the original text is to facilitate our interpretive insights, knowing what the algorithm does seems crucially important. 3. Continuing the conversation: getting algorithms out of the black box Deformance, like computational stylistics, necessarily focuses our attention on certain narratives of meaning, drawing our attention to specific words, phrases, images, and patterns at the expense of others. Lotaria in Calvino’s If on a Winter’s Night a Traveler (1979) “reads” a novel by looking at the words that appear 19 times—namely, “blood, cartridge, belt, commander, do, have, immediately, it, life, seen, sentry, shots, spider,” and so forth—and observes that “it’s a war novel, all action, brisk writing, with certain underlying violence” (182). She answers the literary equivalent of a factual question using a simple and relatively transparent method. However, when Ramsay lists the most characteristic words used by characters in The Waves in order to “participate in [a] literary critical endeavor beyond fact-checking” through the use of the tf-idf algorithm (Ramsay 2011: 10), the resulting “deformed text” he discusses cannot be reproduced by the procedure he describes. This is partly because he does not use the precise tf-idf equation he presents, but (more importantly) because of some interpretative decisions that are by no means obvious. Our point, however, is less to quarrel with Ramsay’s decisions than to interrogate his method. Ramsay (2011) does not explain the exact algorithm that produces his menonly and women-only words, but they are simple enough to identify. Rather than 90 men-only and 14 women-only words, however, we found 117 and 10. The mismatches are caused mostly by Bernard’s final retrospective chapter, which Ramsay has (quite reasonably) omitted from his analysis (though he confirms by email that he should have noted this). Re-analyzing without the final chapter reveals a few remaining discrepancies. For example, Ramsay lists “banker” and “Brisbane” as men-only words, but they appear in Neville’s and Bernard’s monologues only as imagined quotations from Louis. Discussing this kind of decision, we suggest, could deepen and engage a conversation about The Waves. More significantly, Ramsay’s provocative lists of 90 men-only and 14 womenonly words rest problematically on the amounts of text by the two genders. Even without the final chapter, there are about 35,000 words by the men and only 20,000 by the women, a discrepancy that “explains” the preponderance of male-only words. To “prove” this one could simply cut each male monologue to the length of its corresponding female monologue. (Corresponding how? Matching longest to longest, shortest to shortest? Why?) We chose a different “deformation,” randomizing the monologue lines and cutting each monologue to the length of the shortest, equalizing each character’s contribution. (Why?) This deformed text produces 31 women-only and 29 men-only words. Ramsay is right that the algorithm merely begins the argument, but the “provocative” revelation that the men share more words than the women seems deceptively and inappropriately provocative: it rests merely on the lengths of the monologues. (Why are the male monologues longer?) The nature of the male and female words remains provocative and suggestive for a conversation about gender in The Waves:    But our deformation’s doubling of women-only words raises interesting questions, and warns against over-interpreting the lists. Obviously, men use some of our women-only words in the parts we left out, a problem exacerbated by the rarity of these only-words: the highest frequency above is 4. All this suggests a reconsideration of the initial decision to use tf-idf in the first place.We use this opportunity to argue both that it is imperative that text analysis researchers carefully outline their procedures so that others can reproduce the original results and that more attention be given by the community at large to reexamining earlier results. Framing literary questions algorithmically is beneficial because stating our assumptions in computable terms may reveal our own hidden assumptions about the text we are examining, our assumptions regarding literary interpretation, or both. However, transforming our own literary methods into algorithms is always somewhat imperfect: foregrounding these interpretative decisions rather than hiding them allows the critical conversation to continue in a more valuable fashion by incorporating the difficulties of the algorithm into the act of literary interpretation itself.  ",
       "article_title":"Starting the Conversation: Literary Studies, Algorithmic Opacity, and Computer-Assisted Literary Insight",
       "authors":[
          {
             "given":"Aaron",
             "family":"Plasek",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          },
          {
             "given":"David L.",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":"New York University, United States of America",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "literary studies",
          "content analysis",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction Over the last few decades as more texts have been digitized, numerous software systems have arisen to display the texts and allow scholars to analyze them. These software systems have varied in their delivery (web-based, desktop software, mobile app, etc.) and their functionality, yet nearly all of them have included full-text search capabilities. Search is a central tool for scholars researching a corpus, and it is a task for which computers are perfectly suited. Despite the ubiquity of searching capabilities, there is no single method for displaying search results. When a user has requested to see a key word in its context, how much context should be presented to the user? In choosing the context to present, there is no single solution that will always be best. At times, users will want to see detailed context requiring several lines. At other times, users will want to see as many search results as possible in a small visual space. Thus, providing multiple ways of viewing search results is desirable. When each search result is contained in a single line, perhaps the most attractive presentation currently in common use contains the key word in the middle, showing whatever context that fits on each side, a presentation style also found in some print concordances (Clarke, 1984; The Computer Bible, 1970-).   Fig. 1: . Results from a KWIC (Key Word in Context) search using Perseus under Philologic at perseus.uchicago.edu.  However, there is another method of displaying search results on a single line that is found in some print concordances that antedate digital tools. In this tradition, the context surrounding the key word is chosen manually so as to provide the reader as much information as possible about the key word’s context.   Fig. 2: The entry “strengthen” in (Strong, 1890).  Unfortunately, this method requires a tremendous amount of manual effort; it has only been practical in concordances of the Bible and other heavily-studied texts. The following concordances that antedate the maturation of the digital age take this approach: Bible: Cruden (1737); Young (1882); Strong (1890); Mandelkern (1896); Hazard (1922); Gant (1950); Lisowsky (1958); Even-Shoshan (1977); Homer: Prendergast (1875: Iliad); Dunbar (1880: Odyssey); Shakespeare: Clarke (1846). Since the full flowering of the digital age, it has been abandoned in most concordances and even in commercial Bible software, as shown in the following figures.   Fig. 3: Search results from Logos Bible Software (logos.com) on a PC.    Fig. 4: . Search results from BibleWorks (BibleWorks.com) on a PC.    Fig. 5: Search results from Olive Tree Bible Software (OliveTree.com) on a Samsung Galaxy S3 smartphone. As a disclaimer, I wrote the search engine—but not the code to display the search results—for Olive Tree Bible Software as an independent contractor.  There have been some print concordances in the digital age for which the single-line context has been produced algorithmically, at least in part (e.g. Ellison, 1957; Spevack, 1968-1975; Goodrick and Kohlenberger, 1990; Kohlenberger, 1991; Dixon and Dawson, 1992; Mounce, 2012). Where algorithmic details have been published in part (Soule, 1956; Dixon, 1974; Dawson, 1977; Burton, 1982), they have often been primarily dependent on punctuation and/or manual annotation as a pre-processing step. While pioneering in their day, computing resources are more plentiful today, and the field of natural language processing has advanced greatly. I propose an algorithm that seeks to mimic a human reader’s choice of context for a search term. The goal is to produce the most relevant context for a key word on a line that is of arbitrary width using an arbitrary font. This provides the benefits of traditional print concordances without the tens or hundreds of person-years required to produce them for even a single line width and font size.   2. Algorithm 2.1 Preprocessing The text must, of course, be available in electronic form, but a syntactic parsing is also necessary. There are some electronic texts that have been parsed syntactically by hand (e.g. Andersen and Forbes, 2012), but the recent development of general-purpose parsers has made this work possible on a broader scale. As these parsers are developed for more languages and dialects and as they improve, the approach outlined here will become more and more useful. For this work, I generated phrase structure trees and dependency trees using StanfordCoreNLP (version 1.3.5, nlp.stanford.edu/software/index.shtml) on three texts (cf. Toutanova et al., 2003; de Marneffe et al., 2006). In keeping with the traditional use of concordances with Bible translations, I chose two Bible translations along with one novel: the King James Version (KJV) of the Bible (1769 text edition), the English Standard Version (ESV) of the Bible (2011 text edition, Old Testament/Hebrew Bible portion only), and Henry James’ novel What Maisie Knew (Maisie). A small amount of preprocessing was done before and after StanfordCoreNLP’s parsing, both to fix some repetitive errors in StanfordCoreNLP’s analysis and also to remove, and then reinstate, the main archaisms in the KJV.   2.2 Algorithm In order to develop an algorithm for mimicking a human’s choice of context, I developed training data by randomly choosing key words from the ESV and line lengths, ranging from what might fit legibly on a typical smartphone to a line three times as long. I then displayed all possible contexts that fit on the line but make maximum use of the space on the line. That is, no more words could fit on either side. In addition, sensible rules concerning which types of punctuation were appropriate at the beginning or end were employed (e.g. a possible context could not begin with a comma or end with an open quotation mark), and possible contexts could not cross verse boundaries. In the rare case that there was only one option, that key word was discarded. A user selected his preferred context for 500 such key words, occasionally choosing two or three different contexts if they seemed equally desirable1. After analyzing his choices, I produced the following metric w(k,n) to give a value (weight) to each nearby word n for the key word k:    Let ap be the nearest common ancestor of k and n in the phrase structure tree and ad be the nearest common ancestor of k and n in the dependency tree. Then dpk is the distance between ap and k in the phrase structure tree, dpn is the distance between ap and n in the phrase structure tree, ddk is the distance between ad and k in the dependency tree, and ddn is the distance between ad and n in the dependency tree. Each possible context is evaluated as the sum of w(k,n) for each n in the possible context; the context with the highest value is chosen. If multiple possible contexts have identical values, any can be chosen; I picked the one that had the most context before the key word. The constants were optimized to the following values using a Monte Carlo particle filter on the training data:    These constants reveal that the dependency tree was more important than the phrase structure tree.   3. Results In addition to the above-mentioned training set, test sets were then generated from the ESV and Maisie with 100 key words each, and four human annotators made selections for each. The results are listed in Table 1. Since human annotators occasionally selected two or three contexts as equally good, a match for a given key word is calculated as:        ESV training set ESV test set Maisie test set   Algorithm matches user selection 67.8% 62.5% 47.8%   Expected algorithm matches if selections were random from a uniform distribution 27.4% 25.5% 21.9%   Inter-annotator agreement N/A 65.8% 53.0%   Expected inter-annotator agreement if selections were random from a uniform distribution N/A 27.0% 23.5%   These results indicate that on average, the algorithm matches a given human annotator slightly less often than another human annotator does. Assuming that human intuition presents the gold standard for this task, this means that the algorithm is doing only slightly worse than humans at picking the best context for the key word. Some screenshots of algorithmically-generated key words in context are shown below.   Fig. 6: KWIC search for “Aaron” in ESV, KJV; “Maisie” in Maisie (from left to right).    Fig. 7: Randomly Selected Key Words from ESV, KJV and Maisie (from left to right).    4. Conclusion Searching for key words is one of the core functions of text analysis software. The work presented here holds promise as a way of improving the way in which search results are displayed by automating a time-consuming manual technique traditionally used in print concordances. In addition, future work could deal with more complex displays, including possibly not using all the space available, possibly using ellipses, and dealing with displaying results of searches involving multiple key words.  ",
       "article_title":"Marrying the Benefits of Print and Digital: Algorithmically Selecting Context for a Key Word",
       "authors":[
          {
             "given":"Drayton Callen",
             "family":"Benner",
             "affiliation":[
                {
                   "original_name":"University of Chicago, United States of America ",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "natural language processing",
          "corpora and corpus activities",
          "concording and indexing",
          "information retrieval",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Historical legal charters can provide an important window into the workings of a society, and because of their legal significance they often survive as evidence of a society when there is little other documentary evidence available. Of course, original medieval charters have always been difficult to access, and for this reason work was undertaken even in the 19th century to prepare printed editions of them. Nowadays they are published over the WWW, since space is unlimited and access for potential users is quick and easy. See, for example, the efforts of the Institute of Historical Research in its British History Online project which has been transcribing printed editions of collections and making them available over the WWW (e.g. Marwick 1894), and the monasterium.net (Mom 2013) from which, as its website says, \"Original documents [are] available world wide 24 hours a day, independent of the researcher’s physical location.\"  Once the texts have been digitised, the question arises about what can be done with them. One strand for this is exemplified in the work of the ChartEx project (CharteEx 2013) which has been funded by the Digging into Data Challenge and is developing \"new ways of exploring the full text content of digital historical records\" through the use of natural language processing and data mining techniques to extract automatically information about people, events and places and find relationships between them. ChartEx, then, by its use of advanced text processing and analysis techniques follows one of the long traditions of the digital humanities. The other long standing approach to dealing with text – markup – is also active with the work of the Charters Encoding Initiative project (CEI 2009) which has developed a TEI-derived markup scheme suitable for charter texts. CEI takes, perhaps not surprisingly, primarily a diplomatic approach. Georg Vogeler, writing about the CEI, claims that medieval European charters \"reflect contemporary attitudes and mindsets as regards legal and representation issues\" and \"are tools of diplomatic criticism\" (Vogeler 2005, p 276). He further claims that through CEI markup one has a \"platform for seeing the European Middle Ages as they are reflected in their charters\" (Vogeler 2005, p 279).  DDH at King's College London has been involved in several projects that have used charters as their subject matter, but these projects have not taken either the text analysis or markup approaches. Charters were a substantial component of the Prosopography of Anglo-Saxon England (PASE 2010). Two other more recent projects have made charters their primary focus, even more than PASE. The Paradox of Medieval Scotland project (PoMS 2010) (extended more recently in the People of Medieval Scotland (PoMS 2013)) represents more than 6000 charters and claims to be \"the first online prosopographical database to comprise exclusively and exhaustively the corpus of administrative documents of a European kingdom in the central middle ages.\" (Hammond 2013, p 7). More recently, and still in development, the Making of Charlemagne's Europe project (Charlemagne 2013) is \"a database of prosopographical and socio-economic data found in the more than four thousand legal documents surviving from Charlemagne’s reign.\" An important difference between these projects and the methodologies implied by projects such as Chartex or CEI is that none of PASE, PoMS or Charlemagne actually represent the texts themselves. Instead all three projects take a structured data approach – developing a formal model in highly structured data that aims to express some aspects of what the charters are talking about. The very nature of charter texts is that they provide what their creators thought of as a formal approach to the dealing with their property. The models used for these three projects try to capture explicitly formal structures that the charter's creators seemingly had in their mind when they created them. Although studies of the charter documents such as that carried out in diplomatics provides one of the bases for the work here, as Hammond says about PoMS: \"A clear distinction has been dawn [in PoMS] between the form of the document, based on its diplomatic, and the events which are transactions ... in the document\" (Hammond 2013, p 11). What can be said about the charters in these projects when the charter text itself is not present? It turns out that the absence of the text enables a representation of the material about a charter that somehow liberates it from being grounded too exclusively in the words of the text itself.  All three of these projects grew out of the thinking about structure that is represented by DDH's factoid approach to prosopography, described in Bradley and Short 2005 and extended into the world of the Semantic Web in Pasin and Bradley 2013. The virtue of the factoid approach is that it provides a source-grounded place for our researchers to say many different kinds of things about their documents. It allowed researchers to record not only the main things the charter was created to talk about – the arrangements about property – but also the wealth of further information that they capture: relationships between people (kinship and otherwise), the titles they held, other events they were involved in, etc. The factoid model also allows for the roles of people and associated other objects mentioned in the charters to be specified and for the way in which their name was recorded in a particular spot in the charter text to be recorded as well. Furthermore – and most relevant here – it supported a richer understanding of a charter's complex set of associated events. In PASE, for example, we were able to formally separate the event of the charter signing, with its perhaps specific historical significance, from the activities – usually transactions on property – that the charter talked about. Multiple exchanges in a single charter could be captured as separate transaction-like event factoids.  In the end the factoid approach to the charters encouraged us to also think about how to formalise other aspects of what our charters represented. People, for example, turn up not only as agents in the charters but sometimes as possessions. Possessions are complex entities in charters: often they are pieces of land or institutions sitting on land, but charters also show people possessing rights of many kinds, such as the right to run a fair, to collect taxes, to move away from a piece of property, to celebrate divine service, etc. Furthermore, what was actually being given in relationship to a possession could be complex too. PoMS researcher John Reuben Davies developed a scheme to clarify and enrich the classification of transactions presented in charters, going beyond simple classifications such as 'grants' and 'confirmations' to include 'renewals' and 'successions', among many others. Charlemagne has been exploring the terms and conditions attached to the exchange of possessions as described in their charters, and has also been exploring the complex way in which historical places are presented in their charter documents – not only mapping them, where possible, to modern places, but attempting to represent the complex and often obscure connections between place and historical region that their charters describe. In the end, the rich formal structure that underpins PoMS and Charlemagne represents some of the subtlety of the legal understanding that was emerging in Charlemagne's time and in the days of Medieval Scotland. It exposes some part of what the charter creators were trying to achieve in their efforts to formalise their agreements between themselves about their possessions.  In this presentation we will introduce some of the structure that extends the factoid model and represents several of the complex processes that these charter documents were attempting to formalise. The resultant models attempt to structure aspects of these documents than have not been tackled before and tries to find a point that both enriches our understanding through formalising them, and avoids ignoring, through excessive formalisation, the ambiguity and vagueness of the emerging legal process that happened both in the time of Charlemagne and Medieval Scotland. By developing a formal model for our understanding of the framework in which the charters operated, we believe that we complement the text based approaches of both Chartex and the CEI.  Finally, our work is, as the famous historian E.H. Carr said, an attempt to recognise \"History [as] a process\". With its extension of the charter documents into the historical world of people, places and possessions, it attempts to recognise that \"you cannot isolate a bit of the process and study it on its own\" (Evans 2001). Our models make explicit a set of views and assumptions by our researchers about these medieval worlds, and in their formality and clarity make it more possible to, in the words of the historian Richard J. Evans, \"subordinate them to the intractabilities of the material with which they are working, and enable readers to study [our] work critically by making these views and assumptions explicit\". In this presentation we hope to encourage this kind of dialogue.   ",
       "article_title":"Exploring a model for the semantics of Medieval Legal Charters",
       "authors":[
          {
             "given":"John",
             "family":"Bradley",
             "affiliation":[
                {
                   "original_name":"Department of Digital Humanities, King's College London, United Kingdom",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Alice",
             "family":"Rio",
             "affiliation":[
                {
                   "original_name":"Department of History, King's College London, United Kingdom",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Matthew",
             "family":"Hammond",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Glasgos, United Kingdom ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Dauvit",
             "family":"Broun",
             "affiliation":[
                {
                   "original_name":"Department of History, University of Glasgos, United Kingdom ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "medieval studies",
          "databases & dbms",
          "historical studies",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Abstract “Play” is not a foreign concept to the digital humanities nor to the Digital Humanities Conference. Indeed, there have been a number of presentations over the years that focus on the ludic interactions between player and game in virtual worlds; that consider game design; that attempt to preserve experiences of gameplay; that view play as integral for the creation of new texts or interpretation of already existing ones; or that philosophically connect play to pedagogy. There have even been panels that examine the literal mechanics of play in the context of video. But while play has been central to these discussions, it has most often been treated very seriously. In contrast, this presentation will highlight a project that placed playfulness at the center of its development and of its final project. Serendip-o-matic (serendipomatic.org) is a “serendipity engine” that connects large texts (e.g., entire articles or syllabi) or personal research libraries to digital materials located in libraries, archives and museums around the world. It recreates the surprising discoveries that frequently accompany research. Serendip- o-matic was built in less than five days as part of One Week | One Tool (OWOT, oneweekonetool.org). OWOT is a playful departure from traditional institutes funded by the U.S.’s National Endowment for the Humanities (NEH) in that the outcomes are largely determined by its participants rather than by the institute’s organizers. And the team that designed and built Serendip-o-matic decided to place play at the center of its process and final product. “Play” has been defined as the “free movement within a more rigid structure\". In the context of tool building, a playful process means dodging the rigid structures that so often define the tool development process. A digital humanities “barn-raising” destabilizes the established ways in which digital humanities tool building projects are traditionally formulated, facilitating a playful process. The impending deadline—just. one. week!—and the rapid-fire completion of tasks and decision-making made the building experience intense and challenging. This presentation will discuss the various ways in which the participants chose to frame the experience, from hack days to games to reality TV. Although it was a very real possibility that Serendip-o-matic might fail to launch on time (if at all), the artificial construction of the challenge—as an NEH institute, wherein the main goal was for participants to learn something—made the consequences of such failure less severe. Separated from the responsibilities of daily life, the immersive format of OWOT offered the team the opportunity to take more risks and engage in a playful practice of ideation, conceptualization, and making. Play can also be a significant method for creating engaging user-experiences in design. This technique has been employed in a variety of user-experience environments, particularly those aimed at engaging the public with humanities and cultural heritage materials, such as the Tate Museum’s “Magic Tate Ball” and the Wellcome Collection’s “High Tea”. The playfulness of Serendip-o-matic is easily visible upon visiting the project’s website. The logo resembles a Rube Goldberg machine, and the colors and iconography throughout the site were chosen and created specifically to evoke the lively nature of the tool itself. The text throughout the site also broadcasts a playful attitude. For example, users are asked not to “select” or “upload” a “document” but instead to simply “grab some text.” Much of the work of the Outreach team during the week was spent crafting this text as well as the name for the tool, with the goal of preserving the playful feel of the entire enterprise. In this presentation, members of the OWOT team will report on creating a playful digital humanities tool. In addition to discussing the rapid and ludic development atmosphere and the choices made to shape user experience, the presentation will discuss the playfulness that was encoded in the tool’s discovery algorithm, which uses named-entity recognition and text parsing to create a 'magic machine' that queries various collection APIs. We will also outline steps that were taken to create a playful voice throughout the deployment of the tool, including the creation of a mascot for the tool (the “Serendhippo”) and rules for public engagement for the mascot as well as the whole team. We will also report on the challenges of a playful project, one of which includes the consideration of how such playful work should be evaluated in the academy. In conclusion, we will argue in favor of the benefits for incorporating more “playful work” in the context of academic research and scholarship and the significant role played by the interface and user experience design skills of several team members. As current digital humanities work relies on collaborative environments (including hackathons, maker spaces, maker challenges, etc.), opportunities like One Week | One Tool provide a space for playful work to encourage more creative risk-taking and engaging user-experiences within the context of digital humanities scholarship and practice.  ",
       "article_title":"Play as Process and Product: On Making Serendip-o-matic",
       "authors":[
          {
             "given":"Mia",
             "family":"Ridge",
             "affiliation":[
                {
                   "original_name":"The Open University, United Kingdom",
                   "normalized_name":"Universidade Aberta",
                   "country":"Portugal",
                   "identifiers":{
                      "ror":"https://ror.org/02rv3w387",
                      "GRID":"grid.26693.38"
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Croxall",
             "affiliation":[
                {
                   "original_name":"Emory University, United States of America",
                   "normalized_name":"Emory University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03czfpz43",
                      "GRID":"grid.189967.8"
                   }
                }
             ]
          },
          {
             "given":"Amy",
             "family":"Papaelias",
             "affiliation":[
                {
                   "original_name":"State University of New York at New Paltz, United States of America",
                   "normalized_name":"State University of New York at New Paltz",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03j3dv688",
                      "GRID":"grid.264270.5"
                   }
                }
             ]
          },
          {
             "given":"Scott",
             "family":"Kleinman",
             "affiliation":[
                {
                   "original_name":"California State University, Northridge",
                   "normalized_name":"California State University, Chico",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/027bzz146",
                      "GRID":"grid.253555.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "games and meaningful play",
          "natural language processing",
          "organization",
          "other",
          "software design and development",
          "project design",
          "digital humanities - nature and significance",
          "GLAM: galleries",
          "digital humanities - institutional support",
          "libraries",
          "management"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Coptic language evolved from the language of the hieroglyphs of the pharaonic era and represents the last phase of the Egyptian language.  It is pivotal for a wide range of humanistic disciplines, such as linguistics, biblical studies, the history of Christianity, Egyptology, and ancient history.  Whereas languages like Classical Greek and Latin have enjoyed advances made in digital humanities with fully-fledged online research environments accessible to students and scholars (such as the Perseus Digital Library), until recently, no computational tools for Coptic have existed. Nor has an open digital research corpus been available.  The research team developing Coptic SCRIPTORIUM (Sahidic Corpus Research: Internet Platform for Interdisciplinary multilayer Methods) is developing and providing open-source technologies and methodologies for interdisciplinary research across multiple disciplines in the Coptic language.  This paper will address the automated tools we are developing for annotating and conducting research on a Coptic digital corpus.   Conducting digitally-assisted and computational research in Coptic using available DH resources is complex for several reasons.  Most texts are preserved from damaged, incomplete, and dismembered manuscripts or papyri.  The DH project papyri.info has begun to create an online open-access resource for the study of Greek papyri and is beginning to digitize Coptic papyri and ostraca (ancient pot-shards with writing).  These texts, however, are primarily documentary, consisting of wills, contracts, personal letters, etc.  Coptic literary and monastic texts, the core of Coptic SCRIPTORIUM, are essential for the study of the Bible, intellectual history, literary history, and religious history.  The manuscripts containing these texts were removed from Egypt in the seventeenth through nineteenth centuries piece by piece (sometimes page by page).  Some have been published, many have not, and very few have been digitized in a format suitable for digital and computational work.  Texts must be must be reconstructed from pieces of manuscripts published in fragments and/or stored in various libraries and museums worldwide. The status of Coptic literary and monastic complicates metadata management and corpus architecture:  what constitutes a “work” – the codex in which a copy of the text appeared (and which may be dispersed across multiple physical repositories)? the manuscript fragment housed in a particular library or museum repository or the work, which only might survive in fragments of multiple codices (all copies of a “book” from the monastery’s library), and thus in fragments not only from more than one codex but also more than one modern repository? Coptic scholarship still lacks many standards for digital publication and language research that are taken for granted in Greek and Latin. As with other ancient languages, Coptic manuscripts are written without spaces.  However, in contrast to its ancient counterparts, scholarly conventions on word division differ substantially from scholar to scholar. Additionally, since Coptic is an agglutinative language, the relevant unit for linguistic analysis is the morpheme, below the ‘word’ level.  This means that segmentation guidelines must be developed for both levels of resolution. In order to search multiple texts, guidelines and tools for normalization, part-of-speech tagging and lemmatization of Coptic must be developed.  These tools need to take into account Coptic’s agglutinative nature, e.g. normalizing and annotating on the morpheme and word levels. Finally, the development of the Coptic language during Egypt’s Greco-Roman era raises questions about the origins of the language, its usage in a multilingual context, and the language practices of its ancient speakers and writers.  Coptic consists of Egyptian grammar, vocabulary, and syntax written primarily in the Greek alphabet; some Egyptian letters were retained, and some Greek and Latin vocabulary was incorporated into the language. The richness of the vocabulary’s languages of origin varies from author to author, genre to genre.  And despite recent publications on the topic, much research remains to be conducted on the extent and nature of multilingualism in late antique Egypt, especially during the fourth and fifth centuries. Additionally, due to the agglutinative nature of the language, one word can be comprised of morphemes with different languages of origin. This paper will focus on the automated tools our project is developing to process the language, especially tokenizing and part-of-speech annotations.  Coptic SCRIPTORIUM has developed the first tokenizer and part-of-speech tagger for the language, and in fact for any language in the Egyptian language family. The presentation will address the unique challenges to processing and annotating the Coptic language.  We will present our current technical solutions, their accuracy rates, and the potential for future research.  We will also address the ways in which this language’s and corpus’s unique featured differentiate them from other more widely studied ancient languages, such as Greek and Latin.  Examples will be drawn from the open-access corpora we are developing and annotating with these tools, available at coptic.pacific.edu (backup site www.carrieschroeder.com/scriptorium).  The Coptic corpora processed and annotated with these tools can be searched and visualized in ANNIS, a tool for multi-layer annotated corpora.  We anticipate this presentation to be of interest to scholars in digital humanities working with ancient languages and manuscript corpora as well as DH linguists and corpus linguists. ",
       "article_title":"Digitizing the Dead and Dismembered: DH Technologies for the Study of Coptic Texts",
       "authors":[
          {
             "given":"Caroline T.",
             "family":"Schroeder",
             "affiliation":[
                {
                   "original_name":"University of the Pacific, United States of America",
                   "normalized_name":"University of the Pacific",
                   "country":"Chile",
                   "identifiers":{
                      "ror":"https://ror.org/005gk0s47",
                      "GRID":"grid.441817.f"
                   }
                }
             ]
          },
          {
             "given":"Amir",
             "family":"Zeldes",
             "affiliation":[
                {
                   "original_name":"Humboldt University, Berlin",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "classical studies",
          "theology",
          "corpora and corpus activities",
          "near eastern studies",
          "xml",
          "historical studies",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  By conservative estimates, several hundred thousand poems appeared in early American and U.S. newspapers from the eighteenth through the early twentieth centuries. Counting snippets of verse that appeared in death notices, advertisements, and articles makes the presence of poetry in historic newspapers even more pervasive. Feminist scholars and others performing recovery work routinely resurrect authors and works from newspaper pages, but until recently this rich trove of newspaper verse as a corpus of its own has been outside the scope of literary study and a footnote in histories of American newspapers. In the last decade, however, scholars have made significant inroads in studying the importance of newspaper verse as a form and the public role of poetry in American culture. Underpinning this scholarship is a growing recognition that the evaluation and history of American poetry should not be based on less than one percent of the poetic record. In addition, this new scholarship values and explores the role of poetry in the daily lives of people, including making sense of what it means to be human and in processing national, social, and individual experiences. To the extent that these new histories depend on traditional methods of archival discovery and analysis, however, they will remain anecdotal—individual narratives extrapolated from a miniscule subset of the whole, with limited means of situating the anecdote as either representative or idiosyncratic. In short, the magnitude of the corpus requires new modes of discovery and analysis.  A fundamental problem in the reappraisal of newspaper verse has been finding and processing poetic content in an efficient manner, which is essential for developing new interpretations, analyses, and literary histories. The primary means of finding this content typically involves paging through original issues of newspapers, scrolling through reels of microfilm, and browsing digital images to scan, by human eye, each page for graphical features that resemble poetry. Dealing with only daily newspapers for a single year, 1860, would require visually scanning nearly half a million newspaper pages. Certainly no individual in a lifetime could complete a count—to say nothing of a comprehensive bibliography or macro-level analysis—of newspaper verse using this strategy.  While the digitization of historic newspapers has mitigated some issues of access, the main avenues for discovery in these collections are browsing and text-based searching. Browsing for poetic content in such collections follows the strategy outlined above: going image by image through digitized pages and visually scanning the images for features typical of printed poetry. Ironically, web interfaces and variations in Internet connection speeds can make digitally paging through a newspaper a slower process than either scrolling through microfilm or flipping physical pages.  How, then, might one discover poetic content in digitized historic newspapers? Our research shows that digital page images hold significant promise for scholarly inquiry with regard to poetic content. The basis for our approach is that the appearance of poetic content usually follows certain patterns that can be visually differentiated from other published texts in newspapers. Given a newspaper page, a person can survey the page and figure out quickly whether it contains a poem, to a certain degree of accuracy, without having to read or understand the text. Our project has the computer do the same visual processing as the human eye and brain when a person moves through a newspaper issue looking for poetic content. This image processing approach can also be used as a powerful filter, removing materials from further consideration that do not meet the specified criteria. That is, not only does the process work to identify pages that appear to include poetry, but it discards those that do not, weeding out much of the noise.  Methodology  The image processing component of the project consisted of two important phases: training and deployment. During the training phase, the goal was to produce a classifier able to categorize an image as either a poem or non-poem image. To produce the classifier, a training dataset was prepared and fed into a machine learning-based classifier. After the classifier was produced, we moved to the deployment phase. During this phase, the steps used to prepare the training sets were streamlined to automatically process and classify new images. In this paper, we focus on the four stages of the training phase: (1) pre-processing, (2) feature extraction, (2) neural networks learning, and (4) testing.  The first stage involved manual extraction of image snippets from digitized newspapers. For training, we developed three sets of snippets: (1) at least part of a poem appears in the snippet, (2) the snippet contains no poetic content, and (3) the snippet has visual cues that are similar to poetic content. Because each snippet is inherently noisy and could be of low quality, we performed 3x3, 5x5, and 7x7 averaging to smooth out noisy pixels, a step known as blurring. To convert the blurred snippet into a binary image—effectively identifying the object pixels from background pixels—we used a bi-Gaussian (or bi-normal) curve approximation (Haverkamp et al., 1995) to obtain the binary segmentation threshold. (See Figure 1 and Figure 2.)   Fig. 1: Binary non-poem image snippet; 5x5 blurring.    Fig. 2: Binary poem image snippet; 7x7 blurring.  After obtaining binary images, the next task was representing and extracting visual cues as salient features. We evaluated three sets of attributes: (1) the left and right margins (number of columns without a dark pixel); (2) the vertical white spacing between adjacent lines of text (mean and standard deviation); (3) the jaggedness of the ends-of-lines for poetic content (mean and standard deviation).  Then, with this imagery data re-represented as numeric data, we used machine learning techniques to train a classifier. Specifically, we used the machine learning approach known as artificial neural networks (ANNs) (Hopfield, 1988; Yegnanarayana, 2009). An ANN learns a vector of weights on features in the dataset to choose labels for new data. Such a network consists of multiple nodes connected to threshold functions or additional layers of nodes. The network is updated iteratively until it correctly predicts labels for training data. We used a back- propagation ANN (Hecht-Nielsen, 1989) where weights connecting the nodes update iteratively based on how the network classifies known data points in the training dataset—whether its classification matches the labels of training data points.  Back-propagation ANNs have two phases. During phase 1, a training instance or data point is fed into the ANN’s multi-layer structure, generating activations in the nodes and resulting in a final output label. During phase 2, \"rewards\" are propagated back to the input layer from the output layer based on whether the final output label matches the ground-truth label of the instance. If the output label is correct, all the linkages within the structure contributing to the correct output are rewarded with an increased weight. If the output label is incorrect, all contributing linkages are penalized accordingly with a reduced weight. In this manner, the network learns incrementally to find a combination of weights for these links.  Finally, to validate the accuracy of the classifier, a ten-fold cross validation process was used. In this process, the total data set was broken into ten groups. The classifier was trained using nine of the groups and then tested on the single remaining set. This process of training and testing was repeated until each group had been used as the test set once. The results were then aggregated and the accuracy computed.  Findings  In addition to discussing the importance of newspapers and newspaper verse for American literary history and the need for new modes of discovery in digitized collections, this paper will report on three results of our research: (1) basic analysis, (2) training and classification analysis, and (3) a comparative study. The basic analysis will present the algorithms we used to extract visual features from the snippets, and the correlation analyses we did to ascertain the feasibility of our image-based approach. This will show the potential discrimination power of our visual features in distinguishing poem or non-poem snippets. The training and classification analysis will document our experimentation with different ANN configurations and report on our training processes. This will include the number of hidden nodes in our ANNs used, learning weights and momentum parameters investigated, convergence rates of the different configurations, and training and classification accuracies. Finally, we will report on our comparative study in terms of the usefulness of our blurring processes and how better to fuse them. In particular, we will show whether submitting each image as a single data point with three sets of attributes or submitting each blurred image with its set of attributes as a single data point leads to more effective training and higher classification accuracy.   ",
       "article_title":"Detection of Poetic Content in Historic Newspapers through Image Analysis",
       "authors":[
          {
             "given":"Elizabeth M",
             "family":"Lorang",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America ",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Leen-Kiat",
             "family":"Soh",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America ",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Joseph",
             "family":"Lunde",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America ",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Grace",
             "family":"Thomas",
             "affiliation":[
                {
                   "original_name":"University of Nebraska-Lincoln, United States of America ",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "image processing",
          "literary studies",
          "and discovery",
          "resource creation",
          "poetry",
          "drama",
          "digitisation",
          "genre-specific studies: prose"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"     This paper reflects on the pros and cons of a mixed contributory, collaborative and co-creative model of community engagement (Bonney et al. 2009) with archaeology through crowdsourcing and crowd-funding. It arises from a recently initiated project entitled Crowd- and Community-fuelled Archaeological Research. This project is collaboration between University College London’s Institute of Archaeology and The British Museum and is codenamed MicroPasts. This project is developing a web platform to promote new online communities that span hitherto different kinds of archaeological enthusiasts, from ‘traditional academics’, to already established groups of volunteer interest such as archaeological societies, and a wider crowd of potential contributors.  These potentially diverse and international communities will have the opportunity of collaborating on one or more of three things:   1. Co-production of open licensed research data, such as 3D models of Bronze Age metal objects from England, or the tagging of historical photographs of early 20th century archaeological excavations in the Levant; 2. Collaborative development of completely new research projects, where several different kinds of contributor will be involved. The intellectual lead need not be an institutionally affiliated academic; 3. Crowd-funding of some of the latter projects alongside a number of other community archaeology initiatives. We hope to thereby formalise and build upon some pioneering but rather spontaneous experiments using crowdsourcing to co-design new research (e.g. Old Weather, or Herbaria@home; Ridge 2013). For example, we will begin with ‘scaffolded’ contributory activities where members of the public are invited to participate in research agendas already outlined by academics. It is then hoped that the contributors can develop these research agendas into a higher or even tangential direction.  On the MicroPasts platform, we hope to foster an ever increasing sequence of participation, by encouraging those involved in data co-production via standard crowd-sourcing to participate in follow-up co-design and also to perhaps fund their projects through crowd-funding initiatives. MicroPasts contributors are entirely free to focus their efforts on just one, or if they prefer, several of these areas. To our knowledge, this is the first model of participatory community engagement of this kind in the archaeology and cultural heritage sector. This paper will present both this overall project rationale and the technical choices we have made to turn it into reality.  All the software employed in this project will be open source and any modifications we make will be released to the open source community via an institutional GitHub account. For example, we will be using a British Museum hosted multi-site Wordpress installation for our main portal and for blogging about our research. We have already customised an instance of the CrowdCrafting platform (see Mansell 2012:8) and the first application to go live on this will allow transcription of the British Museum’s New Bronze Age II card index. For community interaction, we are using the Discourse  platform for discussion and help to be dispensed by the contributors and facilitators. For the crowd-funding element of the project, we will be implementing an instance of the Neighborly community fund raising software (based on the successful Brazilian Catarse software.) We will demonstrate how these software packages contribute to the holistic model of participatory archaeological research that we have in mind especially with regard to how the challenging task of effective co-design might be achieved. We will also discuss how different crowd-sourcing tasks have been incorporated into the CrowdCrafting platform, the challenges and obstacles we faced during this process (for example institutional opposition to micropayments), what influenced the choices that we made and how different tasks succeeded or failed. In so doing we hope to provide wider inspiration for others in the fields associated with Digital Humanities and a replicable model for anyone to copy. For instance, all the software created for this project could conceivably be reused by anyone with an interest in crowdsourcing or crowdfunding. Finally, we will consider what kind of evaluative framework is suitable for understanding community engagement in such activities. We are interested in understanding the experiential, cultural and economic values behind contributors’ involvement in crowdsourcing and crowd-funding research into the human past. To achieve this aim, the following areas will be investigated individually and their inter-relational synthesis: (a) Motivations for contributing; (b) Dynamics of community building and the kinds of relationships that are built and sustained;  (c) Cultural and economic resources mobilised via community members. Our analysis will address existing works on these topics within the science and cultural heritage domains, but will also look to deepen their insights with respect to archaeology and history in particular. For example, previous research such as Raddick et al. 2009, Haklay 2011 and Ridge 2013 can be built upon by closer attention to our contributors’ understanding of the subjects they engage in through our project.  The framework will be applied using a mixed quantitative and qualitative approach, and by combining more ‘traditional’ methods borrowed from the social sciences with “natively digital” ones (Rogers 2013) that (with due attention to ethical considerations) harvest cultural tastes and practices from social networks and try to understand how they influence community formation processes.  ",
       "article_title":"Mixing contributions, collaborations and co-creation: participatory archaeology through crowd-sourcing",
       "authors":[
          {
             "given":"Daniel Edward John",
             "family":"Pett",
             "affiliation":[
                {
                   "original_name":"The British Museum, United Kingdom",
                   "normalized_name":"British Museum",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00pbh0a34",
                      "GRID":"grid.29109.33"
                   }
                }
             ]
          },
          {
             "given":"Chiara",
             "family":"Bonacchi",
             "affiliation":[
                {
                   "original_name":"The Institute of Archaeology, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Andy",
             "family":"Bevan",
             "affiliation":[
                {
                   "original_name":"The Institute of Archaeology, University College London, United Kingdom",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "organization",
          "visualisation",
          "licensing",
          "image processing",
          "software design and development",
          "project design",
          "maps and mapping",
          "content analysis",
          "archaeology",
          "interdisciplinary collaboration",
          "copyright",
          "and Open Access",
          "crowdsourcing",
          "management"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Developing for Distant Listening: Developing Computational Tools for Sound Analysis By Framing User Requirements within Critical Theories for Sound Studies  The Council on Library and Information Resources (CLIR) and the Library of Congress (LoC) issued a 2010 report that suggests that if we do not use sound archives, our cultural heritage institutions will not preserve them. Nancy Davenport, previous president of CLIR, concludes that users want unfettered access and better discovery tools for what she calls “deep listening” (what Charles Bernstein calls “close listening”) or “listening for content, in note, performance, mood, texture, and technology.”  It is a typical digital humanities problem: Without a better understanding of what such listening entails, we cannot build tools that afford such listening; and, because we lack the tools, humanists struggle to imagine how to describe the access they want -- what Jerome McGann calls “imagining what you don’t know.” In an attempt to imagine how to facilitate distant listening with computation, this paper positions user requirements for critical listening software within the context of critical listening theories.  Critical Listening Theories  Walter J. Ong once announced that recording technologies have heralded a new age in the study of the “voice, muted by script and print.” Humanists hold a range of theories and perspectives on how to study music or sound aesthetics in experimental poetry or how to contextualize sounds of the recording space (such as the whir of an old air conditioning or babies crying in the background) or the recording machine (such as the clicks and pops and pauses).  In particular, theoretical perspectives on “the voice” are useful in identifying the role sonic features that are discoverable by computation can play while close listening. Most such theories, for example, position sonic vocal traits as meaningful only within the context of a structural code for meaning such as language. Roland Barthes identifies two aspects of the voice in vocal music, for instance, that contribute to meaning making: the pheno-song, which refers to the structured elements of a piece such as speech or melody (or language codes) and the geno-song, which is the material or corporal aspect of the voice, the “volume of the singing and speaking voice, the space where significations germinate” (or sonic features). 5Privileging the pheno-song as more productive for communicating meaning, Barthes maintains that the geno-song –having “nothing to do with communication, representation (of feelings), expression”—is a system for transmitting that meaning.  Similarly, Michael Chion asserts that sonic features have meaning but that it is our lack of a descriptive system or hermeneutics that precludes our ability to make sense of these features. Chion approaches sound study by parsing listening into causal (for the source of the sound), semantic (to interpret a message), and reduced (to identify sonic traits) listening. Chion argues that reduced listening precludes meaning making for two reasons: (1) the “fixity” of sonic features required for close listening to sonic traits makes sound “physical data” that do not represent what was actually spoken or actually heard in real time “presence”; and (2) our language for describing how we make meaning with such traits is “totally inadequate.” Consequently, because of issues of fixity and inadequate identifiers, reduced listening is “an enterprise that is new, fruitful, and hardly natural.”  This argument, however, that the voice is only meaningful in the context of speech that transmits a message is a logocentric theoretical stance that has been readily contested. Adriana Cavarero who seeks to “understand speech from the perspective of the voice instead of from the perspective of language” wants to “pull speech itself from the deadly grip of logocentrism.” Caravero critiques the viewpoint of scholars such as Walter Ong and Marshall MacLuhan who at once essentialize the voice as “presence” and disembody and mythicize orality. Similarly, Mladen Dolar considers a “linguistics of non-voices” including coughing, hiccups, babbling, screaming, laughing, and singing, placing these sounds outside of the phonemic structure yet not outside of the linguistic structure. He argues that “It is not that our vocabulary is scanty and its deficiency should be remedied: faced with the voice, words structurally fail.” Finding possibilities for study in aspects of the voice such as accent, intonation, and timbre, Dolar asks the question at the heart of all of these queries: “how can we pursue this dimension of the voice?”  User Needs  User perspectives on the kinds of access and analysis advanced technologies with sound can facilitate were gathered by Clement as part of the HiPSTAS (High Performance Sound Technologies for Access and Scholarship) project. HiPSTAS is an NEH-funded, year-long Institute for Advanced Topics in the Humanities for librarians, information scientists, and humanities scholars who work with spoken word collections. Such collections include PennSound’s poetry archive, the American Folklife Center of the Library of Congress, oral histories in StoryCorps, and recordings from more than 50 tribes across Native America in the American Philosophical Society’s Native American Collection among other collections of interest to the participants. User perspectives were gathered from the 20 participants in three ways: (1) through Institute applications; (2) through pre-Institute interviews; and (3) through post-Institute surveys and project reports.  This data shows that defining the sonic features that map to specific cultural characteristics of “the voice” in spoken word recordings was not how participants phrased their research interests. One participant, for example, who was interested in working with the PennSound archive, wanted to consider “media ecologies” by analyzing “sounded affinities between poets” or “concepts of community poetics through sound;” this participant wanted “to look at groups of poets who have a common locale in terms of their community formation” and to use these clusterings to investigate how software “may or may not track affinities across gender lines.” Another participant analyzing PennSound was interested in “identifying, exploring, and categorizing performance variants of the same texts” such as “an aural/visual equivalent to the Versioning Machine)” and enabling “a kind of distant listening, flagging and visualizing generic features of poetry performance traditions (Is there, for example, a New York School style of oral delivery?” Other concerns were focused on reorienting how the archive is discoverable by enabling a “batch analysis of an audio corpus to mark the aural/perceptual relationships of poetry performance and extra-poetic ‘asides’ (which often provide significant contextualization of the poems but which may be ‘invisible’ in the Pennsound archive).”  Another participant interested in the APS recordings wanted to discover what it meant to think through “how a digital archive can recover intangible and ephemeral yet deeply powerful social experiences of sound” including “[w]hat themes of identity, gendered relations, and intercultural relations, may be heard in the Native speakers’ and singers’ expressions and performances of the recorded stories and songs in the collections;” this participant wondered, “how might we thematize and index sounds to address issues of indigenous sonic embodiment in files from which we can hear but not necessarily see the speakers and singers? What are the [sonic] differences and similarities among performers of similar source material? How do these performative differences/similarities map or not map onto other factors (race, gender, region, class, age, etc.)?” Also interested in the APS Native American collections, another participant wanted to analyze these holdings in order to classify Navajo speakers against a map of origin in order to illustrate the location of a speaker. With the ultimate goal of “develop[ing] a cultural map to show spheres of influence of those language-speaking approaches on the stories and motifs across time and in proximity to historical centers of tribal trauma,” this participant wanted to use software “to determine whether dialectical region or if proximity to historical centers of tribal trauma (e.g. boarding school experiences or Navajo Long Walk) influence that speaker’s . . . Beauty Way and Protection Way approaches to speaking the Diné language”.  Conclusion  Ultimately, can a computer be taught to distinguish between paralinguistic commentaries and formal (or informal) poetry readings or a Beauty Way speaker and a Protection Way speaker within a large collection of sound files? This paper attempts to imagine these possibilities by positioning what users want to do with sound within a critical framework of listening theories that understand “the voice” as a cultural phenomenon that reflects the resonance between linguistic and non-linguistic features of sound. This paper will primarily frame user requirements gathered as part of HiPSTAS within listening theories in the humanities. However, this paper will also briefly mention possible ways forward including a use case in which PennSound poets and scholars use TEI speech tags for tagging tempo, rhythm, loudness, pitch, tension, and voice across PennSound poetry files in order to enable machine learning with ARLO (Adaptive Recognition with Layered Optimization) software, a machine learning application for analyzing sound on Stampede, an NSF petascale HPC system at the Texas Advanced Computing Center. Finally, we need to understand what users want to do with sound and the theories behind critical listening in the humanities before we can design distant listening tools that afford sound scholarship.  ",
       "article_title":"Developing for Distant Listening: Developing Computational Tools for Sound Analysis By Framing User Requirements within Critical Theories for Sound Studies",
       "authors":[
          {
             "given":"Tanya",
             "family":"Clement",
             "affiliation":[
                {
                   "original_name":"University of Texas at Austin, United States of America ",
                   "normalized_name":"The University of Texas at Austin",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hj54h04",
                      "GRID":"grid.89336.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "literary studies",
          "information architecture",
          "sustainability and preservation",
          "user studies / user needs",
          "repositories",
          "content analysis",
          "archives",
          "multimedia",
          "data modeling and architecture including hypothesis-driven modeling",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Letters of 1916 Project is the first crowd-sourced digital humanities project in Ireland. The project pivots around one of the most important events in twentieth-century Irish history -- the 1916 Easter Rising -- in which a small group of Irish Volunteers rebelled against the British Army on 24 April, Easter Monday 1916. The Rising was quickly quashed by the British, but the executions of its leaders several weeks later made martyrs of them, and set in motion a series of events that resulted in Irish independence from the United Kingdom in 1922. The Letters of 1916 project focuses on this tumultuous period. Its goal is to create a crowd-sourced digital collection of letters written for a six-month period before and after the Easter Rising (1 November 1915 – 31 October 1916). The project includes letters held at institutions (in Ireland and abroad), alongside those in private collections. The collection criteria is extremely broad: it includes any letter written to or from someone in Ireland, or that contains Irish subject matter.  The ultimate goal is to create a new online resource that will add a novel and heretofore underutilised viewpoint to the events of the period, a confidential and intimate perspective that will form the basis for a revision of our understanding of life in Ireland in the early 20th Century.  Letters 1916 integrates four distinct theoretical and methodological approaches:   The creation of a thematic research collection A crowd-sourced public humanities project Text analysis Social media for dissemination and outreach  This paper will explore phase I of the project: the sourcing, gathering, and transcribing/encoding of collection. It will also document how the project is being used in the classroom at the Masters level.  Phase II will transform the transcribed/encoded letters, along with their accompanying images, into a thematic research collection where users will be able to do full text searching, text analysis, view correspondences, view letters temporally and spatially, and engage with thematic exhibitions. The collection is being viewed as a major new resource and as such, will ultimately be deposited at a cultural heritage institution for long-term preservation.  In her 2009 article Rose Holley made the distinction between social engagement projects, which she defined as ‘giving the public the ability to communicate with us and each other’ via such activities as tagging, commenting, rating, and reviewing, and crowdsourcing which uses social engagement techniques to help a group of people achieve a shared, usually significant, and large goal by working collaboratively together as a group. Crowdsourcing also usually entails a greater level of effort, time and intellectual input from an individual than just socially engaging The project stops short, however, of a collective editing project. Phase 1 of the project was launched in September 2013 to a great deal of press.[1] The database was pre-populated with some 300 letters from cultural heritage institutions. To date, the project has 1500 letters in its workflow (about 800 publicly available).  The project, thus far, uses two distinct crowdsourcing methodologies according to the typology developed by Carletti, et al: a) correction and transcription; and b)  complementing collection. Building on the immense success of the Europeana World War 1 Road shows, we invited the public to bring their letters in for scanning at the launch. This model is being replicated, thus far in Ireland and the UK. Moreover, the software allows the pubic to upload their letters from home, create basic metadata, and provide the project with descriptive information.  At the time of this writing, there are approximately 30 privately-held collections (or letters from small institutions we would have never have thought to reach out to) for letters from this period. The size of individual deposits range from one letter to 90– a correspondence between the donor’s grandmother and grandfather who happened to be courting during 1916.  The public is also invited to transcribe previously uploaded letters. The response has been so overwhelming that the project team is frequently can barely keep up with demand. The engagement, encouragement, and reaching out to volunteers is a major aspect of the project to be discussed. At present the site does not utilise some of the more traditional features of crowdsourced projects (such as badges and icons) to engage its volunteers. However, other methods have been implemented, such as a volunteer forum, featuring contributors both on the project site and in the more traditional forms of media.  The project has benefited immensely from previous crowdsourced projects, from its methodological approaches, to its workflow and software. The project utilises WordPress for its front end, and for its content management system a version of Omeka that was modified by University of Iowa libraries (DIY History) to support crowdsourcing projects. It utilises George Mason’s Scripto tool for transcribing, and the Transcribing Bentham’s TEI/XML toolbar for light encoding. In a domain space that has constantly called for the creation of tools, the ability to fairly rapidly and without a great deal of bespoke programming to string together these diverse tools into one unified web presence, may signal a golden age of tool development for our field.  The Transcribing Bentham toolbar has been an interesting feature which many transcribers have utilised with little instruction. The Transcribing Bentham project designed the toolbar to hide ‘much of the complexity of TEI markup from the transcriber’ (Moyle, 353). An analysis of the markup used in this environment reveals an especially intuitive use of tagging, something the TEI is rarely praised for. Despite the caveat that the toolbar only allows the encoding of 15 out of the TEI’s hundreds of tags, the ease in which individuals with absolutely no experience in textual editing, AML,  or text encoding have been utilizing it may point to new ways to create a more intuitive encoding interface. A downside to the reduced set of tags available is that users, in trying to make sense of the tags available to them, use certain tags, such as <lb> (line break), so ubiquitously that it almost amounts to tag abuse.  This project is pitting, in many respects, methods typical of thematic research collections-- including the creation of a carefully curated dataset with fully transcribed and proofed documents, and hand-crafted markup and metadata created in accordance with the Text Encoding Initiative Guidelines-- against the less painstaking creation of a full text dataset amenable to text analysis and visualisation as a method of discovery and analysis. Ultimately, in Phase II of the project, we intend to offer readers both views – the traditional reading environment of the thematic research collection along with the analytics available via text analysis and visualisations. But the decisions we make at this stage which will be detailed in this talk will impact what is possible in later phases.   The social media dimension of finding and engaging  our audience demonstrates just how significantly the environment in which digital humanists operate has changed over the past decade. An analysis of the first month’s tweets reveal an ever expanding network of cultural institutions, individuals, organisations, and traditional media who have passed the message of the project along. A twitter-feed is a feature of the project’s home page and discussions amongst the project team reveal a shifting notion of how to position the project in a web of similar initiatives in Ireland coming broadly under the rubric of The Decade of Commemoration.[2] The project has already been positioned not as a silo, but as an aggregator embracing multiple communities of holders of these precious objects. We share back with individuals and institutions photographs we take, and will eventually share the fully transcribed/encoded texts files. But as we near the centenary of the Easter Rising, we are aware of our responsibility to, one the one hand, maintain the integrity of the project while, on the other, opening our resource more fully to sister initiatives, while respecting copyright and intellectual property promises to our donators.  ",
       "article_title":"Revisionism as Outreach: The Letters of 1916 Project",
       "authors":[
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":"Trinity College Dublin ",
                   "normalized_name":"Trinity College Dublin",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/02tyrky19",
                      "GRID":"grid.8217.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "historical studies",
          "xml",
          "digital humanities - pedagogy and curriculum"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Writing Longevity  This paper is a return to an open argument: it addresses a critical issue that, as of yet, remains unanswered. Through direct engagement with notable digital writers, we explore issues of reusability and obsolescence in electronic literature. A historical account suggests that closed formats seemingly dominate the elec- tronic literary landscape, contrary to the open culture which writers, publishers and scholars working within this field tend to promote. In order to investigate this matter, we focus our attention on two of the field’s most prominent an- thologies, the Electronic Literature Collection, Volume 1 (2006)[hay, 2006] and Volume 2 (2011)[bor, 2011], curated by the Electronic Literature Organization. Of the 62 works anthologised in Volume 1, an assessment of the 25 pieces con- sidered poems shows that only five can be thought of as open. By “open”, we mean that readers can reasonably access the underlying code. By “open” we do not mean to refer to open access publishing. Our paper is about reusability and access at the level of content creation not consumption, open-source not open access. Thus, in the earlier collection, approximately 20% use an open format. In Volume 2, over half the works use closed formats. By availing of closed-source approaches to publication, the reusability of digital literature is sacrificed, and the works remain at the risk of obsolescence. Furthermore, the open ethos of digital culture is neglected. Non-digital poetry is presented in an open format. The codex is open, developed heuristically over centuries courtesy of a speculative problem-solving feedback loop between author and publisher. The transcribed word has escaped from the monk’s cell. The only time that language is obfuscated is when the addresser wishes to keep the contents of a communication understandable or decodable by a select few. Contrary to the common view that digital art is more open than its predecessors, electronic lit- erature is actually at variance with centuries of open book culture. We are not concerned with commercial or proprietary considerations, but with the notion of “openness”. We do not deny the disseminative qualities of electronic platforms, but focus solely on the openness of the cultural apparatus. Our purpose is not to criticise the ELO or its authors, but to determine why they chose closed platforms, and if issues surrounding reusability, digital preservation, produc- tion and maintenance costs were factored into their creative decisions. To their credit, the ELO and the field’s leading scholars recognised these issues from creative literary practices: “Electronic literature doesn’t come on bound, offset-printed pages. Keeping it on a shelf doesn’t mean that it will be easy, or even possible, to read it in the future. Even putting it into a vault with controlled temperature, light, and humidity won’t ensure its availability.”[Montfort and Wardrip-Fruin, 2004] In recommending approaches to the preservation of electronic literature, Montfort and Wardrip-Fruin encourage authors to avail of open standards: “Those who use open systems and adhere to open standards when creating electronic liter- ature have a much better chance that the format of their literary works will be supported, or decipherable, in the future.”[Montfort and Wardrip-Fruin, 2004] Their warning to authors is based on the reality that closed systems and un- known specifications “are far more difficult to migrate and emulate”, and that such systems are typically controlled by small groups which “may lose interest” or change a “standard without warning, so that older works of electronic lit- erature no longer work on new platforms.”[Montfort and Wardrip-Fruin, 2004] However, Montfort and Wardrip-Fruin also acknowledge that authors do base their selections on artistic considerations: “A closed system may provide im- portant capabilities that are otherwise not available, and some closed systems may be very well suited for the type of literary creation in which authors are interested, so there may be good reasons for authors to use a particu- lar closed system.”[Montfort and Wardrip-Fruin, 2004] Authors doing so must be conscious, they argue, that “such a choice could affect the longevity of their works”[Montfort and Wardrip-Fruin, 2004].Montfort and Wardrip-Fruin’s “Acid-Free Bits: Recommendations for Long Lasting Electronic Literature” is the seminal account of how digital literature should be developed with the threat of obsolescence in mind. In this paper, we hope to build on their work, identify- ing if authors are indeed mindful of such recommendations, and what precisely influences their decisions when it comes to choosing a platform.  2 Methodology  We surveyed contributors from both volumes of the Electronic Literature Col- lection since this provided a list of authors whose work is considered, by the field’s most respected body, of a standard suited to publication under the man- tle of “digital literature”. This allowed us to avail of convenience sampling. We developed a brief questionnaire comprised of open-ended questions, allowing re- spondents the freedom to provide answers that were not shaped or guided by our assumptions. The questions were as follows: 1. When creating those poem(s) included in the ELO Collection, what were your reasons for choosing the tech- nologies that you did? 2. As a writer/artist working with digital media, do you take into account issues relating to reusability? 3. As a writer/artist working with digital media, do you take into account issues relating to obsolescence? 4. As a writer/artist working with digital media, do you take into account issues relating to production and maintenance costs? 5. On the subject of technologies (software and/or hardware) being adopted by digital writers/artists, is there anything else that you would like to add? 6. If you are interested in engaging with us further in relation to your work as a digital author, please pro- vide your name and preferred contact details. You can be assured of anonymity unless otherwise agreed. The research question did not require a measurement or comparison of groups, rather, it sought to elicit the technological motiva- tions of authors. A pluralistic approach to qualitative analysis was used as we were reluctant to be constricted by any one method, potentially missing the importance of certain passages. Repeated readings of the data, combined with a comparison of coding, meant that a more complete understanding could be achieved  3 Findings  A number of thematic axes emerge from our interpretation. One corresponds to author notions about the perceived fragility or stability of digital platforms. Transience and fluidity have clearly come to be associated with digital modes of encapsulation. This is arguably reflected in the subject matter which many of these authors treat; the fragility of memory. As evidenced in our data, associa- tions are made between the ease with which an artefact can be replicated, and the ease with which it might achieve permanence. Authors view the fluidity of digital media as opposed to the fixity of print. This runs counter to the true na- ture of some digital media, particularly closed standards. It is interesting that some authors make what is, particularly in relation to the standards they adopt, something of a false association. It may be a product of authors falling victim to the hyperbolic language that surrounds digital platforms, or, this theme may have emerged because authors focus on the text, rather than the underlying electronic systems. This was supported by explicit responses: “I have always tried to [take into account issues relating to obsolescence]. And yet most of my works are dead now. I think that is the nature of the beast...Now I do only work in HTML/ASCII text so it can be resurrected more easily.” To further test this hypothesis, this paper will present findings as to how digital authors view their work, and whether or not they make a distinction between the literary and paratextual elements of their pieces. Doing so is significant in discerning if author expectations match reality. Another theme to emerge is that of collabo- ration, which has become an important part of the literary process, particularly in relation to electronic works. This can dictate the material aspects of a piece, as authors are restricted to using familiar technologies. Furthermore, we suggest that a lack of reusability – in essence, simple building blocks – may be the cause of this reliance on collaboration. Respondents tended to cite personal reusabil- ity in favour of more communal approaches: several authors revealed that they have their own library of reusable components. Restrictions and constraints that are a product of technical expertise are somewhat unique to this medium, as writers are required to work with, rather than for, the medium. Authors of non-digital literature send their manuscripts to publishers, who in turn produce a book – the author does not necessarily know theworkings of the production process. Literary production on digital platforms requires that authors possess at least an understanding of computing sufficient for communication with their technical collaborators. We are going to present the findings of our interactions with the authors rather than our perspectives – therein lies the novelty.  4 Pragmatism & Virtue  Software development itself can be viewed ethically or pragmatically. That is, some would declare that software development done in the open is measurably better in a practical sense. Others would hold that it is ideally better in the moral sense. Electronic literature may use open or closed formats for distribu- tion. The classic modern examples of this are Web technologies versus products like Adobe Flash. The fact remains that authors may still choose to distribute the source-code for their creations even if the packaging is closed. Furthermore, just because the packaging of the Web is open does not mean that the author has made any concessions to reuse and reusability. Few authors here have placed their materials and code into online repositories and/or code versioning systems. We take both approaches to our assessment. Pragmatically, successful digital phenomena like the Internet, built on open protocols, and Android, built on open source, indicate that this model serves authors and innovators best. Eth- ically, we take virtue theory and apply it to epistemology: considering the act of sharing from a value-neutral position, the virtue (the mean) is openness, the lack of which is secrecy, with “promiscuity” being excess. To contextualise this position, consider non-digital literature, and the level to which its constituent parts are invisible to its readers. We will present our findings to the Digital Humanities community, entering into discussion on the construction of digital literature as typified by the ELO’s contributors, addressing the reusability of such from pragmatic, ethical and literary perspectives.   ",
       "article_title":"On Reusability and Electronic Literature",
       "authors":[
          {
             "given":"Anthony",
             "family":"Durity",
             "affiliation":[
                {
                   "original_name":"University College Cork, Ireland ",
                   "normalized_name":"University College Cork",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/03265fv13",
                      "GRID":"grid.7872.a"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"O'Sullivan",
             "affiliation":[
                {
                   "original_name":"University College Cork, Ireland ",
                   "normalized_name":"University College Cork",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/03265fv13",
                      "GRID":"grid.7872.a"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "publishing and delivery systems",
          "creative and performing arts",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The human capacity to visually discern and detect information is remarkably powerful. This ability forms the basis for the field of paleography, specifically in terms of text identification. However, while traditional paleographical methods yield impressive results for data characterization, a mathematical approach to identification is desirable as a means of additional verification and as a basis for comparison between varying proposals of text identities, especially in the case of damaged manuscripts in which text is wholly or partially illegible. Executed judiciously, mathematical approaches can aid the transcription efforts for manuscripts and encourage discussion of text identity with appeal to statistical distribution, based on both v!isual and contextual information. !  Derek Walvoord initially employed such an approach for the Archimedes’ Palimpsest in 2004.1 He developed a correlation recognition program and probabilistic network system for data analysis that utilized both visual and contextual information in order to create character identity distributions for the data of interest. I am currently working to adapt and improve his approach in order to analyze image data for the Vercelli Book. The oldest existing record of Anglo-Saxon poetry and homilies, this 10th-century manuscript was imaged in March 2013 by a team from the Lazarus Project at the Archivio Capitolare in Vercelli. Led by Dr. Gregory Heyworth, the team was composed of Dr. Roger Easton, Kenneth Boydston and Dr. Keith Knox, accompanied by several students from the University of Mississippi, including myself. Utilizing reflective and transmissive LED arrays in 12 wavebands ranging over the near-ultraviolet, visible, and near- infrared regions, we imaged the manuscript with the aim of penetrating the chemical reagent a!pplied in the 19th century that obscures sections of the text. !  In order to determine the identities of characters and character fragments which image rendering do not conclusively reveal, I am designing a neural network system which will make use of correlation filtering to classify the data, similar to Walvoord’s system; however, by utilizing subsampling of the initial and subsequent output layers, I will be able to meaningfully analyze fine details of the data in a way that Walvoord’s system did not allow. Moreover, my updated system will be able to weight the importance of particular features in determining class membership via a learning architecture, such that each training image improves the classifier function. This method should overcome problems traditionally associated with correlation filtering methods such as blur, lighting differences, and lack of contrast because of its subsampling and weighting mechanisms. Depending upon the results of this neural network approach of classification for the Vercelli Book data, I will evaluate whether employing a probabilistic network system similar to that used for the Archimedes’ Palimpsest will significantly i!mprove overall data classification and proceed with research accordingly.!  This system would serve to bridge the gap between our knowledge of the Vercelli Book’s contents and the information available to us, providing a probabilistically accurate transcription of data associated with a piece of the humanities corpus that has been obscured from view for over a century. This invaluable data will contribute place and context to our current conversations considering Anglo Saxon literature, language, and culture, as well as manuscripts in general. Moreover, this system will ultimately serve to empower paleographers and computer scientists alike by exploring the relationship between the visual acuity and classification knowledge associated with traditional transcription efforts and neural network-based approaches for mathematical ones.   ",
       "article_title":"From the Archimedes’ Palimpsest to the Vercelli Book: Dual Correlation Pattern Recognition and Probabilistic Network Approaches to Paleography in Damaged Manuscripts",
       "authors":[
          {
             "given":"Eleanor Chamberlain ",
             "family":"Anthony",
             "affiliation":[
                {
                   "original_name":"University of Mississippi, United States of America ",
                   "normalized_name":"University of Mississippi",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02teq1165",
                      "GRID":"grid.251313.7"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "text analysis",
          "medieval studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Overview The primary aim of the project “Digital Linguistic Archive of the Dutch East India Company (VOC)” is to bring together in one online platform all Dutch documents from 1600-1825, which were written either by VOC employees or under the auspices of the Company, and relate to the languages of the “newly discovered” territories, mainly South East Asia and Africa. The secondary, yet more ambitious goal is to create an online platform enabling researchers to contribute and exchange knowledge about the source documents, possibly including social editions of some texts. Such collaboration would bring together researchers from different countries and languages, and result in opening access to a vast range of unpublished linguistic material.   2. Source material  The archives of the Dutch East India Company (VOC) are spread between The Hague, Jakarta, Kaapstad, Colombo, Madras and London. Their historical and cultural significance was recognized by UNESCO in 2003, when they were added to UNESCO's Memory of the World Register, a 'World Heritage List' for the preservation of valuable archives and library collections. Considering the size of the VOC archives – comprising 34 million pages in total, including 1.4 kilometers of shelf material in the Hague and 2.5 km in Jakarta – one can only be disappointed at the scarcity of available documents relating to the languages of the Dutch maritime empire. However, a closer inspection of various independent library records yields surprising finds: some of these Dutch manuscripts and printed books may be found scattered in private and public collections from Paris, to London, to Venice, to Sydney. Up to this point, only a few isolated studies on these manuscripts have been available. An online database detailing all bibliographical information available would help unravel the documents’ provenance,  itineraries, and interconnections.  So far, these documents have never been assembled and arranged into one comprehensive collection. Compiling such an inventory would be highly desirable because it would allow a better evaluation of the scope and content of Dutch colonial linguistic heritage. Last but not least, online publishing of digital editions would open access to a vast range of previously unpublished linguistic material.  3. The need for collaboration One of the most significant challenges to face in the modeling of this project is the potential multitude of languages and scripts involved. Although the language of the main body of texts is predominantly Dutch, the grammars and vocabularies, by their very nature, each contain at least one other “exotic” language, often written in the native, non-Latin script. Inasmuch as one part of the multi-layered text may be accessible to a particular researcher, chances are that the remaining levels require further work to make them readily understandable. This is perhaps the most compelling characteristic of the archival material of this type: it lends itself perfectly to a collaborative, crowdsourced (or rather: community-sourced) online undertaking. The tasks may include: adding new documents, transcribing or translating existing documents, proofreading and validating transcriptions, making annotations, and adding references and annotations. Let’s demonstrate it on a real-life example: a newly discovered and digitised 17th century treatise on Tamil letters, recently made available online by the Utrecht University Library (Ms. 1479):     Fig. 1: A newly digitized manuscript: user story  A famous researcher on Tamil would be quite interested in studying it, but she does not know Dutch, or maybe is not familiar with the 17th century Dutch paleography. However, a paleography student may find it motivating and worthwhile to practice his skills on a real-life manuscript; a retired English teacher from Holland may then contribute the English translation. Once the text is rendered into English, it opens new possibilities for a wide community of non-Western scholars who may be interested in early descriptions of their native languages. They, in turn, can contribute their transcription of parts written in non-Latin scripts, as well as annotations regarding the linguistic content of the documents.  3. Content organisation The sources will be indexed by title, author, language, printer / publisher (where applicable), and the relevant linguistic category. This will enable a dynamic visualization of interrelations between any selected two criteria by means of relationship graphs, to help users understand the networks and collaboration patterns. Additional tools, such as geo-referencing, annotations, etc. can also be developed. For the purposes of paleographic research and comparison, a sample of the handwriting from the text would also be provided.  4. Challenges  However, before any IT solutions can be developed, other key issues will have to be addressed, notably in relation to copyrights and intellectual property rights. How can the existing digitized object from different libraries be brought together? How to ensure access to documents behind a paywall? Could the added value of knowledge and online traffic act as trade-off for libraries? The other methodological questions will concern ways of organising and managing the crowdsourcing community based on previous experience from comparable projects. The issues of authority, access and quality control will have to be addressed in order to ensure adherence to rigorous scholarship standards.  ",
       "article_title":"Digital Linguistic Archive of the Dutch East India Company (VOC): Modeling a community-sourcing platform for historical linguistic research",
       "authors":[
          {
             "given":"Anna",
             "family":"Pytlowany",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, The Netherlands",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "asian studies",
          "and discovery",
          "resource creation",
          "sustainability and preservation",
          "repositories",
          "linguistics",
          "archives",
          "digitisation"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  It has long been argued that Brian O’Nolan, operating under the pseudonym of Flann O’Brien, is a disciple of James Joyce. This paper examines the stylometric similarities between the two authors, particularly in relation to At Swim-Two-Birds and, to a lesser extent, The Hard Life, which we demonstrate are stylistically the most Joycean novels from O’Brien’s oeuvre. Emerging from a wider analysis of modernist writers (O’Sullivan, 2014), we will outline the results of a series of quantitative enquiries focused specifically on Joyce and O’Brien, before offering a number of literary interpretations.  rien’s At Swim-Two-Birds, despite considerable critical acclaim, was initially ill-received as a product of its “Joycean undertones”, commentators “tend[ing] to condemn the work as inferior imitation” (Hopper, 1995: 46). Seán Ó Faoláin remarked that the novel had “a general odour of spilt Joyce all over it”, while the New Statesman branded it as “dull” on account of its “long passages in imitation of the Joycean parody” (ibid.). Asbee, while critical of the The Hard Life, accepts that some comparisons can be drawn between it and Joyce’s collection of short stories, Dubliners (Asbee, 2001).  The relationship between O’Brien and Joyce remains a concern for scholars. Hopper argues that “O’Brien is usually lumped in with Joyce” as a result of “their historical and cultural proximity”, but that this is “an assumption which is unfair to both writers” (Hopper, 1995: 14). Stylistically, O’Brien’s novels are littered with parodic tributes to Joyce (O’Grady, 1989). Indeed, while O’Brien demonstrated “repeated efforts to escape his influence” (Dotterer, 2004: 59), “At Swim had everything in the world to do with James Joyce” (Taaffe, 2004: 253). Some critics maintain that the “omnipresence of Joyce ... was to be expected” on account of O’Nolan’s shared affiliation with University College Dublin (ibid.: 249). While Joyce may have been a “talismanic figure” at UCD (ibid.: 249), O’Brien’s Joycean parodies are not always interpreted as positive. Taaffe suggests that O’Brien’s “attitude towards the elder writer ... is equivocal, at the very least” (ibid.: 253), while McMullen argues that “At Swim-Two-Birds enters into dialogue not with James Joyce alone” (McMullen, 1993: 63). Dotterer aptly summarises this debate: “Critical comparison with Joyce has been frequent, as have analytical comparisons of their fiction, but less often has an awareness of this link to Joyce been seen as central and persistent in Brian O’Nolan’s formation of his own work. This link with James Joyce was one O’Nolan embraced, at times begrudgingly or unwillingly, but always out of some inner artistic and psychic necessity” (Dotterer, 2004: 54).  By offering a fresh appraisal based on quantitative methods, this paper identifies the specific points at which O’Brien’s Joycean parodies are most prominent, so that literary interpretations can be focused, with computational precision, on the relevant passages.  Methodology & Results  A number of multivariate stylometric methods were used in this study. Cluster Analysis provided a preliminary insight into the dataset, identifying main groupings. Since Cluster Analysis is very sensitive to the number of features (most frequent words) analysed, the next step involved generating Bootstrap Consensus Trees, or dendrograms averaging numerous single Cluster Analysis trees. We measured the 100 most frequent words, expanding this range from 100 to 1000 in intervals of 100 in order to produce a number of virtual dendrograms combined into one consensus plot. The distance measure in all the tests was derived from Burrows’s Delta (Burrows, 2002; Hoover, 2004). Finally, to identify (possible) peculiarities in sequential development of the analyzed texts, we used Rolling Delta (Rybicki et al., 2013), which forms an authorial signature based on one set of texts, and then applies that fingerprint to another text. Authorial signatures are then plotted over the text in question, with stylistic similarity indicated through proximity to the baseline. The aforementioned methods were applied using the R package “stylo” (Eder et al., 2013).  Initially, a cluster analysis was generated using a selection of English language Irish modernists. Using the 100 most frequent words, with 100% culling, it was interesting that O’Brien’s At Swim-Two-Birds clustered with Joyce’s texts:    This prompted further exploration, so the Bootstrap Consensus Tree, a more robust measure of style, was conducted. As can be seen, O’Brien’s novels continued to cluster with Joyce:    With our cluster analysis and bootstrapping confirming the common belief that O’Brien’s style was strongly influenced by Joyce, we adopted Rolling Delta (Rybicki et al., 2013) as a means of pinpointing specific passages of interest within the relevant corpora. The most significant findings are as follows:  Rolling Delta analysis, Ulysses:    Rolling Delta analysis, A Portrait of the Artist as a Young Man     Rolling Delta analysis, Dubliners:     As evidenced above, there are a number of places in these texts where O’Brien’s authorial signature is particularly clear. Thus, we can identify these sections as areas of a distinct crossover between the style of the two authors. O’Brien-like idiom of At Swim-Two-Birds emerges, quite strongly, in two sections of Ulysses, and in several sections of A Portrait of the Artist as a Young Man. Interestingly, The Hard Life is stylometrically similar to Dubliners throughout, consistently more so than any of Joyce’s other texts. Specific locations within the texts were identified using the following command in BASH:  awk '{for(i=N;i<M;i++) print $i}' RS= ulysses.txt  which prints everything between the Nth and Mth word in the file.  Literary Interpretations  These results contribute significantly to scholarship surrounding Joyce and O’Brien in that they offer a clear picture of where the style of both authors are most similar. Below, we will give specific focus to correlations between At Swim-Two-Birds, Ulysses and A Portrait, as well as The Hard Life and Dubliners.   At Swim-Two-Birds  Our Rolling Delta analysis demonstrates significant similarities between the style of At Swim-Two-Birds and the “Oxen of the Sun” and “Eumeaus” episodes in Ulysses. Interestingly, “Oxen” and “Eumeaus” are stylistically distinct in that they both offer parodies based on language: in “Oxen” the parody is centred around various literary figures, in “Eumeaus”, the focus is on the bourgeois. Incidentally, “Oxen” and “Eumeaus” are among the few episodes in which Stephen and Bloom appear together. Thus, two interpretations present themselves: firstly, that the results of our analysis can be attributed to O’Brien’s imitation of the Joycean parody, of which “Oxen” and “Eumeaus” are archetypal. Joyce’s exaggerated style in “Oxen” parodies the chronological progression of the English literary canon from Early English to Twentieth Century slang. Very much a Menippean satire, At Swim is intensely parodical, and like “Oxen”, draws upon a wide range of sources from “high” modernist works to correspondence with a horse racing pundit.  Alternatively, the presence of Stephen and Bloom may be accountable for the results, the product of their distinct correlation with the At Swim characters. “Oxen of the Sun” is the first episode in which Stephen and Bloom appear together, while they are also both present in “Eumaeus”. O’Brien’s unnamed protagonist in At Swim has long been considered a revival of Joyce’s artist, personified in the figure of Stephen Dedalus, hence possibly Stephen’s presence in these passages is a key. However, in both episodes, Bloom’s consciousness seems more prominent, while the earlier episodes, where Stephen features more heavily, show little proximity to O’Brien’s style. We could conclude from this that connections between the young artists in At Swim and Ulysses are more symbolic than stylistic. An exception to this finding potentially exists in the Portrait, where the style of At Swim-Two-Birds is very similar to the final sections of Joyce’s first novel, which are dominated by a maturing Stephen who appears more assured in his positions and moral development. Much has been said on the nature of Stephen’s progression from A Portrait to Ulysses; our findings would suggest that O’Brien’s student has more in common with the Stephen who is looking to “fly by those nets” (Joyce: 231) than with the Stephen we encounter in Joyce’s epic. A triad of stylometric connections emerges at this juncture. Firstly, in “Eumeaus” Joyce presents a parody of bourgeois attempts at sounding cultured. Besides, a stylistic similarity may be connected with the ironic distance with which Joyce writes Stephen in later parts of the Portrait and “Eumeaus”. Thus, from the perspective of style, we can conclude that O’Brien offers a similar treatment of the bourgeoisie in At Swim-Two-Birds. Another interpretive possibility is connected with W.B. Murphy and Skin-the-Goat Fitzharris, two storytellers in “Eumaeus”, weaving fantastic tales in rambling style, which may find parallels in At Swim. In fact, stylistic similarities between these Ulyssean episodes and O’Brien’s novel may be due to their polyphonic (in the Bakhtinian sense) texture rather than affinities between styles of particular heroes. It is hoped that a more detailed stylometric positioning of similar passages, combined with their close reading, will verify the above hypotheses.   The Hard Life  O’Brien’s tendency to present an archetypal Dublin dialect across many of his novels is another possible explanation for his close proximity to Joyce’s style. Clune argues that it was O’Brien’s Ulster Irish that “sharpened his ear for Dublin dialect” and let him “capture the precise nuances of Dublin speech. He himself claimed that Joyce had the edge on him in this, but there are those who disagree, who argue that only a non-Dubliner could have ‘caught’ his Dubliners so precisely, pinning them down ‘phrase by phrase’ as he put it himself” (Clune, 1986: 6). Indeed, “Dublin dialogue has a special relish for Brian O’Nolan”, and he praised Joyce for the “supernatural skill” which he wrote such (Mays, 1974: 246). Thus, it is perhaps unsurprising that both writers’ affection for Dublin dialect results in their styles being so similar.  While most of O’Brien novels were centred around Dublin, it is The Hard Life which is closest to Dubliners. Published 47 years after Joyce’s collection, the proximity of O’Brien’s style to that of Dubliners demonstrates that O’Brien, though not a Dubliner himself, mastered a style long dominated by Joyce. This is counter to much of the novel’s criticism, which accuses O’Brien of being the overt protégé, too conscious in his attempts at achieving the ideal Joycean parody. Asbee suggests that comparisons between The Hard Life and Joyce’s work is “almost insulting” (Asbee, 2001). While O’Brien may be charged with repeated imitation of Joyce, our analysis illustrates, and this paper will discuss in the context of stylometry, why, in some instances, he cannot be dismissed as having failed in his attempts.   ",
       "article_title":"Two Irish Birds: A Stylometric Analysis of James Joyce and Flann O’Brien",
       "authors":[
          {
             "given":"James",
             "family":"O'Sullivan",
             "affiliation":[
                {
                   "original_name":"University College Cork, Ireland",
                   "normalized_name":"University College Cork",
                   "country":"Ireland",
                   "identifiers":{
                      "ror":"https://ror.org/03265fv13",
                      "GRID":"grid.7872.a"
                   }
                }
             ]
          },
          {
             "given":"Katarzyna",
             "family":"Bazarnik",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          },
          {
             "given":"Maciej",
             "family":"Eder",
             "affiliation":[
                {
                   "original_name":"Pedagogical University of Krakow ",
                   "normalized_name":"Pedagogical University of Kraków",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/030mz2444",
                      "GRID":"grid.412464.1"
                   }
                }
             ]
          },
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":"Jagiellonian University, Krakow",
                   "normalized_name":"Jagiellonian University",
                   "country":"Poland",
                   "identifiers":{
                      "ror":"https://ror.org/03bqmcz70",
                      "GRID":"grid.5522.0"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "stylistics and stylometry",
          "literary studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction CLARIN is the short name for the Common Language Resources and Technology Infrastructure. It aims at providing easy and sustainable access for scholars in the Humanities and Social Sciences (HSS) to digital language data and advanced tools to discover, explore, exploit, annotate, analyse or combine them, independent of where they are located. CLARIN is one of the research infrastructures that were selected for the European Research Infrastructures Roadmap by ESFRI, the European Strategy Forum on Research Infrastructures. The CLARIN Governance and Coordination body at the European level is CLARIN ERIC. An ERIC is a new type of international legal entity, established by the European Commission in 2009. Its members are governments or intergovernmental organisations.  CLARIN is in the process of building a networked federation of European data repositories, service centres and centres of expertise, with single sign-on access for all members of the academic community in all participating countries. Tools and data from different centres will be interoperable, so that data collections can be combined and tools from different sources can be chained to perform complex operations to support researchers in their work. The CLARIN infrastructure is still under construction, but a number of participating centres are already offering access services to data, tools and expertise. The purpose of the present paper is to give an overview of language resources, tools, and services that CLARIN presently offers.   2. Reference Data Sets The federation of CLARIN centers offers a high number of reference data sets that are well-known and widely used in the scientific community. The CLARIN Center in Vienna offers the Austrian Academy Corpus, a very large collection of German texts and German literature covering the period of 1848 to 1989. The German reference corpus DeReKo, the  largest linguistically motivated collection of contemporary German texts with more than 4.0 billion word tokens, is hosted by the CLARIN Center in Mannheim. The CLARIN Center in Berlin provides access to the German Text Archive, a digital collection of German-language printed works from around 1650 to 1900 as full text and as digital facsimile. The CLARIN Center in Sofia offers the Bulgarian Reference Corpus. CLARIN Center in Warsaw hosts the National Corpus of Polish, a reference corpus with more than fifteen hundred million words.  CLARIN centers offer extensive collections of spoken language. The CLARIN Center in Amsterdam is home to thousands of hours of audio material for Dutch, including more than 1000 hours of dialect recordings. The CLARIN Center in Munich specializes in digital corpora for contemporary German. The CLARIN Center in Sofia offers the Bulgarian Political and Journalistic Speech corpus.  CLARIN language resources are not restricted to the languages spoken in CLARIN member countries. The CLARIN Center in Nijmegen offers easy access to the DOBES Archive, which documents endangered languages around the world.  Another key language resource are high-quality lexica. The CLARIN Center in Tartu provides on-line access to a variety of lexica for Estonian http://www.keeleveeb.ee/. The CLARIN center in Berlin is home to the Digitale Wörterbuch der deutschen Sprache (DWDS). The DWDS lexicon uses extensive digital corpus collections to document the actual usage of German words and offers on-line access to all materials at www.dwds.de/. Apart from traditional lexica, CLARIN also offers access to lexical resources that model word meanings in terms of a network of lexical and conceptual relations. The CLARIN center federation currently hosts such word nets for Czech, Danish, Dutch, Estonian, Finnish, German, and Norwegian.  In addition to reference data sets, CLARIN provides access to an extensive set of metadata records. The Virtual Language Observatory (www.clarin.eu/vlo) currently contains more than 500.000 metadata records to language resources and tool. Facetted search and a visual map provide easy-to-use interfaces for HSS scholars to locate language resources and tools that match the needs in a  particular research project.   2.2. Creation of New Resources For new digital data sets, special care must be taken that such data creation efforts adhere to best practises or standards for text encoding whenever possible and follow a data management plan. HSS scholars often lack the necessary experience or access to data repositories to meet these expectations. The CLARIN-D User Guide [1] provides practical information on the use of standards for language resources and on following good practises in data creation.   3. Data Mining and Data Analysis 3.1. Query Tools and Federated Content Search Since data sets available in electronic form are typically very large, CLARIN centers support HSS scholars by providing powerful and easy-to-use query tools for many of the resources described above. Access is greatly facilitated if such query tools are realized as web applications and thus available in any web browser. Two good examples of this kind are the web application for querying the German Text Archive and the MIMORE (http://www.meertens.knaw.nl/mimore/search/tool) tool, which enables researchers to investigate morphosyntactic variation in the Dutch dialects by searching three related databases with a common on-line search engine. The search results can be visualized on geographic maps and exported for statistical analysis. In addition to query interfaces for individual resources, CLARIN offers a Federated Content Search (FCS) functionality that enables HSS scholars to construct a virtual corpus collection hosted by different CLARIN centers and to query this virtual corpus via a common search interface. Currently, nine CLARIN centers in Germany and in the Netherlands make more than 20 resources available to the linguistic researches via the common interface of the CLARIN-D Federated Content Search ( weblicht.sfs.uni-tuebingen.de/Aggregator), and this number is growing. The CLARIN Center at the University of Oslo also provides FCS functionality via the GLOSSA corpus query tool.[5]   3.2. Workflows for Data Annotations Language data that are annotated with linguistic information can be searched with high accuracy for specific data patterns. The CLARIN Centers in Oslo, Prague, Tübingen and at the Dutch Language Union offer linguistically annotated corpora, so-called treebanks, for Czech, Dutch, German, and Norwegian with accompanying query tools. If a collection of language resources does not contain sufficient linguistic information, for example if the word forms in a corpus have not been lemmatized, it is impossible to obtain meaningful word frequency distributions. Likewise, if an HSS scholar wants to search for all person names in a very large newspaper corpus in order obtain an overview of who is currently in the news, then the person names in such a corpus needs to be marked up. CLARIN offers support for HSS scholars who need to add annotations of this kind. The web application WebLicht [2], hosted by the CLARIN Center in Tübingen, is a tool-suite for automatic annotation of text corpora. Linguistic tools such as tokenizers, part of speech taggers can be combined into custom processing chains. The resulting annotations can then be visualized in an appropriate way, such as in a table or tree format. Recently the WebLicht tool suite has been extended to spoken language. This can be achieved with the integration of the WebMaus tool provided by the CLARIN center in Munich. WebMaus takes as input an audio file and its transcription  and automatically aligns the speech signal with its transcriptions. The WebLicht tool can then further annotate the transcriptions so that via the automatic alignment, a user can find the relevant portions of the speech signal for particular data patterns.   4. Data Visualization Visualization tools that render the data analysis results in an easy-to-grasp fashion are particularly important if the data sets involved a very large. While CLARIN cannot provide a comprehensive suite of eHumanities visualization tools, it can already support HSS scholars with a number of helpful applications. [3] The CLARIN Center at the University of Copenhagen has developed a visualization tool for parallel inspection of word nets. CinaViz[6] is web application provided that offers geo-visualizations for tracking city names with particular linguistic features.   5. Data Sharing and Data Archiving CLARIN also provides support for the sharing, publishing and archiving of the data sets. SimpleStore and OwnCloud solutions are available for collaborative work on the same data set. Many CLARIN data repositories offer archiving services for external resources and for finished data sets. For quality assurance, all CLARIN Centers are assessed by the CLARIN Assessment Committeee, according to strictly defined technical requirements (see: http://hdl.handle.net/1839/00-DOCS.CLARIN.EU-78) and have to obtain the Data Seal of Approval[7] for their services.   6. Conclusion Interoperability of language resources and tools in the federation of CLARIN Centers is ensured by adherence to TEI and ISO standards for text encoding, by the use of persistent identifiers as long-lasting references to digital language data as well as by the observance of common protocols: Shibboleth for user authentication and authorization, SRU/CQL for Federated Contents Search, and OAI-PMH for metadata harvesting. Here we could describe only a subset of all CLARIN resources and tools. For comprehensive and up-to-date information we refer interested readers to the CLARIN homepage: www.clarin.eu  ",
       "article_title":"CLARIN: Resources, Tools, and Services for Digital Humanities Research",
       "authors":[
          {
             "given":"Erhard",
             "family":"Hinrichs",
             "affiliation":[
                {
                   "original_name":"Eberhard Karls University Tübingen, Germany",
                   "normalized_name":"University of Tübingen",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03a1kwz48",
                      "GRID":"grid.10392.39"
                   }
                }
             ]
          },
          {
             "given":"Steven",
             "family":"Krauwer",
             "affiliation":[
                {
                   "original_name":"Utrecht University, The Netherlands ",
                   "normalized_name":"Utrecht University",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04pp8hn57",
                      "GRID":"grid.5477.1"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "lexicography",
          "visualisation",
          "corpora and corpus activities",
          "sustainability and preservation",
          "repositories",
          "digital humanities - facilities",
          "linking and annotation",
          "linguistics",
          "archives",
          "information architecture"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction In 2011, the National Library of Wales established a Research programme in Digital Collections. The research focus of the programme is to develop an understanding of use of our existing digital content, using this knowledge to identify ways that the content can be enhanced and made more valuable for use in research, teaching, or community engagement; and building projects that develop new digital content that addresses specific research or education needs, in partnership with academics and other key stakeholders.  This activity, an example of which is described below enables critical reflection on making digital objects and the impact of digital collections on the humanities. This research addresses all aspects of digital research methods in the arts and humanities, taking advantage of the convergent practices that are embedded in digital humanities to add impact and value to digital collections of Wales.   The impact of digital collections in the humanities The NLW Research Programme has carried out research into the use of digital collections, using a variety of methods, especially those included in the TIDSR (Toolkit for the Impact of Digital Resources) developed by the Oxford Internet Institute (microsites.oii.ox.ac.uk/tidsr/welcome). In 2011-12, we used these methods to carry out some analysis of the use of an NLW digital resource “Welsh Journals Online”[. The findings of this investigation were consistent with earlier investigations into the use of digital collections in the humanities. Significantly, the research carried out using this approach led to the conclusion that most studies attempt to measure the use of digital collections after they are launched. From 2012-13, the Library led a JISC-funded collaborative initiative with other archives and special collections to digitise research material about the First World War in Wales.  The result was Cymru 1914 (www.cymru1914.org), a freely accessible online resource containing 190,000 pages of archival materials (including photographs, manuscripts, artworks, and newspapers); 30 hours of audio and approx. 12 hours of audio-visual material.  Approximately 30% of the content is in the medium of Welsh.  This project has been an important opportunity to incorporate earlier findings about the factors that increase the use and impact of digital collections into each stage of the development of the project. This presentation will discuss this process, and present findings about the factors that increase the value of digital collections for scholarship, with recommendations about their implementation into the development of digital collections in the humanities.   Selection of content for digitization The primary source materials for the Welsh experience of the First World War were fragmented, frequently inaccessible and difficult to access, yet collectively they form a unique resource of vital interest to researchers, students, and the public in Wales and beyond. An extensive scoping process of the Library, Archive and Special Collections of Wales highlighted materials in Welsh collections with the greatest relevance to World War One that were suitable for digitization, based on demand for the analogue archive materials and bibliographic research to identify citations of key materials. Research themes were identified that crossed many disciplines, opening up new avenues of research and comparative history. The final refinement of the content selected was completed in consultation academics engaged in teaching and researching the First World War, assessing the content with the greatest value to future scholarship and incorporating considerations of IPR and copyright.    Ingest into an underlying technical repository The project made content available through ingest into the NLW’s Fedora-based digital repository architecture. This supports the archiving of multimedia digital content; and the further exposure process of content for harvesting and aggregation. Existing workflows were modified to allow ingest into the repository of new content types all content types created by the project: printed text; newspapers; photographs; manuscripts; audio, and moving image materials.   Interface development, including bi-lingual support Users appreciate straightforward user interfaces, so a simple bilingual interface was developed to provide material in a variety of formats and at varying levels of archival complexity, while retaining the hierarchical structures of archives that increase usability – through familiarity – of digital resources.   An interface group conducted on-going, iterative usability testing and implementation, including several user workshops: a formative evaluation exercise; an education workshop; and a participatory design workshop, organized by the Humanities Research Institute at the University of Sheffield, who are working on a project entitled Participating in Search Design: a study of George Thomason’s English Newsbooks (http://. The goal of the latter workshop was to see if the participatory design methodology could feed into development of the interface by engaging with potential end users[5]. This paper will supply data about the above activities and they will be presented with cross referral against specific user communities. As the resource is used information gathered will be used to generate user case studies.   Dissemination and stakeholder engagement Stakeholder engagement throughout the development of the project was crucial to ensuring widest use and re-use of the content, via a process of collaboration and outreach to disparate user communities, and usability testing and engagement with the digital outputs of the project. The project team worked with core communities through an iterative process of engagement and input throughout the development of the project, through the establishment of a research network of academics using the content, specifically participating in the three stakeholder workshops, and five Community engagement workshops, organised by the People's Collection Wales (http://www.peoplescollectionwales.co.uk). Post-launch user data from products such as Google Analytics also shape our findings.    Sustainability In many respects, the actions described above to promote use, uptake and embedding of the resource are the surest way to ensure sustainability: digital collections that are used will be sustained over the long-term as they become invaluable to education, research, and community building.[6] A recent report by ITHAKA for the Strategic Content Alliance, “Sustaining our Digital Future”[7] highlighted the need to make planning for sustainability a key component of the digital life cycle. The use of good practice in digitization, and the use of an open-source, scalable repository such as Fedora, is key to sustaining the digital objects, of course, but key to cultivating sustainability of our valuable digital content is to embed planning for impact into the planning and development of digital resources.  Fedora is a vital component of our long-term sustainability plans, and our institutional setting is key to this.  Providing a crucial resource for research, teaching and public engagement around the topic of the Welsh experience of the First World War will promote sustainability of the resource. A key factor in planning and designing the resource as described in this paper is to create a digital content platform that can be added to over time. We also plan to revisit the use of the resource and to use this summative evaluation as the basis for any required modifications to increase its use.    Conclusion It is increasingly obvious that factoring in end use of digital resources as broadly as possible at the outset of a digitization project is crucial: impact is a crucial component of the entire digital life cycle. The ultimate use of digital materials is a consideration that impacts decisions made at every stage of this life cycle: selection, digitization, curation, preservation, and, most importantly, sustainability over the long term. The way that digital resources are used may be unanticipated at the outset; or they may have value for different communities and disciplines than originally intended. The best resources have been developed in such a way that their use and re-use has been anticipated at the outset, and that unforeseen use is anticipated through the use of technical standards and approaches. Just as digital collections that have been developed in formats that are not “open” are far less likely to be re-used for teaching or research, if digitization is to have more impact than being a form of “digital photocopying”, the user needs to be placed at the centre of the process from the outset.  ",
       "article_title":"Building impact and value into the development of digital resources in the humanities: Rhyfel Byd 1914-1918 a’r profiad Cymreig / Welsh experience of World War One 1914-1918",
       "authors":[
          {
             "given":"Lorna",
             "family":"Hughes",
             "affiliation":[
                {
                   "original_name":"National Library of Wales, United Kingdom ",
                   "normalized_name":"National Library of Wales",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03rjyp183",
                      "GRID":"grid.421620.3"
                   }
                }
             ]
          },
          {
             "given":"Owain",
             "family":"Roberts",
             "affiliation":[
                {
                   "original_name":"National Library of Wales, United Kingdom ",
                   "normalized_name":"National Library of Wales",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03rjyp183",
                      "GRID":"grid.421620.3"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"McCann",
             "affiliation":[
                {
                   "original_name":"National Library of Wales, United Kingdom ",
                   "normalized_name":"National Library of Wales",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03rjyp183",
                      "GRID":"grid.421620.3"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "organization",
          "and discovery",
          "resource creation",
          "interface and user experience design",
          "digitisation",
          "sustainability and preservation",
          "project design",
          "repositories",
          "digital humanities - institutional support",
          "archives",
          "historical studies",
          "management"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction After the East Japan Great Earthquake, women victims have been suffering from different problems and worries: they had to care elders, raise children, and find jobs. They needed women-specific items. Administrative authorities wanted to recognize women victims’ specific problems and give them appropriate supports. However, it was difficult to grasp women victims’ requirements timely, because they were really patient and their needs were sometimes neglected under the environments and conditions that had changed from moment to moment. Because it takes a lot of labor to conduct interviews or questionnaire investigations to acquire their needs, we need a more useful and easily way to obtain women victims’ needs.  To properly detect and analyze needs of women victims, we set our research goal was a development of an advisory message board for women victims on the web. To achieve our goal, this paper aims to make clear issues for acquiring women victims’ needs and propose the frame work of an advisory message board for women victims after disasters. In our proposed message board, women victims can post their messages freely, and data mining techniques are utilized to analyze messages and detect specific needs.    2. Our Previous Work  We have already developed the prototype system for detecting time series victims’ needs changes from social media data. The target data was the social media provided by the non-profit organization that aims the March 11 earthquake and Tsunami relief. In our previous work, time series victims’ needs changes/transitions were shown as changes of topics by adopting data mining techniques (Figure 1).   Fig. 1: Example of our previous work result (小文字にすべき単語が)  In Figure 1, the horizontal line shows time, and each circle shows one requirement. Lines between circles show transitions between requirements. There were three types of needs in affected people concerning the time length. The first is basic needs that appear for a long time, the second is early stage needs that appear immediately after the earthquake, and the third is needs after time passed. Needs about relief supplies and jobs are recognized as basic needs. On the other hand, there are needs for moves to temporary houses and evacuation center improvements at an early stage. After time passed, needs have changed, for example needs for new houses, cars, and mental cares, complaints about facilities of temporary houses, fears about the uncertain future, and wishes of living with families, appear.  Our previous method is useful to visualize victims’ needs changes/transitions easily. However, the extracted needs were basically common to all. They were not special to women victims. Actually, most writers of the blog are male, and women victims tend to hesitate to publish their opinions, needs, or problems on social media.    3. Developing Framework of an Advisory Message Board for Women Victims after Disasters 3.1 Target Data As a source of data, we used the following data on the web:  Case Study Data: “The Support Women Victim Wanted! A Collection of Good Practices in Disaster Responses based on the East Japan Disaster”  This booklet collects examples of disaster response activities undertaken by various organizations in, and after, the East Japan Disaster. The data was collected by interviews and questionnaires and provided by the non-profit organization, Women's Network for East Japan Disaster(Rise Together). To design a framework of an advisory message board, it is important to understand what kind of requirements women victims have, and make clear issues for their needs acquisition.   3.2 Issues on Acquiring Women Victims’ Needs For the target data[9], we adopted the morphological analysis technique to extractkeywords for characterizing women victims’ needs. Then, we evaluated each keyword and tried to connect it to one of three types of needs extracted by our previous method (Table I). Table I. Relationships between general needs and women victims’ specific keywords    General Needs (men and women)   Women Victims Specific Needs     Basic Needs Needs for relief supplies  Needs for Jobs     Sanitary goods, Shorts, Bladder control pads, Cosmetics, Sunscreen, Babys’ diaper, Burglar alarm, etc. Day nursery, Day-care center for elders, Incubation      Early Stage Needs Needs for immediately moving to temporary Houses    Female Workers, Clear temporally toilet, Women Area, Respite days     Needs after time passed Needs for new houses, cars, mental cares Complaints about temporary houses Complaints about government Hope to live with families    Networking among women, Health consultation Women area, Gender-segregated toilets, European style toilet Female workers, Violence hotline, Translation for foreigners Support for caring elders, support handicapped people    For example, the relief supplies that women victims needed were “sanitary goods”, “shorts”, “bladder control pads”, “cosmetics”, “diaper”, “burglar alarm”, etc. As for the needs of jobs, women victims needed “day nursery”, “day-care center for elders”, and “incubation”. As for the complaints about governmental responses, women victims needed “female agents”, “a violence hotline”, and “a translation for foreigners” at the early stage. As for the complaints about temporary houses, women victims needed women areas, gender-segregated toilets, and so on. As for the needs of mental care, “networking among women”, and “health consultation” were requested as women victims’ specific needs. Women victims needs did not appear in the general social media data. We found analyzing just general social media data was not enough for acquiring women victims’ needs. Issues on acquiring women victims’ needs are as follows:  Women tend to hesitate to unfold their specific needs  Women’s needs tend to be put little emphasis on  An interview and a questionnaire take a lot of labor Because after the disaster, women become very busy, it is difficult to make a time to post messages     3.3 Framework of anA dvisory Message Board for Women Victims after Disasters  Figure 2 illustrates our proposed framework.   Fig. 2: Proposed framework: an advisory message board for women victims after disaster  We suppose that the board is organized by a responsible organization such as a public agency. In our framework, first, women victims who want to use this service, register themselves (1). Basically, it is an anonymous board but only administrator can identify them. \"Anonymity\" is the point for acquiring women victims’ needs easily.  Then they will select their needs/problems/requirements from the menu provided by the system (2, 3). The menu is hierarchically designed in advance according to our investigation. By providing problems candidates from the menu, the time for inputting messages should be decreased because they are busy. Then, solutions for solving their needs will be shown to them. If there are no corresponding problems in the menu, victims can input a text message freely. As for the free text, data mining technique will be adopted for analyzing problems, and solution candidates will be shown to them (5, 6). The menu contents will be maintained according to victims’ inputs and responses.  The points of the framework is to clarify and predict problems of women victims, and continue to be improved according to their inputs and responses for providing appropriate supports at appropriate timing.    4. Conclusion  In this paper, first, we introduced our previous work, and then made clear issues for acquiring women victims’ needs. Finally we proposed the frame work of an advisory message board for women victims after disaster. In our proposed message board, women victims can post their messages (requirements/opinions/complaints) freely, and data mining techniques are utilized to analyze messages and detect specific needs.  As the future work, we will develop the prototype system of the framework, and conduct the field work with non-profit organizations and actual women victims to receive feedbacks from them.   ",
       "article_title":"Framework of an Advisory Message Board for Women Victims after Disasters",
       "authors":[
          {
             "given":"Takako",
             "family":"Hashimoto",
             "affiliation":[
                {
                   "original_name":"Chiba University of Commerce, Japan",
                   "normalized_name":"Chiba University of Commerce",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02qn0vb48",
                      "GRID":"grid.443770.3"
                   }
                }
             ]
          },
          {
             "given":"Yukari",
             "family":"Shirota",
             "affiliation":[
                {
                   "original_name":"Gakushuin University, Japan",
                   "normalized_name":"Gakushuin University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/037s2db26",
                      "GRID":"grid.256169.f"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "text analysis",
          "social media"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    Fig. 1: Elizabeth Losh and members of Software Studies Initiative explore media visualization of 113 fffcvideo public addresses by U.S. President Barack Obama. Visualizations are available at http://lab.softwarestudies.com/2011/09/digging-into-global-news.html   Television news often serves as the first, most vivid draft of history and shapes the conventions of political speech and civic participation around the world, but undertaking systematic analysis of large digitized corpora of broadcast news video archives and even smaller corpora of government information videos in the public record presents a number of technical, methodological, and institutional challenges. Although a deeper understanding of how the news is represented in moving images promises to improve access to historical records, participation in public dialogue, and education at all levels, large corpora of video news programs are considerably more difficult to catalog, mine, and visualize than text news collections. Furthermore, new forms of dissemination, annotation, and commentary made possible by the Internet are rapidly transforming video news consumption on a global scale. At the same time, traditional newspapers of record are incorporating more multimedia content, so new computational methods such as video search will become increasingly necessary for researchers working with all news collections. By collaborating with archivists and computer scientists, it is possible to analyze visual rhetoric in very large video collections using media visualization  “Visualizing Global News” studies how excerpts from large collections of political speech videos with historically significant personages are remixed into even larger collections of news broadcasts. It applies new techniques in search and visualization to aid both humanities researchers and the greater public interested in exploring the visual details, aesthetic features, and narrative context of source footage that is reused in news broadcasts. The project focuses on materials from four political figures who appear in video news from 2001 to 2011 and tags and visualizes a rich set of intersecting corpora: newscasts from the US program Democracy Now!, the Qatar news channel Al Jazeera English, the top-rated Dutch news show NOS Journaal. This video corpus contains over 30,000 separate news programs, and over 2,000 videos of political speeches.  As participants in the Digital Media Analysis, Search and Management (DMASM) international workshops, we are well aware of the technical problems plaguing automatic systems and that even distinguishing foreground objects from backgrounds can be challenging. Nonetheless, the MediaMill technology has performed well in the yearly TRECVID benchmark competition, and ImagePlot can create compelling visualizations working with key frames. Our presentation aims to provide a policy overview of the opportunities and challenges of DH with global video news archives. Currently the “big data” problem of identifying heterogeneous sources in moving image archives has been largely funded by corporations interested in protecting intellectual property and state entities interested in surveillance. Without digital humanities scholars playing a larger role and generating research questions that merit publication, the most vivid aspects of the historical record are likely to remain under-theorized and analyzed without large- scale comparative study across nations and periods.     Visual rhetoric has a long tradition in the humanities that includes analysis of symbolic objects in portraits of world leaders or the choreography of their oratorical performances. However, contemporary news broadcasts present historical figures in the public record in increasingly visually complex ways that use massive collections of heterogeneous and frequently unsourced footage, motion graphics, and digital effects. In this project we use new computational and visualization techniques to foster new forms of humanities scholarship and public access to historical records. Political figures produce memoirs, letters, editorials, and other forms of written discourse, but the speeches they compose are also performed and thus can be analyzed as much more than written texts. Now that the speeches of contemporary political leaders are recorded and archived, humanists have a rich record of public rhetoric to analyze that includes facial expression, bodily gesture, vocal performance, and frequently the use of sets and props. In the era of digital video new editing and compositing techniques are also part of the “official version” of a given speech, and portions of these speeches may be further edited and composited when they appear as part of news broadcasts. Broadcasters may adopt (or reject) signature visual styles that brand their programs and even signal their political orientation, and these markers of visual rhetoric can be mapped over time or represented comparatively (Manovich, 2011b). Scholars in the fields of rhetoric, performance studies, history, journalism, film and media studies, and civic education can compare and contrast visual and verbal political messages and changes over time in large video collections.  While a transcript of a given speech may give humanities scholars information about word use and specific references to people, places, and events, much of the rhetoric of the news is actually nonverbal. For example, the use of slow motion, freeze frame, or replay techniques may change the meaning of a given rhetorical moment. Visual arguments in news programs are now often advanced by compelling editing, dazzling information graphics, or aesthetic choices that privilege aspects such as warm lighting or patriotic color schemes. At the same time, the size of digital collections of video news is growing rapidly in response to a number of trends: 1) cable television and satellite television, which spawned the twenty-four-hour news cycle, have become international phenomena, 2) television stations now create web-only content to draw Internet viewers to their programs online, 3) newspapers are incorporating multimedia content and interactive features in archives that once only indexed print news, and 4) bloggers and vloggers involved in citizen journalism have participated in a dramatic expansion of the scope and scale of independent media. The difficulty of humanities news analysis is exacerbated by the fact that a typical television news program is actually “database cinema” (Manovich 2005) composed of many different kinds of source footage, including studio shots of anchors, field reporting, tock footage, video news releases, government public relations materials, and witness journalism from cell phones and mobile devices. Source clips may not be attributed much less tagged with date, location, individuals shown, or person/organization behind the camera (Losh 2008, Gregory 2010), and the meaning of objects or gestures in the frame, as evidence of the intention or motivation of particular political actors, can be controversial (Losh 2011).  The UCSD project team created visualizations with the official digital video archive from the Obama administration at WhiteHouse.gov that focused on the collection of “Weekly Addresses” directly addressing an imagined American Internet viewer that includes extensive discussions about current events that range from repeated occurrences (economic boosterism, holiday celebrations, etc.) to one-time disasters and tragedies (the Deepwater Horizon oil spill, the shootings at Fort Hood, etc.). UCSD has also examined a significant subset of videos in the WhiteHouse.gov archive that are addressed to audiences abroad as part of U.S. public diplomacy campaigns, which are sometimes also remixed into global news broadcasts. Even a country with a relatively small population or one that is not usually considered critical to U.S. interests, such as Côte d'Ivoire, may have a designated Obama direct address. This dataset allows those studying the visual rhetoric of international relations to work with a particularly rich and dense corpus that includes acknowledgement of cultural exchanges and national holidays. For example, historians studying U.S. Iranian relations in the twenty first century could look closely at three separate annual addresses recorded by Barack Obama that were intended to go directly to the people of Iran on the occasion of Nowrūz, the Persian new year. Each address is fundamentally different in tone and diction, as the U.S.-Iranian diplomatic relationship seems to deteriorate, as well as in shot composition and White House location.  Media created by a sitting U.S. president are always historically significant, but media created by Barack Obama -- the first test case area in this project -- are especially interesting to humanities researchers. In addition to scholarship done by rhetoricians, historians, political scientists, and performance studies and communication scholars, Barack Obama has also generated significant scholarly attention worldwide from those who study American popular culture, race relations, religion, gender, class, civic participation, digital culture, and globalization. The volume of scholarly publication of peer-reviewed books and articles about Obama and the large number of conferences, panels, and talks reflect the potential importance of this collection to scholarly discourse.  The Obama official video corpus has also been significant for the international press, which often uses content from the weekly addresses and other WhiteHouse.gov speeches and public statements in news broadcasts shown worldwide. Researchers at UCSD have noted the presence of material from the official Obama corpus in global broadcasts that include shows from the BBC, PressTV in Iran, Al Arabiya in the United Arab Emirates, ABS-CBN in the Philippines, and many others. The expense of maintaining news bureaus abroad is an obvious reason to rebroadcast free HD footage, but stations often add their own visuals and editorial commentary. For example, the Indian television network NDTV cropped official U.S. government footage from “President Obama’s Statement on Credit Downgrade” and added its own corporate branding along with a skeptical digital banner that read “Obama Assures But Dow Plunges.”    Fig. 3: UCSD visualization of White House government records footage of an Obama speech to the Iranian people (top) and visualizations of how the same speech appears in Democracy Now! (middle) and Al Jazeera English (bottom). To create these visualizations, the UCSD team first used open source software to automatically detect shot boundaries in the video, and then applied media visualization tools to create a grid of images where each image is the first frame of each UCSD visualization of White House government records footage of an Obama speech to the Iranian people (top) and visualizations of how the same speech appears in Democracy Now! (middle) and Al Jazeera English (bottom). To create these visualizations, the UCSD team first used open source software to automatically detect shot boundaries in the video, and then applied media visualization tools to create a grid of images where each image is the first frame of each shot.  Working with a large set of news archives from different sources will also allow us to develop possible answers to a number of critical research questions for archivists: How can we build systems that allow users to compare historical world events from different national or political perspectives? How can we connect one nation’s collections to the collections of others, and how will those connected collections benefit users? Given the capabilities of new search tools, how can we best improve cataloging processes, which currently depend on costly manual input, to make collections broadly usable and accessible? How can we begin to trace the re-use and re-contextualization of primary materials, in particular, news materials?   Fig. 4: President Obama's 2009 Nowruz address to the Iranian people as excerpted by the TV news program Democracy Now! Each frame represents one shot of the program. The frames are arranged in the order of shots (left to right, top to bottom). Shot 19 (third column, third row) is the excerpt from President Obama'a video address.    Fig. 5: President Obama's 2009 Nowruz address excerpted by Al Jazeera TV broadcast. Each frame represents a sequential shot of the program. Shots 5 and 6 (first row) are excerpts from President Obama's video address.   ",
       "article_title":"Visualizing Global News",
       "authors":[
          {
             "given":"Elizabeth",
             "family":"Losh",
             "affiliation":[
                {
                   "original_name":"University of California, San Diego, United States of America",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          },
          {
             "given":"Lev",
             "family":"Manovich",
             "affiliation":[
                {
                   "original_name":"CUNY Graduate Center, New York, United States of America",
                   "normalized_name":"The Graduate Center, CUNY",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00awd9g61",
                      "GRID":"grid.253482.a"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "corpora and corpus activities",
          "cultural studies",
          "sustainability and preservation",
          "repositories",
          "multilingual / multicultural approaches",
          "archives",
          "stylistics and stylometry"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What is leishu Leishu (類書) is a unique form of Chinese source book for quick references and quotations.  The editor of a leishu would collect a large number of books, develop a knowledge structure of the intended knowledge domain (usually with a number of categories that cover the domain, each with a list of subjects), then extract texts, that the editor deemed relevant to each subject, from the content in the books.   Thus each subject has a list of entries which are texts taken from existing books.  Because the purpose was for quotation, the meanings of the subjects were not explained and the texts were quoted verbatim, usually with the source indicated.  (The lack of explanation of the subjects differentiates a leishu from an encyclopedia.)  The earliest leishu, Huanglan (皇覽), dates back to 220AD. During the past 300 years leishu had been looked down upon by many Chinese scholars.  Its compilation nature was regarded as a lack of originality.  The ease of use as quick reference was criticized as providing a short cut and thus encouraged shallowness in scholarship.  Its value in the scholarly circle had largely been limited to being a vessel from which fragments of lost books were collected. The advance of information technology allows us to look at leishu from a very different angle. The categories and subjects, together with the selections of entries, reflect how the world was perceived at the time when the leishu was compiled. Comparing two leishu from different era, thus, provides a way to observe how the world view had changed during the years between the two tomes.  This was not possible until now, when the availability of leishu in searchable full-text form finally provides a way to study and compare the books in their entirety.   In this paper we present such a study.  The leishu we have chosen are yiwenleiju – YL (藝文類聚), completed in 624AD (early Tang dynasty) by Ouyang Xun, and taipingyulan – TY (太平御覽), completed in 984AD (early Song dynasty) by Li Fang.  Both were commissioned by the emperors and were about the general knowledge of the world.  YL divides its view of the world into 46 categories, further into 734 subjects. TY has 55 categories and 5,597 subjects. It was stated in the preface of TY that YL, written 350 years earlier, was among the three main leishu consulted.  (The two other no longer exist.) With the full texts available through digitization, we can finally check effectively the inheritance relation between TY and YL. It also provide a way to observe the change of the world view between the two books, which is reflected not only by the numbers and titles of the categories, but also by the subjects that each category covers, and the entries that are listed under the subjects.   New subjects indicate the emerging importance of new concepts; the increase of entries under the same subject means more knowledge (or interest) about the subject; and the assignment of the same entry to a different subject signals the change of a viewpoint.  We will give some preliminary findings in this paper. Processing the texts and building a system for comparison The full texts in our study are obtained through Guoxuewang (www.guoxue.com). Treating an entry as a basic unit, we parsed each entry into the content and its source (the book from which the text is taken).  The source is further analyzed to identify the title of the book (sometimes with the chapter and the section), the author (if known), and the era (dynasty). An XML format is designed to associate an entry with its category, subject, source, and content. Further analysis was conducted to resolve name and author conflicts.  According to our results, YL has 14,572 entries, extracted from 5,628 sources, of which 787 are books and the rest (4,841) are individual articles such as poems, letters, and memorials.  TY, on the other hand, has 65,633 entries, extracted from 2,327 books and 1,832 other sources. Among the books 629 are cited by both. Of the remaining 1,698 books that are cited by TY but not YL, 498 are from pre-Tang era (which could have been included in YL) and 980 from era unknown (although the majority should be pre-Tang). The number of books that are certain to have been written after YL was completed is only 220.  The total numbers of words in the two books are about 900,000 and 4,000,000, respectively. We then designed an algorithm based on the longest common sequence method to check if an entry appears in both books. Modifications to LCS are necessary to deal with different styles or errors in the quoted texts. Since the entries of YL are classified into affairs (事) and literature (文) while TY is primarily about affairs, we focused initially on comparing the entries in YL about affairs. We found that among the 9,701 such entries in YL, 7,249, or 75%, appear in TY.  That means TY did indeed heavily reference the earlier book. Those entries appeared 11,022 times in TY, since an entry may appear more than once. We built a visualization system to compare the two books. An important feature is to show how many entries of a category/subject of one book are also cited in the other book and how they are distributed. We present here an example using the category fuming (divine signs, 符命) in YL. Figure 1 shows that of the 41 entries of the affairs part of fuming in YL, 27 also appeared in TY (left-most column). The middle-left column shows that these 27 entries appeared 51 times in TY.  The third column indicates that these 51 appearances belong to 40 subjects, and the right-most column further indicates that they belong to 16 categories.    Fig. 1: Correspondence of entries of fuming in YL and TY  Figure 2 is the list of categories with the number of entries that match those in fuming of YL.   Each category can be expanded to show the subjects that contain some of the related entries.   Fig. 2: Categories and subjects of TY that contain entries of fuming from YL  Figure 3 lists the entries of fuming in YL and the corresponding (similar) entries in TY.   Fig. 3: Comparisons of entries    Evolution of worldview Comparing the two leishu reveals a number of striking changes in worldview between 7th and 10th century China.  We briefly describe three of them.  (1) The disappearance of fuming – divine justification Fuming, a sign that provides divine justification to overthrow a dynasty (and to establish a new one), was an extremely important concept during Han Dynasty (206BC – 220AD).  Indeed, Fuming was included in YL as a category.  However, although the number of categories increased from 46 to 55, fuming conspicuously disappeared from TY (it did not even appear as a subject). Although many of the entries are still included in the latter book, they are listed in subjects such as an emperor or his mother and are scattered over 16 categories.  This means that the very concept of fuming became irrelevant politically at early Song.  Other subjects such as auspicious signs (祥瑞) met similar fate. Song Confusionism went through a dramatic development in mid 11th century.  The disappearance of fuming from TY seems to hint that the tide of ridding mysticism may have started a lot earlier, since TY was completed in 984AD.   (2) The rise and fall of Daoism and Buddhism In YL, entries about Buddhism were listed in the category inner canon (內典), a Buddhist-centric term; non-Buddhist religious books are outer canons.  Entries about Daoism appeared under the subject immortals (仙道) in the category supernatural (靈異).  In TY, Buddhism and Daoism are both categories, with 10 and 53 subjects each.   While the number of entries about Buddhism increased from 169 to 197, those about Daoism went from 137 to 1402.  The 53 subjects include detailed classification of garments and buildings for different religious purposes. Buddhism was introduced to China during the 1st century, and became the national religion during the 5th century.  Buddhism was the mainstream religion in early Tang, when YL was compiled.  After mid Tang, the indigenous Daoism started a revival and became an organized religion.   This trend, coupled with the anti-Buddhist attitude of the royal house and many intellectuals, weakened the dominance of Buddhism in China. This phenomenon is vividly demonstrated in our comparison.   (3) The emergence of foreign peoples Although the concept of foreign peoples (四夷) in China is as old as 1000BC, there was no category nor subject about any foreign peoples in YL.  For instance, Wuo (倭), a country probably located in modern day Kyoshu Japan, started paying tribute to China as early as late Han dynasty (around 100AD).   However, Wuo was not mentioned as a subject in YL, although appeared (3 times) in subjects describing produce such as mulberry.  TY, on the other hand, has under the category foreign peoples 388 different countries and peoples with 920 entries, which include countries as far as the Roman Empire (大秦) and as near as various indigenous (yet alien) ethnic groups within China proper.   The entries are from books spanning across 1,500 years and are mostly about foreign tributes, trades, and mythologies.   Tang was a melting pot and its capital, Changan, was a great cosmopolitan city.   The intimate and frequent interactions with foreign people apparently also contributed to the establishment of China as an individual and separate identity.   Discussion The intellectual progress of China between 7th and 10th century had been described as “mediocrity in an era of prosperity”.   In this paper we showed that the Song Confusionism revival of the 11th century might have had its way paved during this period.   Our observations are mainly through comparing two leishu, yiwenleiju completed in 624AD and taipingyulan in 984AD.  Such a study, however, would not have been possible without a systematic way to compare the knowledge structures and the contents of the two tomes.  This paper presents such a method to tackle this prohibitive task.  ",
       "article_title":"A glimpse of the change of worldview between 7th and 10th century China through two leishu",
       "authors":[
          {
             "given":"Jieh",
             "family":"Hsiang",
             "affiliation":[
                {
                   "original_name":"National Taiwan University",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"lihua",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":"National Taiwan University ",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          },
          {
             "given":"Chia-Hsuan",
             "family":"Chung",
             "affiliation":[
                {
                   "original_name":"National Taiwan University ",
                   "normalized_name":"National Taiwan University",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/05bqach95",
                      "GRID":"grid.19188.39"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Introduction Since the late 90s, a large number of digitization projects relevant to research in the humanities have been carried out. The quality of the digital objects produced by these digitization campaigns most often meets the demands of a \"digital facsimile\". Since - with the exception of text files such as PDF, plain text, etc. where a full text search may be appropriate - digital objects are hardly searchable directly, associated metadata are needed to enable navigation within a collection of digital objects. It should be expected that this simplified accessibility and availability of digitized sources has fundamentally changed research in the humanities by allowing more efficient and broader research methods. However, iseems that this is not yet the case. The reason is that there are very few digital tools available to support the qualitative and comparative methods required in source based research in the humanities. In the following, we will look at three use-cases which exemplify our vision for a research environment in the digital Humanities. Digital Humanist. Susan, a digital humanist, is working with digitized manuscripts. For her research, she needs to transcribe, annotate, and link her annotations, regions of interest, and transcriptions with each other. By employing SALSAH as her work environment, Susan can work on the digitized manuscripts in a fully digital workflow. Long-term Accessibility of Digital Research Data. Jim is at the stage of finishing up a five-year project, and needs to deposit his research results and the data accumulated during his research. The results and the digital data need t be still accessible in the long-term, even after the funding has long since ended. Jim can export his digital research data to an institution deploying SALSAH which will take care of their long-term accessibility. Linked Open DataWorkbench. Karen's research is based on materials whic are provided in different repositories around the internet. She wants to be able to combine, annotate and create links between those digital objects. Also sh would like to share her results and allow other researchers to use them. By usingSALSAH, Karen can connect to external resources shared over custom APIs or SPARQL endpoints, and work with the data as if it were stored locally. Usin the SALSAH API and the provided SPARQL endpoint, other researchers can build upon her work. SALSAH (System for Annotation and Linkage of Sources in Arts and Hu manities) version 2.0 is currently under development at the Digital Humanites Lab (DHLab) of the University of Basel, and represents a browser based VRE that will respond to requirements described in the three scenarios above. The main contribution of this paper lies in the description of novel approache taken in the design of SALSAH 2.0, leading to new features and possibilities. The remainder of this paper is organized as follows. In Section 2, we introduce newly developed features and Section 3 concludes. 2 SALSAH SALSAH integrates digital (re)sources, metadata, research data, and relevan working tools. Using SALSAH, researchers are able to: (1) simultaneously visualize multiple digital objects (e.g., facsimiles, images, texts, transcripts, sound and video), (2) annotate digital objects and share these annotations with others (3) establish relations (links) between digital objects and annotate these relations, (4) access and integrate external data sources (e.g., digital libraries) so that the VRE tools may be applied to these sources without the need for local duplicates, and (5) transcribe manuscripts, speech and video. 2.1 Software Architecture The software is based on a multi-tier architecture in which application logic is distributed between (1) a client application (\"front end\") which users interact with, (2) a more or less centralized server (\"back end\"), and (3) local and/or external data providers which provide the sources that users can work on. The SALSAH software architecture is depicted in Figure 1. While SALSAH has the capability to function as a repository for digital sources, this is not its primary goal. There are many repositories of professionally digitized sources, and it makes no sense to duplicate their content in yet another repository. Following a logical separation of annotation tools and digital representations, SALSAH provides the basis for referencing sources without having to store them itself. Furthermore, SALSAH can provide annotation, linkag information, and metadata to an external data provider via the SALSAH API (as long as the external request has access rights), as well as over a read-only SPARQL endpoint that provides LOD (Linked Open Data).We expect SALSAH in the long term to evolve into a true distributed P2P system.   Fig. 1: Software Architecture of SALSAH  2.2 Data-Model The data model is based on the Resource Description Framework (RDF), the Resource Description Framework Schema (RDFS), and the OWL 2 Web Ontology Language all proposed by the World Wide Web Consortium (W3C) for implementing the Semantic Web. This metadata model makes it possible to describe digital objects in a very flexible way, and to create links and relation between any objects (which are called \\subjects\" in RDF terminology). It is based on statements in the form of subject-predicate-object expressions about these digital subjects. Any number of such expressions can be used to describe subjects and their relations. A given set of predicates is called a vocabulary, and can be used to implement standard metadata schemes such as Dublin Core. Within SALSAH, different vocabularies may be used at the same time to describe a given subject. Since the value of an RDF expression may itself be a subject, RDF allows for a network-like representation of knowledge about a subject and its relations to other subjects. This metadata model is subject-centric, in the sense that for each digital subject, an individual set of predicates may be assigned, in contrast to the relational data model, which is much more restrictive in its ability to assign data field to subjects. Hence, the data model used in SALSAH is especially well suited to the humanities, in which a flexible, qualitative coverage of metadata is essential. Figure 2 (a) depicts an excerpt from the SALSAH ontology, showing how a projects own metadata schema can be incorporated into SALSAH, and (b) a small part of the graph depicting an incunabula of Sebastian Brant with the title \"Das Narrenschi\".   Fig. 2: An excerpt from (a) the SALSAH Ontology and (b) the incunabula of Sebastian Brant.  The data store consists of a native triple-store solution such as Jena, which serves the data over a SPARQL endpoint. 2.3 Versioning SALSAH is a dynamic system in which data can be changed by users having the necessary access rights at any time. In order to use SALSAH as a citable repository, methods will be implemented to \\freeze\" a subset of the data and thus provide versioning. In order to solve this non-trivial problem, SALSAH will use the concept of temporal RDF, in which each element in the RDF graph of a certain granularity will be enriched with temporal information regarding its validity. For example, if the title of a book is changed, the old version is not overwritten, but is instead marked as valid up to the time when the change occurred, while the new title is marked as valid from then on. This allows users to retrieve the state of the RDF graph at any point in time.  Versioning will lead to the concept of a new form of electronic publication. While e-papers and e-journals basically mimic the behavior of their paper equivalents an annotated network of citable sources and links represents a novel form of publication. The reader will be able to navigate through the network and extract his or her own perspectives on the knowledge represented by the interconnected digital objects. This may be the first attempt, within academic publishing in the humanities, to go beyond the phenomenon in which \"new media first mimic older media\", as noted by Marshall McLuhan. 2.4 Digital Long-Term Preservation DISTARNET (DISTributed ARchival NETwork) is a distributed, autonomous long-term digital preservation system. Essentially, DISTARNET exploits dedicated processes to ensure the integrity and consistency of data with a given replication degree. At the data level, DISTARNET supports complex data objects and the management of collections, annotations, and arbitrary links between digital objects. At process level, dynamic replication management, consistency checking, and automated recovery of archived digital objects is provided, using autonomic behavior governed by preservation policies without any centralized component  DISTARNET will be implemented as a layer underneath the SALSAH local repository, and provide long-term preservation of the digital objects and associated metadata. 3 Conclusion While the change from the analog to the digital domain makes sources available on the desktops of scholars and researchers, a real paradigm shift in source-based research requires new tools. Virtual Research Environments such as SALSAH may provide the necessary tools to gain a novel, computeraided knowledge rep- resentation that is well-suited to the needs of humanities research. These tools will undoubtedly change the way research is done in the humanities. They will help researchers organize and retrieve knowledge more efficiently, and may disclose hidden relationships between sources, among other things, but they will not replace the researchers' ingenuity and intuition. SALSAH is in use by several research projects within the University of Basel, and has sparked interest on an international scale.  ",
       "article_title":"Future Development of a System for Annotation and Linkage of Sources in Arts and Humanities",
       "authors":[
          {
             "given":"Ivan",
             "family":"Subotic",
             "affiliation":[
                {
                   "original_name":"University Basel, Switzerland ",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"André",
             "family":"Kilchenmann",
             "affiliation":[
                {
                   "original_name":"University Basel, Switzerland ",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Tobias",
             "family":"Schweizer",
             "affiliation":[
                {
                   "original_name":"University Basel, Switzerland ",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          },
          {
             "given":"Lukas ",
             "family":"Rosenthaler",
             "affiliation":[
                {
                   "original_name":"University Basel, Switzerland ",
                   "normalized_name":"University of Basel",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/02s6k3f65",
                      "GRID":"grid.6612.3"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "ontologies",
          "metadata",
          "visualisation",
          "software design and development",
          "internet / world wide web",
          "semantic web",
          "information architecture"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Introduction The GeoBib project is constructing a georeferenced online bibliography of early Holocaust and camp literature published between 1933 and 1949 (Entrup et al. 2013a). Our immediate objectives include identifying the texts of interest in the first place, composing abstracts for them, researching their history, and annotating relevant places and times. Relations between persons, texts, and places will be visualized using digital maps and GIS software as an integral part of the resulting GeoBib information portal.  The combination of diverse data from varying sources not only enriches our knowledge of these otherwise mostly forgotten texts; it also confronts us with vague, uncertain or even conflicting information. This situation yields challenges for all researchers involved – historians, literary scholars, geographers and computer scientists alike. While the project operates at the intersection of historical and literary studies, the involved computer scientists are in charge of providing a working environment (Entrup et al. 2013b) and processing the collected information in a way that is formalized yet capable of dealing with inevitable vagueness, uncertainty and contradictions. In this paper we focus on the problems and opportunities of encoding and processing fuzzy data.    2 The uncertainty about uncertainty: How to model and represent it The data collected in our project concerns such different entities as texts, persons and places and is compiled from different sources and different scholarly perspectives. The project is entirely interdisciplinary: besides literary scholars and historians, also geographers and computer scientists are involved. Students and researchers from literary and historical studies are our target audience for whom the resulting online platform shall provide an attractive research tool. Hence, the collected data does not only lie in the intersection of research interests of these fields, but extends to the sum of these interests. The platform is supposed to help answer questions that arise in the field of literature, e.g. finding texts concerned with certain places in a given time period, but also to support historians in finding possible eye witness reports of the crimes of Nazi Germany. Accordingly, information of various kinds is collected with the intention of supporting such diverse use cases. The different scholarly perspectives also determine the amount and kind of data we collect, and their information needs can hardly be covered by a single formalism or predefined ontology. We need a flexible yet coherent formalization that is adaptable to our objectives. Our workflow and approach to collecting data is one of divide et impera: Instead of proposing one format that does it all, we distinguish between different kinds of information depending on the entities concerned. Information collected on the authors of the holocaust texts and relevant places is stored in a user friendly MediaWiki system, while information on these texts is stored in TEI/XML files. Both systems are interconnected and geographical references are integrated as well (cf. Entrup et al. 2013b). The resulting information portal will be backed by an object-relational database.   2.1 Persons and places: Capturing FUZZY information in a Wiki System Within the field of prosopography the combination of different, possibly contradicting sources is a well-known problem. Pasin and Bradley (2013) offered insights on how such alternative views on historical events could be described using an underlying ontology. Software libraries intended to support processing of prosopographic data are also being developed (e.g. Barabucci and Zingoni, 2013). The GeoBib project collects information and short biographies of authors – a task that bears resemblance to prosopographic research. Many of those authors only published one text. Researching their personal information often leads to ambiguous results, such as different names used, differing information on birth or death dates as well as other personal data. We extended the MediaWiki system that we use to collect information on persons and places with a set of templates that help to ensure that such information is added in a coherent way, while allowing the data to be vague or apparently contradictory. The Wiki allows the editors to add uncertain information into proposed fields, so that, for instance, different names can be added to one person. Furthermore, the short biographical texts we compose for most of the authors can be used to communicate dissent between different sources.   2.2 TEI/XML: Encoding uncertainty in literary annotations  Literary texts, and as such especially autobiographies and memoires, are not collections of historical facts arranged in an exact chronology. Especially the early Holocaust texts are emotionalizing (cf. Feuchert, 2012, Hickethier 1986) and “conveying the experience made with the National Socialist terror system in a literary, or better, literarised way” (Feuchert, 2012, p. 218). But even apparently factual accounts of events carry a certain degree of vagueness, which leads both historians and literary scholars to interesting research questions, and poses a challenge for data modeling and database design. Vagueness already occurs when collecting formal metadata. For each holocaust text under consideration we try to identify the first published edition. Yet looking at some more widely known texts of that era, we find multiple editions that differ in such basic information as publisher, year of publication, editor, or even the title. Such phenomena are familiar to those concerned with bibliographic information. Special care is required when formalizing such inconsistent data. In our collection of TEI/XML files, every single edition is represented by one XML file. These files contain the bibliographic information in the TEI header, and they are linked to the corresponding other editions.  TEI provides the @cert attribute for indicating (un)certainty of information given in an XML node. While this strategy allows us to indicate possible vagueness in a machine-readable way, we also need to find ways of communicating this uncertainty to the human user of our information portal. For that purpose, uncertainty regarding the plot or the history of reception of a literary work is captured in literary annotations, i.e. running text, which is an effective and straight-forward way of communicating uncertainty to human readers. In this context, we also allow adding <note> elements to be used by our editors for supplying small texts that will be presented to the user describing the kind of uncertainty involved (cf. Bradley and Short, 2005). The combined use of both elements allows conveying uncertainty to the human user while keeping it encoded in a machine-readable (or machine-traceable) way.    2.3 Modeling Uncertainty in a Database The GeoBib information portal will rest on a PostgreSQL database (Scherbaum 2009) [4], where we use object relations for the description of certainty and note elements. Our first example represents a person (see Figure 1a). There are three different names associated with the entity: a birth name, the name after her wedding, and a pseudonym.   Fig. 1: Exemplary database entries for a person : a) person entity model    Fig. 2: Exemplary database entries for a person : b) simplified perso object    Fig. 3: Exemplary database entries for a person : c) related alternative/additional information  While a person entity has certain fields that can be filled in (see Fig. 1b), we use object relations (Fig. 1c) to add alternative information and related values of certainty and/or a note. The second example describes a literary work with an uncertain year of publication. A relational database would require intermediate tables for all attributes of one entity, which may have uncertainties and/or notes attached (cf. Bradley and Short, 2005). In an object relational database using one special entity is sufficient in such a case.   Fig. 4: Work entity and related certainty field \"pub_date\"  As shown in Fig. 2, the table includes the object ID, the table name, the relevant column name, and the alternative content. Each dataset may contain a certainty attribute and/or a note. The certainty field is defined, in accordance with TEI, as either {high, medium, low, unknown} and the note field is a text that will be presented to the user and is meant to explain the uncertainty[6]. In the example above (Fig. 2) a year of publication is given but its certainty is marked as “low”. Such relations can be added for every field of every entity in the database.   3 Discussion and Outlook: Surely more uncertainty  We have just presented our approach of encoding uncertainty in our database. Such information can be used, for instance, to rank search results or to increase recall on certain queries and parameter settings. But we still see more challenges ahead: The encoded uncertainty has to be communicated effectively to the human user. Accordingly, the visualization of uncertainty, and especially the presentation of search results based on uncertain or divergent information will be among our next concerns. A similar problem of visualizing uncertainty arises in the forthcoming georeferencing and geotagging: Literary texts are no geographical maps. They constitute themselves in their geographical space but might encode this information in a way hard to decipher (cf. Reuschel et al., 2013). Fictional place names can sometimes be identified with actual places on a map, but sometimes it is impossible to do so. Geographical locations may be referred to by metaphors, old or forgotten names, local identifiers or nicknames. Such informal references frequently remain geographically imprecise and require interpretation (cf. Hill, 2006, p. 28f). The textual material that our editors provide could also be used to test and improve automatic methods of geographical relation extraction (e.g. Blessing and Schütze, 2010). Still, automatic georesolving is hindered by the limited (historical) coverage of contemporary gazetteers, spelling changes and changing administrative boundaries (Tobin et al., 2010, p. 8). Such limitations also pertain to prisons and camps, those places of high interest in our domain, whose exact geographical locations have to be reconstructed manually before adding them to our databases.     4 Acknowledgements  Funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung, BMBF) from July 2012 to June 2015 (FKZ: 01UG1238A-B).   ",
       "article_title":"Uncertain about Uncertainty: Different ways of processing fuzziness in digital humanities data",
       "authors":[
          {
             "given":"Frank",
             "family":"Binder",
             "affiliation":[
                {
                   "original_name":"Universität Gießen, Germany ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Bastian",
             "family":"Entrup",
             "affiliation":[
                {
                   "original_name":"Universität Gießen, Germany ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ines",
             "family":"Schiller",
             "affiliation":[
                {
                   "original_name":"Universität Gießen, Germany ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Henning",
             "family":"Lobin",
             "affiliation":[
                {
                   "original_name":"Universität Gießen, Germany ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "german studies",
          "metadata",
          "interfaces and technology",
          "geospatial analysis",
          "xml",
          "encoding - theory and practice",
          "databases & dbms",
          "scholarly editing",
          "historical studies",
          "data modeling and architecture including hypothesis-driven modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Campus Medius investigates mediality as historical experience, focusing on a chronotope (Bachtin 2008 & Schlögel 2008) of twenty-four hours in Vienna between May 13 and 14, 1933, and presented on two web-based platforms: an interactive map and an interactive documentary. Methodologically, the project relates Bruno Latour's actor-network theory to dispositif analysis following Michel Foucault, directed at identifying the conditions under which the experiential field of modern media was able to emerge. This approach identifies three levels, distinct in terms of their perspective but empirically overlapping: an archaeology of knowledge forms, a genealogy of power relations, and a typology of subjectivation modes. The twenty-four hours under investigation are marked by the Turks Deliverance Celebrations (Ackerl 1984) held by the Austrian National Socialists and the Home Guards. These events, commemorating the 250th anniversary of Vienna's liberation from the Turkish siege in 1683, were oriented from the outset upon processes of mass communication: the rallies were prepared by the party-political press, partially broadcast live on radio, and captured in propaganda films. To create a counter-public sphere, the Social Democrats published programmatic editorials and organized open-air concerts in Vienna's municipal buildings. While the Burgtheater staged Benito Mussolini's play Campo di Maggio, the large cinemas were screening Fritz Lang's sound movie Das Testament des Dr. Mabuse, a film banned in Germany. Campus Medius reconstructs the media events of this time-space and traces them back to historical media dispositifs. Hence, the project consists of two integral parts: 1. Topography: an interactive map of the chronotope, including hypermedia documents such as video clips, newspaper articles, radio broadcasts, photographs, and archive files. On this mock-up, the events are arranged politically—socialist and communist actors, Austrofascist ators, National Socialist actors, and bourgeois actors.    2. Topology: an argumentative analysis of knowledge forms, power relations, and subjectivation modes underlying the exemplary time-space. Based on the concrete events, this interactive documentary examines functions of media in the classical, modern, and postmodern age—a periodization which constitutes the organizational structure as represented in the diagram.    The technological architecture of the project is comprised of two platforms: the topography exists within the Urban Research Tool (URT) for geospatial mapping, developed at Parsons The New School for Design, and the topology exists within Zeega, a tool for interactive storytelling developed at Harvard's Sensory Ethnography Lab. These platforms provide two complementary lenses for exploring the collection of media material, and will be accessible through one interface: a website with the URLcampusmedius.net. On its front page, an image of Vienna's historical map will be integrated with URT as a kind of basemap, rectified to align with the underlying OpenStreetMap data. The user can either wander through the actor-networks by clicking on the hypertextual points and paths, or follow the Zeega tours that offer three narrative montages, thereby allowing cross-references between the platforms. While the mapped topography describes the events and displays the related media in a networked structure, the Zeegas—titled Sovereign Sign, Disciplinary Gaze, and Controlled Transmission—attempt to explain the historical a priori of the actual experiences, corresponding to the periodization mentioned above. Apart from access to these tours, the interactive map as interface will provide links to three subordinate pages: a text that introduces the project to the users; a glossary of the main theoretical, topical, and technological terms; and a list of references and sources.    Age   Knowledge   Power   Subjectivation     classical   representation   sovereignty   God     modern   man   disciplinarity   individual     postmodern   communication   control   assemblage    The Turks Deliverance Celebrations connect the chronotope with the siege of Vienna in 1683 or—in a broader view—with the classical age. In Foucault's philosophy, this historical dispositif features representation as knowledge form, sovereignty as power relation, and God as subjectivation mode. (Foucault 1966, 1975 & 1976) As shown in the overview of the project's topology, the Viennese weekend actualizes not only these classical attributes, but also features of the modern dispositif in Foucault distinguished by man as knowledge form, disciplinarity as power relation, and individuals in masses as mode of subjectivation. (Foucault 1966, 1975 & 1976) Finally, the exemplary time-space also gives examples of a postmodern dispositif with communication as knowledge form, control as power relation, and assemblages as subjectivation mode. (Deleuze 1990, Hardt/Negri 2000 & Galloway/Thacker 2007) Campus Medius analyzes functions of media in these historical settings as well as effects of their actualization: sovereign leaders of the 17th and 18th century return as theatrical stars; movie houses resemble disciplinary institutions of the 19th century; controls of communication processes as established in the 20th century appear in the form of target groups and public relations; etc. The research project deals with (a) representational, (b) methodological, and (c) empirical questions currently under discussion in cultural studies and humanities: a) Campus Medius is rooted in digital humanities. (McPherson 2009 & Gold 2012) In its clear division between topographical presentation (Presner 2010) and topological analysis (Deleuze 1986), the project utilizes two web-based platforms in order to harness each one's particular affordances. While the interactive URT map shows the actors through hypermedia documents, the Zeega tours focus on the emergence conditions of these events. The capacity of this narrative montages to provide an argumentative explanation of its subject is juxtaposed with the map's encouragement to explore the movie clips, newspaper articles, radio broadcasts, photographs, and archive files in various ways. This implementation sets a high value on technological and scholarly transparency: Campus Medius has been examined in a peer-review process, is being programmed with open-source software, and will be published open-access in the online journal Sensate in 2014. b) Campus Medius contributes to the clarification of the relationship between actor-network theory (Latour 2005) and dispositif analysis (Foucault 1977). According to our thesis, the former suits the description of concrete events that involve human and non-human actors while the latter allows us to derive the examined cases from historical dispositifs of knowledge forms, power relations, and subjectivation modes. (Ganahl 2013) This association is implemented in the dual structure of topography and topology. Due to the media historical subject, the project not only takes up the philosophical (Deleuze 1989 & Agamben 2009) and sociological (Law 1992 & Bührmann/Schneider 2008) discussion, but also the so-called apparatus debate (Baudry 1975 & Rosen 1986). c) Campus Medius relates actors that are usually covered in separate fields of study. Thus, the representative architecture of Schönbrunn Palace and the disciplinary character of Karl-Marx-Hof meet each other as scenes of antagonistic rallies. (Blau 1999) The sensible leader who Mussolini imagined as Napoleon in Campo di Maggio (Dietrich 1976 & Pyrah 2007) confronts the insane criminal as depicted in Lang's Das Testament des Dr. Mabuse (Jacques 1994 & Aurich 2001). While the chancellor's speech at the Turks Deliverance Celebration was broadcast live on radio, the bourgeois newspaper Neue Freie Presse ran an essay about Edward Bernays, a nephew of Sigmund Freud, who carried out propaganda as an \"exact science\" in order to direct public opinion via \"group leaders.\" (Rundt 1933) Bernays had opened his New York office for public relations in the 1920s (Ewen 1996 & Tye 1998), but his concept to build lifestyles around products didn't fully thrive until Paul Lazarsfeld developed complementary statistical methods with his Economic-Psychological Research Bureau in Vienna. (Samuel 2010) The sociologist carried out a survey for Austrian radio in 1932 that broke with an understanding of the audience as a mass of individuals. (Mark 1996) By correlating the listeners' program wishes with their social data, he created target groups that could be exploited in commercial terms.  ",
       "article_title":"CAMPUS MEDIUS--Topography and Topology of a Media Experience",
       "authors":[
          {
             "given":"Simon",
             "family":"Ganahl",
             "affiliation":[
                {
                   "original_name":"Department for German Studies, University of Vienna, Austria",
                   "normalized_name":"University of Vienna",
                   "country":"Austria",
                   "identifiers":{
                      "ror":"https://ror.org/03prydq77",
                      "GRID":"grid.10420.37"
                   }
                }
             ]
          },
          {
             "given":"Rory",
             "family":"Solomon",
             "affiliation":[
                {
                   "original_name":"Parsons The New School for Design, New York, USA",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Mallory",
             "family":"Brennan",
             "affiliation":[
                {
                   "original_name":"School of Media Studies at The New School in New York",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Darius",
             "family":"Daftary",
             "affiliation":[
                {
                   "original_name":"Lead Engineer at Artivest in New York",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "german studies",
          "networks",
          "geospatial analysis",
          "interfaces and technology",
          "relationships",
          "graphs",
          "maps and mapping",
          "spatio-temporal modeling",
          "multimedia",
          "analysis and visualisation",
          "historical studies",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction The Riddle of Literary Quality is a project funded by the Computational Humanities Program of the Royal Netherlands Academy of Arts and Sciences (KNAW). It runs at Huygens ING in partnership with the Institute for Logic, Language and Computation of the University of Amsterdam, and the Fryske Akademy in Leeuwarden. The aim of the project is to develop a method and the necessary software to analyze low-level and high-level formal features in a corpus of modern Dutch long fiction, to find out whether formal features in the texts play a role in the reception and evaluation of the text by the readers. Can we get more insight into the responses of readers to, for instance, texts with on average longer versus shorter sentences, or using a larger vocabulary, or on average showing a more complex syntactical structure (cf. Jautze et al.)? Is there a difference between those texts that readers consider to be highly literary and those that are experienced as more lowbrow? Can we distinguish texts found good or bad by readers based on formal features in these texts? And how do the opinions of readers correlate with the kind of reader they are? The project thus aims to correlate formal features with readers’ opinions and readers’ roles. The analysis of the formal features is done through a chain of μServices that we will deal with in the second part of this paper. The first part is addressed to the analysis of readers’ opinions and readers’ roles.    Survey To gather information about readers and their responses we set up a large online survey in which we asked respondents some personal information (age, gender, postal code, level of education) and sixteen questions to find out what kind of reader they predominantly are: autonomous or ‘distanced’: reading for aesthical pleasure; or heteronomous ‘identifying’: reading for fun, to discover other cultures or places, or to identify with the main characters. We based our distinction and our questions on work done by Von Heydebrand & Winko on sociological aspects of (literary) reading. Next to that, we presented a list of 400 recent novels, Dutch originals or translations into Dutch, and asked them to mark the ones they read. A selection of these novels was presented to them, with the question to evaluate these works on two scales: from ‘not so very literary’ to ‘highly literary’ and from ‘bad’ to ‘good’. The survey ran for six months, and received almost 14000 respondents. Analysis of the results has just started. The results of the survey will be correlated with the results of the measurements of formal features. We would like to describe the technical set-up we have devised to enable the scholars to analyze the texts in the corpus in a way that is trustworthy and sustainable, using μServices written in Java that can also be used by others to repeat and to verify our analyses.   μServices Research infrastructure for the Riddle of Literary Quality is designed with three goals in mind: research results must be reproducible; analytical tools must be reusable; the entire workflow must be maintainable and reliable. We aim to provide a toolset that allows for a verifiable system that will focus the discussion on the selected methodology - the procedures and algorithms. This also means that we will make the code behind each μService open source. To accomplish this goal the digital humanities engineering group at Huygens ING based the research infrastructure both on the results of COST Action IS0704: An Interoperable Supranational Infrastructure for Digital Editions (Interedition) – of which the institute was grant-holder – and the work of Joris van Zundert – chair of the Action. Van Zundert (Huygens ING) specified as an objective of the COST Action the development of lightweight and distributed interoperability solutions. These solutions were implemented through webservices. The CollateX algorithm of Ronald Haentjens Dekker (Huygens ING) and Gregor Middell (University of Würzburg) was among the first and most successful of a series of compact analytical demonstrators called μServices. The Riddle of Literary Quality does not aim to build a workflow management system. Such a top-down standardization methodology is left to large infrastructural programs like CLARIN, DARIAH or the Dutch Nederlab project. Instead we continue Interedition’s grassroots approach and leave (computational) researchers and PhD students free to experiment with high-level and low-level analytical algorithms in languages that range from Python to Java. These algorithms may or may not grow out to be part of the Riddle’s μService infrastructure and those that are deemed useful are eventually hosted at the institute’s servers.  The current services fall in three distinct categories: data import and preparation; analysis and visualization and export. In the first group we offer e.g. a series of tools that convert documents to specified standards (such as ePub/PDF to TEI) and set the data in the correct character encoding (such as a conversion from Windows-1252, ISO8859 to UTF-8). To prepare the data for further analysis we have converted parsers like the Dutch Ucto: Unicode Tokenizer (Radboud University of Nijmegen/University of Tilburg) to a μService. The output data of services in this group is a standardized json format that can be read by the analytical services in the second category. Experiments in The Riddle currently focus on this analysis group. μServices in the third category perform output operations. Some create visualizations while others export the data to external environments for further analysis. For stylometric research e.g. we created a μService to export data from The Riddle to R and integrate it with the Stylo() package created at the Universities of Krakow (Macej Eder/Jan Rybicki) and Antwerp (Mike Kestemont). The entire suite of μServices will remain available for persistent access and may be used in alternate workflows or by external third-party software. Thus the suite does not only allow reproduction of the results of The Riddle but will also support entirely new and original research.   Sample Workflow As an example of a µServices-driven workflow we present one possible use of gathering statistical data from a corpus of ePubs. First, each ePub is sent to a service that prepares it for analysis by converting the book into a structured TEI document. Character-encoding issues are resolved by a second µService, resulting in a normalized, platform-independent UTF-8 version of the TEI-document. Subsequently, a third service offers extraction operations on the structural level of the file. This service is used to extract all relevant paragraphs. These paragraphs are split into sentences and words by one of a family of (TEI agnostic) tokenizers, such as the Ucto-µService. Statistical analysis of these tokens is possible by sending the resulting list of tokens to the exporter µService, which transforms the extracted tokens into a format suitable for use in R.   Conclusion To make sure that we are able to answer the main questions of The Riddle of Literary Quality – whether there are any correlations between readers' opinions about certain novels, readers' predominant reading role, and the values for a list of formal low-level and high-level features of the novels – we have chosen to develop a set of µServices that deal with single aspects of the needed analysis. By making these µServices available to other scholars we enable them to repeat and verify our research results. We provide users with tools that can be used to answer different questions than we have in The Riddle, thereby making the tools also useful in a wider sense for new original research. We hope our approach invites others to contribute µServices for further textual humanities research.  ",
       "article_title":"μServices and The Riddle of Literary Quality",
       "authors":[
          {
             "given":"Gertjan",
             "family":"Filarski",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The ",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Hayco",
             "family":"de Jong",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The ",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          },
          {
             "given":"Karina",
             "family":"van Dalen-Oskam",
             "affiliation":[
                {
                   "original_name":"Huygens ING, Netherlands, The ",
                   "normalized_name":"Huygens Institute for the History of the Netherlands",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04x6kq749",
                      "GRID":"grid.450092.a"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "literary studies",
          "corpora and corpus activities",
          "cultural studies",
          "content analysis",
          "cultural infrastructure",
          "stylistics and stylometry",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction In literary analysis, description – as opposed to narration – has previously often been an underestimated part of fiction. Literary theorists such as Bal, Lopes and Nünning however have made a case for its relevance [1, 6, 8]. Lopes reviews how well-known theorists like Barthes have dismissed description as ‘extra’, irrelevant or stalling the plot; he counters these notions with the statement that “[d]escription and narration constitute the two most basic modes of structuring any prose fiction text” [6, p. 19]. How the plot is conveyed, is relevant for how a text is judged. Literary theorist Wells for instance argues that description is the distinguishing factor between quality literature and ‘simple’ chick-lit novels [15]. Indeed, research has shown that literary novels contain significantly more noun phrases and prepositional phrases than chick lit, indicating a larger amount of description [5]. In this paper, the first steps are taken of a larger project in which description in fiction is computationally analyzed, as opposed to the now popular computational analysis of narrative (see for instance 7). The preliminary question that we want to answer is: how (well) can we extract descriptions from fiction? This will be tested in the current paper by zooming in on a specific domain: the physical description of fictional characters.   2. Motivation Descriptions of physical appearance are chosen as a test case as they are more likely to occur in a current-day novel than for instance landscape description. Moreover, main characters are often introduced in the first chapters. This makes it possible in case of manual tagging (which we have done) to tag only the first chapters of a novel. Finally, it would be an interesting feature for further literary interpretation. Connotations of beauty in folk tales have been researched [i.e. 14], but this has not yet been done for novels.   3. Method The corpus of [5] is used, consisting of 32 novels of recent Dutch fiction, half chick-lit, half literary novels. Two of them were tagged from beginning to end for descriptions of physicality, including clothing. One is a literary novel, De schilder en het meisje (‘The painter and the girl’) by Margriet de Moor, the other chick lit, Zwaar verliefd (‘Heavily in Love’) by Chantal van Gastel. Bal defines description as “a textual fragment in which features are attributed to objects” [1, p. 36], a definition we will follow. We tagged full sentences that were either mainly concerned with physical appearance (example 1a, Van Gastel), mentioned a single feature (1b, De Moor) or somewhere in between.  1a. Hij heeft mooie lippen. He has beautiful lips.  1b. Door de rook heen keek hij naar de porseleinen wangen van mevrouw Cloeck[.]  Through the smoke he watched madam Cloeck’s porcelain cheeks[.]  For the extraction, two approaches are compared: (1) manual development of lexical-linguistic patterns and (2) a Naive Bayes and an SVM classifier. For the former, because patterns were manually developed on the basis of two novels, the patterns were subsequently tested on the other 30 novels, each of which the first 500 sentences were manually tagged.   3.1 Lexical-linguistic patterns  After an initial exploration of the two main novels’ tagged sentences, an approach was adopted of manually developing patterns to detect sentences containing description. Hearst uses similar patterns to harvest hyponyms [3]. Patterns consist of a combination of linguistic and lexical information, see example 2 below. A set of 13 patterns was written. The manual exploration showed that sentences containing physical descriptions, as opposed to sentences with no such descriptions, (a) contain more nouns and adjectives, (b) are regularly coupled with a few specific, static verbs, and (c) contain a couple of recurring base lexical-linguistic patterns, e.g., 'He was [a manNP] [[withPP] [brown eyesNP]]’. To perform extraction, the corpus was parsed with Dutch parser Alpino [2, 12]. Alpino parse trees provide rich linguistic annotations of sentences such as grammatical function of constituents. The trees can be queried with XPath, which was integrated in Van Cranenburgh's TreeSearch interface [5]. Linguistic information alone does not suffice however to target physical descriptions, so we used Cornetto, the Dutch WordNet [13], to expand a manually constructed lexicon of nouns and adjectives related to physical descriptions. The lists were cleaned to exclude words that were not relevant to the topic, resulting in a lexicon of almost 600 words.  An example of a pattern translated to an xPath query is:    //node[@cat=\"pp\" and @rel=\"mod\"]//node[%uiterlijkA%]/../node[%uiterlijkN% or %kleding%]    Example 2: This pattern searches for a modifying prepositional phrase which contains an adjective and a noun from the lexicon.    3.2 Machine learning  We cast the task of extracting physical descriptions as a text classification task in order to use machine learning methods. The task then becomes for a given text to automatically assign a class to it (in our case: physical description or no physical description). Usually, text classification is done on the document level. This means that for each document a corresponding class is predicted [10]. Algorithmic methods used for the classification task vary widely. Naive Bayes classification and Support Vector Machines (SVM) were used, two established straight-forward approaches to text classification [4, 9, 11]. We adapted these approaches to our task of classifying sentences. Each sentence was classified as either a description or not, in order to extract the descriptions.    4. Results 4.1. Lexical-linguistic patterns Precision, recall and F-measure were calculated for each pattern separately for the two main novels, for the test set of 30 novels, for a cumulative set of all pattern results, and for chick-lit versus literary novels; the most important results can be found in table 1. Sentences that were extracted more than once were calculated as one hit.   4.1. Lexical-linguistic patterns      F-measure (%) Precision (%)  Recall (%)    Test set-all novels   31 29  35    Test set-litterature  25 29  22     Test set-chick lit  18  28  13     Main novels  16   24   12     Table 1: Results for lexical-linguistic pattern-based extraction   An unexpected outcome was that the results were much better for the 30 novels in the test set than for the two novels on the basis of which the patterns were developed; the percentage of descriptions might be higher in the first chapters. Another interesting result was the performance on literary novels, which was better than on chick lit. An explanation might be that in chick lit, sentences are shorter [see 5], more often elliptic (‘And his mouth… He has beautiful lips. Precisely full enough.’) and regularly discuss physicality through dialogue, for which it is hard to develop patterns. Generic patterns, containing little more than lexical information, achieved higher scores than more specific ones. The specific patterns did improve the cumulative outcome. Further research is needed, but an expansion of the lexicon might raise performance.    4.2 Machine learning  We trained our classifiers on the two annotated novels. The features selected as input for the classifiers are words weighted with tf.idf, for which we considered the sentences as documents and the novels as the collection of documents. Experiments were also performed for bigrams and part-of-speech tags, but the results were comparable to the results we report here. We performed ten-fold cross validation on the set of sentences from each novel and both novels combined. We found that Naive Bayes outperforms SVM for this task, as can be observed in Table 2.      F-measure (%)   Precision (%)   Recall (%)      Both novels            Naive Bayes   60   57   62     SVM   58   59   58      Zwaar verlifd           Naive Bayes  62   61   64     SVM  57   59   56     De schilder en het meisje          Naive Bayes   58   55   62     SVM  52   53   51     Table 2: Results for the Naive Bayes and SVM classifier   Performance is considerably higher than that of the pattern-based approach. The skewedness of the class distribution (descriptions form only a small portion of a novel) makes this classification task a hard one, but overall this is a promising method. This machine learning approach can be regarded as a baseline: more sophisticated methods might yield better results.   5. Conclusion A comparison of two methods for extracting sentences containing physical descriptions paints a clear picture: extracting such information is a complex matter but not impossible, and machine learning performs better than a manual-based approach. However, the main benefit of using the manual tagging and patterns is the insight they give in the form of the sentences that contain the sought-after descriptions, whereas the bag-of-words approach of the machine learning method is limited to finding features based on individual words. A possibility for future research is extension of the patterns and the lexicon to see if the results can be improved, but we prefer to pursue a bottom-up approach. A combination of the methods could be fruitful: using the patterns as features for machine learning. We could also explore descriptions on a different textual level; especially for the chick-lit novels, where use of ellipsis and dialogue confuses sentence extraction, larger fragments of texts should be analyzed. Targeted topic modeling might be useful for this purpose.  ",
       "article_title":"Beautiful lips and porcelain cheeks: extracting physical descriptions from recent Dutch fiction",
       "authors":[
          {
             "given":"Corina",
             "family":"Koolen",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The ",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Sander",
             "family":"Wubben",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherlands, The ",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          },
          {
             "given":"Andreas",
             "family":" van Cranenburgh",
             "affiliation":[
                {
                   "original_name":"University of Amsterdam, Netherland",
                   "normalized_name":"University of Amsterdam",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/04dkp9463",
                      "GRID":"grid.7177.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "natural language processing",
          "literary studies",
          "poetry",
          "linguistics",
          "drama",
          "text analysis",
          "genre-specific studies: prose"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This short paper will describe the on-going research being conducted jointly by Kings College London and the University of Glasgow to understand the social networks of the medieval Scottish elites from the years from 1093 to 1286. This paper will start with a description of social networks and the concepts of social network analysis. It will then move on to describe some of the uses social network analysis has been put to in historical research. This will be followed by a description of the People of Medieval Scotland database which provides the data for this research. Finally, the social network analysis techniques used in this research will be described and the preliminary results will be discussed.    Social Network Analysis Social networks are defined and measured as connections among people, organisations, political entities (states and nations) and/or other units. Social network analysis is a theoretical perspective and a set of techniques used to understand these relationships (Valente 2010, pg. 3). Christakis and Fowler (2010, pg. 32) say that the science of social networks provides a distinct ways of seeing the world because it is about individuals and groups and how the former becomes latter. Valente (2010, pgs. 3 – 7) says that relationships matter because relationships influence a person’s behaviour above and beyond the influence of his or her attributes. A person’s attributes does influence who people know and spend time with: their social network. Valente quotes Borgatti et al (2009), “one of the most potent ideas in the social sciences is the notion that individuals are embedded in thick webs of social relations and interactions”. The reason that social networks are so important is because human beings are ultra-social animals that create social networks (Haidt, 2006). Christakis and Fowler (2010, pg. 214) add that human beings just don’t live in groups, they live in networks. Valente argues the traditional social science approach of using random sampling is not adequate for measuring network concepts because random sampling removes individuals from the social context that may influence their behaviour. Valente explains that one primary reason social network research has grown in recent decades is that scholars have become dissatisfied with attributes theories of behaviour. Many attribute theories have not explained why some people do things (e.g. quit smoking) while others do not.  Social network explanations have provided good explanations in these cases.   Use of Social Network Analysis in History The seminal work in using social network analysis in historical research is Padgett and Ansell’s (1993) research on the rise of the Medici in renaissance Florence. Their work showed that the rise of the Medicis came from their ability, especially the ability of Cosimo de Medici, to take advantage of the gaps in connections in the social network which the Medicis were able to bridge to take political control of Florence. Since then, the use of social network analysis in historical research has been steadily increasing.    Using Social Network Analysis with the People of Medieval Scotland Database The People of Medieval Scotland (PoMS) database holds data on all known people between 1093 and 1314 mentioned in over 8600 contemporary documents. This was funded by the Arts and Humanities Research Council in the United Kingdom. The current research is part of the Transformation of Gaelic Scotland project funded by the Leverhulme Trust. This exploratory research has the goal of understanding the role of social networks among the elite of medieval Scotland. It also has the goal of exploring the appropriateness of social network analysis techniques for this data set, and perhaps for other similar collections The first technique used was 2 mode networks. In 2 mode networks, two sets of actors are dealt with. This method comes from the pioneering work of Davis et al (1941). In this research, the two sets of actors are legal documents called charters and the people who witness them. As a result, you will see links between witnesses and charters but not among the witnesses and charters. This becomes even more useful by the affiliation technique. Here the software is asked to create a 1 mode network or a network with only one set of actors by connecting witnesses who have witnessed the same charter together. The software can also keep track of how many times a particular witness has witnessed a charter with every other witness. The theory is that the more often two witnesses witness charters together, the more probable there is an actual social relationship between the two people. Therefore, as the number of charters witnessed together rises, the more probable the resulting network is an actual map of the social relationships.  Other techniques used include:  2 mode network with witnesses by locations to identify geographically clustered witnesses Ego networks where the focus is on all the people connected to a selected person and the interconnections among these people Directed network of grantors and beneficiaries. This network is directed because the direction is always from the grantor to the beneficiary. Using cluster analysis and structural equivalence to see if witnesses can be clustered by the similarity of their network connections Using network models of diffusion of innovations to track how charter innovations spread    Findings so far This work is still very preliminary but some interesting findings have appeared from the use of social network analysis. One good example of this is Duncan II, Earl of Fife. Historians knew he was a very prominent noble in Scotland but social network analysis revealed a possible further role he played in Scotland.  Duncan has witnessed more than 20 charters with 27 people while William del Bois, the chancellor whose role is to manage charters, has only done that with 15 other witnesses.  Also, Duncan has witnessed more than 40 charters with 7 people while William has done the same with only 2 other witnesses. However, Duncan has witnessed charters with 630 other witnesses while William, the chancellor, has witnessed charters with 479 other witnesses. Overall, William has witnessed 213 charters while Duncan has witnessed 210 charters. So the question is why does Duncan, Earl of Fife have so many more connections than anyone else? There is no definitive answer to this question yet but the leading hypotheses centre on Duncan’s possible role in the government, taking advantage of his brokerage opportunities and enhanced social skills. The grantor/beneficiary network showed that those who gave the most grants were kings, popes, bishops and senior noblemen such as earls. Those who received the most grants were mainly ecclesiastical institutions such as abbeys, priories and cathedrals. In addition personas such as Saint Cuthbert and the Blessed Virgin Mary received large number of grants. However, none of this was very surprising given the nature of medieval Scottish society. Ego networks for a number of people have been generated. We have compared these ego networks by density, brokerage opportunities and how often the ego acts as a bridge inside the network. No general trends have been discovered yet. We have also looked at turning Burt’s (1992) work on its head by looking not at brokerage opportunities but at the interconnections to determine the characteristics of their social networks. As of now, this has not been completely successful. It is too soon at this date to report findings on using network models on the diffusions of innovations. However, we see this as an exciting prospect as it will allow the tracking of how charter innovations spread or did not spread throughout medieval Scotland. Historians have identified several charter innovations to investigate and we hope to report on this at the conference.   The 2 mode witness by location network did not work because of bad location data in the database which is now being corrected. But, the biggest data issue in using social network analysis with this data is that the People of Medieval Scotland database has only legal documents in it and does not have the marriage, baptismal and tax records that Padgett and Ansell (1993) used. These additional records would allow us to confirm relationships that can be inferred especially from the 2 mode network analysis.    Summary In summary, while this research is still preliminary, it has shown the power of social network analysis to bring a new perspective to old data. Duncan II, Earl of Fife is an example of this. While the historians knew Duncan was very prominent in his time, they had no idea that he might have a possible role in running the Scottish government during the reign of William I. The use of network models of the diffusion of innovations to see how charter innovations did or did not spread in medieval Scotland is another example where this technique will allow us to show the mechanism of how these innovations spread.  ",
       "article_title":"Using Social Network Analysis to Reveal Unseen Relationships in Medieval Scotland",
       "authors":[
          {
             "given":"Cornell Alexander",
             "family":"Jackson",
             "affiliation":[
                {
                   "original_name":"Kings College London, United Kingdom ",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  CUbRIK and the History of Europe App  The integration of human expertise and machine computation enables a new class of applications with significant potential for the digital humanities. So far this potential remains largely untapped due to the severe requirements of such projects: The implementation and integration of advanced algorithms requires specialized know-how and the final users from the humanities are challenged with defining unprecedented tasks for methods which haven’t emerged yet. The FP-7-funded research project CUbRIK (www.cubricproject.eu) implements and integrates research in computer science, the design of human-computation tasks, data visualization, social engineering and the humanities.  In the proposed presentation we would like to showcase one of CUbRIK’s case studies, the demo of the History of Europe application. The application introduces an effective interface to access collections of historical sources and to discover links among and entities within them. Upon completion CUbRIK will offer an innovative approach to human-enhanced time-aware multimedia search by synthesizing research in computer science, crowdsourcing and gamification. We will conclude the presentation with an outlook on the future development of the application.  Humanist-machine interaction  The History of Europe (HoE) application is based on a curated collection of more than 3000 images, representing the main events and actors in the history of the European integration. The collection is curated and hosted by the Centre Virtuel de la Connaissance sur l’Europe (CVCE). In a first step, an image indexation pipeline identifies the location of individual faces in the photographs. The location of these faces is verified by a crowd of “click-workers” with no specific training who evaluate for each recognized face if the depicted image shows a human face or not. Following the face verification process, an automatic face recognition process is triggered that associates each of the now verified faces with a list of ten possible identities. This list of candidates is then disseminated for example through Twitter to a crowd of experts that vote and comment for their preferred identity.  Besides the identities of the different persons, all information that is associated to an image, such as the time or the place where the image was taken as well as contextual information about associated historical events can be reviewed by expert users and delegated to a crowd of domain experts for review.  Data aggregation, visualisation and analysis  Building on the computed co-occurrence of persons in images a social graph is constructed that connects them with each other. Connections gain in strength the more often persons appear together in an image. Finally the result of this process is depicted in a visualization of the social graph with a set of analytical tools.  The social graph in the History of Europe App aims at representing and visualizing dependencies between historically relevant persons in the context of European integration. Thereby the weight of the (social) links between person entities relies on their co-occurrence in historic photographs as identified by the aforementioned image indexation process. The more frequently two persons appear in different photographs, the stronger the link between the corresponding entities in the graph.  Users can interact with the History of Europe social graph in different ways, e.g. a click on a node results on an ego-graph of the selected person and clicking on an edge displays documents that relate to both selected relationship. As the documents stored in the collection very often come with a date of creation, the graph can be filtered by date with the timeline, displaying only the connections of documents created within this timespan. This timeline also shows the amount of photos per date that are contained in the collection. Another filtering option is the number of connecting documents, which allows the visualization of those relationships that are only included in an interval of a minimum and maximum number of documents. This feature is useful to highlight highest co-occurrences. Finally, the number of appearances of a person in the processed collection lets us identify people who appear particularly often in any given time frame.  Crowd discussion and a new approach to the representation of truth in digital research tools  Another challenge for the HoE app and the domain of the Digital Humanities in general is the conception of truth, which differs significantly e.g. to the conceptions of truth in Computer Science. Computer Scientists can rely on a stable foundation of what is true: Any experiment can be replicated and measured precisely. In the humanities the concept of truth is far more complex: It is based on the insight, that there is no neutral or objective way to study human environments. The way, in which questions are asked, how data is selected to answer them, by what means this data is analyzed and finally the way in which the results of such analyses are communicated and received all challenge the idea of “one truth”.  In order to represent the discursive nature of truth in the humanities within HoE we make use of a community-driven tool for question answering, similar to stackoverflow.com. User have the opportunity to answer questions and thus benefit from the knowledge within the expert crowd. However, the system allows for more than one answer and offers its users the possibility to vote and answer up or down, thereby allowing more than one answer to enter in competition with each other whilst also maintaining the full spectre of the discussion.  Summary and outlook  The History of Europe application takes on the challenge to combine cutting edge research in the domains of computer science, the design of human-computation tasks, data visualization, social engineering and the humanities by identifying synergies between the disciplines’ strengths and by compensating for their weaknesses. We do this by building a pipeline which connects face recognition tools, data visualization and input from humans and creates an ongoing cycle of iteratively improved user input and machine output. The History of Europe application stands in line with a range of other online tools for historical research but introduces new social features as well as crowd sourcing from both click-workers and expert users which continuously improves the system. In the future we will expand the selection of sources to include digitized text documents as well as audio and video interviews from different archives.   ",
       "article_title":"Building the social graph of the History of European Integration: A pipeline for the Integration of Human and Machine Computation",
       "authors":[
          {
             "given":"Lars",
             "family":"Wieneke",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l’Europe",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ghislain",
             "family":"Sillaume",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l’Europe",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marten",
             "family":"Düring",
             "affiliation":[
                {
                   "original_name":"Centre Virtuel de la Connaissance sur l’Europe",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Chiara",
             "family":"Pasini",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Elettronica, Informazione e Bioingegneria",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Piero",
             "family":"Fraternali",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Elettronica, Informazione e Bioingegneria",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marco",
             "family":"Tagliasacchi",
             "affiliation":[
                {
                   "original_name":"Dipartimento di Elettronica, Informazione e Bioingegneria",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marc",
             "family":"Melenhorst",
             "affiliation":[
                {
                   "original_name":"Delft University of Technology",
                   "normalized_name":"Delft University of Technology",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02e2c7k09",
                      "GRID":"grid.5292.c"
                   }
                }
             ]
          },
          {
             "given":"Jasminko",
             "family":"Novak",
             "affiliation":[
                {
                   "original_name":"European Institute for Participatory Media",
                   "normalized_name":"European Institute for Participatory Media",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02ay1zt80",
                      "GRID":"grid.434653.3"
                   }
                }
             ]
          },
          {
             "given":"Isabel",
             "family":"Micheel",
             "affiliation":[
                {
                   "original_name":"European Institute for Participatory Media",
                   "normalized_name":"European Institute for Participatory Media",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02ay1zt80",
                      "GRID":"grid.434653.3"
                   }
                }
             ]
          },
          {
             "given":"Erik",
             "family":"Harloff",
             "affiliation":[
                {
                   "original_name":"European Institute for Participatory Media",
                   "normalized_name":"European Institute for Participatory Media",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02ay1zt80",
                      "GRID":"grid.434653.3"
                   }
                }
             ]
          },
          {
             "given":"Javier",
             "family":"Garcia Moron",
             "affiliation":[
                {
                   "original_name":"Homeria Open Solutions S.L.",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Carine",
             "family":"Lallemand",
             "affiliation":[
                {
                   "original_name":"Public Research Centre Henri Tudor",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Croce ",
             "family":"Vincenzo",
             "affiliation":[
                {
                   "original_name":"Engineering Ingegneria Informatica S.p.a. ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Marilena",
             "family":"Lazzaro",
             "affiliation":[
                {
                   "original_name":"Engineering Ingegneria Informatica S.p.a. ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Francesco",
             "family":"Nucci",
             "affiliation":[
                {
                   "original_name":"Engineering Ingegneria Informatica S.p.a. ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "image processing",
          "and discovery",
          "resource creation",
          "digitisation",
          "cultural infrastructure",
          "content analysis",
          "interdisciplinary collaboration",
          "multimedia",
          "information architecture",
          "information retrieval",
          "historical studies",
          "crowdsourcing",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In a recent essay on the stock footage libraries amassed by Hollywood studios in the first half of the 20th century, Rick Prelinger—moving image archivist at the Internet Archive—laments that “archives often seem like a first-aid kit or a rusty tool, resources that we find reassuring but rarely use”  (Prelinger 2012). Although he doesn’t single them out by name, web archives are particularly vulnerable to this charge. User studies, access statistics, page views, and other metrics have in recent years told a consistent story: web content that has been harvested and preserved by collecting institutions, universities, and other organizations often lies fallow, and like Prelinger’s rusty tool may be notable more for its latent potential than for having served any real purpose (Hockx-Yu 2013; Kamps 2013; Huurdeman et al 2013).  While the reasons for neglect are myriad, this paper focuses on one: the lack of tools to support a wide range of interactions with the content. We describe initiatives underway at the University of Maryland to partially redress the problem and highlight the need for qualitative user studies. The Internet Archive’s Wayback Machine is perhaps the best-known and most widely available tool to browse captured content. Both the Internet Archive’s main public site and Archive-It, its subscription-based web archiving service, replicate the experience of viewing web pages on the live web, thus reifying a “close-reading” experience. First developed in the mid-1990s, the software came of age at the same time digital humanities scholars were building the first generation of web collections aimed at providing high-resolution digital facsimiles of literary and artistic works by Blake, Rossetti, Dickinson, Whitman, and others. The emphasis on accurate rendering and display is thus a hallmark of both the Wayback Machine and many early DH projects, the latter of which likewise self-identify as “archives,” albeit archives on a dramatically smaller scale.  Although the capabilities offered by the Internet Archive and other commercial services are significant, we believe considerable technical advances are needed if web archives are to fulfill their promise as tools of analysis as well as preservation.  Within the field of DH, the big data vistas offered by scholars such as Matt Jockers and Ted Underwood provide both inspiration and models on which to base these efforts (Jockers 2013). Unlike the boutique digitization initiatives that characterize the early wave of DH archives of the 1990s and early 2000s, which were often devoted to the works of a single author, the new macroanalytic approaches are premised on mass-digitization of print heritage. The paradigm they embody, moreover, is not digitization in the service of verisimilitude—reproductions that show exact fidelity to their originals—but rather digitization that produces terabytes’ worth of intermediary copies that can be cleaned, normalized, segmented, tokenized, mined, and visualized to yield new insights about the cultural record writ large. Such a paradigm disrupts the usual data-information-knowledge continuum by taking the unitary wholes of creative expression—the “cooked” novels or poems or historical documents in print—and temporarily degrading them to a “raw” data state so that they can be analyzed at scale to make higher-order knowledge claims.   We believe that the technical infrastructure to support macroanalytics or “distant reading” on web archives today is inadequate. Existing tools were built before the coming of age of “big data” technologies and provide wobbly foundations on which to build analytical tools that scale to petabytes of data. As an example, the open-source Wayback Machine is implemented as a monolithic stack primarily designed to scale “up” on more powerful servers and expensive network-attached storage. Its architecture captures the ethos of “state-of-the-art” software engineering practices of the late 1990s. Not surprisingly, the field has advanced by leaps and bounds in the last decade and a half. In the 2000s, Google published a series of seminal papers describing solutions to its data management woes, which involve analyzing, indexing, and searching untold billions of web pages. Instead of scaling “up” on more powerful individual servers, the strategy entailed scaling “out” on clusters of commodity machines (Barroso et al., 2013). Before long, open-source implementations of these Google technologies were created, bringing the same massive data analytic capabilities to “the rest of us.” These systems form the foundation of what we know as “big data” today, and provide the backbone of data analytics infrastructure at Facebook, Twitter, LinkedIn, and many other organizations. Three key systems are:   The Hadoop Distributed File System (HDFS), which is a horizontally-scalable file system designed to store data on clusters of commodity severs in a fault-tolerant manner (Ghemawat et al. 2003). The largest known HDFS instance (by Facebook) holds over 100 petabytes. Hadoop MapReduce, which is a simple yet expressive programming model for distributed computations that works in concert with data stored in HDFS (Dean and Ghemawat, 2004). MapReduce models analytical tasks in two distinct phases: a “map” phase where computations applied in parallel, followed by a “reduce” phase that aggregates partial results. HBase, which is a distributed store for semi-structured data built on top of HDFS that allows low-latency random access to billions of records. Google’s Bigtable (Chang et al., 2006), from which HBase descended, powers Gmail, Google Maps, as well as the company’s indexing pipeline.   Modern big data technologies provide a technical path forward and an accompanying research agenda that does for web archives what macroanalytics or so-called “distant reading” has begun to do for digitized corpora in DH. As a first step in this effort, we are developing Warcbase, an open-source platform for storing, managing, and analyzing web archives built on the three technologies discussed above. The platform provides a flexible data model for organizing web content as well as metadata and extracted knowledge. We have built a prototype application that provides functionality comparable to the Wayback Machine in allowing users to browse different versions of resources in a web archive (typically as WARC or ARC files). Since Warcbase takes advantage of proven open-source technologies, we are confident of the infrastructure’s ability to scale in a seamless and cost-effective manner. Yet Warcbase is only the beginning. We believe that our prototype—and, more generally, the technologies described above—will provide new capabilities that support innovative uses of web archives. Responsive full-text search on massive collections of web pages, one of the first items on a scholarly wishlist, is within reach: the tools exist in various open-source projects, awaiting integration. Longitudinal analyses of web pages such as tracking the frequency of person or place names become possible if we integrate off-the-shelf natural language processing tools. Yet another possibility is topic modeling on a massive scale; a separate project at the University of Maryland has built Mr.LDA, an open-source Hadoop toolkit for scalable topic modeling (Zhai et al., 2012). To provide a hint of what’s possible, we have been working with Congressional archives from the Library of Congress to explore topic modeling and large-scale visualizations of archived content, the results of which we will share during the conference presentation. Why are large web archives so underused? It is surely not due to a lack of culturally significant material. Valuable content, ripe for exploration, ranges across topics such as electronic literature, alternate reality games, digital tools for human rights awareness, the Arab Spring uprising, and Russian parliamentary elections, to name just a few. Restrictive access regimes are partially to blame, but that alone does not provide a sufficient explanation. We believe that the issue, to a large extent, is a technological form of circular reasoning: scholars do little because the right tools don’t exist, and tool builders are hesitant to build for non-existent needs and users.  Progress is necessary to understand the essential activities, methods, and questions of researchers. Interviews with current web archive users are a start, but breakthroughs will require deep collaborations between scholars and technologists. The end goal is a comprehensive set of tools for researchers in the digital humanities and beyond to analyze and explore our digital cultural heritage.   ",
       "article_title":"Supporting \"Distant Reading\" for Web Archives",
       "authors":[
          {
             "given":"Jimmy",
             "family":"Lin",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Kari",
             "family":"Kraus",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ricardo L. Punzalan ",
             "family":"Punzalan",
             "affiliation":[
                {
                   "original_name":"University of Maryland, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "internet / world wide web",
          "sustainability and preservation",
          "user studies / user needs",
          "repositories",
          "cultural infrastructure",
          "archives",
          "information architecture"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1.1  Overview  How might students understand a digitised edition of a major documentary series, created at the height of empire and that is perhaps the largest repository about working people from the mid-19th to the mid-20th centuries? Would differing forms of digitisation affect their understandings of the series? Our research results indicate poorly in answer to the first question and, surprisingly, not at all on the second, because understanding proved to be less a question of form than content. To realise the potential created by the radically democratised access that online sources permit, we first need to understand and respect how 21st century undergraduates see their world. Rather than focusing on form, we need to explain how their experiences as youth in a neo-liberal world on the brink of ecological disaster relate to questions of power, hierarchy and resistance in the past.  1.2 Methodology  In the preliminary stage individual qualitative and intensive examinations of undergraduate students were carried out using two fundamentally different editions of the same digitalised document. One half used a traditional textually-introduced document, the other accessed the same information through individual optional Google pins distributed throughout the document. This research was then supplemented by in-class experiments and work with graduate students and colleagues.   2.1 Preliminary questions The dramatic increase in primary source material available online fundamentally transforms teaching and research in the humanities. Material that was until recently only available to hundreds or at best thousands of people at select, often unique, repositories is now virtually available to potentially millions of people. Historical sources are an important part of this completely unprecedented and radical reorientation of archival practice. The sheer scale of this newly democratised access to the sources of humanity’s story is already extraordinary, and as more and more of the archival treasures long-housed in former imperial centres becomes available for research, and importantly for our purposes teaching in classrooms around the world, the potential for challenging Eurocentric conceptions of our past is great indeed.  This potential will, however, only be realized if our students are equipped with the tools they need to critically analyse these online resources. But is the historical literacy needed to understand a manuscript the same when encountered as a virtual source? Can a student simply transfer the knowledge and skills learnt in supervised archival research to working on the web? Are there different ways of knowing, distinct epistemologies, which are more appropriate to the qualitatively unique ontology of online sources? If so, should we be using the conventions of the digital world to navigate through these virtual sources? (Krug, 2006) How might such techniques affect the necessary respect of the historical distance between a conscientious researcher in the present and the source from the past that she or he is examining?  When you hold a centuries old artefact in your hands, feel its weight, hear the paper crinkle, notice a stain or perhaps just react to the dust, you sense a connection to the past that is simultaneously humbling and enriching. This experience has been at the heart of research in the humanities for centuries. (A. Burton, 2005 & Steedman, 2005) Can a virtual encounter be as meaningful? (Hayles, 2001 & JDH, 2012) Indeed how useful is to think of these virtual representations as being from the past? Are they for our students anything more than a brief illusory encounter with a largely incomprehensible past?  Discussion of these wide-ranging and difficult questions, posed as pedagogues of history, led the authors of this paper to engage in an on-going exploration we call “Explaining ourselves.” An urgency fueled our discussions, as increasingly we realized that our undergraduate students see the world in fundamentally different ways than those we taught only a short time ago. So how might they encounter a major documentary series that was created at the height of empire, and is perhaps the largest repository with documentary consistency about working people from the mid-19th to the mid-20th centuries? Close to hand we had the basis for an answer.   2.2 Finding the answers Memorial University of Newfoundland is a public university with little in the way of endowments. It is the only university in a province that has the highest poverty rates in Canada. Until recently, MUN has been a place where undergraduate teaching was respected. In the early 1970s, young historians raised the necessary funds from the federal government and convinced the local university administration to acquire the bulk of a collection documenting merchant seafarers of the British Empire, held by the Public Record Office in London. (Matthews, 1974) Covering more than a century, the Crew Agreements maintained by MUN’s Maritime History Archive document the workforce of three-quarters of all ocean-going vessels within the British Empire between 1863 and 1939, with declining numbers continuing until the early 1970s. It provides detailed information about the tens of millions of men and women who served aboard the largest merchant marine in history.  Completion of the major public history initiative More Than a List of Crew (V. Burton, 2011) provided the impetus for our study. By asking how would students read digital editions of these complex, multi-layered documents, she reinvigorated her co-author’s decades-long engagement with the use of computers in the classroom (Sweeny, 1988-2010).  We developed two digital editions of the same crew agreement. The first had an extensive, multiple- screen-length, textual introduction, analogous to an introduction to a historical document in a scholarly edition in print. The second used Google pins to provide location specific information to help the user navigate the document. We created two sets of colour-coded pins. The first reproduced in bite-size portions the contents of the textual introduction. The second drew attention to any references to a particular individual onboard. A crew agreement from another vessel a decade later involving the same seafarer was also made available for the students to examine.  We worked with fourteen undergraduate Arts and Science students in individual sessions. We recorded the screen actions for each session. Working alone, students were given up to two hours to familiarize themselves with the document. Half worked with the text edition and half worked with the annotated one. Then the students had two hours to complete two exercises. The first was content-oriented and used a multiple-choice/short answer quiz. The second involved the second crew agreement mentioned above, which was not annotated, but the students could consult the earlier documentation. They were asked to write a brief analytical essay engaging both primary documents. These written assignments were followed up by focused conversations where we asked the students to explain their answers and to identify the problems they had encountered in working with the documents. As we progressed this de-briefing became more effective as we realized the merit of asking students how they would explain these documents to a friend.  In a second phase of the research we broadened our pool. We asked select graduate students in history and colleagues from our department to participate. Students in a third year historical methods class as well as students in a first year introductory course spent a 50 minute class with the documents after having visited the archives, formally in the first case and virtually in the second. They were then asked for written feedback.  The results indicated no significant differences in understanding of the documents, both groups fared poorly. Furthermore, students with substantial historical training did not show appreciable differences. Thus, our presumption that form mattered was misdirected. Instead, student comments and their observed navigation practices both strongly suggest their need to engage directly with the documents in ways which use their existing understandings if they are to explore new ways of seeing.   2.3 Where from here? We need to consciously transcend the form/content divide, if we are to engender in our students both an appreciation of the internal logic of a source and the ability to engage concept and evidence. In this particular case, how a crew agreement embodies the unequal power relationships between seafarers, masters, ship owners and the state would be the most fruitful pedagogical focus. What are they agreeing to and why? Crew agreements, like almost all historical documents, record unequal relations. Young people today live in an increasingly unequal world and this is the key to a progressive pedagogy that opens up our troubled present to our many and varied pasts. Used critically, it might yet allow our students to realize the democratic potential of all those newly accessible documents.  ",
       "article_title":"Realizing the democratic potential of online sources in the classroom",
       "authors":[
          {
             "given":"Robert C.H. ",
             "family":"Sweeny",
             "affiliation":[
                {
                   "original_name":"Memorial University of Newfoundland, Canada",
                   "normalized_name":"Memorial University of Newfoundland",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04haebc03",
                      "GRID":"grid.25055.37"
                   }
                }
             ]
          },
          {
             "given":"Valerie C. ",
             "family":"Burton",
             "affiliation":[
                {
                   "original_name":"Memorial University of Newfoundland, Canada",
                   "normalized_name":"Memorial University of Newfoundland",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/04haebc03",
                      "GRID":"grid.25055.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "ontologies",
          "internet / world wide web",
          "gender studies",
          "digital humanities - pedagogy and curriculum",
          "teaching and pedagogy",
          "databases & dbms",
          "historical studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  TEI is good at what it does: static documents rendered in glorious detail. But TEI is old. Its age doesn’t make TEI irrelevant, but it’s important to be conscious of how the way we weave the fabric of the web has changed since TEI was conceived in 1994, and reevaluate some of our assumptions about its use. In this early work, we are exploring this rethinking as part of a larger study within the center on general methods for isolating the complexity frequently associated with XML-based frameworks. The Richmond Times Dispatch corpus of TEI-encoded newspapers comprises the Confederate newspaper’s Civil War run, 1860 — 1865. It is compelling both in terms of organization and content and amounts to a comprehensive textual index. In addition to the historical allure of its content, the formal properties of the digitized documents made available through the Perseus Collection make the Dispatch an extraordinary raw material for building a rich interactive visual experience that augments the textual one. The Dispatch is not in need of a new home; the Perseus Collection hosts a perfectly functional version and uses best practices for data encoding and organization. Rather than strive to be the primary resource for the source material, our project uses the source material to explore recent patterns in web development as well as alternative, more visually compelling ways to interact with XML corpora in a web application. The goal is to produce a powerful reading environment that is tailored to its source material to an extent that the generalized project of the Perseus Collection can afford. With respect to its implementation, our project fits the genre of a ‘single page application’ (SPA). This project demonstrates best practices for implementing this type of software project using a particular suite of tools; as an open source example of an SPA that is considerably more complex than the usual teaching examples for this kind of thing, we hope that our implementation will be useful to other people who are considering using the same tools, and especially to humanists interested in presenting TEI-encoded documents. The SPA is du juor. We prefer beautiful URLs and smooth transitions. We are less fond of all these big lists cluttering our sidebars and clunky arrays of checkboxes. We don’t expect websites to always be inert collections of documents. We want to be able to control the connective tissues. The web development community has responded to our current expectations with tools to suite them. The client-side MVC (Model-View-Controller) libraries that have recently emerged have reached a high level of maturity A client-side MVC library codifies conventional solutions to the generic problems posed by web traffic. It provides semantics for describing the interaction layer between data and presentation. The codification of conventions that MVC libraries manifest is exciting. It deeply simplifies matters for those who want to make interactive documents. Humanists who have a grasp of the language and concepts involved will be that much better able to articulate and realize project architectures that delight the contemporary reader. For instance, our application is built around a client-side router. The router formalizes protocols for state transitions that allow for timely and efficient request management. We rely heavily on the concept of the run loop, which exposes powerful document management techniques and is tightly linked with a client-driven templating engine. We are able to achieve a remarkably clean separation of concerns in a highly condensed space by exploiting the conventional roles organized and implemented by these libraries. And by shifting our application’s emphasis to the client, we have constant access to a unified programming environment, limiting the context-switching required when developing different parts of the application. In addition to our project’s strong client-focused application architecture, we also demonstrate a data architecture solution to the problems posed by the corpus’ rich TEI markup. To expose the facets embedded in the source XML, the implementation transforms the deeply nested structure inherent into flat relational representations that can be searched efficiently. Furthermore our project demonstrates a novel, pythonic approach to transforming the source XML to browser-ready HTML that is particularly amenable to the constraints of an SPA. XSLT wasn’t very well suited to our TEI transformation problem. One of the key UI features of our application was the ability to discover and search for special entities such as people and places in the text. By implementing a custom transformer in python, we had the flexibility to both translate the TEI tag names into valid HTML versions and retain the original TEI tag names and attributes as attributes on the HTML element. In addition to serving content thus transformed as needed, the role of the server in our application is limited to various precomputation and preprocessing tasks that only need to be run whenever the source material changes--a process that is fully automated with Unix batch processing (via cron) in the cloud. Users never notice. Research projects are often quagmired in a chaotic sprawl of one-off scripts; we demonstrate a coherent architectural pattern for orchestrating these preprocessors. Sometimes the affordances of an SPA make it worthwhile to depart from the original document’s presentation. Content on the web wants a different kind of exposure than a stack of newspapers. You want to be able to find things quickly. You want to be able to highlight and hyperlink, associate and drill down. Once you’ve computed a graph of your stack of newspapers, now you can move laterally, staying in the same section and moving from date to date, just as easily as you could stay on the same date and move top-to-bottom through the articles. We demonstrate a novel, minimalistic navigation scheme for the Dispatch. If you’re taking full advantage of a Javascript environment to render your XML content, you can use modern libraries to plug in visualizations with simplicity, and furthermore to turn these visualizations into interactive filters for a very powerful browsing experience. Using a cluster of technologies surrounding Mike Bostock’s work, we demonstrate how to integrate a visualization library into an SPA. And yet we believe that datafication shouldn’t overwhelm the content. You want to be discrete about placing your controls lest you scare the casual user, but they should be powerful. Live feedback from search inputs has come to be a common expectation for user interfaces and the SPA environment makes it easy to architect that. We show one effective way to make your XML live-searchable. We close with just a few screenshots of the work in progress. It is important to note that this interface is being further refined based on new work Trevor is doing in his new role as a front-end software developer.    Fig. 1: An early version of the splash page, presenting user interface controls for the issue date, section, and subsection. Selecting a subsection would reveal the list of headlines it contains. The date selectors reload the issue content asynchronously, without reloading the page.    Fig. 2: The early version of the article reader, presenting the text in a modal context. The summary of facets across the top act as toggles for corresponding highlights in the text. You can page through the section content using the controls at the bottom of the modal.    Fig. 3: This screen capture shows the direction taken in the latest development. The URL bar demonstrates stateful client-side routing. The content selection controls have been flattened into a trio of type-ahead controls with pagination buttons for navigating forward and backwards both in the document structure and across documents.  This stuff is fun. The tools are a joy to use. The free/open source community behind it is excellent and innovative. We want to see more humanists building applications, and moving away from consuming and rather heavyweight content management systems such as Drupal. Based on our experience, humanists can learn the tools and frameworks quickly with excellent results to boot. We hope that our implementation of the Dispatch will set a strong example for our (and others’) future DH projects.  ",
       "article_title":"Single Page Apps for Humanists: A Case Study using the Perseus Richmond Times Corpus",
       "authors":[
          {
             "given":"Trevor",
             "family":"Borg",
             "affiliation":[
                {
                   "original_name":"Loyola University Chicago, Center for Textual Studies and Digital Humanities",
                   "normalized_name":"Loyola University Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04b6x2g63",
                      "GRID":"grid.164971.c"
                   }
                }
             ]
          },
          {
             "given":"George Kuriakose",
             "family":"Thiruvathukal",
             "affiliation":[
                {
                   "original_name":"Loyola University Chicago, Center for Textual Studies and Digital Humanities",
                   "normalized_name":"Loyola University Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04b6x2g63",
                      "GRID":"grid.164971.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "visualisation",
          "internet / world wide web",
          "sustainability and preservation",
          "publishing and delivery systems",
          "repositories",
          "xml",
          "archives"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction This research falls within the field of digital humanities; the arts and information science engage in dialogue. In the last few decades, dance has become a distinct research subject. Dance research needs data. Dance performances remain elusive, and the traces they leave in their wake need to be documented. However, the documentation practices of performance remain unsatisfactory (Couch 1994: 42; Rowat 2005; Desalme 2007: 13; Chaffee 2011: 125), and the specificity of performing arts such as dance is rarely and quite imperfectly taken into account (Le Boeuf 2002; Miller and Le Boeuf 2005). Dance description in archives needs to be improved because, in this era of massive digital information, the quality of the description impinges on access to the documentation. Description is entangled with access. The better the description, the more efficient the access will be for information seekers. As Nena Couch (2004: 53) once said about dance collections, “If there is no standard language through which a patron may communicate his or her search, the material may be as lost as if the library had never acquired it.” Knowledge extraction seems to offer new opportunities in this regard. Objectives The goal of this research is to contribute to the development of information management tools by evaluating the relevance of knowledge extraction in information resources maintenance and development for performing arts such as dance. Aesthetic experience is an essential part of art; a part, however, that is hard to define, let alone describe, in an archives context. Through knowledge extraction, we obtain a vocabulary for describing the aesthetic experience of modern dance in archives. Choreographic works were described using this vocabulary. Methodology Many contemporary artists include archival material in their artistic practices (Poinsot 2004; Lemay 2009). Performing arts archives and archivists must be as creative as the art they keep records of (Johnson and Fuller Snyder 1999; Jones, Abbott and Ross 2009: 166; Chaffee 2011: 125). Artists are inspired by archives, and archivists should be inspired by artists in return. To obtain a vocabulary for representing the aesthetic experience of modern dance, we drew on modern literature and modern art. At the end of the 19th century, French writer Stéphane Mallarmé praised in Autre étude de danse the dazzling performances of Loïe Fuller, a pioneer of modern dance. Mallarmé was an artist and an aesthete (Delfel 1951), a “métaphysicien du ballet” (Levinson 1983). His work is an aesthetic experience in itself, and is related to dance in many ways (Richard 1961; Kristeva 1974: 537; Block 1977: 96; Levinson 1983; Zachmann 2001). Writing about Mallarmé, Mary Ann Caws (1998: 86) says, “[w]hat he gives us is everything that comes after him.” Gayle Zachmann (2001: 188) mentions that “writers and critics […] have highlighted this poet’s contributions to the theoretical underpinnings and reading of modern dance and/or the significance of his writings on dance for his own aesthetic.” His work foreshadowed the spring of modern dance. We worked from a corpus of texts that includes Mallarmé’s collections Divagations (1897 edition published by Eugène Fasquelle) and Poésies (1899 edition published by Émile Deman) for a total of 119 documents (comprising poetic prose, poem, dialogue), 11,850 types on 70,507 tokens (before lexical filtering) and 7,238 types (after lexical filtering). This corpus has been linked to modern dance for decades, and artists as well as experts in the field of dance studies have drawn on it for inspiration. The vocabulary was obtained through knowledge extraction methods; to be specific, text mining algorithms combining term extraction and clustering. Documents were grouped into clusters using discriminant features (terms). Clustering is the method of choice for thematic discovery and terminology building (Ibekwe-SanJuan 2007; Forest 2012) and, as such, a bottom-up hierarchical clustering method was used. Once the cluster structure was created, characteristic terms were extracted from each cluster to use as the basis for building a basic structured vocabulary of the aesthetic experience of modern dance. Results Two main clusters emerged from the corpus, one of 75 documents, the other of 34 documents. From the 75-document class were drawn three qualifiers, one for each subcluster, of the aesthetic experience of modern dance: petit (small), seul (in solo), beau (beautiful). The antonyms were then drawn directly from the corpus: grandiose (grandiose, great), en couple (in duo) or en troupe (in a body), laid (ugly). From the 34-document class were drawn nine topics related to the aesthetic experience: corps (body), idéal (ideal), nature (nature), nudité (nudity), pureté (purity), rire (laughter), solitude (solitude, loneliness), temps (time), voix (voice). The terms allow the description of what is happening visually onstage beyond the storyline, that is, the visual experience.   Fig. 1: Vocabulary of the aesthetic experience extracted from Mallarmé’s work   We were able to describe choreographic works with the vocabulary. For example, we described Dave St-Pierre’s Un peu de tendresse, bordel de merde! using the following terms: troupe, voix, nudité, rire. St-Pierre’s work is known for theatrical staging, shocking nudity and dark humour. Exploring Mallarmé’s vocabulary has allowed us to better describe dance performance and to develop minimal yet innovative access points to traditional archives. Thus this initial experiment supports the relevance of knowledge extraction in information resources maintenance and development for performing arts such as dance. Knowledge extraction is one of many solutions for creating a vocabulary for dance archives.  Conclusion In this research, we delved into the core of an art, literature, to find a vocabulary for describing art. Humanities computing and non-computing approaches are complementary here. The field of digital humanities, recent advances in information technology and opportunities offered by knowledge extraction all contribute to the possibility of exploring innovative solutions to improve the description of dance performance in archives, as well as fostering a better understanding of the art of dance. In this way, information science empowers the arts while the arts empower information science.   ",
       "article_title":"A vocabulary of the aesthetic experience for modern dance archives",
       "authors":[
          {
             "given":"Ève",
             "family":"Paquette-Bigras",
             "affiliation":[
                {
                   "original_name":"Université de Montréal, Canada ",
                   "normalized_name":"University of Montreal",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0161xgx34",
                      "GRID":"grid.14848.31"
                   }
                }
             ]
          },
          {
             "given":"Dominic",
             "family":"Forest",
             "affiliation":[
                {
                   "original_name":"Université de Montréal, Canada ",
                   "normalized_name":"University of Montreal",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0161xgx34",
                      "GRID":"grid.14848.31"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "french studies",
          "sustainability and preservation",
          "repositories",
          "creative and performing arts",
          "archives",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Digital Yoknapatawpha is a critical database, interactive map, timeline, and network visualization that aims to unite the 15 novels and 48 short stories that William Faulkner, over the course of his career, set in the imagined county of Yoknapatawpha. “Digital Yoknapatawpha: Interpreting a Palimpsest of Place” reports on the collaborative effort of 25 scholars from around the world, led by Stephen Railton, to expand the understanding of Faulkner’s creation and recreation of this one county. This paper, written jointly by four of the project’s collaborators, will describe the digital methods and practices engaged in the project, as well as the inherent challenges produced when digital spaces encounter the fictional world of an author who did not care much about getting the facts right.  William Faulkner, the 20th-century writer known for his portrayal of the American South, envisioned his fictional world, Yoknapatawpha County, as a series of maps. In Faulkner's development of Yoknapatawpha’s geography through narrative, the creators of Digital Yoknapatawpha have discovered the patterns of a digital space, meaning that Faulkner returns to the same places, times, and people in his fiction, though he often alters the details. Like a digital space, these details are ever revisable. Yoknapatawpha County’s imagined geography branches out in a web of connections between people, places and events through multiple representations. Each element layers onto another, creating a palimpsest of place throughout Faulkner’s fictional world. For our purposes “place” is not just a point (lat/long) or even a simple area on a map. Certainly, geography is crucial, however, the “place” that constitutes our primary interest is established by the confluence of human activity at locations, across areas and reverberating in time. To be sure, we can gather ideas about Faulkner’s corpus without the use of digital methods. We argue, however, that without the meticulous harvesting and collation of these elements we can, at best, only derive vague impressions of the complicated webs that inhabit Faulkner’s fiction. While a substantial number of Faulknerian scholars already focus on the role of “place” in understanding human activity, our intent is to complement and extend such scholarship, not supplant it, through internet-accessible interactive visualizations.  Our collaboration began with discussions (recurring small-group meetings and a National Endowments for the Humanities funded, multi-day, full-group workshop) in which we worked to establish “editorial policies” that could guide the interpretive decisions that each team of editors would face as it conducted the close reading of individual stories and novels. Fundamental to the interpretation is the agreement that Faulkner’s “confluence of human activity” may be digitally interpreted by identifying the characters that participate in the events Faulkner composes and by identifying the locations and time-frames attached to each event.  Each team therefore conducts close readings of an individual text with the goal to draw out specific details about the categories of characters, events, locations and time frames as keyed to that specific text. The interpretations derived from those close readings form the basis of the interactive visualization we call Digital Yoknapatawpha. While the interactive aspect of the visualization cannot be conveyed here, we have included Figures 1-5 to provide a basic overview of the information available through the visualization.  The novel, Flags in the Dust, and 11 short stories that constitute the current set of interpreted texts are shown on the prototype of the home page in Figure 1. Selecting the leftmost icon takes one to the visualization specific to Flags in the Dust, as shown in Figure 2. The central area of the display depicts a map of the imagined geography engendered by that text. It contains man-made features (e.g., roads), natural features (e.g., hills and waterways) and the location icons that appear as settings for events in the novel. These are in layers that can be hidden or shown via the controls on the left. To the right are generalized geographies for events that might be interpreted as taking place outside of Yoknapatawpha. When a user selects a specific location on the map, the map yields a textual display that describes the location and provides tabs listing the events occurring at that location and the characters participating in those events (see Figure 3).  The dynamic mode of the visualization becomes crucial when we consider the progression of events in the narratives. We use the term “progression” to emphasize that events play out in two spacio-temporal modes: the narrative or textual order (indicated by page number) and the chronological order as we interpret it using historical and calendar clues within the text. The three bars on the bottom of the display (see Figure 2) provide playable timelines; the uppermost play buttons provide a textually organized progression and the lower buttons provide a chronological one.  Figure 4 provides a snapshot of the progression in which the second event has been selected on the page-order progression bar. Allowing the page-order to “play” through to an event that occurs on page 75 yields the display shown in Figure 5. In this mapping one can see the geographic expanse of the events that have occurred up to that point in the novel while the chronological line demonstrates the way that the novel has narrated human activity across various temporal settings.  As presented above, in the Digital Yoknapatawpha project, we use digital methods that both challenge and mimic Faulkner's aesthetic project in order to discover new insights about his narrative approach. For example, Faulkner manipulates, to great effect, narrative space against textual space. By entering events with a page-order aspect, we are forced to focus on the textual space as Faulkner deliberately shaped it. When we order those events into a chronology—a chronology that perhaps spans his entire fiction—we are forced to confront, in very concrete ways, the absences, contradictions, concurrences, and shifting perspectives that spread out over that textual space.  So much in Faulkner is speculated, projected, imagined, and contradicted. Faulkner himself often claimed to be interested in truth, not facts. Therefore, how can we account for the ambiguity evinced through close reading yet also record the specific information required for visualization? The specific manner of close reading that our project of data collection requires forces us to identify, yet at times bracket, the narratives’ ambiguities and contradictions in order to determine precise locations in the textual space. This approach, and its attendant frustrations, allow us to enliven Faulknerian contradiction with traditional maps and innovative network visualizations, thus making the ambiguity that is so central to Faulkner’s fiction also a central element of the project’s design and display. In turn, the potential in the aesthetics of digital space overcome the challenges and frustration of contradictory positions by providing visual and interactive ways to engage with ambiguity in Faulkner's work.    Fig. 1: Home page indicating novel and stories covered.       Fig. 2: A visualization of the imagined geography (and associated geographies).    Fig. 3: Textual information about a specific location.    Fig. 4: A location highlighted to indicate an event is happening there.    Fig. 5: A different location highlighted, with the accumulated evidence of events already told.   ",
       "article_title":"Digital Yoknapatawpha: Interpreting a Palimpsest of Place",
       "authors":[
          {
             "given":"Dotty J.",
             "family":"Dye",
             "affiliation":[
                {
                   "original_name":"Arizona State University, Tempe, Arizona",
                   "normalized_name":"Arizona State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03efmqc40",
                      "GRID":"grid.215654.1"
                   }
                }
             ]
          },
          {
             "given":"Julie Beth",
             "family":"Napolin",
             "affiliation":[
                {
                   "original_name":"The New School for Liberal Arts, New York, New York",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Cornell",
             "affiliation":[
                {
                   "original_name":"Fordham University, Bronx, New Yor",
                   "normalized_name":"Fordham University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03qnxaf80",
                      "GRID":"grid.256023.0"
                   }
                }
             ]
          },
          {
             "given":"Worthy",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":" University of Virginia, Charlotteville, Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "literary studies",
          "and discovery",
          "resource creation",
          "internet / world wide web",
          "interdisciplinary collaboration",
          "databases & dbms",
          "multimedia",
          "digitisation",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The proposed paper reports on the development of a new digital resource: EGO | European History Online(EGO) is a transcultural history of early modern and modern Europeconcentrating on processes of communication, interaction and interdependency (www.ieg-ego.eu). It is being published by the Leibniz Institute of European History (IEG) in Mainz in cooperation with the University of Trier’s Centre for Digital Humanities. At its heart are transfer processes that extended across individual, familial and local realms and had a long-term impact. EGO traces these transfer processes in and between, amongst others, the realms of religion, law, politics, art, music, literature, economics, technology and the military, science and medicine. One can speak of a \"transfer\" when people, objects and ideas move between different cultures (interpretative systems) and as a result undergo transformation. European History Online is the first history of Europe that links the medium's relevance to the subject matter. The format of an online system of publication is the ideal medium for representing the complexity and dynamics of European communicative and transfer processes. The more than 200 articles are organised into ten thematic threads. These threads group the separate articles into a modular structure arranged thematically and methodologically. These threads are transdisciplinary and multi-thematic; they bring together the perspectives of different historical disciplines and their international authors. At the same time, they are organised diachronically, i.e. they deal with phenomena that – with specific periods of development and significance – are primarily evident throughout modern European history. This organisation offers flexible means of accessing the contributions: in contrast to a printed book, European History Online does not have a beginning and an end. EGO accommodates the dynamics of intensifying communication and the continuously shifting intersections in European history by assigning many articles to more than one section. Within these multiple classifications, one can see how the topics interconnect. The different forms of presentation – surveys, basic elements and focus elements – and their organisation into a modular structure enable nuanced contextualisation. In addition to this multi-layered structure, EGO articles are directly connected via hyperlinks. The aim of these connections is to expose the so far unknown concentrations of communication in European history, inspire new transcultural research in the various disciplines and thereby promote a more dynamic understanding of European history. The versatile search function allows users to put together their own \"history of Europe\" which corresponds to their individual interests. Moreover, EGO pursues a multilingual approachthat acknowledges the need for a workable meta-language / lingua franca in the Humanities but at the same time does justice to the linguistic variety of national academic cultures in Europe: EGO-articles are accepted in English and German. All major contributions are translated by native speakers and published in both languages. In addition, authors may publish their article in their native language. Users are invited to consult both the original and the translation in order to trace differing argumentative patterns and conceptual peculiarities of the respective languages. From the technical point of view EGO is based on a sophisticated infrastructure which on the on hand supports the editorial board (backend of the system), and on the other hand is used to build the frontend of the system for the publication of the EGO articles in the internet. For these purposes the open source enterprise content management system Plone is used, which is a very powerful environment developed for professional use in organisations and companies. Especially its sophisticated and safe user access management as well as its workflow driven content management makes it one of the outstanding web management systems. In Plone it is possible to handle very different content types as for example texts, images, PDF-files, audio and video data. For EGO all these project specific media types are modelled by a corresponding Plone article type, such that they can be configured by the editor himself within the Plone configuration layer. Here the editors can specify all relevant visual and layout attributes for each object according to the underlying web design of EGO and thereby prepare the article for the final publication. This step is integrated in the whole publication workflow which is modelled within the content management system. The participating roles are the authors, the editors, the copy editors and the publishers. Every member of this team has its own role and permissions to work with the documents, which run through different stages from ‘private’ (i.e. newly provided by the author) over ‘review/revision’ (redacted by at least one editor and/or copy editor) and ‘internal preview’ until it is ‘externally published’ on the EGO platform (c.f. figure 1).   Fig. 1: EGO article ‘Mental Maps: The Cognitive Mapping of the Continent as an Object of research of European History’  All information about the documents, the media types, the interlinking between documents and external resources and especially about the people involved is managed in an object database, which serves as storage for the backend as well as for the frontend. This guarantees that all changes to a document are logged in one consistent pool of data. Moreover, on this database the search engine for EGO is installed. In addition to a conventional full text retrieval the engine supports the query for specific categories of EGO articles as for example authors, time ranges, themes/threads, geographical regions of Europe and for media types (image, audio, video etc.). To address aspects of long term availability of the project results the EGO documents are encoded according to international standards (XML/TEI, METS/MODS). The implementation of an open access interface to our Plone system allows a standardized export of the data, which is on the one hand used for long term storage in form of a dark archive. On the other hand it provides possibilities for the reuse of the EGO documents (e.g. linguistic analysis, visualization of the semantic network, compilation of bibliographic information, etc.).  The consequent use of a technical research and publication infrastructure leads to a highly dynamical publication process, to a very dense network of information and thereby to a very flexible presentation with additional benefits compared to traditional printed editions. This change in media undertaken by European History Online challenges the concept of multi-volume published surveys, which, as a rule, must wait 20 to 50 years for a new edition. This dynamic form of publication corresponds to the dynamic understanding of Europe: the articles can be updated regularly, and the system can be extended by new articles in order to keep up with new developments in the research. Older versions of an article will remain accessible. European History Online primarily uses linear, textual presentations of narrative and analysis in order to portray transfer processes in European history. However, EGO enhances spatial perspectives as well. All place names in the articles are georeferenced. They are retrievable in an alphabetical index and being visualized on a dynamic map (via Open Street Map). Thus authors are encouraged reflect their coverage of geographic areas in their resesarch, whereas users, via the general index (and map) of place names, may discern spatial clusters with regard to their topic of interest. EGO thus strives to enhances a spatially-oriented approach in transnational and transcultural history. In addition, European History Onlinecombines different types of media in a – new – interpretative context. Images and audio and visual clips illustrate not only the topic being described, but also narrate their own histories of transfer and enable new interconnections. EGO's transdisciplinary approach is also to a large degree a product of the images, graphics, maps, tables, film clips and audio samples linked to the different textual contributions. This network exists, on the one hand, via internal links to elements published within EGO and, on the other, via links to external images, textual sources and biographical data digitalised or published elsewhere, as well as – in the notes – scholarly literature and other academic resources online. While these external resources represent all national traditions relevant to the history of Europe, EGO makes them accessible to a transnational academic community via a bilingual user interface. The dynamic EGO system thereby brings together and groups thematically the range of international online resources on European history.   ",
       "article_title":"Europe as a Digital Network: EGO European History Online",
       "authors":[
          {
             "given":"Thomas",
             "family":"Burch",
             "affiliation":[
                {
                   "original_name":"Trier Center for Digital Humanities, Germany",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Joachim",
             "family":"Berger",
             "affiliation":[
                {
                   "original_name":"Leibniz Institute of European History (IEG), Germany ",
                   "normalized_name":"Leibniz Institute of European History",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/059dyrr28",
                      "GRID":"grid.461633.5"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "metadata",
          "theology",
          "software design and development",
          "sustainability and preservation",
          "repositories",
          "xml",
          "databases & dbms",
          "archives",
          "multimedia",
          "historical studies",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Topotime: Representing historical temporality Historical analysis within any discipline depends in part upon establishing chronologies (Sewell 2005), but historical data are problematic. The spreadsheets and database systems used for representing and computing over digital chronologies do not handle vague or otherwise uncertain data well. How does one encode “for 6 months before the war,” “around 1832,” or “during harvest seasons in her youth?” And when dates for events lasting days or months are given only in years, how can we calculate contemporaneity? What if our data include both precise dates and vague date ranges with varying granularity? To date, historical researchers and digital humanities application developers have managed temporal uncertainty in ad hoc fashion, normally with one or two date fields using the ISO-8061 standard (e.g. YYYY-MM-DD) for the Gregorian calendar—most would say with less than optimal results. Meanwhile, researchers in computer science, geographical information science, and other fields have done considerable work on some challenges in temporal representation, including uncertainty and qualitative temporal reasoning. (cf. Kauppinen et al 2010;  Holmen & Ore 2009; Crescioli, D’Andrea Niccolucci 2000; Plewe 2002). Some of those results, which often demonstrated only in small exemplars, can be brought to bear on humanists’ requirements. However, we must make explicit our desiderata for temporal representation and computation in order to make headway towards fulfilling them. In light of this, we have initiated the Topotime project, with these initial goals: 1) a specification for computable digital representations of the kinds of temporal entities typically found in historical texts, and some relationships between them; 2) one or more graphical timeline layout programs to parse and render data written in that form; 3) software tools to facilitate the encoding process, and to transform data in two stages—converting spreadsheet exports to the flexible and human-readable data format, JSON-LD, then parsing and transforming those JSON data files into “temporal geometries” for calculations of distance, similarity, and topological relations. In this paper, we briefly outline the draft Topotime specification as it stands, and the software development progress we have made so far. The goals as set out above are admittedly ambitious. Temporal entities found in historical texts and records include a wide range of scales and imprecision, and refer to many calendars and modes of temporal reasoning. We have begun what will be a long-term iterative process of enumerating examples  and fine-tuning a data model to handle them, written in JSON. Two perspectives We are approaching this work from two directions in parallel to meet requirements for both drawing timelines and for calculating temporal relations. These are not mutually exclusive and parsers for both cases have considerable overlap of functionality and comparable complexity; generalities in formal representation that are useful in both cases are emerging. For example, both parsers convert various date expressions to Julian dates for calculations. There are also distinct differences, for example between the data objects best suited for efficiently drawing time bands, dots, and arrows on a timeline, and the temporal geometries referred to earlier. The basic elements of Topotime A Topotime data file describes a PeriodCollection. Each Period is of a class (either Event, HistoricalPeriod, or Lifespan) and has temporal extents described by one or more typed timespans (tSpan). PeriodCollections have Projection definitions which include atom (granularity, such as day or year), origin(day zero on the reference calendar, in Gregorian date terms), and scale (used for timeline rendering). Periods must have a unique id, a source attribution, and a label for graphical display. They can also have any number of optional properties (attributes), although Topotime software does not handle these directly. A PeriodCollection can also include a set of asserted relations, both between periods and between periods and places. These are distinguished from those purely temporal relations between Period timespans, which can be calculated and may be incidental. Timespans   Fig. 1: Timespan with fuzzy interval bounds, as a probability function. This event likely ended by D (~0.7) and certainly by G.  When describing the “when” of an occurrence we ordinarily mean that it took place either throughout some timespan, or for some time during it. Someone born in 1723 was not born for the entire year! In Topotime, a tSpan describes temporal extents and throughout is the default; some time during is noted by adding a (“during”: True) statement, and a duration, e.g. (“d”: “1d”) for a duration of a birth day. In both cases (throughout and during), date ranges describe bounds with a required start (“s”) and optional latest-start (“ls”), earliest-end (“ee”), and end (“e”). This conforms to a pattern commonly seen in graphical representations, from Joseph Priestley’s 18th century timelines to the popular Simile Timeline software[3], and the recent formalizations cited earlier. In this way, timespans can be represented as having either fixed or “fuzzy” bounds[4]. The result is a “temporal geometry” such as pictured in Figure 1. The shapes of the curves between s-ls and ee-e can be articulated more completely by adding “sls” and/or “eee” arrays, as shown, reflecting an author’s understanding of the probabilities over the course of those sub-spans.  Time values for s, ls, ee, and e can be either a day, a month, or a year, and can be qualified by operators for “before” (<), “after” (>), and “about” (~). They can also be pointers to other Period timespans or parts thereof. For example, “>38.s” refers to “after the start of Period 38 in this collection.” Omission of a referent part (e.g. >38 or <38) is taken to mean either its end (“e”) in the first case or its “s” in the latter. Topotime recognizes not only date ranges with fixed or fuzzy bounds, but durations, cyclical timespans (regularly recurring ranges), and multi-spans (arbitrary discontinuous spans) as well. Examples of notation for these are listed in Table 1.  Table 1 - Timespan notation in Topotime, partial listing   fixed range (throughout) {\"s\": \"1901-04-01\", \"e\": \"1963-01-12\"}A lifespan: born April 1, 1901; died Jan 12, 1963   fuzzy range (throughout) {\"s\": \"1923-03-21\", \"ls\": \"1923-06-20\", \"e\": \"1930-10-01\", \"ee\": \"1930-12-31\"}Employed from spring of 1923 to late 1930.   fixed range (during) {“s”:”1934”, “during”: True, “d”: “4m”}Traveled in Spain for 4 months in 1934   fuzzy range (during) {\"s\": \"1923-03-21\", \"ls\": \"1923-06-20\", \"e\": \"1930-10-01\", \"ee\": \"1930-12-31\", “during”: True, “d”: “~6m”}Hospital stay for about 6 months during studies   cyclical {\"s\": \"1951-05-01\", \"e\": \"1999-05-01\", \"cduration\": \"18m\", \"cstep\": \"4y\"}US Presidential campaign seasons in late 20th century   multi-part [{\"s\": \"1901-01-01\", \"ls\": \"1901-02-02\", \"ee\": \"1919-01-01\", \"e\": \"1920-05-05\"}, {\"s\": \"1931-01-01\", \"ls\": \"1935-02-02\", \"ee\": \"1961-01-01\", \"e\": \">12\"}]intermittently as specified, until after Period #12   duration {\"s\": \">1\", \"duration\": \"2m\"}tSpan for Period beginning after Period 1, lasting 2 months   Period relations Meronomic (parts) Purely temporal relationships between Periods (overlap, adjacency, containment) can be calculated from tSpans geometrically. But there are more relationships we routinely assert and represent, for example part-of. We might say, “these 18 events occurring at these times, or in this order, were part-of that larger event”—e.g. a lifespan, war, or political campaign. Another scholar’s chronology for the same composite event might include an entirely different set of sub-events. We may wish to model The Bronze Age as an historical period having spatial-temporal parts such as “Late Bronze Age Southern Levant” and “Bronze Age – Malta.” A Topotime relation consists of a subject, predicate and object (at minimum) in the following form: {“subj”: 23, “pred”: “has-part”, “obj”: 14} Among other things, Topotime part-of relations enable rendering sub-events within parent containers on timelines. Time and place Events and other occurrences are wholly bound to places. Parenthetically, we would argue that places may be best characterized by what has occurred in them. Certainly historical periods are often defined geographically, or are relevant only in particular regions. Some are equally geographic and temporal constructs, e.g. “Pre-dynastic Egypt” (4500-2950 BC), or “The Neolithic Levant.” “Song Dynasty in the Third Imperial Period” is relevant in China and neighboring places, but not elsewhere. Furthermore, simple and complex events all have spatial extension we often want to display on a map alongside a timeline. Period locations in Topotime can be specified with a single spatial location expressed in a standardized format (GeoJSON or WKT), and with an optional name in this form: {“subj”: 23, “pred”: “has-location”, “obj”: {“name”: “Venice“, “geom”: “POINT (45.4375, 12.3358)”, “geomType”: “WKT” } Standards for specifying Places in data objects like these for gazetteers are now emerging, thanks to the coordinating efforts of projects like Pelagios and national historical GIS  projects Great Britain Historical GIS and the China Historical GIS.  Participation Periods having “class”: “Lifespan” can be asserted to participate-in other kinds of periods, such as Event and HistoricalPeriod. Looking ahead Topotime is an open source software development project. We know that many further challenges exist for representing not simply time, but temporality in digital humanities works. Our goal has been to help initiate what will hopefully be an ongoing collaborative process with some concrete steps and functioning software. We are hopeful this work will contribute to the development of interoperable gazetteers of place and period, temporal extensions of the popular GeoJSON format, and improved capabilities for timeline visualizations.  ",
       "article_title":"Topotime: Representing historical temporality",
       "authors":[
          {
             "given":"Karl",
             "family":"Grossner",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          },
          {
             "given":"Elijah",
             "family":"Meeks",
             "affiliation":[
                {
                   "original_name":"Stanford University, United States of America",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-10",
       "keywords":[
          "analysis and visualisation",
          "visualisation",
          "spatio-temporal modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In 2014, the British Museum's department of Learning, Volunteers and Audiences will launch an innovative video conferencing activity: “Roman Britain Treasure Challenge” developed in collaboration with the Portable Antiquities Scheme (PAS) and the Department of Britain, Europe and Prehistory. This new activity will bring archaeology to life within primary schools in the United Kingdom from the British Museum's Samsung Digital Discovery Centre (SDDC)3, a dedicated facility for families and school children to utilise digital technology to enrich their visit. This collaboration between departments is the first of its kind for a digital educational activity in such a venerable institution and could open new avenues for researchers, curators and education practitioners. The activity will reach out to schools that may not have visited or be able to visit the actual site of the British Museum, thus facilitating their participation and broadening scope for interaction with audiences. Building on the digital experience gathered through the delivery of ICT and through the use of the PAS’s innovative and award winning website; curatorial and scientific staff and the world leading SDDC’s museum educational programmes, a new activity has been formulated: Roman Britain Treasure Challenge. This schools session will be aimed at teaching Key Stage 2 children (between the ages of 7 to 11) a variety of life and ethical skills based around the amazing discovery of the Frome Hoard of 52,503 late Roman coins (Bland, Booth & Moorhead 2010) and the working of the legal processes of the Treasure Act (DCMS 1996). The Frome hoard has been acquired by Somerset Museum, and it is now partially on display in their new galleries, whilst the remainder of the hoard is still housed within the British Museum. The department of Conservation and Science’s team of conservators are working on stabilising, cleaning and providing preventative care for many of the coins, before they are returned to Somerset for display. The Portable Antiquities Scheme has been recording small finds of archaeological discoveries in England and Wales since 1997 and its database (accessed online) has records for over 930,000 objects discovered by the general public whilst pursuing their hobbies (for example gardening, walking or metal-detecting) or going about their daily life. These Open Data provided through this system is now providing the basis for a wide variety of research (over 380 projects are now using these data) and innovative visualizations have been produced; for example see the recently released Lost Change application. The close ties between the British Museum’s departments have made it possible to collaborate on this activity. Images, raw data and video footage are combined within a predefined framework to create a structured learning environment. The Roman Britain Treasure Challenge session begins with a discussion based around the question ‘What do you think of when you hear the word treasure?’ Typically this leads to descriptions that, for example include words such as ‘gold’ and ‘how much is it worth?’ (A phrase that is very alien to an archaeologist!) The session aims to dispel this perception, to challenge pre-conceived ideas of Treasure and impress on the young, what is archaeologically important. The session then proceeds through a very brief introduction to the fact that there is a Treasure Act (created in 1996 and replacing the old law of Treasure Trove), a legal definition of Treasure (note the capital T as Treasure is a concept) and a strict procedure that is followed to determine if a find is Treasure. In 2012, 969 cases of Treasure were reported to the British Museum’s Treasure Registrar and 26 in Wales (DCMS 2013); therefore this process is one that could conceivably be experienced by session participants in the future, if they were lucky enough to discover Treasure.   Within the session, children are given simple, but structured tasks based around the discovery process; for example choosing who will comprise the excavation team, formulating and delivering a security strategy and researching the coins to determine which period the hoard is likely to be from (using replica coins posted to the school in advance). Taking the example of choosing the excavation team further; this is part of a wider activity to choose three teams; the excavation and discovery team, protecting the artefacts team and a research and display team. The list of possible candidates include people such as the finder, an archaeologist, the PAS Finds Liaison Officer, an illustrator, conservators and the museum director. Using an information sheet and knowledge gleaned from previous discussions the children (in their own class teams) must make the decision of who to allocate to each team. Who is vital for each stage of the artefacts journey? Who must not be left behind? It is dependent on the children to use knowledge gained during the session, discussion within the group and decision making skills to ensure that best people are tasked with the care of the Frome Hoard.   The session also includes a hands-on element as part of its interactivity. The school teacher is given a choice between two activities based around the study of coins (or numismatics); they can either decipher and interpret the inscription on a paper or digital representation of a coin from the hoard or replica handling coins are sent to the school and the children have to examine them and attempt to place them on to a timeline in either date or issuer (usually an Emperor) order. The session uses a variety of ICT equipment including a dedicated equipment trolley which has been configured with new equipment and set up uniquely for this session with the video conferencing equipment and other equipment such as a ‘visualiser’ for close up viewing of real Roman coins from the Museum’s handling collection (provenance is known for these.). The participating schools will use their own ICT equipment in either their ICT suites or within classrooms. This leads to a fully-fledged activity which will bring the archaeological process to life using presentations, audio-visuals (such as pre-recorded video using trained actors and archaeologists), replica and real coins and a trained museum educator to deliver the session. Using images that the PAS has disseminated via Flickr and also through its own database, children can take home information about museum and archaeological content to display in their school, or at home and easily query our resources in external contexts (for example one can search via post code or upon their local environs). This paper will discuss how this activity was developed over the previous year (drawing inspiration from the National Space Centre’s educational programmes7, and further afield) with specific reference to teaching methods, curriculum suitability, equipment selection (in conjunction with the SDDC's commercial contracts that ties us to using Samsung manufactured and branded equipment), archaeological practice and the legal aspects of the Treasure Act (1996). It will show how some intrinsically complex situations are broken down for the younger audience, how some practical choices had to made to enable the activity to be executed. It will also discuss whether this initiative could be seen as a success, whether it could be replicated in other museums or archaeological facilities, whether it could be delivered to different audiences (for example with community archaeology groups or university students) and approximately how much staff time and budget was expended on development (some costs cannot be calculated as services are provided as ‘in-kind’). This paper will also discuss how this type of session is evaluated and subsequently improved via feedback from participants (children and teachers) and observation from British Museum staff and Portable Antiquities Scheme employees. It will also show how this improvement process impacts on both staff and delivery time in a hectic term time schedule (planned months in advance.) This type of activity is a new venture for the British Museum, at the time of writing this abstract, it is not known how successful this session will have been; presenting the results will be a challenge to the authors and the proposed content will be subject to change.     ",
       "article_title":"Treasure Challenge: an archaeological video conferencing journey",
       "authors":[
          {
             "given":"Daniel Edward John",
             "family":"Pett",
             "affiliation":[
                {
                   "original_name":"The British Museum, United Kingdom",
                   "normalized_name":"British Museum",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00pbh0a34",
                      "GRID":"grid.29109.33"
                   }
                }
             ]
          },
          {
             "given":"Katharine Louise",
             "family":"Kelland",
             "affiliation":[
                {
                   "original_name":"The British Museum, United Kingdom",
                   "normalized_name":"British Museum",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00pbh0a34",
                      "GRID":"grid.29109.33"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "creative and performing arts",
          "programming",
          "archaeology",
          "libraries",
          "GLAM: galleries",
          "archives",
          "virtual and augmented reality",
          "museums",
          "including writing"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Collation is an important step in textual criticism and it is most often an arduous task for most scholars involved in scholarly edition. Unsworth includes “collation” as one of the scholarly primitives which have been basic to scholarship across eras and media. Textual variation has been a pervasive problem affecting literary text since the invention of writing. It can arise in two forms - either due to repeated copying of a manuscript such as the variants in the First Folio of Shakespeare or those advertently inserted by the author/copyist such as the changes made in Mary Shelley’s Frankenstein. In the first case collation aids the scholar in generating a critical edition. In the latter case, collation can help the scholar understand the author’s purpose. Finding variations is important for research in bibliography and book history as well. Most of the focus in digital humanities until now has been on making documents available digitally. Much less focus has been on actually supporting the process of scholarly research. The area of collation too awaits a lot more from technology. In the late 1940s Charlton Hinman invented the Hinman collator. Using optical means, it allowed manual comparison of separate copies of a text in order to detect differences. Descendants of the Hinman, collators like the Mcleod collator, the Lindstrand collator and the Hailey's Comet, are still used today. Mechanical collators take time to setup correctly, cannot be used on varying fonts, can damage the books, and are expensive and not very portable. Another approach is to perform collation using tools like Juxta or Collate X on text obtained by transcription or by OCR. This method is flawed by the limits of OCR technology and human error. The Sapheos project approaches the collation problem interestingly by attempting to unwarp the pages and registering them using SIFT key points, but this approach will fail if the text differs significantly. Most of the tools used today are standalone which inhibits collaboration. Also scholars prefer original copies or facsimiles instead of OCR or transcription versions and most of these tools don’t support that. This work proposes to address these problems. A prototype of the virtual Hinman (vHinman) collator was created and user-evaluation was conducted amongst scholars experienced with collation work. Image-matching algorithms along with context information are used to match words and the tool was integrated into the creativity support environment CritSpace. Moreover, CritSpace provides the functionality to easily extend the tool to support collating multiple (>2) copies. Methodology We developed and evaluated some approaches towards comparing page-images. Some methods worked well only when the images are pre-registered and won’t be practical if the pages have different alignments and different font-sizes, so we used image processing techniques and image matching algorithms to perform comparison of images . We followed an approach similar to to compare word images amongst two scanned pages We find the unique corner points on the individual words and find a feature vector for these points. Then through clustering we assign an id to each point. Then we are able to use sequence of ids for a word to compare with other words for similarity. Then we take into account the context of the words to aid in finding the exact match for the words.   Integration into CritSpace Peter Robinson notes that the greatest effect of the digital revolution is that it is empowering a new model of collaboration among scholars, and between scholars and readers. In sync with this, the goal of this project is to integrate the collation tool into CritSpace to greatly increase its usefulness. CritSpace [Figure 1] is a web based creativity support environment that implements spatial information management strategies to assist scholars in their open-ended research tasks.    Fig. 1: Sample workspace with a text panel, image panel and facsimile viewer  The user-interface explained in figures below was planned to generate an effortless user-experience for digital scholars.   Fig. 2: Figure 2 Screenshot highlighting the differences with green boxes around the words. These are displayed when the user clicks on \"Differences\" button. Notable differences like missing hyphens are outlined as well as the end of the page.    Fig. 3: Figure 3 Screenshot demonstrating the tracking feature. When the user hovers the mouse over any block of word its corresponding match is highlighted in the other page in red. The ones that have already been checked are turned black    Fig. 4: Figure 4 Screenshot of the annotation feature. On enabling annotation mode, the user can select a word and a text box will appear. The text is displayed above the word every time annotation mode is set. A sample use-case has been outlined.    Fig. 5: Screenshot of collation output of two 17th century versions of The Late Tryal and conviction of Count Tariff    Fig. 6: Font variations in two versions of word \"French\"    Dataset  We tested the vHinman tool on various scanned texts available on the Internet Archive website and within TAMU collections. Some of them are digital copies of SherlockHolmes , copies of early printings of Donne’s Poems (1633 and 1635) and copies of The Late Tryal and conviction of Count Tariff. These books have many print and edition variants; for the pages of SherlockHolmes tested, the tool shows an average accuracy of 95% in tracking the matches.   User Evaluation Five subjects were chosen to participate in this study which was a mix of semi-structured interview regarding the experience of scholars on collation, followed by a demo of the prototype and then by questions about the feedback of the tool and suggestions for its improvement.    ID   Area of interest   Career Stage     S1   Eighteenth century literature   Senior     S2   Bibliography   Senior     S3   Scholarly editing   Senior     S4   Scholarly editing   Senior     S5   Book history, Linguistics   Senior    Most of the subjects had prior experience with collation either in their scholarly research or for some classroom activities. Some of the subjects had experience with mechanical collators or text based collators. Many of the subjects still prefer the paper-based manual collation method because they find the supporting tools either inaccurate or too cumbersome to use or both. The need of collation in the subjects’ research varied from the traditional scholarly editing process to bibliographic research and book history research.  S4 pointed out that he didn’t have the resources to do the transcription for each of the documents he works on and also said that they are prone to errors. S1 pointed out the need to be able to find differences in font-styles, ligatures like the move from using the long “s” to the current “s”. S2 liked the idea of integrating the vHinman into CritSpace which can foster collaborative work. She also liked the idea that the tool could have multiple panels (more than two). She pointed out that while supporting multiple images we can display the n-images in the form of medium sized thumbnails as is seen in “Google images”, where the scholar can select any two panels to collate at a time. She noted that the tool could bring forward new uses of collation and could get collation adopted by scholars who currently don’t focus much on it attributing the manual effort and inherent inaccuracies in the current method. S5 suggested a novel use of the tool in verifying the authorship of a poem.   Some of the subjects felt the need to be able to point small differences like punctuation because this is important for a critical edition. Although our tool currently only supports identifying word differences, punctuation support can be added. S4 felt that the current implementation can quicken the collation process by addressing textual differences while punctuation can be addressed separately. The subjects in general liked the ability to use the original facsimile of the document via the tool rather than a transcription or a somewhat inaccurate OCR version of it.   Conclusion This work has investigated the way humanities scholars perform collation work and what role collation plays in their research output. Collation is known to be a laborious and monotonous task unaided by technology so far. To address this problem, a prototype has been developed to perform collation in an automated manner. Image matching techniques are employed in building this prototype, so that the scholars can use the original facsimiles of the documents. The tool was integrated into CritSpace and will benefit from the collaborative environment. A user evaluation was conducted with experienced scholars. In summary, the tool looks very promising to the scholars and also has a high accuracy rate for the books tested so far. Thus this kind of tool can save a massive amount of time for scholars and set up a paradigm of digital collation encouraging even more scholars in finding new uses of collation in their work.   It extends the Hinman’s principles by allowing collating multiple editions of a book in addition to multiple copies of same edition having minor differences. Since it is has application in creating a critical edition, bibliography and book history research, this tool has the capability of gaining widespread adoption.   Future Work Beyond printed material, it will be interesting to evaluate the tool for handwritten documents and make it robust for such documents. Also it will be great to test the tool for non-English documents. We can try out different visualization formats and different ways the scholars can use the output in their work. A detailed usability study can be conducted where scholars can perform some real collation work on few pages and compare their traditional method and the vHinman. Also the accuracy could be tested for warped images as most of the unobtrusive scanning methods produce some warping on the images.  ",
       "article_title":"Aiding Modern Textual Scholarship using a Virtual Hinman Collator",
       "authors":[
          {
             "given":"Gaurav",
             "family":"Kejriwal",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Furuta",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Ryan ",
             "family":"Olivieri",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "image processing",
          "sustainability and preservation",
          "authorship attribution / authority",
          "repositories",
          "digital humanities - facilities",
          "archives",
          "scholarly editing",
          "text analysis",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The content of this presentation develops from previous research and writing on the “Glass Cast,”a prototype for mapping and visualizing complex knowledge networks in which time is crucial, and on “PlotVis,” an interactive visualization resource for displaying XML-encoded fictional narratives that departs from ways of modeling narrative typical of literary pedagogy (see, e.g., Dobson, Michura, Ruecker, Brown, and Rodriguez 2011). Digital humanities scholars (e.g. Zepel 2013) have argued that by thinking about visualization as a tool, we can gain insight into visualization’s purpose in DH scholarship, particularly as a form of “visual thinking.” This perspective on visualization informs the approach we adopt in this presentation. Specifically, we describe aspects of the development of two visualization prototypes, offering a detailed overview of how these prototypes work. We also theorize visualization in terms of the concept of metaphor, examining the implications for visualization of the metaphors implicit  o the respective structures of our experimental prototypes. How does metaphor shape and perhaps even constrain perceptions of the purpose of the two visualization prototypes we discuss? In attending to questions of metaphor and visualization, we contribute to ongoing theorization of the role of visualization, broadly conceived, in digital humanities scholarship,    Fig. 1: Rendering from 3D Model, \"Glass Cast\" Prototype (Left); Still Image of XMLEncoded Literary Narrative from \"PlotVis\" Prototype (Right).  Elsewhere (Peña and Dobson 2013), we elaborate on the conceptualization of the Glassn Cast as part of an effort to trace the usage of “visual literacy” and related notions through time and across different knowledge domains. As we explain, the aim of this experimental prototype is to represent networks chronologically and to facilitate the examination and exploration of these networks from different views in a three-dimensional visualization environment (see Figure 1). The Glass Cast serves as a tool for tracking the origin and mobilization of concepts across time and through different disciplinary fields, a key challenge in humanities and social sciences scholarship. Enabling the examination of knowledge networks along the lines of time, discipline, and connection between texts and authors required a three-dimensional representation. In order to facilitate such a display, we adopted the metaphor of the “glass cast,” thinking in particular about those types of casts in which three-dimensional figures have been impressed in the core of geometric shapes such as cylinders and polyhedrons. The use of such a metaphor in the Glass Cast visualization is intended to structure user interactions around the kinds of experience typical of interactions with the artifact from which the interface take its name. Yet the metaphor of the glass cast is also meant to leverage the understanding of the affordances of the prototype for those who interact with it; the Glass Cast namely (but not exclusively) provides not only a general reading of the data that is showcased, but also permits the display of another level of information depending on the side of the prism that faces the reader. Metaphor is thus germane to any understanding of the purposes of the Glass Cast as a scholarly visualization tool. The Glass Cast aims to showcase abstract relationships between data that are hardly (or impossibly) acquirable from non-visual means, whether because the nature of the data is different and therefore these relationships are not explicit, or otherwise. This property of some visual resources to communicate relationships and the “intellectual skill” required to read or interpret them is what Balchin and Coleman (1966) defined as graphicacy. Shaped through the chosen metaphors, these abstract relationships reveal new layers of information and patterns from either new or already existing data. In the particular case of the Glass Cast, we see this metaphor as a tool to (amongst other possible outcomes) seize the possibilities that the new media gives us to look back and through disciplines in a single artifact. In this context, visualization provides a powerful means of displaying the complex processes through which new disciplines form and at the same time constitutes a rethinking of disciplinarity. PlotVis, which we developed as part of an interdisciplinary project on reading, writing, and teaching complex fiction, emerged out similar concerns with disciplinarity and, in particular, with disciplinary practices. Specifically, the prototype was designed to augment conventional approaches to teaching the concept of plot in fiction, which have tended to rely on a static visualization known as Freytag’s Pyramid. Unlike this static, linear representation in print media, PlotVis, a digital tool, allows users to model and interact with XML-encoded literary narratives in three dimensions. A number of different metaphors (e.g. the Fibonacci Series, tubes, building blocks, cells, the cabinet of curiosities) dictate the structures of the multiple views that can be used to visualize encoded text (see Figure 1). Incorporating multiple metaphors into the PlotVis prototype to facilitate a range of views on a single encoded text (or corpus of encoded texts) underscores an important point, which is that the ways in which plot is visualized informs and sometimes constrains a reader’s understanding of the very concept of plot. Ways of modeling narrative that have traditionally relied on the Cartesian graph, wherein time is plotted on the x axis and the fortunes of the hero or the disposition of the action (whether it is ‘rising’ or ‘falling’) is plotted on the y axis, have for example led many readers to expect literary narratives to conform to this sequence of events. Given this consequence of visualization, PlotVis was (like the Glass Cast) developed to radically alter and, importantly, expand user engagement in order to do justice to the complexity of plot as a central concept of literary criticism. This presentation opens up an opportunity to elaborate on the influences of metaphor on visualizations in PlotVis. While  he metaphors serve as tools for displaying different views on the same text, how might PlotVis metaphors structure users’ understanding of the goals of literary criticism? In conclusion, in the case of both prototypes, we consider how visualization mediates participation within a scholarly discipline and ask, “What does metaphor have to do with this?” By participation, we mean the kinds of scholarly engagement a visualization interface enables and how such engagement might be informed by an interface’s metaphor entailments (see, e.g., Kerr et al., 2013). This presentation thus provides a forum for exploring the relationship not only of metaphor to visualization, but also of visualization to disciplinarity.  ",
       "article_title":"On Metaphor in Text Visualization Prototypes",
       "authors":[
          {
             "given":",Ernesto",
             "family":"Peña",
             "affiliation":[
                {
                   "original_name":"University of British Columbia, Canada ",
                   "normalized_name":"University of British Columbia",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03rmrcq20",
                      "GRID":"grid.17091.3e"
                   }
                }
             ]
          },
          {
             "given":"Monica",
             "family":"Brown",
             "affiliation":[
                {
                   "original_name":"University of British Columbia, Canada ",
                   "normalized_name":"University of British Columbia",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03rmrcq20",
                      "GRID":"grid.17091.3e"
                   }
                }
             ]
          },
          {
             "given":"Teresa ",
             "family":"Dobson",
             "affiliation":[
                {
                   "original_name":"University of British Columbia, Canada ",
                   "normalized_name":"University of British Columbia",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03rmrcq20",
                      "GRID":"grid.17091.3e"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "knowledge representation",
          "visualisation",
          "literary studies",
          "interface and user experience design",
          "relationships",
          "graphs",
          "interdisciplinary collaboration",
          "xml",
          "encoding - theory and practice",
          "information architecture",
          "text analysis",
          "networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Our paper explores the role of student participants in outreach and advocacy for digital humanities centers. Although discussion of undergraduates in digital humanities generally focuses on classroom research activities (Croxall 2013), we define “student participant” broadly, including students who are employed by, or complete internships within, a digital humanities center. Because student participants are highly responsive to outreach efforts, and can themselves support outreach by serving as advocates for the center (and digital humanities scholarship in general) during their student years and beyond, cultivating student participants allows centers to enhance existing outreach efforts, so much so that the benefits for students and the center may justify refocusing energy and resources to better support student engagement. At the same time, digital humanities centers must address institutional barriers and ethical concerns that emerge when students are involved in research. Our talk will provide examples of the rationale and strategies that the IDHMC (Initiative for Digital Humanities, Media and Culture) is developing to strengthen student involvement in digital scholarship. Recognizing that funding and other factors create obstacles to student involvement, we are working towards a flexible model that can be adapted to suit local circumstances.  Although discussions of “outreach” for the digital humanities often focus on publicizing projects (see Brennan 2013), centers are expected to perform generalized outreach to promote the center and its work and to make research, tools, and training available to academic and general audiences (Nowviskie 2011). While these forms of outreach are important, they create considerable demands upon staff (Nowviskie 2010; Ramsay 2010) and are limited in their reach and effectiveness. Bringing traditional humanities scholars into digital humanities scholarship is time and labor intensive, and projects and tools often target specific audiences, leaving out constituencies that are institutionally or strategically important for the center’s growth. Student participants both supplement and extend these forms of outreach.  Focusing outreach efforts on student participants may be more effective than targeting humanities faculty who are not already engaged in digital scholarship. Unlike faculty, whose interest in new technologies is constrained by multiple factors (Wiberly and Jones 2000; O’Donnell 2009), students have time to invest in learning new technologies and approaches and are less invested in traditional disciplinary paradigms. Before graduation, student participants enhance outreach by bridging disciplinary and institutional divisions and communicating their digital humanities interest to other students and faculty. After graduation, student participants may still contribute by incorporating digital tools and collaborative practices into their professional activities, serving as intermediaries between the center and external partners, and continuing to participate in scholarship through crowdsourcing or other means.  The IDHMC uses grant funding to support student involvement in the classroom and as employees, and is hoping to secure funding for a pilot internship program. In 2012, through an internal (Tier One Program) grant, Laura Mandell developed an interdisciplinary, stacked (undergraduate and graduate) course that involved students in designing the IDHMC’s Humanities Visualization Space (HVS). That course has now become a permanent offering. But to reach more students from multiple disciplines, the IDHMC invests in student learning beyond targeted digital humanities classes. Mandell’s eMOP (Early Modern OCR Project) Mellon grant includes graduate and undergraduate student workers who have made important contributions to the project and to the digital humanities community (Torabi, Durgan and Tarpley 2013). Most recently, Mandell, Ives and Earhart targeted multiple forms of student engagement in an internal grant in support of the development of ARC (Advanced Research Consortium, idhmc.tamu.edu/projects/arc/) and the HVS. To infuse digital humanities scholarship in the wider curriculum, this grant funds the creation of workshops and class activities that faculty can integrate into existing classes, as well as a research competition for students who have used the ARC data and the HVS in their projects (we will provide links to these materials in our presentation). Finally, IDHMC is piloting an unpaid undergraduate internship program (idhmc.tamu.edu/blog/2013/06/21/idhmc-undergraduate-internships/) through which students can work on any IDHMC affiliated project. Through employment and internships, we are drawing students from departments that do not offer DH coursework or research opportunities, increasing our impact on campus while creating valuable learning experiences for a variety of students.   To pursue these goals, we continually navigate institutional and disciplinary barriers, including faculty resistance, administrative and curricular roadblocks, and funding issues. The difficulties inherent in adding new courses to the official curriculum make other means of outreach to students especially attractive and valuable. We hope that we will be able to offer stipends to future interns, perhaps ultimately creating something like DePauw’s ITAP (Information Technology Associates Program), an internship program in which liberal arts undergraduates are given a year of technical training and then placed in academic or administrative internships. Programs such as ITAP demonstrate that incorporating undergraduate students into digital humanities research and outreach is a strategy that is neither limited to digital humanities centers such as IDHMC or to research universities, nor dependent upon external grant funding (though there is no doubt that the necessity of finding some source of funding for student researchers is a challenge across the board). Such a strategy does, however, require acceptance of a capacious definition of what “counts” as digital humanities, and resistance to the idea that a valuable student experience can only be obtained through exposure to the kinds of digital humanities work that require heavy institutional investment in equipment and staff support. Ideally, students working at IDHMC will experience a variety of modes of digital humanities research, from digital archives (such as the Cervantes Project), to tool development (via the OCR testing and development that is part of eMOP), to academic instances of social media (the Anachronaut Facebook game for OCR correction), to humanities data visualization. Students working at smaller institutions with fewer resources will not have as many options as we do, but the necessity of limiting hands-on experience to certain kinds of projects does not prevent faculty from exploring other forms of digital scholarship through other means, including collaborative arrangements with other institutions that offer different digital humanities emphases. Regional or national partnerships that facilitate digital project information sharing (something as simple as having faculty involved in a project Skype in to a class) are one way to expand student involvement beyond a single institution’s resources and expertise; over time, more intensive collaborations – a “spring break” research experience at another institution, for example -- might be developed.  More important still is providing the kind of digital humanities acculturation that Geoffrey Rockwell and Stefan Sinclair describe, which students learn to “work in interdisciplinary teams, apply digital practices to the humanities, manage projects, [and] explain technology and build community” (Rockwell and Sinclair 2013). The goal of acculturation is one that can be pursued in many ways and across many institutional structures. For the purposes of outreach, it is extremely valuable given its relevance to students’ career paths.  While we work to develop a funded internship program, we offer independent studies to all interns so that they receive formal academic credit for their work. As a general practice, we encourage all affiliated students to present or publish their work and we include them in our own presentations and publications (Ives et al; Earhart; Mandell 2013). We also feature student contributors in publicity for IDHMC and its projects. While many of these practices are (or should be) universal, articulating them is important, both to insure a positive experience for students and to equip them to serve as advocates for IDHMC and digital humanities generally. The overall goal of our work with students is to enact the values of digital scholarship formulated by Lisa Spiro: openness, collaboration, collegiality and connectedness, diversity, and experimentation (Spiro 2012). We want to instill an ethical perspective that is both specific to digital humanities and generalizable to students’ future careers. Expanding upon studies that emphasize collaboration and cross-disciplinary learning within the classroom (Norcia 2008), we aim at, in Julia Flanders’ words, fostering “a professional academic ecology” that illuminates “the diversity of working roles that contribute to the production of scholarship” (Flanders 307). Our focus on multiple forms of student participation allows us to define the ethos of collaboration broadly, emphasizing respect for and recognition of contributions across disciplinary boundaries as well as educational/professional roles (student, employee, faculty, staff). Successful outreach can be defined in many ways, but the most meaningful measure may well be the extent to which students carry this ethos with them when they leave IDHMC.  ",
       "article_title":"Student Collaborators in Digital Humanities Outreach and Advocacy: Strategies and Examples from the IDHMC at Texas A&M University",
       "authors":[
          {
             "given":"Maura",
             "family":"Ives",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Amy",
             "family":"Earhart",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Grumbach",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Laura ",
             "family":"Mandell",
             "affiliation":[
                {
                   "original_name":"Texas A&M University, United States of America",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "digital humanities - institutional support",
          "other",
          "interdisciplinary collaboration"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction Physical landscape models have been used for hundreds of years as a means of offering people privileged overviews of landscapes, allowing them to effortlessly appreciate spatial relationships between places of interest. The role of digital technology for capturing, preserving and analysing existing models has been demonstrated by Niederoest (2002) using photogrammetric techniques, focussing on the model constructed by Franz Ludwig Pfyffer in the late eighteenth century. This paper will describe the use of digital scanning, processing and 3D printing for exploring a landscape model that no longer exists but where a large number of negative moulds remain. It offers an example of the use of capture and presentation technologies to produce an exhibit from material that would otherwise be inaccessible to audiences. The specific case study is a physical landscape model of the English Lake District created by Thomas and Henry Mayson in 1875 which gave visitors to the town of Keswick, Cumbria an unprecedented view of the landscape they were about to explore. Huge efforts went into creating this model which claimed to faithfully represent the contours and other details of the Ordnance Survey maps which had recently been surveyed but which were not commonly used by the public at that time for recreational purposes. The model is believed to have been displayed until the 1960s but all that now remains are some mouldings created ‘for future use’ (found in storage in 2012) along with some other material including posters, the commissioning letter and some original map sheets (Figure 1).   Fig. 1: Remaining archival material relating to Mayson’s Ordnance Model of 1875.  The Mayson model was larger and more detailed than others displayed at that time and became a popular tourist attraction. A poster advertising the model reads “The model has been constructed mathematically from the Ordnance Survey… Parties visiting this model will see the correct topography of the Lake District, and can thereby readily plan either long or short excursions as time will permit. They will also gain a better idea of the whole of the Lake Country than is to be obtained from any other source”.   Background Whilst very effective cartographic relief representations were available in the nineteenth century, particularly in alpine regions as described by Collier et al (2003), the majority of visitors to the English Lake District may at best have used a guidebook containing small maps but were unlikely to have had a good appreciation of the landscape setting of the town they were visiting. Physical landscape models have often been used in public settings to help convey a sense of spatial context for visitors, using geographical features such as mountains, lakes, buildings and roads to provide a frame of reference, and Keswick in the nineteenth century was no exception. Whilst some models from this period still exist, including the Flintoft model made in 1834, several of the larger models including that of the Mayson brothers, have been destroyed. In the case of the Mayson model there is an opportunity to reconstruct parts of the model due to the discovery of a set of negative moulds. The use of laser-scanning technology as reported by Terdiman (2012) in relation to the Smithsonian archive clearly offers the fidelity of capture necessary to extract the detail present in the Mayson moulds. In addition to scanning the moulds, digital geo-processing and 3D prototyping will enable us to explore the process of model building and the relationships between the model and the maps on which it is said to have been based. This broader view on the potential for physical fabrication to add richness to digital humanities investigations is seen in Elliot et al (2012) and Sayers et al (2013).    Research challenges Through the development of a workflow combining scanning, processing and physical fabrication the project explores the technical aspects of physical model building and in particular the relationship between cartographic survey and physical model construction. The historical context of the model as part of the visitor experience, before Ordnance Survey mapping saw popular recreational use, is of particular interest. A great challenge is to apply technology in a creative manner in order to re-present the collection of objects in a form that is appropriate for public viewing, conveying both the fidelity and scale of the original model, but also advancing our understanding of the process of model building and the original context of its use as an informative visitor attraction.   Digital reconstruction After recovering and cleaning the collection of moulds each item was scanned using a FARO Laser ScanArm V3 and the resulting point clouds tidied within the PolyWorks software. A program was written to convert each point cloud into a form suitable for processing within the ArcGIS Geographical Information System (GIS) package, including the inversion of the vertical dimension. The points were then processed to produce continuous Digital Surface Models from which hillshaded images were derived to assist in locating each mould for the purpose of geo-referencing, as the location and orientation of most of the moulds was not known. From initial processing within the GIS there is some evidence of slight vertical exaggeration when comparing the model against modern elevation data however this will need a full and systematic study and will form part of the ongoing research agenda. Selected tiles were sent to a CMS Athena 5-axis CNC milling machine to produce positive reconstructions of the relevant mould. The generalised workflow is shown in Figure 2.   Fig. 2: Digital Reconstruction workflow.    Designing a new visitor experience The knowledge gained about the Mayson model both in terms of the process of construction and the context of its display will form part of a public display in Keswick museum from May 2014. The challenge for re-presenting this collection is to convey the detail of the original model, the process through which the original model was made and how this relates to modern forms of survey, mapping and model building, helped by the active involvement of the Ordnance Survey, on whose maps the original model was based. Figure 3 shows some key elements of the display, including an example of an original mould, a 3D fabrication of the scanned data from that original mould with map data projected down onto it (Priestnall et al. 2012) including modern cartographic representations but also the 1860s map (Figure 3, centre), attempting to emphasise the data from which a model has been derived but also how it could be re-presented and re-interpreted (Lorenz, Schofield and Noond, 2006). The scale and context of display of the original model will also be presented (below right) along with a temporary installation featuring ten fabricated tiles distributed around the gallery space in their true to-scale geographical positions to convey the size of the original model.   Fig. 3: Re-presenting the collection    Conclusion Digital technologies have allowed us to explore a nineteenth century physical landscape model that no longer exists. We have gained an insight into its creation, its detail and accuracy, and the importance of its role in the visitor experience at a time when maps were less commonly used for recreational purposes. We have developed workflows to combine 3D scanning, digital processing, 3D fabrication and projection to produce an exhibit from historical objects that would otherwise be inaccessible to the general public. In this case not only has the technology been of value to preserve a collection but it has enabled us to digitally reconstruct new forms of information from the artefacts that remain. Particular issues arising from the need to apply geographic coordinate information to the scanned objects were handled within a GIS and work is ongoing to compare the historic terrain model with modern digital equivalents. The overall findings resonate with ongoing research into the development of projection-enhanced physical landscape models for public display and their power for promoting an awareness of spatial context in viewers of those models. The techniques could also contribute to more general design guidelines for using technology to promote an awareness of spatial context and to support the process of interpretation and reconstruction where the geographic landscape model is the backdrop but not necessarily the focus of attention.    Acknowledgements Charlotte Stead and Pat Maskell at Keswick Museum; Nikki Tofts at Helena Thompson Museum; Glen Hart, Head of Research at the Ordnance Survey; Sarah Beardsley, Scott Wheaver and James Hazzledine at the Centre for 3D Design, School of Architecture and the Built Environment, the University of Nottingham; Sally Bowden at the Centre for Advanced Studies, the University of Nottingham; Frank Priestnall; and Ian Conway, School of Geography, the University of Nottingham. This work has been supported by the AHRC Creative Economy Knowledge Exchange Project “Archives, Assets and Audiences”.  ",
       "article_title":"Reconstruction and Display of a Nineteenth Century Landscape Model",
       "authors":[
          {
             "given":"Gary",
             "family":"Priestnall",
             "affiliation":[
                {
                   "original_name":"1School of Geography, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          },
          {
             "given":"Lorenz",
             "family":"Katharina",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Centre, Department of Classics, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          },
          {
             "given":"Heffernan",
             "family":"Mike",
             "affiliation":[
                {
                   "original_name":"1School of Geography, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          },
          {
             "given":"Bailey",
             "family":"Joe",
             "affiliation":[
                {
                   "original_name":"1School of Geography, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          },
          {
             "given":"Goodere",
             "family":"Craig",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Centre, Department of Classics, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          },
          {
             "given":"Sullivan",
             "family":"Robyn",
             "affiliation":[
                {
                   "original_name":"Digital Humanities Centre, Department of Classics, the University of Nottingham, United Kingdom",
                   "normalized_name":"University of Nottingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01ee9ar58",
                      "GRID":"grid.4563.4"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "GLAM: galleries",
          "libraries",
          "archives",
          "scholarly editing",
          "museums",
          "bibliographic methods / textual studies"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Introduction On July 19th, 1848, 300 concerned United States citizens gathered in Seneca Falls, New York, for the women’s rights convention that would culminate in the signing of the Declaration of Rights and Sentiments, the first major document (in the US) to call for women’s right to vote. In The North Star, Frederick Douglass, the former slave turned abolitionist, extolled the event as a “grand movement for attaining the civil, social, political, and religious rights of women” (1848). In the Oneida Whig, the same event was ridiculed as the “most shocking and unnatural event ever recorded in the history of womanity” (1848). As demonstrated by these contradictory accounts, published opinions varied greatly -- about the women’s rights movement in the nineteenth-century United States, and about current events generally conceived. Large-scale digitization projects have increasingly enabled humanities scholars to search newspapers, such as those just cited, for significant words and phrases. But exploring more open-ended questions such as, “How did the discourse surrounding women’s rights in the United States change in the wake of the 1848 Seneca Falls Convention?” or “Did the women’s rights movement borrow language from the nation’s contemporaneous anti-slavery campaign?” remains a challenge. Synthesizing current research on exploratory data analysis with techniques from the fields of computational linguistics and data visualization, we propose a new set of methods to assist humanities scholars in computationally-assisted exploratory research.    Background and Overview Exploratory data analysis (EDA) has played a fundamental role in quantitative research since at least the 1970s (Tukey 1977). In comparison to formal hypothesis testing, exploratory data analysis is more open-ended, and is meant to help the researcher develop a general sense of the properties of the dataset before embarking on more specific inquiries (Russell, Stefik, Pirolli, and Card, 1993). EDA typically combines visualizations such as scatterplots and histograms with lightweight quantitative analysis, serving to check basic assumptions, reveal errors in the data-processing pipeline, identify relationships between variables, and suggest preliminary models. More recently, Andrew Gelman (2004) has argued that EDA should be interwoven with formal statistical modeling, facilitating an iterative design process driven by experimenter insight. The questions about women’s rights, posed above, suggest the potential of EDA for humanities research-- a possibility also noted by Muralidharan and Hearst (2012). That team employs automatic syntactic analysis to identify and visualize recurring grammatical patterns, which, when combined with document metadata, reveals insights at the sentence level. By contrast, we combine metadata with techniques such as topic modeling in order to reveal insights at the document level. Inspired by the increasing use of topic models to make literary and cultural arguments (Underwood 2012, Rhody 2012, and Jockers 2013), we ask how the exploratorythematicanalysis of documents might be incorporated into the initial phase of humanities research. Our approach encompasses both traditional topic models and innovative visualizations, as well as alternative computational techniques targeted at the questions that topic modeling raises but leaves unanswered. By designing new visualizations and text-mining algorithms within the context of a specific, humanities-driven research effort, we hope to prototype a new mode of multi-disciplinary scholarship that will facilitate the iterative research methodology advocated by Gelman (2004). Specifically, we aim to facilitate the thematic exploration of document archives as a precursor to more informed keyword searching, more sustained close reading, and more systematic evidence gathering.   User Scenario and Interface Prototypes Our focus is on a set of abolitionist newspapers from the nineteenth-century United States, in which antislavery advocates mounted moral, social, and political arguments in favor of emancipation. These newspapers present a particularly compelling dataset for thematic analysis, as similar ideas were purportedly framed differently by (and for) women and men (Dudden 2011). Here, we focus on one newspaper, The Anti-Slavery Bugle, published in New Lisbon, Ohio, between 1845 and 1861. Significantly, it was the source of much reprinting (Golden 2013), and underwent several shifts in editorial control.  Standard LDA topic analysis (MALLET; McCallum 2002) with 100 topics and standard parametrization reveals a number of topics that might intrigue a scholar in the initial phases of research, including:  T40: states state law constitution tho government power united laws congress rightspeople con ohio tion act union question property T56: indians indian tribes tribe chiefs frontier dian treaties tiger hawk antelope annuity fiscal lllack hyenas tigers dians avalanche savages T59: woman women rightshusband wife sex sho marriage property married mrs female legal sphere equality estate social duties sexes  Topic 59 (T59) suggests that the Bugle may offer insight into the relationship between the antislavery movement and the nascent drive for women’s rights. The accompanying metadata reveals that the newspaper was co-edited by a woman between 1845 and 1849; however, this topic peaked in the late 1850s (a time when the women’s rights movement was ascendant). At this point, we reach the limit of what can be learned from a topic model alone. We cannot easily answer, for instance, how the treatment of this topic in the Bugle might have differed from that of other newspapers; whether the editors of the Bugle were early advocates for the women’s rights movement; or whether the peak in the late 1850s followed a national trend.    The above screenshot documents a prototype interface for the visualization and analysis of topic models that can begin to answer these questions. We apply a dust-and-magnetvisualization (Yi et al., 2005), in which user-selected topics exert a magnetic “force” on individual issues of the newspaper (represented as “dust”). The temporal trajectories of several newspapers are shown as “dust trails” in the visual space, with colors indicating the terms of different editorial teams, and with the Bugle highlighted so as to facilitate comparison with contemporaneous newspapers. Next, we address the topics themselves. In such models, topics are defined by sets of words, with the assumption that each word has a single meaning across all usage contexts. However, much humanities scholarship entails a sensitivity to shifting meanings and uses. A scholar may wonder, for instance, how women’s “rights” (as indicated by the keyword in T59) were described in relation to the legal “rights” featured in T40. She may ask if the rhetoric of one borrowed from the other, or if the use of the word “rights” changed when it was employed to describe women’s vs. legal rights. Again, the scholar seeks to know more than what can be inferred by LDA alone. We propose to link LDA’s high-level thematic analysis with visualizations that drill down to the level of individual examples. Building on the traditional keyword-in-context (KWIC) models, we are developing a computational algorithm for selecting contexts that are both strongly associated with each topic of interest (for example, the contexts for “rights” in T40 and T59), while simultaneously revealing the full range of thematic possibilities within each topic.  While the range of connotations of individual words in a topic presents one kind of interpretive challenge, the topics themselves can at times present another: when a topic includes words associated with seemingly divergent themes. In T56, the scholar might observe a (seemingly) obvious connection, for the nineteenth-century, between words that describe Native Americans and those that describe nature. However, unlike the words “antelope” or “hawk,” the words “tiger” and “hyena,” also included in the topic, do not describe animals indigenous to North America. Does an explanation lie in a figurative vocabulary for describing native peoples? Or is this collection of words merely an accident of statistical analysis, a result of being built on a randomized algorithm?    To address this question, we propose a spatial visualization using multidimensional scaling (Cox and Cox, 2010) to position the keywords for each topic according to their contextual similarity. As shown in the figure above-left, the terms “indian”, “indians”, and “tribes” are located apart from “hyena”, “tiger”, and “tigers”, which are themselves closely associated. The spatial layout suggests a relatively weak connection between these terms. For comparison, we also include the spatial visualization for a topic relating to the Mexican-American War, above-right, in which terms related to the conduct of the war (“Taylor”, “troops”) are spatially distinguished from those related to its outcome (“treaty”, “annexation”).    Conclusion and Next Steps The goal of our ongoing work on exploratory thematic analysis is to provide a comprehensive set of algorithms and visualizations for understanding newspaper archives. Topic modeling is an important first step, but if we are to move beyond suggestive word lists in order to contribute to humanities scholarship, topic models must be linked to relevant metadata and concrete examples. Moreover, scholars must be provided with new visual modes that illuminate the substructures within the generalized themes that the topic model produces. Such techniques can reveal new insights about the transmission and circulation of ideas among social and political coalitions, and how the framing of these ideas relates to authors’ genders. By linking technical innovation with real humanistic inquiry, we hope to produce algorithms and visualizations that will meet the needs of substantive humanities research.   ",
       "article_title":"Exploratory Thematic Analysis for Historical Newspaper Archives",
       "authors":[
          {
             "given":"Jacob",
             "family":"Eisenstein",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America ",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          },
          {
             "given":"Iris",
             "family":"Sun",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America ",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          },
          {
             "given":"Lauren F.",
             "family":"Klein",
             "affiliation":[
                {
                   "original_name":"Georgia Institute of Technology, United States of America ",
                   "normalized_name":"Georgia Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01zkghx44",
                      "GRID":"grid.213917.f"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "historical studies",
          "other",
          "gender studies",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Issue: Beginning in the 1950s, social historical investigations of single villages, towns, cities, and regions of early modern Europe emerged as a significant genre of historical writing. Authors such as Pierre Goubert and Emmanuel Le Roy Ladurie demonstrated that the archival information density for early modern European towns and villages was just right for undertaking a kind of “total history” of the locality. These studies made claims to address important historical issues beyond the locality under investigation (often having to do with the transition to modernity) that were impossible to cover at the level of the nation or Europe as a whole. Though often identified by fairly generic labels such as “social histories” or “micro-histories,” they distinguished themselves from traditional local histories by explicitly promoting their exemplary character. Sources were generally more abundant than in medieval Europe, as parish registers, tax lists, and court protocols became more abundant; but the information was also pre-statistical, unprocessed by its creators. The timing for working with denser local sources was apt because new computer technology in the 1960s allowed historians to build databases that eased analysis of the pre-statistical material. Historians began to use specialized techniques from the social sciences such as Gini-coefficients of inequality and family reconstitution to work with the historical data in order to work with the data from this pre-statistical age.  As a result, early modern European history emerged as one of the historiographically richest historical fields by the 1970s. By the 2000s, however, the initial promise of “total history” from quantitative social science history was in retreat. Some of that retreat may be attributed to conceptual overreach in the first wave of local studies. Some works seemed to make excessively sweeping claims on the basis of limited cases. But perhaps equally consequential is that the sheer numbers of local studies made it hard to gain an overview of what had and had not been investigated for different parts of Europe.  Current Implementation: The Early Modern European Social History Geospatial Bibliography Project (EMESHGB) is designed to help transcend these limitations and potentially spur new research in local histories by demonstrating the achievements of earlier research and identifying new paths to explore. It operates on several levels. At its simplest, it is a database of monographs about the localities of early modern Europe written since the 1950s. That database is enhanced by being accessible immediately through a map-based interface with a temporal slider. It is the first systematic effort to show the relationships between works concerning different parts of Europe, so that one can quickly establish what regions have been studied in depth and which are relatively under-researched. But the database will contain additional layers of information that can also be accessed geo-spatially and temporally from the interface. It is these additional layers that I call “deep annotation.” The deep annotation in the database is a more complex process. Some of it can be carried out without access to the books, directly from MARC records, but other parts of it require access to the works themselves. So far, the development of the ontology of the database and the required vocabulary for search is being done as an iterative process by looking at the individual works. In this early phase, it is limited to monographs written in English about all parts of Europe, along with a small subset of monographs written in French about France and in German about Germany, with the intention of expanding the geographical and linguistic scope once the robustness of the database ontology is clear.  The purpose of the deep annotations will be to group information contained in each work in ways that will facilitate comparisons. The database will contain a full bibliographic citation. It will also extract information easily from that bibliographic citation that users would especially wish to search by: 1) date of publication, 2) range of historical dates covered in the book, 3) name of the locality covered, current country of locality, 4) type of locality covered (e.g. village, town, seigneurie, neighborhood of a city, historic province). From that information, the user could locate, for example, all village studies that cover the years from 1550-1600. The value-added annotations in the database will make even more complex comparisons possible. The key ones will be 1) principal sources used for analysis (e.g. local court protocols, parish registers, cadastral records, tax lists, personal correspondence), 2) social groups analyzed (e.g. peasants, nobles, artisans, burghers, women, children, outcasts), 3) social history concepts invoked (e.g. historical demography, proto-industrialization, inheritance practices, rebellion and resistance), and 4) social science methods employed (e.g. family reconstitution, transition matrices, Gini coefficients of inequality, total factor productivity). These categories are populated not by locating every possible mention of a technique or source, but by identifying those that figure most prominently in the overall arguments of each work.  There is yet another way in which the geo-spatial and temporal information in the database will allow for a more complex visualization of historiography. Some of the works cited in each individual work will be other works in the database. There will, of course, also be many works cited that are not in the database. The ability to visualize citation networks will help establish which works have been most influential within and across national research traditions.  The linkage of a database with a geo-spatial/temporal interface is no longer that rare in digital humanities. There is also a precedent for the database in question being on its most basic level a bibliography. For example, the Perseus Project located classical works on a spatio-temporal interface. Nevertheless, the EMESHGB is innovative in allowing users to isolate thematic elements in the bibliography for comparison on the map interface. It will prove an important new tool for the next generation of research into Europe’s transformation to modernity.  Researchers interested in undertaking their own local studies and wishing to maximize their impact will be able to quickly assess what kinds of questions have been addressed using what kinds of materials for what parts of Europe. They can examine under-researched regions, identify key questions that might usefully be applied to a different geo-spatial region, and determine how their study might be innovative within the genre. Researchers interested in the comparative development of Europe will be able to quickly identify comparable studies for cross-cultural comparisons.  Future Directions:  The first phase of building out the database of the bibliography relies on the subject matter expertise of a single scholar. The initial set of monographs is large enough to provide a useful test of the concept of the database and interface with a complete collection of information, without being so large that it cannot be processed. One reason for beginning the project “by hand” is because it is useful to check the structure of the database against its output with works that are familiar. Also, the fact that the works included in the database are almost all still under copyright has limited the opportunities for larger scale corpus analysis. However, there are opportunities for automating the assignment of information to the different fields in the database by means of topic modeling, the results of which can then be compared with those produced by the scholar. At the same time, we will be establishing a process to transition the project from a “one-off” single development team product to a permanently extensible project. That process will almost certainly involve some kind of tool for allowing people to contribute to the database directly. The project can be extended in several segments after the initial concept has been demonstrated. With each extension, the original database will gain additional utility. After completing the English-language monographs, the most obvious extension is to cover the foreign-language monographs from the same time period. After completing that phase, which would have to be done in some collaborative format with scholars in Europe, we can consider extending the project on one of several dimensions – chronologically (e.g. adding works addressing the medieval period), geographically (e.g. adding works on Latin America and North America in the early modern era), or genre of historical writing (e.g. adding journal articles).     ",
       "article_title":"A \"Deeply Annotated\" Bibliography of Local Social Histories of Early Modern Europe",
       "authors":[
          {
             "given":"John Christopher ",
             "family":"Theibault",
             "affiliation":[
                {
                   "original_name":"Richard Stockton College of New Jersey, United States of America ",
                   "normalized_name":"Richard Stockton College of New Jersey",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0442n1j98",
                      "GRID":"grid.262550.6"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       "keywords":[
          "historical studies",
          "bibliographic methods / textual studies",
          "spatio-temporal modeling"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   “Humanists have always been explorers. They sail not on seas of water but on seas of color, sound, and, most especially, words.” — John B. Smith, 1984  Why is big data such a big deal? Modern digital communications are producing and recording data at a feverish rate, with 90% of the world’s recorded data, most of it unstructured, having been produced in just the last two years (Dragland, 2013). In addition, more traditional texts are being digitized all the time, and Crane (2006) tells us that while the largest academic digital libraries now hold tens of thousands of books, a completed Google Library will likely have more than ten million. To further Smith’s analogy, we are adrift in a sea of data, and we are at risk of floundering.  Front page news events like Edward Snowden’s May 2013 revelations disclosing the secret NSA (America’s National Security Agency) collection and analysis of massive amounts of ostensibly private data both domestically and internationally, are making big data analytics part of the public lexicon.  Major corporations, led by IBM with $1.3 billion in big data revenue in 2012, are scrambling to adopt practices that will allow them to capitalize on the volume of information now being generated. Global big data related revenues in 2012 were $11.6 billion, and are projected to break $18 billion in 2013 (Kelly et al.). While one might shudder to think what the NSA will do with all that information, outside of the world of politics and espionage, what can researchers and academics do with the volume of information now, or at least soon to be at our disposal? Matthew Jockers asks, “How do we mine them [texts] to find something we don’t already know?” (University of Nebraska-Lincoln) To utilize such massive corpora and avoid succumbing to the dilemma of what McCormick et al (1987) refer to as “information without interpretation” we need to find the most effective ways for end-users to explore, visualize and interact with big data such that it is both meaningful and understandable, if possible by both the trained and untrained eye.  Big data analytics and computer-supported visualization offer ways to read collections as our cognitive abilities are stretched to their limit with the sheer volume of data available (Araya, 2003). However, as Franco Moretti has pointed out, what we are reading when we use text mining methods and visualizations are really models of the collections. (Moretti 2013, p. 157) It is important therefore to survey the visual models emerging and question if they can better be designed to suit humanities exploration. In this paper we therefore propose to look at visualization for text mining in the following ways:  Survey text mining visualization in the humanities. Who is using text mining and why? What kinds of visualizations do they find compelling and why? Identify and Combine commonly presented visualizations for modeling. What visual models could be used for the exploration of large corpora? How could they be combined?   Model interactive prototypes of different combinations of visualizations for exploration.   Surveying: In a poster at DH 2013 we presented a framework of text mining tools that are useful in the humanities. Now we will survey the variety of visualizations used to present mining results. We will begin with early discussions of visualization like Smith’s “Computer Criticism” (Style 1978, p. 326), where he notes with agreement Paul de Man’s observation that as late as 1973 there had been no evolution beyond the close reading “techniques of description and interpretation” being used by literary critics since the 1930s or 40s, and suggests “pictorial representation” as one of the potential uses of computer aided text analysis. Brunet in a 1989 article talks about exploiting large corpora and provides a number of examples of visualizations. Our survey will examine advances in practice and understanding in the intervening thirty plus years, leading to contemporary works like Franco Moretti’s Graphs, Maps, and Trees, which, as the title suggests, introduces visual models for literary exploration. Our survey pays particular attention to recent text mining projects and tools including “The Proceedings of the Old Bailey”, David L. Hoover’s work with cluster analyses at NYU using “MiniTab”, Matt Jockers’ topic modeling work in “Macroanalysis”, the University of Waikato’s “WEKA”, the open-source visualization tool “Gephi”, the UMass machine learning tool “MALLET”, the research tools for textual study reviewed on “TAPoR”as well as some of our own INKE related projects such as “Dynamic Table of Contents”, “CiteLens”, “TextTiles” and “dialR”.  Identifying and Combining: While we recognize that output will assume a variety of formats including for instance heat maps, topographic plots or scatter plots, our survey suggests that text-mining projects commonly use five principal types of visualization:   1. Dendrograms, showing data clustering within sets 2. Histograms, showing change over time 3. Networkdiagrams, showing how entities are connected within a network 4. Wordclouds, representing topics of words 5. Scatterplots, showing words or parts in an abstract space   Visualizations that are presented in print are typically Spartan, focusing the attention on the results through careful design. All affordances are removed. The same types of visualizations automatically generated from large data sets, however, tend to be too dense to be useful and have to include affordances if meant to be interactive. Simple representative visualizations, like histograms for instance, are insufficient to display complex interrelationships. Dendrograms, especially if you are working with massive data sets, quickly become an illegible mass of inter-connectivity. Word clouds are good at showing the relative frequency of words in a text or topic, but not at comparing one text or topic to another. Network diagrams produce some beautiful results, but suffer from the same difficulties as dendrograms; large data sets quickly lead to illegibility.  “By visualizing information, we turn it into a landscape that you can explore with your eyes, a sort of information map. And when you’re lost in information, an information map is kind of useful.” — David McCandless, 2010  Modeling: Our research now is looking at ways to increase the exploratory power of visualization of large data sets. Used in combination, visualizations can provide otherwise elusive insight and clarity. They can also provide affordances for each other – a histogram can be used to explore a dendrogram. We have developed interactive prototypes using combinations of visualizations, and focusing on not simply allowing, but even encouraging the user to truly explore the (big) data, “function[ing] almost instinctively”, as McCullough (1996) stated, “to serve the process of development”. The more we can encourage users to explore and play with the data, the more likely they are to develop useful insights.   Fig. 1: Combination of a modified dendrogram showing clustering, and a diachronic timeline of 500 philosophy papers.  Figure 1 shows a prototype of a combination of dendrogram and histogram that we developed to visualize the clustering of 500 papers published between 1966 and 2004. The prototype is a combination of a modified dendrogram showing the clusters, and a diachronic visualization displaying the subjects over time. The displayed data is intuitively explorable, and the modified dendrogram is designed to encourage exploration. We developed the R code to prepare the data for interactivity. In Voyant we have developed skins that combine scatter plots and histograms, word clouds and histograms, and network diagrams with other tools. Again, the tools are open for others to recombine. To conclude, surveying commonly used graphical representations allowed us to identify commonly used visualizations that humanists find useful. To scale these so that they can be used interactively to explore large data sets we have prototyped combinations, where one visualization can be used to explore another and vice versa. The goal is visualizations that help researchers make sense of big data; visualizations that let us explore the forest, not just the trees so that we can draw accurate and appropriate inferences from the data.   ",
       "article_title":"Seeing the Trees & Understanding the Forest",
       "authors":[
          {
             "given":"John Joseph ",
             "family":"Montague",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Geoffrey",
             "family":"Rockwell",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Stan",
             "family":"Ruecker",
             "affiliation":[
                {
                   "original_name":"IIT - Institute of Design, USA",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Stéfan",
             "family":"Sinclair",
             "affiliation":[
                {
                   "original_name":"McGill University, Canada ",
                   "normalized_name":"McGill University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01pxwe438",
                      "GRID":"grid.14709.3b"
                   }
                }
             ]
          },
          {
             "given":"Susan",
             "family":"Brown",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Ryan",
             "family":"Chartier",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Luciano",
             "family":"Frizzera",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"John",
             "family":"Simpson",
             "affiliation":[
                {
                   "original_name":"University of Alberta, Canada",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "visualisation",
          "corpora and corpus activities",
          "interface and user experience design",
          "relationships",
          "graphs",
          "content analysis",
          "text analysis",
          "networks"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What can your computer habits reveal about you?  The answer might surprise you.  Previous work (Juola, et al., 2013) has shown that just a few minutes of computer usage can be used to identify who is at the keyboard and their demographic and psychological attributes with a fairly high degree of accuracy.  We expand upon this to show that the same usage data can be used to thoroughly profile a previously-unknown user to obtain valuable psychological information about the user.  Authorship attribution, the analysis of a document’s writing style to infer the author’s identity, is a well-established problem in text classification.  Previously, we used classical authorship attribution techniques to identify “who was at the keyboard” using the DARPA Active Authentication Corpus (Juola et al, 2013).  Researchers have successfully applied the analysis of language usage to infer authorship of written documents (Juola, 2006. Koppel et al, 2009. Stamatatos, 2009. Jockers & Witten, 2010), and stylometric analysis has also been applied to things like gender (Argamon et al, 2006), personality (Luyckx & Daelemans, 2008), and even psychological disorders like depression (Rude et al, 2004). Here, we attempt to perform the same technique with groups composed of individuals who share common psychological traits.  Previous work (Luyckx & Daelemans, 2008. Noecker & Juola, 2013) on personality profiling has so far focused on analyzing previously-written documents.  In contrast, our system provides a method for real-time psychological profiling of a user based on his or her interactions with a computer over a relatively short period of time (approximately 30 minutes).  The ultimate goal is two-fold: to learn something about a previously-unobserved user (traditional stylometric identification techniques require us to have training data on a user before we can identify him) and to use psychological traits as an enhancement to current user authentication methods. Currently, exact accuracy on the user-based authentication is approximately 90%.  This task becomes more difficult (and the accuracy becomes correspondingly lower) as the pool of potential author models grows.  In order to improve overall accuracy of the user authentication task, we propose to include these psychological profiling tools in the authentication system.  If a given user can be identified as the most likely candidate with 90% probability, and several facets of that user’s personality can be confirmed with similarly high confidence, this will increase the overall robustness of the authentication system. For our purposes, we used two personality/intelligence measurement systems to profile users: Myers-Briggs Type Indicator (MBTI) and Multiple Intelligences Developmental Assessment Scales (MIDAS). The Myers-Briggs type indicator (MBTI) assigns four binary classifications to define personality (Myers & Myers, 1980)   Extroversion vs Introversion iNtuition vs Sensing Thinking vs Feeling Judgement vs Perception   The Multiple Intelligences Developmental Assessment Scales (MIDAS) were developed by Dr. Howard Gardner in his 1983 book “Frames of Mind” (Gardner, 1983).  He used a unique definition of intelligence: “The ability to solve a problem or create a product that is valued within one or more cultures” (MI Research and Consulting).  He identified 8 primary intelligent scales, each of which have several subscales (MI Research and Consulting):    Musical   Vocal Ability   Instrumental Skill   Composer   Appreciation     Kinesthetic   Athletics   Dexterity      Logical-Mathematical   Everyday Math    School Math    Everyday Problem Solving    Strategy Games      Spatial    Space Awareness    Working with Objects   Artistic Design      Linguistic   Expressive Sensitivity   Rhetorical Skill   Written-academic     Interpersonal   Social Sensitivity   Social Persuasion   Interpersonal Work     Intrapersonal   Personal Knowledge / Efficacy   Effectiveness   Calculations   Spatial Problem Solving     Naturalist   Animal Care    Plant Care      We also include a 9th main scale, Leadership, with its own subscales: Communication, Management, and Social.  Materials and Methods  Corpus  In order to create the most accurate corpus possible, we set up a simulated office environment and hired 80 temporary workers for one week each.  Workers were tasked to perform a long-term blogging project (research and write blog articles on topics “related to Pittsburgh in some way”) over the course of a normal workweek.  For this study, we use the Free Key Logger output, which provides the exact text typed by each user.  We do not include any information about the applications being used or any data the user pastes from the clipboard. Feature Extraction  For our analysis, we used the Java Graphical Authorship Attribution Program (JGAAP) (Juola et al, 2009).  JGAAP is a Java-based, modular program for textual analysis, text categorization, and authorship attribution.  It provides a comprehensive framework, allowing us to rapidly test the effectiveness of different analysis techniques on the recorded data. JGAAP divides analysis into several steps: Canonicization (Preprocessing), Event Set (Feature) Generation, and Analysis.  In Canonicization, preprocessors are used to standardize the text.  For this step, we converted all input letters to lower case (“Unify Case”) and converted all strings of whitespace characters into a single space character (“Normalize Whitespace”).  At this stage, we also processed a variety of special keyboard characters, converting these non-printable characters into a printable placeholder (e.g. “backspace” was replaced with “β”).  Finally, we divided the input data into blocks of 1,000 characters, representing about 30 minutes of computer usage. For the event set generation, we tested character N-grams for all N from 1 to 15, and word N-grams for N from 1 to 5.  We then applied a number of analysis methods for each experiment: Cosine Distance, Intersection Distance, Manhattan Distance, and Matusita Distance.  For each method, we used a centroid-based nearest neighbor classifier.  We performed leave-one-out cross-validation to reach our final conclusion. Models For the MBTI classifiers, we built four binary classifiers (i.e. E vs I, N vs S, T vs F, and J vs P).  For the MIDAS classifiers, we first built a single 9-way classifier to identify a user’s principle main scale.  This was the scale along which the user scored highest (i.e. the scale for which the user showed the highest preference).  For example, a user might have a preference for “Musical” or “Linguistic”.  We also developed subscale classifiers, which identify a user’s preference within each major scale.  For instance, a user might be identified as “(Musical) Vocal Ability” and “(Kinesthetic) Dexterity”, etc.  Thus, each user was identified by a single main scale preference as well as nine subscale preferences.   Results  MBTI For the MBTI classifiers, we averaged an accuracy of 81.5%.  The expected baseline average (assuming we pick the most prevalent personality type for each category) is 55%.  MIDAS For the MIDAS main category identification, our best performing classifier had accuracy of 70.7%.  This was using character 15 grams with Intersection Distance.  The expected baseline accuracy (achieved by choosing the most common main scale, “Linguistic”) was 22.1%. For the MIDAS subscale identification, the best performing classifiers used a variety of Character n-grams, again with Intersection Distance as the top performing analysis method.  The average subscale accuracy was 81%.  Conclusion  We have shown here a method to reliably psychologically profile a computer user based on only a short period (about 30 minutes) of usage time.  In addition to providing valuable information about the user in question, this method can also be used to provide additional layers of security for the active authentication system we have described previously.  Even in an adversarial situation, the difficulty of imitating both an individual user’s style, as well as mimicking the psychological profile of the user, will provide additional security to the authentication system. Also interesting to note is the limited usage data required to perform these analyses.  The initial user psychological testing period took approximately 3 hours, but accurate results were obtained for only 30 minutes of computer usage.  In addition, the three hours of testing were completely lost time – the users were able to work only on the tests during this time.  In contrast, the 30 minutes of analysis can be done on whatever the user is working on at the time.  No downtime is required to perform these analyses.  We believe this system could be useful anywhere a non-intrusive analysis of a user might be beneficial (e.g. determining whether a potential employee would be a good fit). For future work, we intend to focus on reducing the amount of data needed even further.  Preliminary results on as little as 500 characters (about 15 minutes of usage time) have been promising.  Additional work is also being done to integrate these methods into the broader active authentication system in order to bolster the overall reliability of the system.  ",
       "article_title":"Active Authentication through Psychometrics",
       "authors":[
          {
             "given":"John",
             "family":"Noecker Jr",
             "affiliation":[
                {
                   "original_name":"Juola & Associates, United States of America ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  From lighthouse to framework: visualising digital scholarly editions with pathways and histories  The Woolf Online project (1), now in its second phase of development at Loyola University Chicago’s Center for Textual Studies and Digitial Humanities, continues to address the following fundamental questions about the nature and development of literature:   - how does a literary text come into being?  - what kinds of influence are at work upon the writer during the process of initial composition, and thereafter?   The project has continued to investigate the various ways in which different recoverable histories of a particular text can be used to illuminate the process of its composition. By recording the history of a particular text, its cultural, political, and autobiographical contexts and their interaction, we can visually begin to answer some of the following important questions:   how is textual history related to other histories of a text?  - what use does literary criticism make of textual and contextual histories?   With these goals in mind, we have developed and implemented digital tools and methods that allow the dynamic rendering of a literary text, which is then embedded within a contextual environment relevant to its known, original period of composition. For example, a user can view each day's writing for the Initial Holograph Draft within the context of Woolf's own diary entries and letters, newspapers of the day, and other historical data appropriate to the period. A user can compare different stages of the genetic edition of 'Time Passes', for example, and view earlier and later material that may have impacted upon or been influenced by the composition of 'Time Passes'. This material currently includes extracts from 'Sketch of the Past' and her 1905 journal. We have also delved in to the history of the Stephen Family, and the impact St Ives in Cornwall, and in particular Talland House, may have had upon the composition of this text.  As part of this onging development within the Center for Textual Studies and Digital Humanites, we have developed an extensible development and publication framework, called ‘Mojulem’  (2), for editing, publication, and visualisation of digital scholarly editions. Mojulem now allows us to build on the concept of ‘knowledge sites’, as suggested by Peter Shillingsburg (3) , supplementing a core publication framework with modules/plugins such as editors and image viewers. It is also able to host multiple projects within one installed framework, thereby enabling cross-project research and the option to aggregate specified data. Development of Mojulem, with the Woolf Online project as one of the ongoing working environments, has followed the need for four underlying core structures. These structures include CorPix, CorTex, CorCode, and CorForm, which are detailed as follows.  CorPix  Manuscripts and printed texts materially unite the iconic and lexical, the autographic and allographic (4), whereas all digital representations separate these constituent elements into images and transcriptions. With Mojulem projects, the common default display reunites the image and transcription by mapping the one to the other at the pixel level. CorPix software currently includes eHinman (5) , Transparent (6) , TransparentOCR, Magnify, and Zoom. For example, pixel-level positioning and coordinate fixing is an inherent feature of both Transparent and TransparentOCR, within both editor and visualisation tools. eHinman is a digital adaptation of the original Hinman collator, and enables fade from the image of a page from one copy to another, thereby enabling a visual collation of multiple copies. Transparent is used within both the visualisation and editing stages of a project’s development, enabling an editor and user alike to view the image as the primary entry consideration for the project.  CorTex  The CorTex is the stable resource containing the merged or compacted plain text transcriptions of the variant expressions of a work. It stores all information about text and variations, ready to be extracted for display of variation amongst versions; it is not necessary to recompute them. The CorTex is the entity to which all standoff properties (markup, annotations, links, etc.) points and on whose stability the system depends. It is as the source of each version’s text and variation from other texts. The stability and endurance of the CorTex is protected by multiplying duplicate copies locked with a digital signature, which verifies for each user that a CorTex copy is viable.  CorCode  CorCode is the add-on value of analysis, argument, and explanation. Mojulem stores markup separately, as standoff properties, applying it as the user invokes it for the rendering of a specific item’s image or text within a given visualisation, such as a transparent view of a page of the Initial Holograph Draft of ‘To the Lighthouse’ (7) . To do this, Mojulem includes an editor which saves text and encoding separately, and filters for converting legacy, code-embedded transcriptions, including TEI encoded documents, into separate forms with markup analysed into properties, and filters for reversing this process.  CorForm  A CorForm is a CSS stylesheet, containing special formatting rules, used to transform the overlapping properties of the CorCode into HTML. Each CorCode has a default CorForm, but other Corforms can be used in combination or as alternatives. Since a CorTex may have many CorCodes, and each CorCode many CorForms, structuring or formatting of the text can be attained by specifying some combination of already available resources, or by supplying new ones.  The CorPix, CorTex, CorCode, and CorForm are aggregated for a project within the Mojulem framework. Each such item is identified by a unique key, which is used as an index into the repository or database.  The development and combination of these four cores, within the modular and adaptable framework Mojulem, has allowed the second phase of the Woolf Online project to begin to approach the fundamental questions about the nature and development of literature, as outlined above. As such, this paper and presentation will outline and demonstrate visual methods and considerations for implementing such perceived networked histories. It will also discuss the different aspects of Mojulem, as outlined above, and ground such a discussion within the example of the ongoing Woolf Online project.  Notes 1 Project site is available at the following URL: http://www.woolfonline.com 2 Our current test framework for the Woolf Online project, as of 6th March 2014, can be viewed at the following URL: http://dhdev.ctsdh.luc.edu/projects/edfu/  3 Shillingsburg, P. L. (2006). From Gutenberg to Google: Electronic Representations of Literary Texts: Cambridge University Press. 4 Nelson Goodman noted this distinction to separate art forms with a unique authentic form, for example painting and sculpture, from art forms which are performative with multiple copies, including writing and music.  5 An earlier stand-alone example can be viewed at the following URL: http://dhdev.ctsdh.luc.edu/testing/imaging/ehinman-dep/v6/  6 A test example page of the Initial Holograph Draft can be viewed at the following URLs: Image =  http://dhdev.ctsdh.luc.edu/projects/edfu/?node=content/image/gallery&project=1&parent=2&taxa=6&content =353&pos=21 and transcription = http://dhdev.ctsdh.luc.edu/projects/edfu/?node=content/text/transcriptions&project=1&parent=2&taxa=6&co ntent=5136&pos=21  7 For example, Image =  http://dhdev.ctsdh.luc.edu/projects/edfu/?node=content/image/gallery&project=1&parent=6&taxa=24&conte nt=385&pos=49 and transcription = http://dhdev.ctsdh.luc.edu/projects/edfu/?node=content/text/transcriptions&project=1&parent=2&taxa=6&co ntent=5164&pos=49  8 An earlier TEI derived stand-alone example of this concept, using material from the Malory project, can be viewed at the following URL: http://dhdev.ctsdh.luc.edu/testing/tei/teiparser_full/   ",
       "article_title":"From lighthouse to framework: visualising digital scholarly editions with pathways and histories",
       "authors":[
          {
             "given":"Nicholas John ",
             "family":"Hayward",
             "affiliation":[
                {
                   "original_name":"Loyola University Chicago, United States of America ",
                   "normalized_name":"Loyola University Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04b6x2g63",
                      "GRID":"grid.164971.c"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "framework",
          "digital",
          "scholarly edition",
          "woolf"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction  Archaeology and epigraphy have made significant progress to decipher the hieroglyphic writings of the Ancient Maya, which today can be found spread over space (in sites in Mexico and Central America and museums in the US and Europe) and media types (in stone, ceramics, and codices.) While the deciphering goal remains unfinished, technological advances in automatic analysis of digital images and large-scale information management systems are enabling the possibility to analyze, organize, and visualize hieroglyphic data that can ultimately support and accelerate the deciphering challenge.  We present an overview of the MAAYA project (http://www.idiap.ch/project/maaya/), an interdisciplinary effort integrating the work of epigraphists and computer scientists with three goals:  (1) Design and development of computational tools for visual analysis and information management that effectively support the work of Maya hieroglyphic scholars;  (2) Advancement of the state of Maya epigraphy through the coupling of expert knowledge and the use of these tools; and  (3) Design and implementation of an online system that supports search and retrieval, annotation, and visualization tasks.    Our team approaches the above goals acknowledging that work needs to be conducted at multiple levels, including data preparation and modeling; epigraphic analysis; semi-automated and automated pattern analysis of visual and textual data; and information search, discovery, and visualization. In this abstract, we concisely describe three ongoing research threads, namely data sources and epigraphic analysis (Section 2), glyph visual analysis (Section 3), and data access and visualization (Section 4). We provide final remarks in Section 5.  2. Data sources and epigraphic analysis  The project focuses on Maya hieroglyphic inscriptions produced within the Yucatan Peninsula, inside the northern Maya lowlands, which encompasses sites within the Mexican states of Yucatan, Campeche, parts of Quintana Roo and a northern-most portion of Belize (see Fig. 1). Our research targets the three Maya Books (Codices) produced inside the Yucatan peninsula during the Postclassic period (1000-1521 AD). The first one is the Dresden Codex, housed at the University Library of Dresden, Germany.  For this data source, our project relies on published facsimiles (Förstemann, 1880; Codex Dresden, 1962; Codex Dresden, 1989) and on high-resolution, open-access images provided by the SLUB. The Codex Madrid is stored at the Museo de América in Madrid, Spain, and for its study, our project relies on published facsimiles and line drawings (Codex Madrid, 1967; Villacorta and Villacorta, 1976). For the Paris Codex, the project relies on published facsimiles and images provided online by the Biblioteque Nationale de France.   Fig. 1: Map indicating main archaeological sites under study by our project.  Codex pages were usually divided by red lines or t'ols (Fig. 2). Each of these t'ols is further subdivided in frames relevant to the specific dates, texts and imagery depicted. Frames contain several glyph blocks organized in a grid-like pattern with columns and rows, calendric glyphs, captions, and iconographic motives. Briefly stated, t'ols are \"segmented\" into their main constituent elements (Fig. 2). Images are post-processed and from these, high-quality, scale-independent vectorial images of the individual hieroglyphs and iconography are generated in three modes: (a) grayscale/color, (b) binary, and (c) reconstructed forms (marked in blue), which are based on epigraphic comparison of all available similar contexts (Figs. 3-4)     Fig. 2: Page 47c (44c) of the Dresden Codex framing main individual constituent elements (by Carlos Pallán based on SLUB online  open source image)  The process of annotating the Codices entails an analysis comprising the following steps: (a) identification of individual signs on (Thompson, 1964) catalog, i.e. T0588:0181; (b) identification of individual signs on (Macri and Vail, 2008) catalog, i.e. SSL:ZU1; (c) identification of individual signs on (Evrenov et al., 1961) catalog, i.e. 400-010-030; (d) identification of signs on (Zimmermann, 1956) catalog, i.e., Z0702-0060; (e) transcription, specifying phonetic values for individual signs as syllables (lowercase bold) or logograms (uppercase bold), i.e. K'UH-OK-ki; (f) transliteration, conveying reconstructed Classic Maya speech (words) formed by the combination of individual signs, i.e. k'uhul ook); (g) morphological segmentation, a division into morphemes for later linguistic analysis), i.e. k'uh-ul Ok; (h) morphological analysis, assigning each of the previous segments to a definite linguistic category, i.e. god-ADJ step(s)/foot; (i) English translation: \"Divine step(s)/foot\". Taken together, the processing steps within this workflow provide the ground for more advanced multimedia analyses (Fig. 5).     Fig. 3: Process to generate vectorial representations of the Dresden Codex: a)color/grayscale; b) binary; c) reconstructed (blue) forms (by Carlos Pallán based on SLUB online  open source images)    Fig. 4: Vectorial representations of the Madrid Codex, Page  (T'ol) 10b, Frame 1: a) color/grayscale; b) binary; c) reconstructed (blue) forms (by Guido Krempel based on (Codex Madrid, 1967))    Fig. 5: Multivariable fields used to annotate textual contents of Dresden Page (T'ol) 47c (44c) (by Carlos Pallán)   3. Visual analysis of glyphs  Modeling Maya glyph shape is challenging due to the complexity and high intra-class variability of glyphs. We are developing methods to characterize glyphs for visual matching and retrieval tasks. In previous work, we proposed a shape descriptor based on visual bags-of-words (HOOSC: Histogram-of-Orientations-Shape Context) and used it for isolated glyph retrieval (Roman-Rangel, 2011). We are pursuing two research lines to extend our current capabilities. Improved shape representations. Three directions are being considered: (1) the improvement of bag representations to retrieve syllabic glyphs. In particular, we developed a method to detect visual stop-words (Roman-Rangel, 2013a), and a statistical approach to construct robust bag-of-phrases (Roman-Rangel, 2013b); (2) the use of neural-network architectures like auto-encoders (Ngiam, 2011) that automatically build representations from training data. These approaches represent an alternative to handcrafted descriptors like the HOOSC, and provide a principled way to quickly adapt representations to different data sources (codex vs. monument glyphs); (3) the use of representations based on the decomposition of glyphs into graphs of segments, from which shape primitives can be extracted. This representation might be more suitable than histogram-based descriptors like HOOSC at identifying which strokes of a shape are discriminative, potentially allowing comparisons with so-called diagnostic features provided by epigraphers (Fig. 6).   Fig. 6: Three glyph instances of the same sign. Right: one diagnostic features and variant.  Co-occurrence modeling. We are exploring ways to exploit the fact that glyphs do not occur in isolation within inscriptions but in ordered groups (glyph-blocks) (Fig. 2). To this end, we are studying options to build models relying on glyph co-occurrence statistics or further accounting for the glyph spatial position within the blocks. We plan to investigate how such information can be used in a retrieval system to improve performance and to help scholars deal with unknown or damaged glyphs. This has several dimensions like query types (e.g. single glyphs with known identity of other glyphs within the block), and contextual combination of shape similarity with text metadata.  4. Data access and visualization   Our work in this direction focuses on visualization of and effective access to image databases with archaeological value. We are developing a repository that will serve further goals within the project. This database stores visual elements of the Madrid, Dresden, and Paris codices. It is complemented with an online system, shown in Fig. 7, which allows for capturing and annotation of codices. More specifically, the repository contains relevant information regarding the composition of the codices, such as hierarchical relations between components and bounding boxes of glyphs. Therefore, it allows to query visual elements at different levels of semantic structure, i.e., page, t'ol, glyph-block, individual glyph, etc. The repository will also allow to query and study statistics of the Mayan writing system, e.g., hieroglyph co-occurrences.   Fig. 7: Snapshot of the online tool that feeds the database with imagery data and its corresponding annotations, i.e., codex name, t'ol, glyph-block reference, Thompson and Macri and Looper catalogs.  The second research line is the advancement of visualization techniques, and more precisely, the development of techniques that will allow exploring the feature space of a number of visual shape descriptors used to represent Mayan hieroglyphs for retrieval purposes. By relying on these visualization methods, our goals are detecting, understanding, and interactively overcoming some of the drawbacks associated with the shape descriptors currently in use (Vondrick, 2013).  5. Conclusions  We presented an overview of the MAAYA project’s work-in-progress on epigraphic analysis, automatic visual analysis, and data access and visualization. Our close integration of work in computing and epigraphy is producing initial steps towards the design of computing methods tailored for epigraphy work; and can create opportunities to revisit findings in Maya epigraphy under the light of what computer-based methods can reveal (e.g., data-driven analyses of glyph diagnostic features.) At the same time, several of our machine learning, computer vision, and information retrieval methods are applicable to other problems in digital humanities. We would be interested in investigating applications of these methodologies to other sources of Cultural Heritage materials.  Acknowledgments.   We thank the support of the Swiss National Science Foundation (SNSF) and the German Research Foundation (DFG). We also thank all the members of the team (Rui Hu, Gulcan Can, April Morton, Oscar Dabrowski, and Peter Biro) for their contribution.  ",
       "article_title":"The MAAYA Project: Multimedia Analysis and Access for Documentation and Decipherment of Maya Epigraphy",
       "authors":[
          {
             "given":"Daniel",
             "family":"Gatica-Perez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Carlos",
             "family":"Pallan",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Stephane",
             "family":"Marchand-Maillet",
             "affiliation":[
                {
                   "original_name":"University of Geneva, Switzerland ",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          },
          {
             "given":"Jean-Marc",
             "family":"Odobez",
             "affiliation":[
                {
                   "original_name":"Idiap Research Institute, Switzerland",
                   "normalized_name":"Idiap Research Institute",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/05932h694",
                      "GRID":"grid.482253.a"
                   }
                }
             ]
          },
          {
             "given":"Edgar",
             "family":"Roman Rangel",
             "affiliation":[
                {
                   "original_name":"University of Geneva, Switzerland ",
                   "normalized_name":"University of Geneva",
                   "country":"Switzerland",
                   "identifiers":{
                      "ror":"https://ror.org/01swzsf04",
                      "GRID":"grid.8591.5"
                   }
                }
             ]
          },
          {
             "given":"Nikolai",
             "family":"Grube",
             "affiliation":[
                {
                   "original_name":"University of Bonn, Germany",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "image processing",
          "sustainability and preservation",
          "repositories",
          "archaeology",
          "archives",
          "multimedia",
          "information retrieval",
          "video",
          "audio"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1 Introduction Nowadays, large collections of old historical manuscripts, which contain valuable information about our cultural heritage, exist in libraries around the world. Recently, there has been much interest in their digitization for preservation reasons, since many of the available manuscripts’ quality has deteriorated from exposure to the environment. Digitization though is only the first step to make the information contained in manuscripts accessible to researchers and to the interested public. What we create after digitization is only a “digital image” of the page and further processing steps need to be applied during the handwriting recognition process, so that the manuscript’s content is transformed into a form that is interpretable by a computer. One important step in the handwriting recognition process is that of text line extraction, which aims at extracting individual text lines from the manuscript page. In this paper, we propose a binarization-free text line extraction method using seam carving. The main idea is to compute an energy map of the input text blocks and determine minimum energy paths that pass through them. The energy map is constructed in a way so that gaps between text lines have low energy values. Therefore, a minimum energy path will pass only through these regions and will successfully separate two text lines. Our algorithm has the following two advantages: 1. We make direct use of the original image representation of the manuscript page without any need for prior binarization, which can introduce information loss. This loss can produce unreliable results for the text line extraction algorithm (see Figure 1). 2. Our algorithm is general and can be applied to diverse manuscripts of different time periods and handwritings. Results in Figures 3 and 4 show the applicability of our algorithm to diverse historical manuscripts.   2 Related Work We briefly summarize the research that has already been done for text line extraction. Most of the state-of-the-art approaches operate on a binary image of the historical manuscript. One method based on dynamic programming computes the paths with minimum cost between two consecutive lines [11] and has been extensively used in automatic transcription and ground truth creation of historical documents [8, 7]. The work of [6] is based on horizontal projection profiles of black pixel changes. An additional post-processing step is applied, which follows the contour of the ink obtaining curve-linear line separators. Another similar approach is proposed in [13] where the output of the horizontal projections is post-processed based on properties of the computed connected components. The works of [12, 10] are based on the Hough transform, which is able to detect straight lines in images. Smearing methods, such as the ones in [17, 16, 14], try to fill-in the white pixel gaps with black pixels if their distance is less than a threshold. That way, homogeneous blocks of the document page are grouped together. Other approaches use multi-oriented filters and active contours for text line extraction [4, 5].    Fig. 1: Left: seams generated using the original scanned image of the manuscript as input. Right: seams generated using a binary version of the original manuscript scan as input. The information loss in the binary version is so extensive that the generated seams do not clearly separate text lines of the original manuscript scan.  A notable exception of an algorithm, which does not depend on binarization, is the work of, where the text lines are found using extracted features from interest points of the original manuscript image. A very recent work uses a framework similar to that in adapted this time to the text line extraction problem. Our method is closely related to the one in, where the authors use seam carving to generate seams that pass through connected components of a binary image. Unclassified components, which do not belong to any text line are assigned in a post-processing step according to their position and geometric characteristics. The main difference in our approach is that we do not need to binarize the input, which can lead to information loss. Additionally, we are able to generate robust text contours even for manuscripts of deteriorated quality (see Figures 1, 4). The text contours can always be overlaid on the original manuscript scan, even if they have been generated using as input a binarized version of the original scan. However, the technique of , which assigns text components to lines is not able to extract lines from the original manuscript, since the binarization process is not reversible. Binary text components contain only a subset of the information available in the original manuscript image.   3 Our Approach Our proposed algorithm is inspired by Seam Carving, a computer vision algorithm used for image resizing [1]. We build upon this idea and propose a seam carving algorithm, which operates on the original color image and extracts lines in a sequential way. First, an energy map is calculated and the minimum energy path is computed based on dynamic programming. From the peaks of the horizontal projection profile of the derivative image we can find horizontal line positions. In each such region between two consecutive lines, we apply our seam carving algorithm sequentially until the whole manuscript image has been processed. In Figure 2 we show some examples of seams between two such lines.   Fig. 2: Examples of image blocks and their computed seams.  In the following we use the convention that an image I ∊ ℝ n×m converted to grayscale has n rows and m columns. The notation I(i,j) denotes the image value at the i-th row and j-th column. The coordinate system has its origin in the upper left corner of the image.   3.1 Energy Map We modify the energy function proposed in [1] so that it can be effectively used for generating text line separation contours. First, we compute an edge image as  Let us denote the energy map between two text lines by Eb = E(J,:) ∊ Rl×m, where J = {s,...,e} is the set of i coordinates between the start and the end of the energy block map and l = e - s + 1. This energy block is weighted by the following linear function, which penalizes the larger i coordinates more:  and the final energy map for this block is  The idea behind this weighting is the observation that we want our seam to be closer to the upper line than the lower one. This will correct for situations where the author has written words in the gap between lines, which always belong to the lower line (see Figure 2a).   3.2 Seam Computation A seam that passes horizontally through an image block can be defined as  The seam computation is done using dynamic programming. We look for the optimal seam in the image block that minimizes   The first step is to traverse the image block and compute the cumulative minimum energy Mb for all possible connected seams for each pixel position:   The minimum value of the last column in Mb will indicate the end of the minimal connected horizontal seam. Therefore, in the second step we traverse the cumulative energy Mb backwards to find the path of the optimal seam.   4 Experimental Results We apply our algorithm to original manuscript pages of the work Aline by the important Swiss-French writer Charles-Ferdinand Ramuz. Some examples of manuscript pages overlaid with the text line extraction seams are shown in Figure 3. We observe that our algorithm creates seams that pass through parchment regions, successfully segmenting the text lines. Even when the writer corrects a line or a word and writes above, the seam is able to avoid cutting the text and assigns the word to the line below it. In order to illustrate the ability of the algorithm to generalize to diverse manuscripts, we provide in Figure 4 results on manuscripts of the 16-th and 18-th century. We observe that our algorithm can be applied to manuscripts of very different quality and handwriting styles.   Fig. 9: Seam carving results on two pages of Aline (1905), C.F. Ramuz.    Fig. 10: Seam carving results on manuscripts of the 16-th and 18-th century respectively. Even in the lower left manuscript with extreme bleed-through, our algorithm is able to produce a robust result.    5 Conclusion We propose a novel text line extraction algorithm for color scans of historical manuscripts based on seam carving. We show that we can obtain state-of-the-art results on these color images without any prior binarization. The next step after the text line extraction process is the application of a learning algorithm for handwritten word recognition in each extracted text line.  ",
       "article_title":"Binarization-free Text Line Extraction for Historical Manuscripts",
       "authors":[
          {
             "given":"Nikolaos",
             "family":"Arvanitopoulos Darginis",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sabine",
             "family":"Süsstrunk",
             "affiliation":[
                {
                   "original_name":"EPFL, Switzerland ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-11",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  1. Introduction As part of its KOTONOHA (meaning ‘words of the language’ in classical Japanese) Project, the National Institute of Japanese Language and Linguistics (NINJAL) is conducting a morphological analysis of Japanese classics. Both the selection and digitization of classic documents are required in order to execute the analysis correctly. Digitization and morphological analysis have been done thus far on the literature of several ages and styles (Ogiso et al., 2012; Ichimura et al., 2012; 2013), and various text corpora have been published. These text corpora are marked up with NINJAL’s original Document Type Definition (DTD). However, some elements are used in common on all corpora but basically are not unified nor standardized. Under this circumstance, it enables structural analysis and string extraction from a single corpus but causes problems with structural comparison and numerical analyses between several corpora. Thus, it is necessary to design and mark up a unified definition from a higher level in order to conduct analyses concurrently. In the previous study, we examined the possibility of converting classic documents with TEI-compliant XML (Kawase et al., 2013), in particular, by designing a tag-set and strictly structuring an old wood-block printed book from Sharebon’s “Keisei-kai Futasuji-no-michi” (published in 1798, e.g. Fig. 1) as a model case, and sorted out this problem. This work aims to provide further insights into those problems and propose a solution.   Fig. 1: Excerpt image of Sharebon's “Keisei-kai Futasuji-michi” (owned by NINJAL)  2. Significance of Encoding Classic Documents The reason for choosing Sharebon as a subject is because it is important primary linguistic data from the early modern period (Keene, 1999) and it has the following three features: (1) published in the broad age from the 18th to the first half of the 19th century; (2) uses colloquial words and expressions among conversations between characters; (3) abundantly describes things in Edo (former name of Tokyo) language and Kamigata (Kyoto-Osaka region) language.  Therefore, describing Sharebon in a machine-readable manner has profound significance not only for bringing numerical analysis of unexplained colloquial expressions of the modern period into reality but also for facilitating humanities research in language history and descriptive bibliography. Furthermore, since many texts that have a similar structure to Sharebon were published over the same period of time, this study will offer a common format for archiving coeval literature.  3. Issues in Encoding Sharebon In order to analyze the manuscript from a corpus linguistic viewpoint, not only outward mark-ups but also internal mark-ups of both document structure and linguistic structure are required. In general, Sharebon is composed of a combination of three parts: the front matter; the narrative body that contains colloquial expressions and descriptive texts; and the back matter. For instance, in the case of “Keisei-kai Futasuji-michi”, the narrative body is made up of two chapters, ‘Natsu-no-toko’ (Summer Alcove) and ‘Fuyu-no-toko’ (Winter Alcove). Since the composition of this whole document is well accorded with the composition of an orthodox Western manuscript, we may mark up most of the elements in reference to TEI P5: Guidelines (Burnard and Baumanm, 2007). Table 1 shows the list of elements with an explication of their roles used to mark up the document.  The problems incurred in the process of encoding Sharebon can be broadly classified into two matters: (A) text formatting on a #PCDATA (character data) level; and (B) structuring ruby annotations. We will discuss these two matters in more detail in the following sections.    Fig. 2: List of elements with an explication of their roles to mark up Sharebon  3.1 Text Formatting on #PCDATA  To design and assure a high-quality corpus in terms of linguistic resources, it is necessary to index information about lexical morphemes (e.g. word class, inflected forms, pronunciations) at the body-text level. However, since there are three problems in the sentences of early modern Japanese, it is difficult to conduct morphological analysis properly: (A-1) voice markings called dakuten are missing where they should be; (A-2) corresponding phonetic characters called hiragana and katakana are not unified; (A-3) iteration symbols called odoriji are unmodified.  For example, dakuten makes a phonetic shift from ka, ki, ku, ke, ko (written in hiragana letters as か, き, く, け, こ) to ga, gi, gu, ge, go (rendered as が, ぎ, ぐ, げ, ご).  3.2 Structuring Ruby Annotations Such annotations are the small-text characters rendered alongside the base text. In vertical writing, ruby is typically printed on the right-hand side. However, since the three problems exist, it is difficult to mark up both outward information and the linguistic structure simultaneously: (B-1) ruby text carries extended characters (gaiji), damaged and missing characters, and some typographical errors and omissions; (B-2) words of such text and base text are not in one-to-one correspondence; (B-3) There can also be another ruby annotation on the left-hand side simultaneously.  4. Means for Solving the Problem 4.1 Text Formatting on #PCDATA At NINJAL, for problems (A-1), (A-2), and (A-3), the character level is corrected using the original elements of <vMark>, <kana>, and <odoriji>, respectively (Ichimura et al., 2012; 2013). We accurately describe the text formatting in a uniform way by combining the TEI elements <seg> and <choice> (e.g. Fig. 2). According to TEI P5: Guidelines (Burnard and Baumanm, 2007), <seg> (arbitrary segment) represents any segmentation of text below the ‘chunk’ level, and <choice> groups a number of alternative encodings at the same point in the text. Distinctions in the corrections of (A-1), (A-2), and (A-3) can be shown by writing @type, holding selectable values from vMark, kana, and odoriji, to the attribute of element <seg>. The original text marked with TEI element <orig> (original form), along with the corrected text (the optimal version for morphological analysis) marked with TEI element <reg> (regularization) are written below the <choice> level. This description policy preserves both the outward and linguistic structure, even if problems (A-1), (A-2), and (A-3) carry over extended characters or omissions in the original manuscript, and brings morphological analysis into reality as well.  4.2 Structuring Ruby Annotations At NINJAL, regarding structuring ruby annotation, since priority is given to morphological analysis, words and characters are encoded using the original defined element <ruby> with an attribute @rubyText (e.g. Fig. 3a). To express this structure with TEI-compliant XML, we can simply substitute the element <ruby> with <w> (word) and the attribute @rubyText with @ana (e.g. Fig. 3b). However, the important ruby text information is described inside @ana as a value, we cannot solve problem (B-1) under this policy. Depending on the technology, CSS, XHTML, and HTML5 platforms might be considered as alternatives in order to structure ruby annotation (Benoit, 2010) (e.g. Fig. 3c).  Here, each <ruby>, <rt>, and <rp> represent inline elements that contain base text with ruby annotation, the superscript which comes over the base text, and ruby parentheses which are used to wrap around opening and closing parentheses <rt>, respectively. However, since the ruby text comes over the #PCDATA level, we cannot solve problem (B-2) under this policy. In addition, we have to solve the above problem and problem (B-3), securing the coexistence of ruby on the right-hand side and left-hand side, simultaneously.   Fig. 3: Examples for encoding ruby annotations    Fig. 4: Examples for ruby annotations on both sides  Imaginably, as shown in Fig. 4a, we will be able to suggest one solution to express the character on base text in a stand-off fashion, in case of the base text carries double ruby on the both sides. However, the above suggestion may work only the character string and both sides of ruby correspond each other. As shown in Fig. 4b, when ruby on either side does not fit or exceed the each target character, this suggestion would be inapplicable. Currently, we have difficulty in choosing a path that allows specifying both the object outline and structure of Japanese language together; therefore, a new definition over ruby annotation is needed as a means to this end. As shown in Fig. 5, our current solution is to mark up the manuscript in a way which makes it possible to output two types of XML file that specify the information of the object and the structure of Japanese language separately, rather than to fulfill both at the same time.   Fig. 5: Solution for problem (B); exporting two types of XML file  5. Conclusion In this study, developing a corpus for historical documents in a comprehensive and versatile way was considered the ultimate goal. We devised a tag-set based on the TEI-element, and structured Sharebon’s “Keisei-kai Futasuji-michi” as a model case. We examined the problems of (A) text formatting on #PCDATA level and (B) structuring ruby annotations which were previously unsolved. For problem (A), a concrete policy to bring morphological analysis into reality, by combing the element of <seg> and <choice>, was considered. For problem (B), major difficulties of structuring both outward information and the linguistic structure of Japanese documents at the same time were presented, and problems to solve were pointed out. Especially, ruby annotation is absolutely imperative for Japanese documents. This extra-textual addition is common and employed almost everywhere from historical documents to modern comics for highlighting the pronunciation or meaning of a word. Our future challenge is to examine a compromise that satisfies both structures simultaneously while working out the problems with the TEI Council.  Funding This work was supported by the collaborative research project “Design of a Diachronic Corpus” and “Study of the History of the Japanese Language Using Statistics and Machine Learning” carried at the National Institute for Japanese Language and Linguistics.   ",
       "article_title":"Problems in Encoding Documents of Early Modern Japanese",
       "authors":[
          {
             "given":"Akihiro",
             "family":"Kawase",
             "affiliation":[
                {
                   "original_name":"National Institute for Japanese Language and Linguistics, Japan",
                   "normalized_name":"National Institute for Japanese Language and Linguistics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02caqc891",
                      "GRID":"grid.471854.b"
                   }
                }
             ]
          },
          {
             "given":"Taro",
             "family":"Ichimura",
             "affiliation":[
                {
                   "original_name":"National Institute for Japanese Language and Linguistics, Japan",
                   "normalized_name":"National Institute for Japanese Language and Linguistics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02caqc891",
                      "GRID":"grid.471854.b"
                   }
                }
             ]
          },
          {
             "given":"Toshinobu",
             "family":"Ogiso",
             "affiliation":[
                {
                   "original_name":"National Institute for Japanese Language and Linguistics, Japan",
                   "normalized_name":"National Institute for Japanese Language and Linguistics",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02caqc891",
                      "GRID":"grid.471854.b"
                   }
                }
             ]
          }
       ],
       "publisher":"EPFL, Switzerland",
       "date":"2014-07-09",
       "keywords":[
          "asian studies",
          "sustainability and preservation",
          "repositories",
          "xml",
          "encoding - theory and practice",
          "archives",
          "text analysis"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    }
 ]